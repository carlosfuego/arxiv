[
    {
        "keyword": "kv cache",
        "arxiv_results": [
            {
                "id": "http://arxiv.org/abs/2412.20166v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.20166v2",
                "updated": "2025-01-15T01:34:46Z",
                "updated_parsed": [
                    2025,
                    1,
                    15,
                    1,
                    34,
                    46,
                    2,
                    15,
                    0
                ],
                "published": "2024-12-28T14:38:16Z",
                "published_parsed": [
                    2024,
                    12,
                    28,
                    14,
                    38,
                    16,
                    5,
                    363,
                    0
                ],
                "title": "LoL-PIM: Long-Context LLM Decoding with Scalable DRAM-PIM System",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LoL-PIM: Long-Context LLM Decoding with Scalable DRAM-PIM System"
                },
                "summary": "The expansion of large language models (LLMs) with hundreds of billions of\nparameters presents significant challenges to computational resources,\nparticularly data movement and memory bandwidth. Long-context LLMs, which\nprocess sequences of tens of thousands of tokens, further increase the demand\non the memory system as the complexity in attention layers and key-value cache\nsizes is proportional to the context length. Processing-in-Memory (PIM)\nmaximizes memory bandwidth by moving compute to the data and can address the\nmemory bandwidth challenges; however, PIM is not necessarily scalable to\naccelerate long-context LLM because of limited per-module memory capacity and\nthe inflexibility of fixed-functional unit PIM architecture and static memory\nmanagement. In this work, we propose LoL-PIM which is a multi-node PIM\narchitecture that accelerates long context LLM through hardware-software\nco-design. In particular, we propose how pipeline parallelism can be exploited\nacross a multi-PIM module while a direct PIM access (DPA) controller (or DMA\nfor PIM) is proposed that enables dynamic PIM memory management and results in\nefficient PIM utilization across a diverse range of context length. We\ndeveloped an MLIR-based compiler for LoL-PIM extending a commercial PIM-based\ncompiler where the software modifications were implemented and evaluated, while\nthe hardware changes were modeled in the simulator. Our evaluations demonstrate\nthat LoL-PIM significantly improves throughput and reduces latency for\nlong-context LLM inference, outperforming both multi-GPU and GPU-PIM systems\n(up to 8.54x and 16.0x speedup, respectively), thereby enabling more efficient\ndeployment of LLMs in real-world applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The expansion of large language models (LLMs) with hundreds of billions of\nparameters presents significant challenges to computational resources,\nparticularly data movement and memory bandwidth. Long-context LLMs, which\nprocess sequences of tens of thousands of tokens, further increase the demand\non the memory system as the complexity in attention layers and key-value cache\nsizes is proportional to the context length. Processing-in-Memory (PIM)\nmaximizes memory bandwidth by moving compute to the data and can address the\nmemory bandwidth challenges; however, PIM is not necessarily scalable to\naccelerate long-context LLM because of limited per-module memory capacity and\nthe inflexibility of fixed-functional unit PIM architecture and static memory\nmanagement. In this work, we propose LoL-PIM which is a multi-node PIM\narchitecture that accelerates long context LLM through hardware-software\nco-design. In particular, we propose how pipeline parallelism can be exploited\nacross a multi-PIM module while a direct PIM access (DPA) controller (or DMA\nfor PIM) is proposed that enables dynamic PIM memory management and results in\nefficient PIM utilization across a diverse range of context length. We\ndeveloped an MLIR-based compiler for LoL-PIM extending a commercial PIM-based\ncompiler where the software modifications were implemented and evaluated, while\nthe hardware changes were modeled in the simulator. Our evaluations demonstrate\nthat LoL-PIM significantly improves throughput and reduces latency for\nlong-context LLM inference, outperforming both multi-GPU and GPU-PIM systems\n(up to 8.54x and 16.0x speedup, respectively), thereby enabling more efficient\ndeployment of LLMs in real-world applications."
                },
                "authors": [
                    {
                        "name": "Hyucksung Kwon"
                    },
                    {
                        "name": "Kyungmo Koo"
                    },
                    {
                        "name": "Janghyeon Kim"
                    },
                    {
                        "name": "Woongkyu Lee"
                    },
                    {
                        "name": "Minjae Lee"
                    },
                    {
                        "name": "Hyungdeok Lee"
                    },
                    {
                        "name": "Yousub Jung"
                    },
                    {
                        "name": "Jaehan Park"
                    },
                    {
                        "name": "Yosub Song"
                    },
                    {
                        "name": "Byeongsu Yang"
                    },
                    {
                        "name": "Haerang Choi"
                    },
                    {
                        "name": "Guhyun Kim"
                    },
                    {
                        "name": "Jongsoon Won"
                    },
                    {
                        "name": "Woojae Shin"
                    },
                    {
                        "name": "Changhyun Kim"
                    },
                    {
                        "name": "Gyeongcheol Shin"
                    },
                    {
                        "name": "Yongkee Kwon"
                    },
                    {
                        "name": "Ilkon Kim"
                    },
                    {
                        "name": "Euicheol Lim"
                    },
                    {
                        "name": "John Kim"
                    },
                    {
                        "name": "Jungwook Choi"
                    }
                ],
                "author_detail": {
                    "name": "Jungwook Choi"
                },
                "author": "Jungwook Choi",
                "arxiv_comment": "15 pages, 12 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.20166v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.20166v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.08484v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.08484v1",
                "updated": "2025-01-14T23:13:14Z",
                "updated_parsed": [
                    2025,
                    1,
                    14,
                    23,
                    13,
                    14,
                    1,
                    14,
                    0
                ],
                "published": "2025-01-14T23:13:14Z",
                "published_parsed": [
                    2025,
                    1,
                    14,
                    23,
                    13,
                    14,
                    1,
                    14,
                    0
                ],
                "title": "CORD: Co-design of Resource Allocation and Deadline Decomposition with\n  Generative Profiling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CORD: Co-design of Resource Allocation and Deadline Decomposition with\n  Generative Profiling"
                },
                "summary": "As multicore hardware is becoming increasingly common in real-time systems,\ntraditional scheduling techniques that assume a single worst-case execution\ntime for a task are no longer adequate, since they ignore the impact of shared\nresources on execution time. When tasks execute concurrently on different\ncores, their execution times often vary substantially with their allocated\nbudgets of shared resources, such as cache and memory bandwidth. Even under a\nspecific resource allocation, the resource use pattern of a task also changes\nwith time during a job execution. It is therefore important to consider the\nrelationship between multicore resources and execution time in task modeling\nand scheduling algorithm design.\n  In this paper, we propose a much more precise execution model for DAG-based\nreal-time tasks that captures the time-varying resource use characteristics of\na task under different budgets of shared resources. We present a generative\nresource profiling algorithm that efficiently predicts, from limited\nmeasurement data, the resource profile of a task at any time during its\nexecution under a given resource budget. The generative profiles can then be\nused to construct the execution models for tasks, using which one can make\ninformed resource allocation decisions. We further introduce a multicore\nresource allocation and deadline decomposition co-design technique for\nDAG-based tasks that leverages the generated execution models to jointly\nallocate resources and deadlines to subtasks, to maximize resource efficiency\nand schedulability. Our evaluation results show that our generative profiling\nalgorithm achieves high accuracy while being efficient, and that our\nco-allocation technique substantially improves schedulability compared to a\nstate-of-the-art deadline decomposition method.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As multicore hardware is becoming increasingly common in real-time systems,\ntraditional scheduling techniques that assume a single worst-case execution\ntime for a task are no longer adequate, since they ignore the impact of shared\nresources on execution time. When tasks execute concurrently on different\ncores, their execution times often vary substantially with their allocated\nbudgets of shared resources, such as cache and memory bandwidth. Even under a\nspecific resource allocation, the resource use pattern of a task also changes\nwith time during a job execution. It is therefore important to consider the\nrelationship between multicore resources and execution time in task modeling\nand scheduling algorithm design.\n  In this paper, we propose a much more precise execution model for DAG-based\nreal-time tasks that captures the time-varying resource use characteristics of\na task under different budgets of shared resources. We present a generative\nresource profiling algorithm that efficiently predicts, from limited\nmeasurement data, the resource profile of a task at any time during its\nexecution under a given resource budget. The generative profiles can then be\nused to construct the execution models for tasks, using which one can make\ninformed resource allocation decisions. We further introduce a multicore\nresource allocation and deadline decomposition co-design technique for\nDAG-based tasks that leverages the generated execution models to jointly\nallocate resources and deadlines to subtasks, to maximize resource efficiency\nand schedulability. Our evaluation results show that our generative profiling\nalgorithm achieves high accuracy while being efficient, and that our\nco-allocation technique substantially improves schedulability compared to a\nstate-of-the-art deadline decomposition method."
                },
                "authors": [
                    {
                        "name": "Robert Gifford"
                    },
                    {
                        "name": "Abby Eisenklam"
                    },
                    {
                        "name": "Georgiy A. Bondar"
                    },
                    {
                        "name": "Yifan Cai"
                    },
                    {
                        "name": "Tushar Sial"
                    },
                    {
                        "name": "Linh Thi Xuan Phan"
                    },
                    {
                        "name": "Abhishek Halder"
                    }
                ],
                "author_detail": {
                    "name": "Abhishek Halder"
                },
                "author": "Abhishek Halder",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.08484v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.08484v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.OS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.OS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.02380v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.02380v3",
                "updated": "2025-01-14T20:04:15Z",
                "updated_parsed": [
                    2025,
                    1,
                    14,
                    20,
                    4,
                    15,
                    1,
                    14,
                    0
                ],
                "published": "2025-01-04T20:59:34Z",
                "published_parsed": [
                    2025,
                    1,
                    4,
                    20,
                    59,
                    34,
                    5,
                    4,
                    0
                ],
                "title": "Reciprocating Locks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reciprocating Locks"
                },
                "summary": "We present \"Reciprocating Locks\", a novel mutual exclusion locking algorithm,\ntargeting cache-coherent shared memory (CC), that enjoys a number of desirable\nproperties. The doorway arrival phase and the release operation both run in\nconstant-time. Waiting threads use local spinning and only a single waiting\nelement is required per thread, regardless of the number of locks a thread\nmight hold at a given time. While our lock does not provide strict FIFO\nadmission, it bounds bypass and has strong anti-starvation properties. The lock\nis compact, space efficient, and has been intentionally designed to be readily\nusable in real-world general purpose computing environments such as the linux\nkernel, pthreads, or C++. We show the lock exhibits high throughput under\ncontention and low latency in the uncontended case. The performance of\nReciprocating Locks is competitive with and often better than the best\nstate-of-the-art scalable spin locks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present \"Reciprocating Locks\", a novel mutual exclusion locking algorithm,\ntargeting cache-coherent shared memory (CC), that enjoys a number of desirable\nproperties. The doorway arrival phase and the release operation both run in\nconstant-time. Waiting threads use local spinning and only a single waiting\nelement is required per thread, regardless of the number of locks a thread\nmight hold at a given time. While our lock does not provide strict FIFO\nadmission, it bounds bypass and has strong anti-starvation properties. The lock\nis compact, space efficient, and has been intentionally designed to be readily\nusable in real-world general purpose computing environments such as the linux\nkernel, pthreads, or C++. We show the lock exhibits high throughput under\ncontention and low latency in the uncontended case. The performance of\nReciprocating Locks is competitive with and often better than the best\nstate-of-the-art scalable spin locks."
                },
                "authors": [
                    {
                        "name": "Dave Dice"
                    },
                    {
                        "name": "Alex Kogan"
                    }
                ],
                "author_detail": {
                    "name": "Alex Kogan"
                },
                "author": "Alex Kogan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.02380v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.02380v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "D.4.1",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.08192v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.08192v1",
                "updated": "2025-01-14T15:14:10Z",
                "updated_parsed": [
                    2025,
                    1,
                    14,
                    15,
                    14,
                    10,
                    1,
                    14,
                    0
                ],
                "published": "2025-01-14T15:14:10Z",
                "published_parsed": [
                    2025,
                    1,
                    14,
                    15,
                    14,
                    10,
                    1,
                    14,
                    0
                ],
                "title": "PRESERVE: Prefetching Model Weights and KV-Cache in Distributed LLM\n  Serving",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PRESERVE: Prefetching Model Weights and KV-Cache in Distributed LLM\n  Serving"
                },
                "summary": "Large language models (LLMs) are widely used across various applications, but\ntheir substantial computational requirements pose significant challenges,\nparticularly in terms of HBM bandwidth bottlenecks and inter-device\ncommunication overhead. In this paper, we present PRESERVE, a novel prefetching\nframework designed to optimize LLM inference by overlapping memory reads for\nmodel weights and KV-cache with collective communication operations. Through\nextensive experiments conducted on commercial AI accelerators, we demonstrate\nup to 1.6x end-to-end speedup on state-of-the-art, open-source LLMs.\nAdditionally, we perform a design space exploration that identifies the optimal\nhardware configuration for the proposed method, showing a further 1.25x\nimprovement in performance per cost by selecting the optimal L2 cache size. Our\nresults show that PRESERVE has the potential to mitigate the memory bottlenecks\nand communication overheads, offering a solution to improve the performance and\nscalability of the LLM inference systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) are widely used across various applications, but\ntheir substantial computational requirements pose significant challenges,\nparticularly in terms of HBM bandwidth bottlenecks and inter-device\ncommunication overhead. In this paper, we present PRESERVE, a novel prefetching\nframework designed to optimize LLM inference by overlapping memory reads for\nmodel weights and KV-cache with collective communication operations. Through\nextensive experiments conducted on commercial AI accelerators, we demonstrate\nup to 1.6x end-to-end speedup on state-of-the-art, open-source LLMs.\nAdditionally, we perform a design space exploration that identifies the optimal\nhardware configuration for the proposed method, showing a further 1.25x\nimprovement in performance per cost by selecting the optimal L2 cache size. Our\nresults show that PRESERVE has the potential to mitigate the memory bottlenecks\nand communication overheads, offering a solution to improve the performance and\nscalability of the LLM inference systems."
                },
                "authors": [
                    {
                        "name": "Ahmet Caner Yüzügüler"
                    },
                    {
                        "name": "Jiawei Zhuang"
                    },
                    {
                        "name": "Lukas Cavigelli"
                    }
                ],
                "author_detail": {
                    "name": "Lukas Cavigelli"
                },
                "author": "Lukas Cavigelli",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.08192v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.08192v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.15102v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.15102v2",
                "updated": "2025-01-14T14:07:55Z",
                "updated_parsed": [
                    2025,
                    1,
                    14,
                    14,
                    7,
                    55,
                    1,
                    14,
                    0
                ],
                "published": "2024-11-22T18:06:14Z",
                "published_parsed": [
                    2024,
                    11,
                    22,
                    18,
                    6,
                    14,
                    4,
                    327,
                    0
                ],
                "title": "AttriBoT: A Bag of Tricks for Efficiently Approximating Leave-One-Out\n  Context Attribution",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AttriBoT: A Bag of Tricks for Efficiently Approximating Leave-One-Out\n  Context Attribution"
                },
                "summary": "The influence of contextual input on the behavior of large language models\n(LLMs) has prompted the development of context attribution methods that aim to\nquantify each context span's effect on an LLM's generations. The leave-one-out\n(LOO) error, which measures the change in the likelihood of the LLM's response\nwhen a given span of the context is removed, provides a principled way to\nperform context attribution, but can be prohibitively expensive to compute for\nlarge models. In this work, we introduce AttriBoT, a series of novel techniques\nfor efficiently computing an approximation of the LOO error for context\nattribution. Specifically, AttriBoT uses cached activations to avoid redundant\noperations, performs hierarchical attribution to reduce computation, and\nemulates the behavior of large target models with smaller proxy models. Taken\ntogether, AttriBoT can provide a >300x speedup while remaining more faithful to\na target model's LOO error than prior context attribution methods. This stark\nincrease in performance makes computing context attributions for a given\nresponse 30x faster than generating the response itself, empowering real-world\napplications that require computing attributions at scale. We release a\nuser-friendly and efficient implementation of AttriBoT to enable efficient LLM\ninterpretability as well as encourage future development of efficient context\nattribution methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The influence of contextual input on the behavior of large language models\n(LLMs) has prompted the development of context attribution methods that aim to\nquantify each context span's effect on an LLM's generations. The leave-one-out\n(LOO) error, which measures the change in the likelihood of the LLM's response\nwhen a given span of the context is removed, provides a principled way to\nperform context attribution, but can be prohibitively expensive to compute for\nlarge models. In this work, we introduce AttriBoT, a series of novel techniques\nfor efficiently computing an approximation of the LOO error for context\nattribution. Specifically, AttriBoT uses cached activations to avoid redundant\noperations, performs hierarchical attribution to reduce computation, and\nemulates the behavior of large target models with smaller proxy models. Taken\ntogether, AttriBoT can provide a >300x speedup while remaining more faithful to\na target model's LOO error than prior context attribution methods. This stark\nincrease in performance makes computing context attributions for a given\nresponse 30x faster than generating the response itself, empowering real-world\napplications that require computing attributions at scale. We release a\nuser-friendly and efficient implementation of AttriBoT to enable efficient LLM\ninterpretability as well as encourage future development of efficient context\nattribution methods."
                },
                "authors": [
                    {
                        "name": "Fengyuan Liu"
                    },
                    {
                        "name": "Nikhil Kandpal"
                    },
                    {
                        "name": "Colin Raffel"
                    }
                ],
                "author_detail": {
                    "name": "Colin Raffel"
                },
                "author": "Colin Raffel",
                "arxiv_comment": "29 pages, 11 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.15102v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.15102v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.04987v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.04987v2",
                "updated": "2025-01-14T12:06:33Z",
                "updated_parsed": [
                    2025,
                    1,
                    14,
                    12,
                    6,
                    33,
                    1,
                    14,
                    0
                ],
                "published": "2025-01-09T06:00:27Z",
                "published_parsed": [
                    2025,
                    1,
                    9,
                    6,
                    0,
                    27,
                    3,
                    9,
                    0
                ],
                "title": "TreeKV: Smooth Key-Value Cache Compression with Tree Structures",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TreeKV: Smooth Key-Value Cache Compression with Tree Structures"
                },
                "summary": "Efficient key-value (KV) cache compression is critical for scaling\ntransformer-based Large Language Models (LLMs) in long sequences and\nresource-limited settings. Existing methods evict tokens based on their\npositions or importance scores, but position-based strategies can miss crucial\ninformation outside predefined regions, while those relying on global\nimportance scores resulting in strong regional biases, limiting the KV cache's\noverall context retention and potentially impairing the performance of LLMs on\ncomplex tasks. Our wavelet analysis reveals that as tokens approach the end of\nsequence, their contributions to generation gradually increase and tends to\ndiverge more from neighboring tokens, indicating a smooth transition with\nincreasing complexity and variability from distant to nearby context. Motivated\nby this observation, we propose TreeKV, an intuitive, training-free method that\nemploys a tree structure for smooth cache compression. TreeKV maintains a fixed\ncache size, allowing LLMs to deliver high-quality output even in long text\nscenarios. Unlike most compression methods, TreeKV is applicable to both the\ngeneration and prefilling stages. TreeKV consistently surpasses all baseline\nmodels in language modeling tasks on PG19 and OpenWebText2, allowing LLMs\ntrained with short context window to generalize to longer window with a 16x\ncache reduction. On the Longbench benchmark, TreeKV achieves the best\nperformance with only 6\\% of the budget at optimal efficiency.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient key-value (KV) cache compression is critical for scaling\ntransformer-based Large Language Models (LLMs) in long sequences and\nresource-limited settings. Existing methods evict tokens based on their\npositions or importance scores, but position-based strategies can miss crucial\ninformation outside predefined regions, while those relying on global\nimportance scores resulting in strong regional biases, limiting the KV cache's\noverall context retention and potentially impairing the performance of LLMs on\ncomplex tasks. Our wavelet analysis reveals that as tokens approach the end of\nsequence, their contributions to generation gradually increase and tends to\ndiverge more from neighboring tokens, indicating a smooth transition with\nincreasing complexity and variability from distant to nearby context. Motivated\nby this observation, we propose TreeKV, an intuitive, training-free method that\nemploys a tree structure for smooth cache compression. TreeKV maintains a fixed\ncache size, allowing LLMs to deliver high-quality output even in long text\nscenarios. Unlike most compression methods, TreeKV is applicable to both the\ngeneration and prefilling stages. TreeKV consistently surpasses all baseline\nmodels in language modeling tasks on PG19 and OpenWebText2, allowing LLMs\ntrained with short context window to generalize to longer window with a 16x\ncache reduction. On the Longbench benchmark, TreeKV achieves the best\nperformance with only 6\\% of the budget at optimal efficiency."
                },
                "authors": [
                    {
                        "name": "Ziwei He"
                    },
                    {
                        "name": "Jian Yuan"
                    },
                    {
                        "name": "Haoli Bai"
                    },
                    {
                        "name": "Jingwen Leng"
                    },
                    {
                        "name": "Bo Jiang"
                    }
                ],
                "author_detail": {
                    "name": "Bo Jiang"
                },
                "author": "Bo Jiang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.04987v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.04987v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2403.15896v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2403.15896v2",
                "updated": "2025-01-14T11:41:14Z",
                "updated_parsed": [
                    2025,
                    1,
                    14,
                    11,
                    41,
                    14,
                    1,
                    14,
                    0
                ],
                "published": "2024-03-23T17:38:57Z",
                "published_parsed": [
                    2024,
                    3,
                    23,
                    17,
                    38,
                    57,
                    5,
                    83,
                    0
                ],
                "title": "Cell-level modelling of homeostasis in confined epithelial monolayers",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cell-level modelling of homeostasis in confined epithelial monolayers"
                },
                "summary": "Tissue homeostasis, the biological process of maintaining a steady state in\ntissue via control of cell proliferation, death, and metabolic function, is\nessential for the development, growth, maintenance, and proper function of\nliving organisms. Disruptions to this process can lead to serious diseases and\neven death. In this study, we use the vertex model for the cell-level\ndescription of tissue mechanics to investigate the impact of the tissue\nmicroenvironment and local mechanical properties of cells on homeostasis in\nconfined epithelial tissues. We find a dynamic steady state, where the balance\nbetween cell divisions and removals sustains homeostasis. By characterising\nhomeostasis in terms of cell count, tissue area, and the cells' neighbour count\ndistribution, we identify the factors that govern regulated and ordered tissue\ngrowth. This work, therefore, sheds light on the mechanisms underlying tissue\nhomeostasis and highlights the importance of mechanics in the control of\nbiological processes such as tissue development and disease pathology.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Tissue homeostasis, the biological process of maintaining a steady state in\ntissue via control of cell proliferation, death, and metabolic function, is\nessential for the development, growth, maintenance, and proper function of\nliving organisms. Disruptions to this process can lead to serious diseases and\neven death. In this study, we use the vertex model for the cell-level\ndescription of tissue mechanics to investigate the impact of the tissue\nmicroenvironment and local mechanical properties of cells on homeostasis in\nconfined epithelial tissues. We find a dynamic steady state, where the balance\nbetween cell divisions and removals sustains homeostasis. By characterising\nhomeostasis in terms of cell count, tissue area, and the cells' neighbour count\ndistribution, we identify the factors that govern regulated and ordered tissue\ngrowth. This work, therefore, sheds light on the mechanisms underlying tissue\nhomeostasis and highlights the importance of mechanics in the control of\nbiological processes such as tissue development and disease pathology."
                },
                "authors": [
                    {
                        "name": "KVS Chaithanya"
                    },
                    {
                        "name": "Jan Rozman"
                    },
                    {
                        "name": "Andrej Košmrlj"
                    },
                    {
                        "name": "Rastko Sknepnek"
                    }
                ],
                "author_detail": {
                    "name": "Rastko Sknepnek"
                },
                "author": "Rastko Sknepnek",
                "arxiv_comment": "18 pages, 13 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2403.15896v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2403.15896v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.bio-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.bio-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cond-mat.soft",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.19255v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.19255v2",
                "updated": "2025-01-14T05:48:07Z",
                "updated_parsed": [
                    2025,
                    1,
                    14,
                    5,
                    48,
                    7,
                    1,
                    14,
                    0
                ],
                "published": "2024-12-26T15:45:45Z",
                "published_parsed": [
                    2024,
                    12,
                    26,
                    15,
                    45,
                    45,
                    3,
                    361,
                    0
                ],
                "title": "Multi-matrix Factorization Attention",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-matrix Factorization Attention"
                },
                "summary": "We propose novel attention architectures, Multi-matrix Factorization\nAttention (MFA) and MFA-Key-Reuse (MFA-KR). Existing variants for standard\nMulti-Head Attention (MHA), including SOTA methods like MLA, fail to maintain\nas strong performance under stringent Key-Value cache (KV cache) constraints.\nMFA enhances model capacity by efficiently scaling up both the number and\ndimension of attention heads through low-rank matrix factorization in the\nQuery-Key (QK) circuit. Extending MFA, MFA-KR further reduces memory\nrequirements by repurposing the key cache as value through value projection\nre-parameterization. MFA's design enables strong model capacity when working\nunder tight KV cache budget, while MFA-KR is suitable for even harsher KV cache\nlimits with minor performance trade-off. Notably, in our extensive and\nlarge-scale experiments, the proposed architecture outperforms MLA and performs\ncomparably to MHA, while reducing KV cache usage by up to 56% and 93.7%,\nrespectively.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We propose novel attention architectures, Multi-matrix Factorization\nAttention (MFA) and MFA-Key-Reuse (MFA-KR). Existing variants for standard\nMulti-Head Attention (MHA), including SOTA methods like MLA, fail to maintain\nas strong performance under stringent Key-Value cache (KV cache) constraints.\nMFA enhances model capacity by efficiently scaling up both the number and\ndimension of attention heads through low-rank matrix factorization in the\nQuery-Key (QK) circuit. Extending MFA, MFA-KR further reduces memory\nrequirements by repurposing the key cache as value through value projection\nre-parameterization. MFA's design enables strong model capacity when working\nunder tight KV cache budget, while MFA-KR is suitable for even harsher KV cache\nlimits with minor performance trade-off. Notably, in our extensive and\nlarge-scale experiments, the proposed architecture outperforms MLA and performs\ncomparably to MHA, while reducing KV cache usage by up to 56% and 93.7%,\nrespectively."
                },
                "authors": [
                    {
                        "name": "Jingcheng Hu"
                    },
                    {
                        "name": "Houyi Li"
                    },
                    {
                        "name": "Yinmin Zhang"
                    },
                    {
                        "name": "Zili Wang"
                    },
                    {
                        "name": "Shuigeng Zhou"
                    },
                    {
                        "name": "Xiangyu Zhang"
                    },
                    {
                        "name": "Heung-Yeung Shum"
                    },
                    {
                        "name": "Daxin Jiang"
                    }
                ],
                "author_detail": {
                    "name": "Daxin Jiang"
                },
                "author": "Daxin Jiang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.19255v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.19255v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.10480v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.10480v2",
                "updated": "2025-01-14T05:00:34Z",
                "updated_parsed": [
                    2025,
                    1,
                    14,
                    5,
                    0,
                    34,
                    1,
                    14,
                    0
                ],
                "published": "2024-05-17T00:52:39Z",
                "published_parsed": [
                    2024,
                    5,
                    17,
                    0,
                    52,
                    39,
                    4,
                    138,
                    0
                ],
                "title": "Lean Attention: Hardware-Aware Scalable Attention Mechanism for the\n  Decode-Phase of Transformers",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Lean Attention: Hardware-Aware Scalable Attention Mechanism for the\n  Decode-Phase of Transformers"
                },
                "summary": "Transformer-based models have emerged as one of the most widely used\narchitectures for natural language processing, natural language generation, and\nimage generation. The size of the state-of-the-art models has increased\nsteadily reaching billions of parameters. These huge models are memory hungry\nand incur significant inference latency even on cutting edge AI-accelerators,\nsuch as GPUs. Specifically, the time and memory complexity of the attention\noperation is quadratic in terms of the total context length, i.e., prompt and\noutput tokens. Thus, several optimizations such as key-value tensor caching and\nFlashAttention computation have been proposed to deliver the low latency\ndemands of applications relying on such large models. However, these techniques\ndo not cater to the computationally distinct nature of different phases during\ninference.\n  To that end, we propose LeanAttention, a scalable technique of computing\nself-attention for the token-generation phase (decode-phase) of decoder-only\ntransformer models. LeanAttention enables scaling the attention mechanism\nimplementation for the challenging case of long context lengths by re-designing\nthe execution flow for the decode-phase. We identify that the associative\nproperty of online softmax can be treated as a reduction operation thus\nallowing us to parallelize the attention computation over these large context\nlengths. We extend the \"stream-K\" style reduction of tiled calculation to\nself-attention to enable parallel computation resulting in an average of 2.6x\nattention execution speedup over FlashAttention-2 and up to 8.33x speedup for\n512k context lengths.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Transformer-based models have emerged as one of the most widely used\narchitectures for natural language processing, natural language generation, and\nimage generation. The size of the state-of-the-art models has increased\nsteadily reaching billions of parameters. These huge models are memory hungry\nand incur significant inference latency even on cutting edge AI-accelerators,\nsuch as GPUs. Specifically, the time and memory complexity of the attention\noperation is quadratic in terms of the total context length, i.e., prompt and\noutput tokens. Thus, several optimizations such as key-value tensor caching and\nFlashAttention computation have been proposed to deliver the low latency\ndemands of applications relying on such large models. However, these techniques\ndo not cater to the computationally distinct nature of different phases during\ninference.\n  To that end, we propose LeanAttention, a scalable technique of computing\nself-attention for the token-generation phase (decode-phase) of decoder-only\ntransformer models. LeanAttention enables scaling the attention mechanism\nimplementation for the challenging case of long context lengths by re-designing\nthe execution flow for the decode-phase. We identify that the associative\nproperty of online softmax can be treated as a reduction operation thus\nallowing us to parallelize the attention computation over these large context\nlengths. We extend the \"stream-K\" style reduction of tiled calculation to\nself-attention to enable parallel computation resulting in an average of 2.6x\nattention execution speedup over FlashAttention-2 and up to 8.33x speedup for\n512k context lengths."
                },
                "authors": [
                    {
                        "name": "Rya Sanovar"
                    },
                    {
                        "name": "Srikant Bharadwaj"
                    },
                    {
                        "name": "Renee St. Amant"
                    },
                    {
                        "name": "Victor Rühle"
                    },
                    {
                        "name": "Saravan Rajmohan"
                    }
                ],
                "author_detail": {
                    "name": "Saravan Rajmohan"
                },
                "author": "Saravan Rajmohan",
                "arxiv_comment": "13 pages, 10 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.10480v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.10480v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.7; C.1.4",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.05262v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.05262v2",
                "updated": "2025-01-14T02:02:01Z",
                "updated_parsed": [
                    2025,
                    1,
                    14,
                    2,
                    2,
                    1,
                    1,
                    14,
                    0
                ],
                "published": "2025-01-09T14:16:43Z",
                "published_parsed": [
                    2025,
                    1,
                    9,
                    14,
                    16,
                    43,
                    3,
                    9,
                    0
                ],
                "title": "QMDB: Quick Merkle Database",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "QMDB: Quick Merkle Database"
                },
                "summary": "Quick Merkle Database (QMDB) addresses longstanding bottlenecks in blockchain\nstate management by integrating key-value (KV) and Merkle tree storage into a\nsingle unified architecture. QMDB delivers a significant throughput improvement\nover existing architectures, achieving up to 6X over the widely used RocksDB\nand 8X over NOMT, a leading verifiable database. Its novel append-only\ntwig-based design enables one SSD read per state access, O(1) IOs for updates,\nand in-memory Merkleization on a memory footprint as small as 2.3 bytes per\nentry, enabling it to run on even modest consumer-grade PCs. QMDB scales\nseamlessly across both commodity and enterprise hardware, achieving up to 2.28\nmillion state updates per second. This performance enables support for 1\nmillion token transfers per second (TPS), marking QMDB as the first solution\nachieving such a milestone. QMDB has been benchmarked with workloads exceeding\n15 billion entries (10X Ethereum's 2024 state) and has proven the capacity to\nscale to 280 billion entries on a single server. Furthermore, QMDB introduces\nhistorical proofs, unlocking the ability to query its blockchain's historical\nstate at the latest block. QMDB not only meets the demands of current\nblockchains but also provides a robust foundation for building scalable,\nefficient, and verifiable decentralized applications across diverse use cases.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Quick Merkle Database (QMDB) addresses longstanding bottlenecks in blockchain\nstate management by integrating key-value (KV) and Merkle tree storage into a\nsingle unified architecture. QMDB delivers a significant throughput improvement\nover existing architectures, achieving up to 6X over the widely used RocksDB\nand 8X over NOMT, a leading verifiable database. Its novel append-only\ntwig-based design enables one SSD read per state access, O(1) IOs for updates,\nand in-memory Merkleization on a memory footprint as small as 2.3 bytes per\nentry, enabling it to run on even modest consumer-grade PCs. QMDB scales\nseamlessly across both commodity and enterprise hardware, achieving up to 2.28\nmillion state updates per second. This performance enables support for 1\nmillion token transfers per second (TPS), marking QMDB as the first solution\nachieving such a milestone. QMDB has been benchmarked with workloads exceeding\n15 billion entries (10X Ethereum's 2024 state) and has proven the capacity to\nscale to 280 billion entries on a single server. Furthermore, QMDB introduces\nhistorical proofs, unlocking the ability to query its blockchain's historical\nstate at the latest block. QMDB not only meets the demands of current\nblockchains but also provides a robust foundation for building scalable,\nefficient, and verifiable decentralized applications across diverse use cases."
                },
                "authors": [
                    {
                        "name": "Isaac Zhang"
                    },
                    {
                        "name": "Ryan Zarick"
                    },
                    {
                        "name": "Daniel Wong"
                    },
                    {
                        "name": "Thomas Kim"
                    },
                    {
                        "name": "Bryan Pellegrino"
                    },
                    {
                        "name": "Mignon Li"
                    },
                    {
                        "name": "Kelvin Wong"
                    }
                ],
                "author_detail": {
                    "name": "Kelvin Wong"
                },
                "author": "Kelvin Wong",
                "arxiv_comment": "11 pages, 3 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.05262v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.05262v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.07523v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.07523v1",
                "updated": "2025-01-13T17:50:30Z",
                "updated_parsed": [
                    2025,
                    1,
                    13,
                    17,
                    50,
                    30,
                    0,
                    13,
                    0
                ],
                "published": "2025-01-13T17:50:30Z",
                "published_parsed": [
                    2025,
                    1,
                    13,
                    17,
                    50,
                    30,
                    0,
                    13,
                    0
                ],
                "title": "Parallel Key-Value Cache Fusion for Position Invariant RAG",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Parallel Key-Value Cache Fusion for Position Invariant RAG"
                },
                "summary": "Recent advancements in Large Language Models (LLMs) underscore the necessity\nof Retrieval Augmented Generation (RAG) to leverage external information.\nHowever, LLMs are sensitive to the position of relevant information within\ncontexts and tend to generate incorrect responses when such information is\nplaced in the middle, known as `Lost in the Middle' phenomenon. In this paper,\nwe introduce a framework that generates consistent outputs for decoder-only\nmodels, irrespective of the input context order. Experimental results for three\nopen domain question answering tasks demonstrate position invariance, where the\nmodel is not sensitive to input context order, and superior robustness to\nirrelevent passages compared to prevailing approaches for RAG pipelines.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advancements in Large Language Models (LLMs) underscore the necessity\nof Retrieval Augmented Generation (RAG) to leverage external information.\nHowever, LLMs are sensitive to the position of relevant information within\ncontexts and tend to generate incorrect responses when such information is\nplaced in the middle, known as `Lost in the Middle' phenomenon. In this paper,\nwe introduce a framework that generates consistent outputs for decoder-only\nmodels, irrespective of the input context order. Experimental results for three\nopen domain question answering tasks demonstrate position invariance, where the\nmodel is not sensitive to input context order, and superior robustness to\nirrelevent passages compared to prevailing approaches for RAG pipelines."
                },
                "authors": [
                    {
                        "name": "Philhoon Oh"
                    },
                    {
                        "name": "Jinwoo Shin"
                    },
                    {
                        "name": "James Thorne"
                    }
                ],
                "author_detail": {
                    "name": "James Thorne"
                },
                "author": "James Thorne",
                "arxiv_comment": "5 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.07523v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.07523v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.07752v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.07752v2",
                "updated": "2025-01-13T17:34:22Z",
                "updated_parsed": [
                    2025,
                    1,
                    13,
                    17,
                    34,
                    22,
                    0,
                    13,
                    0
                ],
                "published": "2024-12-10T18:50:37Z",
                "published_parsed": [
                    2024,
                    12,
                    10,
                    18,
                    50,
                    37,
                    1,
                    345,
                    0
                ],
                "title": "FlashRNN: Optimizing Traditional RNNs on Modern Hardware",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FlashRNN: Optimizing Traditional RNNs on Modern Hardware"
                },
                "summary": "While Transformers and other sequence-parallelizable neural network\narchitectures seem like the current state of the art in sequence modeling, they\nspecifically lack state-tracking capabilities. These are important for\ntime-series tasks and logical reasoning. Traditional RNNs like LSTMs and GRUs,\nas well as modern variants like sLSTM do have these capabilities at the cost of\nstrictly sequential processing. While this is often seen as a strong\nlimitation, we show how fast these networks can get with our\nhardware-optimization FlashRNN in Triton and CUDA, optimizing kernels to the\nregister level on modern GPUs. We extend traditional RNNs with a\nparallelization variant that processes multiple RNNs of smaller hidden state in\nparallel, similar to the head-wise processing in Transformers. To enable\nflexibility on different GPU variants, we introduce a new optimization\nframework for hardware-internal cache sizes, memory and compute handling. It\nmodels the hardware in a setting using polyhedral-like constraints, including\nthe notion of divisibility. This speeds up the solution process in our\nConstrINT library for general integer constraint satisfaction problems (integer\nCSPs). We show that our kernels can achieve 50x speed-ups over a vanilla\nPyTorch implementation and allow 40x larger hidden sizes compared to our Triton\nimplementation. Our open-source kernels and the optimization library are\nreleased here to boost research in the direction of state-tracking enabled RNNs\nand sequence modeling: \\url{https://github.com/NX-AI/flashrnn}",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While Transformers and other sequence-parallelizable neural network\narchitectures seem like the current state of the art in sequence modeling, they\nspecifically lack state-tracking capabilities. These are important for\ntime-series tasks and logical reasoning. Traditional RNNs like LSTMs and GRUs,\nas well as modern variants like sLSTM do have these capabilities at the cost of\nstrictly sequential processing. While this is often seen as a strong\nlimitation, we show how fast these networks can get with our\nhardware-optimization FlashRNN in Triton and CUDA, optimizing kernels to the\nregister level on modern GPUs. We extend traditional RNNs with a\nparallelization variant that processes multiple RNNs of smaller hidden state in\nparallel, similar to the head-wise processing in Transformers. To enable\nflexibility on different GPU variants, we introduce a new optimization\nframework for hardware-internal cache sizes, memory and compute handling. It\nmodels the hardware in a setting using polyhedral-like constraints, including\nthe notion of divisibility. This speeds up the solution process in our\nConstrINT library for general integer constraint satisfaction problems (integer\nCSPs). We show that our kernels can achieve 50x speed-ups over a vanilla\nPyTorch implementation and allow 40x larger hidden sizes compared to our Triton\nimplementation. Our open-source kernels and the optimization library are\nreleased here to boost research in the direction of state-tracking enabled RNNs\nand sequence modeling: \\url{https://github.com/NX-AI/flashrnn}"
                },
                "authors": [
                    {
                        "name": "Korbinian Pöppel"
                    },
                    {
                        "name": "Maximilian Beck"
                    },
                    {
                        "name": "Sepp Hochreiter"
                    }
                ],
                "author_detail": {
                    "name": "Sepp Hochreiter"
                },
                "author": "Sepp Hochreiter",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.07752v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.07752v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.07533v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.07533v4",
                "updated": "2025-01-13T09:33:25Z",
                "updated_parsed": [
                    2025,
                    1,
                    13,
                    9,
                    33,
                    25,
                    0,
                    13,
                    0
                ],
                "published": "2024-05-13T08:03:32Z",
                "published_parsed": [
                    2024,
                    5,
                    13,
                    8,
                    3,
                    32,
                    0,
                    134,
                    0
                ],
                "title": "DID Link: Authentication in TLS with Decentralized Identifiers and\n  Verifiable Credentials",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DID Link: Authentication in TLS with Decentralized Identifiers and\n  Verifiable Credentials"
                },
                "summary": "Authentication in TLS is predominately carried out with X.509 digital\ncertificates issued by certificate authorities (CA). The centralized nature of\ncurrent public key infrastructures, however, comes along with severe risks,\nsuch as single points of failure and susceptibility to cyber-attacks,\npotentially undermining the security and trustworthiness of the entire system.\nWith Decentralized Identifiers (DID) alongside distributed ledger technology,\nit becomes technically feasible to prove ownership of a unique identifier\nwithout requiring an attestation of the proof's public key by a centralized and\ntherefore vulnerable CA. This article presents DID Link, a novel authentication\nscheme for TLS 1.3 that empowers entities to authenticate in a TLS-compliant\nway with self-issued X.509 certificates that are equipped with ledger-anchored\nDIDs instead of CA-issued identifiers. It facilitates the exchange of\ntamper-proof and 3rd-party attested claims in the form of DID-bound Verifiable\nCredentials after the TLS handshake to complete the authentication with a full\nidentification of the communication partner. A prototypical implementation\nshows comparable TLS handshake durations of DID Link if verification material\nis cached and reasonable prolongations if it is obtained from a ledger. The\nsignificant speed improvement of the resulting TLS channel over a widely used,\nDID-based alternative transport protocol on the application layer demonstrates\nthe potential of DID Link to become a viable solution for the establishment of\nsecure and trustful end-to-end communication links with decentrally managed\ndigital identities.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Authentication in TLS is predominately carried out with X.509 digital\ncertificates issued by certificate authorities (CA). The centralized nature of\ncurrent public key infrastructures, however, comes along with severe risks,\nsuch as single points of failure and susceptibility to cyber-attacks,\npotentially undermining the security and trustworthiness of the entire system.\nWith Decentralized Identifiers (DID) alongside distributed ledger technology,\nit becomes technically feasible to prove ownership of a unique identifier\nwithout requiring an attestation of the proof's public key by a centralized and\ntherefore vulnerable CA. This article presents DID Link, a novel authentication\nscheme for TLS 1.3 that empowers entities to authenticate in a TLS-compliant\nway with self-issued X.509 certificates that are equipped with ledger-anchored\nDIDs instead of CA-issued identifiers. It facilitates the exchange of\ntamper-proof and 3rd-party attested claims in the form of DID-bound Verifiable\nCredentials after the TLS handshake to complete the authentication with a full\nidentification of the communication partner. A prototypical implementation\nshows comparable TLS handshake durations of DID Link if verification material\nis cached and reasonable prolongations if it is obtained from a ledger. The\nsignificant speed improvement of the resulting TLS channel over a widely used,\nDID-based alternative transport protocol on the application layer demonstrates\nthe potential of DID Link to become a viable solution for the establishment of\nsecure and trustful end-to-end communication links with decentrally managed\ndigital identities."
                },
                "authors": [
                    {
                        "name": "Sandro Rodriguez Garzon"
                    },
                    {
                        "name": "Dennis Natusch"
                    },
                    {
                        "name": "Artur Philipp"
                    },
                    {
                        "name": "Axel Küpper"
                    },
                    {
                        "name": "Hans Joachim Einsiedler"
                    },
                    {
                        "name": "Daniela Schneider"
                    }
                ],
                "author_detail": {
                    "name": "Daniela Schneider"
                },
                "author": "Daniela Schneider",
                "arxiv_doi": "10.1109/PST62714.2024.10788053",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1109/PST62714.2024.10788053",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2405.07533v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.07533v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "Accepted by and presented at 21st Annual International Conference on\n  Privacy, Security, and Trust (PST2024)",
                "arxiv_journal_ref": "2024 21st Annual International Conference on Privacy, Security and\n  Trust (PST), 2024, pp. 1-11",
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.07056v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.07056v1",
                "updated": "2025-01-13T04:31:04Z",
                "updated_parsed": [
                    2025,
                    1,
                    13,
                    4,
                    31,
                    4,
                    0,
                    13,
                    0
                ],
                "published": "2025-01-13T04:31:04Z",
                "published_parsed": [
                    2025,
                    1,
                    13,
                    4,
                    31,
                    4,
                    0,
                    13,
                    0
                ],
                "title": "Generating Data Locality to Accelerate Sparse Matrix-Matrix\n  Multiplication on CPUs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Generating Data Locality to Accelerate Sparse Matrix-Matrix\n  Multiplication on CPUs"
                },
                "summary": "Sparse GEneral Matrix-matrix Multiplication (SpGEMM) is a critical operation\nin many applications. Current multithreaded implementations are based on\nGustavson's algorithm and often perform poorly on large matrices due to limited\ncache reuse by the accumulators. We present MAGNUS (Matrix Algebra for Gigantic\nNUmerical Systems), a novel algorithm to maximize data locality in SpGEMM. To\ngenerate locality, MAGNUS reorders the intermediate product into discrete\ncache-friendly chunks using a two-level hierarchical approach. The accumulator\nis applied to each chunk, where the chunk size is chosen such that the\naccumulator is cache-efficient. MAGNUS is input- and system-aware: based on the\nmatrix characteristics and target system specifications, the optimal number of\nchunks is computed by minimizing the storage cost of the necessary data\nstructures. MAGNUS allows for a hybrid accumulation strategy in which each\nchunk uses a different accumulator based on an input threshold. We consider two\naccumulators: an AVX-512 vectorized bitonic sorting algorithm and classical\ndense accumulation. An OpenMP implementation of MAGNUS is compared with several\nbaselines for a variety of different matrices on three Intel x86 architectures.\nFor matrices from the SuiteSparse collection, MAGNUS is faster than all the\nbaselines in most cases and is orders of magnitude faster than Intel MKL for\nseveral matrices. For massive random matrices that model social network graphs,\nMAGNUS scales to the largest matrix sizes, while the baselines fail to do so.\nFurthermore, MAGNUS is close to the optimal bound for these matrices,\nregardless of the matrix size, structure, and density.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Sparse GEneral Matrix-matrix Multiplication (SpGEMM) is a critical operation\nin many applications. Current multithreaded implementations are based on\nGustavson's algorithm and often perform poorly on large matrices due to limited\ncache reuse by the accumulators. We present MAGNUS (Matrix Algebra for Gigantic\nNUmerical Systems), a novel algorithm to maximize data locality in SpGEMM. To\ngenerate locality, MAGNUS reorders the intermediate product into discrete\ncache-friendly chunks using a two-level hierarchical approach. The accumulator\nis applied to each chunk, where the chunk size is chosen such that the\naccumulator is cache-efficient. MAGNUS is input- and system-aware: based on the\nmatrix characteristics and target system specifications, the optimal number of\nchunks is computed by minimizing the storage cost of the necessary data\nstructures. MAGNUS allows for a hybrid accumulation strategy in which each\nchunk uses a different accumulator based on an input threshold. We consider two\naccumulators: an AVX-512 vectorized bitonic sorting algorithm and classical\ndense accumulation. An OpenMP implementation of MAGNUS is compared with several\nbaselines for a variety of different matrices on three Intel x86 architectures.\nFor matrices from the SuiteSparse collection, MAGNUS is faster than all the\nbaselines in most cases and is orders of magnitude faster than Intel MKL for\nseveral matrices. For massive random matrices that model social network graphs,\nMAGNUS scales to the largest matrix sizes, while the baselines fail to do so.\nFurthermore, MAGNUS is close to the optimal bound for these matrices,\nregardless of the matrix size, structure, and density."
                },
                "authors": [
                    {
                        "name": "Jordi Wolfson-Pou"
                    },
                    {
                        "name": "Jan Laukemann"
                    },
                    {
                        "name": "Fabrizio Petrini"
                    }
                ],
                "author_detail": {
                    "name": "Fabrizio Petrini"
                },
                "author": "Fabrizio Petrini",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.07056v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.07056v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.03058v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.03058v5",
                "updated": "2025-01-13T03:11:28Z",
                "updated_parsed": [
                    2025,
                    1,
                    13,
                    3,
                    11,
                    28,
                    0,
                    13,
                    0
                ],
                "published": "2024-05-05T21:41:43Z",
                "published_parsed": [
                    2024,
                    5,
                    5,
                    21,
                    41,
                    43,
                    6,
                    126,
                    0
                ],
                "title": "A Unified Framework for Automated Code Transformation and Pragma\n  Insertion",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Unified Framework for Automated Code Transformation and Pragma\n  Insertion"
                },
                "summary": "High-level synthesis, source-to-source compilers, and various Design Space\nExploration techniques for pragma insertion have significantly improved the\nQuality of Results of generated designs. These tools offer benefits such as\nreduced development time and enhanced performance. However, achieving\nhigh-quality results often requires additional manual code transformations and\ntiling selections, which are typically performed separately or as\npre-processing steps. Although DSE techniques enable code transformation\nupfront, the vastness of the search space often limits the exploration of all\npossible code transformations, making it challenging to determine which\ntransformations are necessary. Additionally, ensuring correctness remains\nchallenging, especially for complex transformations and optimizations.\n  To tackle this obstacle, we first propose a comprehensive framework\nleveraging HLS compilers. Our system streamlines code transformation, pragma\ninsertion, and tiles size selection for on-chip data caching through a unified\noptimization problem, aiming to enhance parallelization, particularly\nbeneficial for computation-bound kernels. Them employing a novel Non-Linear\nProgramming (NLP) approach, we simultaneously ascertain transformations,\npragmas, and tile sizes, focusing on regular loop-based kernels. Our evaluation\ndemonstrates that our framework adeptly identifies the appropriate\ntransformations, including scenarios where no transformation is necessary, and\ninserts pragmas to achieve a favorable Quality of Results.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "High-level synthesis, source-to-source compilers, and various Design Space\nExploration techniques for pragma insertion have significantly improved the\nQuality of Results of generated designs. These tools offer benefits such as\nreduced development time and enhanced performance. However, achieving\nhigh-quality results often requires additional manual code transformations and\ntiling selections, which are typically performed separately or as\npre-processing steps. Although DSE techniques enable code transformation\nupfront, the vastness of the search space often limits the exploration of all\npossible code transformations, making it challenging to determine which\ntransformations are necessary. Additionally, ensuring correctness remains\nchallenging, especially for complex transformations and optimizations.\n  To tackle this obstacle, we first propose a comprehensive framework\nleveraging HLS compilers. Our system streamlines code transformation, pragma\ninsertion, and tiles size selection for on-chip data caching through a unified\noptimization problem, aiming to enhance parallelization, particularly\nbeneficial for computation-bound kernels. Them employing a novel Non-Linear\nProgramming (NLP) approach, we simultaneously ascertain transformations,\npragmas, and tile sizes, focusing on regular loop-based kernels. Our evaluation\ndemonstrates that our framework adeptly identifies the appropriate\ntransformations, including scenarios where no transformation is necessary, and\ninserts pragmas to achieve a favorable Quality of Results."
                },
                "authors": [
                    {
                        "name": "Stéphane Pouget"
                    },
                    {
                        "name": "Louis-Noël Pouchet"
                    },
                    {
                        "name": "Jason Cong"
                    }
                ],
                "author_detail": {
                    "name": "Jason Cong"
                },
                "author": "Jason Cong",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.03058v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.03058v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.06872v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.06872v1",
                "updated": "2025-01-12T17:01:40Z",
                "updated_parsed": [
                    2025,
                    1,
                    12,
                    17,
                    1,
                    40,
                    6,
                    12,
                    0
                ],
                "published": "2025-01-12T17:01:40Z",
                "published_parsed": [
                    2025,
                    1,
                    12,
                    17,
                    1,
                    40,
                    6,
                    12,
                    0
                ],
                "title": "On Optimizing Locality of Graph Transposition on Modern Architectures",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "On Optimizing Locality of Graph Transposition on Modern Architectures"
                },
                "summary": "This paper investigates the shared-memory Graph Transposition (GT) problem, a\nfundamental graph algorithm that is widely used in graph analytics and\nscientific computing.\n  Previous GT algorithms have significant memory requirements that are\nproportional to the number of vertices and threads which obstructs their use on\nlarge graphs. Moreover, atomic memory operations have become comparably fast on\nrecent CPU architectures, which creates new opportunities for improving the\nperformance of concurrent atomic accesses in GT.\n  We design PoTra, a GT algorithm which leverages graph structure and processor\nand memory architecture to optimize locality and performance. PoTra limits the\nsize of additional data structures close to CPU cache sizes and utilizes the\nskewed degree distribution of graph datasets to optimize locality and\nperformance. We present the performance model of PoTra to explain the\nconnection between cache and memory response times and graph locality.\n  Our evaluation of PoTra on three CPU architectures and 20 real-world and\nsynthetic graph datasets with up to 128 billion edges demonstrates that PoTra\nachieves up to 8.7 times speedup compared to previous works and if there is a\nperformance loss it remains limited to 15.7%, on average.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper investigates the shared-memory Graph Transposition (GT) problem, a\nfundamental graph algorithm that is widely used in graph analytics and\nscientific computing.\n  Previous GT algorithms have significant memory requirements that are\nproportional to the number of vertices and threads which obstructs their use on\nlarge graphs. Moreover, atomic memory operations have become comparably fast on\nrecent CPU architectures, which creates new opportunities for improving the\nperformance of concurrent atomic accesses in GT.\n  We design PoTra, a GT algorithm which leverages graph structure and processor\nand memory architecture to optimize locality and performance. PoTra limits the\nsize of additional data structures close to CPU cache sizes and utilizes the\nskewed degree distribution of graph datasets to optimize locality and\nperformance. We present the performance model of PoTra to explain the\nconnection between cache and memory response times and graph locality.\n  Our evaluation of PoTra on three CPU architectures and 20 real-world and\nsynthetic graph datasets with up to 128 billion edges demonstrates that PoTra\nachieves up to 8.7 times speedup compared to previous works and if there is a\nperformance loss it remains limited to 15.7%, on average."
                },
                "authors": [
                    {
                        "name": "Mohsen Koohi Esfahani"
                    },
                    {
                        "name": "Hans Vandierendonck"
                    }
                ],
                "author_detail": {
                    "name": "Hans Vandierendonck"
                },
                "author": "Hans Vandierendonck",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.06872v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.06872v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.06807v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.06807v1",
                "updated": "2025-01-12T13:18:04Z",
                "updated_parsed": [
                    2025,
                    1,
                    12,
                    13,
                    18,
                    4,
                    6,
                    12,
                    0
                ],
                "published": "2025-01-12T13:18:04Z",
                "published_parsed": [
                    2025,
                    1,
                    12,
                    13,
                    18,
                    4,
                    6,
                    12,
                    0
                ],
                "title": "MPCache: MPC-Friendly KV Cache Eviction for Efficient Private Large\n  Language Model Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MPCache: MPC-Friendly KV Cache Eviction for Efficient Private Large\n  Language Model Inference"
                },
                "summary": "Private large language model (LLM) inference based on secure multi-party\ncomputation (MPC) offers cryptographically-secure protection for both user\nprompt and proprietary model weights. However, it suffers from large latency\noverhead especially for long input sequences. While key-value (KV) cache\neviction algorithms have been proposed to reduce the computation and memory\ncost for plaintext inference, they are not designed for MPC and cannot benefit\nprivate inference easily. In this paper, we propose an accurate and\nMPC-friendly KV cache eviction framework, dubbed MPCache. MPCache is built on\nthe observation that historical tokens in a long sequence may have different\neffects on the downstream decoding. Hence, MPCache combines a look-once static\neviction algorithm to discard unimportant tokens and a query-aware dynamic\nselection algorithm to further select a small subset of tokens for attention\ncomputation. As existing dynamic selection algorithms incur too much latency,\nwe propose a series of optimizations to drastically reduce the KV cache\nselection overhead, including MPC-friendly similarity approximation,\nhierarchical KV cache clustering, and cross-layer index sharing strategy. With\nextensive experiments, we demonstrate that MPCache consistently outperforms\nprior-art KV cache eviction baselines across different LLM generation tasks and\nachieves 1.8~2.01x and 3.39~8.37x decoding latency and communication reduction\non different sequence lengths, respectively.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Private large language model (LLM) inference based on secure multi-party\ncomputation (MPC) offers cryptographically-secure protection for both user\nprompt and proprietary model weights. However, it suffers from large latency\noverhead especially for long input sequences. While key-value (KV) cache\neviction algorithms have been proposed to reduce the computation and memory\ncost for plaintext inference, they are not designed for MPC and cannot benefit\nprivate inference easily. In this paper, we propose an accurate and\nMPC-friendly KV cache eviction framework, dubbed MPCache. MPCache is built on\nthe observation that historical tokens in a long sequence may have different\neffects on the downstream decoding. Hence, MPCache combines a look-once static\neviction algorithm to discard unimportant tokens and a query-aware dynamic\nselection algorithm to further select a small subset of tokens for attention\ncomputation. As existing dynamic selection algorithms incur too much latency,\nwe propose a series of optimizations to drastically reduce the KV cache\nselection overhead, including MPC-friendly similarity approximation,\nhierarchical KV cache clustering, and cross-layer index sharing strategy. With\nextensive experiments, we demonstrate that MPCache consistently outperforms\nprior-art KV cache eviction baselines across different LLM generation tasks and\nachieves 1.8~2.01x and 3.39~8.37x decoding latency and communication reduction\non different sequence lengths, respectively."
                },
                "authors": [
                    {
                        "name": "Wenxuan Zeng"
                    },
                    {
                        "name": "Ye Dong"
                    },
                    {
                        "name": "Jinjin Zhou"
                    },
                    {
                        "name": "Junming Ma"
                    },
                    {
                        "name": "Jin Tan"
                    },
                    {
                        "name": "Runsheng Wang"
                    },
                    {
                        "name": "Meng Li"
                    }
                ],
                "author_detail": {
                    "name": "Meng Li"
                },
                "author": "Meng Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.06807v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.06807v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.02882v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.02882v2",
                "updated": "2025-01-12T12:01:47Z",
                "updated_parsed": [
                    2025,
                    1,
                    12,
                    12,
                    1,
                    47,
                    6,
                    12,
                    0
                ],
                "published": "2024-04-03T17:33:21Z",
                "published_parsed": [
                    2024,
                    4,
                    3,
                    17,
                    33,
                    21,
                    2,
                    94,
                    0
                ],
                "title": "Linear Attention Sequence Parallelism",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Linear Attention Sequence Parallelism"
                },
                "summary": "Sequence parallelism (SP) serves as a prevalent strategy to handle long\nsequences that exceed the memory limit of a single device. However, for linear\nsequence modeling methods like linear attention, existing SP approaches do not\ntake advantage of their right-product-first feature, resulting in sub-optimal\ncommunication efficiency and usability. In this paper, we introduce Linear\nAttention Sequence Parallelism (LASP), an efficient SP approach designed for\nlinear attention-based transformer models. Specifically, we design an efficient\npoint-to-point ring-style communication mechanism to leverage the right-product\nkernel trick of linear attention, which sharply decreases the communication\noverhead, comparing with existing SP methods. We enhance the computation\nefficiency of LASP by performing kernel fusion and intermediate state caching,\nmaking the implementation of LASP hardware-friendly on GPUs. Furthermore, we\nmeticulously ensure the compatibility of sequence-level LASP with all types of\nbatch-level data parallel methods, which is vital for distributed training on\nlarge clusters with very-long sequences. We also discuss the generalization of\nLASP on other linear sequence modeling methods. Extensive experiments on linear\nattention-based models are conducted with varying sequence lengths from 2K to\n4096K. LASP scales sequence length up to 4096K on 128 GPUs, which is 8$\\times$\nlonger than existing SP methods. The code is available at\nhttps://github.com/OpenNLPLab/LASP.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Sequence parallelism (SP) serves as a prevalent strategy to handle long\nsequences that exceed the memory limit of a single device. However, for linear\nsequence modeling methods like linear attention, existing SP approaches do not\ntake advantage of their right-product-first feature, resulting in sub-optimal\ncommunication efficiency and usability. In this paper, we introduce Linear\nAttention Sequence Parallelism (LASP), an efficient SP approach designed for\nlinear attention-based transformer models. Specifically, we design an efficient\npoint-to-point ring-style communication mechanism to leverage the right-product\nkernel trick of linear attention, which sharply decreases the communication\noverhead, comparing with existing SP methods. We enhance the computation\nefficiency of LASP by performing kernel fusion and intermediate state caching,\nmaking the implementation of LASP hardware-friendly on GPUs. Furthermore, we\nmeticulously ensure the compatibility of sequence-level LASP with all types of\nbatch-level data parallel methods, which is vital for distributed training on\nlarge clusters with very-long sequences. We also discuss the generalization of\nLASP on other linear sequence modeling methods. Extensive experiments on linear\nattention-based models are conducted with varying sequence lengths from 2K to\n4096K. LASP scales sequence length up to 4096K on 128 GPUs, which is 8$\\times$\nlonger than existing SP methods. The code is available at\nhttps://github.com/OpenNLPLab/LASP."
                },
                "authors": [
                    {
                        "name": "Weigao Sun"
                    },
                    {
                        "name": "Zhen Qin"
                    },
                    {
                        "name": "Dong Li"
                    },
                    {
                        "name": "Xuyang Shen"
                    },
                    {
                        "name": "Yu Qiao"
                    },
                    {
                        "name": "Yiran Zhong"
                    }
                ],
                "author_detail": {
                    "name": "Yiran Zhong"
                },
                "author": "Yiran Zhong",
                "arxiv_comment": "Technical report, 20 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.02882v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.02882v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.07196v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.07196v3",
                "updated": "2025-01-12T11:15:41Z",
                "updated_parsed": [
                    2025,
                    1,
                    12,
                    11,
                    15,
                    41,
                    6,
                    12,
                    0
                ],
                "published": "2024-09-11T11:40:23Z",
                "published_parsed": [
                    2024,
                    9,
                    11,
                    11,
                    40,
                    23,
                    2,
                    255,
                    0
                ],
                "title": "Sub-cycle Nanotip Field Emission of Electrons Driven by Air Plasma\n  Generated THz Pulses",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Sub-cycle Nanotip Field Emission of Electrons Driven by Air Plasma\n  Generated THz Pulses"
                },
                "summary": "Terahertz pulses generated by two-color laser plasmas have reported peak\nfield strengths exceeding MV/cm, and when illuminating metal nanotips the\nnear-field enhancement at the tip apex should result in extremely high bunch\ncharges and electron energies via sub-cycle cold field emission. Here, electron\nemission from tungsten nanotips driven by THz pulses generated by a long\nfilament air-plasma are reported. Electron energies up to 1.1 keV and bunch\ncharges up to 2x$10^5$ electrons per pulse were detected, well below values\nexpected for peak field calculated via the time averaged Poynting vector.\nInvestigations revealed a failure in the use of the time-averaged Poynting\nvector when applied to long filament THz pulses, due to spatio-temporal\nrestructuring of the THz pulse in the focus. Accounting for this restructuring\nsignificantly reduces the field strength to approximately 160 ~kV/cm,\nconsistent with the observed electron bunch charges, peak energies and their\ndependence on the tip position in the THz focus. Despite these findings, our\nresults surpass previous THz plasma-driven electron generation by an order of\nmagnitude in both electron energy and bunch charge and a path to increasing\nthese by an additional order of magnitude by modification of the THz optics is\nproposed.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Terahertz pulses generated by two-color laser plasmas have reported peak\nfield strengths exceeding MV/cm, and when illuminating metal nanotips the\nnear-field enhancement at the tip apex should result in extremely high bunch\ncharges and electron energies via sub-cycle cold field emission. Here, electron\nemission from tungsten nanotips driven by THz pulses generated by a long\nfilament air-plasma are reported. Electron energies up to 1.1 keV and bunch\ncharges up to 2x$10^5$ electrons per pulse were detected, well below values\nexpected for peak field calculated via the time averaged Poynting vector.\nInvestigations revealed a failure in the use of the time-averaged Poynting\nvector when applied to long filament THz pulses, due to spatio-temporal\nrestructuring of the THz pulse in the focus. Accounting for this restructuring\nsignificantly reduces the field strength to approximately 160 ~kV/cm,\nconsistent with the observed electron bunch charges, peak energies and their\ndependence on the tip position in the THz focus. Despite these findings, our\nresults surpass previous THz plasma-driven electron generation by an order of\nmagnitude in both electron energy and bunch charge and a path to increasing\nthese by an additional order of magnitude by modification of the THz optics is\nproposed."
                },
                "authors": [
                    {
                        "name": "Benjamin Colmey"
                    },
                    {
                        "name": "Rodrigo T. Paulino"
                    },
                    {
                        "name": "Gaspard Beaufort"
                    },
                    {
                        "name": "David G. Cooke"
                    }
                ],
                "author_detail": {
                    "name": "David G. Cooke"
                },
                "author": "David G. Cooke",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.07196v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.07196v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cond-mat.mtrl-sci",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cond-mat.mtrl-sci",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "physics.plasm-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.00857v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.00857v2",
                "updated": "2025-01-12T05:25:06Z",
                "updated_parsed": [
                    2025,
                    1,
                    12,
                    5,
                    25,
                    6,
                    6,
                    12,
                    0
                ],
                "published": "2024-12-01T15:45:26Z",
                "published_parsed": [
                    2024,
                    12,
                    1,
                    15,
                    45,
                    26,
                    6,
                    336,
                    0
                ],
                "title": "Advanced Video Inpainting Using Optical Flow-Guided Efficient Diffusion",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Advanced Video Inpainting Using Optical Flow-Guided Efficient Diffusion"
                },
                "summary": "Recently, diffusion-based methods have achieved great improvements in the\nvideo inpainting task. However, these methods still face many challenges, such\nas maintaining temporal consistency and the time-consuming issue. This paper\nproposes an advanced video inpainting framework using optical Flow-guided\nEfficient Diffusion, called FloED. Specifically, FloED employs a dual-branch\narchitecture, where a flow branch first restores corrupted flow and a\nmulti-scale flow adapter provides motion guidance to the main inpainting\nbranch. Additionally, a training-free latent interpolation method is proposed\nto accelerate the multi-step denoising process using flow warping. Further\nintroducing a flow attention cache mechanism, FLoED efficiently reduces the\ncomputational cost brought by incorporating optical flow. Comprehensive\nexperiments in both background restoration and object removal tasks demonstrate\nthat FloED outperforms state-of-the-art methods from the perspective of both\nperformance and efficiency.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recently, diffusion-based methods have achieved great improvements in the\nvideo inpainting task. However, these methods still face many challenges, such\nas maintaining temporal consistency and the time-consuming issue. This paper\nproposes an advanced video inpainting framework using optical Flow-guided\nEfficient Diffusion, called FloED. Specifically, FloED employs a dual-branch\narchitecture, where a flow branch first restores corrupted flow and a\nmulti-scale flow adapter provides motion guidance to the main inpainting\nbranch. Additionally, a training-free latent interpolation method is proposed\nto accelerate the multi-step denoising process using flow warping. Further\nintroducing a flow attention cache mechanism, FLoED efficiently reduces the\ncomputational cost brought by incorporating optical flow. Comprehensive\nexperiments in both background restoration and object removal tasks demonstrate\nthat FloED outperforms state-of-the-art methods from the perspective of both\nperformance and efficiency."
                },
                "authors": [
                    {
                        "name": "Bohai Gu"
                    },
                    {
                        "name": "Hao Luo"
                    },
                    {
                        "name": "Song Guo"
                    },
                    {
                        "name": "Peiran Dong"
                    }
                ],
                "author_detail": {
                    "name": "Peiran Dong"
                },
                "author": "Peiran Dong",
                "arxiv_comment": "Project page: https://nevsnev.github.io/FloED/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.00857v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.00857v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.06709v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.06709v1",
                "updated": "2025-01-12T04:29:39Z",
                "updated_parsed": [
                    2025,
                    1,
                    12,
                    4,
                    29,
                    39,
                    6,
                    12,
                    0
                ],
                "published": "2025-01-12T04:29:39Z",
                "published_parsed": [
                    2025,
                    1,
                    12,
                    4,
                    29,
                    39,
                    6,
                    12,
                    0
                ],
                "title": "Mell: Memory-Efficient Large Language Model Serving via Multi-GPU KV\n  Cache Management",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mell: Memory-Efficient Large Language Model Serving via Multi-GPU KV\n  Cache Management"
                },
                "summary": "Serving large language models (LLMs) for massive users is challenged by the\nsignificant memory footprint of the transient state, known as the key-value\n(KV) cache, which scales with sequence length and number of requests. Instead\nof renting or buying more expensive GPUs, the load imbalance of the KV cache\nacross GPUs, coupled with recent advances in inter-GPU communication, provides\nan opportunity to serve more requests via request migration. However, high\nmigration overhead and unpredictable request patterns make it challenging.\nTherefore, this paper proposes MELL, a memory-efficient LLM serving system via\nmulti-GPU KV cache management. It saves the number of GPUs needed in the system\nby considering the dynamic KV cache load and the costly request migration.\nSpecifically, we first develop an adaptive request migration mechanism to\nbalance the computational and communication overheads and adapt to diverse\nresource conditions. Then, we design an online algorithm tailored to a\nmulti-LLM request and multi-GPU scheduling problem with migration enabled. It\naims to minimise the required GPUs while limiting the number of migrations.\nFinally, we implement a prototype of MELL and demonstrate that it reduces the\nnumber of GPUs by 31% and increases the GPU utilization by 43% at most compared\nto existing LLM serving systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Serving large language models (LLMs) for massive users is challenged by the\nsignificant memory footprint of the transient state, known as the key-value\n(KV) cache, which scales with sequence length and number of requests. Instead\nof renting or buying more expensive GPUs, the load imbalance of the KV cache\nacross GPUs, coupled with recent advances in inter-GPU communication, provides\nan opportunity to serve more requests via request migration. However, high\nmigration overhead and unpredictable request patterns make it challenging.\nTherefore, this paper proposes MELL, a memory-efficient LLM serving system via\nmulti-GPU KV cache management. It saves the number of GPUs needed in the system\nby considering the dynamic KV cache load and the costly request migration.\nSpecifically, we first develop an adaptive request migration mechanism to\nbalance the computational and communication overheads and adapt to diverse\nresource conditions. Then, we design an online algorithm tailored to a\nmulti-LLM request and multi-GPU scheduling problem with migration enabled. It\naims to minimise the required GPUs while limiting the number of migrations.\nFinally, we implement a prototype of MELL and demonstrate that it reduces the\nnumber of GPUs by 31% and increases the GPU utilization by 43% at most compared\nto existing LLM serving systems."
                },
                "authors": [
                    {
                        "name": "Liu Qianli"
                    },
                    {
                        "name": "Hong Zicong"
                    },
                    {
                        "name": "Chen Fahao"
                    },
                    {
                        "name": "Li Peng"
                    },
                    {
                        "name": "Guo Song"
                    }
                ],
                "author_detail": {
                    "name": "Guo Song"
                },
                "author": "Guo Song",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.06709v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.06709v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.17918v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.17918v4",
                "updated": "2025-01-11T15:26:48Z",
                "updated_parsed": [
                    2025,
                    1,
                    11,
                    15,
                    26,
                    48,
                    5,
                    11,
                    0
                ],
                "published": "2024-06-25T20:00:32Z",
                "published_parsed": [
                    2024,
                    6,
                    25,
                    20,
                    0,
                    32,
                    1,
                    177,
                    0
                ],
                "title": "GraphSnapShot: Caching Local Structure for Fast Graph Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GraphSnapShot: Caching Local Structure for Fast Graph Learning"
                },
                "summary": "In our recent research, we have developed a framework called GraphSnapShot,\nwhich has been proven an useful tool for graph learning acceleration.\nGraphSnapShot is a framework for fast cache, storage, retrieval and computation\nfor graph learning. It can quickly store and update the local topology of graph\nstructure and allows us to track patterns in the structure of graph networks,\njust like take snapshots of the graphs. In experiments, GraphSnapShot shows\nefficiency, it can achieve up to 30% training acceleration and 73% memory\nreduction for lossless graph ML training compared to current baselines such as\ndgl.This technique is particular useful for large dynamic graph learning tasks\nsuch as social media analysis and recommendation systems to process complex\nrelationships between entities.\n  The code for GraphSnapShot is publicly available at\nhttps://github.com/NoakLiu/GraphSnapShot.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In our recent research, we have developed a framework called GraphSnapShot,\nwhich has been proven an useful tool for graph learning acceleration.\nGraphSnapShot is a framework for fast cache, storage, retrieval and computation\nfor graph learning. It can quickly store and update the local topology of graph\nstructure and allows us to track patterns in the structure of graph networks,\njust like take snapshots of the graphs. In experiments, GraphSnapShot shows\nefficiency, it can achieve up to 30% training acceleration and 73% memory\nreduction for lossless graph ML training compared to current baselines such as\ndgl.This technique is particular useful for large dynamic graph learning tasks\nsuch as social media analysis and recommendation systems to process complex\nrelationships between entities.\n  The code for GraphSnapShot is publicly available at\nhttps://github.com/NoakLiu/GraphSnapShot."
                },
                "authors": [
                    {
                        "name": "Dong Liu"
                    },
                    {
                        "name": "Roger Waleffe"
                    },
                    {
                        "name": "Meng Jiang"
                    },
                    {
                        "name": "Shivaram Venkataraman"
                    }
                ],
                "author_detail": {
                    "name": "Shivaram Venkataraman"
                },
                "author": "Shivaram Venkataraman",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.17918v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.17918v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.06428v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.06428v1",
                "updated": "2025-01-11T03:47:04Z",
                "updated_parsed": [
                    2025,
                    1,
                    11,
                    3,
                    47,
                    4,
                    5,
                    11,
                    0
                ],
                "published": "2025-01-11T03:47:04Z",
                "published_parsed": [
                    2025,
                    1,
                    11,
                    3,
                    47,
                    4,
                    5,
                    11,
                    0
                ],
                "title": "Optimizing digital experiences with content delivery networks:\n  Architectures, performance strategies, and future trends",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Optimizing digital experiences with content delivery networks:\n  Architectures, performance strategies, and future trends"
                },
                "summary": "This research investigates how CDNs (Content Delivery Networks) can improve\nthe digital experience, as consumers increasingly expect fast, efficient, and\neffortless access to online resources. CDNs play a crucial role in reducing\nlatency, enhancing scalability, and optimizing delivery mechanisms, which is\nevident across various platforms and regions. The study focuses on key CDN\nconcerns, such as foundational and modern CDN architectures, edge computing,\nhybrid CDNs, and multi-CDN strategies. It also explores performance-enhancing\ntopics, including caching, load balancing, and the novel features of HTTP/3 and\nQUIC.\n  Current trends, such as integrating CDNs with 5G networks, serverless\narchitectures, and AI-driven traffic management, are examined to demonstrate\nhow CDN technology is likely to evolve. The study also addresses challenges\nrelated to security, cost, and global regulations. Practical examples from the\ne-commerce, streaming, and gaming industries highlight how enhanced CDNs are\ntransforming these sectors.\n  The conclusions emphasize the need to evolve CDN strategies to meet growing\nuser expectations and adapt to the rapidly changing digital landscape.\nAdditionally, the research identifies future research opportunities,\nparticularly in exploring the impact of QC, the enhancement of AI services, and\nthe sustainability of CDN solutions. Overall, the study situates architectural\ndesign, performance strategies, and emerging trends to address gaps and create\na more efficient and secure approach for improving digital experiences.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This research investigates how CDNs (Content Delivery Networks) can improve\nthe digital experience, as consumers increasingly expect fast, efficient, and\neffortless access to online resources. CDNs play a crucial role in reducing\nlatency, enhancing scalability, and optimizing delivery mechanisms, which is\nevident across various platforms and regions. The study focuses on key CDN\nconcerns, such as foundational and modern CDN architectures, edge computing,\nhybrid CDNs, and multi-CDN strategies. It also explores performance-enhancing\ntopics, including caching, load balancing, and the novel features of HTTP/3 and\nQUIC.\n  Current trends, such as integrating CDNs with 5G networks, serverless\narchitectures, and AI-driven traffic management, are examined to demonstrate\nhow CDN technology is likely to evolve. The study also addresses challenges\nrelated to security, cost, and global regulations. Practical examples from the\ne-commerce, streaming, and gaming industries highlight how enhanced CDNs are\ntransforming these sectors.\n  The conclusions emphasize the need to evolve CDN strategies to meet growing\nuser expectations and adapt to the rapidly changing digital landscape.\nAdditionally, the research identifies future research opportunities,\nparticularly in exploring the impact of QC, the enhancement of AI services, and\nthe sustainability of CDN solutions. Overall, the study situates architectural\ndesign, performance strategies, and emerging trends to address gaps and create\na more efficient and secure approach for improving digital experiences."
                },
                "authors": [
                    {
                        "name": "Anuj Tyagi"
                    }
                ],
                "author_detail": {
                    "name": "Anuj Tyagi"
                },
                "author": "Anuj Tyagi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.06428v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.06428v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.06425v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.06425v1",
                "updated": "2025-01-11T03:37:10Z",
                "updated_parsed": [
                    2025,
                    1,
                    11,
                    3,
                    37,
                    10,
                    5,
                    11,
                    0
                ],
                "published": "2025-01-11T03:37:10Z",
                "published_parsed": [
                    2025,
                    1,
                    11,
                    3,
                    37,
                    10,
                    5,
                    11,
                    0
                ],
                "title": "Tensor Product Attention Is All You Need",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Tensor Product Attention Is All You Need"
                },
                "summary": "Scaling language models to handle longer input sequences typically\nnecessitates large key-value (KV) caches, resulting in substantial memory\noverhead during inference. In this paper, we propose Tensor Product Attention\n(TPA), a novel attention mechanism that uses tensor decompositions to represent\nqueries, keys, and values compactly, significantly shrinking KV cache size at\ninference time. By factorizing these representations into contextual low-rank\ncomponents (contextual factorization) and seamlessly integrating with RoPE, TPA\nachieves improved model quality alongside memory efficiency. Based on TPA, we\nintroduce the Tensor ProducT ATTenTion Transformer (T6), a new model\narchitecture for sequence modeling. Through extensive empirical evaluation of\nlanguage modeling tasks, we demonstrate that T6 exceeds the performance of\nstandard Transformer baselines including MHA, MQA, GQA, and MLA across various\nmetrics, including perplexity and a range of renowned evaluation benchmarks.\nNotably, TPAs memory efficiency enables the processing of significantly longer\nsequences under fixed resource constraints, addressing a critical scalability\nchallenge in modern language models. The code is available at\nhttps://github.com/tensorgi/T6.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Scaling language models to handle longer input sequences typically\nnecessitates large key-value (KV) caches, resulting in substantial memory\noverhead during inference. In this paper, we propose Tensor Product Attention\n(TPA), a novel attention mechanism that uses tensor decompositions to represent\nqueries, keys, and values compactly, significantly shrinking KV cache size at\ninference time. By factorizing these representations into contextual low-rank\ncomponents (contextual factorization) and seamlessly integrating with RoPE, TPA\nachieves improved model quality alongside memory efficiency. Based on TPA, we\nintroduce the Tensor ProducT ATTenTion Transformer (T6), a new model\narchitecture for sequence modeling. Through extensive empirical evaluation of\nlanguage modeling tasks, we demonstrate that T6 exceeds the performance of\nstandard Transformer baselines including MHA, MQA, GQA, and MLA across various\nmetrics, including perplexity and a range of renowned evaluation benchmarks.\nNotably, TPAs memory efficiency enables the processing of significantly longer\nsequences under fixed resource constraints, addressing a critical scalability\nchallenge in modern language models. The code is available at\nhttps://github.com/tensorgi/T6."
                },
                "authors": [
                    {
                        "name": "Yifan Zhang"
                    },
                    {
                        "name": "Yifeng Liu"
                    },
                    {
                        "name": "Huizhuo Yuan"
                    },
                    {
                        "name": "Zhen Qin"
                    },
                    {
                        "name": "Yang Yuan"
                    },
                    {
                        "name": "Quanquan Gu"
                    },
                    {
                        "name": "Andrew Chi-Chih Yao"
                    }
                ],
                "author_detail": {
                    "name": "Andrew Chi-Chih Yao"
                },
                "author": "Andrew Chi-Chih Yao",
                "arxiv_comment": "23 pages, 5 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.06425v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.06425v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.06394v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.06394v1",
                "updated": "2025-01-11T00:47:29Z",
                "updated_parsed": [
                    2025,
                    1,
                    11,
                    0,
                    47,
                    29,
                    5,
                    11,
                    0
                ],
                "published": "2025-01-11T00:47:29Z",
                "published_parsed": [
                    2025,
                    1,
                    11,
                    0,
                    47,
                    29,
                    5,
                    11,
                    0
                ],
                "title": "Unispeaker: A Unified Approach for Multimodality-driven Speaker\n  Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Unispeaker: A Unified Approach for Multimodality-driven Speaker\n  Generation"
                },
                "summary": "Recent advancements in personalized speech generation have brought synthetic\nspeech increasingly close to the realism of target speakers' recordings, yet\nmultimodal speaker generation remains on the rise. This paper introduces\nUniSpeaker, a unified approach for multimodality-driven speaker generation.\nSpecifically, we propose a unified voice aggregator based on KV-Former,\napplying soft contrastive loss to map diverse voice description modalities into\na shared voice space, ensuring that the generated voice aligns more closely\nwith the input descriptions. To evaluate multimodality-driven voice control, we\nbuild the first multimodality-based voice control (MVC) benchmark, focusing on\nvoice suitability, voice diversity, and speech quality. UniSpeaker is evaluated\nacross five tasks using the MVC benchmark, and the experimental results\ndemonstrate that UniSpeaker outperforms previous modality-specific models.\nSpeech samples are available at \\url{https://UniSpeaker.github.io}.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advancements in personalized speech generation have brought synthetic\nspeech increasingly close to the realism of target speakers' recordings, yet\nmultimodal speaker generation remains on the rise. This paper introduces\nUniSpeaker, a unified approach for multimodality-driven speaker generation.\nSpecifically, we propose a unified voice aggregator based on KV-Former,\napplying soft contrastive loss to map diverse voice description modalities into\na shared voice space, ensuring that the generated voice aligns more closely\nwith the input descriptions. To evaluate multimodality-driven voice control, we\nbuild the first multimodality-based voice control (MVC) benchmark, focusing on\nvoice suitability, voice diversity, and speech quality. UniSpeaker is evaluated\nacross five tasks using the MVC benchmark, and the experimental results\ndemonstrate that UniSpeaker outperforms previous modality-specific models.\nSpeech samples are available at \\url{https://UniSpeaker.github.io}."
                },
                "authors": [
                    {
                        "name": "Zhengyan Sheng"
                    },
                    {
                        "name": "Zhihao Du"
                    },
                    {
                        "name": "Heng Lu"
                    },
                    {
                        "name": "Shiliang Zhang"
                    },
                    {
                        "name": "Zhen-Hua Ling"
                    }
                ],
                "author_detail": {
                    "name": "Zhen-Hua Ling"
                },
                "author": "Zhen-Hua Ling",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.06394v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.06394v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SD",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.01030v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.01030v2",
                "updated": "2025-01-10T10:11:45Z",
                "updated_parsed": [
                    2025,
                    1,
                    10,
                    10,
                    11,
                    45,
                    4,
                    10,
                    0
                ],
                "published": "2024-07-01T07:25:08Z",
                "published_parsed": [
                    2024,
                    7,
                    1,
                    7,
                    25,
                    8,
                    0,
                    183,
                    0
                ],
                "title": "Tame fields, Graded Rings and Finite Complete Sequences of Key\n  Polynomials",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Tame fields, Graded Rings and Finite Complete Sequences of Key\n  Polynomials"
                },
                "summary": "In this paper, we present a criterion for $(K,v)$ to be henselian and\ndefectless in terms of finite complete sequences of key polynomials. For this,\nwe use the theory of Mac Lane-Vaqui\\'e chains and abstract key polynomials. We\nthen prove that a valued field $(K,v)$ is tame if and only if $vK$ is\n$p$-divisible, $Kv$ is perfect and every simple algebraic extension of $K$\nadmits a finite complete sequence of key polynomials. The properties $vK$\n$p$-divisible and $Kv$ perfect are described by the Frobenius endomorphism on\nthe associated graded ring. We also make considerations on simply defectless\nand algebraically maximal valued fields and purely inertial and purely ramified\nextensions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this paper, we present a criterion for $(K,v)$ to be henselian and\ndefectless in terms of finite complete sequences of key polynomials. For this,\nwe use the theory of Mac Lane-Vaqui\\'e chains and abstract key polynomials. We\nthen prove that a valued field $(K,v)$ is tame if and only if $vK$ is\n$p$-divisible, $Kv$ is perfect and every simple algebraic extension of $K$\nadmits a finite complete sequence of key polynomials. The properties $vK$\n$p$-divisible and $Kv$ perfect are described by the Frobenius endomorphism on\nthe associated graded ring. We also make considerations on simply defectless\nand algebraically maximal valued fields and purely inertial and purely ramified\nextensions."
                },
                "authors": [
                    {
                        "name": "Caio Henrique Silva de Souza"
                    }
                ],
                "author_detail": {
                    "name": "Caio Henrique Silva de Souza"
                },
                "author": "Caio Henrique Silva de Souza",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.01030v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.01030v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "math.AC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "math.AC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "13A18",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.20433v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.20433v2",
                "updated": "2025-01-09T15:14:05Z",
                "updated_parsed": [
                    2025,
                    1,
                    9,
                    15,
                    14,
                    5,
                    3,
                    9,
                    0
                ],
                "published": "2024-09-30T15:53:36Z",
                "published_parsed": [
                    2024,
                    9,
                    30,
                    15,
                    53,
                    36,
                    0,
                    274,
                    0
                ],
                "title": "Handover_Management_in_UAV_Networks_with_Blockages",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Handover_Management_in_UAV_Networks_with_Blockages"
                },
                "summary": "We investigate the performance of unmanned aerial vehicle (UAV)-based\nnetworks in urban environments characterized by blockages, focusing on their\ncapability to support the service demands of mobile users. The UAV-base\nstations (UAV-BSs) are modeled using a two-dimensional (2-D) marked- Poisson\npoint process (MPPP), where the marks represent the altitude of each UAV-BS.\nLeveraging stochastic geometry, we analyze the impact of blockages on network\nreliability by studying the meta distribution (MD) of the\nsignal-to-interference noise ratio (SINR) for a specific reliability threshold\nand the association probabilities for both line-of-sight (LoS) and non\nline-of-sight (NLoS) UAV-BSs. Furthermore, to enhance the performance of mobile\nusers, we propose a novel cache-based handover management strategy that\ndynamically selects the cell search time and delays the received signal\nstrength (RSS)-based base station (BS) associations. This strategy aims to\nminimize unnecessary handovers (HOs) experienced by users by leveraging caching\ncapabilities at user equipment (UE), thus reducing latency, ensuring seamless\nconnectivity, and maintaining the quality of service (QoS). This study provides\nvaluable insights into optimizing UAV network deployments to support the\nstringent requirements in the network, ensuring reliable, low-latency, and\nhigh-throughput communication for next-generation smart cities.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We investigate the performance of unmanned aerial vehicle (UAV)-based\nnetworks in urban environments characterized by blockages, focusing on their\ncapability to support the service demands of mobile users. The UAV-base\nstations (UAV-BSs) are modeled using a two-dimensional (2-D) marked- Poisson\npoint process (MPPP), where the marks represent the altitude of each UAV-BS.\nLeveraging stochastic geometry, we analyze the impact of blockages on network\nreliability by studying the meta distribution (MD) of the\nsignal-to-interference noise ratio (SINR) for a specific reliability threshold\nand the association probabilities for both line-of-sight (LoS) and non\nline-of-sight (NLoS) UAV-BSs. Furthermore, to enhance the performance of mobile\nusers, we propose a novel cache-based handover management strategy that\ndynamically selects the cell search time and delays the received signal\nstrength (RSS)-based base station (BS) associations. This strategy aims to\nminimize unnecessary handovers (HOs) experienced by users by leveraging caching\ncapabilities at user equipment (UE), thus reducing latency, ensuring seamless\nconnectivity, and maintaining the quality of service (QoS). This study provides\nvaluable insights into optimizing UAV network deployments to support the\nstringent requirements in the network, ensuring reliable, low-latency, and\nhigh-throughput communication for next-generation smart cities."
                },
                "authors": [
                    {
                        "name": "Neetu R R"
                    },
                    {
                        "name": "Gourab Ghatak"
                    },
                    {
                        "name": "Vivek Ashok Bohara"
                    }
                ],
                "author_detail": {
                    "name": "Vivek Ashok Bohara"
                },
                "author": "Vivek Ashok Bohara",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.20433v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.20433v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SP",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.04993v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.04993v1",
                "updated": "2025-01-09T06:18:39Z",
                "updated_parsed": [
                    2025,
                    1,
                    9,
                    6,
                    18,
                    39,
                    3,
                    9,
                    0
                ],
                "published": "2025-01-09T06:18:39Z",
                "published_parsed": [
                    2025,
                    1,
                    9,
                    6,
                    18,
                    39,
                    3,
                    9,
                    0
                ],
                "title": "ByteFS: System Support for (CXL-based) Memory-Semantic Solid-State\n  Drives",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ByteFS: System Support for (CXL-based) Memory-Semantic Solid-State\n  Drives"
                },
                "summary": "Unlike non-volatile memory that resides on the processor memory bus,\nmemory-semantic solid-state drives (SSDs) support both byte and block access\ngranularity via PCIe or CXL interconnects. They provide scalable memory\ncapacity using NAND flash at a much lower cost. In addition, they have\ndifferent performance characteristics for their dual byte/block interface\nrespectively, while offering essential memory semantics for upper-level\nsoftware. Such a byte-accessible storage device provides new implications on\nthe software system design.\n  In this paper, we develop a new file system, named ByteFS, by rethinking the\ndesign primitives of file systems and SSD firmware to exploit the advantages of\nboth byte and block-granular data accesses. ByteFS supports byte-granular data\npersistence to retain the persistence nature of SSDs. It extends the core data\nstructure of file systems by enabling dual byte/block-granular data accesses.\nTo facilitate the support for byte-granular writes, \\pname{} manages the\ninternal DRAM of SSD firmware in a log-structured manner and enables data\ncoalescing to reduce the unnecessary I/O traffic to flash chips. ByteFS also\nenables coordinated data caching between the host page cache and SSD cache for\nbest utilizing the precious memory resource. We implement ByteFS on both a real\nprogrammable SSD and an emulated memory-semantic SSD for sensitivity study.\nCompared to state-of-the-art file systems for non-volatile memory and\nconventional SSDs, ByteFS outperforms them by up to 2.7$\\times$, while\npreserving the essential properties of a file system. ByteFS also reduces the\nwrite traffic to SSDs by up to 5.1$\\times$ by alleviating unnecessary writes\ncaused by both metadata and data updates in file systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Unlike non-volatile memory that resides on the processor memory bus,\nmemory-semantic solid-state drives (SSDs) support both byte and block access\ngranularity via PCIe or CXL interconnects. They provide scalable memory\ncapacity using NAND flash at a much lower cost. In addition, they have\ndifferent performance characteristics for their dual byte/block interface\nrespectively, while offering essential memory semantics for upper-level\nsoftware. Such a byte-accessible storage device provides new implications on\nthe software system design.\n  In this paper, we develop a new file system, named ByteFS, by rethinking the\ndesign primitives of file systems and SSD firmware to exploit the advantages of\nboth byte and block-granular data accesses. ByteFS supports byte-granular data\npersistence to retain the persistence nature of SSDs. It extends the core data\nstructure of file systems by enabling dual byte/block-granular data accesses.\nTo facilitate the support for byte-granular writes, \\pname{} manages the\ninternal DRAM of SSD firmware in a log-structured manner and enables data\ncoalescing to reduce the unnecessary I/O traffic to flash chips. ByteFS also\nenables coordinated data caching between the host page cache and SSD cache for\nbest utilizing the precious memory resource. We implement ByteFS on both a real\nprogrammable SSD and an emulated memory-semantic SSD for sensitivity study.\nCompared to state-of-the-art file systems for non-volatile memory and\nconventional SSDs, ByteFS outperforms them by up to 2.7$\\times$, while\npreserving the essential properties of a file system. ByteFS also reduces the\nwrite traffic to SSDs by up to 5.1$\\times$ by alleviating unnecessary writes\ncaused by both metadata and data updates in file systems."
                },
                "authors": [
                    {
                        "name": "Shaobo Li"
                    },
                    {
                        "name": "Yirui Eric Zhou"
                    },
                    {
                        "name": "Hao Ren"
                    },
                    {
                        "name": "Jian Huang"
                    }
                ],
                "author_detail": {
                    "name": "Jian Huang"
                },
                "author": "Jian Huang",
                "arxiv_comment": "This paper is accepted at the 30th Conference on Architectural\n  Support for Programming Languages and Operating Systems (ASPLOS 2025)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.04993v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.04993v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.OS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.OS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.04216v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.04216v2",
                "updated": "2025-01-09T03:02:31Z",
                "updated_parsed": [
                    2025,
                    1,
                    9,
                    3,
                    2,
                    31,
                    3,
                    9,
                    0
                ],
                "published": "2025-01-08T01:23:29Z",
                "published_parsed": [
                    2025,
                    1,
                    8,
                    1,
                    23,
                    29,
                    2,
                    8,
                    0
                ],
                "title": "Optimal Oblivious Algorithms for Multi-way Joins",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Optimal Oblivious Algorithms for Multi-way Joins"
                },
                "summary": "In cloud databases, cloud computation over sensitive data uploaded by clients\ninevitably causes concern about data security and privacy. Even when encryption\nprimitives and trusted computing environments are integrated into query\nprocessing to safeguard the actual contents of the data, access patterns of\nalgorithms can still leak private information about the data. Oblivious Random\nAccess Memory (ORAM) and circuits are two generic approaches to address this\nissue, ensuring that access patterns of algorithms remain oblivious to the\ndata. However, deploying these methods on insecure algorithms, particularly for\nmulti-way join processing, is computationally expensive and inherently\nchallenging.\n  In this paper, we propose a novel sorting-based algorithm for multi-way join\nprocessing that operates without relying on ORAM simulations or other security\nassumptions. Our algorithm is a non-trivial, provably oblivious composition of\nbasic primitives, with time complexity matching the insecure worst-case optimal\njoin algorithm, up to a logarithmic factor. Furthermore, it is cache-agnostic,\nwith cache complexity matching the insecure lower bound, also up to a\nlogarithmic factor. This clean and straightforward approach has the potential\nto be extended to other security settings and implemented in practical database\nsystems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In cloud databases, cloud computation over sensitive data uploaded by clients\ninevitably causes concern about data security and privacy. Even when encryption\nprimitives and trusted computing environments are integrated into query\nprocessing to safeguard the actual contents of the data, access patterns of\nalgorithms can still leak private information about the data. Oblivious Random\nAccess Memory (ORAM) and circuits are two generic approaches to address this\nissue, ensuring that access patterns of algorithms remain oblivious to the\ndata. However, deploying these methods on insecure algorithms, particularly for\nmulti-way join processing, is computationally expensive and inherently\nchallenging.\n  In this paper, we propose a novel sorting-based algorithm for multi-way join\nprocessing that operates without relying on ORAM simulations or other security\nassumptions. Our algorithm is a non-trivial, provably oblivious composition of\nbasic primitives, with time complexity matching the insecure worst-case optimal\njoin algorithm, up to a logarithmic factor. Furthermore, it is cache-agnostic,\nwith cache complexity matching the insecure lower bound, also up to a\nlogarithmic factor. This clean and straightforward approach has the potential\nto be extended to other security settings and implemented in practical database\nsystems."
                },
                "authors": [
                    {
                        "name": "Xiao Hu"
                    },
                    {
                        "name": "Zhiang Wu"
                    }
                ],
                "author_detail": {
                    "name": "Zhiang Wu"
                },
                "author": "Zhiang Wu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.04216v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.04216v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.04394v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.04394v1",
                "updated": "2025-01-08T10:14:19Z",
                "updated_parsed": [
                    2025,
                    1,
                    8,
                    10,
                    14,
                    19,
                    2,
                    8,
                    0
                ],
                "published": "2025-01-08T10:14:19Z",
                "published_parsed": [
                    2025,
                    1,
                    8,
                    10,
                    14,
                    19,
                    2,
                    8,
                    0
                ],
                "title": "Modern Hardware Security: A Review of Attacks and Countermeasures",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Modern Hardware Security: A Review of Attacks and Countermeasures"
                },
                "summary": "With the exponential rise in the use of cloud services, smart devices, and\nIoT devices, advanced cyber attacks have become increasingly sophisticated and\nubiquitous. Furthermore, the rapid evolution of computing architectures and\nmemory technologies has created an urgent need to understand and address\nhardware security vulnerabilities. In this paper, we review the current state\nof vulnerabilities and mitigation strategies in contemporary computing systems.\nWe discuss cache side-channel attacks (including Spectre and Meltdown), power\nside-channel attacks (such as Simple Power Analysis, Differential Power\nAnalysis, Correlation Power Analysis, and Template Attacks), and advanced\ntechniques like Voltage Glitching and Electromagnetic Analysis to help\nunderstand and build robust cybersecurity defense systems and guide further\nresearch. We also examine memory encryption, focusing on confidentiality,\ngranularity, key management, masking, and re-keying strategies. Additionally,\nwe cover Cryptographic Instruction Set Architectures, Secure Boot, Root of\nTrust mechanisms, Physical Unclonable Functions, and hardware fault injection\ntechniques. The paper concludes with an analysis of the RISC-V architecture's\nunique security challenges. The comprehensive analysis presented in this paper\nis essential for building resilient hardware security solutions that can\nprotect against both current and emerging threats in an increasingly\nchallenging security landscape.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the exponential rise in the use of cloud services, smart devices, and\nIoT devices, advanced cyber attacks have become increasingly sophisticated and\nubiquitous. Furthermore, the rapid evolution of computing architectures and\nmemory technologies has created an urgent need to understand and address\nhardware security vulnerabilities. In this paper, we review the current state\nof vulnerabilities and mitigation strategies in contemporary computing systems.\nWe discuss cache side-channel attacks (including Spectre and Meltdown), power\nside-channel attacks (such as Simple Power Analysis, Differential Power\nAnalysis, Correlation Power Analysis, and Template Attacks), and advanced\ntechniques like Voltage Glitching and Electromagnetic Analysis to help\nunderstand and build robust cybersecurity defense systems and guide further\nresearch. We also examine memory encryption, focusing on confidentiality,\ngranularity, key management, masking, and re-keying strategies. Additionally,\nwe cover Cryptographic Instruction Set Architectures, Secure Boot, Root of\nTrust mechanisms, Physical Unclonable Functions, and hardware fault injection\ntechniques. The paper concludes with an analysis of the RISC-V architecture's\nunique security challenges. The comprehensive analysis presented in this paper\nis essential for building resilient hardware security solutions that can\nprotect against both current and emerging threats in an increasingly\nchallenging security landscape."
                },
                "authors": [
                    {
                        "name": "Jyotiprakash Mishra"
                    },
                    {
                        "name": "Sanjay K. Sahay"
                    }
                ],
                "author_detail": {
                    "name": "Sanjay K. Sahay"
                },
                "author": "Sanjay K. Sahay",
                "arxiv_comment": "25 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.04394v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.04394v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.00799v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.00799v2",
                "updated": "2025-01-07T17:32:19Z",
                "updated_parsed": [
                    2025,
                    1,
                    7,
                    17,
                    32,
                    19,
                    1,
                    7,
                    0
                ],
                "published": "2025-01-01T10:50:35Z",
                "published_parsed": [
                    2025,
                    1,
                    1,
                    10,
                    50,
                    35,
                    2,
                    1,
                    0
                ],
                "title": "Follow The Approximate Sparse Leader for No-Regret Online Sparse Linear\n  Approximation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Follow The Approximate Sparse Leader for No-Regret Online Sparse Linear\n  Approximation"
                },
                "summary": "We consider the problem of \\textit{online sparse linear approximation}, where\none predicts the best sparse approximation of a sequence of measurements in\nterms of linear combination of columns of a given measurement matrix. Such\nonline prediction problems are ubiquitous, ranging from medical trials to web\ncaching to resource allocation. The inherent difficulty of offline recovery\nalso makes the online problem challenging. In this letter, we propose\nFollow-The-Approximate-Sparse-Leader, an efficient online meta-policy to\naddress this online problem. Through a detailed theoretical analysis, we prove\nthat under certain assumptions on the measurement sequence, the proposed policy\nenjoys a data-dependent sublinear upper bound on the static regret, which can\nrange from logarithmic to square-root. Numerical simulations are performed to\ncorroborate the theoretical findings and demonstrate the efficacy of the\nproposed online policy.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We consider the problem of \\textit{online sparse linear approximation}, where\none predicts the best sparse approximation of a sequence of measurements in\nterms of linear combination of columns of a given measurement matrix. Such\nonline prediction problems are ubiquitous, ranging from medical trials to web\ncaching to resource allocation. The inherent difficulty of offline recovery\nalso makes the online problem challenging. In this letter, we propose\nFollow-The-Approximate-Sparse-Leader, an efficient online meta-policy to\naddress this online problem. Through a detailed theoretical analysis, we prove\nthat under certain assumptions on the measurement sequence, the proposed policy\nenjoys a data-dependent sublinear upper bound on the static regret, which can\nrange from logarithmic to square-root. Numerical simulations are performed to\ncorroborate the theoretical findings and demonstrate the efficacy of the\nproposed online policy."
                },
                "authors": [
                    {
                        "name": "Samrat Mukhopadhyay"
                    },
                    {
                        "name": "Debasmita Mukherjee"
                    }
                ],
                "author_detail": {
                    "name": "Debasmita Mukherjee"
                },
                "author": "Debasmita Mukherjee",
                "arxiv_comment": "12 pages, 5 figures, corrected title, added proof of a lemma in\n  appendix",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.00799v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.00799v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.OC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.03940v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.03940v1",
                "updated": "2025-01-07T17:00:49Z",
                "updated_parsed": [
                    2025,
                    1,
                    7,
                    17,
                    0,
                    49,
                    1,
                    7,
                    0
                ],
                "published": "2025-01-07T17:00:49Z",
                "published_parsed": [
                    2025,
                    1,
                    7,
                    17,
                    0,
                    49,
                    1,
                    7,
                    0
                ],
                "title": "Not all tokens are created equal: Perplexity Attention Weighted Networks\n  for AI generated text detection",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Not all tokens are created equal: Perplexity Attention Weighted Networks\n  for AI generated text detection"
                },
                "summary": "The rapid advancement in large language models (LLMs) has significantly\nenhanced their ability to generate coherent and contextually relevant text,\nraising concerns about the misuse of AI-generated content and making it\ncritical to detect it. However, the task remains challenging, particularly in\nunseen domains or with unfamiliar LLMs. Leveraging LLM next-token distribution\noutputs offers a theoretically appealing approach for detection, as they\nencapsulate insights from the models' extensive pre-training on diverse\ncorpora. Despite its promise, zero-shot methods that attempt to operationalize\nthese outputs have met with limited success. We hypothesize that one of the\nproblems is that they use the mean to aggregate next-token distribution metrics\nacross tokens, when some tokens are naturally easier or harder to predict and\nshould be weighted differently. Based on this idea, we propose the Perplexity\nAttention Weighted Network (PAWN), which uses the last hidden states of the LLM\nand positions to weight the sum of a series of features based on metrics from\nthe next-token distribution across the sequence length. Although not zero-shot,\nour method allows us to cache the last hidden states and next-token\ndistribution metrics on disk, greatly reducing the training resource\nrequirements. PAWN shows competitive and even better performance\nin-distribution than the strongest baselines (fine-tuned LMs) with a fraction\nof their trainable parameters. Our model also generalizes better to unseen\ndomains and source models, with smaller variability in the decision boundary\nacross distribution shifts. It is also more robust to adversarial attacks, and\nif the backbone has multilingual capabilities, it presents decent\ngeneralization to languages not seen during supervised training, with LLaMA3-1B\nreaching a mean macro-averaged F1 score of 81.46% in cross-validation with nine\nlanguages.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rapid advancement in large language models (LLMs) has significantly\nenhanced their ability to generate coherent and contextually relevant text,\nraising concerns about the misuse of AI-generated content and making it\ncritical to detect it. However, the task remains challenging, particularly in\nunseen domains or with unfamiliar LLMs. Leveraging LLM next-token distribution\noutputs offers a theoretically appealing approach for detection, as they\nencapsulate insights from the models' extensive pre-training on diverse\ncorpora. Despite its promise, zero-shot methods that attempt to operationalize\nthese outputs have met with limited success. We hypothesize that one of the\nproblems is that they use the mean to aggregate next-token distribution metrics\nacross tokens, when some tokens are naturally easier or harder to predict and\nshould be weighted differently. Based on this idea, we propose the Perplexity\nAttention Weighted Network (PAWN), which uses the last hidden states of the LLM\nand positions to weight the sum of a series of features based on metrics from\nthe next-token distribution across the sequence length. Although not zero-shot,\nour method allows us to cache the last hidden states and next-token\ndistribution metrics on disk, greatly reducing the training resource\nrequirements. PAWN shows competitive and even better performance\nin-distribution than the strongest baselines (fine-tuned LMs) with a fraction\nof their trainable parameters. Our model also generalizes better to unseen\ndomains and source models, with smaller variability in the decision boundary\nacross distribution shifts. It is also more robust to adversarial attacks, and\nif the backbone has multilingual capabilities, it presents decent\ngeneralization to languages not seen during supervised training, with LLaMA3-1B\nreaching a mean macro-averaged F1 score of 81.46% in cross-validation with nine\nlanguages."
                },
                "authors": [
                    {
                        "name": "Pablo Miralles-González"
                    },
                    {
                        "name": "Javier Huertas-Tato"
                    },
                    {
                        "name": "Alejandro Martín"
                    },
                    {
                        "name": "David Camacho"
                    }
                ],
                "author_detail": {
                    "name": "David Camacho"
                },
                "author": "David Camacho",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.03940v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.03940v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.09275v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.09275v2",
                "updated": "2025-01-06T23:16:22Z",
                "updated_parsed": [
                    2025,
                    1,
                    6,
                    23,
                    16,
                    22,
                    0,
                    6,
                    0
                ],
                "published": "2024-11-14T08:25:31Z",
                "published_parsed": [
                    2024,
                    11,
                    14,
                    8,
                    25,
                    31,
                    3,
                    319,
                    0
                ],
                "title": "Parallel $k$d-tree with Batch Updates",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Parallel $k$d-tree with Batch Updates"
                },
                "summary": "The $k$d-tree is one of the most widely used data structures to manage\nmulti-dimensional data. Due to the ever-growing data volume, it is imperative\nto consider parallelism in $k$d-trees. However, we observed challenges in\nexisting parallel kd-tree implementations, for both constructions and updates.\n  The goal of this paper is to develop efficient in-memory $k$d-trees by\nsupporting high parallelism and cache-efficiency. We propose the Pkd-tree\n(Parallel $k$d-tree), a parallel $k$d-tree that is efficient both in theory and\nin practice. The Pkd-tree supports parallel tree construction, batch update\n(insertion and deletion), and various queries including k-nearest neighbor\nsearch, range query, and range count. We proved that our algorithms have strong\ntheoretical bounds in work (sequential time complexity), span (parallelism),\nand cache complexity. Our key techniques include 1) an efficient construction\nalgorithm that optimizes work, span, and cache complexity simultaneously, and\n2) reconstruction-based update algorithms that guarantee the tree to be\nweight-balanced. With the new algorithmic insights and careful engineering\neffort, we achieved a highly optimized implementation of the Pkd-tree.\n  We tested Pkd-tree with various synthetic and real-world datasets, including\nboth uniform and highly skewed data. We compare the Pkd-tree with\nstate-of-the-art parallel $k$d-tree implementations. In all tests, with better\nor competitive query performance, Pkd-tree is much faster in construction and\nupdates consistently than all baselines. We released our code.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The $k$d-tree is one of the most widely used data structures to manage\nmulti-dimensional data. Due to the ever-growing data volume, it is imperative\nto consider parallelism in $k$d-trees. However, we observed challenges in\nexisting parallel kd-tree implementations, for both constructions and updates.\n  The goal of this paper is to develop efficient in-memory $k$d-trees by\nsupporting high parallelism and cache-efficiency. We propose the Pkd-tree\n(Parallel $k$d-tree), a parallel $k$d-tree that is efficient both in theory and\nin practice. The Pkd-tree supports parallel tree construction, batch update\n(insertion and deletion), and various queries including k-nearest neighbor\nsearch, range query, and range count. We proved that our algorithms have strong\ntheoretical bounds in work (sequential time complexity), span (parallelism),\nand cache complexity. Our key techniques include 1) an efficient construction\nalgorithm that optimizes work, span, and cache complexity simultaneously, and\n2) reconstruction-based update algorithms that guarantee the tree to be\nweight-balanced. With the new algorithmic insights and careful engineering\neffort, we achieved a highly optimized implementation of the Pkd-tree.\n  We tested Pkd-tree with various synthetic and real-world datasets, including\nboth uniform and highly skewed data. We compare the Pkd-tree with\nstate-of-the-art parallel $k$d-tree implementations. In all tests, with better\nor competitive query performance, Pkd-tree is much faster in construction and\nupdates consistently than all baselines. We released our code."
                },
                "authors": [
                    {
                        "name": "Ziyang Men"
                    },
                    {
                        "name": "Zheqi Shen"
                    },
                    {
                        "name": "Yan Gu"
                    },
                    {
                        "name": "Yihan Sun"
                    }
                ],
                "author_detail": {
                    "name": "Yihan Sun"
                },
                "author": "Yihan Sun",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.09275v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.09275v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.04052v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.04052v1",
                "updated": "2025-01-06T22:40:40Z",
                "updated_parsed": [
                    2025,
                    1,
                    6,
                    22,
                    40,
                    40,
                    0,
                    6,
                    0
                ],
                "published": "2025-01-06T22:40:40Z",
                "published_parsed": [
                    2025,
                    1,
                    6,
                    22,
                    40,
                    40,
                    0,
                    6,
                    0
                ],
                "title": "The Power of Negative Zero: Datatype Customization for Quantized Large\n  Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Power of Negative Zero: Datatype Customization for Quantized Large\n  Language Models"
                },
                "summary": "Large language models (LLMs) have demonstrated remarkable performance across\nvarious machine learning tasks, quickly becoming one of the most prevalent AI\nworkloads. Yet the substantial memory requirement of LLMs significantly hinders\ntheir deployment for end users. Post-training quantization (PTQ) serves as one\nof the most hardware-efficient methods to mitigate the memory and computational\ndemands of LLMs. Although the traditional integer (INT) datatype has received\nwidespread adoption in PTQ methods, floating-point (FP) quantization has\nemerged as a viable alternative thanks to its effectiveness in fitting LLM\nnumerical distributions. However, the FP datatype in sign-magnitude binary\nrepresentation contains both positive and negative zero, which constrains its\nrepresentation capability, particularly under low precision (3 and 4 bits). In\nthis paper, we extend the basic FP datatype to perform Redundant Zero Remapping\n(RaZeR), which remaps the negative zero FP encoding to a set of pre-defined\nspecial values to maximally utilize FP quantization encodings and to better fit\nLLM numerical distributions. Through careful selection of special values, RaZeR\noutperforms conventional asymmetric INT quantization while achieving high\ncomputational efficiency. We demonstrate that RaZeR can be seamlessly\nintegrated with quantization algorithms for both weights and KV-cache,\nincluding advanced methods with clipping and transformations, and consistently\nachieve better model accuracy. Additionally, we implement a fast GEMV kernel\nwith fused dequantization that efficiently converts the 4-bit RaZeR value to\nFP16 through novel bit-level manipulation. On modern GPUs, our evaluation shows\nthat RaZeR improves the GEMV speed by up to 7.56$\\times$ compared to the FP16\nimplementation, while achieving up to 2.72$\\times$ speedup in the LLM decoding\nthroughput.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have demonstrated remarkable performance across\nvarious machine learning tasks, quickly becoming one of the most prevalent AI\nworkloads. Yet the substantial memory requirement of LLMs significantly hinders\ntheir deployment for end users. Post-training quantization (PTQ) serves as one\nof the most hardware-efficient methods to mitigate the memory and computational\ndemands of LLMs. Although the traditional integer (INT) datatype has received\nwidespread adoption in PTQ methods, floating-point (FP) quantization has\nemerged as a viable alternative thanks to its effectiveness in fitting LLM\nnumerical distributions. However, the FP datatype in sign-magnitude binary\nrepresentation contains both positive and negative zero, which constrains its\nrepresentation capability, particularly under low precision (3 and 4 bits). In\nthis paper, we extend the basic FP datatype to perform Redundant Zero Remapping\n(RaZeR), which remaps the negative zero FP encoding to a set of pre-defined\nspecial values to maximally utilize FP quantization encodings and to better fit\nLLM numerical distributions. Through careful selection of special values, RaZeR\noutperforms conventional asymmetric INT quantization while achieving high\ncomputational efficiency. We demonstrate that RaZeR can be seamlessly\nintegrated with quantization algorithms for both weights and KV-cache,\nincluding advanced methods with clipping and transformations, and consistently\nachieve better model accuracy. Additionally, we implement a fast GEMV kernel\nwith fused dequantization that efficiently converts the 4-bit RaZeR value to\nFP16 through novel bit-level manipulation. On modern GPUs, our evaluation shows\nthat RaZeR improves the GEMV speed by up to 7.56$\\times$ compared to the FP16\nimplementation, while achieving up to 2.72$\\times$ speedup in the LLM decoding\nthroughput."
                },
                "authors": [
                    {
                        "name": "Yuzong Chen"
                    },
                    {
                        "name": "Xilai Dai"
                    },
                    {
                        "name": "Chi-chih Chang"
                    },
                    {
                        "name": "Yash Akhauri"
                    },
                    {
                        "name": "Mohamed S. Abdelfattah"
                    }
                ],
                "author_detail": {
                    "name": "Mohamed S. Abdelfattah"
                },
                "author": "Mohamed S. Abdelfattah",
                "arxiv_comment": "under submission",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.04052v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.04052v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.03322v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.03322v1",
                "updated": "2025-01-06T19:00:03Z",
                "updated_parsed": [
                    2025,
                    1,
                    6,
                    19,
                    0,
                    3,
                    0,
                    6,
                    0
                ],
                "published": "2025-01-06T19:00:03Z",
                "published_parsed": [
                    2025,
                    1,
                    6,
                    19,
                    0,
                    3,
                    0,
                    6,
                    0
                ],
                "title": "Twinkle: A GPU-based binary-lens microlensing code with contour\n  integration method",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Twinkle: A GPU-based binary-lens microlensing code with contour\n  integration method"
                },
                "summary": "With the rapidly increasing rate of microlensing planet detections,\nmicrolensing modeling software faces significant challenges in computation\nefficiency. Here, we develop the Twinkle code, an efficient and robust\nbinary-lens modeling software suite optimized for heterogeneous computing\ndevices, especially GPUs. Existing microlensing codes have the issue of\ncatastrophic cancellation that undermines the numerical stability and\nprecision, and Twinkle resolves them by refining the coefficients of the\nbinary-lens equation. We also devise an improved method for robustly\nidentifying ghost images, thereby enhancing computational reliability. We have\nadvanced the state of the art by optimizing Twinkle specifically for\nheterogeneous computing devices by taking into account the unique task and\ncache memory dispatching patterns of GPUs, while the compatibility with the\ntraditional computing architectures of CPUs is still maintained. Twinkle has\ndemonstrated an acceleration of approximately 2 orders of magnitude (>~100\ntimes) on contemporary GPUs. The enhancement in computational speed of Twinkle\nwill translate to the delivery of accurate and highly efficient data analysis\nfor ongoing and upcoming microlensing projects. Both GPU and CPU versions of\nTwinkle are open-source and publicly available.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the rapidly increasing rate of microlensing planet detections,\nmicrolensing modeling software faces significant challenges in computation\nefficiency. Here, we develop the Twinkle code, an efficient and robust\nbinary-lens modeling software suite optimized for heterogeneous computing\ndevices, especially GPUs. Existing microlensing codes have the issue of\ncatastrophic cancellation that undermines the numerical stability and\nprecision, and Twinkle resolves them by refining the coefficients of the\nbinary-lens equation. We also devise an improved method for robustly\nidentifying ghost images, thereby enhancing computational reliability. We have\nadvanced the state of the art by optimizing Twinkle specifically for\nheterogeneous computing devices by taking into account the unique task and\ncache memory dispatching patterns of GPUs, while the compatibility with the\ntraditional computing architectures of CPUs is still maintained. Twinkle has\ndemonstrated an acceleration of approximately 2 orders of magnitude (>~100\ntimes) on contemporary GPUs. The enhancement in computational speed of Twinkle\nwill translate to the delivery of accurate and highly efficient data analysis\nfor ongoing and upcoming microlensing projects. Both GPU and CPU versions of\nTwinkle are open-source and publicly available."
                },
                "authors": [
                    {
                        "name": "Suwei Wang"
                    },
                    {
                        "name": "Lile Wang"
                    },
                    {
                        "name": "Subo Dong"
                    }
                ],
                "author_detail": {
                    "name": "Subo Dong"
                },
                "author": "Subo Dong",
                "arxiv_comment": "Accepted by ApJS, GitHub link:\n  https://github.com/AsterLight0626/Twinkle",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.03322v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.03322v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.IM",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.IM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "astro-ph.EP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "astro-ph.GA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "astro-ph.SR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.19919v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.19919v2",
                "updated": "2025-01-06T15:59:23Z",
                "updated_parsed": [
                    2025,
                    1,
                    6,
                    15,
                    59,
                    23,
                    0,
                    6,
                    0
                ],
                "published": "2024-12-27T20:47:23Z",
                "published_parsed": [
                    2024,
                    12,
                    27,
                    20,
                    47,
                    23,
                    4,
                    362,
                    0
                ],
                "title": "Direct Comparison of Magnetic Penetration Depth in Kagome\n  Superconductors AV$_3$Sb$_5$ (A = Cs, K, Rb)",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Direct Comparison of Magnetic Penetration Depth in Kagome\n  Superconductors AV$_3$Sb$_5$ (A = Cs, K, Rb)"
                },
                "summary": "We report measurements of the local temperature-dependent penetration depth,\n$\\lambda(T)$, in the Kagome superconductors AV$_3$Sb$_5$ (A = Cs, K, Rb) using\nscanning superconducting quantum interference device (SQUID) microscopy. Our\nresults suggest that the superconducting order in all three compounds is fully\ngapped, in contrast to reports of nodal superconductivity in KV$_3$Sb$_5$ and\nRbV$_3$Sb$_5$. Analysis of the temperature-dependent superfluid density,\n$\\rho_s(T)$, shows deviations from the behavior expected for a single isotropic\ngap, but the data are well described by models incorporating either a single\nanisotropic gap or two isotropic gaps. Notably, the temperature dependences of\n$\\lambda(T)$ and $\\rho_s(T)$ in KV$_3$Sb$_5$ and RbV$_3$Sb$_5$ are\nqualitatively more similar to each other than to CsV$_3$Sb$_5$, consistent with\nthe superconducting phase reflecting features of the normal-state band\nstructure. Our findings provide a direct comparison of the superconducting\nproperties across the AV$_3$Sb$_5$ family.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We report measurements of the local temperature-dependent penetration depth,\n$\\lambda(T)$, in the Kagome superconductors AV$_3$Sb$_5$ (A = Cs, K, Rb) using\nscanning superconducting quantum interference device (SQUID) microscopy. Our\nresults suggest that the superconducting order in all three compounds is fully\ngapped, in contrast to reports of nodal superconductivity in KV$_3$Sb$_5$ and\nRbV$_3$Sb$_5$. Analysis of the temperature-dependent superfluid density,\n$\\rho_s(T)$, shows deviations from the behavior expected for a single isotropic\ngap, but the data are well described by models incorporating either a single\nanisotropic gap or two isotropic gaps. Notably, the temperature dependences of\n$\\lambda(T)$ and $\\rho_s(T)$ in KV$_3$Sb$_5$ and RbV$_3$Sb$_5$ are\nqualitatively more similar to each other than to CsV$_3$Sb$_5$, consistent with\nthe superconducting phase reflecting features of the normal-state band\nstructure. Our findings provide a direct comparison of the superconducting\nproperties across the AV$_3$Sb$_5$ family."
                },
                "authors": [
                    {
                        "name": "Austin Kaczmarek"
                    },
                    {
                        "name": "Andrea Capa Salinas"
                    },
                    {
                        "name": "Stephen D. Wilson"
                    },
                    {
                        "name": "Katja C. Nowack"
                    }
                ],
                "author_detail": {
                    "name": "Katja C. Nowack"
                },
                "author": "Katja C. Nowack",
                "arxiv_comment": "5 pages, 3 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.19919v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.19919v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cond-mat.supr-con",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cond-mat.supr-con",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.02803v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.02803v1",
                "updated": "2025-01-06T06:44:13Z",
                "updated_parsed": [
                    2025,
                    1,
                    6,
                    6,
                    44,
                    13,
                    0,
                    6,
                    0
                ],
                "published": "2025-01-06T06:44:13Z",
                "published_parsed": [
                    2025,
                    1,
                    6,
                    6,
                    44,
                    13,
                    0,
                    6,
                    0
                ],
                "title": "Enhancing Lifelong Multi-Agent Path Finding with Cache Mechanism",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Enhancing Lifelong Multi-Agent Path Finding with Cache Mechanism"
                },
                "summary": "Multi-Agent Path Finding (MAPF), which focuses on finding collision-free\npaths for multiple robots, is crucial in autonomous warehouse operations.\nLifelong MAPF (L-MAPF), where agents are continuously reassigned new targets\nupon completing their current tasks, offers a more realistic approximation of\nreal-world warehouse scenarios. While cache storage systems can enhance\nefficiency and reduce operational costs, existing approaches primarily rely on\nexpectations and mathematical models, often without adequately addressing the\nchallenges of multi-robot planning and execution. In this paper, we introduce a\nnovel mechanism called Lifelong MAPF with Cache Mechanism (L-MAPF-CM), which\nintegrates high-level cache storage with low-level path planning. We have\ninvolved a new type of map grid called cache for temporary item storage.\nAdditionally, we involved a task assigner (TA) with a locking mechanism to\nbridge the gap between the new cache grid and L-MAPF algorithm. The TA\ndynamically allocates target locations to agents based on their status in\nvarious scenarios. We evaluated L-MAPF-CM using different cache replacement\npolicies and task distributions. L-MAPF-CM has demonstrated performance\nimprovements particularly with high cache hit rates and smooth traffic\nconditions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-Agent Path Finding (MAPF), which focuses on finding collision-free\npaths for multiple robots, is crucial in autonomous warehouse operations.\nLifelong MAPF (L-MAPF), where agents are continuously reassigned new targets\nupon completing their current tasks, offers a more realistic approximation of\nreal-world warehouse scenarios. While cache storage systems can enhance\nefficiency and reduce operational costs, existing approaches primarily rely on\nexpectations and mathematical models, often without adequately addressing the\nchallenges of multi-robot planning and execution. In this paper, we introduce a\nnovel mechanism called Lifelong MAPF with Cache Mechanism (L-MAPF-CM), which\nintegrates high-level cache storage with low-level path planning. We have\ninvolved a new type of map grid called cache for temporary item storage.\nAdditionally, we involved a task assigner (TA) with a locking mechanism to\nbridge the gap between the new cache grid and L-MAPF algorithm. The TA\ndynamically allocates target locations to agents based on their status in\nvarious scenarios. We evaluated L-MAPF-CM using different cache replacement\npolicies and task distributions. L-MAPF-CM has demonstrated performance\nimprovements particularly with high cache hit rates and smooth traffic\nconditions."
                },
                "authors": [
                    {
                        "name": "Yimin Tang"
                    },
                    {
                        "name": "Zhenghong Yu"
                    },
                    {
                        "name": "Yi Zheng"
                    },
                    {
                        "name": "T. K. Satish Kumar"
                    },
                    {
                        "name": "Jiaoyang Li"
                    },
                    {
                        "name": "Sven Koenig"
                    }
                ],
                "author_detail": {
                    "name": "Sven Koenig"
                },
                "author": "Sven Koenig",
                "arxiv_comment": "arXiv admin note: substantial text overlap with arXiv:2403.13421",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.02803v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.02803v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.07772v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.07772v2",
                "updated": "2025-01-06T01:26:42Z",
                "updated_parsed": [
                    2025,
                    1,
                    6,
                    1,
                    26,
                    42,
                    0,
                    6,
                    0
                ],
                "published": "2024-12-10T18:59:50Z",
                "published_parsed": [
                    2024,
                    12,
                    10,
                    18,
                    59,
                    50,
                    1,
                    345,
                    0
                ],
                "title": "From Slow Bidirectional to Fast Autoregressive Video Diffusion Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "From Slow Bidirectional to Fast Autoregressive Video Diffusion Models"
                },
                "summary": "Current video diffusion models achieve impressive generation quality but\nstruggle in interactive applications due to bidirectional attention\ndependencies. The generation of a single frame requires the model to process\nthe entire sequence, including the future. We address this limitation by\nadapting a pretrained bidirectional diffusion transformer to an autoregressive\ntransformer that generates frames on-the-fly. To further reduce latency, we\nextend distribution matching distillation (DMD) to videos, distilling 50-step\ndiffusion model into a 4-step generator. To enable stable and high-quality\ndistillation, we introduce a student initialization scheme based on teacher's\nODE trajectories, as well as an asymmetric distillation strategy that\nsupervises a causal student model with a bidirectional teacher. This approach\neffectively mitigates error accumulation in autoregressive generation, allowing\nlong-duration video synthesis despite training on short clips. Our model\nachieves a total score of 84.27 on the VBench-Long benchmark, surpassing all\nprevious video generation models. It enables fast streaming generation of\nhigh-quality videos at 9.4 FPS on a single GPU thanks to KV caching. Our\napproach also enables streaming video-to-video translation, image-to-video, and\ndynamic prompting in a zero-shot manner. We will release the code based on an\nopen-source model in the future.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Current video diffusion models achieve impressive generation quality but\nstruggle in interactive applications due to bidirectional attention\ndependencies. The generation of a single frame requires the model to process\nthe entire sequence, including the future. We address this limitation by\nadapting a pretrained bidirectional diffusion transformer to an autoregressive\ntransformer that generates frames on-the-fly. To further reduce latency, we\nextend distribution matching distillation (DMD) to videos, distilling 50-step\ndiffusion model into a 4-step generator. To enable stable and high-quality\ndistillation, we introduce a student initialization scheme based on teacher's\nODE trajectories, as well as an asymmetric distillation strategy that\nsupervises a causal student model with a bidirectional teacher. This approach\neffectively mitigates error accumulation in autoregressive generation, allowing\nlong-duration video synthesis despite training on short clips. Our model\nachieves a total score of 84.27 on the VBench-Long benchmark, surpassing all\nprevious video generation models. It enables fast streaming generation of\nhigh-quality videos at 9.4 FPS on a single GPU thanks to KV caching. Our\napproach also enables streaming video-to-video translation, image-to-video, and\ndynamic prompting in a zero-shot manner. We will release the code based on an\nopen-source model in the future."
                },
                "authors": [
                    {
                        "name": "Tianwei Yin"
                    },
                    {
                        "name": "Qiang Zhang"
                    },
                    {
                        "name": "Richard Zhang"
                    },
                    {
                        "name": "William T. Freeman"
                    },
                    {
                        "name": "Fredo Durand"
                    },
                    {
                        "name": "Eli Shechtman"
                    },
                    {
                        "name": "Xun Huang"
                    }
                ],
                "author_detail": {
                    "name": "Xun Huang"
                },
                "author": "Xun Huang",
                "arxiv_comment": "Project Page: https://causvid.github.io/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.07772v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.07772v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.20504v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.20504v2",
                "updated": "2025-01-05T14:11:48Z",
                "updated_parsed": [
                    2025,
                    1,
                    5,
                    14,
                    11,
                    48,
                    6,
                    5,
                    0
                ],
                "published": "2024-12-29T15:42:24Z",
                "published_parsed": [
                    2024,
                    12,
                    29,
                    15,
                    42,
                    24,
                    6,
                    364,
                    0
                ],
                "title": "ReTaKe: Reducing Temporal and Knowledge Redundancy for Long Video\n  Understanding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ReTaKe: Reducing Temporal and Knowledge Redundancy for Long Video\n  Understanding"
                },
                "summary": "Video Large Language Models (VideoLLMs) have achieved remarkable progress in\nvideo understanding. However, existing VideoLLMs often inherit the limitations\nof their backbone LLMs in handling long sequences, leading to challenges for\nlong video understanding. Common solutions either simply uniformly sample\nvideos' frames or compress visual tokens, which focus primarily on low-level\ntemporal visual redundancy, overlooking high-level knowledge redundancy. This\nlimits the achievable compression rate with minimal loss. To this end. we\nintroduce a training-free method, $\\textbf{ReTaKe}$, containing two novel\nmodules DPSelect and PivotKV, to jointly model and reduce both temporal visual\nredundancy and knowledge redundancy for long video understanding. Specifically,\nDPSelect identifies keyframes with local maximum peak distance based on their\nvisual features, which are closely aligned with human video perception. PivotKV\nemploys the obtained keyframes as pivots and conducts KV-Cache compression for\nthe non-pivot tokens with low attention scores, which are derived from the\nlearned prior knowledge of LLMs. Experiments on benchmarks VideoMME, MLVU, and\nLVBench, show that ReTaKe can support 4x longer video sequences with minimal\nperformance loss (<1%) and outperform all similar-size VideoLLMs with 3%-5%,\neven surpassing or on par with much larger ones. Our code is available at\nhttps://github.com/SCZwangxiao/video-ReTaKe",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Video Large Language Models (VideoLLMs) have achieved remarkable progress in\nvideo understanding. However, existing VideoLLMs often inherit the limitations\nof their backbone LLMs in handling long sequences, leading to challenges for\nlong video understanding. Common solutions either simply uniformly sample\nvideos' frames or compress visual tokens, which focus primarily on low-level\ntemporal visual redundancy, overlooking high-level knowledge redundancy. This\nlimits the achievable compression rate with minimal loss. To this end. we\nintroduce a training-free method, $\\textbf{ReTaKe}$, containing two novel\nmodules DPSelect and PivotKV, to jointly model and reduce both temporal visual\nredundancy and knowledge redundancy for long video understanding. Specifically,\nDPSelect identifies keyframes with local maximum peak distance based on their\nvisual features, which are closely aligned with human video perception. PivotKV\nemploys the obtained keyframes as pivots and conducts KV-Cache compression for\nthe non-pivot tokens with low attention scores, which are derived from the\nlearned prior knowledge of LLMs. Experiments on benchmarks VideoMME, MLVU, and\nLVBench, show that ReTaKe can support 4x longer video sequences with minimal\nperformance loss (<1%) and outperform all similar-size VideoLLMs with 3%-5%,\neven surpassing or on par with much larger ones. Our code is available at\nhttps://github.com/SCZwangxiao/video-ReTaKe"
                },
                "authors": [
                    {
                        "name": "Xiao Wang"
                    },
                    {
                        "name": "Qingyi Si"
                    },
                    {
                        "name": "Jianlong Wu"
                    },
                    {
                        "name": "Shiyu Zhu"
                    },
                    {
                        "name": "Li Cao"
                    },
                    {
                        "name": "Liqiang Nie"
                    }
                ],
                "author_detail": {
                    "name": "Liqiang Nie"
                },
                "author": "Liqiang Nie",
                "arxiv_comment": "Update performance in MLVU-dev and LVBench",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.20504v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.20504v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.02524v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.02524v1",
                "updated": "2025-01-05T12:51:08Z",
                "updated_parsed": [
                    2025,
                    1,
                    5,
                    12,
                    51,
                    8,
                    6,
                    5,
                    0
                ],
                "published": "2025-01-05T12:51:08Z",
                "published_parsed": [
                    2025,
                    1,
                    5,
                    12,
                    51,
                    8,
                    6,
                    5,
                    0
                ],
                "title": "A Full-System Simulation Framework for CXL-Based SSD Memory System",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Full-System Simulation Framework for CXL-Based SSD Memory System"
                },
                "summary": "Compute eXpress Link (CXL) is a promising technology for memory\ndisaggregation and expansion. Especially, CXL makes it more effectively for\nlarge-capacity storage devices such as Solid State Drive (SSD) to be deployed\nin the memory pool. However, CXL-based SSDs are still in early stages,\nnecessitating the development of reliable simulation tools. In this paper, we\npropose CXL-SSD-Sim, the first open-source full-system simulator designed to\nsimulate CXL-based SSD memory system. Constructed on the foundation of gem5 and\nSimpleSSD, CXL-SSD-Sim extends an high fidelity SSD memory expander model along\nwith the corresponding device driver. In addition, CXL-SSD-Sim models a DRAM\nlayer as a caching mechanism for the SSD, meticulously engineered to counteract\nlatency issues inherent to CXL-based SSD memory access. Experiments are\nperformed among five different memory devices with CXL-SSD-Sim in aspect of\nlatency, bandwidth and real-world benchmark performance. These experiments\nserve to underscore the efficacy of our simulation tool in providing a\ncomprehensive analysis of CXL-based SSD memory systems. The CXL-SSD-Sim\nsimulator is available at https://github.com/WangYaohuii/CXL-SSD-Sim.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Compute eXpress Link (CXL) is a promising technology for memory\ndisaggregation and expansion. Especially, CXL makes it more effectively for\nlarge-capacity storage devices such as Solid State Drive (SSD) to be deployed\nin the memory pool. However, CXL-based SSDs are still in early stages,\nnecessitating the development of reliable simulation tools. In this paper, we\npropose CXL-SSD-Sim, the first open-source full-system simulator designed to\nsimulate CXL-based SSD memory system. Constructed on the foundation of gem5 and\nSimpleSSD, CXL-SSD-Sim extends an high fidelity SSD memory expander model along\nwith the corresponding device driver. In addition, CXL-SSD-Sim models a DRAM\nlayer as a caching mechanism for the SSD, meticulously engineered to counteract\nlatency issues inherent to CXL-based SSD memory access. Experiments are\nperformed among five different memory devices with CXL-SSD-Sim in aspect of\nlatency, bandwidth and real-world benchmark performance. These experiments\nserve to underscore the efficacy of our simulation tool in providing a\ncomprehensive analysis of CXL-based SSD memory systems. The CXL-SSD-Sim\nsimulator is available at https://github.com/WangYaohuii/CXL-SSD-Sim."
                },
                "authors": [
                    {
                        "name": "Yaohui Wang"
                    },
                    {
                        "name": "Zicong Wang"
                    },
                    {
                        "name": "Fanfeng Meng"
                    },
                    {
                        "name": "Yanjing Wang"
                    },
                    {
                        "name": "Yang Ou"
                    },
                    {
                        "name": "Lizhou Wu"
                    },
                    {
                        "name": "Wentao Hong"
                    },
                    {
                        "name": "Xuran Ge"
                    },
                    {
                        "name": "Jijun Cao"
                    }
                ],
                "author_detail": {
                    "name": "Jijun Cao"
                },
                "author": "Jijun Cao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.02524v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.02524v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.02469v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.02469v1",
                "updated": "2025-01-05T07:41:53Z",
                "updated_parsed": [
                    2025,
                    1,
                    5,
                    7,
                    41,
                    53,
                    6,
                    5,
                    0
                ],
                "published": "2025-01-05T07:41:53Z",
                "published_parsed": [
                    2025,
                    1,
                    5,
                    7,
                    41,
                    53,
                    6,
                    5,
                    0
                ],
                "title": "LoRaConnect: Unlocking HTTP Potential on LoRa Backbones for Remote Areas\n  and Ad-Hoc Networks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LoRaConnect: Unlocking HTTP Potential on LoRa Backbones for Remote Areas\n  and Ad-Hoc Networks"
                },
                "summary": "The minimal infrastructure requirements of LoRa make it suitable for\ndeployments in remote and disaster-stricken areas. Concomitantly, the modern\nera is witnessing the proliferation of web applications in all aspects of human\nlife, including IoT and other network services. Contemporary IoT and network\nsolutions heavily rely on web applications to render services. However, despite\nthe recent research and development pivoted around LoRa, there is still a lack\nof studies focusing on web application access over LoRa networks. Specifically,\ntechnical challenges like payload size limitation, low data rate, and\ncontentions in multi-user setups limit the applicability of LoRa for web\napplications. Hence, we propose LoRaWeb, which enables web access over LoRa\nnetworks. The LoRaWeb hardware tethers a WiFi hotspot to which the client\ndevices connect and access the web pages using a web browser. LoRa backbone of\nthe network handles the web page transmission from the requester and receiver\ndevices. LoRaWeb implements a synchronization procedure to address the\naforementioned challenges for effective message exchange between requesters and\nresponders. The system implements a caching mechanism to reduce latency and\ncontention. Additionally, it implements a message-slicing mechanism in the\napplication layer to overcome the hardware limitations on the message length.\nThe actual hardware-based implementation results indicate seamless deployment,\nand the results indicate an average access time of ~$0.95 S$ for a $1.5 KB$ and\n~$6 S$ for a $10 KB$ size web page.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The minimal infrastructure requirements of LoRa make it suitable for\ndeployments in remote and disaster-stricken areas. Concomitantly, the modern\nera is witnessing the proliferation of web applications in all aspects of human\nlife, including IoT and other network services. Contemporary IoT and network\nsolutions heavily rely on web applications to render services. However, despite\nthe recent research and development pivoted around LoRa, there is still a lack\nof studies focusing on web application access over LoRa networks. Specifically,\ntechnical challenges like payload size limitation, low data rate, and\ncontentions in multi-user setups limit the applicability of LoRa for web\napplications. Hence, we propose LoRaWeb, which enables web access over LoRa\nnetworks. The LoRaWeb hardware tethers a WiFi hotspot to which the client\ndevices connect and access the web pages using a web browser. LoRa backbone of\nthe network handles the web page transmission from the requester and receiver\ndevices. LoRaWeb implements a synchronization procedure to address the\naforementioned challenges for effective message exchange between requesters and\nresponders. The system implements a caching mechanism to reduce latency and\ncontention. Additionally, it implements a message-slicing mechanism in the\napplication layer to overcome the hardware limitations on the message length.\nThe actual hardware-based implementation results indicate seamless deployment,\nand the results indicate an average access time of ~$0.95 S$ for a $1.5 KB$ and\n~$6 S$ for a $10 KB$ size web page."
                },
                "authors": [
                    {
                        "name": "Atonu Ghosh"
                    },
                    {
                        "name": "Sudip Misra"
                    }
                ],
                "author_detail": {
                    "name": "Sudip Misra"
                },
                "author": "Sudip Misra",
                "arxiv_comment": "11 pages, 15 figures, and 2 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.02469v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.02469v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.01805v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.01805v1",
                "updated": "2025-01-03T13:32:57Z",
                "updated_parsed": [
                    2025,
                    1,
                    3,
                    13,
                    32,
                    57,
                    4,
                    3,
                    0
                ],
                "published": "2025-01-03T13:32:57Z",
                "published_parsed": [
                    2025,
                    1,
                    3,
                    13,
                    32,
                    57,
                    4,
                    3,
                    0
                ],
                "title": "End-to-End Long Document Summarization using Gradient Caching",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "End-to-End Long Document Summarization using Gradient Caching"
                },
                "summary": "Training transformer-based encoder-decoder models for long document\nsummarization poses a significant challenge due to the quadratic memory\nconsumption during training. Several approaches have been proposed to extend\nthe input length at test time, but training with these approaches is still\ndifficult, requiring truncation of input documents and causing a mismatch\nbetween training and test conditions. In this work, we propose CachED (Gradient\n$\\textbf{Cach}$ing for $\\textbf{E}$ncoder-$\\textbf{D}$ecoder models), an\napproach that enables end-to-end training of existing transformer-based\nencoder-decoder models, using the entire document without truncation.\nSpecifically, we apply non-overlapping sliding windows to input documents,\nfollowed by fusion in decoder. During backpropagation, the gradients are cached\nat the decoder and are passed through the encoder in chunks by re-computing the\nhidden vectors, similar to gradient checkpointing. In the experiments on long\ndocument summarization, we extend BART to CachED BART, processing more than\n500K tokens during training and achieving superior performance without using\nany additional parameters.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Training transformer-based encoder-decoder models for long document\nsummarization poses a significant challenge due to the quadratic memory\nconsumption during training. Several approaches have been proposed to extend\nthe input length at test time, but training with these approaches is still\ndifficult, requiring truncation of input documents and causing a mismatch\nbetween training and test conditions. In this work, we propose CachED (Gradient\n$\\textbf{Cach}$ing for $\\textbf{E}$ncoder-$\\textbf{D}$ecoder models), an\napproach that enables end-to-end training of existing transformer-based\nencoder-decoder models, using the entire document without truncation.\nSpecifically, we apply non-overlapping sliding windows to input documents,\nfollowed by fusion in decoder. During backpropagation, the gradients are cached\nat the decoder and are passed through the encoder in chunks by re-computing the\nhidden vectors, similar to gradient checkpointing. In the experiments on long\ndocument summarization, we extend BART to CachED BART, processing more than\n500K tokens during training and achieving superior performance without using\nany additional parameters."
                },
                "authors": [
                    {
                        "name": "Rohit Saxena"
                    },
                    {
                        "name": "Hao Tang"
                    },
                    {
                        "name": "Frank Keller"
                    }
                ],
                "author_detail": {
                    "name": "Frank Keller"
                },
                "author": "Frank Keller",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.01805v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.01805v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.01792v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.01792v1",
                "updated": "2025-01-03T12:51:37Z",
                "updated_parsed": [
                    2025,
                    1,
                    3,
                    12,
                    51,
                    37,
                    4,
                    3,
                    0
                ],
                "published": "2025-01-03T12:51:37Z",
                "published_parsed": [
                    2025,
                    1,
                    3,
                    12,
                    51,
                    37,
                    4,
                    3,
                    0
                ],
                "title": "Efficient LLM Inference with Activation Checkpointing and Hybrid Caching",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient LLM Inference with Activation Checkpointing and Hybrid Caching"
                },
                "summary": "Recent large language models (LLMs) with enormous model sizes use many GPUs\nto meet memory capacity requirements incurring substantial costs for token\ngeneration. To provide cost-effective LLM inference with relaxed latency\nconstraints, extensive research has focused on expanding GPU memory by\nleveraging the host memory. However, LLM inference engines that utilize the\nhost memory often face underutilization of GPU compute units, as a considerable\nportion of inference time is spent in loading the model onto the GPU via\nhost-GPU interconnect. To tackle these challenges of the host memory offloading\nfor LLM, we introduce HybridServe, an LLM inference system with activation\ncheckpointing based on activation caching. The activation cache stores\nactivation checkpoints generated during intermediate inference stages, allowing\nthe fast recomputation of KV cache while model parameters are transferred to\nGPU from host memory. Unlike conventional methods that recompute the KV cache\nfrom scratch using token IDs, the activation cache allows bypassing projection\nand FFN operations. To balance between the activation recomputation and\nparameter loading overhead, this study proposes a KV-activation hybrid caching\nscheme which finds the best ratio of the key-value and activation caches to\nadjust the recomputation time. Our system achieves 2.19x throughput improvement\nover the state-of-the-art prior work for offloading both model weights and KV\ncache.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent large language models (LLMs) with enormous model sizes use many GPUs\nto meet memory capacity requirements incurring substantial costs for token\ngeneration. To provide cost-effective LLM inference with relaxed latency\nconstraints, extensive research has focused on expanding GPU memory by\nleveraging the host memory. However, LLM inference engines that utilize the\nhost memory often face underutilization of GPU compute units, as a considerable\nportion of inference time is spent in loading the model onto the GPU via\nhost-GPU interconnect. To tackle these challenges of the host memory offloading\nfor LLM, we introduce HybridServe, an LLM inference system with activation\ncheckpointing based on activation caching. The activation cache stores\nactivation checkpoints generated during intermediate inference stages, allowing\nthe fast recomputation of KV cache while model parameters are transferred to\nGPU from host memory. Unlike conventional methods that recompute the KV cache\nfrom scratch using token IDs, the activation cache allows bypassing projection\nand FFN operations. To balance between the activation recomputation and\nparameter loading overhead, this study proposes a KV-activation hybrid caching\nscheme which finds the best ratio of the key-value and activation caches to\nadjust the recomputation time. Our system achieves 2.19x throughput improvement\nover the state-of-the-art prior work for offloading both model weights and KV\ncache."
                },
                "authors": [
                    {
                        "name": "Sanghyeon Lee"
                    },
                    {
                        "name": "Hongbeen Kim"
                    },
                    {
                        "name": "Soojin Hwang"
                    },
                    {
                        "name": "Guseul Heo"
                    },
                    {
                        "name": "Minwoo Noh"
                    },
                    {
                        "name": "Jaehyuk Huh"
                    }
                ],
                "author_detail": {
                    "name": "Jaehyuk Huh"
                },
                "author": "Jaehyuk Huh",
                "arxiv_comment": "14 pages, 15 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.01792v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.01792v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.01424v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.01424v1",
                "updated": "2025-01-02T18:59:44Z",
                "updated_parsed": [
                    2025,
                    1,
                    2,
                    18,
                    59,
                    44,
                    3,
                    2,
                    0
                ],
                "published": "2025-01-02T18:59:44Z",
                "published_parsed": [
                    2025,
                    1,
                    2,
                    18,
                    59,
                    44,
                    3,
                    2,
                    0
                ],
                "title": "Object-level Visual Prompts for Compositional Image Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Object-level Visual Prompts for Compositional Image Generation"
                },
                "summary": "We introduce a method for composing object-level visual prompts within a\ntext-to-image diffusion model. Our approach addresses the task of generating\nsemantically coherent compositions across diverse scenes and styles, similar to\nthe versatility and expressiveness offered by text prompts. A key challenge in\nthis task is to preserve the identity of the objects depicted in the input\nvisual prompts, while also generating diverse compositions across different\nimages. To address this challenge, we introduce a new KV-mixed cross-attention\nmechanism, in which keys and values are learned from distinct visual\nrepresentations. The keys are derived from an encoder with a small bottleneck\nfor layout control, whereas the values come from a larger bottleneck encoder\nthat captures fine-grained appearance details. By mixing keys and values from\nthese complementary sources, our model preserves the identity of the visual\nprompts while supporting flexible variations in object arrangement, pose, and\ncomposition. During inference, we further propose object-level compositional\nguidance to improve the method's identity preservation and layout correctness.\nResults show that our technique produces diverse scene compositions that\npreserve the unique characteristics of each visual prompt, expanding the\ncreative potential of text-to-image generation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce a method for composing object-level visual prompts within a\ntext-to-image diffusion model. Our approach addresses the task of generating\nsemantically coherent compositions across diverse scenes and styles, similar to\nthe versatility and expressiveness offered by text prompts. A key challenge in\nthis task is to preserve the identity of the objects depicted in the input\nvisual prompts, while also generating diverse compositions across different\nimages. To address this challenge, we introduce a new KV-mixed cross-attention\nmechanism, in which keys and values are learned from distinct visual\nrepresentations. The keys are derived from an encoder with a small bottleneck\nfor layout control, whereas the values come from a larger bottleneck encoder\nthat captures fine-grained appearance details. By mixing keys and values from\nthese complementary sources, our model preserves the identity of the visual\nprompts while supporting flexible variations in object arrangement, pose, and\ncomposition. During inference, we further propose object-level compositional\nguidance to improve the method's identity preservation and layout correctness.\nResults show that our technique produces diverse scene compositions that\npreserve the unique characteristics of each visual prompt, expanding the\ncreative potential of text-to-image generation."
                },
                "authors": [
                    {
                        "name": "Gaurav Parmar"
                    },
                    {
                        "name": "Or Patashnik"
                    },
                    {
                        "name": "Kuan-Chieh Wang"
                    },
                    {
                        "name": "Daniil Ostashev"
                    },
                    {
                        "name": "Srinivasa Narasimhan"
                    },
                    {
                        "name": "Jun-Yan Zhu"
                    },
                    {
                        "name": "Daniel Cohen-Or"
                    },
                    {
                        "name": "Kfir Aberman"
                    }
                ],
                "author_detail": {
                    "name": "Kfir Aberman"
                },
                "author": "Kfir Aberman",
                "arxiv_comment": "Project: https://snap-research.github.io/visual-composer/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.01424v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.01424v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.GR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.01039v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.01039v1",
                "updated": "2025-01-02T03:41:32Z",
                "updated_parsed": [
                    2025,
                    1,
                    2,
                    3,
                    41,
                    32,
                    3,
                    2,
                    0
                ],
                "published": "2025-01-02T03:41:32Z",
                "published_parsed": [
                    2025,
                    1,
                    2,
                    3,
                    41,
                    32,
                    3,
                    2,
                    0
                ],
                "title": "MSWA: Refining Local Attention with Multi-ScaleWindow Attention",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MSWA: Refining Local Attention with Multi-ScaleWindow Attention"
                },
                "summary": "Transformer-based LLMs have achieved exceptional performance across a wide\nrange of NLP tasks. However, the standard self-attention mechanism suffers from\nquadratic time complexity and linearly increased cache size. Sliding window\nattention (SWA) solves this problem by restricting the attention range to a\nfixed-size local context window. Nevertheless, SWA employs a uniform window\nsize for each head in each layer, making it inefficient in capturing context of\nvarying scales. To mitigate this limitation, we propose Multi-Scale Window\nAttention (MSWA) which applies diverse window sizes across heads and layers in\nthe Transformer. It not only allows for different window sizes among heads\nwithin the same layer but also progressively increases window size allocation\nfrom shallow to deep layers, thus enabling the model to capture contextual\ninformation with different lengths and distances. Experimental results on\nlanguage modeling and common-sense reasoning tasks substantiate that MSWA\noutperforms traditional local attention in both effectiveness and efficiency.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Transformer-based LLMs have achieved exceptional performance across a wide\nrange of NLP tasks. However, the standard self-attention mechanism suffers from\nquadratic time complexity and linearly increased cache size. Sliding window\nattention (SWA) solves this problem by restricting the attention range to a\nfixed-size local context window. Nevertheless, SWA employs a uniform window\nsize for each head in each layer, making it inefficient in capturing context of\nvarying scales. To mitigate this limitation, we propose Multi-Scale Window\nAttention (MSWA) which applies diverse window sizes across heads and layers in\nthe Transformer. It not only allows for different window sizes among heads\nwithin the same layer but also progressively increases window size allocation\nfrom shallow to deep layers, thus enabling the model to capture contextual\ninformation with different lengths and distances. Experimental results on\nlanguage modeling and common-sense reasoning tasks substantiate that MSWA\noutperforms traditional local attention in both effectiveness and efficiency."
                },
                "authors": [
                    {
                        "name": "Yixing Xu"
                    },
                    {
                        "name": "Shivank Nag"
                    },
                    {
                        "name": "Dong Li"
                    },
                    {
                        "name": "Lu Tian"
                    },
                    {
                        "name": "Emad Barsoum"
                    }
                ],
                "author_detail": {
                    "name": "Emad Barsoum"
                },
                "author": "Emad Barsoum",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.01039v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.01039v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.19442v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.19442v2",
                "updated": "2025-01-02T03:40:15Z",
                "updated_parsed": [
                    2025,
                    1,
                    2,
                    3,
                    40,
                    15,
                    3,
                    2,
                    0
                ],
                "published": "2024-12-27T04:17:57Z",
                "published_parsed": [
                    2024,
                    12,
                    27,
                    4,
                    17,
                    57,
                    4,
                    362,
                    0
                ],
                "title": "A Survey on Large Language Model Acceleration based on KV Cache\n  Management",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Survey on Large Language Model Acceleration based on KV Cache\n  Management"
                },
                "summary": "Large Language Models (LLMs) have revolutionized a wide range of domains such\nas natural language processing, computer vision, and multi-modal tasks due to\ntheir ability to comprehend context and perform logical reasoning. However, the\ncomputational and memory demands of LLMs, particularly during inference, pose\nsignificant challenges when scaling them to real-world, long-context, and\nreal-time applications. Key-Value (KV) cache management has emerged as a\ncritical optimization technique for accelerating LLM inference by reducing\nredundant computations and improving memory utilization. This survey provides a\ncomprehensive overview of KV cache management strategies for LLM acceleration,\ncategorizing them into token-level, model-level, and system-level\noptimizations. Token-level strategies include KV cache selection, budget\nallocation, merging, quantization, and low-rank decomposition, while\nmodel-level optimizations focus on architectural innovations and attention\nmechanisms to enhance KV reuse. System-level approaches address memory\nmanagement, scheduling, and hardware-aware designs to improve efficiency across\ndiverse computing environments. Additionally, the survey provides an overview\nof both text and multimodal datasets and benchmarks used to evaluate these\nstrategies. By presenting detailed taxonomies and comparative analyses, this\nwork aims to offer useful insights for researchers and practitioners to support\nthe development of efficient and scalable KV cache management techniques,\ncontributing to the practical deployment of LLMs in real-world applications.\nThe curated paper list for KV cache management is in:\n\\href{https://github.com/TreeAI-Lab/Awesome-KV-Cache-Management}{https://github.com/TreeAI-Lab/Awesome-KV-Cache-Management}.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have revolutionized a wide range of domains such\nas natural language processing, computer vision, and multi-modal tasks due to\ntheir ability to comprehend context and perform logical reasoning. However, the\ncomputational and memory demands of LLMs, particularly during inference, pose\nsignificant challenges when scaling them to real-world, long-context, and\nreal-time applications. Key-Value (KV) cache management has emerged as a\ncritical optimization technique for accelerating LLM inference by reducing\nredundant computations and improving memory utilization. This survey provides a\ncomprehensive overview of KV cache management strategies for LLM acceleration,\ncategorizing them into token-level, model-level, and system-level\noptimizations. Token-level strategies include KV cache selection, budget\nallocation, merging, quantization, and low-rank decomposition, while\nmodel-level optimizations focus on architectural innovations and attention\nmechanisms to enhance KV reuse. System-level approaches address memory\nmanagement, scheduling, and hardware-aware designs to improve efficiency across\ndiverse computing environments. Additionally, the survey provides an overview\nof both text and multimodal datasets and benchmarks used to evaluate these\nstrategies. By presenting detailed taxonomies and comparative analyses, this\nwork aims to offer useful insights for researchers and practitioners to support\nthe development of efficient and scalable KV cache management techniques,\ncontributing to the practical deployment of LLMs in real-world applications.\nThe curated paper list for KV cache management is in:\n\\href{https://github.com/TreeAI-Lab/Awesome-KV-Cache-Management}{https://github.com/TreeAI-Lab/Awesome-KV-Cache-Management}."
                },
                "authors": [
                    {
                        "name": "Haoyang Li"
                    },
                    {
                        "name": "Yiming Li"
                    },
                    {
                        "name": "Anxin Tian"
                    },
                    {
                        "name": "Tianhao Tang"
                    },
                    {
                        "name": "Zhanchao Xu"
                    },
                    {
                        "name": "Xuejia Chen"
                    },
                    {
                        "name": "Nicole Hu"
                    },
                    {
                        "name": "Wei Dong"
                    },
                    {
                        "name": "Qing Li"
                    },
                    {
                        "name": "Lei Chen"
                    }
                ],
                "author_detail": {
                    "name": "Lei Chen"
                },
                "author": "Lei Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.19442v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.19442v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.01005v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.01005v1",
                "updated": "2025-01-02T02:02:20Z",
                "updated_parsed": [
                    2025,
                    1,
                    2,
                    2,
                    2,
                    20,
                    3,
                    2,
                    0
                ],
                "published": "2025-01-02T02:02:20Z",
                "published_parsed": [
                    2025,
                    1,
                    2,
                    2,
                    2,
                    20,
                    3,
                    2,
                    0
                ],
                "title": "FlashInfer: Efficient and Customizable Attention Engine for LLM\n  Inference Serving",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FlashInfer: Efficient and Customizable Attention Engine for LLM\n  Inference Serving"
                },
                "summary": "Transformers, driven by attention mechanisms, form the foundation of large\nlanguage models (LLMs). As these models scale up, efficient GPU attention\nkernels become essential for high-throughput and low-latency inference. Diverse\nLLM applications demand flexible and high-performance attention solutions. We\npresent FlashInfer: a customizable and efficient attention engine for LLM\nserving. FlashInfer tackles KV-cache storage heterogeneity using block-sparse\nformat and composable formats to optimize memory access and reduce redundancy.\nIt also offers a customizable attention template, enabling adaptation to\nvarious settings through Just-In-Time (JIT) compilation. Additionally,\nFlashInfer's load-balanced scheduling algorithm adjusts to dynamism of user\nrequests while maintaining compatibility with CUDAGraph which requires static\nconfiguration. FlashInfer have been integrated into leading LLM serving\nframeworks like SGLang, vLLM and MLC-Engine. Comprehensive kernel-level and\nend-to-end evaluations demonstrate FlashInfer's ability to significantly boost\nkernel performance across diverse inference scenarios: compared to\nstate-of-the-art LLM serving solutions, FlashInfer achieve 29-69%\ninter-token-latency reduction compared to compiler backends for LLM serving\nbenchmark, 28-30% latency reduction for long-context inference, and 13-17%\nspeedup for LLM serving with parallel generation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Transformers, driven by attention mechanisms, form the foundation of large\nlanguage models (LLMs). As these models scale up, efficient GPU attention\nkernels become essential for high-throughput and low-latency inference. Diverse\nLLM applications demand flexible and high-performance attention solutions. We\npresent FlashInfer: a customizable and efficient attention engine for LLM\nserving. FlashInfer tackles KV-cache storage heterogeneity using block-sparse\nformat and composable formats to optimize memory access and reduce redundancy.\nIt also offers a customizable attention template, enabling adaptation to\nvarious settings through Just-In-Time (JIT) compilation. Additionally,\nFlashInfer's load-balanced scheduling algorithm adjusts to dynamism of user\nrequests while maintaining compatibility with CUDAGraph which requires static\nconfiguration. FlashInfer have been integrated into leading LLM serving\nframeworks like SGLang, vLLM and MLC-Engine. Comprehensive kernel-level and\nend-to-end evaluations demonstrate FlashInfer's ability to significantly boost\nkernel performance across diverse inference scenarios: compared to\nstate-of-the-art LLM serving solutions, FlashInfer achieve 29-69%\ninter-token-latency reduction compared to compiler backends for LLM serving\nbenchmark, 28-30% latency reduction for long-context inference, and 13-17%\nspeedup for LLM serving with parallel generation."
                },
                "authors": [
                    {
                        "name": "Zihao Ye"
                    },
                    {
                        "name": "Lequn Chen"
                    },
                    {
                        "name": "Ruihang Lai"
                    },
                    {
                        "name": "Wuwei Lin"
                    },
                    {
                        "name": "Yineng Zhang"
                    },
                    {
                        "name": "Stephanie Wang"
                    },
                    {
                        "name": "Tianqi Chen"
                    },
                    {
                        "name": "Baris Kasikci"
                    },
                    {
                        "name": "Vinod Grover"
                    },
                    {
                        "name": "Arvind Krishnamurthy"
                    },
                    {
                        "name": "Luis Ceze"
                    }
                ],
                "author_detail": {
                    "name": "Luis Ceze"
                },
                "author": "Luis Ceze",
                "arxiv_comment": "code available at http://github.com/flashinfer-ai/flashinfer",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.01005v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.01005v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.00946v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.00946v1",
                "updated": "2025-01-01T20:16:27Z",
                "updated_parsed": [
                    2025,
                    1,
                    1,
                    20,
                    16,
                    27,
                    2,
                    1,
                    0
                ],
                "published": "2025-01-01T20:16:27Z",
                "published_parsed": [
                    2025,
                    1,
                    1,
                    20,
                    16,
                    27,
                    2,
                    1,
                    0
                ],
                "title": "Cached Adaptive Token Merging: Dynamic Token Reduction and Redundant\n  Computation Elimination in Diffusion Model",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cached Adaptive Token Merging: Dynamic Token Reduction and Redundant\n  Computation Elimination in Diffusion Model"
                },
                "summary": "Diffusion models have emerged as a promising approach for generating\nhigh-quality, high-dimensional images. Nevertheless, these models are hindered\nby their high computational cost and slow inference, partly due to the\nquadratic computational complexity of the self-attention mechanisms with\nrespect to input size. Various approaches have been proposed to address this\ndrawback. One such approach focuses on reducing the number of tokens fed into\nthe self-attention, known as token merging (ToMe). In our method, which is\ncalled cached adaptive token merging(CA-ToMe), we calculate the similarity\nbetween tokens and then merge the r proportion of the most similar tokens.\nHowever, due to the repetitive patterns observed in adjacent steps and the\nvariation in the frequency of similarities, we aim to enhance this approach by\nimplementing an adaptive threshold for merging tokens and adding a caching\nmechanism that stores similar pairs across several adjacent steps. Empirical\nresults demonstrate that our method operates as a training-free acceleration\nmethod, achieving a speedup factor of 1.24 in the denoising process while\nmaintaining the same FID scores compared to existing approaches.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion models have emerged as a promising approach for generating\nhigh-quality, high-dimensional images. Nevertheless, these models are hindered\nby their high computational cost and slow inference, partly due to the\nquadratic computational complexity of the self-attention mechanisms with\nrespect to input size. Various approaches have been proposed to address this\ndrawback. One such approach focuses on reducing the number of tokens fed into\nthe self-attention, known as token merging (ToMe). In our method, which is\ncalled cached adaptive token merging(CA-ToMe), we calculate the similarity\nbetween tokens and then merge the r proportion of the most similar tokens.\nHowever, due to the repetitive patterns observed in adjacent steps and the\nvariation in the frequency of similarities, we aim to enhance this approach by\nimplementing an adaptive threshold for merging tokens and adding a caching\nmechanism that stores similar pairs across several adjacent steps. Empirical\nresults demonstrate that our method operates as a training-free acceleration\nmethod, achieving a speedup factor of 1.24 in the denoising process while\nmaintaining the same FID scores compared to existing approaches."
                },
                "authors": [
                    {
                        "name": "Omid Saghatchian"
                    },
                    {
                        "name": "Atiyeh Gh. Moghadam"
                    },
                    {
                        "name": "Ahmad Nickabadi"
                    }
                ],
                "author_detail": {
                    "name": "Ahmad Nickabadi"
                },
                "author": "Ahmad Nickabadi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.00946v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.00946v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.21023v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.21023v2",
                "updated": "2024-12-31T20:40:43Z",
                "updated_parsed": [
                    2024,
                    12,
                    31,
                    20,
                    40,
                    43,
                    1,
                    366,
                    0
                ],
                "published": "2024-12-30T15:46:53Z",
                "published_parsed": [
                    2024,
                    12,
                    30,
                    15,
                    46,
                    53,
                    0,
                    365,
                    0
                ],
                "title": "EdgeRAG: Online-Indexed RAG for Edge Devices",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "EdgeRAG: Online-Indexed RAG for Edge Devices"
                },
                "summary": "Deploying Retrieval Augmented Generation (RAG) on resource-constrained edge\ndevices is challenging due to limited memory and processing power. In this\nwork, we propose EdgeRAG which addresses the memory constraint by pruning\nembeddings within clusters and generating embeddings on-demand during\nretrieval. To avoid the latency of generating embeddings for large tail\nclusters, EdgeRAG pre-computes and stores embeddings for these clusters, while\nadaptively caching remaining embeddings to minimize redundant computations and\nfurther optimize latency. The result from BEIR suite shows that EdgeRAG offers\nsignificant latency reduction over the baseline IVF index, but with similar\ngeneration quality while allowing all of our evaluated datasets to fit into the\nmemory.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Deploying Retrieval Augmented Generation (RAG) on resource-constrained edge\ndevices is challenging due to limited memory and processing power. In this\nwork, we propose EdgeRAG which addresses the memory constraint by pruning\nembeddings within clusters and generating embeddings on-demand during\nretrieval. To avoid the latency of generating embeddings for large tail\nclusters, EdgeRAG pre-computes and stores embeddings for these clusters, while\nadaptively caching remaining embeddings to minimize redundant computations and\nfurther optimize latency. The result from BEIR suite shows that EdgeRAG offers\nsignificant latency reduction over the baseline IVF index, but with similar\ngeneration quality while allowing all of our evaluated datasets to fit into the\nmemory."
                },
                "authors": [
                    {
                        "name": "Korakit Seemakhupt"
                    },
                    {
                        "name": "Sihang Liu"
                    },
                    {
                        "name": "Samira Khan"
                    }
                ],
                "author_detail": {
                    "name": "Samira Khan"
                },
                "author": "Samira Khan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.21023v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.21023v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.00375v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.00375v1",
                "updated": "2024-12-31T09:56:40Z",
                "updated_parsed": [
                    2024,
                    12,
                    31,
                    9,
                    56,
                    40,
                    1,
                    366,
                    0
                ],
                "published": "2024-12-31T09:56:40Z",
                "published_parsed": [
                    2024,
                    12,
                    31,
                    9,
                    56,
                    40,
                    1,
                    366,
                    0
                ],
                "title": "Token Pruning for Caching Better: 9 Times Acceleration on Stable\n  Diffusion for Free",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Token Pruning for Caching Better: 9 Times Acceleration on Stable\n  Diffusion for Free"
                },
                "summary": "Stable Diffusion has achieved remarkable success in the field of\ntext-to-image generation, with its powerful generative capabilities and diverse\ngeneration results making a lasting impact. However, its iterative denoising\nintroduces high computational costs and slows generation speed, limiting\nbroader adoption. The community has made numerous efforts to reduce this\ncomputational burden, with methods like feature caching attracting attention\ndue to their effectiveness and simplicity. Nonetheless, simply reusing features\ncomputed at previous timesteps causes the features across adjacent timesteps to\nbecome similar, reducing the dynamics of features over time and ultimately\ncompromising the quality of generated images. In this paper, we introduce a\ndynamics-aware token pruning (DaTo) approach that addresses the limitations of\nfeature caching. DaTo selectively prunes tokens with lower dynamics, allowing\nonly high-dynamic tokens to participate in self-attention layers, thereby\nextending feature dynamics across timesteps. DaTo combines feature caching with\ntoken pruning in a training-free manner, achieving both temporal and token-wise\ninformation reuse. Applied to Stable Diffusion on the ImageNet, our approach\ndelivered a 9$\\times$ speedup while reducing FID by 0.33, indicating enhanced\nimage quality. On the COCO-30k, we observed a 7$\\times$ acceleration coupled\nwith a notable FID reduction of 2.17.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Stable Diffusion has achieved remarkable success in the field of\ntext-to-image generation, with its powerful generative capabilities and diverse\ngeneration results making a lasting impact. However, its iterative denoising\nintroduces high computational costs and slows generation speed, limiting\nbroader adoption. The community has made numerous efforts to reduce this\ncomputational burden, with methods like feature caching attracting attention\ndue to their effectiveness and simplicity. Nonetheless, simply reusing features\ncomputed at previous timesteps causes the features across adjacent timesteps to\nbecome similar, reducing the dynamics of features over time and ultimately\ncompromising the quality of generated images. In this paper, we introduce a\ndynamics-aware token pruning (DaTo) approach that addresses the limitations of\nfeature caching. DaTo selectively prunes tokens with lower dynamics, allowing\nonly high-dynamic tokens to participate in self-attention layers, thereby\nextending feature dynamics across timesteps. DaTo combines feature caching with\ntoken pruning in a training-free manner, achieving both temporal and token-wise\ninformation reuse. Applied to Stable Diffusion on the ImageNet, our approach\ndelivered a 9$\\times$ speedup while reducing FID by 0.33, indicating enhanced\nimage quality. On the COCO-30k, we observed a 7$\\times$ acceleration coupled\nwith a notable FID reduction of 2.17."
                },
                "authors": [
                    {
                        "name": "Evelyn Zhang"
                    },
                    {
                        "name": "Bang Xiao"
                    },
                    {
                        "name": "Jiayi Tang"
                    },
                    {
                        "name": "Qianli Ma"
                    },
                    {
                        "name": "Chang Zou"
                    },
                    {
                        "name": "Xuefei Ning"
                    },
                    {
                        "name": "Xuming Hu"
                    },
                    {
                        "name": "Linfeng Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Linfeng Zhang"
                },
                "author": "Linfeng Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.00375v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.00375v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.10516v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.10516v3",
                "updated": "2024-12-31T07:11:00Z",
                "updated_parsed": [
                    2024,
                    12,
                    31,
                    7,
                    11,
                    0,
                    1,
                    366,
                    0
                ],
                "published": "2024-09-16T17:59:52Z",
                "published_parsed": [
                    2024,
                    9,
                    16,
                    17,
                    59,
                    52,
                    0,
                    260,
                    0
                ],
                "title": "RetrievalAttention: Accelerating Long-Context LLM Inference via Vector\n  Retrieval",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RetrievalAttention: Accelerating Long-Context LLM Inference via Vector\n  Retrieval"
                },
                "summary": "Transformer-based Large Language Models (LLMs) have become increasingly\nimportant. However, due to the quadratic time complexity of attention\ncomputation, scaling LLMs to longer contexts incurs extremely slow inference\nspeed and high GPU memory consumption for caching key-value (KV) vectors. This\npaper proposes RetrievalAttention, a training-free approach to both accelerate\nattention computation and reduce GPU memory consumption. By leveraging the\ndynamic sparsity of attention mechanism, RetrievalAttention proposes to build\napproximate nearest neighbor search (ANNS) indexes for KV vectors in CPU memory\nand retrieve the most relevant ones through vector search during generation.\nUnfortunately, we observe that the off-the-shelf ANNS indexes are often\nineffective for such retrieval tasks due to the out-of-distribution (OOD)\nbetween query vectors and key vectors in the attention mechanism.\nRetrievalAttention addresses the OOD challenge by designing an attention-aware\nvector search algorithm that can adapt to the distribution of query vectors.\nOur evaluation demonstrates that RetrievalAttention achieves near full\nattention accuracy while only requiring access to 1--3% of the data. This leads\nto a significant reduction in the inference cost of long-context LLMs, with a\nmuch lower GPU memory footprint. In particular, RetrievalAttention only needs a\nsingle NVIDIA RTX4090 (24GB) to serve 128K tokens for LLMs with 8B parameters,\nwhich is capable of generating one token in 0.188 seconds.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Transformer-based Large Language Models (LLMs) have become increasingly\nimportant. However, due to the quadratic time complexity of attention\ncomputation, scaling LLMs to longer contexts incurs extremely slow inference\nspeed and high GPU memory consumption for caching key-value (KV) vectors. This\npaper proposes RetrievalAttention, a training-free approach to both accelerate\nattention computation and reduce GPU memory consumption. By leveraging the\ndynamic sparsity of attention mechanism, RetrievalAttention proposes to build\napproximate nearest neighbor search (ANNS) indexes for KV vectors in CPU memory\nand retrieve the most relevant ones through vector search during generation.\nUnfortunately, we observe that the off-the-shelf ANNS indexes are often\nineffective for such retrieval tasks due to the out-of-distribution (OOD)\nbetween query vectors and key vectors in the attention mechanism.\nRetrievalAttention addresses the OOD challenge by designing an attention-aware\nvector search algorithm that can adapt to the distribution of query vectors.\nOur evaluation demonstrates that RetrievalAttention achieves near full\nattention accuracy while only requiring access to 1--3% of the data. This leads\nto a significant reduction in the inference cost of long-context LLMs, with a\nmuch lower GPU memory footprint. In particular, RetrievalAttention only needs a\nsingle NVIDIA RTX4090 (24GB) to serve 128K tokens for LLMs with 8B parameters,\nwhich is capable of generating one token in 0.188 seconds."
                },
                "authors": [
                    {
                        "name": "Di Liu"
                    },
                    {
                        "name": "Meng Chen"
                    },
                    {
                        "name": "Baotong Lu"
                    },
                    {
                        "name": "Huiqiang Jiang"
                    },
                    {
                        "name": "Zhenhua Han"
                    },
                    {
                        "name": "Qianxi Zhang"
                    },
                    {
                        "name": "Qi Chen"
                    },
                    {
                        "name": "Chengruidong Zhang"
                    },
                    {
                        "name": "Bailu Ding"
                    },
                    {
                        "name": "Kai Zhang"
                    },
                    {
                        "name": "Chen Chen"
                    },
                    {
                        "name": "Fan Yang"
                    },
                    {
                        "name": "Yuqing Yang"
                    },
                    {
                        "name": "Lili Qiu"
                    }
                ],
                "author_detail": {
                    "name": "Lili Qiu"
                },
                "author": "Lili Qiu",
                "arxiv_comment": "19 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.10516v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.10516v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.00279v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.00279v1",
                "updated": "2024-12-31T05:24:30Z",
                "updated_parsed": [
                    2024,
                    12,
                    31,
                    5,
                    24,
                    30,
                    1,
                    366,
                    0
                ],
                "published": "2024-12-31T05:24:30Z",
                "published_parsed": [
                    2024,
                    12,
                    31,
                    5,
                    24,
                    30,
                    1,
                    366,
                    0
                ],
                "title": "Performant Automatic BLAS Offloading on Unified Memory Architecture with\n  OpenMP First-Touch Style Data Movement",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Performant Automatic BLAS Offloading on Unified Memory Architecture with\n  OpenMP First-Touch Style Data Movement"
                },
                "summary": "BLAS is a fundamental building block of advanced linear algebra libraries and\nmany modern scientific computing applications. GPUs are known for their strong\narithmetic computing capabilities and are highly suited for BLAS operations.\nHowever, porting code to GPUs often requires significant effort, especially for\nlarge, complex codes or legacy codes, even for BLAS-heavy applications. While\nvarious tools exist to automatically offload BLAS to GPUs, they are often\nimpractical due to the high costs associated with mandatory data transfers. The\nadvent of unified memory architectures in recent GPU designs, such as the\nNVIDIA Grace-Hopper, allows cache-coherent memory access across all types of\nmemory for both CPU and GPU, potentially eliminating the bottlenecks faced in\nconventional architectures. This breakthrough paves the way for innovative\napplication developments and porting strategies. Building on our preliminary\nwork demonstrating the potential of automatic *gemm offload, this paper extends\nthe framework to all level-3 BLAS operations and introduces SCILIB-Accel, a\nnovel tool for automatic BLAS offload. SCILIB-Accel leverages the memory\ncoherency in Grace-Hopper and introduces a Device First-Use data movement\npolicy inspired by the OpenMP First-Touch approach in multi-socket CPU\nprogramming, minimizing CPU-GPU data transfers for typical scientific computing\ncodes. Additionally, utilizing dynamic binary instrumentation, the tool\nintercepts BLAS symbols directly from a CPU binary, requiring no code\nmodifications or recompilation. SCILIB-Accel has been evaluated using multiple\nquantum physics codes on up to a few hundred GPU nodes, yielding promising\nspeedups. Notably, for the LSMS method in the MuST suite, a 3x speedup was\nachieved on Grace-Hopper compared to Grace-Grace.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "BLAS is a fundamental building block of advanced linear algebra libraries and\nmany modern scientific computing applications. GPUs are known for their strong\narithmetic computing capabilities and are highly suited for BLAS operations.\nHowever, porting code to GPUs often requires significant effort, especially for\nlarge, complex codes or legacy codes, even for BLAS-heavy applications. While\nvarious tools exist to automatically offload BLAS to GPUs, they are often\nimpractical due to the high costs associated with mandatory data transfers. The\nadvent of unified memory architectures in recent GPU designs, such as the\nNVIDIA Grace-Hopper, allows cache-coherent memory access across all types of\nmemory for both CPU and GPU, potentially eliminating the bottlenecks faced in\nconventional architectures. This breakthrough paves the way for innovative\napplication developments and porting strategies. Building on our preliminary\nwork demonstrating the potential of automatic *gemm offload, this paper extends\nthe framework to all level-3 BLAS operations and introduces SCILIB-Accel, a\nnovel tool for automatic BLAS offload. SCILIB-Accel leverages the memory\ncoherency in Grace-Hopper and introduces a Device First-Use data movement\npolicy inspired by the OpenMP First-Touch approach in multi-socket CPU\nprogramming, minimizing CPU-GPU data transfers for typical scientific computing\ncodes. Additionally, utilizing dynamic binary instrumentation, the tool\nintercepts BLAS symbols directly from a CPU binary, requiring no code\nmodifications or recompilation. SCILIB-Accel has been evaluated using multiple\nquantum physics codes on up to a few hundred GPU nodes, yielding promising\nspeedups. Notably, for the LSMS method in the MuST suite, a 3x speedup was\nachieved on Grace-Hopper compared to Grace-Grace."
                },
                "authors": [
                    {
                        "name": "Junjie Li"
                    }
                ],
                "author_detail": {
                    "name": "Junjie Li"
                },
                "author": "Junjie Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.00279v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.00279v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.00243v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.00243v1",
                "updated": "2024-12-31T03:19:38Z",
                "updated_parsed": [
                    2024,
                    12,
                    31,
                    3,
                    19,
                    38,
                    1,
                    366,
                    0
                ],
                "published": "2024-12-31T03:19:38Z",
                "published_parsed": [
                    2024,
                    12,
                    31,
                    3,
                    19,
                    38,
                    1,
                    366,
                    0
                ],
                "title": "Cross-Layer Cache Aggregation for Token Reduction in Ultra-Fine-Grained\n  Image Recognition",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cross-Layer Cache Aggregation for Token Reduction in Ultra-Fine-Grained\n  Image Recognition"
                },
                "summary": "Ultra-fine-grained image recognition (UFGIR) is a challenging task that\ninvolves classifying images within a macro-category. While traditional FGIR\ndeals with classifying different species, UFGIR goes beyond by classifying\nsub-categories within a species such as cultivars of a plant. In recent times\nthe usage of Vision Transformer-based backbones has allowed methods to obtain\noutstanding recognition performances in this task but this comes at a\nsignificant cost in terms of computation specially since this task\nsignificantly benefits from incorporating higher resolution images. Therefore,\ntechniques such as token reduction have emerged to reduce the computational\ncost. However, dropping tokens leads to loss of essential information for\nfine-grained categories, specially as the token keep rate is reduced.\nTherefore, to counteract the loss of information brought by the usage of token\nreduction we propose a novel Cross-Layer Aggregation Classification Head and a\nCross-Layer Cache mechanism to recover and access information from previous\nlayers in later locations. Extensive experiments covering more than 2000 runs\nacross diverse settings including 5 datasets, 9 backbones, 7 token reduction\nmethods, 5 keep rates, and 2 image sizes demonstrate the effectiveness of the\nproposed plug-and-play modules and allow us to push the boundaries of accuracy\nvs cost for UFGIR by reducing the kept tokens to extremely low ratios of up to\n10\\% while maintaining a competitive accuracy to state-of-the-art models. Code\nis available at: \\url{https://github.com/arkel23/CLCA}",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Ultra-fine-grained image recognition (UFGIR) is a challenging task that\ninvolves classifying images within a macro-category. While traditional FGIR\ndeals with classifying different species, UFGIR goes beyond by classifying\nsub-categories within a species such as cultivars of a plant. In recent times\nthe usage of Vision Transformer-based backbones has allowed methods to obtain\noutstanding recognition performances in this task but this comes at a\nsignificant cost in terms of computation specially since this task\nsignificantly benefits from incorporating higher resolution images. Therefore,\ntechniques such as token reduction have emerged to reduce the computational\ncost. However, dropping tokens leads to loss of essential information for\nfine-grained categories, specially as the token keep rate is reduced.\nTherefore, to counteract the loss of information brought by the usage of token\nreduction we propose a novel Cross-Layer Aggregation Classification Head and a\nCross-Layer Cache mechanism to recover and access information from previous\nlayers in later locations. Extensive experiments covering more than 2000 runs\nacross diverse settings including 5 datasets, 9 backbones, 7 token reduction\nmethods, 5 keep rates, and 2 image sizes demonstrate the effectiveness of the\nproposed plug-and-play modules and allow us to push the boundaries of accuracy\nvs cost for UFGIR by reducing the kept tokens to extremely low ratios of up to\n10\\% while maintaining a competitive accuracy to state-of-the-art models. Code\nis available at: \\url{https://github.com/arkel23/CLCA}"
                },
                "authors": [
                    {
                        "name": "Edwin Arkel Rios"
                    },
                    {
                        "name": "Jansen Christopher Yuanda"
                    },
                    {
                        "name": "Vincent Leon Ghanz"
                    },
                    {
                        "name": "Cheng-Wei Yu"
                    },
                    {
                        "name": "Bo-Cheng Lai"
                    },
                    {
                        "name": "Min-Chun Hu"
                    }
                ],
                "author_detail": {
                    "name": "Min-Chun Hu"
                },
                "author": "Min-Chun Hu",
                "arxiv_comment": "Accepted to ICASSP 2025. Main: 5 pages, 4 figures, 1 table",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.00243v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.00243v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2; I.4",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.21015v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.21015v1",
                "updated": "2024-12-30T15:33:19Z",
                "updated_parsed": [
                    2024,
                    12,
                    30,
                    15,
                    33,
                    19,
                    0,
                    365,
                    0
                ],
                "published": "2024-12-30T15:33:19Z",
                "published_parsed": [
                    2024,
                    12,
                    30,
                    15,
                    33,
                    19,
                    0,
                    365,
                    0
                ],
                "title": "MapQaTor: A System for Efficient Annotation of Map Query Datasets",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MapQaTor: A System for Efficient Annotation of Map Query Datasets"
                },
                "summary": "Mapping and navigation services like Google Maps, Apple Maps, Openstreet\nMaps, are essential for accessing various location-based data, yet they often\nstruggle to handle natural language geospatial queries. Recent advancements in\nLarge Language Models (LLMs) show promise in question answering (QA), but\ncreating reliable geospatial QA datasets from map services remains challenging.\nWe introduce MapQaTor, a web application that streamlines the creation of\nreproducible, traceable map-based QA datasets. With its plug-and-play\narchitecture, MapQaTor enables seamless integration with any maps API, allowing\nusers to gather and visualize data from diverse sources with minimal setup. By\ncaching API responses, the platform ensures consistent ground truth, enhancing\nthe reliability of the data even as real-world information evolves. MapQaTor\ncentralizes data retrieval, annotation, and visualization within a single\nplatform, offering a unique opportunity to evaluate the current state of\nLLM-based geospatial reasoning while advancing their capabilities for improved\ngeospatial understanding. Evaluation metrics show that, MapQaTor speeds up the\nannotation process by at least 30 times compared to manual methods,\nunderscoring its potential for developing geospatial resources, such as complex\nmap reasoning datasets. The website is live at: https://mapqator.github.io/ and\na demo video is available at: https://youtu.be/7_aV9Wmhs6Q.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mapping and navigation services like Google Maps, Apple Maps, Openstreet\nMaps, are essential for accessing various location-based data, yet they often\nstruggle to handle natural language geospatial queries. Recent advancements in\nLarge Language Models (LLMs) show promise in question answering (QA), but\ncreating reliable geospatial QA datasets from map services remains challenging.\nWe introduce MapQaTor, a web application that streamlines the creation of\nreproducible, traceable map-based QA datasets. With its plug-and-play\narchitecture, MapQaTor enables seamless integration with any maps API, allowing\nusers to gather and visualize data from diverse sources with minimal setup. By\ncaching API responses, the platform ensures consistent ground truth, enhancing\nthe reliability of the data even as real-world information evolves. MapQaTor\ncentralizes data retrieval, annotation, and visualization within a single\nplatform, offering a unique opportunity to evaluate the current state of\nLLM-based geospatial reasoning while advancing their capabilities for improved\ngeospatial understanding. Evaluation metrics show that, MapQaTor speeds up the\nannotation process by at least 30 times compared to manual methods,\nunderscoring its potential for developing geospatial resources, such as complex\nmap reasoning datasets. The website is live at: https://mapqator.github.io/ and\na demo video is available at: https://youtu.be/7_aV9Wmhs6Q."
                },
                "authors": [
                    {
                        "name": "Mahir Labib Dihan"
                    },
                    {
                        "name": "Mohammed Eunus Ali"
                    },
                    {
                        "name": "Md Rizwan Parvez"
                    }
                ],
                "author_detail": {
                    "name": "Md Rizwan Parvez"
                },
                "author": "Md Rizwan Parvez",
                "arxiv_comment": "13 pages, 35 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.21015v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.21015v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.12094v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.12094v3",
                "updated": "2024-12-30T14:54:29Z",
                "updated_parsed": [
                    2024,
                    12,
                    30,
                    14,
                    54,
                    29,
                    0,
                    365,
                    0
                ],
                "published": "2024-12-16T18:58:57Z",
                "published_parsed": [
                    2024,
                    12,
                    16,
                    18,
                    58,
                    57,
                    0,
                    351,
                    0
                ],
                "title": "SepLLM: Accelerate Large Language Models by Compressing One Segment into\n  One Separator",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SepLLM: Accelerate Large Language Models by Compressing One Segment into\n  One Separator"
                },
                "summary": "Large Language Models (LLMs) have exhibited exceptional performance across a\nspectrum of natural language processing tasks. However, their substantial sizes\npose considerable challenges, particularly in computational demands and\ninference speed, due to their quadratic complexity. In this work, we have\nidentified a key pattern: certain seemingly meaningless special tokens (i.e.,\nseparators) contribute disproportionately to attention scores compared to\nsemantically meaningful tokens. This observation suggests that information of\nthe segments between these separator tokens can be effectively condensed into\nthe separator tokens themselves without significant information loss. Guided by\nthis insight, we introduce SepLLM, a plug-and-play framework that accelerates\ninference by compressing these segments and eliminating redundant tokens.\nAdditionally, we implement efficient kernels for training acceleration.\nExperimental results across training-free, training-from-scratch, and\npost-training settings demonstrate SepLLM's effectiveness. Notably, using the\nLlama-3-8B backbone, SepLLM achieves over 50% reduction in KV cache on the\nGSM8K-CoT benchmark while maintaining comparable performance. Furthermore, in\nstreaming settings, SepLLM effectively processes sequences of up to 4 million\ntokens or more while maintaining consistent language modeling capabilities.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have exhibited exceptional performance across a\nspectrum of natural language processing tasks. However, their substantial sizes\npose considerable challenges, particularly in computational demands and\ninference speed, due to their quadratic complexity. In this work, we have\nidentified a key pattern: certain seemingly meaningless special tokens (i.e.,\nseparators) contribute disproportionately to attention scores compared to\nsemantically meaningful tokens. This observation suggests that information of\nthe segments between these separator tokens can be effectively condensed into\nthe separator tokens themselves without significant information loss. Guided by\nthis insight, we introduce SepLLM, a plug-and-play framework that accelerates\ninference by compressing these segments and eliminating redundant tokens.\nAdditionally, we implement efficient kernels for training acceleration.\nExperimental results across training-free, training-from-scratch, and\npost-training settings demonstrate SepLLM's effectiveness. Notably, using the\nLlama-3-8B backbone, SepLLM achieves over 50% reduction in KV cache on the\nGSM8K-CoT benchmark while maintaining comparable performance. Furthermore, in\nstreaming settings, SepLLM effectively processes sequences of up to 4 million\ntokens or more while maintaining consistent language modeling capabilities."
                },
                "authors": [
                    {
                        "name": "Guoxuan Chen"
                    },
                    {
                        "name": "Han Shi"
                    },
                    {
                        "name": "Jiawei Li"
                    },
                    {
                        "name": "Yihang Gao"
                    },
                    {
                        "name": "Xiaozhe Ren"
                    },
                    {
                        "name": "Yimeng Chen"
                    },
                    {
                        "name": "Xin Jiang"
                    },
                    {
                        "name": "Zhenguo Li"
                    },
                    {
                        "name": "Weiyang Liu"
                    },
                    {
                        "name": "Chao Huang"
                    }
                ],
                "author_detail": {
                    "name": "Chao Huang"
                },
                "author": "Chao Huang",
                "arxiv_comment": "We have made our code publicly available at sepllm.github.io. Our\n  codebase supports efficient multi-node distributed training with accelerated\n  attention module Sep-Attention and also supports numerous existing Fusion\n  Operators to accelerate the training process, such as fused rope, etc. If you\n  find our code helpful, please kindly consider giving us a **star** on\n  GitHub^_^. Thank you very much!",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.12094v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.12094v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.20887v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.20887v1",
                "updated": "2024-12-30T11:54:19Z",
                "updated_parsed": [
                    2024,
                    12,
                    30,
                    11,
                    54,
                    19,
                    0,
                    365,
                    0
                ],
                "published": "2024-12-30T11:54:19Z",
                "published_parsed": [
                    2024,
                    12,
                    30,
                    11,
                    54,
                    19,
                    0,
                    365,
                    0
                ],
                "title": "A Hidden Quantum Paraelectric Phase in SrTiO3 Induced by Terahertz Field",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Hidden Quantum Paraelectric Phase in SrTiO3 Induced by Terahertz Field"
                },
                "summary": "Coherent manipulation of lattice vibrations using ultrafast light pulses\nenables access to nonequilibrium 'hidden' phases with designed functionalities\nin quantum materials. However, expanding the understanding of nonlinear\nlight-phonon interaction mechanisms remains crucial for developing new\nstrategies. Here, we report re-entrant ultrafast phase transitions in SrTiO3\ndriven by intense terahertz excitation. As the terahertz field increases, the\nsystem transitions from the quantum paraelectric (QPE) ground state to an\nintermediate ferroelectric phase, and then unexpectedly reverts to a QPE state\nabove ~500 kV/cm. The latter hidden QPE phase exhibits distinct lattice\ndynamics compared to the initial phases, highlighting activated\nantiferrodistortive phonon modes. Aided by first-principles dynamical\ncalculations, we identify the mechanism for these complex behaviors as a\nsuperposition of multiple coherently excited eigenstates of the polar soft\nmode. Our results reveal a previously uncharted quantum facet of SrTiO3 and\nopen pathways for harnessing high-order excitations to engineer quantum\nmaterials in the ultrafast regime.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Coherent manipulation of lattice vibrations using ultrafast light pulses\nenables access to nonequilibrium 'hidden' phases with designed functionalities\nin quantum materials. However, expanding the understanding of nonlinear\nlight-phonon interaction mechanisms remains crucial for developing new\nstrategies. Here, we report re-entrant ultrafast phase transitions in SrTiO3\ndriven by intense terahertz excitation. As the terahertz field increases, the\nsystem transitions from the quantum paraelectric (QPE) ground state to an\nintermediate ferroelectric phase, and then unexpectedly reverts to a QPE state\nabove ~500 kV/cm. The latter hidden QPE phase exhibits distinct lattice\ndynamics compared to the initial phases, highlighting activated\nantiferrodistortive phonon modes. Aided by first-principles dynamical\ncalculations, we identify the mechanism for these complex behaviors as a\nsuperposition of multiple coherently excited eigenstates of the polar soft\nmode. Our results reveal a previously uncharted quantum facet of SrTiO3 and\nopen pathways for harnessing high-order excitations to engineer quantum\nmaterials in the ultrafast regime."
                },
                "authors": [
                    {
                        "name": "Wei Li"
                    },
                    {
                        "name": "Hanbyul Kim"
                    },
                    {
                        "name": "Xinbo Wang"
                    },
                    {
                        "name": "Jianlin Luo"
                    },
                    {
                        "name": "Simone Latini"
                    },
                    {
                        "name": "Dongbin Shin"
                    },
                    {
                        "name": "Jun-Ming Liu"
                    },
                    {
                        "name": "Jing-Feng Li"
                    },
                    {
                        "name": "Angel Rubio"
                    },
                    {
                        "name": "Ce-Wen Nan"
                    },
                    {
                        "name": "Qian Li"
                    }
                ],
                "author_detail": {
                    "name": "Qian Li"
                },
                "author": "Qian Li",
                "arxiv_comment": "18 pages, 4 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.20887v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.20887v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cond-mat.mtrl-sci",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cond-mat.mtrl-sci",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "physics.optics",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.08545v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.08545v2",
                "updated": "2024-12-30T05:01:44Z",
                "updated_parsed": [
                    2024,
                    12,
                    30,
                    5,
                    1,
                    44,
                    0,
                    365,
                    0
                ],
                "published": "2024-08-16T06:11:21Z",
                "published_parsed": [
                    2024,
                    8,
                    16,
                    6,
                    11,
                    21,
                    4,
                    229,
                    0
                ],
                "title": "SelectLLM: Query-Aware Efficient Selection Algorithm for Large Language\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SelectLLM: Query-Aware Efficient Selection Algorithm for Large Language\n  Models"
                },
                "summary": "Large language models (LLMs) have seen widespread adoption due to their\nremarkable performance across various applications, driving the accelerated\ndevelopment of a large number of diverse LLMs. However, these individual LLMs\nshow limitations in generalization and performance on complex tasks due to\ninherent training biases, model size constraints, and the quality or diversity\nof pre-training datasets. A promising direction is to efficiently harness the\ndiverse capabilities of LLMs to overcome these individual limitations. To\naddress these limitations, we introduce a novel LLM selection algorithm called\nSelectLLM, which efficiently directs input queries to the most suitable subset\nof LLMs from a large pool, ensuring that the selected models collectively\nprovide accurate responses. SelectLLM employs a multi-label classifier and\npolicy based on the classifier's predictions and confidence scores in selecting\nan optimal, query-aware, and lightweight subset of LLMs. Our findings indicate\nthat the proposed model outperforms existing ensemble-based baselines and\nachieves competitive performance with similarly sized top-performing LLMs while\nmaintaining efficiency. Specifically, it achieves a huge reduction in inference\nlatency on two challenging reasoning benchmarks: 13% on GSM8K and 70% on MMLU,\ncompared to the top-performing baselines. Also, we establish a theoretical\nupper bound by an oracle with LLMs and explore in-depth linguistic analysis to\nunderstand the performance gap between Oracle and SelectLLM.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have seen widespread adoption due to their\nremarkable performance across various applications, driving the accelerated\ndevelopment of a large number of diverse LLMs. However, these individual LLMs\nshow limitations in generalization and performance on complex tasks due to\ninherent training biases, model size constraints, and the quality or diversity\nof pre-training datasets. A promising direction is to efficiently harness the\ndiverse capabilities of LLMs to overcome these individual limitations. To\naddress these limitations, we introduce a novel LLM selection algorithm called\nSelectLLM, which efficiently directs input queries to the most suitable subset\nof LLMs from a large pool, ensuring that the selected models collectively\nprovide accurate responses. SelectLLM employs a multi-label classifier and\npolicy based on the classifier's predictions and confidence scores in selecting\nan optimal, query-aware, and lightweight subset of LLMs. Our findings indicate\nthat the proposed model outperforms existing ensemble-based baselines and\nachieves competitive performance with similarly sized top-performing LLMs while\nmaintaining efficiency. Specifically, it achieves a huge reduction in inference\nlatency on two challenging reasoning benchmarks: 13% on GSM8K and 70% on MMLU,\ncompared to the top-performing baselines. Also, we establish a theoretical\nupper bound by an oracle with LLMs and explore in-depth linguistic analysis to\nunderstand the performance gap between Oracle and SelectLLM."
                },
                "authors": [
                    {
                        "name": "Kaushal Kumar Maurya"
                    },
                    {
                        "name": "KV Aditya Srivatsa"
                    },
                    {
                        "name": "Ekaterina Kochmar"
                    }
                ],
                "author_detail": {
                    "name": "Ekaterina Kochmar"
                },
                "author": "Ekaterina Kochmar",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.08545v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.08545v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.20677v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.20677v1",
                "updated": "2024-12-30T03:05:45Z",
                "updated_parsed": [
                    2024,
                    12,
                    30,
                    3,
                    5,
                    45,
                    0,
                    365,
                    0
                ],
                "published": "2024-12-30T03:05:45Z",
                "published_parsed": [
                    2024,
                    12,
                    30,
                    3,
                    5,
                    45,
                    0,
                    365,
                    0
                ],
                "title": "Align Attention Heads Before Merging Them: An Effective Way for\n  Converting MHA to GQA",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Align Attention Heads Before Merging Them: An Effective Way for\n  Converting MHA to GQA"
                },
                "summary": "Large language models have been shown to perform well on a variety of natural\nlanguage processing problems. However, as the model size and the input\nsequence's length increase, the rapid increase of KV Cache significantly slows\ndown inference speed. Therefore GQA model, as an alternative to MHA model, has\nbeen widely introduced into LLMs. In this work, we propose a low-cost method\nfor pruning MHA models into GQA models with any compression ratio of key-value\nheads. Our method is based on $\\mathit{L_0}$ masks to gradually remove\nredundant parameters. In addition, we apply orthogonal transformations to\nattention heads without changing the model to increase similarity between\nattention heads before pruning training, in order to further improve\nperformance of the model. Our method can be compatible with rotary position\nembedding (RoPE), which means the model after training can be fully adapted to\nthe mainstream standard GQA framework. Experiments demonstrate that our\nstrategy can compress up to 87.5% of key-value heads of the LLaMA2-7B model\nwithout too much performance degradation, just achieved through supervised\nfine-tuning.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models have been shown to perform well on a variety of natural\nlanguage processing problems. However, as the model size and the input\nsequence's length increase, the rapid increase of KV Cache significantly slows\ndown inference speed. Therefore GQA model, as an alternative to MHA model, has\nbeen widely introduced into LLMs. In this work, we propose a low-cost method\nfor pruning MHA models into GQA models with any compression ratio of key-value\nheads. Our method is based on $\\mathit{L_0}$ masks to gradually remove\nredundant parameters. In addition, we apply orthogonal transformations to\nattention heads without changing the model to increase similarity between\nattention heads before pruning training, in order to further improve\nperformance of the model. Our method can be compatible with rotary position\nembedding (RoPE), which means the model after training can be fully adapted to\nthe mainstream standard GQA framework. Experiments demonstrate that our\nstrategy can compress up to 87.5% of key-value heads of the LLaMA2-7B model\nwithout too much performance degradation, just achieved through supervised\nfine-tuning."
                },
                "authors": [
                    {
                        "name": "Qingyun Jin"
                    },
                    {
                        "name": "Xiaohui Song"
                    },
                    {
                        "name": "Feng Zhou"
                    },
                    {
                        "name": "Zengchang Qin"
                    }
                ],
                "author_detail": {
                    "name": "Zengchang Qin"
                },
                "author": "Zengchang Qin",
                "arxiv_comment": "12 pages, 4 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.20677v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.20677v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.00068v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.00068v1",
                "updated": "2024-12-29T17:41:40Z",
                "updated_parsed": [
                    2024,
                    12,
                    29,
                    17,
                    41,
                    40,
                    6,
                    364,
                    0
                ],
                "published": "2024-12-29T17:41:40Z",
                "published_parsed": [
                    2024,
                    12,
                    29,
                    17,
                    41,
                    40,
                    6,
                    364,
                    0
                ],
                "title": "Dynamic Optimization of Storage Systems Using Reinforcement Learning\n  Techniques",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Dynamic Optimization of Storage Systems Using Reinforcement Learning\n  Techniques"
                },
                "summary": "The exponential growth of data-intensive applications has placed\nunprecedented demands on modern storage systems, necessitating dynamic and\nefficient optimization strategies. Traditional heuristics employed for storage\nperformance optimization often fail to adapt to the variability and complexity\nof contemporary workloads, leading to significant performance bottlenecks and\nresource inefficiencies. To address these challenges, this paper introduces\nRL-Storage, a novel reinforcement learning (RL)-based framework designed to\ndynamically optimize storage system configurations. RL-Storage leverages deep\nQ-learning algorithms to continuously learn from real-time I/O patterns and\npredict optimal storage parameters, such as cache size, queue depths, and\nreadahead settings[1]. The proposed framework operates within the storage\nkernel, ensuring minimal latency and low computational overhead. Through an\nadaptive feedback mechanism, RL-Storage dynamically adjusts critical\nparameters, achieving efficient resource utilization across a wide range of\nworkloads. Experimental evaluations conducted on a range of benchmarks,\nincluding RocksDB and PostgreSQL, demonstrate significant improvements, with\nthroughput gains of up to 2.6x and latency reductions of 43% compared to\nbaseline heuristics. Additionally, RL-Storage achieves these performance\nenhancements with a negligible CPU overhead of 0.11% and a memory footprint of\nonly 5 KB, making it suitable for seamless deployment in production\nenvironments. This work underscores the transformative potential of\nreinforcement learning techniques in addressing the dynamic nature of modern\nstorage systems. By autonomously adapting to workload variations in real time,\nRL-Storage provides a robust and scalable solution for optimizing storage\nperformance, paving the way for next-generation intelligent storage\ninfrastructures.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The exponential growth of data-intensive applications has placed\nunprecedented demands on modern storage systems, necessitating dynamic and\nefficient optimization strategies. Traditional heuristics employed for storage\nperformance optimization often fail to adapt to the variability and complexity\nof contemporary workloads, leading to significant performance bottlenecks and\nresource inefficiencies. To address these challenges, this paper introduces\nRL-Storage, a novel reinforcement learning (RL)-based framework designed to\ndynamically optimize storage system configurations. RL-Storage leverages deep\nQ-learning algorithms to continuously learn from real-time I/O patterns and\npredict optimal storage parameters, such as cache size, queue depths, and\nreadahead settings[1]. The proposed framework operates within the storage\nkernel, ensuring minimal latency and low computational overhead. Through an\nadaptive feedback mechanism, RL-Storage dynamically adjusts critical\nparameters, achieving efficient resource utilization across a wide range of\nworkloads. Experimental evaluations conducted on a range of benchmarks,\nincluding RocksDB and PostgreSQL, demonstrate significant improvements, with\nthroughput gains of up to 2.6x and latency reductions of 43% compared to\nbaseline heuristics. Additionally, RL-Storage achieves these performance\nenhancements with a negligible CPU overhead of 0.11% and a memory footprint of\nonly 5 KB, making it suitable for seamless deployment in production\nenvironments. This work underscores the transformative potential of\nreinforcement learning techniques in addressing the dynamic nature of modern\nstorage systems. By autonomously adapting to workload variations in real time,\nRL-Storage provides a robust and scalable solution for optimizing storage\nperformance, paving the way for next-generation intelligent storage\ninfrastructures."
                },
                "authors": [
                    {
                        "name": "Chiyu Cheng"
                    },
                    {
                        "name": "Chang Zhou"
                    },
                    {
                        "name": "Yang Zhao"
                    },
                    {
                        "name": "Jin Cao"
                    }
                ],
                "author_detail": {
                    "name": "Jin Cao"
                },
                "author": "Jin Cao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.00068v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.00068v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.OS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.OS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.20524v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.20524v1",
                "updated": "2024-12-29T17:18:21Z",
                "updated_parsed": [
                    2024,
                    12,
                    29,
                    17,
                    18,
                    21,
                    6,
                    364,
                    0
                ],
                "published": "2024-12-29T17:18:21Z",
                "published_parsed": [
                    2024,
                    12,
                    29,
                    17,
                    18,
                    21,
                    6,
                    364,
                    0
                ],
                "title": "Ns3 meets Sionna: Using Realistic Channels in Network Simulation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Ns3 meets Sionna: Using Realistic Channels in Network Simulation"
                },
                "summary": "Network simulators are indispensable tools for the advancement of wireless\nnetwork technologies, offering a cost-effective and controlled environment to\nsimulate real-world network behavior. However, traditional simulators, such as\nthe widely used ns-3, exhibit limitations in accurately modeling indoor and\noutdoor scenarios due to their reliance on simplified statistical and\nstochastic channel propagation models, which often fail to accurately capture\nphysical phenomena like multipath signal propagation and shadowing by obstacles\nin the line-of-sight path. We present Ns3Sionna, which integrates a ray\ntracing-based channel model, implemented using the Sionna RT framework, within\nthe ns-3 network simulator. It allows to simulate environment-specific and\nphysically accurate channel realizations for a given 3D scene and wireless\ndevice positions. Additionally, a mobility model based on ray tracing was\ndeveloped to accurately represent device movements within the simulated 3D\nspace. Ns3Sionna provides more realistic path and delay loss estimates for both\nindoor and outdoor environments than existing ns-3 propagation models,\nparticularly in terms of spatial and temporal correlation. Moreover,\nfine-grained channel state information is provided, which could be used for the\ndevelopment of sensing applications. Due to the significant computational\ndemands of ray tracing, Ns3Sionna takes advantage of the parallel execution\ncapabilities of modern GPUs and multi-core CPUs by incorporating intelligent\npre-caching mechanisms that leverage the channel's coherence time to optimize\nruntime performance. This enables the efficient simulation of scenarios with a\nsmall to medium number of mobile nodes.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Network simulators are indispensable tools for the advancement of wireless\nnetwork technologies, offering a cost-effective and controlled environment to\nsimulate real-world network behavior. However, traditional simulators, such as\nthe widely used ns-3, exhibit limitations in accurately modeling indoor and\noutdoor scenarios due to their reliance on simplified statistical and\nstochastic channel propagation models, which often fail to accurately capture\nphysical phenomena like multipath signal propagation and shadowing by obstacles\nin the line-of-sight path. We present Ns3Sionna, which integrates a ray\ntracing-based channel model, implemented using the Sionna RT framework, within\nthe ns-3 network simulator. It allows to simulate environment-specific and\nphysically accurate channel realizations for a given 3D scene and wireless\ndevice positions. Additionally, a mobility model based on ray tracing was\ndeveloped to accurately represent device movements within the simulated 3D\nspace. Ns3Sionna provides more realistic path and delay loss estimates for both\nindoor and outdoor environments than existing ns-3 propagation models,\nparticularly in terms of spatial and temporal correlation. Moreover,\nfine-grained channel state information is provided, which could be used for the\ndevelopment of sensing applications. Due to the significant computational\ndemands of ray tracing, Ns3Sionna takes advantage of the parallel execution\ncapabilities of modern GPUs and multi-core CPUs by incorporating intelligent\npre-caching mechanisms that leverage the channel's coherence time to optimize\nruntime performance. This enables the efficient simulation of scenarios with a\nsmall to medium number of mobile nodes."
                },
                "authors": [
                    {
                        "name": "Anatolij Zubow"
                    },
                    {
                        "name": "Yannik Pilz"
                    },
                    {
                        "name": "Sascha Rösler"
                    },
                    {
                        "name": "Falko Dressler"
                    }
                ],
                "author_detail": {
                    "name": "Falko Dressler"
                },
                "author": "Falko Dressler",
                "arxiv_comment": "9 pages, 11 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.20524v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.20524v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.20221v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.20221v1",
                "updated": "2024-12-28T17:17:03Z",
                "updated_parsed": [
                    2024,
                    12,
                    28,
                    17,
                    17,
                    3,
                    5,
                    363,
                    0
                ],
                "published": "2024-12-28T17:17:03Z",
                "published_parsed": [
                    2024,
                    12,
                    28,
                    17,
                    17,
                    3,
                    5,
                    363,
                    0
                ],
                "title": "Revisiting Cache Freshness for Emerging Real-Time Applications",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Revisiting Cache Freshness for Emerging Real-Time Applications"
                },
                "summary": "Caching is widely used in industry to improve application performance by\nreducing data-access latency and taking the load off the backend\ninfrastructure. TTLs have become the de-facto mechanism used to keep cached\ndata reasonably fresh (i.e., not too out of date with the backend). However,\nthe emergence of real-time applications requires tighter data freshness, which\nis impractical to achieve with TTLs. We discuss why this is the case, and\npropose a simple yet effective adaptive policy to achieve the desired\nfreshness.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Caching is widely used in industry to improve application performance by\nreducing data-access latency and taking the load off the backend\ninfrastructure. TTLs have become the de-facto mechanism used to keep cached\ndata reasonably fresh (i.e., not too out of date with the backend). However,\nthe emergence of real-time applications requires tighter data freshness, which\nis impractical to achieve with TTLs. We discuss why this is the case, and\npropose a simple yet effective adaptive policy to achieve the desired\nfreshness."
                },
                "authors": [
                    {
                        "name": "Ziming Mao"
                    },
                    {
                        "name": "Rishabh Iyer"
                    },
                    {
                        "name": "Scott Shenker"
                    },
                    {
                        "name": "Ion Stoica"
                    }
                ],
                "author_detail": {
                    "name": "Ion Stoica"
                },
                "author": "Ion Stoica",
                "arxiv_doi": "10.1145/3696348.3696858",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3696348.3696858",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2412.20221v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.20221v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "HotNets '24",
                "arxiv_primary_category": {
                    "term": "cs.OS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.OS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.20105v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.20105v1",
                "updated": "2024-12-28T10:17:29Z",
                "updated_parsed": [
                    2024,
                    12,
                    28,
                    10,
                    17,
                    29,
                    5,
                    363,
                    0
                ],
                "published": "2024-12-28T10:17:29Z",
                "published_parsed": [
                    2024,
                    12,
                    28,
                    10,
                    17,
                    29,
                    5,
                    363,
                    0
                ],
                "title": "ST$^3$: Accelerating Multimodal Large Language Model by Spatial-Temporal\n  Visual Token Trimming",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ST$^3$: Accelerating Multimodal Large Language Model by Spatial-Temporal\n  Visual Token Trimming"
                },
                "summary": "Multimodal large language models (MLLMs) enhance their perceptual\ncapabilities by integrating visual and textual information. However, processing\nthe massive number of visual tokens incurs a significant computational cost.\nExisting analysis of the MLLM attention mechanisms remains shallow, leading to\ncoarse-grain token pruning strategies that fail to effectively balance speed\nand accuracy. In this paper, we conduct a comprehensive investigation of MLLM\nattention mechanisms with LLaVA. We find that numerous visual tokens and\npartial attention computations are redundant during the decoding process. Based\non this insight, we propose Spatial-Temporal Visual Token Trimming\n($\\textbf{ST}^{3}$), a framework designed to accelerate MLLM inference without\nretraining. $\\textbf{ST}^{3}$ consists of two primary components: 1)\nProgressive Visual Token Pruning (\\textbf{PVTP}), which eliminates inattentive\nvisual tokens across layers, and 2) Visual Token Annealing (\\textbf{VTA}),\nwhich dynamically reduces the number of visual tokens in each layer as the\ngenerated tokens grow. Together, these techniques deliver around\n$\\mathbf{2\\times}$ faster inference with only about $\\mathbf{30\\%}$ KV cache\nmemory compared to the original LLaVA, while maintaining consistent performance\nacross various datasets. Crucially, $\\textbf{ST}^{3}$ can be seamlessly\nintegrated into existing pre-trained MLLMs, providing a plug-and-play solution\nfor efficient inference.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multimodal large language models (MLLMs) enhance their perceptual\ncapabilities by integrating visual and textual information. However, processing\nthe massive number of visual tokens incurs a significant computational cost.\nExisting analysis of the MLLM attention mechanisms remains shallow, leading to\ncoarse-grain token pruning strategies that fail to effectively balance speed\nand accuracy. In this paper, we conduct a comprehensive investigation of MLLM\nattention mechanisms with LLaVA. We find that numerous visual tokens and\npartial attention computations are redundant during the decoding process. Based\non this insight, we propose Spatial-Temporal Visual Token Trimming\n($\\textbf{ST}^{3}$), a framework designed to accelerate MLLM inference without\nretraining. $\\textbf{ST}^{3}$ consists of two primary components: 1)\nProgressive Visual Token Pruning (\\textbf{PVTP}), which eliminates inattentive\nvisual tokens across layers, and 2) Visual Token Annealing (\\textbf{VTA}),\nwhich dynamically reduces the number of visual tokens in each layer as the\ngenerated tokens grow. Together, these techniques deliver around\n$\\mathbf{2\\times}$ faster inference with only about $\\mathbf{30\\%}$ KV cache\nmemory compared to the original LLaVA, while maintaining consistent performance\nacross various datasets. Crucially, $\\textbf{ST}^{3}$ can be seamlessly\nintegrated into existing pre-trained MLLMs, providing a plug-and-play solution\nfor efficient inference."
                },
                "authors": [
                    {
                        "name": "Jiedong Zhuang"
                    },
                    {
                        "name": "Lu Lu"
                    },
                    {
                        "name": "Ming Dai"
                    },
                    {
                        "name": "Rui Hu"
                    },
                    {
                        "name": "Jian Chen"
                    },
                    {
                        "name": "Qiang Liu"
                    },
                    {
                        "name": "Haoji Hu"
                    }
                ],
                "author_detail": {
                    "name": "Haoji Hu"
                },
                "author": "Haoji Hu",
                "arxiv_comment": "Accepted to AAAI2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.20105v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.20105v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.19991v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.19991v1",
                "updated": "2024-12-28T03:28:52Z",
                "updated_parsed": [
                    2024,
                    12,
                    28,
                    3,
                    28,
                    52,
                    5,
                    363,
                    0
                ],
                "published": "2024-12-28T03:28:52Z",
                "published_parsed": [
                    2024,
                    12,
                    28,
                    3,
                    28,
                    52,
                    5,
                    363,
                    0
                ],
                "title": "A Robust Federated Learning Framework for Undependable Devices at Scale",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Robust Federated Learning Framework for Undependable Devices at Scale"
                },
                "summary": "In a federated learning (FL) system, many devices, such as smartphones, are\noften undependable (e.g., frequently disconnected from WiFi) during training.\nExisting FL frameworks always assume a dependable environment and exclude\nundependable devices from training, leading to poor model performance and\nresource wastage. In this paper, we propose FLUDE to effectively deal with\nundependable environments. First, FLUDE assesses the dependability of devices\nbased on the probability distribution of their historical behaviors (e.g., the\nlikelihood of successfully completing training). Based on this assessment,\nFLUDE adaptively selects devices with high dependability for training. To\nmitigate resource wastage during the training phase, FLUDE maintains a model\ncache on each device, aiming to preserve the latest training state for later\nuse in case local training on an undependable device is interrupted. Moreover,\nFLUDE proposes a staleness-aware strategy to judiciously distribute the global\nmodel to a subset of devices, thus significantly reducing resource wastage\nwhile maintaining model performance. We have implemented FLUDE on two physical\nplatforms with 120 smartphones and NVIDIA Jetson devices. Extensive\nexperimental results demonstrate that FLUDE can effectively improve model\nperformance and resource efficiency of FL training in undependable\nenvironments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In a federated learning (FL) system, many devices, such as smartphones, are\noften undependable (e.g., frequently disconnected from WiFi) during training.\nExisting FL frameworks always assume a dependable environment and exclude\nundependable devices from training, leading to poor model performance and\nresource wastage. In this paper, we propose FLUDE to effectively deal with\nundependable environments. First, FLUDE assesses the dependability of devices\nbased on the probability distribution of their historical behaviors (e.g., the\nlikelihood of successfully completing training). Based on this assessment,\nFLUDE adaptively selects devices with high dependability for training. To\nmitigate resource wastage during the training phase, FLUDE maintains a model\ncache on each device, aiming to preserve the latest training state for later\nuse in case local training on an undependable device is interrupted. Moreover,\nFLUDE proposes a staleness-aware strategy to judiciously distribute the global\nmodel to a subset of devices, thus significantly reducing resource wastage\nwhile maintaining model performance. We have implemented FLUDE on two physical\nplatforms with 120 smartphones and NVIDIA Jetson devices. Extensive\nexperimental results demonstrate that FLUDE can effectively improve model\nperformance and resource efficiency of FL training in undependable\nenvironments."
                },
                "authors": [
                    {
                        "name": "Shilong Wang"
                    },
                    {
                        "name": "Jianchun Liu"
                    },
                    {
                        "name": "Hongli Xu"
                    },
                    {
                        "name": "Chunming Qiao"
                    },
                    {
                        "name": "Huarong Deng"
                    },
                    {
                        "name": "Qiuye Zheng"
                    },
                    {
                        "name": "Jiantao Gong"
                    }
                ],
                "author_detail": {
                    "name": "Jiantao Gong"
                },
                "author": "Jiantao Gong",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.19991v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.19991v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.19051v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.19051v1",
                "updated": "2024-12-26T04:13:52Z",
                "updated_parsed": [
                    2024,
                    12,
                    26,
                    4,
                    13,
                    52,
                    3,
                    361,
                    0
                ],
                "published": "2024-12-26T04:13:52Z",
                "published_parsed": [
                    2024,
                    12,
                    26,
                    4,
                    13,
                    52,
                    3,
                    361,
                    0
                ],
                "title": "Performance Characterization and Optimizations of Traditional ML\n  Applications",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Performance Characterization and Optimizations of Traditional ML\n  Applications"
                },
                "summary": "Even in the era of Deep Learning based methods, traditional machine learning\nmethods with large data sets continue to attract significant attention.\nHowever, we find an apparent lack of a detailed performance characterization of\nthese methods in the context of large training datasets. In this work, we study\nthe system's behavior of a number of traditional ML methods as implemented in\npopular free software libraries/modules to identify critical performance\nbottlenecks experienced by these applications. The performance characterization\nstudy reveals several interesting insights on the performance of these\napplications. Then we evaluate the performance benefits of applying some\nwell-known optimizations at the levels of caches and the main memory. More\nspecifically, we test the usefulness of optimizations such as (i) software\nprefetching to improve cache performance and (ii) data layout and computation\nreordering optimizations to improve locality in DRAM accesses. These\noptimizations are implemented as modifications to the well-known scikit-learn\nlibrary, and hence can be easily leveraged by application programmers. We\nevaluate the impact of the proposed optimizations using a combination of\nsimulation and execution on a real system. The software prefetching\noptimization results in performance benefits varying from 5.2%-27.1% on\ndifferent ML applications while the data layout and computation reordering\napproaches yield 6.16%-28.0% performance improvement.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Even in the era of Deep Learning based methods, traditional machine learning\nmethods with large data sets continue to attract significant attention.\nHowever, we find an apparent lack of a detailed performance characterization of\nthese methods in the context of large training datasets. In this work, we study\nthe system's behavior of a number of traditional ML methods as implemented in\npopular free software libraries/modules to identify critical performance\nbottlenecks experienced by these applications. The performance characterization\nstudy reveals several interesting insights on the performance of these\napplications. Then we evaluate the performance benefits of applying some\nwell-known optimizations at the levels of caches and the main memory. More\nspecifically, we test the usefulness of optimizations such as (i) software\nprefetching to improve cache performance and (ii) data layout and computation\nreordering optimizations to improve locality in DRAM accesses. These\noptimizations are implemented as modifications to the well-known scikit-learn\nlibrary, and hence can be easily leveraged by application programmers. We\nevaluate the impact of the proposed optimizations using a combination of\nsimulation and execution on a real system. The software prefetching\noptimization results in performance benefits varying from 5.2%-27.1% on\ndifferent ML applications while the data layout and computation reordering\napproaches yield 6.16%-28.0% performance improvement."
                },
                "authors": [
                    {
                        "name": "Harsh Kumar"
                    },
                    {
                        "name": "R. Govindarajan"
                    }
                ],
                "author_detail": {
                    "name": "R. Govindarajan"
                },
                "author": "R. Govindarajan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.19051v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.19051v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.PF",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.18960v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.18960v1",
                "updated": "2024-12-25T18:36:21Z",
                "updated_parsed": [
                    2024,
                    12,
                    25,
                    18,
                    36,
                    21,
                    2,
                    360,
                    0
                ],
                "published": "2024-12-25T18:36:21Z",
                "published_parsed": [
                    2024,
                    12,
                    25,
                    18,
                    36,
                    21,
                    2,
                    360,
                    0
                ],
                "title": "XRFlux: Virtual Reality Benchmark for Edge Caching Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "XRFlux: Virtual Reality Benchmark for Edge Caching Systems"
                },
                "summary": "We introduce a Unity based benchmark XRFlux for evaluating Virtual Reality\n(VR) delivery systems using edge-cloud caching. As VR applications and systems\nprogress, the need to meet strict latency and Quality of Experience (QoE)\nrequirements is increasingly evident. In the context of VR, traditional cloud\narchitectures (e.g., remote AWS S3 for content delivery) often struggle to meet\nthese demands, especially for users of the same application in different\nlocations. With edge computing, resources are brought closer to users in\nefforts to reduce latency and improve QoEs. However, VR's dynamic nature, with\nchanging fields of view (FoVs) and user synchronization requirements, creates\nvarious challenges for edge caching. We address the lack of suitable benchmarks\nand propose a framework that simulates multiuser VR scenarios while logging\nusers' interaction with objects within their actual and predicted FoVs. The\nbenchmark's activity log can then be played back through an edge cache to\nassess the resulting QoEs. This tool fills a gap by supporting research in the\noptimization of edge caching (and other edge-cloud functions) for VR streaming.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce a Unity based benchmark XRFlux for evaluating Virtual Reality\n(VR) delivery systems using edge-cloud caching. As VR applications and systems\nprogress, the need to meet strict latency and Quality of Experience (QoE)\nrequirements is increasingly evident. In the context of VR, traditional cloud\narchitectures (e.g., remote AWS S3 for content delivery) often struggle to meet\nthese demands, especially for users of the same application in different\nlocations. With edge computing, resources are brought closer to users in\nefforts to reduce latency and improve QoEs. However, VR's dynamic nature, with\nchanging fields of view (FoVs) and user synchronization requirements, creates\nvarious challenges for edge caching. We address the lack of suitable benchmarks\nand propose a framework that simulates multiuser VR scenarios while logging\nusers' interaction with objects within their actual and predicted FoVs. The\nbenchmark's activity log can then be played back through an edge cache to\nassess the resulting QoEs. This tool fills a gap by supporting research in the\noptimization of edge caching (and other edge-cloud functions) for VR streaming."
                },
                "authors": [
                    {
                        "name": "Nader Alfares"
                    },
                    {
                        "name": "George Kesidis"
                    }
                ],
                "author_detail": {
                    "name": "George Kesidis"
                },
                "author": "George Kesidis",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.18960v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.18960v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.PF",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.18914v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.18914v1",
                "updated": "2024-12-25T14:14:31Z",
                "updated_parsed": [
                    2024,
                    12,
                    25,
                    14,
                    14,
                    31,
                    2,
                    360,
                    0
                ],
                "published": "2024-12-25T14:14:31Z",
                "published_parsed": [
                    2024,
                    12,
                    25,
                    14,
                    14,
                    31,
                    2,
                    360,
                    0
                ],
                "title": "Long-Range Tasks Using Short-Context LLMs: Incremental Reasoning With\n  Structured Memories",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Long-Range Tasks Using Short-Context LLMs: Incremental Reasoning With\n  Structured Memories"
                },
                "summary": "Long-range tasks require reasoning over long inputs. Existing solutions\neither need large compute budgets, training data, access to model weights, or\nuse complex, task-specific approaches. We present PRISM, which alleviates these\nconcerns by processing information as a stream of chunks, maintaining a\nstructured in-context memory specified by a typed hierarchy schema. This\napproach demonstrates superior performance to baselines on diverse tasks while\nusing at least 4x smaller contexts than long-context models. Moreover, PRISM is\ntoken-efficient. By producing short outputs and efficiently leveraging\nkey-value (KV) caches, it achieves up to 54% cost reduction when compared to\nalternative short-context approaches. The method also scales down to tiny\ninformation chunks (e.g., 500 tokens) without increasing the number of tokens\nencoded or sacrificing quality. Furthermore, we show that it is possible to\ngenerate schemas to generalize our approach to new tasks with minimal effort.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Long-range tasks require reasoning over long inputs. Existing solutions\neither need large compute budgets, training data, access to model weights, or\nuse complex, task-specific approaches. We present PRISM, which alleviates these\nconcerns by processing information as a stream of chunks, maintaining a\nstructured in-context memory specified by a typed hierarchy schema. This\napproach demonstrates superior performance to baselines on diverse tasks while\nusing at least 4x smaller contexts than long-context models. Moreover, PRISM is\ntoken-efficient. By producing short outputs and efficiently leveraging\nkey-value (KV) caches, it achieves up to 54% cost reduction when compared to\nalternative short-context approaches. The method also scales down to tiny\ninformation chunks (e.g., 500 tokens) without increasing the number of tokens\nencoded or sacrificing quality. Furthermore, we show that it is possible to\ngenerate schemas to generalize our approach to new tasks with minimal effort."
                },
                "authors": [
                    {
                        "name": "Dulhan Jayalath"
                    },
                    {
                        "name": "James Bradley Wendt"
                    },
                    {
                        "name": "Nicholas Monath"
                    },
                    {
                        "name": "Sandeep Tata"
                    },
                    {
                        "name": "Beliz Gunel"
                    }
                ],
                "author_detail": {
                    "name": "Beliz Gunel"
                },
                "author": "Beliz Gunel",
                "arxiv_comment": "23 pages, 7 figures, 5 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.18914v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.18914v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.18911v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.18911v1",
                "updated": "2024-12-25T14:00:14Z",
                "updated_parsed": [
                    2024,
                    12,
                    25,
                    14,
                    0,
                    14,
                    2,
                    360,
                    0
                ],
                "published": "2024-12-25T14:00:14Z",
                "published_parsed": [
                    2024,
                    12,
                    25,
                    14,
                    0,
                    14,
                    2,
                    360,
                    0
                ],
                "title": "Accelerating Diffusion Transformers with Dual Feature Caching",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Accelerating Diffusion Transformers with Dual Feature Caching"
                },
                "summary": "Diffusion Transformers (DiT) have become the dominant methods in image and\nvideo generation yet still suffer substantial computational costs. As an\neffective approach for DiT acceleration, feature caching methods are designed\nto cache the features of DiT in previous timesteps and reuse them in the next\ntimesteps, allowing us to skip the computation in the next timesteps. However,\non the one hand, aggressively reusing all the features cached in previous\ntimesteps leads to a severe drop in generation quality. On the other hand,\nconservatively caching only the features in the redundant layers or tokens but\nstill computing the important ones successfully preserves the generation\nquality but results in reductions in acceleration ratios. Observing such a\ntradeoff between generation quality and acceleration performance, this paper\nbegins by quantitatively studying the accumulated error from cached features.\nSurprisingly, we find that aggressive caching does not introduce significantly\nmore caching errors in the caching step, and the conservative feature caching\ncan fix the error introduced by aggressive caching. Thereby, we propose a dual\ncaching strategy that adopts aggressive and conservative caching iteratively,\nleading to significant acceleration and high generation quality at the same\ntime. Besides, we further introduce a V-caching strategy for token-wise\nconservative caching, which is compatible with flash attention and requires no\ntraining and calibration data.\n  Our codes have been released in Github: \\textbf{Code:\n\\href{https://github.com/Shenyi-Z/DuCa}{\\texttt{\\textcolor{cyan}{https://github.com/Shenyi-Z/DuCa}}}}",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion Transformers (DiT) have become the dominant methods in image and\nvideo generation yet still suffer substantial computational costs. As an\neffective approach for DiT acceleration, feature caching methods are designed\nto cache the features of DiT in previous timesteps and reuse them in the next\ntimesteps, allowing us to skip the computation in the next timesteps. However,\non the one hand, aggressively reusing all the features cached in previous\ntimesteps leads to a severe drop in generation quality. On the other hand,\nconservatively caching only the features in the redundant layers or tokens but\nstill computing the important ones successfully preserves the generation\nquality but results in reductions in acceleration ratios. Observing such a\ntradeoff between generation quality and acceleration performance, this paper\nbegins by quantitatively studying the accumulated error from cached features.\nSurprisingly, we find that aggressive caching does not introduce significantly\nmore caching errors in the caching step, and the conservative feature caching\ncan fix the error introduced by aggressive caching. Thereby, we propose a dual\ncaching strategy that adopts aggressive and conservative caching iteratively,\nleading to significant acceleration and high generation quality at the same\ntime. Besides, we further introduce a V-caching strategy for token-wise\nconservative caching, which is compatible with flash attention and requires no\ntraining and calibration data.\n  Our codes have been released in Github: \\textbf{Code:\n\\href{https://github.com/Shenyi-Z/DuCa}{\\texttt{\\textcolor{cyan}{https://github.com/Shenyi-Z/DuCa}}}}"
                },
                "authors": [
                    {
                        "name": "Chang Zou"
                    },
                    {
                        "name": "Evelyn Zhang"
                    },
                    {
                        "name": "Runlin Guo"
                    },
                    {
                        "name": "Haohang Xu"
                    },
                    {
                        "name": "Conghui He"
                    },
                    {
                        "name": "Xuming Hu"
                    },
                    {
                        "name": "Linfeng Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Linfeng Zhang"
                },
                "author": "Linfeng Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.18911v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.18911v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.18885v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.18885v1",
                "updated": "2024-12-25T11:59:17Z",
                "updated_parsed": [
                    2024,
                    12,
                    25,
                    11,
                    59,
                    17,
                    2,
                    360,
                    0
                ],
                "published": "2024-12-25T11:59:17Z",
                "published_parsed": [
                    2024,
                    12,
                    25,
                    11,
                    59,
                    17,
                    2,
                    360,
                    0
                ],
                "title": "Aspect-oriented Programming with Julia",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Aspect-oriented Programming with Julia"
                },
                "summary": "This paper proposes integrating Aspect-oriented Programming (AOP) into Julia,\na language widely used in scientific and High-Performance Computing (HPC). AOP\nenhances software modularity by encapsulating cross-cutting concerns, such as\nlogging, caching, and parallelizing, into separate, reusable aspects.\nLeveraging Julia's powerful metaprogramming and abstract syntax tree (AST)\nmanipulation capabilities, we introduce AspectJulia, an AOP framework designed\nto operate within Julia's runtime environment as a package. AspectJulia enables\ndevelopers to define and apply aspects seamlessly, leading to more modular,\nmaintainable, and adaptable code. We detail the implementation of AspectJulia\nand present diverse use cases, ranging from HPC and scientific computing to\nbusiness applications, demonstrating its effectiveness in managing\ncross-cutting concerns. This integration simplifies application development and\nimproves the adaptability of existing Julia modules and packages, paving the\nway for more efficient and maintainable software systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper proposes integrating Aspect-oriented Programming (AOP) into Julia,\na language widely used in scientific and High-Performance Computing (HPC). AOP\nenhances software modularity by encapsulating cross-cutting concerns, such as\nlogging, caching, and parallelizing, into separate, reusable aspects.\nLeveraging Julia's powerful metaprogramming and abstract syntax tree (AST)\nmanipulation capabilities, we introduce AspectJulia, an AOP framework designed\nto operate within Julia's runtime environment as a package. AspectJulia enables\ndevelopers to define and apply aspects seamlessly, leading to more modular,\nmaintainable, and adaptable code. We detail the implementation of AspectJulia\nand present diverse use cases, ranging from HPC and scientific computing to\nbusiness applications, demonstrating its effectiveness in managing\ncross-cutting concerns. This integration simplifies application development and\nimproves the adaptability of existing Julia modules and packages, paving the\nway for more efficient and maintainable software systems."
                },
                "authors": [
                    {
                        "name": "Osamu Ishimura"
                    },
                    {
                        "name": "Yoshihide Yoshimoto"
                    }
                ],
                "author_detail": {
                    "name": "Yoshihide Yoshimoto"
                },
                "author": "Yoshihide Yoshimoto",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.18885v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.18885v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.PL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.PL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.05460v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.05460v1",
                "updated": "2024-12-25T10:11:31Z",
                "updated_parsed": [
                    2024,
                    12,
                    25,
                    10,
                    11,
                    31,
                    2,
                    360,
                    0
                ],
                "published": "2024-12-25T10:11:31Z",
                "published_parsed": [
                    2024,
                    12,
                    25,
                    10,
                    11,
                    31,
                    2,
                    360,
                    0
                ],
                "title": "Efficiently serving large multimedia models using EPD Disaggregation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficiently serving large multimedia models using EPD Disaggregation"
                },
                "summary": "Large Multimodal Models (LMMs) extend Large Language Models (LLMs) by\nhandling diverse inputs such as images, audio, and video, but at the cost of\nadding a multimodal encoding stage that increases both computational and memory\noverhead. This step helps convert raw inputs into tokenized representations\nthat inflate the token sequence for the prefill phase, negatively impacting key\nService Level Objectives (SLOs) like time to first token (TTFT) and end-to-end\nthroughput. We introduce Encode-Prefill-Decode (EPD) Disaggregation, a novel\nframework that separates the encoding, prefill, and decode stages onto\ndedicated resources. Unlike current systems, which bundle encoding and prefill\ntogether, our disaggregation approach alleviates memory bottlenecks, mitigates\nsynchronization delays, and supports flexible batching. Specifically, we employ\na new caching mechanism for multimodal tokens, enabling asynchronous transfer\nof multimodal tokens and introduce an integrated module to find optimal config\nfor EPD system and minimize resource usage while maximizing SLO-based\nperformance metric. Experimental evaluations with popular LMMs show substantial\ngains in memory efficiency (up to 15$\\times$ lesser for encoding-stage GPUs),\nthat supports upto 22$\\times$ higher batch sizes, 10$\\times$ more number of\nimages/ request, 2.2$\\times$ higher kv cache size. Further, it leads to\nsignificant improvements in end-to-end throughput (up to 57\\% better), and\nlatency metrics (TTFT up to 71\\% lower), compared to systems that do not\ndisaggregate. Our findings underscore the potential of EPD disaggregation to\nenable resource-efficient and high-performance multimodal inference at scale.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Multimodal Models (LMMs) extend Large Language Models (LLMs) by\nhandling diverse inputs such as images, audio, and video, but at the cost of\nadding a multimodal encoding stage that increases both computational and memory\noverhead. This step helps convert raw inputs into tokenized representations\nthat inflate the token sequence for the prefill phase, negatively impacting key\nService Level Objectives (SLOs) like time to first token (TTFT) and end-to-end\nthroughput. We introduce Encode-Prefill-Decode (EPD) Disaggregation, a novel\nframework that separates the encoding, prefill, and decode stages onto\ndedicated resources. Unlike current systems, which bundle encoding and prefill\ntogether, our disaggregation approach alleviates memory bottlenecks, mitigates\nsynchronization delays, and supports flexible batching. Specifically, we employ\na new caching mechanism for multimodal tokens, enabling asynchronous transfer\nof multimodal tokens and introduce an integrated module to find optimal config\nfor EPD system and minimize resource usage while maximizing SLO-based\nperformance metric. Experimental evaluations with popular LMMs show substantial\ngains in memory efficiency (up to 15$\\times$ lesser for encoding-stage GPUs),\nthat supports upto 22$\\times$ higher batch sizes, 10$\\times$ more number of\nimages/ request, 2.2$\\times$ higher kv cache size. Further, it leads to\nsignificant improvements in end-to-end throughput (up to 57\\% better), and\nlatency metrics (TTFT up to 71\\% lower), compared to systems that do not\ndisaggregate. Our findings underscore the potential of EPD disaggregation to\nenable resource-efficient and high-performance multimodal inference at scale."
                },
                "authors": [
                    {
                        "name": "Gursimran Singh"
                    },
                    {
                        "name": "Xinglu Wang"
                    },
                    {
                        "name": "Ivan Hu"
                    },
                    {
                        "name": "Timothy Yu"
                    },
                    {
                        "name": "Linzi Xing"
                    },
                    {
                        "name": "Wei Jiang"
                    },
                    {
                        "name": "Zhefeng Wang"
                    },
                    {
                        "name": "Xiaolong Bai"
                    },
                    {
                        "name": "Yi Li"
                    },
                    {
                        "name": "Ying Xiong"
                    },
                    {
                        "name": "Yong Zhang"
                    },
                    {
                        "name": "Zhenan Fan"
                    }
                ],
                "author_detail": {
                    "name": "Zhenan Fan"
                },
                "author": "Zhenan Fan",
                "arxiv_comment": "13 pages, 6 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.05460v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.05460v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.16187v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.16187v2",
                "updated": "2024-12-24T13:04:45Z",
                "updated_parsed": [
                    2024,
                    12,
                    24,
                    13,
                    4,
                    45,
                    1,
                    359,
                    0
                ],
                "published": "2024-12-13T06:00:27Z",
                "published_parsed": [
                    2024,
                    12,
                    13,
                    6,
                    0,
                    27,
                    4,
                    348,
                    0
                ],
                "title": "HashEvict: A Pre-Attention KV Cache Eviction Strategy using\n  Locality-Sensitive Hashing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "HashEvict: A Pre-Attention KV Cache Eviction Strategy using\n  Locality-Sensitive Hashing"
                },
                "summary": "Transformer-based large language models (LLMs) use the key-value (KV) cache\nto significantly accelerate inference by storing the key and value embeddings\nof past tokens. However, this cache consumes significant GPU memory. In this\nwork, we introduce HashEvict, an algorithm that uses locality-sensitive hashing\n(LSH) to compress the KV cache. HashEvict quickly locates tokens in the cache\nthat are cosine dissimilar to the current query token. This is achieved by\ncomputing the Hamming distance between binarized Gaussian projections of the\ncurrent token query and cached token keys, with a projection length much\nsmaller than the embedding dimension. We maintain a lightweight binary\nstructure in GPU memory to facilitate these calculations. Unlike existing\ncompression strategies that compute attention to determine token retention,\nHashEvict makes these decisions pre-attention, thereby reducing computational\ncosts. Additionally, HashEvict is dynamic - at every decoding step, the key and\nvalue of the current token replace the embeddings of a token expected to\nproduce the lowest attention score. We demonstrate that HashEvict can compress\nthe KV cache by 30%-70% while maintaining high performance across reasoning,\nmultiple-choice, long-context retrieval and summarization tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Transformer-based large language models (LLMs) use the key-value (KV) cache\nto significantly accelerate inference by storing the key and value embeddings\nof past tokens. However, this cache consumes significant GPU memory. In this\nwork, we introduce HashEvict, an algorithm that uses locality-sensitive hashing\n(LSH) to compress the KV cache. HashEvict quickly locates tokens in the cache\nthat are cosine dissimilar to the current query token. This is achieved by\ncomputing the Hamming distance between binarized Gaussian projections of the\ncurrent token query and cached token keys, with a projection length much\nsmaller than the embedding dimension. We maintain a lightweight binary\nstructure in GPU memory to facilitate these calculations. Unlike existing\ncompression strategies that compute attention to determine token retention,\nHashEvict makes these decisions pre-attention, thereby reducing computational\ncosts. Additionally, HashEvict is dynamic - at every decoding step, the key and\nvalue of the current token replace the embeddings of a token expected to\nproduce the lowest attention score. We demonstrate that HashEvict can compress\nthe KV cache by 30%-70% while maintaining high performance across reasoning,\nmultiple-choice, long-context retrieval and summarization tasks."
                },
                "authors": [
                    {
                        "name": "Minghui Liu"
                    },
                    {
                        "name": "Tahseen Rabbani"
                    },
                    {
                        "name": "Tony O'Halloran"
                    },
                    {
                        "name": "Ananth Sankaralingam"
                    },
                    {
                        "name": "Mary-Anne Hartley"
                    },
                    {
                        "name": "Brian Gravelle"
                    },
                    {
                        "name": "Furong Huang"
                    },
                    {
                        "name": "Cornelia Fermüller"
                    },
                    {
                        "name": "Yiannis Aloimonos"
                    }
                ],
                "author_detail": {
                    "name": "Yiannis Aloimonos"
                },
                "author": "Yiannis Aloimonos",
                "arxiv_comment": "10 pages, 6 figures, 2 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.16187v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.16187v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.01959v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.01959v2",
                "updated": "2024-12-24T00:46:00Z",
                "updated_parsed": [
                    2024,
                    12,
                    24,
                    0,
                    46,
                    0,
                    1,
                    359,
                    0
                ],
                "published": "2024-12-02T20:39:56Z",
                "published_parsed": [
                    2024,
                    12,
                    2,
                    20,
                    39,
                    56,
                    0,
                    337,
                    0
                ],
                "title": "Development and Application of a Decentralized Domain Name Service",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Development and Application of a Decentralized Domain Name Service"
                },
                "summary": "The current Domain Name System (DNS), as a core infrastructure of the\ninternet, exhibits several shortcomings: its centralized architecture leads to\ncensorship risks and single points of failure, making domain name resolution\nvulnerable to attacks. The lack of encryption in the resolution process exposes\nit to DNS hijacking and cache poisoning attacks. Additionally, the high\noperational costs limit participation and innovation among small to\nmedium-sized users. To address these issues, this paper proposes a\nDecentralized Domain Name Service (DDNS) based on blockchain (Phicoin) and\ndistributed storage (IPFS). By leveraging the immutability of blockchain and\nthe content verification of IPFS, the system achieves decentralized storage and\ndistribution of domain name records, eliminating the centralized dependencies\nof traditional DNS. With a block time of 15 seconds, the system supports rapid\nbroadcasting of domain name updates, significantly improving resolution\nefficiency. The DDNS aims to serve as a complement or backup to the existing\nDNS system, providing a pollution-resistant, censorship-resistant,\nhigh-performance, and low-cost domain name resolution solution, offering a new\ntechnical path for the security and stability of the internet.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The current Domain Name System (DNS), as a core infrastructure of the\ninternet, exhibits several shortcomings: its centralized architecture leads to\ncensorship risks and single points of failure, making domain name resolution\nvulnerable to attacks. The lack of encryption in the resolution process exposes\nit to DNS hijacking and cache poisoning attacks. Additionally, the high\noperational costs limit participation and innovation among small to\nmedium-sized users. To address these issues, this paper proposes a\nDecentralized Domain Name Service (DDNS) based on blockchain (Phicoin) and\ndistributed storage (IPFS). By leveraging the immutability of blockchain and\nthe content verification of IPFS, the system achieves decentralized storage and\ndistribution of domain name records, eliminating the centralized dependencies\nof traditional DNS. With a block time of 15 seconds, the system supports rapid\nbroadcasting of domain name updates, significantly improving resolution\nefficiency. The DDNS aims to serve as a complement or backup to the existing\nDNS system, providing a pollution-resistant, censorship-resistant,\nhigh-performance, and low-cost domain name resolution solution, offering a new\ntechnical path for the security and stability of the internet."
                },
                "authors": [
                    {
                        "name": "Guang Yang"
                    }
                ],
                "author_detail": {
                    "name": "Guang Yang"
                },
                "author": "Guang Yang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.01959v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.01959v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.17747v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.17747v1",
                "updated": "2024-12-23T18:02:25Z",
                "updated_parsed": [
                    2024,
                    12,
                    23,
                    18,
                    2,
                    25,
                    0,
                    358,
                    0
                ],
                "published": "2024-12-23T18:02:25Z",
                "published_parsed": [
                    2024,
                    12,
                    23,
                    18,
                    2,
                    25,
                    0,
                    358,
                    0
                ],
                "title": "Deliberation in Latent Space via Differentiable Cache Augmentation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Deliberation in Latent Space via Differentiable Cache Augmentation"
                },
                "summary": "Techniques enabling large language models (LLMs) to \"think more\" by\ngenerating and attending to intermediate reasoning steps have shown promise in\nsolving complex problems. However, the standard approaches generate sequences\nof discrete tokens immediately before responding, and so they can incur\nsignificant latency costs and be challenging to optimize. In this work, we\ndemonstrate that a frozen LLM can be augmented with an offline coprocessor that\noperates on the model's key-value (kv) cache. This coprocessor augments the\ncache with a set of latent embeddings designed to improve the fidelity of\nsubsequent decoding. We train this coprocessor using the language modeling loss\nfrom the decoder on standard pretraining data, while keeping the decoder itself\nfrozen. This approach enables the model to learn, in an end-to-end\ndifferentiable fashion, how to distill additional computation into its\nkv-cache. Because the decoder remains unchanged, the coprocessor can operate\noffline and asynchronously, and the language model can function normally if the\ncoprocessor is unavailable or if a given cache is deemed not to require extra\ncomputation. We show experimentally that when a cache is augmented, the decoder\nachieves lower perplexity on numerous subsequent tokens. Furthermore, even\nwithout any task-specific training, our experiments demonstrate that cache\naugmentation consistently reduces perplexity and improves performance across a\nrange of reasoning-intensive tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Techniques enabling large language models (LLMs) to \"think more\" by\ngenerating and attending to intermediate reasoning steps have shown promise in\nsolving complex problems. However, the standard approaches generate sequences\nof discrete tokens immediately before responding, and so they can incur\nsignificant latency costs and be challenging to optimize. In this work, we\ndemonstrate that a frozen LLM can be augmented with an offline coprocessor that\noperates on the model's key-value (kv) cache. This coprocessor augments the\ncache with a set of latent embeddings designed to improve the fidelity of\nsubsequent decoding. We train this coprocessor using the language modeling loss\nfrom the decoder on standard pretraining data, while keeping the decoder itself\nfrozen. This approach enables the model to learn, in an end-to-end\ndifferentiable fashion, how to distill additional computation into its\nkv-cache. Because the decoder remains unchanged, the coprocessor can operate\noffline and asynchronously, and the language model can function normally if the\ncoprocessor is unavailable or if a given cache is deemed not to require extra\ncomputation. We show experimentally that when a cache is augmented, the decoder\nachieves lower perplexity on numerous subsequent tokens. Furthermore, even\nwithout any task-specific training, our experiments demonstrate that cache\naugmentation consistently reduces perplexity and improves performance across a\nrange of reasoning-intensive tasks."
                },
                "authors": [
                    {
                        "name": "Luyang Liu"
                    },
                    {
                        "name": "Jonas Pfeiffer"
                    },
                    {
                        "name": "Jiaxing Wu"
                    },
                    {
                        "name": "Jun Xie"
                    },
                    {
                        "name": "Arthur Szlam"
                    }
                ],
                "author_detail": {
                    "name": "Arthur Szlam"
                },
                "author": "Arthur Szlam",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.17747v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.17747v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.17685v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.17685v1",
                "updated": "2024-12-23T16:11:18Z",
                "updated_parsed": [
                    2024,
                    12,
                    23,
                    16,
                    11,
                    18,
                    0,
                    358,
                    0
                ],
                "published": "2024-12-23T16:11:18Z",
                "published_parsed": [
                    2024,
                    12,
                    23,
                    16,
                    11,
                    18,
                    0,
                    358,
                    0
                ],
                "title": "A Reproducible Method for Mapping Electricity Transmission\n  Infrastructure for Space Weather Risk Assessment",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Reproducible Method for Mapping Electricity Transmission\n  Infrastructure for Space Weather Risk Assessment"
                },
                "summary": "Space weather impact assessment is constrained by the lack of available asset\ninformation to undertake modeling of Geomagnetically Induced Currents (GICs) in\nExtra High Voltage electricity infrastructure networks. The U.S. National Space\nWeather Strategy and Action Plan identifies underutilized data as a central\nissue for improving risk assessment, motivating this research. Accurate GIC\nprediction is generally not possible without information on the electrical\ncircuit, therefore we define a reproducible method based on open-source data,\nwhich enables risk analysts to collect their own substation component data.\nThis process converts OpenStreetMap (OSM) substation locations to\nhigh-resolution, component-level mapping of electricity transmission assets by\nutilizing an innovative web-browser platform to facilitate component\nannotation. As a case study example, we convert an initial 1,313 high-voltage\n(>115 kV) substations to 52,273 substation components via Google Earth APIs\nutilizing low-altitude, satellite, and Streetview imagery. We find that a total\nof 41,642 substation components (79.6%) connect to the highest substation\nvoltage levels (>345 kV) and are possibly susceptible to GIC, with a total of\n7,949 transformers identified. Compared to the initial OSM baseline, we provide\nnew detailed insights on voltage levels, line capacities, and substation\nconfigurations. Two validation workshops were undertaken to align the method\nand data with GIC assessment needs. The approach ensures consistency and rapid\nscalability, enabling users to quickly count components via a flexible\nweb-browser application.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Space weather impact assessment is constrained by the lack of available asset\ninformation to undertake modeling of Geomagnetically Induced Currents (GICs) in\nExtra High Voltage electricity infrastructure networks. The U.S. National Space\nWeather Strategy and Action Plan identifies underutilized data as a central\nissue for improving risk assessment, motivating this research. Accurate GIC\nprediction is generally not possible without information on the electrical\ncircuit, therefore we define a reproducible method based on open-source data,\nwhich enables risk analysts to collect their own substation component data.\nThis process converts OpenStreetMap (OSM) substation locations to\nhigh-resolution, component-level mapping of electricity transmission assets by\nutilizing an innovative web-browser platform to facilitate component\nannotation. As a case study example, we convert an initial 1,313 high-voltage\n(>115 kV) substations to 52,273 substation components via Google Earth APIs\nutilizing low-altitude, satellite, and Streetview imagery. We find that a total\nof 41,642 substation components (79.6%) connect to the highest substation\nvoltage levels (>345 kV) and are possibly susceptible to GIC, with a total of\n7,949 transformers identified. Compared to the initial OSM baseline, we provide\nnew detailed insights on voltage levels, line capacities, and substation\nconfigurations. Two validation workshops were undertaken to align the method\nand data with GIC assessment needs. The approach ensures consistency and rapid\nscalability, enabling users to quickly count components via a flexible\nweb-browser application."
                },
                "authors": [
                    {
                        "name": "Edward J. Oughton"
                    },
                    {
                        "name": "Evan Alexander Peters"
                    },
                    {
                        "name": "Dennies Bor"
                    },
                    {
                        "name": "Noah Rivera"
                    },
                    {
                        "name": "C. Trevor Gaunt"
                    },
                    {
                        "name": "Robert Weigel"
                    }
                ],
                "author_detail": {
                    "name": "Robert Weigel"
                },
                "author": "Robert Weigel",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.17685v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.17685v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.geo-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.geo-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.18919v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.18919v2",
                "updated": "2024-12-23T14:40:26Z",
                "updated_parsed": [
                    2024,
                    12,
                    23,
                    14,
                    40,
                    26,
                    0,
                    358,
                    0
                ],
                "published": "2024-05-29T09:22:25Z",
                "published_parsed": [
                    2024,
                    5,
                    29,
                    9,
                    22,
                    25,
                    2,
                    150,
                    0
                ],
                "title": "Inter-Satellite Link-Enhanced Transmission Scheme Towards Aviation IoT\n  in SAGIN",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Inter-Satellite Link-Enhanced Transmission Scheme Towards Aviation IoT\n  in SAGIN"
                },
                "summary": "The rapid development of the aviation Internet of Things (IoT) has positioned\nin-flight connectivity (IFC) as one of its critical applications.\nSpace-air-ground integrated networks (SAGIN) are essential for ensuring the\nperformance of IFC by enabling seamless and reliable connectivity. However,\nmost existing research treats satellites merely as transparent forwarding nodes\nand overlooks their potential caching capabilities to enhance IFC data rates.\nIn this article, we explore an IFC-oriented SAGIN where satellites and ground\nstations (GSs) work together to transmit content to airborne passengers,\nthereby facilitating airborne communication. By categorizing files into cached\n(instantly accessible via satellites) and non-cached files (available only\nthrough GSs), this article pioneers the integration of multiple inter-satellite\nlinks (ISLs) into the IFC framework, thus innovating the content delivery\nprocess for both types of files. To minimize the average delay of content\ndelivery, we formulate the corresponding optimization problems: 1) For cached\nfiles, we propose an exact penalty-based method to determine the satellite\nassociation scheme. 2) For non-cached files, we present an efficient algorithm\nbased on alternating optimization to jointly optimize satellite association and\nGS bandwidth allocation. Our proposed framework is low in complexity, paving\nthe way for high-speed Internet connectivity for aviation passengers. Finally,\nsimulation results are provided to demonstrate the effectiveness of our\nproposed IFC framework for SAGIN.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rapid development of the aviation Internet of Things (IoT) has positioned\nin-flight connectivity (IFC) as one of its critical applications.\nSpace-air-ground integrated networks (SAGIN) are essential for ensuring the\nperformance of IFC by enabling seamless and reliable connectivity. However,\nmost existing research treats satellites merely as transparent forwarding nodes\nand overlooks their potential caching capabilities to enhance IFC data rates.\nIn this article, we explore an IFC-oriented SAGIN where satellites and ground\nstations (GSs) work together to transmit content to airborne passengers,\nthereby facilitating airborne communication. By categorizing files into cached\n(instantly accessible via satellites) and non-cached files (available only\nthrough GSs), this article pioneers the integration of multiple inter-satellite\nlinks (ISLs) into the IFC framework, thus innovating the content delivery\nprocess for both types of files. To minimize the average delay of content\ndelivery, we formulate the corresponding optimization problems: 1) For cached\nfiles, we propose an exact penalty-based method to determine the satellite\nassociation scheme. 2) For non-cached files, we present an efficient algorithm\nbased on alternating optimization to jointly optimize satellite association and\nGS bandwidth allocation. Our proposed framework is low in complexity, paving\nthe way for high-speed Internet connectivity for aviation passengers. Finally,\nsimulation results are provided to demonstrate the effectiveness of our\nproposed IFC framework for SAGIN."
                },
                "authors": [
                    {
                        "name": "Qian Chen"
                    },
                    {
                        "name": "Chenyu Wu"
                    },
                    {
                        "name": "Shuai Han"
                    },
                    {
                        "name": "Weixiao Meng"
                    },
                    {
                        "name": "Tony Q. S. Quek"
                    }
                ],
                "author_detail": {
                    "name": "Tony Q. S. Quek"
                },
                "author": "Tony Q. S. Quek",
                "arxiv_comment": "14 pages, 13 figures. This work has been accepted by IEEE Internet of\n  Things Journal. It is expanded on our previous research presented at the IEEE\n  Globecom 2024: Q. Chen, C. Wu, S. Han, W. Meng, and T. Q. Quek, \"Exploiting\n  Inter-Satellite Links for In-Flight Connectivity Scheme in Space-Air-Ground\n  Integrated Networks,\" in Proc. GLOBECOM 2024, Cape Town, South Africa, 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.18919v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.18919v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IT",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2402.03408v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2402.03408v3",
                "updated": "2024-12-23T12:55:21Z",
                "updated_parsed": [
                    2024,
                    12,
                    23,
                    12,
                    55,
                    21,
                    0,
                    358,
                    0
                ],
                "published": "2024-02-05T15:10:42Z",
                "published_parsed": [
                    2024,
                    2,
                    5,
                    15,
                    10,
                    42,
                    0,
                    36,
                    0
                ],
                "title": "A Framework for Effective Invocation Methods of Various LLM Services",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Framework for Effective Invocation Methods of Various LLM Services"
                },
                "summary": "Large Language Models (LLMs) have shown impressive abilities in solving\nvarious natural language processing tasks and are now widely offered as\nservices. LLM services enable users to accomplish tasks without requiring\nspecialized knowledge, simply by paying service providers. However, numerous\nproviders offer various LLM services with variations in pricing, latency, and\nperformance. These factors are also affected by different invocation methods,\nsuch as the choice of context and the use of cache, which lead to unpredictable\nand uncontrollable service cost and quality. Consequently, utilizing various\nLLM services invocation methods to construct an effective (cost-saving,\nlow-latency and high-performance) invocation strategy that best meets task\ndemands becomes a pressing challenge. This paper provides a comprehensive\noverview of methods help LLM services to be invoked efficiently. Technically,\nwe define the problem of constructing an effective LLM services invocation\nstrategy, and based on this, propose a unified LLM service invocation\nframework. The framework classifies existing methods into four categories:\ninput abstraction, semantic cache, solution design, and output enhancement,\nwhich can be used separately or jointly during the invocation life cycle. We\ndiscuss the methods in each category and compare them to provide valuable\nguidance for researchers. Finally, we emphasize the open challenges in this\ndomain and shed light on future research.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have shown impressive abilities in solving\nvarious natural language processing tasks and are now widely offered as\nservices. LLM services enable users to accomplish tasks without requiring\nspecialized knowledge, simply by paying service providers. However, numerous\nproviders offer various LLM services with variations in pricing, latency, and\nperformance. These factors are also affected by different invocation methods,\nsuch as the choice of context and the use of cache, which lead to unpredictable\nand uncontrollable service cost and quality. Consequently, utilizing various\nLLM services invocation methods to construct an effective (cost-saving,\nlow-latency and high-performance) invocation strategy that best meets task\ndemands becomes a pressing challenge. This paper provides a comprehensive\noverview of methods help LLM services to be invoked efficiently. Technically,\nwe define the problem of constructing an effective LLM services invocation\nstrategy, and based on this, propose a unified LLM service invocation\nframework. The framework classifies existing methods into four categories:\ninput abstraction, semantic cache, solution design, and output enhancement,\nwhich can be used separately or jointly during the invocation life cycle. We\ndiscuss the methods in each category and compare them to provide valuable\nguidance for researchers. Finally, we emphasize the open challenges in this\ndomain and shed light on future research."
                },
                "authors": [
                    {
                        "name": "Can Wang"
                    },
                    {
                        "name": "Dianbo Sui"
                    },
                    {
                        "name": "Bolin Zhang"
                    },
                    {
                        "name": "Xiaoyu Liu"
                    },
                    {
                        "name": "Jiabao Kang"
                    },
                    {
                        "name": "Zhidong Qiao"
                    },
                    {
                        "name": "Zhiying Tu"
                    }
                ],
                "author_detail": {
                    "name": "Zhiying Tu"
                },
                "author": "Zhiying Tu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2402.03408v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2402.03408v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.17464v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.17464v1",
                "updated": "2024-12-23T10:41:18Z",
                "updated_parsed": [
                    2024,
                    12,
                    23,
                    10,
                    41,
                    18,
                    0,
                    358,
                    0
                ],
                "published": "2024-12-23T10:41:18Z",
                "published_parsed": [
                    2024,
                    12,
                    23,
                    10,
                    41,
                    18,
                    0,
                    358,
                    0
                ],
                "title": "CALLIC: Content Adaptive Learning for Lossless Image Compression",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CALLIC: Content Adaptive Learning for Lossless Image Compression"
                },
                "summary": "Learned lossless image compression has achieved significant advancements in\nrecent years. However, existing methods often rely on training amortized\ngenerative models on massive datasets, resulting in sub-optimal probability\ndistribution estimation for specific testing images during encoding process. To\naddress this challenge, we explore the connection between the Minimum\nDescription Length (MDL) principle and Parameter-Efficient Transfer Learning\n(PETL), leading to the development of a novel content-adaptive approach for\nlearned lossless image compression, dubbed CALLIC. Specifically, we first\npropose a content-aware autoregressive self-attention mechanism by leveraging\nconvolutional gating operations, termed Masked Gated ConvFormer (MGCF), and\npretrain MGCF on training dataset. Cache then Crop Inference (CCI) is proposed\nto accelerate the coding process. During encoding, we decompose pre-trained\nlayers, including depth-wise convolutions, using low-rank matrices and then\nadapt the incremental weights on testing image by Rate-guided Progressive\nFine-Tuning (RPFT). RPFT fine-tunes with gradually increasing patches that are\nsorted in descending order by estimated entropy, optimizing learning process\nand reducing adaptation time. Extensive experiments across diverse datasets\ndemonstrate that CALLIC sets a new state-of-the-art (SOTA) for learned lossless\nimage compression.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Learned lossless image compression has achieved significant advancements in\nrecent years. However, existing methods often rely on training amortized\ngenerative models on massive datasets, resulting in sub-optimal probability\ndistribution estimation for specific testing images during encoding process. To\naddress this challenge, we explore the connection between the Minimum\nDescription Length (MDL) principle and Parameter-Efficient Transfer Learning\n(PETL), leading to the development of a novel content-adaptive approach for\nlearned lossless image compression, dubbed CALLIC. Specifically, we first\npropose a content-aware autoregressive self-attention mechanism by leveraging\nconvolutional gating operations, termed Masked Gated ConvFormer (MGCF), and\npretrain MGCF on training dataset. Cache then Crop Inference (CCI) is proposed\nto accelerate the coding process. During encoding, we decompose pre-trained\nlayers, including depth-wise convolutions, using low-rank matrices and then\nadapt the incremental weights on testing image by Rate-guided Progressive\nFine-Tuning (RPFT). RPFT fine-tunes with gradually increasing patches that are\nsorted in descending order by estimated entropy, optimizing learning process\nand reducing adaptation time. Extensive experiments across diverse datasets\ndemonstrate that CALLIC sets a new state-of-the-art (SOTA) for learned lossless\nimage compression."
                },
                "authors": [
                    {
                        "name": "Daxin Li"
                    },
                    {
                        "name": "Yuanchao Bai"
                    },
                    {
                        "name": "Kai Wang"
                    },
                    {
                        "name": "Junjun Jiang"
                    },
                    {
                        "name": "Xianming Liu"
                    },
                    {
                        "name": "Wen Gao"
                    }
                ],
                "author_detail": {
                    "name": "Wen Gao"
                },
                "author": "Wen Gao",
                "arxiv_comment": "Accepted by AAAI 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.17464v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.17464v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.IV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.17246v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.17246v1",
                "updated": "2024-12-23T03:38:46Z",
                "updated_parsed": [
                    2024,
                    12,
                    23,
                    3,
                    38,
                    46,
                    0,
                    358,
                    0
                ],
                "published": "2024-12-23T03:38:46Z",
                "published_parsed": [
                    2024,
                    12,
                    23,
                    3,
                    38,
                    46,
                    0,
                    358,
                    0
                ],
                "title": "Fast and Live Model Auto Scaling with O(1) Host Caching",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Fast and Live Model Auto Scaling with O(1) Host Caching"
                },
                "summary": "Model autoscaling is the key mechanism to achieve serverless\nmodel-as-a-service, but it faces a fundamental trade-off between scaling speed\nand storage/memory usage to cache parameters, and cannot meet frequent scaling\nrequirements across multiple hosts. The key problem is that data plane\nperformance is slow, and scaled instances remain stopped while parameters are\nloading. We first show that data plane can be made fast with no/O(1) caching by\nloading parameters through the compute network between GPUs because: (1) its\nspeed is comparable host cache and is underutilized; (2) scaling multiple\ninstances requires no or O(1) caching with network-optimized multicast. Second,\nautoscaling can be made live by breaking the scaling abstraction from a\ncoarse-grained instance-level to a fine-grained layer-level. This allows us to\noffload the layer computation from the overloaded serving instances to the\nscaled instance with cooperative execution, thus handles cases even when the\ncompute network is not sufficiently fast. Our system BLITZSCALE reduces the\nserving tail latencies by up to 86% without caching, and we achieve comparable\nperformance (or even better) to an optimal setup where all the parameters are\ncached at all the host for autoscaling.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Model autoscaling is the key mechanism to achieve serverless\nmodel-as-a-service, but it faces a fundamental trade-off between scaling speed\nand storage/memory usage to cache parameters, and cannot meet frequent scaling\nrequirements across multiple hosts. The key problem is that data plane\nperformance is slow, and scaled instances remain stopped while parameters are\nloading. We first show that data plane can be made fast with no/O(1) caching by\nloading parameters through the compute network between GPUs because: (1) its\nspeed is comparable host cache and is underutilized; (2) scaling multiple\ninstances requires no or O(1) caching with network-optimized multicast. Second,\nautoscaling can be made live by breaking the scaling abstraction from a\ncoarse-grained instance-level to a fine-grained layer-level. This allows us to\noffload the layer computation from the overloaded serving instances to the\nscaled instance with cooperative execution, thus handles cases even when the\ncompute network is not sufficiently fast. Our system BLITZSCALE reduces the\nserving tail latencies by up to 86% without caching, and we achieve comparable\nperformance (or even better) to an optimal setup where all the parameters are\ncached at all the host for autoscaling."
                },
                "authors": [
                    {
                        "name": "Dingyan Zhang"
                    },
                    {
                        "name": "Haotian Wang"
                    },
                    {
                        "name": "Yang Liu"
                    },
                    {
                        "name": "Xingda Wei"
                    },
                    {
                        "name": "Yizhou Shan"
                    },
                    {
                        "name": "Rong Chen"
                    },
                    {
                        "name": "Haibo Chen"
                    }
                ],
                "author_detail": {
                    "name": "Haibo Chen"
                },
                "author": "Haibo Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.17246v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.17246v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.OS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.05831v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.05831v2",
                "updated": "2024-12-23T02:52:36Z",
                "updated_parsed": [
                    2024,
                    12,
                    23,
                    2,
                    52,
                    36,
                    0,
                    358,
                    0
                ],
                "published": "2024-12-08T06:37:27Z",
                "published_parsed": [
                    2024,
                    12,
                    8,
                    6,
                    37,
                    27,
                    6,
                    343,
                    0
                ],
                "title": "Semi-Supervised Contrastive Learning for Controllable Video-to-Music\n  Retrieval",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Semi-Supervised Contrastive Learning for Controllable Video-to-Music\n  Retrieval"
                },
                "summary": "Content creators often use music to enhance their videos, from soundtracks in\nmovies to background music in video blogs and social media content. However,\nidentifying the best music for a video can be a difficult and time-consuming\ntask. To address this challenge, we propose a novel framework for automatically\nretrieving a matching music clip for a given video, and vice versa. Our\napproach leverages annotated music labels, as well as the inherent artistic\ncorrespondence between visual and music elements. Distinct from previous\ncross-modal music retrieval works, our method combines both self-supervised and\nsupervised training objectives. We use self-supervised and label-supervised\ncontrastive learning to train a joint embedding space between music and video.\nWe show the effectiveness of our approach by using music genre labels for the\nsupervised training component, and our framework can be generalized to other\nmusic annotations (e.g., emotion, instrument, etc.). Furthermore, our method\nenables fine-grained control over how much the retrieval process focuses on\nself-supervised vs. label information at inference time. We evaluate the\nlearned embeddings through a variety of video-to-music and music-to-video\nretrieval tasks. Our experiments show that the proposed approach successfully\ncombines self-supervised and supervised objectives and is effective for\ncontrollable music-video retrieval.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Content creators often use music to enhance their videos, from soundtracks in\nmovies to background music in video blogs and social media content. However,\nidentifying the best music for a video can be a difficult and time-consuming\ntask. To address this challenge, we propose a novel framework for automatically\nretrieving a matching music clip for a given video, and vice versa. Our\napproach leverages annotated music labels, as well as the inherent artistic\ncorrespondence between visual and music elements. Distinct from previous\ncross-modal music retrieval works, our method combines both self-supervised and\nsupervised training objectives. We use self-supervised and label-supervised\ncontrastive learning to train a joint embedding space between music and video.\nWe show the effectiveness of our approach by using music genre labels for the\nsupervised training component, and our framework can be generalized to other\nmusic annotations (e.g., emotion, instrument, etc.). Furthermore, our method\nenables fine-grained control over how much the retrieval process focuses on\nself-supervised vs. label information at inference time. We evaluate the\nlearned embeddings through a variety of video-to-music and music-to-video\nretrieval tasks. Our experiments show that the proposed approach successfully\ncombines self-supervised and supervised objectives and is effective for\ncontrollable music-video retrieval."
                },
                "authors": [
                    {
                        "name": "Shanti Stewart"
                    },
                    {
                        "name": "Gouthaman KV"
                    },
                    {
                        "name": "Lie Lu"
                    },
                    {
                        "name": "Andrea Fanelli"
                    }
                ],
                "author_detail": {
                    "name": "Andrea Fanelli"
                },
                "author": "Andrea Fanelli",
                "arxiv_comment": "Accepted at ICASSP 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.05831v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.05831v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.MM",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.MM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.17203v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.17203v1",
                "updated": "2024-12-23T00:46:53Z",
                "updated_parsed": [
                    2024,
                    12,
                    23,
                    0,
                    46,
                    53,
                    0,
                    358,
                    0
                ],
                "published": "2024-12-23T00:46:53Z",
                "published_parsed": [
                    2024,
                    12,
                    23,
                    0,
                    46,
                    53,
                    0,
                    358,
                    0
                ],
                "title": "Agile TLB Prefetching and Prediction Replacement Policy",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Agile TLB Prefetching and Prediction Replacement Policy"
                },
                "summary": "Virtual-to-physical address translation is a critical performance bottleneck\nin paging-based virtual memory systems. The Translation Lookaside Buffer (TLB)\naccelerates address translation by caching frequently accessed mappings, but\nTLB misses lead to costly page walks. Hardware and software techniques address\nthis challenge. Hardware approaches enhance TLB reach through system-level\nsupport, while software optimizations include TLB prefetching, replacement\npolicies, superpages, and page size adjustments. Prefetching Page Table Entries\n(PTEs) for future accesses reduces bottlenecks but may incur overhead from\nincorrect predictions. Integrating an Agile TLB Prefetcher (ATP) with SBFP\noptimizes performance by leveraging page table locality and dynamically\nidentifying essential free PTEs during page walks. Predictive replacement\npolicies further improve TLB performance. Traditional LRU replacement is\nlimited to near-instant references, while advanced policies like SRRIP, GHRP,\nSHiP, SDBP, and CHiRP enhance performance by targeting specific inefficiencies.\nCHiRP, tailored for L2 TLBs, surpasses other policies by leveraging control\nflow history to detect dead blocks, utilizing L2 TLB entries for learning\ninstead of sampling. These integrated techniques collectively address key\nchallenges in virtual memory management.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Virtual-to-physical address translation is a critical performance bottleneck\nin paging-based virtual memory systems. The Translation Lookaside Buffer (TLB)\naccelerates address translation by caching frequently accessed mappings, but\nTLB misses lead to costly page walks. Hardware and software techniques address\nthis challenge. Hardware approaches enhance TLB reach through system-level\nsupport, while software optimizations include TLB prefetching, replacement\npolicies, superpages, and page size adjustments. Prefetching Page Table Entries\n(PTEs) for future accesses reduces bottlenecks but may incur overhead from\nincorrect predictions. Integrating an Agile TLB Prefetcher (ATP) with SBFP\noptimizes performance by leveraging page table locality and dynamically\nidentifying essential free PTEs during page walks. Predictive replacement\npolicies further improve TLB performance. Traditional LRU replacement is\nlimited to near-instant references, while advanced policies like SRRIP, GHRP,\nSHiP, SDBP, and CHiRP enhance performance by targeting specific inefficiencies.\nCHiRP, tailored for L2 TLBs, surpasses other policies by leveraging control\nflow history to detect dead blocks, utilizing L2 TLB entries for learning\ninstead of sampling. These integrated techniques collectively address key\nchallenges in virtual memory management."
                },
                "authors": [
                    {
                        "name": "Melkamu Mersha"
                    },
                    {
                        "name": "Tsion Abay"
                    },
                    {
                        "name": "Mingziem Bitewa"
                    },
                    {
                        "name": "Gedare Bloom"
                    }
                ],
                "author_detail": {
                    "name": "Gedare Bloom"
                },
                "author": "Gedare Bloom",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.17203v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.17203v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.16897v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.16897v1",
                "updated": "2024-12-22T07:14:45Z",
                "updated_parsed": [
                    2024,
                    12,
                    22,
                    7,
                    14,
                    45,
                    6,
                    357,
                    0
                ],
                "published": "2024-12-22T07:14:45Z",
                "published_parsed": [
                    2024,
                    12,
                    22,
                    7,
                    14,
                    45,
                    6,
                    357,
                    0
                ],
                "title": "MVREC: A General Few-shot Defect Classification Model Using Multi-View\n  Region-Context",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MVREC: A General Few-shot Defect Classification Model Using Multi-View\n  Region-Context"
                },
                "summary": "Few-shot defect multi-classification (FSDMC) is an emerging trend in quality\ncontrol within industrial manufacturing. However, current FSDMC research often\nlacks generalizability due to its focus on specific datasets. Additionally,\ndefect classification heavily relies on contextual information within images,\nand existing methods fall short of effectively extracting this information. To\naddress these challenges, we propose a general FSDMC framework called MVREC,\nwhich offers two primary advantages: (1) MVREC extracts general features for\ndefect instances by incorporating the pre-trained AlphaCLIP model. (2) It\nutilizes a region-context framework to enhance defect features by leveraging\nmask region input and multi-view context augmentation. Furthermore, Few-shot\nZip-Adapter(-F) classifiers within the model are introduced to cache the visual\nfeatures of the support set and perform few-shot classification. We also\nintroduce MVTec-FS, a new FSDMC benchmark based on MVTec AD, which includes\n1228 defect images with instance-level mask annotations and 46 defect types.\nExtensive experiments conducted on MVTec-FS and four additional datasets\ndemonstrate its effectiveness in general defect classification and its ability\nto incorporate contextual information to improve classification performance.\nCode: https://github.com/ShuaiLYU/MVREC",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Few-shot defect multi-classification (FSDMC) is an emerging trend in quality\ncontrol within industrial manufacturing. However, current FSDMC research often\nlacks generalizability due to its focus on specific datasets. Additionally,\ndefect classification heavily relies on contextual information within images,\nand existing methods fall short of effectively extracting this information. To\naddress these challenges, we propose a general FSDMC framework called MVREC,\nwhich offers two primary advantages: (1) MVREC extracts general features for\ndefect instances by incorporating the pre-trained AlphaCLIP model. (2) It\nutilizes a region-context framework to enhance defect features by leveraging\nmask region input and multi-view context augmentation. Furthermore, Few-shot\nZip-Adapter(-F) classifiers within the model are introduced to cache the visual\nfeatures of the support set and perform few-shot classification. We also\nintroduce MVTec-FS, a new FSDMC benchmark based on MVTec AD, which includes\n1228 defect images with instance-level mask annotations and 46 defect types.\nExtensive experiments conducted on MVTec-FS and four additional datasets\ndemonstrate its effectiveness in general defect classification and its ability\nto incorporate contextual information to improve classification performance.\nCode: https://github.com/ShuaiLYU/MVREC"
                },
                "authors": [
                    {
                        "name": "Shuai Lyu"
                    },
                    {
                        "name": "Fangjian Liao"
                    },
                    {
                        "name": "Zeqi Ma"
                    },
                    {
                        "name": "Rongchen Zhang"
                    },
                    {
                        "name": "Dongmei Mo"
                    },
                    {
                        "name": "Waikeung Wong"
                    }
                ],
                "author_detail": {
                    "name": "Waikeung Wong"
                },
                "author": "Waikeung Wong",
                "arxiv_comment": "Accepted by AAAI 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.16897v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.16897v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.17565v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.17565v3",
                "updated": "2024-12-21T13:55:49Z",
                "updated_parsed": [
                    2024,
                    12,
                    21,
                    13,
                    55,
                    49,
                    5,
                    356,
                    0
                ],
                "published": "2024-06-25T14:02:08Z",
                "published_parsed": [
                    2024,
                    6,
                    25,
                    14,
                    2,
                    8,
                    1,
                    177,
                    0
                ],
                "title": "MemServe: Context Caching for Disaggregated LLM Serving with Elastic\n  Memory Pool",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MemServe: Context Caching for Disaggregated LLM Serving with Elastic\n  Memory Pool"
                },
                "summary": "Large language model (LLM) serving has transformed from stateless to stateful\nsystems, utilizing techniques like context caching and disaggregated inference.\nThese optimizations extend the lifespan and domain of the KV cache,\nnecessitating a new architectural approach. We present MemServe, a unified\nsystem that integrates both inter-request and intra-request optimizations.\nMemServe introduces MemPool, an elastic memory pool managing distributed memory\nand KV caches across serving instances. Using MemPool APIs, MemServe combines\ncontext caching with disaggregated inference for the first time, supported by a\nglobal scheduler that enhances cache reuse through a global prompt tree-based\nlocality-aware policy. Tests show that MemServe significantly improves job\ncompletion time and time-to-first-time.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language model (LLM) serving has transformed from stateless to stateful\nsystems, utilizing techniques like context caching and disaggregated inference.\nThese optimizations extend the lifespan and domain of the KV cache,\nnecessitating a new architectural approach. We present MemServe, a unified\nsystem that integrates both inter-request and intra-request optimizations.\nMemServe introduces MemPool, an elastic memory pool managing distributed memory\nand KV caches across serving instances. Using MemPool APIs, MemServe combines\ncontext caching with disaggregated inference for the first time, supported by a\nglobal scheduler that enhances cache reuse through a global prompt tree-based\nlocality-aware policy. Tests show that MemServe significantly improves job\ncompletion time and time-to-first-time."
                },
                "authors": [
                    {
                        "name": "Cunchen Hu"
                    },
                    {
                        "name": "Heyang Huang"
                    },
                    {
                        "name": "Junhao Hu"
                    },
                    {
                        "name": "Jiang Xu"
                    },
                    {
                        "name": "Xusheng Chen"
                    },
                    {
                        "name": "Tao Xie"
                    },
                    {
                        "name": "Chenxi Wang"
                    },
                    {
                        "name": "Sa Wang"
                    },
                    {
                        "name": "Yungang Bao"
                    },
                    {
                        "name": "Ninghui Sun"
                    },
                    {
                        "name": "Yizhou Shan"
                    }
                ],
                "author_detail": {
                    "name": "Yizhou Shan"
                },
                "author": "Yizhou Shan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.17565v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.17565v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.16585v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.16585v1",
                "updated": "2024-12-21T11:20:26Z",
                "updated_parsed": [
                    2024,
                    12,
                    21,
                    11,
                    20,
                    26,
                    5,
                    356,
                    0
                ],
                "published": "2024-12-21T11:20:26Z",
                "published_parsed": [
                    2024,
                    12,
                    21,
                    11,
                    20,
                    26,
                    5,
                    356,
                    0
                ],
                "title": "Parameterized Complexity of Caching in Networks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Parameterized Complexity of Caching in Networks"
                },
                "summary": "The fundamental caching problem in networks asks to find an allocation of\ncontents to a network of caches with the aim of maximizing the cache hit rate.\nDespite the problem's importance to a variety of research areas -- including\nnot only content delivery, but also edge intelligence and inference -- and the\nextensive body of work on empirical aspects of caching, very little is known\nabout the exact boundaries of tractability for the problem beyond its general\nNP-hardness. We close this gap by performing a comprehensive\ncomplexity-theoretic analysis of the problem through the lens of the\nparameterized complexity paradigm, which is designed to provide more precise\nstatements regarding algorithmic tractability than classical complexity. Our\nresults include algorithmic lower and upper bounds which together establish the\nconditions under which the caching problem becomes tractable.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The fundamental caching problem in networks asks to find an allocation of\ncontents to a network of caches with the aim of maximizing the cache hit rate.\nDespite the problem's importance to a variety of research areas -- including\nnot only content delivery, but also edge intelligence and inference -- and the\nextensive body of work on empirical aspects of caching, very little is known\nabout the exact boundaries of tractability for the problem beyond its general\nNP-hardness. We close this gap by performing a comprehensive\ncomplexity-theoretic analysis of the problem through the lens of the\nparameterized complexity paradigm, which is designed to provide more precise\nstatements regarding algorithmic tractability than classical complexity. Our\nresults include algorithmic lower and upper bounds which together establish the\nconditions under which the caching problem becomes tractable."
                },
                "authors": [
                    {
                        "name": "Robert Ganian"
                    },
                    {
                        "name": "Fionn Mc Inerney"
                    },
                    {
                        "name": "Dimitra Tsigkari"
                    }
                ],
                "author_detail": {
                    "name": "Dimitra Tsigkari"
                },
                "author": "Dimitra Tsigkari",
                "arxiv_comment": "A shorter version of this paper will appear in the proceedings of\n  AAAI 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.16585v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.16585v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.01253v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.01253v4",
                "updated": "2024-12-21T02:36:03Z",
                "updated_parsed": [
                    2024,
                    12,
                    21,
                    2,
                    36,
                    3,
                    5,
                    356,
                    0
                ],
                "published": "2024-12-02T08:22:56Z",
                "published_parsed": [
                    2024,
                    12,
                    2,
                    8,
                    22,
                    56,
                    0,
                    337,
                    0
                ],
                "title": "Yi-Lightning Technical Report",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Yi-Lightning Technical Report"
                },
                "summary": "This technical report presents Yi-Lightning, our latest flagship large\nlanguage model (LLM). It achieves exceptional performance, ranking 6th overall\non Chatbot Arena, with particularly strong results (2nd to 4th place) in\nspecialized categories including Chinese, Math, Coding, and Hard Prompts.\nYi-Lightning leverages an enhanced Mixture-of-Experts (MoE) architecture,\nfeaturing advanced expert segmentation and routing mechanisms coupled with\noptimized KV-caching techniques. Our development process encompasses\ncomprehensive pre-training, supervised fine-tuning (SFT), and reinforcement\nlearning from human feedback (RLHF), where we devise deliberate strategies for\nmulti-stage training, synthetic data construction, and reward modeling.\nFurthermore, we implement RAISE (Responsible AI Safety Engine), a\nfour-component framework to address safety issues across pre-training,\npost-training, and serving phases. Empowered by our scalable super-computing\ninfrastructure, all these innovations substantially reduce training, deployment\nand inference costs while maintaining high-performance standards. With further\nevaluations on public academic benchmarks, Yi-Lightning demonstrates\ncompetitive performance against top-tier LLMs, while we observe a notable\ndisparity between traditional, static benchmark results and real-world, dynamic\nhuman preferences. This observation prompts a critical reassessment of\nconventional benchmarks' utility in guiding the development of more intelligent\nand powerful AI systems for practical applications. Yi-Lightning is now\navailable through our developer platform at https://platform.lingyiwanwu.com.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This technical report presents Yi-Lightning, our latest flagship large\nlanguage model (LLM). It achieves exceptional performance, ranking 6th overall\non Chatbot Arena, with particularly strong results (2nd to 4th place) in\nspecialized categories including Chinese, Math, Coding, and Hard Prompts.\nYi-Lightning leverages an enhanced Mixture-of-Experts (MoE) architecture,\nfeaturing advanced expert segmentation and routing mechanisms coupled with\noptimized KV-caching techniques. Our development process encompasses\ncomprehensive pre-training, supervised fine-tuning (SFT), and reinforcement\nlearning from human feedback (RLHF), where we devise deliberate strategies for\nmulti-stage training, synthetic data construction, and reward modeling.\nFurthermore, we implement RAISE (Responsible AI Safety Engine), a\nfour-component framework to address safety issues across pre-training,\npost-training, and serving phases. Empowered by our scalable super-computing\ninfrastructure, all these innovations substantially reduce training, deployment\nand inference costs while maintaining high-performance standards. With further\nevaluations on public academic benchmarks, Yi-Lightning demonstrates\ncompetitive performance against top-tier LLMs, while we observe a notable\ndisparity between traditional, static benchmark results and real-world, dynamic\nhuman preferences. This observation prompts a critical reassessment of\nconventional benchmarks' utility in guiding the development of more intelligent\nand powerful AI systems for practical applications. Yi-Lightning is now\navailable through our developer platform at https://platform.lingyiwanwu.com."
                },
                "authors": [
                    {
                        "name": "Alan Wake"
                    },
                    {
                        "name": "Bei Chen"
                    },
                    {
                        "name": "C. X. Lv"
                    },
                    {
                        "name": "Chao Li"
                    },
                    {
                        "name": "Chengen Huang"
                    },
                    {
                        "name": "Chenglin Cai"
                    },
                    {
                        "name": "Chujie Zheng"
                    },
                    {
                        "name": "Daniel Cooper"
                    },
                    {
                        "name": "Fan Zhou"
                    },
                    {
                        "name": "Feng Hu"
                    },
                    {
                        "name": "Guoyin Wang"
                    },
                    {
                        "name": "Heng Ji"
                    },
                    {
                        "name": "Howard Qiu"
                    },
                    {
                        "name": "Jiangcheng Zhu"
                    },
                    {
                        "name": "Jun Tian"
                    },
                    {
                        "name": "Katherine Su"
                    },
                    {
                        "name": "Lihuan Zhang"
                    },
                    {
                        "name": "Liying Li"
                    },
                    {
                        "name": "Ming Song"
                    },
                    {
                        "name": "Mou Li"
                    },
                    {
                        "name": "Peng Liu"
                    },
                    {
                        "name": "Qicheng Hu"
                    },
                    {
                        "name": "Shawn Wang"
                    },
                    {
                        "name": "Shijun Zhou"
                    },
                    {
                        "name": "Shiming Yang"
                    },
                    {
                        "name": "Shiyong Li"
                    },
                    {
                        "name": "Tianhang Zhu"
                    },
                    {
                        "name": "Wen Xie"
                    },
                    {
                        "name": "Xiang He"
                    },
                    {
                        "name": "Xiaobo Chen"
                    },
                    {
                        "name": "Xiaohui Hu"
                    },
                    {
                        "name": "Xiaoyi Ren"
                    },
                    {
                        "name": "Xinyao Niu"
                    },
                    {
                        "name": "Yanpeng Li"
                    },
                    {
                        "name": "Yongke Zhao"
                    },
                    {
                        "name": "Yongzhen Luo"
                    },
                    {
                        "name": "Yuchi Xu"
                    },
                    {
                        "name": "Yuxuan Sha"
                    },
                    {
                        "name": "Zhaodong Yan"
                    },
                    {
                        "name": "Zhiyuan Liu"
                    },
                    {
                        "name": "Zirui Zhang"
                    },
                    {
                        "name": "Zonghong Dai"
                    }
                ],
                "author_detail": {
                    "name": "Zonghong Dai"
                },
                "author": "Zonghong Dai",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.01253v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.01253v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.16434v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.16434v1",
                "updated": "2024-12-21T01:48:52Z",
                "updated_parsed": [
                    2024,
                    12,
                    21,
                    1,
                    48,
                    52,
                    5,
                    356,
                    0
                ],
                "published": "2024-12-21T01:48:52Z",
                "published_parsed": [
                    2024,
                    12,
                    21,
                    1,
                    48,
                    52,
                    5,
                    356,
                    0
                ],
                "title": "SYMPHONY: Improving Memory Management for LLM Inference Workloads",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SYMPHONY: Improving Memory Management for LLM Inference Workloads"
                },
                "summary": "Large Language Models (LLMs) are increasingly being deployed in applications\nsuch as chatbots, code editors, and conversational agents. A key feature of\nLLMs is their ability to engage in multi-turn interactions with humans or\nexternal tools, enabling a wide range of tasks. Each new request in a\nmulti-turn interaction depends on the intermediate state, specifically the\nkey-value (K,V) caches, from previous requests in the ongoing interaction.\nExisting serving engines either recompute the K,V caches or offload them to\nmain memory. Profiling reveals that recomputation can result in over 99% of\nprocessed tokens being redundant. On the other hand, offloading K,V caches from\nGPU memory makes inference serving stateful, leading to load imbalances across\nthe cluster. To address these challenges, we developed SYMPHONY. SYMPHONY\nleverages the observation that multi-turn work loads provide additional hints\nthat allow K,V caches to be migrated off the critical serving path. By\nutilizing these hints, SYMPHONY dynamically migrates K,V caches to enable\nfinegrained scheduling of inference requests. Our experiments demonstrate that\nSYMPHONY can handle over 8x the number of requests compared to state-of-the-art\nbaselines, with a similar latency profile.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) are increasingly being deployed in applications\nsuch as chatbots, code editors, and conversational agents. A key feature of\nLLMs is their ability to engage in multi-turn interactions with humans or\nexternal tools, enabling a wide range of tasks. Each new request in a\nmulti-turn interaction depends on the intermediate state, specifically the\nkey-value (K,V) caches, from previous requests in the ongoing interaction.\nExisting serving engines either recompute the K,V caches or offload them to\nmain memory. Profiling reveals that recomputation can result in over 99% of\nprocessed tokens being redundant. On the other hand, offloading K,V caches from\nGPU memory makes inference serving stateful, leading to load imbalances across\nthe cluster. To address these challenges, we developed SYMPHONY. SYMPHONY\nleverages the observation that multi-turn work loads provide additional hints\nthat allow K,V caches to be migrated off the critical serving path. By\nutilizing these hints, SYMPHONY dynamically migrates K,V caches to enable\nfinegrained scheduling of inference requests. Our experiments demonstrate that\nSYMPHONY can handle over 8x the number of requests compared to state-of-the-art\nbaselines, with a similar latency profile."
                },
                "authors": [
                    {
                        "name": "Saurabh Agarwal"
                    },
                    {
                        "name": "Anyong Mao"
                    },
                    {
                        "name": "Aditya Akella"
                    },
                    {
                        "name": "Shivaram Venkataraman"
                    }
                ],
                "author_detail": {
                    "name": "Shivaram Venkataraman"
                },
                "author": "Shivaram Venkataraman",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.16434v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.16434v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.16001v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.16001v1",
                "updated": "2024-12-20T15:51:42Z",
                "updated_parsed": [
                    2024,
                    12,
                    20,
                    15,
                    51,
                    42,
                    4,
                    355,
                    0
                ],
                "published": "2024-12-20T15:51:42Z",
                "published_parsed": [
                    2024,
                    12,
                    20,
                    15,
                    51,
                    42,
                    4,
                    355,
                    0
                ],
                "title": "Multi-Strided Access Patterns to Boost Hardware Prefetching",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-Strided Access Patterns to Boost Hardware Prefetching"
                },
                "summary": "Important memory-bound kernels, such as linear algebra, convolutions, and\nstencils, rely on SIMD instructions as well as optimizations targeting improved\nvectorized data traversal and data re-use to attain satisfactory performance.\nOn on temporary CPU architectures, the hardware prefetcher is of key importance\nfor efficient utilization of the memory hierarchy. In this paper, we\ndemonstrate that transforming a memory access pattern consisting of a single\nstride to one that concurrently accesses multiple strides, can boost the\nutilization of the hardware prefetcher, and in turn improves the performance of\nmemory-bound kernels significantly. Using a set of micro-benchmarks, we\nestablish that accessing memory in a multi-strided manner enables more cache\nlines to be concurrently brought into the cache, resulting in improved cache\nhit ratios and higher effective memory bandwidth without the introduction of\ncostly software prefetch instructions. Subsequently, we show that multi-strided\nvariants of a collection of six memory-bound dense compute kernels outperform\nstate-of-the-art counterparts on three different micro-architectures. More\nspecifically, for kernels among which Matrix Vector Multiplication, Convolution\nStencil and kernels from PolyBench, we achieve significant speedups of up to\n12.55x over Polly, 2.99x over MKL, 1.98x over OpenBLAS, 1.08x over Halide and\n1.87x over OpenCV. The code transformation to take advantage of multi-strided\nmemory access is a natural extension of the loop unroll and loop interchange\ntechniques, allowing this method to be incorporated into compiler pipelines in\nthe future.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Important memory-bound kernels, such as linear algebra, convolutions, and\nstencils, rely on SIMD instructions as well as optimizations targeting improved\nvectorized data traversal and data re-use to attain satisfactory performance.\nOn on temporary CPU architectures, the hardware prefetcher is of key importance\nfor efficient utilization of the memory hierarchy. In this paper, we\ndemonstrate that transforming a memory access pattern consisting of a single\nstride to one that concurrently accesses multiple strides, can boost the\nutilization of the hardware prefetcher, and in turn improves the performance of\nmemory-bound kernels significantly. Using a set of micro-benchmarks, we\nestablish that accessing memory in a multi-strided manner enables more cache\nlines to be concurrently brought into the cache, resulting in improved cache\nhit ratios and higher effective memory bandwidth without the introduction of\ncostly software prefetch instructions. Subsequently, we show that multi-strided\nvariants of a collection of six memory-bound dense compute kernels outperform\nstate-of-the-art counterparts on three different micro-architectures. More\nspecifically, for kernels among which Matrix Vector Multiplication, Convolution\nStencil and kernels from PolyBench, we achieve significant speedups of up to\n12.55x over Polly, 2.99x over MKL, 1.98x over OpenBLAS, 1.08x over Halide and\n1.87x over OpenCV. The code transformation to take advantage of multi-strided\nmemory access is a natural extension of the loop unroll and loop interchange\ntechniques, allowing this method to be incorporated into compiler pipelines in\nthe future."
                },
                "authors": [
                    {
                        "name": "Miguel O. Blom"
                    },
                    {
                        "name": "Kristian F. D. Rietveld"
                    },
                    {
                        "name": "Rob V. van Nieuwpoort"
                    }
                ],
                "author_detail": {
                    "name": "Rob V. van Nieuwpoort"
                },
                "author": "Rob V. van Nieuwpoort",
                "arxiv_comment": "12 pages, 6 figures, 2 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.16001v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.16001v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.PF",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.14485v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.14485v2",
                "updated": "2024-12-20T15:18:44Z",
                "updated_parsed": [
                    2024,
                    12,
                    20,
                    15,
                    18,
                    44,
                    4,
                    355,
                    0
                ],
                "published": "2024-12-19T03:11:33Z",
                "published_parsed": [
                    2024,
                    12,
                    19,
                    3,
                    11,
                    33,
                    3,
                    354,
                    0
                ],
                "title": "Towards Projected and Incremental Pseudo-Boolean Model Counting",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Towards Projected and Incremental Pseudo-Boolean Model Counting"
                },
                "summary": "Model counting is a fundamental task that involves determining the number of\nsatisfying assignments to a logical formula, typically in conjunctive normal\nform (CNF). While CNF model counting has received extensive attention over\nrecent decades, interest in Pseudo-Boolean (PB) model counting is just emerging\npartly due to the greater flexibility of PB formulas. As such, we observed\nfeature gaps in existing PB counters such as a lack of support for projected\nand incremental settings, which could hinder adoption. In this work, our main\ncontribution is the introduction of the PB model counter PBCount2, the first\nexact PB model counter with support for projected and incremental model\ncounting. Our counter, PBCount2, uses our Least Occurrence Weighted Min Degree\n(LOW-MD) computation ordering heuristic to support projected model counting and\na cache mechanism to enable incremental model counting. In our evaluations,\nPBCount2 completed at least 1.40x the number of benchmarks of competing methods\nfor projected model counting and at least 1.18x of competing methods in\nincremental model counting.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Model counting is a fundamental task that involves determining the number of\nsatisfying assignments to a logical formula, typically in conjunctive normal\nform (CNF). While CNF model counting has received extensive attention over\nrecent decades, interest in Pseudo-Boolean (PB) model counting is just emerging\npartly due to the greater flexibility of PB formulas. As such, we observed\nfeature gaps in existing PB counters such as a lack of support for projected\nand incremental settings, which could hinder adoption. In this work, our main\ncontribution is the introduction of the PB model counter PBCount2, the first\nexact PB model counter with support for projected and incremental model\ncounting. Our counter, PBCount2, uses our Least Occurrence Weighted Min Degree\n(LOW-MD) computation ordering heuristic to support projected model counting and\na cache mechanism to enable incremental model counting. In our evaluations,\nPBCount2 completed at least 1.40x the number of benchmarks of competing methods\nfor projected model counting and at least 1.18x of competing methods in\nincremental model counting."
                },
                "authors": [
                    {
                        "name": "Suwei Yang"
                    },
                    {
                        "name": "Kuldeep S. Meel"
                    }
                ],
                "author_detail": {
                    "name": "Kuldeep S. Meel"
                },
                "author": "Kuldeep S. Meel",
                "arxiv_comment": "To appear in AAAI25",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.14485v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.14485v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.15605v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.15605v1",
                "updated": "2024-12-20T06:58:32Z",
                "updated_parsed": [
                    2024,
                    12,
                    20,
                    6,
                    58,
                    32,
                    4,
                    355,
                    0
                ],
                "published": "2024-12-20T06:58:32Z",
                "published_parsed": [
                    2024,
                    12,
                    20,
                    6,
                    58,
                    32,
                    4,
                    355,
                    0
                ],
                "title": "Don't Do RAG: When Cache-Augmented Generation is All You Need for\n  Knowledge Tasks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Don't Do RAG: When Cache-Augmented Generation is All You Need for\n  Knowledge Tasks"
                },
                "summary": "Retrieval-augmented generation (RAG) has gained traction as a powerful\napproach for enhancing language models by integrating external knowledge\nsources. However, RAG introduces challenges such as retrieval latency,\npotential errors in document selection, and increased system complexity. With\nthe advent of large language models (LLMs) featuring significantly extended\ncontext windows, this paper proposes an alternative paradigm, cache-augmented\ngeneration (CAG) that bypasses real-time retrieval. Our method involves\npreloading all relevant resources, especially when the documents or knowledge\nfor retrieval are of a limited and manageable size, into the LLM's extended\ncontext and caching its runtime parameters. During inference, the model\nutilizes these preloaded parameters to answer queries without additional\nretrieval steps. Comparative analyses reveal that CAG eliminates retrieval\nlatency and minimizes retrieval errors while maintaining context relevance.\nPerformance evaluations across multiple benchmarks highlight scenarios where\nlong-context LLMs either outperform or complement traditional RAG pipelines.\nThese findings suggest that, for certain applications, particularly those with\na constrained knowledge base, CAG provide a streamlined and efficient\nalternative to RAG, achieving comparable or superior results with reduced\ncomplexity.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Retrieval-augmented generation (RAG) has gained traction as a powerful\napproach for enhancing language models by integrating external knowledge\nsources. However, RAG introduces challenges such as retrieval latency,\npotential errors in document selection, and increased system complexity. With\nthe advent of large language models (LLMs) featuring significantly extended\ncontext windows, this paper proposes an alternative paradigm, cache-augmented\ngeneration (CAG) that bypasses real-time retrieval. Our method involves\npreloading all relevant resources, especially when the documents or knowledge\nfor retrieval are of a limited and manageable size, into the LLM's extended\ncontext and caching its runtime parameters. During inference, the model\nutilizes these preloaded parameters to answer queries without additional\nretrieval steps. Comparative analyses reveal that CAG eliminates retrieval\nlatency and minimizes retrieval errors while maintaining context relevance.\nPerformance evaluations across multiple benchmarks highlight scenarios where\nlong-context LLMs either outperform or complement traditional RAG pipelines.\nThese findings suggest that, for certain applications, particularly those with\na constrained knowledge base, CAG provide a streamlined and efficient\nalternative to RAG, achieving comparable or superior results with reduced\ncomplexity."
                },
                "authors": [
                    {
                        "name": "Brian J Chan"
                    },
                    {
                        "name": "Chao-Ting Chen"
                    },
                    {
                        "name": "Jui-Hung Cheng"
                    },
                    {
                        "name": "Hen-Hsen Huang"
                    }
                ],
                "author_detail": {
                    "name": "Hen-Hsen Huang"
                },
                "author": "Hen-Hsen Huang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.15605v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.15605v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.02820v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.02820v3",
                "updated": "2024-12-19T23:52:16Z",
                "updated_parsed": [
                    2024,
                    12,
                    19,
                    23,
                    52,
                    16,
                    3,
                    354,
                    0
                ],
                "published": "2024-11-05T05:41:41Z",
                "published_parsed": [
                    2024,
                    11,
                    5,
                    5,
                    41,
                    41,
                    1,
                    310,
                    0
                ],
                "title": "DroidSpeak: KV Cache Sharing for Cross-LLM Communication and Multi-LLM\n  Serving",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DroidSpeak: KV Cache Sharing for Cross-LLM Communication and Multi-LLM\n  Serving"
                },
                "summary": "Large Language Models (LLMs) are increasingly employed in complex workflows,\nwhere different LLMs and fine-tuned variants collaboratively address complex\ntasks. However, these systems face significant inefficiencies due to redundant\ncontext processing of the shared context. We propose DroidSpeak, a framework\nthat optimizes context sharing between fine-tuned LLMs derived from the same\nfoundational model. DroidSpeak identifies critical layers in the KV cache and\nselectively recomputes them, enabling effective reuse of intermediate data\nwhile maintaining high accuracy.\n  Our approach balances computational efficiency and task fidelity,\nsignificantly reducing inference latency and throughput bottlenecks.\nExperiments on diverse datasets and model pairs demonstrate that DroidSpeak\nachieves up to 3x higher throughputs and 2.6x faster prefill times with\nnegligible accuracy loss compared to full recomputation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) are increasingly employed in complex workflows,\nwhere different LLMs and fine-tuned variants collaboratively address complex\ntasks. However, these systems face significant inefficiencies due to redundant\ncontext processing of the shared context. We propose DroidSpeak, a framework\nthat optimizes context sharing between fine-tuned LLMs derived from the same\nfoundational model. DroidSpeak identifies critical layers in the KV cache and\nselectively recomputes them, enabling effective reuse of intermediate data\nwhile maintaining high accuracy.\n  Our approach balances computational efficiency and task fidelity,\nsignificantly reducing inference latency and throughput bottlenecks.\nExperiments on diverse datasets and model pairs demonstrate that DroidSpeak\nachieves up to 3x higher throughputs and 2.6x faster prefill times with\nnegligible accuracy loss compared to full recomputation."
                },
                "authors": [
                    {
                        "name": "Yuhan Liu"
                    },
                    {
                        "name": "Yuyang Huang"
                    },
                    {
                        "name": "Jiayi Yao"
                    },
                    {
                        "name": "Zhuohan Gu"
                    },
                    {
                        "name": "Kuntai Du"
                    },
                    {
                        "name": "Hanchen Li"
                    },
                    {
                        "name": "Yihua Cheng"
                    },
                    {
                        "name": "Junchen Jiang"
                    },
                    {
                        "name": "Shan Lu"
                    },
                    {
                        "name": "Madan Musuvathi"
                    },
                    {
                        "name": "Esha Choukse"
                    }
                ],
                "author_detail": {
                    "name": "Esha Choukse"
                },
                "author": "Esha Choukse",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.02820v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.02820v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.MA",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.12592v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.12592v2",
                "updated": "2024-12-19T22:34:37Z",
                "updated_parsed": [
                    2024,
                    12,
                    19,
                    22,
                    34,
                    37,
                    3,
                    354,
                    0
                ],
                "published": "2024-08-22T17:56:29Z",
                "published_parsed": [
                    2024,
                    8,
                    22,
                    17,
                    56,
                    29,
                    3,
                    235,
                    0
                ],
                "title": "Exposing Shadow Branches",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Exposing Shadow Branches"
                },
                "summary": "Modern processors implement a decoupled front-end in the form of Fetch\nDirected Instruction Prefetching (FDIP) to avoid front-end stalls. FDIP is\ndriven by the Branch Prediction Unit (BPU), relying on the BPU's accuracy and\nbranch target tracking structures to speculatively fetch instructions into the\nInstruction Cache (L1I). As data center applications become more complex, their\ncode footprints also grow, resulting in an increase in Branch Target Buffer\n(BTB) misses. FDIP can alleviate L1I cache misses, but when it encounters a BTB\nmiss, the BPU may not identify the current instruction as a branch to FDIP.\nThis can prevent FDIP from prefetching or cause it to speculate down the wrong\npath, further polluting the L1I cache. We observe that the vast majority, 75%,\nof BTB-missing, unidentified branches are actually present in instruction cache\nlines that FDIP has previously fetched but, these missing branches have not yet\nbeen decoded and inserted into the BTB. This is because the instruction line is\ndecoded from an entry point (which is the target of the previous taken branch)\ntill an exit point (the taken branch). Branch instructions present in the\nignored portion of the cache line we call them \"Shadow Branches\". Here we\npresent Skeia, a novel shadow branch decoding technique that identifies and\ndecodes unused bytes in cache lines fetched by FDIP, inserting them into a\nShadow Branch Buffer (SBB). The SBB is accessed in parallel with the BTB,\nallowing FDIP to speculate despite a BTB miss. With a minimal storage state of\n12.25KB, Skeia delivers a geomean speedup of ~5.7% over an 8K-entry BTB (78KB)\nand ~2% versus adding an equal amount of state to the BTB across 16 front-end\nbound applications. Since many branches stored in the SBB are unique compared\nto those in a similarly sized BTB, we consistently observe greater performance\ngains with Skeia across all examined sizes until saturation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Modern processors implement a decoupled front-end in the form of Fetch\nDirected Instruction Prefetching (FDIP) to avoid front-end stalls. FDIP is\ndriven by the Branch Prediction Unit (BPU), relying on the BPU's accuracy and\nbranch target tracking structures to speculatively fetch instructions into the\nInstruction Cache (L1I). As data center applications become more complex, their\ncode footprints also grow, resulting in an increase in Branch Target Buffer\n(BTB) misses. FDIP can alleviate L1I cache misses, but when it encounters a BTB\nmiss, the BPU may not identify the current instruction as a branch to FDIP.\nThis can prevent FDIP from prefetching or cause it to speculate down the wrong\npath, further polluting the L1I cache. We observe that the vast majority, 75%,\nof BTB-missing, unidentified branches are actually present in instruction cache\nlines that FDIP has previously fetched but, these missing branches have not yet\nbeen decoded and inserted into the BTB. This is because the instruction line is\ndecoded from an entry point (which is the target of the previous taken branch)\ntill an exit point (the taken branch). Branch instructions present in the\nignored portion of the cache line we call them \"Shadow Branches\". Here we\npresent Skeia, a novel shadow branch decoding technique that identifies and\ndecodes unused bytes in cache lines fetched by FDIP, inserting them into a\nShadow Branch Buffer (SBB). The SBB is accessed in parallel with the BTB,\nallowing FDIP to speculate despite a BTB miss. With a minimal storage state of\n12.25KB, Skeia delivers a geomean speedup of ~5.7% over an 8K-entry BTB (78KB)\nand ~2% versus adding an equal amount of state to the BTB across 16 front-end\nbound applications. Since many branches stored in the SBB are unique compared\nto those in a similarly sized BTB, we consistently observe greater performance\ngains with Skeia across all examined sizes until saturation."
                },
                "authors": [
                    {
                        "name": "Chrysanthos Pepi"
                    },
                    {
                        "name": "Bhargav Reddy Godala"
                    },
                    {
                        "name": "Krishnam Tibrewala"
                    },
                    {
                        "name": "Gino Chacon"
                    },
                    {
                        "name": "Paul V. Gratz"
                    },
                    {
                        "name": "Daniel A. Jiménez"
                    },
                    {
                        "name": "Gilles A. Pokam"
                    },
                    {
                        "name": "David I. August"
                    }
                ],
                "author_detail": {
                    "name": "David I. August"
                },
                "author": "David I. August",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.12592v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.12592v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.14838v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.14838v1",
                "updated": "2024-12-19T13:28:42Z",
                "updated_parsed": [
                    2024,
                    12,
                    19,
                    13,
                    28,
                    42,
                    3,
                    354,
                    0
                ],
                "published": "2024-12-19T13:28:42Z",
                "published_parsed": [
                    2024,
                    12,
                    19,
                    13,
                    28,
                    42,
                    3,
                    354,
                    0
                ],
                "title": "DynamicKV: Task-Aware Adaptive KV Cache Compression for Long Context\n  LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DynamicKV: Task-Aware Adaptive KV Cache Compression for Long Context\n  LLMs"
                },
                "summary": "Efficient KV cache management in LLMs is crucial for long-context tasks like\nRAG and summarization. Existing KV cache compression methods enforce a fixed\npattern, neglecting task-specific characteristics and reducing the retention of\nessential information. However, we observe distinct activation patterns across\nlayers in various tasks, highlighting the need for adaptive strategies tailored\nto each task's unique demands. Based on this insight, we propose DynamicKV, a\nmethod that dynamically optimizes token retention by adjusting the number of\ntokens retained at each layer to adapt to the specific task. DynamicKV\nestablishes global and per-layer maximum KV cache budgets, temporarily\nretaining the maximum budget for the current layer, and periodically updating\nthe KV cache sizes of all preceding layers during inference. Our method retains\nonly 1.7% of the KV cache size while achieving ~85% of the Full KV cache\nperformance on LongBench. Notably, even under extreme compression (0.9%),\nDynamicKV surpasses state-of-the-art (SOTA) methods by 11% in the\nNeedle-in-a-Haystack test using Mistral-7B-Instruct-v0.2. The code will be\nreleased.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient KV cache management in LLMs is crucial for long-context tasks like\nRAG and summarization. Existing KV cache compression methods enforce a fixed\npattern, neglecting task-specific characteristics and reducing the retention of\nessential information. However, we observe distinct activation patterns across\nlayers in various tasks, highlighting the need for adaptive strategies tailored\nto each task's unique demands. Based on this insight, we propose DynamicKV, a\nmethod that dynamically optimizes token retention by adjusting the number of\ntokens retained at each layer to adapt to the specific task. DynamicKV\nestablishes global and per-layer maximum KV cache budgets, temporarily\nretaining the maximum budget for the current layer, and periodically updating\nthe KV cache sizes of all preceding layers during inference. Our method retains\nonly 1.7% of the KV cache size while achieving ~85% of the Full KV cache\nperformance on LongBench. Notably, even under extreme compression (0.9%),\nDynamicKV surpasses state-of-the-art (SOTA) methods by 11% in the\nNeedle-in-a-Haystack test using Mistral-7B-Instruct-v0.2. The code will be\nreleased."
                },
                "authors": [
                    {
                        "name": "Xiabin Zhou"
                    },
                    {
                        "name": "Wenbin Wang"
                    },
                    {
                        "name": "Minyan Zeng"
                    },
                    {
                        "name": "Jiaxian Guo"
                    },
                    {
                        "name": "Xuebo Liu"
                    },
                    {
                        "name": "Li Shen"
                    },
                    {
                        "name": "Min Zhang"
                    },
                    {
                        "name": "Liang Ding"
                    }
                ],
                "author_detail": {
                    "name": "Liang Ding"
                },
                "author": "Liang Ding",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.14838v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.14838v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.05317v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.05317v3",
                "updated": "2024-12-19T12:38:23Z",
                "updated_parsed": [
                    2024,
                    12,
                    19,
                    12,
                    38,
                    23,
                    3,
                    354,
                    0
                ],
                "published": "2024-10-05T03:47:06Z",
                "published_parsed": [
                    2024,
                    10,
                    5,
                    3,
                    47,
                    6,
                    5,
                    279,
                    0
                ],
                "title": "Accelerating Diffusion Transformers with Token-wise Feature Caching",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Accelerating Diffusion Transformers with Token-wise Feature Caching"
                },
                "summary": "Diffusion transformers have shown significant effectiveness in both image and\nvideo synthesis at the expense of huge computation costs. To address this\nproblem, feature caching methods have been introduced to accelerate diffusion\ntransformers by caching the features in previous timesteps and reusing them in\nthe following timesteps. However, previous caching methods ignore that\ndifferent tokens exhibit different sensitivities to feature caching, and\nfeature caching on some tokens may lead to 10$\\times$ more destruction to the\noverall generation quality compared with other tokens. In this paper, we\nintroduce token-wise feature caching, allowing us to adaptively select the most\nsuitable tokens for caching, and further enable us to apply different caching\nratios to neural layers in different types and depths. Extensive experiments on\nPixArt-$\\alpha$, OpenSora, and DiT demonstrate our effectiveness in both image\nand video generation with no requirements for training. For instance,\n2.36$\\times$ and 1.93$\\times$ acceleration are achieved on OpenSora and\nPixArt-$\\alpha$ with almost no drop in generation quality.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion transformers have shown significant effectiveness in both image and\nvideo synthesis at the expense of huge computation costs. To address this\nproblem, feature caching methods have been introduced to accelerate diffusion\ntransformers by caching the features in previous timesteps and reusing them in\nthe following timesteps. However, previous caching methods ignore that\ndifferent tokens exhibit different sensitivities to feature caching, and\nfeature caching on some tokens may lead to 10$\\times$ more destruction to the\noverall generation quality compared with other tokens. In this paper, we\nintroduce token-wise feature caching, allowing us to adaptively select the most\nsuitable tokens for caching, and further enable us to apply different caching\nratios to neural layers in different types and depths. Extensive experiments on\nPixArt-$\\alpha$, OpenSora, and DiT demonstrate our effectiveness in both image\nand video generation with no requirements for training. For instance,\n2.36$\\times$ and 1.93$\\times$ acceleration are achieved on OpenSora and\nPixArt-$\\alpha$ with almost no drop in generation quality."
                },
                "authors": [
                    {
                        "name": "Chang Zou"
                    },
                    {
                        "name": "Xuyang Liu"
                    },
                    {
                        "name": "Ting Liu"
                    },
                    {
                        "name": "Siteng Huang"
                    },
                    {
                        "name": "Linfeng Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Linfeng Zhang"
                },
                "author": "Linfeng Zhang",
                "arxiv_comment": "In this version, we achieved a nearly lossless acceleration of 1.51\n  times for ToCa on FLUX in the appendix",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.05317v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.05317v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.14392v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.14392v1",
                "updated": "2024-12-18T22:52:12Z",
                "updated_parsed": [
                    2024,
                    12,
                    18,
                    22,
                    52,
                    12,
                    2,
                    353,
                    0
                ],
                "published": "2024-12-18T22:52:12Z",
                "published_parsed": [
                    2024,
                    12,
                    18,
                    22,
                    52,
                    12,
                    2,
                    353,
                    0
                ],
                "title": "Nemesis: Noise-randomized Encryption with Modular Efficiency and Secure\n  Integration in Machine Learning Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Nemesis: Noise-randomized Encryption with Modular Efficiency and Secure\n  Integration in Machine Learning Systems"
                },
                "summary": "Machine learning (ML) systems that guarantee security and privacy often rely\non Fully Homomorphic Encryption (FHE) as a cornerstone technique, enabling\ncomputations on encrypted data without exposing sensitive information. However,\na critical limitation of FHE is its computational inefficiency, making it\nimpractical for large-scale applications. In this work, we propose\n\\textit{Nemesis}, a framework that accelerates FHE-based systems without\ncompromising accuracy or security. The design of Nemesis is inspired by Rache\n(SIGMOD'23), which introduced a caching mechanism for encrypted integers and\nscalars. Nemesis extends this idea with more advanced caching techniques and\nmathematical tools, enabling efficient operations over multi-slot FHE schemes\nand overcoming Rache's limitations to support general plaintext structures. We\nformally prove the security of Nemesis under standard cryptographic assumptions\nand evaluate its performance extensively on widely used datasets, including\nMNIST, FashionMNIST, and CIFAR-10. Experimental results show that Nemesis\nsignificantly reduces the computational overhead of FHE-based ML systems,\npaving the way for broader adoption of privacy-preserving technologies.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Machine learning (ML) systems that guarantee security and privacy often rely\non Fully Homomorphic Encryption (FHE) as a cornerstone technique, enabling\ncomputations on encrypted data without exposing sensitive information. However,\na critical limitation of FHE is its computational inefficiency, making it\nimpractical for large-scale applications. In this work, we propose\n\\textit{Nemesis}, a framework that accelerates FHE-based systems without\ncompromising accuracy or security. The design of Nemesis is inspired by Rache\n(SIGMOD'23), which introduced a caching mechanism for encrypted integers and\nscalars. Nemesis extends this idea with more advanced caching techniques and\nmathematical tools, enabling efficient operations over multi-slot FHE schemes\nand overcoming Rache's limitations to support general plaintext structures. We\nformally prove the security of Nemesis under standard cryptographic assumptions\nand evaluate its performance extensively on widely used datasets, including\nMNIST, FashionMNIST, and CIFAR-10. Experimental results show that Nemesis\nsignificantly reduces the computational overhead of FHE-based ML systems,\npaving the way for broader adoption of privacy-preserving technologies."
                },
                "authors": [
                    {
                        "name": "Dongfang Zhao"
                    }
                ],
                "author_detail": {
                    "name": "Dongfang Zhao"
                },
                "author": "Dongfang Zhao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.14392v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.14392v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.14363v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.14363v1",
                "updated": "2024-12-18T22:01:55Z",
                "updated_parsed": [
                    2024,
                    12,
                    18,
                    22,
                    1,
                    55,
                    2,
                    353,
                    0
                ],
                "published": "2024-12-18T22:01:55Z",
                "published_parsed": [
                    2024,
                    12,
                    18,
                    22,
                    1,
                    55,
                    2,
                    353,
                    0
                ],
                "title": "ResQ: Mixed-Precision Quantization of Large Language Models with\n  Low-Rank Residuals",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ResQ: Mixed-Precision Quantization of Large Language Models with\n  Low-Rank Residuals"
                },
                "summary": "Post-training quantization (PTQ) of large language models (LLMs) holds the\npromise in reducing the prohibitive computational cost at inference time.\nQuantization of all weight, activation and key-value (KV) cache tensors to\n4-bit without significantly degrading generalizability is challenging, due to\nthe high quantization error caused by extreme outliers in activations. To\ntackle this problem, we propose ResQ, a PTQ method that pushes further the\nstate-of-the-art. By means of principal component analysis (PCA), it identifies\na low-rank subspace (in practice 1/8 of the hidden dimension) in which\nactivation variances are highest, and keep the coefficients within this\nsubspace in high precision, e.g. 8-bit, while quantizing the rest to 4-bit.\nWithin each subspace, invariant random rotation is applied to further suppress\noutliers. We show that this is a provably optimal mixed precision quantization\nscheme that minimizes error. With the Llama families of models, we demonstrate\nthat ResQ outperforms recent uniform and mixed precision PTQ methods on a\nvariety of benchmarks, achieving up to 33% lower perplexity on Wikitext than\nthe next best method SpinQuant, and a 2.4x speedup over 16-bit baseline. Code\nis available at https://github.com/utkarsh-dmx/project-resq.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Post-training quantization (PTQ) of large language models (LLMs) holds the\npromise in reducing the prohibitive computational cost at inference time.\nQuantization of all weight, activation and key-value (KV) cache tensors to\n4-bit without significantly degrading generalizability is challenging, due to\nthe high quantization error caused by extreme outliers in activations. To\ntackle this problem, we propose ResQ, a PTQ method that pushes further the\nstate-of-the-art. By means of principal component analysis (PCA), it identifies\na low-rank subspace (in practice 1/8 of the hidden dimension) in which\nactivation variances are highest, and keep the coefficients within this\nsubspace in high precision, e.g. 8-bit, while quantizing the rest to 4-bit.\nWithin each subspace, invariant random rotation is applied to further suppress\noutliers. We show that this is a provably optimal mixed precision quantization\nscheme that minimizes error. With the Llama families of models, we demonstrate\nthat ResQ outperforms recent uniform and mixed precision PTQ methods on a\nvariety of benchmarks, achieving up to 33% lower perplexity on Wikitext than\nthe next best method SpinQuant, and a 2.4x speedup over 16-bit baseline. Code\nis available at https://github.com/utkarsh-dmx/project-resq."
                },
                "authors": [
                    {
                        "name": "Utkarsh Saxena"
                    },
                    {
                        "name": "Sayeh Sharify"
                    },
                    {
                        "name": "Kaushik Roy"
                    },
                    {
                        "name": "Xin Wang"
                    }
                ],
                "author_detail": {
                    "name": "Xin Wang"
                },
                "author": "Xin Wang",
                "arxiv_comment": "14 pages, 6 figures, 6 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.14363v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.14363v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.14335v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.14335v1",
                "updated": "2024-12-18T21:09:08Z",
                "updated_parsed": [
                    2024,
                    12,
                    18,
                    21,
                    9,
                    8,
                    2,
                    353,
                    0
                ],
                "published": "2024-12-18T21:09:08Z",
                "published_parsed": [
                    2024,
                    12,
                    18,
                    21,
                    9,
                    8,
                    2,
                    353,
                    0
                ],
                "title": "Optimizing ML Concurrent Computation and Communication with GPU DMA\n  Engines",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Optimizing ML Concurrent Computation and Communication with GPU DMA\n  Engines"
                },
                "summary": "Concurrent computation and communication (C3) is a pervasive paradigm in ML\nand other domains, making its performance optimization crucial. In this paper,\nwe carefully characterize C3 in ML on GPUs, which are most widely deployed for\nML training and inference. We observe that while C3 leads to performance\nuplifts, the uplifts are far lower than ideal speedups (serial computation and\ncommunication versus maximum of computation or communication; all times from\nisolated executions). C3 on average achieves only 21% of ideal speedup, this is\ndue to known challenges of compute and memory interference between concurrent\nGPU kernels (that is, sharing of GPU's compute units, caches and HBM).\n  To attain better performance for C3, first, we evaluate dual strategies of\nschedule prioritization and careful resource partitioning of compute units on\nGPUs to push performance attained with C3 (on average 42% of ideal speedup). We\nalso provide heuristics that can guide a runtime while employing these\nstrategies. To further enhance C3 performance, we propose to mitigate C3\ninterference by offloading communication tasks to the GPU's DMA engines. To\nthis end, we build Concurrent Communication CoLlectives (ConCCL)\nproof-of-concepts that harness DMA engines for communication. We show how\nConCCL considerably closes the gap between realized and ideal speedup for C3\n(on average 72% of ideal speedup is realized, up to 1.67x speedup). Overall,\nour work makes a strong case for GPU DMA engine advancements to better support\nC3 on GPUs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Concurrent computation and communication (C3) is a pervasive paradigm in ML\nand other domains, making its performance optimization crucial. In this paper,\nwe carefully characterize C3 in ML on GPUs, which are most widely deployed for\nML training and inference. We observe that while C3 leads to performance\nuplifts, the uplifts are far lower than ideal speedups (serial computation and\ncommunication versus maximum of computation or communication; all times from\nisolated executions). C3 on average achieves only 21% of ideal speedup, this is\ndue to known challenges of compute and memory interference between concurrent\nGPU kernels (that is, sharing of GPU's compute units, caches and HBM).\n  To attain better performance for C3, first, we evaluate dual strategies of\nschedule prioritization and careful resource partitioning of compute units on\nGPUs to push performance attained with C3 (on average 42% of ideal speedup). We\nalso provide heuristics that can guide a runtime while employing these\nstrategies. To further enhance C3 performance, we propose to mitigate C3\ninterference by offloading communication tasks to the GPU's DMA engines. To\nthis end, we build Concurrent Communication CoLlectives (ConCCL)\nproof-of-concepts that harness DMA engines for communication. We show how\nConCCL considerably closes the gap between realized and ideal speedup for C3\n(on average 72% of ideal speedup is realized, up to 1.67x speedup). Overall,\nour work makes a strong case for GPU DMA engine advancements to better support\nC3 on GPUs."
                },
                "authors": [
                    {
                        "name": "Anirudha Agrawal"
                    },
                    {
                        "name": "Shaizeen Aga"
                    },
                    {
                        "name": "Suchita Pati"
                    },
                    {
                        "name": "Mahzabeen Islam"
                    }
                ],
                "author_detail": {
                    "name": "Mahzabeen Islam"
                },
                "author": "Mahzabeen Islam",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.14335v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.14335v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.16179v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.16179v4",
                "updated": "2024-12-18T17:36:36Z",
                "updated_parsed": [
                    2024,
                    12,
                    18,
                    17,
                    36,
                    36,
                    2,
                    353,
                    0
                ],
                "published": "2024-10-21T16:44:51Z",
                "published_parsed": [
                    2024,
                    10,
                    21,
                    16,
                    44,
                    51,
                    0,
                    295,
                    0
                ],
                "title": "MagicPIG: LSH Sampling for Efficient LLM Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MagicPIG: LSH Sampling for Efficient LLM Generation"
                },
                "summary": "Large language models (LLMs) with long context windows have gained\nsignificant attention. However, the KV cache, stored to avoid re-computation,\nbecomes a bottleneck. Various dynamic sparse or TopK-based attention\napproximation methods have been proposed to leverage the common insight that\nattention is sparse. In this paper, we first show that TopK attention itself\nsuffers from quality degradation in certain downstream tasks because attention\nis not always as sparse as expected. Rather than selecting the keys and values\nwith the highest attention scores, sampling with theoretical guarantees can\nprovide a better estimation for attention output. To make the sampling-based\napproximation practical in LLM generation, we propose MagicPIG, a heterogeneous\nsystem based on Locality Sensitive Hashing (LSH). MagicPIG significantly\nreduces the workload of attention computation while preserving high accuracy\nfor diverse tasks. MagicPIG stores the LSH hash tables and runs the attention\ncomputation on the CPU, which allows it to serve longer contexts and larger\nbatch sizes with high approximation accuracy. MagicPIG can improve decoding\nthroughput by up to $5\\times$ across various GPU hardware and achieve 54ms\ndecoding latency on a single RTX 4090 for Llama-3.1-8B-Instruct model with a\ncontext of 96k tokens. The code is available at\nhttps://github.com/Infini-AI-Lab/MagicPIG.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) with long context windows have gained\nsignificant attention. However, the KV cache, stored to avoid re-computation,\nbecomes a bottleneck. Various dynamic sparse or TopK-based attention\napproximation methods have been proposed to leverage the common insight that\nattention is sparse. In this paper, we first show that TopK attention itself\nsuffers from quality degradation in certain downstream tasks because attention\nis not always as sparse as expected. Rather than selecting the keys and values\nwith the highest attention scores, sampling with theoretical guarantees can\nprovide a better estimation for attention output. To make the sampling-based\napproximation practical in LLM generation, we propose MagicPIG, a heterogeneous\nsystem based on Locality Sensitive Hashing (LSH). MagicPIG significantly\nreduces the workload of attention computation while preserving high accuracy\nfor diverse tasks. MagicPIG stores the LSH hash tables and runs the attention\ncomputation on the CPU, which allows it to serve longer contexts and larger\nbatch sizes with high approximation accuracy. MagicPIG can improve decoding\nthroughput by up to $5\\times$ across various GPU hardware and achieve 54ms\ndecoding latency on a single RTX 4090 for Llama-3.1-8B-Instruct model with a\ncontext of 96k tokens. The code is available at\nhttps://github.com/Infini-AI-Lab/MagicPIG."
                },
                "authors": [
                    {
                        "name": "Zhuoming Chen"
                    },
                    {
                        "name": "Ranajoy Sadhukhan"
                    },
                    {
                        "name": "Zihao Ye"
                    },
                    {
                        "name": "Yang Zhou"
                    },
                    {
                        "name": "Jianyu Zhang"
                    },
                    {
                        "name": "Niklas Nolte"
                    },
                    {
                        "name": "Yuandong Tian"
                    },
                    {
                        "name": "Matthijs Douze"
                    },
                    {
                        "name": "Leon Bottou"
                    },
                    {
                        "name": "Zhihao Jia"
                    },
                    {
                        "name": "Beidi Chen"
                    }
                ],
                "author_detail": {
                    "name": "Beidi Chen"
                },
                "author": "Beidi Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.16179v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.16179v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.13779v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.13779v1",
                "updated": "2024-12-18T12:16:41Z",
                "updated_parsed": [
                    2024,
                    12,
                    18,
                    12,
                    16,
                    41,
                    2,
                    353,
                    0
                ],
                "published": "2024-12-18T12:16:41Z",
                "published_parsed": [
                    2024,
                    12,
                    18,
                    12,
                    16,
                    41,
                    2,
                    353,
                    0
                ],
                "title": "Rehearsal-Free Continual Federated Learning with Synergistic\n  Regularization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Rehearsal-Free Continual Federated Learning with Synergistic\n  Regularization"
                },
                "summary": "Continual Federated Learning (CFL) allows distributed devices to\ncollaboratively learn novel concepts from continuously shifting training data\nwhile avoiding knowledge forgetting of previously seen tasks. To tackle this\nchallenge, most current CFL approaches rely on extensive rehearsal of previous\ndata. Despite effectiveness, rehearsal comes at a cost to memory, and it may\nalso violate data privacy. Considering these, we seek to apply regularization\ntechniques to CFL by considering their cost-efficient properties that do not\nrequire sample caching or rehearsal. Specifically, we first apply traditional\nregularization techniques to CFL and observe that existing regularization\ntechniques, especially synaptic intelligence, can achieve promising results\nunder homogeneous data distribution but fail when the data is heterogeneous.\nBased on this observation, we propose a simple yet effective regularization\nalgorithm for CFL named FedSSI, which tailors the synaptic intelligence for the\nCFL with heterogeneous data settings. FedSSI can not only reduce computational\noverhead without rehearsal but also address the data heterogeneity issue.\nExtensive experiments show that FedSSI achieves superior performance compared\nto state-of-the-art methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Continual Federated Learning (CFL) allows distributed devices to\ncollaboratively learn novel concepts from continuously shifting training data\nwhile avoiding knowledge forgetting of previously seen tasks. To tackle this\nchallenge, most current CFL approaches rely on extensive rehearsal of previous\ndata. Despite effectiveness, rehearsal comes at a cost to memory, and it may\nalso violate data privacy. Considering these, we seek to apply regularization\ntechniques to CFL by considering their cost-efficient properties that do not\nrequire sample caching or rehearsal. Specifically, we first apply traditional\nregularization techniques to CFL and observe that existing regularization\ntechniques, especially synaptic intelligence, can achieve promising results\nunder homogeneous data distribution but fail when the data is heterogeneous.\nBased on this observation, we propose a simple yet effective regularization\nalgorithm for CFL named FedSSI, which tailors the synaptic intelligence for the\nCFL with heterogeneous data settings. FedSSI can not only reduce computational\noverhead without rehearsal but also address the data heterogeneity issue.\nExtensive experiments show that FedSSI achieves superior performance compared\nto state-of-the-art methods."
                },
                "authors": [
                    {
                        "name": "Yichen Li"
                    },
                    {
                        "name": "Yuying Wang"
                    },
                    {
                        "name": "Tianzhe Xiao"
                    },
                    {
                        "name": "Haozhao Wang"
                    },
                    {
                        "name": "Yining Qi"
                    },
                    {
                        "name": "Ruixuan Li"
                    }
                ],
                "author_detail": {
                    "name": "Ruixuan Li"
                },
                "author": "Ruixuan Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.13779v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.13779v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.13771v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.13771v1",
                "updated": "2024-12-18T12:07:58Z",
                "updated_parsed": [
                    2024,
                    12,
                    18,
                    12,
                    7,
                    58,
                    2,
                    353,
                    0
                ],
                "published": "2024-12-18T12:07:58Z",
                "published_parsed": [
                    2024,
                    12,
                    18,
                    12,
                    7,
                    58,
                    2,
                    353,
                    0
                ],
                "title": "Semantic Convergence: Harmonizing Recommender Systems via Two-Stage\n  Alignment and Behavioral Semantic Tokenization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Semantic Convergence: Harmonizing Recommender Systems via Two-Stage\n  Alignment and Behavioral Semantic Tokenization"
                },
                "summary": "Large language models (LLMs), endowed with exceptional reasoning\ncapabilities, are adept at discerning profound user interests from historical\nbehaviors, thereby presenting a promising avenue for the advancement of\nrecommendation systems. However, a notable discrepancy persists between the\nsparse collaborative semantics typically found in recommendation systems and\nthe dense token representations within LLMs. In our study, we propose a novel\nframework that harmoniously merges traditional recommendation models with the\nprowess of LLMs. We initiate this integration by transforming ItemIDs into\nsequences that align semantically with the LLMs space, through the proposed\nAlignment Tokenization module. Additionally, we design a series of specialized\nsupervised learning tasks aimed at aligning collaborative signals with the\nsubtleties of natural language semantics. To ensure practical applicability, we\noptimize online inference by pre-caching the top-K results for each user,\nreducing latency and improving effciency. Extensive experimental evidence\nindicates that our model markedly improves recall metrics and displays\nremarkable scalability of recommendation systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs), endowed with exceptional reasoning\ncapabilities, are adept at discerning profound user interests from historical\nbehaviors, thereby presenting a promising avenue for the advancement of\nrecommendation systems. However, a notable discrepancy persists between the\nsparse collaborative semantics typically found in recommendation systems and\nthe dense token representations within LLMs. In our study, we propose a novel\nframework that harmoniously merges traditional recommendation models with the\nprowess of LLMs. We initiate this integration by transforming ItemIDs into\nsequences that align semantically with the LLMs space, through the proposed\nAlignment Tokenization module. Additionally, we design a series of specialized\nsupervised learning tasks aimed at aligning collaborative signals with the\nsubtleties of natural language semantics. To ensure practical applicability, we\noptimize online inference by pre-caching the top-K results for each user,\nreducing latency and improving effciency. Extensive experimental evidence\nindicates that our model markedly improves recall metrics and displays\nremarkable scalability of recommendation systems."
                },
                "authors": [
                    {
                        "name": "Guanghan Li"
                    },
                    {
                        "name": "Xun Zhang"
                    },
                    {
                        "name": "Yufei Zhang"
                    },
                    {
                        "name": "Yifan Yin"
                    },
                    {
                        "name": "Guojun Yin"
                    },
                    {
                        "name": "Wei Lin"
                    }
                ],
                "author_detail": {
                    "name": "Wei Lin"
                },
                "author": "Wei Lin",
                "arxiv_comment": "7 pages, 3 figures, AAAI 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.13771v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.13771v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.15024v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.15024v2",
                "updated": "2024-12-18T09:47:25Z",
                "updated_parsed": [
                    2024,
                    12,
                    18,
                    9,
                    47,
                    25,
                    2,
                    353,
                    0
                ],
                "published": "2024-11-22T15:55:19Z",
                "published_parsed": [
                    2024,
                    11,
                    22,
                    15,
                    55,
                    19,
                    4,
                    327,
                    0
                ],
                "title": "DyCoke: Dynamic Compression of Tokens for Fast Video Large Language\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DyCoke: Dynamic Compression of Tokens for Fast Video Large Language\n  Models"
                },
                "summary": "Video large language models (VLLMs) have significantly advanced recently in\nprocessing complex video content, yet their inference efficiency remains\nconstrained because of the high computational cost stemming from the thousands\nof visual tokens generated from the video inputs. We empirically observe that,\nunlike single image inputs, VLLMs typically attend visual tokens from different\nframes at different decoding iterations, making a one-shot pruning strategy\nprone to removing important tokens by mistake. Motivated by this, we present\nDyCoke, a training-free token compression method to optimize token\nrepresentation and accelerate VLLMs. DyCoke incorporates a plug-and-play\ntemporal compression module to minimize temporal redundancy by merging\nredundant tokens across frames, and applies dynamic KV cache reduction to prune\nspatially redundant tokens selectively. It ensures high-quality inference by\ndynamically retaining the critical tokens at each decoding step. Extensive\nexperimental results demonstrate that DyCoke can outperform the prior SoTA\ncounterparts, achieving 1.5X inference speedup, 1.4X memory reduction against\nthe baseline VLLM, while still improving the performance, with no training.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Video large language models (VLLMs) have significantly advanced recently in\nprocessing complex video content, yet their inference efficiency remains\nconstrained because of the high computational cost stemming from the thousands\nof visual tokens generated from the video inputs. We empirically observe that,\nunlike single image inputs, VLLMs typically attend visual tokens from different\nframes at different decoding iterations, making a one-shot pruning strategy\nprone to removing important tokens by mistake. Motivated by this, we present\nDyCoke, a training-free token compression method to optimize token\nrepresentation and accelerate VLLMs. DyCoke incorporates a plug-and-play\ntemporal compression module to minimize temporal redundancy by merging\nredundant tokens across frames, and applies dynamic KV cache reduction to prune\nspatially redundant tokens selectively. It ensures high-quality inference by\ndynamically retaining the critical tokens at each decoding step. Extensive\nexperimental results demonstrate that DyCoke can outperform the prior SoTA\ncounterparts, achieving 1.5X inference speedup, 1.4X memory reduction against\nthe baseline VLLM, while still improving the performance, with no training."
                },
                "authors": [
                    {
                        "name": "Keda Tao"
                    },
                    {
                        "name": "Can Qin"
                    },
                    {
                        "name": "Haoxuan You"
                    },
                    {
                        "name": "Yang Sui"
                    },
                    {
                        "name": "Huan Wang"
                    }
                ],
                "author_detail": {
                    "name": "Huan Wang"
                },
                "author": "Huan Wang",
                "arxiv_comment": "12 pages, 6 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.15024v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.15024v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.13649v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.13649v1",
                "updated": "2024-12-18T09:27:33Z",
                "updated_parsed": [
                    2024,
                    12,
                    18,
                    9,
                    27,
                    33,
                    2,
                    353,
                    0
                ],
                "published": "2024-12-18T09:27:33Z",
                "published_parsed": [
                    2024,
                    12,
                    18,
                    9,
                    27,
                    33,
                    2,
                    353,
                    0
                ],
                "title": "SCOPE: Optimizing Key-Value Cache Compression in Long-context Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SCOPE: Optimizing Key-Value Cache Compression in Long-context Generation"
                },
                "summary": "Key-Value (KV) cache has become a bottleneck of LLMs for long-context\ngeneration. Despite the numerous efforts in this area, the optimization for the\ndecoding phase is generally ignored. However, we believe such optimization is\ncrucial, especially for long-output generation tasks based on the following two\nobservations: (i) Excessive compression during the prefill phase, which\nrequires specific full context impairs the comprehension of the reasoning task;\n(ii) Deviation of heavy hitters occurs in the reasoning tasks with long\noutputs. Therefore, SCOPE, a simple yet efficient framework that separately\nperforms KV cache optimization during the prefill and decoding phases, is\nintroduced. Specifically, the KV cache during the prefill phase is preserved to\nmaintain the essential information, while a novel strategy based on sliding is\nproposed to select essential heavy hitters for the decoding phase. Memory usage\nand memory transfer are further optimized using adaptive and discontinuous\nstrategies. Extensive experiments on LongGenBench show the effectiveness and\ngeneralization of SCOPE and its compatibility as a plug-in to other\nprefill-only KV compression methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Key-Value (KV) cache has become a bottleneck of LLMs for long-context\ngeneration. Despite the numerous efforts in this area, the optimization for the\ndecoding phase is generally ignored. However, we believe such optimization is\ncrucial, especially for long-output generation tasks based on the following two\nobservations: (i) Excessive compression during the prefill phase, which\nrequires specific full context impairs the comprehension of the reasoning task;\n(ii) Deviation of heavy hitters occurs in the reasoning tasks with long\noutputs. Therefore, SCOPE, a simple yet efficient framework that separately\nperforms KV cache optimization during the prefill and decoding phases, is\nintroduced. Specifically, the KV cache during the prefill phase is preserved to\nmaintain the essential information, while a novel strategy based on sliding is\nproposed to select essential heavy hitters for the decoding phase. Memory usage\nand memory transfer are further optimized using adaptive and discontinuous\nstrategies. Extensive experiments on LongGenBench show the effectiveness and\ngeneralization of SCOPE and its compatibility as a plug-in to other\nprefill-only KV compression methods."
                },
                "authors": [
                    {
                        "name": "Jialong Wu"
                    },
                    {
                        "name": "Zhenglin Wang"
                    },
                    {
                        "name": "Linhai Zhang"
                    },
                    {
                        "name": "Yilong Lai"
                    },
                    {
                        "name": "Yulan He"
                    },
                    {
                        "name": "Deyu Zhou"
                    }
                ],
                "author_detail": {
                    "name": "Deyu Zhou"
                },
                "author": "Deyu Zhou",
                "arxiv_comment": "Preprint",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.13649v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.13649v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.08584v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.08584v2",
                "updated": "2024-12-18T07:45:11Z",
                "updated_parsed": [
                    2024,
                    12,
                    18,
                    7,
                    45,
                    11,
                    2,
                    353,
                    0
                ],
                "published": "2024-10-11T07:24:21Z",
                "published_parsed": [
                    2024,
                    10,
                    11,
                    7,
                    24,
                    21,
                    4,
                    285,
                    0
                ],
                "title": "ZipVL: Efficient Large Vision-Language Models with Dynamic Token\n  Sparsification",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ZipVL: Efficient Large Vision-Language Models with Dynamic Token\n  Sparsification"
                },
                "summary": "The efficiency of large vision-language models (LVLMs) is constrained by the\ncomputational bottleneck of the attention mechanism during the prefill phase\nand the memory bottleneck of fetching the key-value (KV) cache in the decoding\nphase, particularly in scenarios involving high-resolution images or videos.\nVisual content often exhibits substantial redundancy, resulting in highly\nsparse attention maps within LVLMs. This sparsity can be leveraged to\naccelerate attention computation or compress the KV cache through various\napproaches. However, most studies focus on addressing only one of these\nbottlenecks and do not adequately support dynamic adjustment of sparsity\nconcerning distinct layers or tasks. In this paper, we present ZipVL, an\nefficient inference framework designed for LVLMs through a dynamic ratio\nallocation strategy of important tokens. This ratio is adaptively determined\nbased on the layer-specific distribution of attention scores, rather than fixed\nhyper-parameters, thereby improving efficiency for less complex tasks while\nmaintaining high performance for more challenging ones. Then we select\nimportant tokens based on their normalized attention scores and perform sparse\nattention mechanism solely on those important tokens, reducing the latency in\nthe prefill phase. Tokens deemed less important will be discarded to reduce KV\ncache size, alleviating the memory bottleneck in the decoding phase. Our\nexperiments demonstrate that ZipVL can accelerate the prefill phase by\n2.3$\\times$ and improve decoding throughput by 2.8$\\times$, with a minimal\naccuracy reduction of only 0.5\\% on VQAv2 benchmark over LLaVA-Next-13B model,\neffectively enhancing the generation efficiency of LVLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The efficiency of large vision-language models (LVLMs) is constrained by the\ncomputational bottleneck of the attention mechanism during the prefill phase\nand the memory bottleneck of fetching the key-value (KV) cache in the decoding\nphase, particularly in scenarios involving high-resolution images or videos.\nVisual content often exhibits substantial redundancy, resulting in highly\nsparse attention maps within LVLMs. This sparsity can be leveraged to\naccelerate attention computation or compress the KV cache through various\napproaches. However, most studies focus on addressing only one of these\nbottlenecks and do not adequately support dynamic adjustment of sparsity\nconcerning distinct layers or tasks. In this paper, we present ZipVL, an\nefficient inference framework designed for LVLMs through a dynamic ratio\nallocation strategy of important tokens. This ratio is adaptively determined\nbased on the layer-specific distribution of attention scores, rather than fixed\nhyper-parameters, thereby improving efficiency for less complex tasks while\nmaintaining high performance for more challenging ones. Then we select\nimportant tokens based on their normalized attention scores and perform sparse\nattention mechanism solely on those important tokens, reducing the latency in\nthe prefill phase. Tokens deemed less important will be discarded to reduce KV\ncache size, alleviating the memory bottleneck in the decoding phase. Our\nexperiments demonstrate that ZipVL can accelerate the prefill phase by\n2.3$\\times$ and improve decoding throughput by 2.8$\\times$, with a minimal\naccuracy reduction of only 0.5\\% on VQAv2 benchmark over LLaVA-Next-13B model,\neffectively enhancing the generation efficiency of LVLMs."
                },
                "authors": [
                    {
                        "name": "Yefei He"
                    },
                    {
                        "name": "Feng Chen"
                    },
                    {
                        "name": "Jing Liu"
                    },
                    {
                        "name": "Wenqi Shao"
                    },
                    {
                        "name": "Hong Zhou"
                    },
                    {
                        "name": "Kaipeng Zhang"
                    },
                    {
                        "name": "Bohan Zhuang"
                    }
                ],
                "author_detail": {
                    "name": "Bohan Zhuang"
                },
                "author": "Bohan Zhuang",
                "arxiv_comment": "13 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.08584v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.08584v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            }
        ]
    },
    {
        "keyword": "LLM Inference",
        "arxiv_results": [
            {
                "id": "http://arxiv.org/abs/2501.09015v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.09015v1",
                "updated": "2025-01-15T18:57:33Z",
                "updated_parsed": [
                    2025,
                    1,
                    15,
                    18,
                    57,
                    33,
                    2,
                    15,
                    0
                ],
                "published": "2025-01-15T18:57:33Z",
                "published_parsed": [
                    2025,
                    1,
                    15,
                    18,
                    57,
                    33,
                    2,
                    15,
                    0
                ],
                "title": "Family-wise Error Rate Control with E-values",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Family-wise Error Rate Control with E-values"
                },
                "summary": "The closure principle is a standard tool for achieving family-wise error rate\n(FWER) control in multiple testing problems. In general, the computational cost\nfor closed testing can be exponential in the number of hypotheses. The\ncelebrated graphical approach of FWER control overcomes the computational\nhurdle by using weighted Bonferroni local tests on p-values with appropriately\nchosen weights. In this study, we extend the graphical approach to e-values.\nWith valid e-values -- common in settings of sequential hypothesis testing or\nuniversal inference for irregular parametric models -- we can derive strictly\nmore powerful local tests based on weighted averages of e-values. Consequently,\nthis e-value-based closed test is more powerful than the corresponding\ngraphical approach with inverse e-values as p-values. Although the\ncomputational shortcuts for the p-value-based graphical approach are not\napplicable, we develop efficient polynomial-time algorithms using dynamic\nprogramming for e-value-based graphical approaches with any directed acyclic\ngraph. For special graphs, such as those used in the Holm's procedure and\nfallback procedure, we develop tailored algorithms with computation cost linear\nin the number of hypotheses, up to logarithmic factors.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The closure principle is a standard tool for achieving family-wise error rate\n(FWER) control in multiple testing problems. In general, the computational cost\nfor closed testing can be exponential in the number of hypotheses. The\ncelebrated graphical approach of FWER control overcomes the computational\nhurdle by using weighted Bonferroni local tests on p-values with appropriately\nchosen weights. In this study, we extend the graphical approach to e-values.\nWith valid e-values -- common in settings of sequential hypothesis testing or\nuniversal inference for irregular parametric models -- we can derive strictly\nmore powerful local tests based on weighted averages of e-values. Consequently,\nthis e-value-based closed test is more powerful than the corresponding\ngraphical approach with inverse e-values as p-values. Although the\ncomputational shortcuts for the p-value-based graphical approach are not\napplicable, we develop efficient polynomial-time algorithms using dynamic\nprogramming for e-value-based graphical approaches with any directed acyclic\ngraph. For special graphs, such as those used in the Holm's procedure and\nfallback procedure, we develop tailored algorithms with computation cost linear\nin the number of hypotheses, up to logarithmic factors."
                },
                "authors": [
                    {
                        "name": "Will Hartog"
                    },
                    {
                        "name": "Lihua Lei"
                    }
                ],
                "author_detail": {
                    "name": "Lihua Lei"
                },
                "author": "Lihua Lei",
                "arxiv_comment": "16 pages, 5 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.09015v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.09015v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ME",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "62J15",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.09012v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.09012v1",
                "updated": "2025-01-15T18:56:22Z",
                "updated_parsed": [
                    2025,
                    1,
                    15,
                    18,
                    56,
                    22,
                    2,
                    15,
                    0
                ],
                "published": "2025-01-15T18:56:22Z",
                "published_parsed": [
                    2025,
                    1,
                    15,
                    18,
                    56,
                    22,
                    2,
                    15,
                    0
                ],
                "title": "Multimodal LLMs Can Reason about Aesthetics in Zero-Shot",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multimodal LLMs Can Reason about Aesthetics in Zero-Shot"
                },
                "summary": "We present the first study on how Multimodal LLMs' (MLLMs) reasoning ability\nshall be elicited to evaluate the aesthetics of artworks. To facilitate this\ninvestigation, we construct MM-StyleBench, a novel high-quality dataset for\nbenchmarking artistic stylization. We then develop a principled method for\nhuman preference modeling and perform a systematic correlation analysis between\nMLLMs' responses and human preference. Our experiments reveal an inherent\nhallucination issue of MLLMs in art evaluation, associated with response\nsubjectivity. ArtCoT is proposed, demonstrating that art-specific task\ndecomposition and the use of concrete language boost MLLMs' reasoning ability\nfor aesthetics. Our findings offer valuable insights into MLLMs for art and can\nbenefit a wide range of downstream applications, such as style transfer and\nartistic image generation. Code available at\nhttps://github.com/songrise/MLLM4Art.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present the first study on how Multimodal LLMs' (MLLMs) reasoning ability\nshall be elicited to evaluate the aesthetics of artworks. To facilitate this\ninvestigation, we construct MM-StyleBench, a novel high-quality dataset for\nbenchmarking artistic stylization. We then develop a principled method for\nhuman preference modeling and perform a systematic correlation analysis between\nMLLMs' responses and human preference. Our experiments reveal an inherent\nhallucination issue of MLLMs in art evaluation, associated with response\nsubjectivity. ArtCoT is proposed, demonstrating that art-specific task\ndecomposition and the use of concrete language boost MLLMs' reasoning ability\nfor aesthetics. Our findings offer valuable insights into MLLMs for art and can\nbenefit a wide range of downstream applications, such as style transfer and\nartistic image generation. Code available at\nhttps://github.com/songrise/MLLM4Art."
                },
                "authors": [
                    {
                        "name": "Ruixiang Jiang"
                    },
                    {
                        "name": "Changwen Chen"
                    }
                ],
                "author_detail": {
                    "name": "Changwen Chen"
                },
                "author": "Changwen Chen",
                "arxiv_comment": "WIP, Homepage https://github.com/songrise/MLLM4Art",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.09012v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.09012v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.09009v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.09009v1",
                "updated": "2025-01-15T18:50:52Z",
                "updated_parsed": [
                    2025,
                    1,
                    15,
                    18,
                    50,
                    52,
                    2,
                    15,
                    0
                ],
                "published": "2025-01-15T18:50:52Z",
                "published_parsed": [
                    2025,
                    1,
                    15,
                    18,
                    50,
                    52,
                    2,
                    15,
                    0
                ],
                "title": "Towards Fast, Specialized Machine Learning Force Fields: Distilling\n  Foundation Models via Energy Hessians",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Towards Fast, Specialized Machine Learning Force Fields: Distilling\n  Foundation Models via Energy Hessians"
                },
                "summary": "The foundation model (FM) paradigm is transforming Machine Learning Force\nFields (MLFFs), leveraging general-purpose representations and scalable\ntraining to perform a variety of computational chemistry tasks. Although MLFF\nFMs have begun to close the accuracy gap relative to first-principles methods,\nthere is still a strong need for faster inference speed. Additionally, while\nresearch is increasingly focused on general-purpose models which transfer\nacross chemical space, practitioners typically only study a small subset of\nsystems at a given time. This underscores the need for fast, specialized MLFFs\nrelevant to specific downstream applications, which preserve test-time physical\nsoundness while maintaining train-time scalability. In this work, we introduce\na method for transferring general-purpose representations from MLFF foundation\nmodels to smaller, faster MLFFs specialized to specific regions of chemical\nspace. We formulate our approach as a knowledge distillation procedure, where\nthe smaller \"student\" MLFF is trained to match the Hessians of the energy\npredictions of the \"teacher\" foundation model. Our specialized MLFFs can be up\nto 20 $\\times$ faster than the original foundation model, while retaining, and\nin some cases exceeding, its performance and that of undistilled models. We\nalso show that distilling from a teacher model with a direct force\nparameterization into a student model trained with conservative forces (i.e.,\ncomputed as derivatives of the potential energy) successfully leverages the\nrepresentations from the large-scale teacher for improved accuracy, while\nmaintaining energy conservation during test-time molecular dynamics\nsimulations. More broadly, our work suggests a new paradigm for MLFF\ndevelopment, in which foundation models are released along with smaller,\nspecialized simulation \"engines\" for common chemical subsets.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The foundation model (FM) paradigm is transforming Machine Learning Force\nFields (MLFFs), leveraging general-purpose representations and scalable\ntraining to perform a variety of computational chemistry tasks. Although MLFF\nFMs have begun to close the accuracy gap relative to first-principles methods,\nthere is still a strong need for faster inference speed. Additionally, while\nresearch is increasingly focused on general-purpose models which transfer\nacross chemical space, practitioners typically only study a small subset of\nsystems at a given time. This underscores the need for fast, specialized MLFFs\nrelevant to specific downstream applications, which preserve test-time physical\nsoundness while maintaining train-time scalability. In this work, we introduce\na method for transferring general-purpose representations from MLFF foundation\nmodels to smaller, faster MLFFs specialized to specific regions of chemical\nspace. We formulate our approach as a knowledge distillation procedure, where\nthe smaller \"student\" MLFF is trained to match the Hessians of the energy\npredictions of the \"teacher\" foundation model. Our specialized MLFFs can be up\nto 20 $\\times$ faster than the original foundation model, while retaining, and\nin some cases exceeding, its performance and that of undistilled models. We\nalso show that distilling from a teacher model with a direct force\nparameterization into a student model trained with conservative forces (i.e.,\ncomputed as derivatives of the potential energy) successfully leverages the\nrepresentations from the large-scale teacher for improved accuracy, while\nmaintaining energy conservation during test-time molecular dynamics\nsimulations. More broadly, our work suggests a new paradigm for MLFF\ndevelopment, in which foundation models are released along with smaller,\nspecialized simulation \"engines\" for common chemical subsets."
                },
                "authors": [
                    {
                        "name": "Ishan Amin"
                    },
                    {
                        "name": "Sanjeev Raja"
                    },
                    {
                        "name": "Aditi Krishnapriyan"
                    }
                ],
                "author_detail": {
                    "name": "Aditi Krishnapriyan"
                },
                "author": "Aditi Krishnapriyan",
                "arxiv_comment": "Under Review at ICLR 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.09009v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.09009v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.chem-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.chem-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cond-mat.mtrl-sci",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "physics.bio-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.09004v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.09004v1",
                "updated": "2025-01-15T18:37:08Z",
                "updated_parsed": [
                    2025,
                    1,
                    15,
                    18,
                    37,
                    8,
                    2,
                    15,
                    0
                ],
                "published": "2025-01-15T18:37:08Z",
                "published_parsed": [
                    2025,
                    1,
                    15,
                    18,
                    37,
                    8,
                    2,
                    15,
                    0
                ],
                "title": "Aegis2.0: A Diverse AI Safety Dataset and Risks Taxonomy for Alignment\n  of LLM Guardrails",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Aegis2.0: A Diverse AI Safety Dataset and Risks Taxonomy for Alignment\n  of LLM Guardrails"
                },
                "summary": "As Large Language Models (LLMs) and generative AI become increasingly\nwidespread, concerns about content safety have grown in parallel. Currently,\nthere is a clear lack of high-quality, human-annotated datasets that address\nthe full spectrum of LLM-related safety risks and are usable for commercial\napplications. To bridge this gap, we propose a comprehensive and adaptable\ntaxonomy for categorizing safety risks, structured into 12 top-level hazard\ncategories with an extension to 9 fine-grained subcategories. This taxonomy is\ndesigned to meet the diverse requirements of downstream users, offering more\ngranular and flexible tools for managing various risk types. Using a hybrid\ndata generation pipeline that combines human annotations with a multi-LLM\n\"jury\" system to assess the safety of responses, we obtain Aegis 2.0, a\ncarefully curated collection of 34,248 samples of human-LLM interactions,\nannotated according to our proposed taxonomy. To validate its effectiveness, we\ndemonstrate that several lightweight models, trained using parameter-efficient\ntechniques on Aegis 2.0, achieve performance competitive with leading safety\nmodels fully fine-tuned on much larger, non-commercial datasets. In addition,\nwe introduce a novel training blend that combines safety with topic following\ndata.This approach enhances the adaptability of guard models, enabling them to\ngeneralize to new risk categories defined during inference. We plan to\nopen-source Aegis 2.0 data and models to the research community to aid in the\nsafety guardrailing of LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As Large Language Models (LLMs) and generative AI become increasingly\nwidespread, concerns about content safety have grown in parallel. Currently,\nthere is a clear lack of high-quality, human-annotated datasets that address\nthe full spectrum of LLM-related safety risks and are usable for commercial\napplications. To bridge this gap, we propose a comprehensive and adaptable\ntaxonomy for categorizing safety risks, structured into 12 top-level hazard\ncategories with an extension to 9 fine-grained subcategories. This taxonomy is\ndesigned to meet the diverse requirements of downstream users, offering more\ngranular and flexible tools for managing various risk types. Using a hybrid\ndata generation pipeline that combines human annotations with a multi-LLM\n\"jury\" system to assess the safety of responses, we obtain Aegis 2.0, a\ncarefully curated collection of 34,248 samples of human-LLM interactions,\nannotated according to our proposed taxonomy. To validate its effectiveness, we\ndemonstrate that several lightweight models, trained using parameter-efficient\ntechniques on Aegis 2.0, achieve performance competitive with leading safety\nmodels fully fine-tuned on much larger, non-commercial datasets. In addition,\nwe introduce a novel training blend that combines safety with topic following\ndata.This approach enhances the adaptability of guard models, enabling them to\ngeneralize to new risk categories defined during inference. We plan to\nopen-source Aegis 2.0 data and models to the research community to aid in the\nsafety guardrailing of LLMs."
                },
                "authors": [
                    {
                        "name": "Shaona Ghosh"
                    },
                    {
                        "name": "Prasoon Varshney"
                    },
                    {
                        "name": "Makesh Narsimhan Sreedhar"
                    },
                    {
                        "name": "Aishwarya Padmakumar"
                    },
                    {
                        "name": "Traian Rebedea"
                    },
                    {
                        "name": "Jibin Rajan Varghese"
                    },
                    {
                        "name": "Christopher Parisien"
                    }
                ],
                "author_detail": {
                    "name": "Christopher Parisien"
                },
                "author": "Christopher Parisien",
                "arxiv_comment": "arXiv admin note: text overlap with arXiv:2404.05993",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.09004v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.09004v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.09000v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.09000v1",
                "updated": "2025-01-15T18:29:32Z",
                "updated_parsed": [
                    2025,
                    1,
                    15,
                    18,
                    29,
                    32,
                    2,
                    15,
                    0
                ],
                "published": "2025-01-15T18:29:32Z",
                "published_parsed": [
                    2025,
                    1,
                    15,
                    18,
                    29,
                    32,
                    2,
                    15,
                    0
                ],
                "title": "Bayesian analysis of analog gravity systems with the Rezzolla-Zhidenko\n  metric",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Bayesian analysis of analog gravity systems with the Rezzolla-Zhidenko\n  metric"
                },
                "summary": "Analog gravity systems have the unique opportunity to probe theoretical\naspects of black hole physics in a controlled laboratory environment that one\ncannot easily observe for astrophysical black holes. In this work, we address\nthe question of whether one could use controlled initial perturbations to\nexcite the black hole ringdown and infer the effective black hole metric. Using\na theory-agnostic ansatz for the effective metric described by the\nRezzolla-Zhidenko metric and evolving perturbations on that background, we\nquantify with Bayesian analysis what regions of the effective spacetime could\nbe constrained in experiments. In contrast to standard ringdown analyses based\non quasi-normal mode extraction, a laboratory-controlled setup, in combination\nwith our framework, allows one to model the entire signal, including the prompt\nresponse and possible effects of late-time tails. Therefore, it has the\nintriguing advantage of not relying on start and end times when the\nsuperposition of quasi-normal modes is a good signal approximation. It also\navoids the non-trivial question of how many modes are present. We demonstrate\nthat this approach is feasible in principle and discuss opportunities beyond\nthis study.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Analog gravity systems have the unique opportunity to probe theoretical\naspects of black hole physics in a controlled laboratory environment that one\ncannot easily observe for astrophysical black holes. In this work, we address\nthe question of whether one could use controlled initial perturbations to\nexcite the black hole ringdown and infer the effective black hole metric. Using\na theory-agnostic ansatz for the effective metric described by the\nRezzolla-Zhidenko metric and evolving perturbations on that background, we\nquantify with Bayesian analysis what regions of the effective spacetime could\nbe constrained in experiments. In contrast to standard ringdown analyses based\non quasi-normal mode extraction, a laboratory-controlled setup, in combination\nwith our framework, allows one to model the entire signal, including the prompt\nresponse and possible effects of late-time tails. Therefore, it has the\nintriguing advantage of not relying on start and end times when the\nsuperposition of quasi-normal modes is a good signal approximation. It also\navoids the non-trivial question of how many modes are present. We demonstrate\nthat this approach is feasible in principle and discuss opportunities beyond\nthis study."
                },
                "authors": [
                    {
                        "name": "Saulo Albuquerque"
                    },
                    {
                        "name": "Sebastian H. Völkel"
                    }
                ],
                "author_detail": {
                    "name": "Sebastian H. Völkel"
                },
                "author": "Sebastian H. Völkel",
                "arxiv_comment": "13 pages, 11 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.09000v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.09000v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "gr-qc",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "gr-qc",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.06848v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.06848v2",
                "updated": "2025-01-15T18:28:37Z",
                "updated_parsed": [
                    2025,
                    1,
                    15,
                    18,
                    28,
                    37,
                    2,
                    15,
                    0
                ],
                "published": "2025-01-12T15:34:24Z",
                "published_parsed": [
                    2025,
                    1,
                    12,
                    15,
                    34,
                    24,
                    6,
                    12,
                    0
                ],
                "title": "A General Framework for Inference-time Scaling and Steering of Diffusion\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A General Framework for Inference-time Scaling and Steering of Diffusion\n  Models"
                },
                "summary": "Diffusion models produce impressive results in modalities ranging from images\nand video to protein design and text. However, generating samples with\nuser-specified properties remains a challenge. Recent research proposes\nfine-tuning models to maximize rewards that capture desired properties, but\nthese methods require expensive training and are prone to mode collapse. In\nthis work, we propose Feynman Kac (FK) steering, an inference-time framework\nfor steering diffusion models with reward functions. FK steering works by\nsampling a system of multiple interacting diffusion processes, called\nparticles, and resampling particles at intermediate steps based on scores\ncomputed using functions called potentials. Potentials are defined using\nrewards for intermediate states and are selected such that a high value\nindicates that the particle will yield a high-reward sample. We explore various\nchoices of potentials, intermediate rewards, and samplers. We evaluate FK\nsteering on text-to-image and text diffusion models. For steering text-to-image\nmodels with a human preference reward, we find that FK steering a 0.8B\nparameter model outperforms a 2.6B parameter fine-tuned model on prompt\nfidelity, with faster sampling and no training. For steering text diffusion\nmodels with rewards for text quality and specific text attributes, we find that\nFK steering generates lower perplexity, more linguistically acceptable outputs\nand enables gradient-free control of attributes like toxicity. Our results\ndemonstrate that inference-time scaling and steering of diffusion models, even\nwith off-the-shelf rewards, can provide significant sample quality gains and\ncontrollability benefits. Code is available at\nhttps://github.com/zacharyhorvitz/Fk-Diffusion-Steering .",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion models produce impressive results in modalities ranging from images\nand video to protein design and text. However, generating samples with\nuser-specified properties remains a challenge. Recent research proposes\nfine-tuning models to maximize rewards that capture desired properties, but\nthese methods require expensive training and are prone to mode collapse. In\nthis work, we propose Feynman Kac (FK) steering, an inference-time framework\nfor steering diffusion models with reward functions. FK steering works by\nsampling a system of multiple interacting diffusion processes, called\nparticles, and resampling particles at intermediate steps based on scores\ncomputed using functions called potentials. Potentials are defined using\nrewards for intermediate states and are selected such that a high value\nindicates that the particle will yield a high-reward sample. We explore various\nchoices of potentials, intermediate rewards, and samplers. We evaluate FK\nsteering on text-to-image and text diffusion models. For steering text-to-image\nmodels with a human preference reward, we find that FK steering a 0.8B\nparameter model outperforms a 2.6B parameter fine-tuned model on prompt\nfidelity, with faster sampling and no training. For steering text diffusion\nmodels with rewards for text quality and specific text attributes, we find that\nFK steering generates lower perplexity, more linguistically acceptable outputs\nand enables gradient-free control of attributes like toxicity. Our results\ndemonstrate that inference-time scaling and steering of diffusion models, even\nwith off-the-shelf rewards, can provide significant sample quality gains and\ncontrollability benefits. Code is available at\nhttps://github.com/zacharyhorvitz/Fk-Diffusion-Steering ."
                },
                "authors": [
                    {
                        "name": "Raghav Singhal"
                    },
                    {
                        "name": "Zachary Horvitz"
                    },
                    {
                        "name": "Ryan Teehan"
                    },
                    {
                        "name": "Mengye Ren"
                    },
                    {
                        "name": "Zhou Yu"
                    },
                    {
                        "name": "Kathleen McKeown"
                    },
                    {
                        "name": "Rajesh Ranganath"
                    }
                ],
                "author_detail": {
                    "name": "Rajesh Ranganath"
                },
                "author": "Rajesh Ranganath",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.06848v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.06848v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.08102v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.08102v2",
                "updated": "2025-01-15T18:10:00Z",
                "updated_parsed": [
                    2025,
                    1,
                    15,
                    18,
                    10,
                    0,
                    2,
                    15,
                    0
                ],
                "published": "2025-01-14T13:19:47Z",
                "published_parsed": [
                    2025,
                    1,
                    14,
                    13,
                    19,
                    47,
                    1,
                    14,
                    0
                ],
                "title": "Consistency of Responses and Continuations Generated by Large Language\n  Models on Social Media",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Consistency of Responses and Continuations Generated by Large Language\n  Models on Social Media"
                },
                "summary": "Large Language Models (LLMs) demonstrate remarkable capabilities in text\ngeneration, yet their emotional consistency and semantic coherence in social\nmedia contexts remain insufficiently understood. This study investigates how\nLLMs handle emotional content and maintain semantic relationships through\ncontinuation and response tasks using two open-source models: Gemma and Llama.\nBy analyzing climate change discussions from Twitter and Reddit, we examine\nemotional transitions, intensity patterns, and semantic similarity between\nhuman-authored and LLM-generated content. Our findings reveal that while both\nmodels maintain high semantic coherence, they exhibit distinct emotional\npatterns: Gemma shows a tendency toward negative emotion amplification,\nparticularly anger, while maintaining certain positive emotions like optimism.\nLlama demonstrates superior emotional preservation across a broader spectrum of\naffects. Both models systematically generate responses with attenuated\nemotional intensity compared to human-authored content and show a bias toward\npositive emotions in response tasks. Additionally, both models maintain strong\nsemantic similarity with original texts, though performance varies between\ncontinuation and response tasks. These findings provide insights into LLMs'\nemotional and semantic processing capabilities, with implications for their\ndeployment in social media contexts and human-AI interaction design.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) demonstrate remarkable capabilities in text\ngeneration, yet their emotional consistency and semantic coherence in social\nmedia contexts remain insufficiently understood. This study investigates how\nLLMs handle emotional content and maintain semantic relationships through\ncontinuation and response tasks using two open-source models: Gemma and Llama.\nBy analyzing climate change discussions from Twitter and Reddit, we examine\nemotional transitions, intensity patterns, and semantic similarity between\nhuman-authored and LLM-generated content. Our findings reveal that while both\nmodels maintain high semantic coherence, they exhibit distinct emotional\npatterns: Gemma shows a tendency toward negative emotion amplification,\nparticularly anger, while maintaining certain positive emotions like optimism.\nLlama demonstrates superior emotional preservation across a broader spectrum of\naffects. Both models systematically generate responses with attenuated\nemotional intensity compared to human-authored content and show a bias toward\npositive emotions in response tasks. Additionally, both models maintain strong\nsemantic similarity with original texts, though performance varies between\ncontinuation and response tasks. These findings provide insights into LLMs'\nemotional and semantic processing capabilities, with implications for their\ndeployment in social media contexts and human-AI interaction design."
                },
                "authors": [
                    {
                        "name": "Wenlu Fan"
                    },
                    {
                        "name": "Yuqi Zhu"
                    },
                    {
                        "name": "Chenyang Wang"
                    },
                    {
                        "name": "Bin Wang"
                    },
                    {
                        "name": "Wentao Xu"
                    }
                ],
                "author_detail": {
                    "name": "Wentao Xu"
                },
                "author": "Wentao Xu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.08102v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.08102v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.08988v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.08988v1",
                "updated": "2025-01-15T18:05:26Z",
                "updated_parsed": [
                    2025,
                    1,
                    15,
                    18,
                    5,
                    26,
                    2,
                    15,
                    0
                ],
                "published": "2025-01-15T18:05:26Z",
                "published_parsed": [
                    2025,
                    1,
                    15,
                    18,
                    5,
                    26,
                    2,
                    15,
                    0
                ],
                "title": "Feldman-Cousins' ML Cousin: Sterile Neutrino Global Fits using\n  Simulation-Based Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Feldman-Cousins' ML Cousin: Sterile Neutrino Global Fits using\n  Simulation-Based Inference"
                },
                "summary": "For many small-signal particle physics analyses, Wilks' theorem, a\nsimplifying assumption that presumes log-likelihood asymptotic normality, does\nnot hold. The most common alternative approach applied in particle physics is a\nhighly computationally expensive procedure put forward by Feldman and Cousins.\nWhen many experiments are combined for a global fit to data, deviations from\nWilks' theorem are exacerbated, and Feldman-Cousins becomes computationally\nintractable. We present a novel, machine learning-based procedure that can\napproximate a full-fledged Bayesian analysis 200 times faster than the\nFeldman-Cousins method. We demonstrate the utility of this novel method by\nperforming a joint analysis of electron neutrino/antineutrino disappearance\ndata within a single sterile neutrino oscillation framework. Although we\npresent a prototypical simulation-based inference method for a sterile neutrino\nglobal fit, we anticipate that similar procedures will be useful for global\nfits of all kinds, especially those in which Feldman-Cousins is too\ncomputationally expensive to use.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "For many small-signal particle physics analyses, Wilks' theorem, a\nsimplifying assumption that presumes log-likelihood asymptotic normality, does\nnot hold. The most common alternative approach applied in particle physics is a\nhighly computationally expensive procedure put forward by Feldman and Cousins.\nWhen many experiments are combined for a global fit to data, deviations from\nWilks' theorem are exacerbated, and Feldman-Cousins becomes computationally\nintractable. We present a novel, machine learning-based procedure that can\napproximate a full-fledged Bayesian analysis 200 times faster than the\nFeldman-Cousins method. We demonstrate the utility of this novel method by\nperforming a joint analysis of electron neutrino/antineutrino disappearance\ndata within a single sterile neutrino oscillation framework. Although we\npresent a prototypical simulation-based inference method for a sterile neutrino\nglobal fit, we anticipate that similar procedures will be useful for global\nfits of all kinds, especially those in which Feldman-Cousins is too\ncomputationally expensive to use."
                },
                "authors": [
                    {
                        "name": "Joshua Villarreal"
                    },
                    {
                        "name": "John M. Hardin"
                    },
                    {
                        "name": "Janet M. Conrad"
                    }
                ],
                "author_detail": {
                    "name": "Janet M. Conrad"
                },
                "author": "Janet M. Conrad",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.08988v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.08988v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "hep-ex",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "hep-ex",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.08977v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.08977v1",
                "updated": "2025-01-15T17:47:57Z",
                "updated_parsed": [
                    2025,
                    1,
                    15,
                    17,
                    47,
                    57,
                    2,
                    15,
                    0
                ],
                "published": "2025-01-15T17:47:57Z",
                "published_parsed": [
                    2025,
                    1,
                    15,
                    17,
                    47,
                    57,
                    2,
                    15,
                    0
                ],
                "title": "Development and Validation of the Provider Documentation Summarization\n  Quality Instrument for Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Development and Validation of the Provider Documentation Summarization\n  Quality Instrument for Large Language Models"
                },
                "summary": "As Large Language Models (LLMs) are integrated into electronic health record\n(EHR) workflows, validated instruments are essential to evaluate their\nperformance before implementation. Existing instruments for provider\ndocumentation quality are often unsuitable for the complexities of\nLLM-generated text and lack validation on real-world data. The Provider\nDocumentation Summarization Quality Instrument (PDSQI-9) was developed to\nevaluate LLM-generated clinical summaries. Multi-document summaries were\ngenerated from real-world EHR data across multiple specialties using several\nLLMs (GPT-4o, Mixtral 8x7b, and Llama 3-8b). Validation included Pearson\ncorrelation for substantive validity, factor analysis and Cronbach's alpha for\nstructural validity, inter-rater reliability (ICC and Krippendorff's alpha) for\ngeneralizability, a semi-Delphi process for content validity, and comparisons\nof high- versus low-quality summaries for discriminant validity. Seven\nphysician raters evaluated 779 summaries and answered 8,329 questions,\nachieving over 80% power for inter-rater reliability. The PDSQI-9 demonstrated\nstrong internal consistency (Cronbach's alpha = 0.879; 95% CI: 0.867-0.891) and\nhigh inter-rater reliability (ICC = 0.867; 95% CI: 0.867-0.868), supporting\nstructural validity and generalizability. Factor analysis identified a 4-factor\nmodel explaining 58% of the variance, representing organization, clarity,\naccuracy, and utility. Substantive validity was supported by correlations\nbetween note length and scores for Succinct (rho = -0.200, p = 0.029) and\nOrganized (rho = -0.190, p = 0.037). Discriminant validity distinguished high-\nfrom low-quality summaries (p < 0.001). The PDSQI-9 demonstrates robust\nconstruct validity, supporting its use in clinical practice to evaluate\nLLM-generated summaries and facilitate safer integration of LLMs into\nhealthcare workflows.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As Large Language Models (LLMs) are integrated into electronic health record\n(EHR) workflows, validated instruments are essential to evaluate their\nperformance before implementation. Existing instruments for provider\ndocumentation quality are often unsuitable for the complexities of\nLLM-generated text and lack validation on real-world data. The Provider\nDocumentation Summarization Quality Instrument (PDSQI-9) was developed to\nevaluate LLM-generated clinical summaries. Multi-document summaries were\ngenerated from real-world EHR data across multiple specialties using several\nLLMs (GPT-4o, Mixtral 8x7b, and Llama 3-8b). Validation included Pearson\ncorrelation for substantive validity, factor analysis and Cronbach's alpha for\nstructural validity, inter-rater reliability (ICC and Krippendorff's alpha) for\ngeneralizability, a semi-Delphi process for content validity, and comparisons\nof high- versus low-quality summaries for discriminant validity. Seven\nphysician raters evaluated 779 summaries and answered 8,329 questions,\nachieving over 80% power for inter-rater reliability. The PDSQI-9 demonstrated\nstrong internal consistency (Cronbach's alpha = 0.879; 95% CI: 0.867-0.891) and\nhigh inter-rater reliability (ICC = 0.867; 95% CI: 0.867-0.868), supporting\nstructural validity and generalizability. Factor analysis identified a 4-factor\nmodel explaining 58% of the variance, representing organization, clarity,\naccuracy, and utility. Substantive validity was supported by correlations\nbetween note length and scores for Succinct (rho = -0.200, p = 0.029) and\nOrganized (rho = -0.190, p = 0.037). Discriminant validity distinguished high-\nfrom low-quality summaries (p < 0.001). The PDSQI-9 demonstrates robust\nconstruct validity, supporting its use in clinical practice to evaluate\nLLM-generated summaries and facilitate safer integration of LLMs into\nhealthcare workflows."
                },
                "authors": [
                    {
                        "name": "Emma Croxford"
                    },
                    {
                        "name": "Yanjun Gao"
                    },
                    {
                        "name": "Nicholas Pellegrino"
                    },
                    {
                        "name": "Karen K. Wong"
                    },
                    {
                        "name": "Graham Wills"
                    },
                    {
                        "name": "Elliot First"
                    },
                    {
                        "name": "Miranda Schnier"
                    },
                    {
                        "name": "Kyle Burton"
                    },
                    {
                        "name": "Cris G. Ebby"
                    },
                    {
                        "name": "Jillian Gorskic"
                    },
                    {
                        "name": "Matthew Kalscheur"
                    },
                    {
                        "name": "Samy Khalil"
                    },
                    {
                        "name": "Marie Pisani"
                    },
                    {
                        "name": "Tyler Rubeor"
                    },
                    {
                        "name": "Peter Stetson"
                    },
                    {
                        "name": "Frank Liao"
                    },
                    {
                        "name": "Cherodeep Goswami"
                    },
                    {
                        "name": "Brian Patterson"
                    },
                    {
                        "name": "Majid Afshar"
                    }
                ],
                "author_detail": {
                    "name": "Majid Afshar"
                },
                "author": "Majid Afshar",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.08977v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.08977v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.04216v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.04216v2",
                "updated": "2025-01-15T17:47:22Z",
                "updated_parsed": [
                    2025,
                    1,
                    15,
                    17,
                    47,
                    22,
                    2,
                    15,
                    0
                ],
                "published": "2024-11-06T19:24:34Z",
                "published_parsed": [
                    2024,
                    11,
                    6,
                    19,
                    24,
                    34,
                    2,
                    311,
                    0
                ],
                "title": "Debiasing Synthetic Data Generated by Deep Generative Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Debiasing Synthetic Data Generated by Deep Generative Models"
                },
                "summary": "While synthetic data hold great promise for privacy protection, their\nstatistical analysis poses significant challenges that necessitate innovative\nsolutions. The use of deep generative models (DGMs) for synthetic data\ngeneration is known to induce considerable bias and imprecision into synthetic\ndata analyses, compromising their inferential utility as opposed to original\ndata analyses. This bias and uncertainty can be substantial enough to impede\nstatistical convergence rates, even in seemingly straightforward analyses like\nmean calculation. The standard errors of such estimators then exhibit slower\nshrinkage with sample size than the typical 1 over root-$n$ rate. This\ncomplicates fundamental calculations like p-values and confidence intervals,\nwith no straightforward remedy currently available. In response to these\nchallenges, we propose a new strategy that targets synthetic data created by\nDGMs for specific data analyses. Drawing insights from debiased and targeted\nmachine learning, our approach accounts for biases, enhances convergence rates,\nand facilitates the calculation of estimators with easily approximated large\nsample variances. We exemplify our proposal through a simulation study on toy\ndata and two case studies on real-world data, highlighting the importance of\ntailoring DGMs for targeted data analysis. This debiasing strategy contributes\nto advancing the reliability and applicability of synthetic data in statistical\ninference.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While synthetic data hold great promise for privacy protection, their\nstatistical analysis poses significant challenges that necessitate innovative\nsolutions. The use of deep generative models (DGMs) for synthetic data\ngeneration is known to induce considerable bias and imprecision into synthetic\ndata analyses, compromising their inferential utility as opposed to original\ndata analyses. This bias and uncertainty can be substantial enough to impede\nstatistical convergence rates, even in seemingly straightforward analyses like\nmean calculation. The standard errors of such estimators then exhibit slower\nshrinkage with sample size than the typical 1 over root-$n$ rate. This\ncomplicates fundamental calculations like p-values and confidence intervals,\nwith no straightforward remedy currently available. In response to these\nchallenges, we propose a new strategy that targets synthetic data created by\nDGMs for specific data analyses. Drawing insights from debiased and targeted\nmachine learning, our approach accounts for biases, enhances convergence rates,\nand facilitates the calculation of estimators with easily approximated large\nsample variances. We exemplify our proposal through a simulation study on toy\ndata and two case studies on real-world data, highlighting the importance of\ntailoring DGMs for targeted data analysis. This debiasing strategy contributes\nto advancing the reliability and applicability of synthetic data in statistical\ninference."
                },
                "authors": [
                    {
                        "name": "Alexander Decruyenaere"
                    },
                    {
                        "name": "Heidelinde Dehaene"
                    },
                    {
                        "name": "Paloma Rabaey"
                    },
                    {
                        "name": "Christiaan Polet"
                    },
                    {
                        "name": "Johan Decruyenaere"
                    },
                    {
                        "name": "Thomas Demeester"
                    },
                    {
                        "name": "Stijn Vansteelandt"
                    }
                ],
                "author_detail": {
                    "name": "Stijn Vansteelandt"
                },
                "author": "Stijn Vansteelandt",
                "arxiv_comment": "Accepted for the 38th Conference on Neural Information Processing\n  Systems (NeurIPS 2024), joint first authors",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.04216v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.04216v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ML",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.16828v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.16828v2",
                "updated": "2025-01-15T17:41:31Z",
                "updated_parsed": [
                    2025,
                    1,
                    15,
                    17,
                    41,
                    31,
                    2,
                    15,
                    0
                ],
                "published": "2024-08-29T18:00:04Z",
                "published_parsed": [
                    2024,
                    8,
                    29,
                    18,
                    0,
                    4,
                    3,
                    242,
                    0
                ],
                "title": "A neural network emulator of the Advanced LIGO and Advanced Virgo\n  selection function",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A neural network emulator of the Advanced LIGO and Advanced Virgo\n  selection function"
                },
                "summary": "Characterization of search selection effects comprises a core element of\ngravitational-wave data analysis. Knowledge of selection effects is needed to\npredict observational prospects for future surveys and is essential in the\nstatistical inference of astrophysical source populations from observed\ncatalogs of compact binary mergers. Although gravitational-wave selection\nfunctions can be directly measured via injection campaigns -- the insertion and\nattempted recovery of simulated signals added to real instrumental data -- such\nefforts are computationally expensive. Moreover, the inability to interpolate\nbetween discrete injections limits the ability to which we can study narrow or\ndiscontinuous features in the compact binary population. For this reason, there\nis a growing need for alternative representations of gravitational-wave\nselection functions that are computationally cheap to evaluate and can be\ncomputed across a continuous range of compact binary parameters. In this paper,\nwe describe one such representation. Using pipeline injections performed during\nAdvanced LIGO & Advanced Virgo's third observing run (O3), we train a neural\nnetwork emulator for $P(\\mathrm{det}|\\theta)$, the probability that given a\ncompact binary with parameters is successfully detected, averaged over the\ncourse of O3. The emulator captures the dependence of $P(\\mathrm{det}|\\theta)$\non binary masses, spins, distance, sky position, and orbital orientation, and\nit is valid for compact binaries with components masses between\n$1$--$100\\,M_\\odot$. We test the emulator's ability to produce accurate\ndistributions of detectable events, and demonstrate its use in hierarchical\ninference of the binary black hole population.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Characterization of search selection effects comprises a core element of\ngravitational-wave data analysis. Knowledge of selection effects is needed to\npredict observational prospects for future surveys and is essential in the\nstatistical inference of astrophysical source populations from observed\ncatalogs of compact binary mergers. Although gravitational-wave selection\nfunctions can be directly measured via injection campaigns -- the insertion and\nattempted recovery of simulated signals added to real instrumental data -- such\nefforts are computationally expensive. Moreover, the inability to interpolate\nbetween discrete injections limits the ability to which we can study narrow or\ndiscontinuous features in the compact binary population. For this reason, there\nis a growing need for alternative representations of gravitational-wave\nselection functions that are computationally cheap to evaluate and can be\ncomputed across a continuous range of compact binary parameters. In this paper,\nwe describe one such representation. Using pipeline injections performed during\nAdvanced LIGO & Advanced Virgo's third observing run (O3), we train a neural\nnetwork emulator for $P(\\mathrm{det}|\\theta)$, the probability that given a\ncompact binary with parameters is successfully detected, averaged over the\ncourse of O3. The emulator captures the dependence of $P(\\mathrm{det}|\\theta)$\non binary masses, spins, distance, sky position, and orbital orientation, and\nit is valid for compact binaries with components masses between\n$1$--$100\\,M_\\odot$. We test the emulator's ability to produce accurate\ndistributions of detectable events, and demonstrate its use in hierarchical\ninference of the binary black hole population."
                },
                "authors": [
                    {
                        "name": "Thomas A. Callister"
                    },
                    {
                        "name": "Reed Essick"
                    },
                    {
                        "name": "Daniel E. Holz"
                    }
                ],
                "author_detail": {
                    "name": "Daniel E. Holz"
                },
                "author": "Daniel E. Holz",
                "arxiv_doi": "10.1103/PhysRevD.110.123041",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1103/PhysRevD.110.123041",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2408.16828v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.16828v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "Minor update to match published version. 26 pages, 16 figures. Code\n  is available at https://github.com/tcallister/learning-p-det/ and data at\n  https://doi.org/10.5281/zenodo.13362691 . Neural network emulator is released\n  at https://github.com/tcallister/pdet/",
                "arxiv_journal_ref": "Phys. Rev. D 110, 123041 (2024)",
                "arxiv_primary_category": {
                    "term": "astro-ph.HE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.HE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "gr-qc",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.08974v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.08974v1",
                "updated": "2025-01-15T17:36:56Z",
                "updated_parsed": [
                    2025,
                    1,
                    15,
                    17,
                    36,
                    56,
                    2,
                    15,
                    0
                ],
                "published": "2025-01-15T17:36:56Z",
                "published_parsed": [
                    2025,
                    1,
                    15,
                    17,
                    36,
                    56,
                    2,
                    15,
                    0
                ],
                "title": "Learning to Extract Cross-Domain Aspects and Understanding Sentiments\n  Using Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Learning to Extract Cross-Domain Aspects and Understanding Sentiments\n  Using Large Language Models"
                },
                "summary": "Aspect-based sentiment analysis (ASBA) is a refined approach to sentiment\nanalysis that aims to extract and classify sentiments based on specific aspects\nor features of a product, service, or entity. Unlike traditional sentiment\nanalysis, which assigns a general sentiment score to entire reviews or texts,\nABSA focuses on breaking down the text into individual components or aspects\n(e.g., quality, price, service) and evaluating the sentiment towards each. This\nallows for a more granular level of understanding of customer opinions,\nenabling businesses to pinpoint specific areas of strength and improvement. The\nprocess involves several key steps, including aspect extraction, sentiment\nclassification, and aspect-level sentiment aggregation for a review paragraph\nor any other form that the users have provided. ABSA has significant\napplications in areas such as product reviews, social media monitoring,\ncustomer feedback analysis, and market research. By leveraging techniques from\nnatural language processing (NLP) and machine learning, ABSA facilitates the\nextraction of valuable insights, enabling companies to make data-driven\ndecisions that enhance customer satisfaction and optimize offerings. As ABSA\nevolves, it holds the potential to greatly improve personalized customer\nexperiences by providing a deeper understanding of sentiment across various\nproduct aspects. In this work, we have analyzed the strength of LLMs for a\ncomplete cross-domain aspect-based sentiment analysis with the aim of defining\nthe framework for certain products and using it for other similar situations.\nWe argue that it is possible to that at an effectiveness of 92\\% accuracy for\nthe Aspect Based Sentiment Analysis dataset of SemEval-2015 Task 12.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Aspect-based sentiment analysis (ASBA) is a refined approach to sentiment\nanalysis that aims to extract and classify sentiments based on specific aspects\nor features of a product, service, or entity. Unlike traditional sentiment\nanalysis, which assigns a general sentiment score to entire reviews or texts,\nABSA focuses on breaking down the text into individual components or aspects\n(e.g., quality, price, service) and evaluating the sentiment towards each. This\nallows for a more granular level of understanding of customer opinions,\nenabling businesses to pinpoint specific areas of strength and improvement. The\nprocess involves several key steps, including aspect extraction, sentiment\nclassification, and aspect-level sentiment aggregation for a review paragraph\nor any other form that the users have provided. ABSA has significant\napplications in areas such as product reviews, social media monitoring,\ncustomer feedback analysis, and market research. By leveraging techniques from\nnatural language processing (NLP) and machine learning, ABSA facilitates the\nextraction of valuable insights, enabling companies to make data-driven\ndecisions that enhance customer satisfaction and optimize offerings. As ABSA\nevolves, it holds the potential to greatly improve personalized customer\nexperiences by providing a deeper understanding of sentiment across various\nproduct aspects. In this work, we have analyzed the strength of LLMs for a\ncomplete cross-domain aspect-based sentiment analysis with the aim of defining\nthe framework for certain products and using it for other similar situations.\nWe argue that it is possible to that at an effectiveness of 92\\% accuracy for\nthe Aspect Based Sentiment Analysis dataset of SemEval-2015 Task 12."
                },
                "authors": [
                    {
                        "name": "Karukriti Kaushik Ghosh"
                    },
                    {
                        "name": "Chiranjib Sur"
                    }
                ],
                "author_detail": {
                    "name": "Chiranjib Sur"
                },
                "author": "Chiranjib Sur",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.08974v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.08974v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.05179v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.05179v2",
                "updated": "2025-01-15T17:34:26Z",
                "updated_parsed": [
                    2025,
                    1,
                    15,
                    17,
                    34,
                    26,
                    2,
                    15,
                    0
                ],
                "published": "2025-01-09T11:57:58Z",
                "published_parsed": [
                    2025,
                    1,
                    9,
                    11,
                    57,
                    58,
                    3,
                    9,
                    0
                ],
                "title": "Compression with Global Guidance: Towards Training-free High-Resolution\n  MLLMs Acceleration",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Compression with Global Guidance: Towards Training-free High-Resolution\n  MLLMs Acceleration"
                },
                "summary": "Multimodal large language models (MLLMs) have attracted considerable\nattention due to their exceptional performance in visual content understanding\nand reasoning. However, their inference efficiency has been a notable concern,\nas the increasing length of multimodal contexts leads to quadratic complexity.\nToken compression techniques, which reduce the number of visual tokens, have\ndemonstrated their effectiveness in reducing computational costs. Yet, these\napproaches have struggled to keep pace with the rapid advancements in MLLMs,\nespecially the AnyRes strategy in the context of high-resolution image\nunderstanding. In this paper, we propose a novel token compression method,\nGlobalCom$^2$, tailored for high-resolution MLLMs that receive both the\nthumbnail and multiple crops. GlobalCom$^2$ treats the tokens derived from the\nthumbnail as the \"commander\" of the entire token compression process, directing\nthe allocation of retention ratios and the specific compression for each crop.\nIn this way, redundant tokens are eliminated while important local details are\nadaptively preserved to the highest extent feasible. Empirical results across\n10 benchmarks reveal that GlobalCom$^2$ achieves an optimal balance between\nperformance and efficiency, and consistently outperforms state-of-the-art token\ncompression methods with LLaVA-NeXT-7B/13B models. Our code is released at\nhttps://github.com/xuyang-liu16/GlobalCom2.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multimodal large language models (MLLMs) have attracted considerable\nattention due to their exceptional performance in visual content understanding\nand reasoning. However, their inference efficiency has been a notable concern,\nas the increasing length of multimodal contexts leads to quadratic complexity.\nToken compression techniques, which reduce the number of visual tokens, have\ndemonstrated their effectiveness in reducing computational costs. Yet, these\napproaches have struggled to keep pace with the rapid advancements in MLLMs,\nespecially the AnyRes strategy in the context of high-resolution image\nunderstanding. In this paper, we propose a novel token compression method,\nGlobalCom$^2$, tailored for high-resolution MLLMs that receive both the\nthumbnail and multiple crops. GlobalCom$^2$ treats the tokens derived from the\nthumbnail as the \"commander\" of the entire token compression process, directing\nthe allocation of retention ratios and the specific compression for each crop.\nIn this way, redundant tokens are eliminated while important local details are\nadaptively preserved to the highest extent feasible. Empirical results across\n10 benchmarks reveal that GlobalCom$^2$ achieves an optimal balance between\nperformance and efficiency, and consistently outperforms state-of-the-art token\ncompression methods with LLaVA-NeXT-7B/13B models. Our code is released at\nhttps://github.com/xuyang-liu16/GlobalCom2."
                },
                "authors": [
                    {
                        "name": "Xuyang Liu"
                    },
                    {
                        "name": "Ziming Wang"
                    },
                    {
                        "name": "Yuhang Han"
                    },
                    {
                        "name": "Yingyao Wang"
                    },
                    {
                        "name": "Jiale Yuan"
                    },
                    {
                        "name": "Jun Song"
                    },
                    {
                        "name": "Bo Zheng"
                    },
                    {
                        "name": "Linfeng Zhang"
                    },
                    {
                        "name": "Siteng Huang"
                    },
                    {
                        "name": "Honggang Chen"
                    }
                ],
                "author_detail": {
                    "name": "Honggang Chen"
                },
                "author": "Honggang Chen",
                "arxiv_comment": "Our code is released at\n  \\url{https://github.com/xuyang-liu16/GlobalCom2}",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.05179v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.05179v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.08970v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.08970v1",
                "updated": "2025-01-15T17:28:53Z",
                "updated_parsed": [
                    2025,
                    1,
                    15,
                    17,
                    28,
                    53,
                    2,
                    15,
                    0
                ],
                "published": "2025-01-15T17:28:53Z",
                "published_parsed": [
                    2025,
                    1,
                    15,
                    17,
                    28,
                    53,
                    2,
                    15,
                    0
                ],
                "title": "Trusted Machine Learning Models Unlock Private Inference for Problems\n  Currently Infeasible with Cryptography",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Trusted Machine Learning Models Unlock Private Inference for Problems\n  Currently Infeasible with Cryptography"
                },
                "summary": "We often interact with untrusted parties. Prioritization of privacy can limit\nthe effectiveness of these interactions, as achieving certain goals\nnecessitates sharing private data. Traditionally, addressing this challenge has\ninvolved either seeking trusted intermediaries or constructing cryptographic\nprotocols that restrict how much data is revealed, such as multi-party\ncomputations or zero-knowledge proofs. While significant advances have been\nmade in scaling cryptographic approaches, they remain limited in terms of the\nsize and complexity of applications they can be used for. In this paper, we\nargue that capable machine learning models can fulfill the role of a trusted\nthird party, thus enabling secure computations for applications that were\npreviously infeasible. In particular, we describe Trusted Capable Model\nEnvironments (TCMEs) as an alternative approach for scaling secure computation,\nwhere capable machine learning model(s) interact under input/output\nconstraints, with explicit information flow control and explicit statelessness.\nThis approach aims to achieve a balance between privacy and computational\nefficiency, enabling private inference where classical cryptographic solutions\nare currently infeasible. We describe a number of use cases that are enabled by\nTCME, and show that even some simple classic cryptographic problems can already\nbe solved with TCME. Finally, we outline current limitations and discuss the\npath forward in implementing them.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We often interact with untrusted parties. Prioritization of privacy can limit\nthe effectiveness of these interactions, as achieving certain goals\nnecessitates sharing private data. Traditionally, addressing this challenge has\ninvolved either seeking trusted intermediaries or constructing cryptographic\nprotocols that restrict how much data is revealed, such as multi-party\ncomputations or zero-knowledge proofs. While significant advances have been\nmade in scaling cryptographic approaches, they remain limited in terms of the\nsize and complexity of applications they can be used for. In this paper, we\nargue that capable machine learning models can fulfill the role of a trusted\nthird party, thus enabling secure computations for applications that were\npreviously infeasible. In particular, we describe Trusted Capable Model\nEnvironments (TCMEs) as an alternative approach for scaling secure computation,\nwhere capable machine learning model(s) interact under input/output\nconstraints, with explicit information flow control and explicit statelessness.\nThis approach aims to achieve a balance between privacy and computational\nefficiency, enabling private inference where classical cryptographic solutions\nare currently infeasible. We describe a number of use cases that are enabled by\nTCME, and show that even some simple classic cryptographic problems can already\nbe solved with TCME. Finally, we outline current limitations and discuss the\npath forward in implementing them."
                },
                "authors": [
                    {
                        "name": "Ilia Shumailov"
                    },
                    {
                        "name": "Daniel Ramage"
                    },
                    {
                        "name": "Sarah Meiklejohn"
                    },
                    {
                        "name": "Peter Kairouz"
                    },
                    {
                        "name": "Florian Hartmann"
                    },
                    {
                        "name": "Borja Balle"
                    },
                    {
                        "name": "Eugene Bagdasarian"
                    }
                ],
                "author_detail": {
                    "name": "Eugene Bagdasarian"
                },
                "author": "Eugene Bagdasarian",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.08970v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.08970v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.05541v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.05541v2",
                "updated": "2025-01-15T17:23:23Z",
                "updated_parsed": [
                    2025,
                    1,
                    15,
                    17,
                    23,
                    23,
                    2,
                    15,
                    0
                ],
                "published": "2025-01-09T19:27:28Z",
                "published_parsed": [
                    2025,
                    1,
                    9,
                    19,
                    27,
                    28,
                    3,
                    9,
                    0
                ],
                "title": "Customizable LLM-Powered Chatbot for Behavioral Science Research",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Customizable LLM-Powered Chatbot for Behavioral Science Research"
                },
                "summary": "The rapid advancement of Artificial Intelligence has resulted in the advent\nof Large Language Models (LLMs) with the capacity to produce text that closely\nresembles human communication. These models have been seamlessly integrated\ninto diverse applications, enabling interactive and responsive communication\nacross multiple platforms. The potential utility of chatbots transcends these\ntraditional applications, particularly in research contexts, wherein they can\noffer valuable insights and facilitate the design of innovative experiments. In\nthis study, we present a Customizable LLM-Powered Chatbot (CLPC), a web-based\nchatbot system designed to assist in behavioral science research. The system is\nmeticulously designed to function as an experimental instrument rather than a\nconventional chatbot, necessitating users to input a username and experiment\ncode upon access. This setup facilitates precise data cross-referencing,\nthereby augmenting the integrity and applicability of the data collected for\nresearch purposes. It can be easily expanded to accommodate new basic events as\nneeded; and it allows researchers to integrate their own logging events without\nthe necessity of implementing a separate logging mechanism. It is worth noting\nthat our system was built to assist primarily behavioral science research but\nis not limited to it, it can easily be adapted to assist information retrieval\nresearch or interacting with chat bot agents in general.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rapid advancement of Artificial Intelligence has resulted in the advent\nof Large Language Models (LLMs) with the capacity to produce text that closely\nresembles human communication. These models have been seamlessly integrated\ninto diverse applications, enabling interactive and responsive communication\nacross multiple platforms. The potential utility of chatbots transcends these\ntraditional applications, particularly in research contexts, wherein they can\noffer valuable insights and facilitate the design of innovative experiments. In\nthis study, we present a Customizable LLM-Powered Chatbot (CLPC), a web-based\nchatbot system designed to assist in behavioral science research. The system is\nmeticulously designed to function as an experimental instrument rather than a\nconventional chatbot, necessitating users to input a username and experiment\ncode upon access. This setup facilitates precise data cross-referencing,\nthereby augmenting the integrity and applicability of the data collected for\nresearch purposes. It can be easily expanded to accommodate new basic events as\nneeded; and it allows researchers to integrate their own logging events without\nthe necessity of implementing a separate logging mechanism. It is worth noting\nthat our system was built to assist primarily behavioral science research but\nis not limited to it, it can easily be adapted to assist information retrieval\nresearch or interacting with chat bot agents in general."
                },
                "authors": [
                    {
                        "name": "Zenon Lamprou"
                    },
                    {
                        "name": "Yashar Moshfeghi"
                    }
                ],
                "author_detail": {
                    "name": "Yashar Moshfeghi"
                },
                "author": "Yashar Moshfeghi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.05541v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.05541v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.08958v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.08958v1",
                "updated": "2025-01-15T17:09:07Z",
                "updated_parsed": [
                    2025,
                    1,
                    15,
                    17,
                    9,
                    7,
                    2,
                    15,
                    0
                ],
                "published": "2025-01-15T17:09:07Z",
                "published_parsed": [
                    2025,
                    1,
                    15,
                    17,
                    9,
                    7,
                    2,
                    15,
                    0
                ],
                "title": "Kolmogorov-Arnold Networks for Time Series Granger Causality Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Kolmogorov-Arnold Networks for Time Series Granger Causality Inference"
                },
                "summary": "We introduce Granger Causality Kolmogorov-Arnold Networks (GCKAN), an\ninnovative architecture that extends the recently proposed Kolmogorov-Arnold\nNetworks (KAN) to the domain of causal inference. By extracting base weights\nfrom KAN layers and incorporating the sparsity-inducing penalty along with\nridge regularization, GCKAN infers the Granger causality from time series while\nenabling automatic time lag selection. Additionally, we propose an algorithm\nleveraging time-reversed Granger causality to enhance inference accuracy. The\nalgorithm compares prediction and sparse-inducing losses derived from the\noriginal and time-reversed series, automatically selecting the casual\nrelationship with the higher score or integrating the results to mitigate\nspurious connectivities. Comprehensive experiments conducted on Lorenz-96, gene\nregulatory networks, fMRI BOLD signals, and VAR datasets demonstrate that the\nproposed model achieves competitive performance to state-of-the-art methods in\ninferring Granger causality from nonlinear, high-dimensional, and\nlimited-sample time series.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce Granger Causality Kolmogorov-Arnold Networks (GCKAN), an\ninnovative architecture that extends the recently proposed Kolmogorov-Arnold\nNetworks (KAN) to the domain of causal inference. By extracting base weights\nfrom KAN layers and incorporating the sparsity-inducing penalty along with\nridge regularization, GCKAN infers the Granger causality from time series while\nenabling automatic time lag selection. Additionally, we propose an algorithm\nleveraging time-reversed Granger causality to enhance inference accuracy. The\nalgorithm compares prediction and sparse-inducing losses derived from the\noriginal and time-reversed series, automatically selecting the casual\nrelationship with the higher score or integrating the results to mitigate\nspurious connectivities. Comprehensive experiments conducted on Lorenz-96, gene\nregulatory networks, fMRI BOLD signals, and VAR datasets demonstrate that the\nproposed model achieves competitive performance to state-of-the-art methods in\ninferring Granger causality from nonlinear, high-dimensional, and\nlimited-sample time series."
                },
                "authors": [
                    {
                        "name": "Meiliang Liu"
                    },
                    {
                        "name": "Yunfang Xu"
                    },
                    {
                        "name": "Zijin Li"
                    },
                    {
                        "name": "Zhengye Si"
                    },
                    {
                        "name": "Xiaoxiao Yang"
                    },
                    {
                        "name": "Xinyue Yang"
                    },
                    {
                        "name": "Zhiwen Zhao"
                    }
                ],
                "author_detail": {
                    "name": "Zhiwen Zhao"
                },
                "author": "Zhiwen Zhao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.08958v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.08958v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.08951v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.08951v1",
                "updated": "2025-01-15T16:56:26Z",
                "updated_parsed": [
                    2025,
                    1,
                    15,
                    16,
                    56,
                    26,
                    2,
                    15,
                    0
                ],
                "published": "2025-01-15T16:56:26Z",
                "published_parsed": [
                    2025,
                    1,
                    15,
                    16,
                    56,
                    26,
                    2,
                    15,
                    0
                ],
                "title": "Analyzing the Ethical Logic of Six Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Analyzing the Ethical Logic of Six Large Language Models"
                },
                "summary": "This study examines the ethical reasoning of six prominent generative large\nlanguage models: OpenAI GPT-4o, Meta LLaMA 3.1, Perplexity, Anthropic Claude\n3.5 Sonnet, Google Gemini, and Mistral 7B. The research explores how these\nmodels articulate and apply ethical logic, particularly in response to moral\ndilemmas such as the Trolley Problem, and Heinz Dilemma. Departing from\ntraditional alignment studies, the study adopts an explainability-transparency\nframework, prompting models to explain their ethical reasoning. This approach\nis analyzed through three established ethical typologies: the\nconsequentialist-deontological analytic, Moral Foundations Theory, and the\nKohlberg Stages of Moral Development Model. Findings reveal that LLMs exhibit\nlargely convergent ethical logic, marked by a rationalist, consequentialist\nemphasis, with decisions often prioritizing harm minimization and fairness.\nDespite similarities in pre-training and model architecture, a mixture of\nnuanced and significant differences in ethical reasoning emerge across models,\nreflecting variations in fine-tuning and post-training processes. The models\nconsistently display erudition, caution, and self-awareness, presenting ethical\nreasoning akin to a graduate-level discourse in moral philosophy. In striking\nuniformity these systems all describe their ethical reasoning as more\nsophisticated than what is characteristic of typical human moral logic.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This study examines the ethical reasoning of six prominent generative large\nlanguage models: OpenAI GPT-4o, Meta LLaMA 3.1, Perplexity, Anthropic Claude\n3.5 Sonnet, Google Gemini, and Mistral 7B. The research explores how these\nmodels articulate and apply ethical logic, particularly in response to moral\ndilemmas such as the Trolley Problem, and Heinz Dilemma. Departing from\ntraditional alignment studies, the study adopts an explainability-transparency\nframework, prompting models to explain their ethical reasoning. This approach\nis analyzed through three established ethical typologies: the\nconsequentialist-deontological analytic, Moral Foundations Theory, and the\nKohlberg Stages of Moral Development Model. Findings reveal that LLMs exhibit\nlargely convergent ethical logic, marked by a rationalist, consequentialist\nemphasis, with decisions often prioritizing harm minimization and fairness.\nDespite similarities in pre-training and model architecture, a mixture of\nnuanced and significant differences in ethical reasoning emerge across models,\nreflecting variations in fine-tuning and post-training processes. The models\nconsistently display erudition, caution, and self-awareness, presenting ethical\nreasoning akin to a graduate-level discourse in moral philosophy. In striking\nuniformity these systems all describe their ethical reasoning as more\nsophisticated than what is characteristic of typical human moral logic."
                },
                "authors": [
                    {
                        "name": "W. Russell Neuman"
                    },
                    {
                        "name": "Chad Coleman"
                    },
                    {
                        "name": "Manan Shah"
                    }
                ],
                "author_detail": {
                    "name": "Manan Shah"
                },
                "author": "Manan Shah",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.08951v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.08951v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.13749v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.13749v2",
                "updated": "2025-01-15T16:50:11Z",
                "updated_parsed": [
                    2025,
                    1,
                    15,
                    16,
                    50,
                    11,
                    2,
                    15,
                    0
                ],
                "published": "2024-10-17T16:48:51Z",
                "published_parsed": [
                    2024,
                    10,
                    17,
                    16,
                    48,
                    51,
                    3,
                    291,
                    0
                ],
                "title": "Supervised Kernel Thinning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Supervised Kernel Thinning"
                },
                "summary": "The kernel thinning algorithm of Dwivedi & Mackey (2024) provides a\nbetter-than-i.i.d. compression of a generic set of points. By generating\nhigh-fidelity coresets of size significantly smaller than the input points, KT\nis known to speed up unsupervised tasks like Monte Carlo integration,\nuncertainty quantification, and non-parametric hypothesis testing, with minimal\nloss in statistical accuracy. In this work, we generalize the KT algorithm to\nspeed up supervised learning problems involving kernel methods. Specifically,\nwe combine two classical algorithms--Nadaraya-Watson (NW) regression or kernel\nsmoothing, and kernel ridge regression (KRR)--with KT to provide a quadratic\nspeed-up in both training and inference times. We show how distribution\ncompression with KT in each setting reduces to constructing an appropriate\nkernel, and introduce the Kernel-Thinned NW and Kernel-Thinned KRR estimators.\nWe prove that KT-based regression estimators enjoy significantly superior\ncomputational efficiency over the full-data estimators and improved statistical\nefficiency over i.i.d. subsampling of the training data. En route, we also\nprovide a novel multiplicative error guarantee for compressing with KT. We\nvalidate our design choices with both simulations and real data experiments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The kernel thinning algorithm of Dwivedi & Mackey (2024) provides a\nbetter-than-i.i.d. compression of a generic set of points. By generating\nhigh-fidelity coresets of size significantly smaller than the input points, KT\nis known to speed up unsupervised tasks like Monte Carlo integration,\nuncertainty quantification, and non-parametric hypothesis testing, with minimal\nloss in statistical accuracy. In this work, we generalize the KT algorithm to\nspeed up supervised learning problems involving kernel methods. Specifically,\nwe combine two classical algorithms--Nadaraya-Watson (NW) regression or kernel\nsmoothing, and kernel ridge regression (KRR)--with KT to provide a quadratic\nspeed-up in both training and inference times. We show how distribution\ncompression with KT in each setting reduces to constructing an appropriate\nkernel, and introduce the Kernel-Thinned NW and Kernel-Thinned KRR estimators.\nWe prove that KT-based regression estimators enjoy significantly superior\ncomputational efficiency over the full-data estimators and improved statistical\nefficiency over i.i.d. subsampling of the training data. En route, we also\nprovide a novel multiplicative error guarantee for compressing with KT. We\nvalidate our design choices with both simulations and real data experiments."
                },
                "authors": [
                    {
                        "name": "Albert Gong"
                    },
                    {
                        "name": "Kyuseong Choi"
                    },
                    {
                        "name": "Raaz Dwivedi"
                    }
                ],
                "author_detail": {
                    "name": "Raaz Dwivedi"
                },
                "author": "Raaz Dwivedi",
                "arxiv_comment": "Published at NeurIPS 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.13749v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.13749v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.ST",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.TH",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.08944v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.08944v1",
                "updated": "2025-01-15T16:46:32Z",
                "updated_parsed": [
                    2025,
                    1,
                    15,
                    16,
                    46,
                    32,
                    2,
                    15,
                    0
                ],
                "published": "2025-01-15T16:46:32Z",
                "published_parsed": [
                    2025,
                    1,
                    15,
                    16,
                    46,
                    32,
                    2,
                    15,
                    0
                ],
                "title": "Physical AI Agents: Integrating Cognitive Intelligence with Real-World\n  Action",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Physical AI Agents: Integrating Cognitive Intelligence with Real-World\n  Action"
                },
                "summary": "Vertical AI Agents are revolutionizing industries by delivering\ndomain-specific intelligence and tailored solutions. However, many sectors,\nsuch as manufacturing, healthcare, and logistics, demand AI systems capable of\nextending their intelligence into the physical world, interacting directly with\nobjects, environments, and dynamic conditions. This need has led to the\nemergence of Physical AI Agents--systems that integrate cognitive reasoning,\npowered by specialized LLMs, with precise physical actions to perform\nreal-world tasks.\n  This work introduces Physical AI Agents as an evolution of shared principles\nwith Vertical AI Agents, tailored for physical interaction. We propose a\nmodular architecture with three core blocks--perception, cognition, and\nactuation--offering a scalable framework for diverse industries. Additionally,\nwe present the Physical Retrieval Augmented Generation (Ph-RAG) design pattern,\nwhich connects physical intelligence to industry-specific LLMs for real-time\ndecision-making and reporting informed by physical context.\n  Through case studies, we demonstrate how Physical AI Agents and the Ph-RAG\nframework are transforming industries like autonomous vehicles, warehouse\nrobotics, healthcare, and manufacturing, offering businesses a pathway to\nintegrate embodied AI for operational efficiency and innovation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Vertical AI Agents are revolutionizing industries by delivering\ndomain-specific intelligence and tailored solutions. However, many sectors,\nsuch as manufacturing, healthcare, and logistics, demand AI systems capable of\nextending their intelligence into the physical world, interacting directly with\nobjects, environments, and dynamic conditions. This need has led to the\nemergence of Physical AI Agents--systems that integrate cognitive reasoning,\npowered by specialized LLMs, with precise physical actions to perform\nreal-world tasks.\n  This work introduces Physical AI Agents as an evolution of shared principles\nwith Vertical AI Agents, tailored for physical interaction. We propose a\nmodular architecture with three core blocks--perception, cognition, and\nactuation--offering a scalable framework for diverse industries. Additionally,\nwe present the Physical Retrieval Augmented Generation (Ph-RAG) design pattern,\nwhich connects physical intelligence to industry-specific LLMs for real-time\ndecision-making and reporting informed by physical context.\n  Through case studies, we demonstrate how Physical AI Agents and the Ph-RAG\nframework are transforming industries like autonomous vehicles, warehouse\nrobotics, healthcare, and manufacturing, offering businesses a pathway to\nintegrate embodied AI for operational efficiency and innovation."
                },
                "authors": [
                    {
                        "name": "Fouad Bousetouane"
                    }
                ],
                "author_detail": {
                    "name": "Fouad Bousetouane"
                },
                "author": "Fouad Bousetouane",
                "arxiv_comment": "27 pages, 3 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.08944v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.08944v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.MA",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.08925v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.08925v1",
                "updated": "2025-01-15T16:30:29Z",
                "updated_parsed": [
                    2025,
                    1,
                    15,
                    16,
                    30,
                    29,
                    2,
                    15,
                    0
                ],
                "published": "2025-01-15T16:30:29Z",
                "published_parsed": [
                    2025,
                    1,
                    15,
                    16,
                    30,
                    29,
                    2,
                    15,
                    0
                ],
                "title": "Disentangling Exploration of Large Language Models by Optimal\n  Exploitation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Disentangling Exploration of Large Language Models by Optimal\n  Exploitation"
                },
                "summary": "Exploration is a crucial skill for self-improvement and open-ended\nproblem-solving. However, it remains uncertain whether large language models\ncan effectively explore the state-space. Existing evaluations predominantly\nfocus on the trade-off between exploration and exploitation, often assessed in\nmulti-armed bandit problems. In contrast, this work isolates exploration as the\nsole objective, tasking the agent with delivering information that enhances\nfuture returns. For the evaluation, we propose to decompose missing rewards\ninto exploration and exploitation components by measuring the optimal\nachievable return for the states already explored. Our experiments with various\nLLMs reveal that most models struggle to sufficiently explore the state-space\nand that weak exploration is insufficient. We observe a positive correlation\nbetween model size and exploration performance, with larger models\ndemonstrating superior capabilities. Furthermore, we show that our\ndecomposition provides insights into differences in behaviors driven by agent\ninstructions during prompt engineering, offering a valuable tool for refining\nLLM performance in exploratory tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Exploration is a crucial skill for self-improvement and open-ended\nproblem-solving. However, it remains uncertain whether large language models\ncan effectively explore the state-space. Existing evaluations predominantly\nfocus on the trade-off between exploration and exploitation, often assessed in\nmulti-armed bandit problems. In contrast, this work isolates exploration as the\nsole objective, tasking the agent with delivering information that enhances\nfuture returns. For the evaluation, we propose to decompose missing rewards\ninto exploration and exploitation components by measuring the optimal\nachievable return for the states already explored. Our experiments with various\nLLMs reveal that most models struggle to sufficiently explore the state-space\nand that weak exploration is insufficient. We observe a positive correlation\nbetween model size and exploration performance, with larger models\ndemonstrating superior capabilities. Furthermore, we show that our\ndecomposition provides insights into differences in behaviors driven by agent\ninstructions during prompt engineering, offering a valuable tool for refining\nLLM performance in exploratory tasks."
                },
                "authors": [
                    {
                        "name": "Tim Grams"
                    },
                    {
                        "name": "Patrick Betz"
                    },
                    {
                        "name": "Christian Bartelt"
                    }
                ],
                "author_detail": {
                    "name": "Christian Bartelt"
                },
                "author": "Christian Bartelt",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.08925v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.08925v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.08919v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.08919v1",
                "updated": "2025-01-15T16:23:22Z",
                "updated_parsed": [
                    2025,
                    1,
                    15,
                    16,
                    23,
                    22,
                    2,
                    15,
                    0
                ],
                "published": "2025-01-15T16:23:22Z",
                "published_parsed": [
                    2025,
                    1,
                    15,
                    16,
                    23,
                    22,
                    2,
                    15,
                    0
                ],
                "title": "Revealing Local Structures through Machine-Learning- Fused Multimodal\n  Spectroscopy",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Revealing Local Structures through Machine-Learning- Fused Multimodal\n  Spectroscopy"
                },
                "summary": "Atomistic structures of materials offer valuable insights into their\nfunctionality. Determining these structures remains a fundamental challenge in\nmaterials science, especially for systems with defects. While both experimental\nand computational methods exist, each has limitations in resolving nanoscale\nstructures. Core-level spectroscopies, such as x-ray absorption (XAS) or\nelectron energy-loss spectroscopies (EELS), have been used to determine the\nlocal bonding environment and structure of materials. Recently, machine\nlearning (ML) methods have been applied to extract structural and bonding\ninformation from XAS/EELS, but most of these frameworks rely on a single data\nstream, which is often insufficient. In this work, we address this challenge by\nintegrating multimodal ab initio simulations, experimental data acquisition,\nand ML techniques for structure characterization. Our goal is to determine\nlocal structures and properties using EELS and XAS data from multiple elements\nand edges. To showcase our approach, we use various lithium nickel manganese\ncobalt (NMC) oxide compounds which are used for lithium ion batteries,\nincluding those with oxygen vacancies and antisite defects, as the sample\nmaterial system. We successfully inferred local element content, ranging from\nlithium to transition metals, with quantitative agreement with experimental\ndata. Beyond improving prediction accuracy, we find that ML model based on\nmultimodal spectroscopic data is able to determine whether local defects such\nas oxygen vacancy and antisites are present, a task which is impossible for\nsingle mode spectra or other experimental techniques. Furthermore, our\nframework is able to provide physical interpretability, bridging spectroscopy\nwith the local atomic and electronic structures.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Atomistic structures of materials offer valuable insights into their\nfunctionality. Determining these structures remains a fundamental challenge in\nmaterials science, especially for systems with defects. While both experimental\nand computational methods exist, each has limitations in resolving nanoscale\nstructures. Core-level spectroscopies, such as x-ray absorption (XAS) or\nelectron energy-loss spectroscopies (EELS), have been used to determine the\nlocal bonding environment and structure of materials. Recently, machine\nlearning (ML) methods have been applied to extract structural and bonding\ninformation from XAS/EELS, but most of these frameworks rely on a single data\nstream, which is often insufficient. In this work, we address this challenge by\nintegrating multimodal ab initio simulations, experimental data acquisition,\nand ML techniques for structure characterization. Our goal is to determine\nlocal structures and properties using EELS and XAS data from multiple elements\nand edges. To showcase our approach, we use various lithium nickel manganese\ncobalt (NMC) oxide compounds which are used for lithium ion batteries,\nincluding those with oxygen vacancies and antisite defects, as the sample\nmaterial system. We successfully inferred local element content, ranging from\nlithium to transition metals, with quantitative agreement with experimental\ndata. Beyond improving prediction accuracy, we find that ML model based on\nmultimodal spectroscopic data is able to determine whether local defects such\nas oxygen vacancy and antisites are present, a task which is impossible for\nsingle mode spectra or other experimental techniques. Furthermore, our\nframework is able to provide physical interpretability, bridging spectroscopy\nwith the local atomic and electronic structures."
                },
                "authors": [
                    {
                        "name": "Haili Jia"
                    },
                    {
                        "name": "Yiming Chen"
                    },
                    {
                        "name": "Gi-Hyeok Lee"
                    },
                    {
                        "name": "Jacob Smith"
                    },
                    {
                        "name": "Miaofang Chi"
                    },
                    {
                        "name": "Wanli Yang"
                    },
                    {
                        "name": "Maria K. Y. Chan"
                    }
                ],
                "author_detail": {
                    "name": "Maria K. Y. Chan"
                },
                "author": "Maria K. Y. Chan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.08919v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.08919v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cond-mat.mtrl-sci",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cond-mat.mtrl-sci",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "physics.chem-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "physics.comp-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "physics.data-an",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.08915v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.08915v1",
                "updated": "2025-01-15T16:22:11Z",
                "updated_parsed": [
                    2025,
                    1,
                    15,
                    16,
                    22,
                    11,
                    2,
                    15,
                    0
                ],
                "published": "2025-01-15T16:22:11Z",
                "published_parsed": [
                    2025,
                    1,
                    15,
                    16,
                    22,
                    11,
                    2,
                    15,
                    0
                ],
                "title": "The case for a low dark matter density in dynamical dark energy model\n  from local probes",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The case for a low dark matter density in dynamical dark energy model\n  from local probes"
                },
                "summary": "In this work we investigate, through a Bayesian study, the ability of a local\nlow matter density $\\Omega_{\\rm M}$, in discrepancy with the value usually\ninferred from the CMB angular power spectrum, to accommodate observations from\nlocal probes without being in tension with the local values of the Hubble\nconstant $H_0$ or the matter fluctuation $\\sigma_8$ parameters. For that, we\ncombine multiple local probes, with the criteria that they either can constrain\nthe matter density parameter independently from the CMB constraints, or can\nhelp in doing so after making their relevant observations more model\nindependent by relaxing their relevant calibration parameters. We assume\nhowever, either a dynamical dark energy model, or the standard $\\Lambda$CDM\nmodel, when computing the corresponding theoretical observables. We also add,\nin almost all of our Monte Carlo runs, the latest Baryonic acoustic\noscillations (BAO) measurements from the DESI year one release to our core\ngroup. We found that, within $\\Lambda$CDM model, for different combinations of\nour probes, we can accommodate a low matter density along with the $H_0$ and\n$\\sigma_8$ values usually obtained from local probes, providing we promote the\nsound drag $r_s$ component in BAO calculations to a free parameter, and that\neven if we combine with the Pantheon+ Supernova sample. Assuming $w_0w_a$CDM,\nwe also found that relaxing $r_s$ allow us to accommodate $\\Omega_{\\rm M}$,\n$H_0$ and $\\sigma_8$ within their local values, with still however a preference\nfor $w_0w_a$ values far from $\\Lambda$CDM. However, when including Pantheon+\nSupernova sample, we found that the latter preference for high matter density\npushes $\\sigma_8$ to much smaller values, mitigating by then a low matter\ndensity solution to the two common tensions. We conclude that a low matter\ndensity value, helps in preserving the concordance within $\\Lambda$CDM model.\n(abridged)",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this work we investigate, through a Bayesian study, the ability of a local\nlow matter density $\\Omega_{\\rm M}$, in discrepancy with the value usually\ninferred from the CMB angular power spectrum, to accommodate observations from\nlocal probes without being in tension with the local values of the Hubble\nconstant $H_0$ or the matter fluctuation $\\sigma_8$ parameters. For that, we\ncombine multiple local probes, with the criteria that they either can constrain\nthe matter density parameter independently from the CMB constraints, or can\nhelp in doing so after making their relevant observations more model\nindependent by relaxing their relevant calibration parameters. We assume\nhowever, either a dynamical dark energy model, or the standard $\\Lambda$CDM\nmodel, when computing the corresponding theoretical observables. We also add,\nin almost all of our Monte Carlo runs, the latest Baryonic acoustic\noscillations (BAO) measurements from the DESI year one release to our core\ngroup. We found that, within $\\Lambda$CDM model, for different combinations of\nour probes, we can accommodate a low matter density along with the $H_0$ and\n$\\sigma_8$ values usually obtained from local probes, providing we promote the\nsound drag $r_s$ component in BAO calculations to a free parameter, and that\neven if we combine with the Pantheon+ Supernova sample. Assuming $w_0w_a$CDM,\nwe also found that relaxing $r_s$ allow us to accommodate $\\Omega_{\\rm M}$,\n$H_0$ and $\\sigma_8$ within their local values, with still however a preference\nfor $w_0w_a$ values far from $\\Lambda$CDM. However, when including Pantheon+\nSupernova sample, we found that the latter preference for high matter density\npushes $\\sigma_8$ to much smaller values, mitigating by then a low matter\ndensity solution to the two common tensions. We conclude that a low matter\ndensity value, helps in preserving the concordance within $\\Lambda$CDM model.\n(abridged)"
                },
                "authors": [
                    {
                        "name": "Ziad Sakr"
                    }
                ],
                "author_detail": {
                    "name": "Ziad Sakr"
                },
                "author": "Ziad Sakr",
                "arxiv_comment": "Comments or missing references are welcomed. Started from an invited\n  talk to Marcel Grossmann meeting",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.08915v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.08915v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.CO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.CO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.08913v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.08913v1",
                "updated": "2025-01-15T16:21:09Z",
                "updated_parsed": [
                    2025,
                    1,
                    15,
                    16,
                    21,
                    9,
                    2,
                    15,
                    0
                ],
                "published": "2025-01-15T16:21:09Z",
                "published_parsed": [
                    2025,
                    1,
                    15,
                    16,
                    21,
                    9,
                    2,
                    15,
                    0
                ],
                "title": "GenAI Content Detection Task 3: Cross-Domain Machine-Generated Text\n  Detection Challenge",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GenAI Content Detection Task 3: Cross-Domain Machine-Generated Text\n  Detection Challenge"
                },
                "summary": "Recently there have been many shared tasks targeting the detection of\ngenerated text from Large Language Models (LLMs). However, these shared tasks\ntend to focus either on cases where text is limited to one particular domain or\ncases where text can be from many domains, some of which may not be seen during\ntest time. In this shared task, using the newly released RAID benchmark, we aim\nto answer whether or not models can detect generated text from a large, yet\nfixed, number of domains and LLMs, all of which are seen during training. Over\nthe course of three months, our task was attempted by 9 teams with 23 detector\nsubmissions. We find that multiple participants were able to obtain accuracies\nof over 99% on machine-generated text from RAID while maintaining a 5% False\nPositive Rate -- suggesting that detectors are able to robustly detect text\nfrom many domains and models simultaneously. We discuss potential\ninterpretations of this result and provide directions for future research.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recently there have been many shared tasks targeting the detection of\ngenerated text from Large Language Models (LLMs). However, these shared tasks\ntend to focus either on cases where text is limited to one particular domain or\ncases where text can be from many domains, some of which may not be seen during\ntest time. In this shared task, using the newly released RAID benchmark, we aim\nto answer whether or not models can detect generated text from a large, yet\nfixed, number of domains and LLMs, all of which are seen during training. Over\nthe course of three months, our task was attempted by 9 teams with 23 detector\nsubmissions. We find that multiple participants were able to obtain accuracies\nof over 99% on machine-generated text from RAID while maintaining a 5% False\nPositive Rate -- suggesting that detectors are able to robustly detect text\nfrom many domains and models simultaneously. We discuss potential\ninterpretations of this result and provide directions for future research."
                },
                "authors": [
                    {
                        "name": "Liam Dugan"
                    },
                    {
                        "name": "Andrew Zhu"
                    },
                    {
                        "name": "Firoj Alam"
                    },
                    {
                        "name": "Preslav Nakov"
                    },
                    {
                        "name": "Marianna Apidianaki"
                    },
                    {
                        "name": "Chris Callison-Burch"
                    }
                ],
                "author_detail": {
                    "name": "Chris Callison-Burch"
                },
                "author": "Chris Callison-Burch",
                "arxiv_comment": "COLING 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.08913v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.08913v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.7",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2308.07898v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2308.07898v2",
                "updated": "2025-01-15T16:19:36Z",
                "updated_parsed": [
                    2025,
                    1,
                    15,
                    16,
                    19,
                    36,
                    2,
                    15,
                    0
                ],
                "published": "2023-08-15T17:39:52Z",
                "published_parsed": [
                    2023,
                    8,
                    15,
                    17,
                    39,
                    52,
                    1,
                    227,
                    0
                ],
                "title": "A Foundation Language-Image Model of the Retina (FLAIR): Encoding Expert\n  Knowledge in Text Supervision",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Foundation Language-Image Model of the Retina (FLAIR): Encoding Expert\n  Knowledge in Text Supervision"
                },
                "summary": "Foundation vision-language models are currently transforming computer vision,\nand are on the rise in medical imaging fueled by their very promising\ngeneralization capabilities. However, the initial attempts to transfer this new\nparadigm to medical imaging have shown less impressive performances than those\nobserved in other domains, due to the significant domain shift and the complex,\nexpert domain knowledge inherent to medical-imaging tasks. Motivated by the\nneed for domain-expert foundation models, we present FLAIR, a pre-trained\nvision-language model for universal retinal fundus image understanding. To this\nend, we compiled 38 open-access, mostly categorical fundus imaging datasets\nfrom various sources, with up to 101 different target conditions and 288,307\nimages. We integrate the expert's domain knowledge in the form of descriptive\ntextual prompts, during both pre-training and zero-shot inference, enhancing\nthe less-informative categorical supervision of the data. Such a textual\nexpert's knowledge, which we compiled from the relevant clinical literature and\ncommunity standards, describes the fine-grained features of the pathologies as\nwell as the hierarchies and dependencies between them. We report comprehensive\nevaluations, which illustrate the benefit of integrating expert knowledge and\nthe strong generalization capabilities of FLAIR under difficult scenarios with\ndomain shifts or unseen categories. When adapted with a lightweight linear\nprobe, FLAIR outperforms fully-trained, dataset-focused models, more so in the\nfew-shot regimes. Interestingly, FLAIR outperforms by a wide margin\nlarger-scale generalist image-language models and retina domain-specific\nself-supervised networks, which emphasizes the potential of embedding experts'\ndomain knowledge and the limitations of generalist models in medical imaging.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Foundation vision-language models are currently transforming computer vision,\nand are on the rise in medical imaging fueled by their very promising\ngeneralization capabilities. However, the initial attempts to transfer this new\nparadigm to medical imaging have shown less impressive performances than those\nobserved in other domains, due to the significant domain shift and the complex,\nexpert domain knowledge inherent to medical-imaging tasks. Motivated by the\nneed for domain-expert foundation models, we present FLAIR, a pre-trained\nvision-language model for universal retinal fundus image understanding. To this\nend, we compiled 38 open-access, mostly categorical fundus imaging datasets\nfrom various sources, with up to 101 different target conditions and 288,307\nimages. We integrate the expert's domain knowledge in the form of descriptive\ntextual prompts, during both pre-training and zero-shot inference, enhancing\nthe less-informative categorical supervision of the data. Such a textual\nexpert's knowledge, which we compiled from the relevant clinical literature and\ncommunity standards, describes the fine-grained features of the pathologies as\nwell as the hierarchies and dependencies between them. We report comprehensive\nevaluations, which illustrate the benefit of integrating expert knowledge and\nthe strong generalization capabilities of FLAIR under difficult scenarios with\ndomain shifts or unseen categories. When adapted with a lightweight linear\nprobe, FLAIR outperforms fully-trained, dataset-focused models, more so in the\nfew-shot regimes. Interestingly, FLAIR outperforms by a wide margin\nlarger-scale generalist image-language models and retina domain-specific\nself-supervised networks, which emphasizes the potential of embedding experts'\ndomain knowledge and the limitations of generalist models in medical imaging."
                },
                "authors": [
                    {
                        "name": "Julio Silva-Rodríguez"
                    },
                    {
                        "name": "Hadi Chakor"
                    },
                    {
                        "name": "Riadh Kobbi"
                    },
                    {
                        "name": "Jose Dolz"
                    },
                    {
                        "name": "Ismail Ben Ayed"
                    }
                ],
                "author_detail": {
                    "name": "Ismail Ben Ayed"
                },
                "author": "Ismail Ben Ayed",
                "arxiv_doi": "10.1016/j.media.2024.103357",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1016/j.media.2024.103357",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2308.07898v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2308.07898v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "Accepted in Medical Image Analysis. The pre-trained model is\n  available at: https://github.com/jusiro/FLAIR",
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.00781v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.00781v2",
                "updated": "2025-01-15T16:17:19Z",
                "updated_parsed": [
                    2025,
                    1,
                    15,
                    16,
                    17,
                    19,
                    2,
                    15,
                    0
                ],
                "published": "2024-10-01T15:22:42Z",
                "published_parsed": [
                    2024,
                    10,
                    1,
                    15,
                    22,
                    42,
                    1,
                    275,
                    0
                ],
                "title": "Modeling Neural Switching via Drift-Diffusion Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Modeling Neural Switching via Drift-Diffusion Models"
                },
                "summary": "Neural encoding, or neural representation, is a field in neuroscience that\nfocuses on characterizing how information from stimuli is encoded in the\nspiking activity of neurons. When more than one stimulus is present, a theory\nknown as multiplexing posits that neurons temporally switch between encoding\nvarious stimuli, creating a fluctuating firing pattern. Here, we propose a new\nstatistical framework to analyze rate fluctuations and discern whether neurons\nemploy multiplexing as a means of encoding multiple stimuli. We propose a\nmechanistic approach to modeling multiplexing by constructing a non-Markovian\nendogenous state-space model. Specifically, we propose that multiplexing arises\nfrom competition between the stimuli, which are modeled as latent\ndrift-diffusion processes. We propose a new MCMC algorithm for conducting\nposterior inference on similar types of state-space models, where typical\nstate-space MCMC methods fail due to strong dependence between the parameters.\nIn addition to a multiplexing-specific model, we develop alternative models\nthat represent a wide class of alternative encoding theories and perform model\ncomparison using WAIC to determine whether the data suggest the occurrence\nmultiplexing over alternative theories of neural encoding. We show that WAIC is\nhighly informative in model selection and discuss different considerations when\nusing WAIC for general point process data and state-space models. Using the\nproposed framework, we provide evidence of multiplexing within the inferior\ncolliculus and novel insight into the switching dynamics.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Neural encoding, or neural representation, is a field in neuroscience that\nfocuses on characterizing how information from stimuli is encoded in the\nspiking activity of neurons. When more than one stimulus is present, a theory\nknown as multiplexing posits that neurons temporally switch between encoding\nvarious stimuli, creating a fluctuating firing pattern. Here, we propose a new\nstatistical framework to analyze rate fluctuations and discern whether neurons\nemploy multiplexing as a means of encoding multiple stimuli. We propose a\nmechanistic approach to modeling multiplexing by constructing a non-Markovian\nendogenous state-space model. Specifically, we propose that multiplexing arises\nfrom competition between the stimuli, which are modeled as latent\ndrift-diffusion processes. We propose a new MCMC algorithm for conducting\nposterior inference on similar types of state-space models, where typical\nstate-space MCMC methods fail due to strong dependence between the parameters.\nIn addition to a multiplexing-specific model, we develop alternative models\nthat represent a wide class of alternative encoding theories and perform model\ncomparison using WAIC to determine whether the data suggest the occurrence\nmultiplexing over alternative theories of neural encoding. We show that WAIC is\nhighly informative in model selection and discuss different considerations when\nusing WAIC for general point process data and state-space models. Using the\nproposed framework, we provide evidence of multiplexing within the inferior\ncolliculus and novel insight into the switching dynamics."
                },
                "authors": [
                    {
                        "name": "Nicholas Marco"
                    },
                    {
                        "name": "Jennifer M. Groh"
                    },
                    {
                        "name": "Surya T. Tokdar"
                    }
                ],
                "author_detail": {
                    "name": "Surya T. Tokdar"
                },
                "author": "Surya T. Tokdar",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.00781v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.00781v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ME",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.08902v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.08902v1",
                "updated": "2025-01-15T16:11:24Z",
                "updated_parsed": [
                    2025,
                    1,
                    15,
                    16,
                    11,
                    24,
                    2,
                    15,
                    0
                ],
                "published": "2025-01-15T16:11:24Z",
                "published_parsed": [
                    2025,
                    1,
                    15,
                    16,
                    11,
                    24,
                    2,
                    15,
                    0
                ],
                "title": "Multi-View Transformers for Airway-To-Lung Ratio Inference on Cardiac CT\n  Scans: The C4R Study",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-View Transformers for Airway-To-Lung Ratio Inference on Cardiac CT\n  Scans: The C4R Study"
                },
                "summary": "The ratio of airway tree lumen to lung size (ALR), assessed at full\ninspiration on high resolution full-lung computed tomography (CT), is a major\nrisk factor for chronic obstructive pulmonary disease (COPD). There is growing\ninterest to infer ALR from cardiac CT images, which are widely available in\nepidemiological cohorts, to investigate the relationship of ALR to severe\nCOVID-19 and post-acute sequelae of SARS-CoV-2 infection (PASC). Previously,\ncardiac scans included approximately 2/3 of the total lung volume with 5-6x\ngreater slice thickness than high-resolution (HR) full-lung (FL) CT. In this\nstudy, we present a novel attention-based Multi-view Swin Transformer to infer\nFL ALR values from segmented cardiac CT scans. For the supervised training we\nexploit paired full-lung and cardiac CTs acquired in the Multi-Ethnic Study of\nAtherosclerosis (MESA). Our network significantly outperforms a proxy direct\nALR inference on segmented cardiac CT scans and achieves accuracy and\nreproducibility comparable with a scan-rescan reproducibility of the FL ALR\nground-truth.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The ratio of airway tree lumen to lung size (ALR), assessed at full\ninspiration on high resolution full-lung computed tomography (CT), is a major\nrisk factor for chronic obstructive pulmonary disease (COPD). There is growing\ninterest to infer ALR from cardiac CT images, which are widely available in\nepidemiological cohorts, to investigate the relationship of ALR to severe\nCOVID-19 and post-acute sequelae of SARS-CoV-2 infection (PASC). Previously,\ncardiac scans included approximately 2/3 of the total lung volume with 5-6x\ngreater slice thickness than high-resolution (HR) full-lung (FL) CT. In this\nstudy, we present a novel attention-based Multi-view Swin Transformer to infer\nFL ALR values from segmented cardiac CT scans. For the supervised training we\nexploit paired full-lung and cardiac CTs acquired in the Multi-Ethnic Study of\nAtherosclerosis (MESA). Our network significantly outperforms a proxy direct\nALR inference on segmented cardiac CT scans and achieves accuracy and\nreproducibility comparable with a scan-rescan reproducibility of the FL ALR\nground-truth."
                },
                "authors": [
                    {
                        "name": "Sneha N. Naik"
                    },
                    {
                        "name": "Elsa D. Angelini"
                    },
                    {
                        "name": "Eric A. Hoffman"
                    },
                    {
                        "name": "Elizabeth C. Oelsner"
                    },
                    {
                        "name": "R. Graham Barr"
                    },
                    {
                        "name": "Benjamin M. Smith"
                    },
                    {
                        "name": "Andrew F. Laine"
                    }
                ],
                "author_detail": {
                    "name": "Andrew F. Laine"
                },
                "author": "Andrew F. Laine",
                "arxiv_comment": "Accepted to appear in Proceedings of International Symposium on\n  Biomedical Imaging (ISBI), 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.08902v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.08902v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.IV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.IV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.08900v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.08900v1",
                "updated": "2025-01-15T16:08:25Z",
                "updated_parsed": [
                    2025,
                    1,
                    15,
                    16,
                    8,
                    25,
                    2,
                    15,
                    0
                ],
                "published": "2025-01-15T16:08:25Z",
                "published_parsed": [
                    2025,
                    1,
                    15,
                    16,
                    8,
                    25,
                    2,
                    15,
                    0
                ],
                "title": "Enhanced Multi-Scale Cross-Attention for Person Image Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Enhanced Multi-Scale Cross-Attention for Person Image Generation"
                },
                "summary": "In this paper, we propose a novel cross-attention-based generative\nadversarial network (GAN) for the challenging person image generation task.\nCross-attention is a novel and intuitive multi-modal fusion method in which an\nattention/correlation matrix is calculated between two feature maps of\ndifferent modalities. Specifically, we propose the novel XingGAN (or\nCrossingGAN), which consists of two generation branches that capture the\nperson's appearance and shape, respectively. Moreover, we propose two novel\ncross-attention blocks to effectively transfer and update the person's shape\nand appearance embeddings for mutual improvement. This has not been considered\nby any other existing GAN-based image generation work. To further learn the\nlong-range correlations between different person poses at different scales and\nsub-regions, we propose two novel multi-scale cross-attention blocks. To tackle\nthe issue of independent correlation computations within the cross-attention\nmechanism leading to noisy and ambiguous attention weights, which hinder\nperformance improvements, we propose a module called enhanced attention (EA).\nLastly, we introduce a novel densely connected co-attention module to fuse\nappearance and shape features at different stages effectively. Extensive\nexperiments on two public datasets demonstrate that the proposed method\noutperforms current GAN-based methods and performs on par with diffusion-based\nmethods. However, our method is significantly faster than diffusion-based\nmethods in both training and inference.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this paper, we propose a novel cross-attention-based generative\nadversarial network (GAN) for the challenging person image generation task.\nCross-attention is a novel and intuitive multi-modal fusion method in which an\nattention/correlation matrix is calculated between two feature maps of\ndifferent modalities. Specifically, we propose the novel XingGAN (or\nCrossingGAN), which consists of two generation branches that capture the\nperson's appearance and shape, respectively. Moreover, we propose two novel\ncross-attention blocks to effectively transfer and update the person's shape\nand appearance embeddings for mutual improvement. This has not been considered\nby any other existing GAN-based image generation work. To further learn the\nlong-range correlations between different person poses at different scales and\nsub-regions, we propose two novel multi-scale cross-attention blocks. To tackle\nthe issue of independent correlation computations within the cross-attention\nmechanism leading to noisy and ambiguous attention weights, which hinder\nperformance improvements, we propose a module called enhanced attention (EA).\nLastly, we introduce a novel densely connected co-attention module to fuse\nappearance and shape features at different stages effectively. Extensive\nexperiments on two public datasets demonstrate that the proposed method\noutperforms current GAN-based methods and performs on par with diffusion-based\nmethods. However, our method is significantly faster than diffusion-based\nmethods in both training and inference."
                },
                "authors": [
                    {
                        "name": "Hao Tang"
                    },
                    {
                        "name": "Ling Shao"
                    },
                    {
                        "name": "Nicu Sebe"
                    },
                    {
                        "name": "Luc Van Gool"
                    }
                ],
                "author_detail": {
                    "name": "Luc Van Gool"
                },
                "author": "Luc Van Gool",
                "arxiv_comment": "Accepted to TPAMI, an extended version of a paper published in\n  ECCV2020. arXiv admin note: substantial text overlap with arXiv:2007.09278",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.08900v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.08900v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.08897v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.08897v1",
                "updated": "2025-01-15T16:06:10Z",
                "updated_parsed": [
                    2025,
                    1,
                    15,
                    16,
                    6,
                    10,
                    2,
                    15,
                    0
                ],
                "published": "2025-01-15T16:06:10Z",
                "published_parsed": [
                    2025,
                    1,
                    15,
                    16,
                    6,
                    10,
                    2,
                    15,
                    0
                ],
                "title": "Leveraging Large Language Models as Knowledge-Driven Agents for Reliable\n  Retrosynthesis Planning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Leveraging Large Language Models as Knowledge-Driven Agents for Reliable\n  Retrosynthesis Planning"
                },
                "summary": "Identifying reliable synthesis pathways in materials chemistry is a complex\ntask, particularly in polymer science, due to the intricate and often\nnon-unique nomenclature of macromolecules. To address this challenge, we\npropose an agent system that integrates large language models (LLMs) and\nknowledge graphs (KGs). By leveraging LLMs' powerful capabilities for\nextracting and recognizing chemical substance names, and storing the extracted\ndata in a structured knowledge graph, our system fully automates the retrieval\nof relevant literatures, extraction of reaction data, database querying,\nconstruction of retrosynthetic pathway trees, further expansion through the\nretrieval of additional literature and recommendation of optimal reaction\npathways. A novel Multi-branched Reaction Pathway Search (MBRPS) algorithm\nenables the exploration of all pathways, with a particular focus on\nmulti-branched ones, helping LLMs overcome weak reasoning in multi-branched\npaths. This work represents the first attempt to develop a fully automated\nretrosynthesis planning agent tailored specially for macromolecules powered by\nLLMs. Applied to polyimide synthesis, our new approach constructs a\nretrosynthetic pathway tree with hundreds of pathways and recommends optimized\nroutes, including both known and novel pathways, demonstrating its\neffectiveness and potential for broader applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Identifying reliable synthesis pathways in materials chemistry is a complex\ntask, particularly in polymer science, due to the intricate and often\nnon-unique nomenclature of macromolecules. To address this challenge, we\npropose an agent system that integrates large language models (LLMs) and\nknowledge graphs (KGs). By leveraging LLMs' powerful capabilities for\nextracting and recognizing chemical substance names, and storing the extracted\ndata in a structured knowledge graph, our system fully automates the retrieval\nof relevant literatures, extraction of reaction data, database querying,\nconstruction of retrosynthetic pathway trees, further expansion through the\nretrieval of additional literature and recommendation of optimal reaction\npathways. A novel Multi-branched Reaction Pathway Search (MBRPS) algorithm\nenables the exploration of all pathways, with a particular focus on\nmulti-branched ones, helping LLMs overcome weak reasoning in multi-branched\npaths. This work represents the first attempt to develop a fully automated\nretrosynthesis planning agent tailored specially for macromolecules powered by\nLLMs. Applied to polyimide synthesis, our new approach constructs a\nretrosynthetic pathway tree with hundreds of pathways and recommends optimized\nroutes, including both known and novel pathways, demonstrating its\neffectiveness and potential for broader applications."
                },
                "authors": [
                    {
                        "name": "Qinyu Ma"
                    },
                    {
                        "name": "Yuhao Zhou"
                    },
                    {
                        "name": "Jianfeng Li"
                    }
                ],
                "author_detail": {
                    "name": "Jianfeng Li"
                },
                "author": "Jianfeng Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.08897v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.08897v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.08885v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.08885v1",
                "updated": "2025-01-15T15:56:06Z",
                "updated_parsed": [
                    2025,
                    1,
                    15,
                    15,
                    56,
                    6,
                    2,
                    15,
                    0
                ],
                "published": "2025-01-15T15:56:06Z",
                "published_parsed": [
                    2025,
                    1,
                    15,
                    15,
                    56,
                    6,
                    2,
                    15,
                    0
                ],
                "title": "Feature-based One-For-All: A Universal Framework for Heterogeneous\n  Knowledge Distillation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Feature-based One-For-All: A Universal Framework for Heterogeneous\n  Knowledge Distillation"
                },
                "summary": "Knowledge distillation (KD) involves transferring knowledge from a\npre-trained heavy teacher model to a lighter student model, thereby reducing\nthe inference cost while maintaining comparable effectiveness. Prior KD\ntechniques typically assume homogeneity between the teacher and student models.\nHowever, as technology advances, a wide variety of architectures have emerged,\nranging from initial Convolutional Neural Networks (CNNs) to Vision\nTransformers (ViTs), and Multi-Level Perceptrons (MLPs). Consequently,\ndeveloping a universal KD framework compatible with any architecture has become\nan important research topic. In this paper, we introduce a feature-based\none-for-all (FOFA) KD framework to enable feature distillation across diverse\narchitecture. Our framework comprises two key components. First, we design\nprompt tuning blocks that incorporate student feedback, allowing teacher\nfeatures to adapt to the student model's learning process. Second, we propose\nregion-aware attention to mitigate the view mismatch problem between\nheterogeneous architecture. By leveraging these two modules, effective\ndistillation of intermediate features can be achieved across heterogeneous\narchitectures. Extensive experiments on CIFAR, ImageNet, and COCO demonstrate\nthe superiority of the proposed method.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Knowledge distillation (KD) involves transferring knowledge from a\npre-trained heavy teacher model to a lighter student model, thereby reducing\nthe inference cost while maintaining comparable effectiveness. Prior KD\ntechniques typically assume homogeneity between the teacher and student models.\nHowever, as technology advances, a wide variety of architectures have emerged,\nranging from initial Convolutional Neural Networks (CNNs) to Vision\nTransformers (ViTs), and Multi-Level Perceptrons (MLPs). Consequently,\ndeveloping a universal KD framework compatible with any architecture has become\nan important research topic. In this paper, we introduce a feature-based\none-for-all (FOFA) KD framework to enable feature distillation across diverse\narchitecture. Our framework comprises two key components. First, we design\nprompt tuning blocks that incorporate student feedback, allowing teacher\nfeatures to adapt to the student model's learning process. Second, we propose\nregion-aware attention to mitigate the view mismatch problem between\nheterogeneous architecture. By leveraging these two modules, effective\ndistillation of intermediate features can be achieved across heterogeneous\narchitectures. Extensive experiments on CIFAR, ImageNet, and COCO demonstrate\nthe superiority of the proposed method."
                },
                "authors": [
                    {
                        "name": "Jhe-Hao Lin"
                    },
                    {
                        "name": "Yi Yao"
                    },
                    {
                        "name": "Chan-Feng Hsu"
                    },
                    {
                        "name": "Hongxia Xie"
                    },
                    {
                        "name": "Hong-Han Shuai"
                    },
                    {
                        "name": "Wen-Huang Cheng"
                    }
                ],
                "author_detail": {
                    "name": "Wen-Huang Cheng"
                },
                "author": "Wen-Huang Cheng",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.08885v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.08885v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.16749v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.16749v4",
                "updated": "2025-01-15T15:40:12Z",
                "updated_parsed": [
                    2025,
                    1,
                    15,
                    15,
                    40,
                    12,
                    2,
                    15,
                    0
                ],
                "published": "2024-06-24T15:57:49Z",
                "published_parsed": [
                    2024,
                    6,
                    24,
                    15,
                    57,
                    49,
                    0,
                    176,
                    0
                ],
                "title": "Inferring stochastic low-rank recurrent neural networks from neural data",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Inferring stochastic low-rank recurrent neural networks from neural data"
                },
                "summary": "A central aim in computational neuroscience is to relate the activity of\nlarge populations of neurons to an underlying dynamical system. Models of these\nneural dynamics should ideally be both interpretable and fit the observed data\nwell. Low-rank recurrent neural networks (RNNs) exhibit such interpretability\nby having tractable dynamics. However, it is unclear how to best fit low-rank\nRNNs to data consisting of noisy observations of an underlying stochastic\nsystem. Here, we propose to fit stochastic low-rank RNNs with variational\nsequential Monte Carlo methods. We validate our method on several datasets\nconsisting of both continuous and spiking neural data, where we obtain lower\ndimensional latent dynamics than current state of the art methods.\nAdditionally, for low-rank models with piecewise linear nonlinearities, we show\nhow to efficiently identify all fixed points in polynomial rather than\nexponential cost in the number of units, making analysis of the inferred\ndynamics tractable for large RNNs. Our method both elucidates the dynamical\nsystems underlying experimental recordings and provides a generative model\nwhose trajectories match observed variability.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A central aim in computational neuroscience is to relate the activity of\nlarge populations of neurons to an underlying dynamical system. Models of these\nneural dynamics should ideally be both interpretable and fit the observed data\nwell. Low-rank recurrent neural networks (RNNs) exhibit such interpretability\nby having tractable dynamics. However, it is unclear how to best fit low-rank\nRNNs to data consisting of noisy observations of an underlying stochastic\nsystem. Here, we propose to fit stochastic low-rank RNNs with variational\nsequential Monte Carlo methods. We validate our method on several datasets\nconsisting of both continuous and spiking neural data, where we obtain lower\ndimensional latent dynamics than current state of the art methods.\nAdditionally, for low-rank models with piecewise linear nonlinearities, we show\nhow to efficiently identify all fixed points in polynomial rather than\nexponential cost in the number of units, making analysis of the inferred\ndynamics tractable for large RNNs. Our method both elucidates the dynamical\nsystems underlying experimental recordings and provides a generative model\nwhose trajectories match observed variability."
                },
                "authors": [
                    {
                        "name": "Matthijs Pals"
                    },
                    {
                        "name": "A Erdem Sağtekin"
                    },
                    {
                        "name": "Felix Pei"
                    },
                    {
                        "name": "Manuel Gloeckler"
                    },
                    {
                        "name": "Jakob H Macke"
                    }
                ],
                "author_detail": {
                    "name": "Jakob H Macke"
                },
                "author": "Jakob H Macke",
                "arxiv_journal_ref": "The Thirty-eighth Annual Conference on Neural Information\n  Processing Systems (NeurIPS) 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.16749v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.16749v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "q-bio.NC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.08863v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.08863v1",
                "updated": "2025-01-15T15:23:42Z",
                "updated_parsed": [
                    2025,
                    1,
                    15,
                    15,
                    23,
                    42,
                    2,
                    15,
                    0
                ],
                "published": "2025-01-15T15:23:42Z",
                "published_parsed": [
                    2025,
                    1,
                    15,
                    15,
                    23,
                    42,
                    2,
                    15,
                    0
                ],
                "title": "High Lithium Abundance Connection with the Chromospheric Helium in Red\n  Giants: Spectroscopic and Asteroseismic analyses",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "High Lithium Abundance Connection with the Chromospheric Helium in Red\n  Giants: Spectroscopic and Asteroseismic analyses"
                },
                "summary": "We present a study of correlations between high Li abundances and strong\nchromospheric He I 10830 \\AA\\ absorption line strengths in Kepler field giant\nstars. Our sample includes 84 giants with detectable solar-like oscillations in\ntheir lightcurves, and their Li abundances come from the literature or were\nmeasured here using LAMOST medium-resolution spectra. Evolutionary phases are\ndetermined through asteroseismic analysis, with mixed-mode period spacing\n(\\Delta P) used to infer the time evolution of RC giants. Near-infrared\nobservations of the He I \\lambda 10830 line were obtained with the\nhigh-resolution Habitable-zone Planet Finder (HPF) spectrograph on the\nHobby-Eberly Telescope (HET). We find high Li abundances and strong He I lines\nexclusively among red clump (RC) giants, with their absence in red giant branch\nstars suggesting a shared origin linked to the He-flash. Additionally, a steady\ndecline in He I strength with decreasing Li abundance among RC giants indicates\na correlation between these properties. Older, Li-normal RC giants are He-weak,\nwhile most younger super-Li-rich giants are He-strong, suggesting temporal\nevolution of both phenomena. We hypothesize that the core He-flash and\nsubsequent sub-flashes may enhance Li abundances in RC giant photospheres and\ntrigger heightened chromospheric activity, leading to stronger He I \\lambda\n10830 \\AA\\ lines in younger RCs. Over time, post-He-flash, chromospheric\nactivity diminishes, resulting in weaker He I lines in older, Li-normal RCs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present a study of correlations between high Li abundances and strong\nchromospheric He I 10830 \\AA\\ absorption line strengths in Kepler field giant\nstars. Our sample includes 84 giants with detectable solar-like oscillations in\ntheir lightcurves, and their Li abundances come from the literature or were\nmeasured here using LAMOST medium-resolution spectra. Evolutionary phases are\ndetermined through asteroseismic analysis, with mixed-mode period spacing\n(\\Delta P) used to infer the time evolution of RC giants. Near-infrared\nobservations of the He I \\lambda 10830 line were obtained with the\nhigh-resolution Habitable-zone Planet Finder (HPF) spectrograph on the\nHobby-Eberly Telescope (HET). We find high Li abundances and strong He I lines\nexclusively among red clump (RC) giants, with their absence in red giant branch\nstars suggesting a shared origin linked to the He-flash. Additionally, a steady\ndecline in He I strength with decreasing Li abundance among RC giants indicates\na correlation between these properties. Older, Li-normal RC giants are He-weak,\nwhile most younger super-Li-rich giants are He-strong, suggesting temporal\nevolution of both phenomena. We hypothesize that the core He-flash and\nsubsequent sub-flashes may enhance Li abundances in RC giant photospheres and\ntrigger heightened chromospheric activity, leading to stronger He I \\lambda\n10830 \\AA\\ lines in younger RCs. Over time, post-He-flash, chromospheric\nactivity diminishes, resulting in weaker He I lines in older, Li-normal RCs."
                },
                "authors": [
                    {
                        "name": "Anohita Mallick"
                    },
                    {
                        "name": "Christopher Sneden"
                    },
                    {
                        "name": "Bacham E. Reddy"
                    },
                    {
                        "name": "Melike Afşar"
                    }
                ],
                "author_detail": {
                    "name": "Melike Afşar"
                },
                "author": "Melike Afşar",
                "arxiv_comment": "17 pages, 11 figures, 1 table, Accepted for publication in ApJ",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.08863v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.08863v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.SR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.SR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.08848v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.08848v1",
                "updated": "2025-01-15T15:00:11Z",
                "updated_parsed": [
                    2025,
                    1,
                    15,
                    15,
                    0,
                    11,
                    2,
                    15,
                    0
                ],
                "published": "2025-01-15T15:00:11Z",
                "published_parsed": [
                    2025,
                    1,
                    15,
                    15,
                    0,
                    11,
                    2,
                    15,
                    0
                ],
                "title": "RouteNet-Gauss: Hardware-Enhanced Network Modeling with Machine Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RouteNet-Gauss: Hardware-Enhanced Network Modeling with Machine Learning"
                },
                "summary": "Network simulation is pivotal in network modeling, assisting with tasks\nranging from capacity planning to performance estimation. Traditional\napproaches such as Discrete Event Simulation (DES) face limitations in terms of\ncomputational cost and accuracy. This paper introduces RouteNet-Gauss, a novel\nintegration of a testbed network with a Machine Learning (ML) model to address\nthese challenges. By using the testbed as a hardware accelerator,\nRouteNet-Gauss generates training datasets rapidly and simulates network\nscenarios with high fidelity to real-world conditions. Experimental results\nshow that RouteNet-Gauss significantly reduces prediction errors by up to 95%\nand achieves a 488x speedup in inference time compared to state-of-the-art\nDES-based methods. RouteNet-Gauss's modular architecture is dynamically\nconstructed based on the specific characteristics of the network scenario, such\nas topology and routing. This enables it to understand and generalize to\ndifferent network configurations beyond those seen during training, including\nnetworks up to 10x larger. Additionally, it supports Temporal Aggregated\nPerformance Estimation (TAPE), providing configurable temporal granularity and\nmaintaining high accuracy in flow performance metrics. This approach shows\npromise in improving both simulation efficiency and accuracy, offering a\nvaluable tool for network operators.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Network simulation is pivotal in network modeling, assisting with tasks\nranging from capacity planning to performance estimation. Traditional\napproaches such as Discrete Event Simulation (DES) face limitations in terms of\ncomputational cost and accuracy. This paper introduces RouteNet-Gauss, a novel\nintegration of a testbed network with a Machine Learning (ML) model to address\nthese challenges. By using the testbed as a hardware accelerator,\nRouteNet-Gauss generates training datasets rapidly and simulates network\nscenarios with high fidelity to real-world conditions. Experimental results\nshow that RouteNet-Gauss significantly reduces prediction errors by up to 95%\nand achieves a 488x speedup in inference time compared to state-of-the-art\nDES-based methods. RouteNet-Gauss's modular architecture is dynamically\nconstructed based on the specific characteristics of the network scenario, such\nas topology and routing. This enables it to understand and generalize to\ndifferent network configurations beyond those seen during training, including\nnetworks up to 10x larger. Additionally, it supports Temporal Aggregated\nPerformance Estimation (TAPE), providing configurable temporal granularity and\nmaintaining high accuracy in flow performance metrics. This approach shows\npromise in improving both simulation efficiency and accuracy, offering a\nvaluable tool for network operators."
                },
                "authors": [
                    {
                        "name": "Carlos Güemes-Palau"
                    },
                    {
                        "name": "Miquel Ferriol-Galmés"
                    },
                    {
                        "name": "Jordi Paillisse-Vilanova"
                    },
                    {
                        "name": "Albert López-Brescó"
                    },
                    {
                        "name": "Pere Barlet-Ros"
                    },
                    {
                        "name": "Albert Cabellos-Aparicio"
                    }
                ],
                "author_detail": {
                    "name": "Albert Cabellos-Aparicio"
                },
                "author": "Albert Cabellos-Aparicio",
                "arxiv_comment": "13 pages, 11 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.08848v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.08848v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.08841v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.08841v1",
                "updated": "2025-01-15T14:52:20Z",
                "updated_parsed": [
                    2025,
                    1,
                    15,
                    14,
                    52,
                    20,
                    2,
                    15,
                    0
                ],
                "published": "2025-01-15T14:52:20Z",
                "published_parsed": [
                    2025,
                    1,
                    15,
                    14,
                    52,
                    20,
                    2,
                    15,
                    0
                ],
                "title": "Exploring Task-Level Optimal Prompts for Visual In-Context Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Exploring Task-Level Optimal Prompts for Visual In-Context Learning"
                },
                "summary": "With the development of Vision Foundation Models (VFMs) in recent years,\nVisual In-Context Learning (VICL) has become a better choice compared to\nmodifying models in most scenarios. Different from retraining or fine-tuning\nmodel, VICL does not require modifications to the model's weights or\narchitecture, and only needs a prompt with demonstrations to teach VFM how to\nsolve tasks. Currently, significant computational cost for finding optimal\nprompts for every test sample hinders the deployment of VICL, as determining\nwhich demonstrations to use for constructing prompts is very costly. In this\npaper, however, we find a counterintuitive phenomenon that most test samples\nactually achieve optimal performance under the same prompts, and searching for\nsample-level prompts only costs more time but results in completely identical\nprompts. Therefore, we propose task-level prompting to reduce the cost of\nsearching for prompts during the inference stage and introduce two time-saving\nyet effective task-level prompt search strategies. Extensive experimental\nresults show that our proposed method can identify near-optimal prompts and\nreach the best VICL performance with a minimal cost that prior work has never\nachieved.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the development of Vision Foundation Models (VFMs) in recent years,\nVisual In-Context Learning (VICL) has become a better choice compared to\nmodifying models in most scenarios. Different from retraining or fine-tuning\nmodel, VICL does not require modifications to the model's weights or\narchitecture, and only needs a prompt with demonstrations to teach VFM how to\nsolve tasks. Currently, significant computational cost for finding optimal\nprompts for every test sample hinders the deployment of VICL, as determining\nwhich demonstrations to use for constructing prompts is very costly. In this\npaper, however, we find a counterintuitive phenomenon that most test samples\nactually achieve optimal performance under the same prompts, and searching for\nsample-level prompts only costs more time but results in completely identical\nprompts. Therefore, we propose task-level prompting to reduce the cost of\nsearching for prompts during the inference stage and introduce two time-saving\nyet effective task-level prompt search strategies. Extensive experimental\nresults show that our proposed method can identify near-optimal prompts and\nreach the best VICL performance with a minimal cost that prior work has never\nachieved."
                },
                "authors": [
                    {
                        "name": "Yan Zhu"
                    },
                    {
                        "name": "Huan Ma"
                    },
                    {
                        "name": "Changqing Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Changqing Zhang"
                },
                "author": "Changqing Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.08841v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.08841v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.08838v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.08838v1",
                "updated": "2025-01-15T14:47:02Z",
                "updated_parsed": [
                    2025,
                    1,
                    15,
                    14,
                    47,
                    2,
                    2,
                    15,
                    0
                ],
                "published": "2025-01-15T14:47:02Z",
                "published_parsed": [
                    2025,
                    1,
                    15,
                    14,
                    47,
                    2,
                    2,
                    15,
                    0
                ],
                "title": "ToMATO: Verbalizing the Mental States of Role-Playing LLMs for\n  Benchmarking Theory of Mind",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ToMATO: Verbalizing the Mental States of Role-Playing LLMs for\n  Benchmarking Theory of Mind"
                },
                "summary": "Existing Theory of Mind (ToM) benchmarks diverge from real-world scenarios in\nthree aspects: 1) they assess a limited range of mental states such as beliefs,\n2) false beliefs are not comprehensively explored, and 3) the diverse\npersonality traits of characters are overlooked. To address these challenges,\nwe introduce ToMATO, a new ToM benchmark formulated as multiple-choice QA over\nconversations. ToMATO is generated via LLM-LLM conversations featuring\ninformation asymmetry. By employing a prompting method that requires\nrole-playing LLMs to verbalize their thoughts before each utterance, we capture\nboth first- and second-order mental states across five categories: belief,\nintention, desire, emotion, and knowledge. These verbalized thoughts serve as\nanswers to questions designed to assess the mental states of characters within\nconversations. Furthermore, the information asymmetry introduced by hiding\nthoughts from others induces the generation of false beliefs about various\nmental states. Assigning distinct personality traits to LLMs further\ndiversifies both utterances and thoughts. ToMATO consists of 5.4k questions,\n753 conversations, and 15 personality trait patterns. Our analysis shows that\nthis dataset construction approach frequently generates false beliefs due to\nthe information asymmetry between role-playing LLMs, and effectively reflects\ndiverse personalities. We evaluate nine LLMs on ToMATO and find that even\nGPT-4o mini lags behind human performance, especially in understanding false\nbeliefs, and lacks robustness to various personality traits.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Existing Theory of Mind (ToM) benchmarks diverge from real-world scenarios in\nthree aspects: 1) they assess a limited range of mental states such as beliefs,\n2) false beliefs are not comprehensively explored, and 3) the diverse\npersonality traits of characters are overlooked. To address these challenges,\nwe introduce ToMATO, a new ToM benchmark formulated as multiple-choice QA over\nconversations. ToMATO is generated via LLM-LLM conversations featuring\ninformation asymmetry. By employing a prompting method that requires\nrole-playing LLMs to verbalize their thoughts before each utterance, we capture\nboth first- and second-order mental states across five categories: belief,\nintention, desire, emotion, and knowledge. These verbalized thoughts serve as\nanswers to questions designed to assess the mental states of characters within\nconversations. Furthermore, the information asymmetry introduced by hiding\nthoughts from others induces the generation of false beliefs about various\nmental states. Assigning distinct personality traits to LLMs further\ndiversifies both utterances and thoughts. ToMATO consists of 5.4k questions,\n753 conversations, and 15 personality trait patterns. Our analysis shows that\nthis dataset construction approach frequently generates false beliefs due to\nthe information asymmetry between role-playing LLMs, and effectively reflects\ndiverse personalities. We evaluate nine LLMs on ToMATO and find that even\nGPT-4o mini lags behind human performance, especially in understanding false\nbeliefs, and lacks robustness to various personality traits."
                },
                "authors": [
                    {
                        "name": "Kazutoshi Shinoda"
                    },
                    {
                        "name": "Nobukatsu Hojo"
                    },
                    {
                        "name": "Kyosuke Nishida"
                    },
                    {
                        "name": "Saki Mizuno"
                    },
                    {
                        "name": "Keita Suzuki"
                    },
                    {
                        "name": "Ryo Masumura"
                    },
                    {
                        "name": "Hiroaki Sugiyama"
                    },
                    {
                        "name": "Kuniko Saito"
                    }
                ],
                "author_detail": {
                    "name": "Kuniko Saito"
                },
                "author": "Kuniko Saito",
                "arxiv_comment": "Accepted by AAAI 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.08838v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.08838v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.11192v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.11192v2",
                "updated": "2025-01-15T14:38:01Z",
                "updated_parsed": [
                    2025,
                    1,
                    15,
                    14,
                    38,
                    1,
                    2,
                    15,
                    0
                ],
                "published": "2024-06-17T03:57:35Z",
                "published_parsed": [
                    2024,
                    6,
                    17,
                    3,
                    57,
                    35,
                    0,
                    169,
                    0
                ],
                "title": "Beyond Boundaries: Learning a Universal Entity Taxonomy across Datasets\n  and Languages for Open Named Entity Recognition",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Beyond Boundaries: Learning a Universal Entity Taxonomy across Datasets\n  and Languages for Open Named Entity Recognition"
                },
                "summary": "Open Named Entity Recognition (NER), which involves identifying arbitrary\ntypes of entities from arbitrary domains, remains challenging for Large\nLanguage Models (LLMs). Recent studies suggest that fine-tuning LLMs on\nextensive NER data can boost their performance. However, training directly on\nexisting datasets neglects their inconsistent entity definitions and redundant\ndata, limiting LLMs to dataset-specific learning and hindering out-of-domain\nadaptation. To address this, we present B2NERD, a compact dataset designed to\nguide LLMs' generalization in Open NER under a universal entity taxonomy.\nB2NERD is refined from 54 existing English and Chinese datasets using a\ntwo-step process. First, we detect inconsistent entity definitions across\ndatasets and clarify them by distinguishable label names to construct a\nuniversal taxonomy of 400+ entity types. Second, we address redundancy using a\ndata pruning strategy that selects fewer samples with greater category and\nsemantic diversity. Comprehensive evaluation shows that B2NERD significantly\nenhances LLMs' Open NER capabilities. Our B2NER models, trained on B2NERD,\noutperform GPT-4 by 6.8-12.0 F1 points and surpass previous methods in 3\nout-of-domain benchmarks across 15 datasets and 6 languages. The data, models,\nand code are publicly available at https://github.com/UmeanNever/B2NER.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Open Named Entity Recognition (NER), which involves identifying arbitrary\ntypes of entities from arbitrary domains, remains challenging for Large\nLanguage Models (LLMs). Recent studies suggest that fine-tuning LLMs on\nextensive NER data can boost their performance. However, training directly on\nexisting datasets neglects their inconsistent entity definitions and redundant\ndata, limiting LLMs to dataset-specific learning and hindering out-of-domain\nadaptation. To address this, we present B2NERD, a compact dataset designed to\nguide LLMs' generalization in Open NER under a universal entity taxonomy.\nB2NERD is refined from 54 existing English and Chinese datasets using a\ntwo-step process. First, we detect inconsistent entity definitions across\ndatasets and clarify them by distinguishable label names to construct a\nuniversal taxonomy of 400+ entity types. Second, we address redundancy using a\ndata pruning strategy that selects fewer samples with greater category and\nsemantic diversity. Comprehensive evaluation shows that B2NERD significantly\nenhances LLMs' Open NER capabilities. Our B2NER models, trained on B2NERD,\noutperform GPT-4 by 6.8-12.0 F1 points and surpass previous methods in 3\nout-of-domain benchmarks across 15 datasets and 6 languages. The data, models,\nand code are publicly available at https://github.com/UmeanNever/B2NER."
                },
                "authors": [
                    {
                        "name": "Yuming Yang"
                    },
                    {
                        "name": "Wantong Zhao"
                    },
                    {
                        "name": "Caishuang Huang"
                    },
                    {
                        "name": "Junjie Ye"
                    },
                    {
                        "name": "Xiao Wang"
                    },
                    {
                        "name": "Huiyuan Zheng"
                    },
                    {
                        "name": "Yang Nan"
                    },
                    {
                        "name": "Yuran Wang"
                    },
                    {
                        "name": "Xueying Xu"
                    },
                    {
                        "name": "Kaixin Huang"
                    },
                    {
                        "name": "Yunke Zhang"
                    },
                    {
                        "name": "Tao Gui"
                    },
                    {
                        "name": "Qi Zhang"
                    },
                    {
                        "name": "Xuanjing Huang"
                    }
                ],
                "author_detail": {
                    "name": "Xuanjing Huang"
                },
                "author": "Xuanjing Huang",
                "arxiv_comment": "Accepted at COLING 2025. Camera-ready version updated. Project page:\n  https://github.com/UmeanNever/B2NER",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.11192v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.11192v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.08810v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.08810v1",
                "updated": "2025-01-15T14:09:17Z",
                "updated_parsed": [
                    2025,
                    1,
                    15,
                    14,
                    9,
                    17,
                    2,
                    15,
                    0
                ],
                "published": "2025-01-15T14:09:17Z",
                "published_parsed": [
                    2025,
                    1,
                    15,
                    14,
                    9,
                    17,
                    2,
                    15,
                    0
                ],
                "title": "Nonparametric inference for Poisson-Laguerre tessellations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Nonparametric inference for Poisson-Laguerre tessellations"
                },
                "summary": "In this paper, we consider statistical inference for Poisson-Laguerre\ntessellations in $\\mathbb{R}^d$. The object of interest is a distribution\nfunction $F$ which uniquely determines the intensity measure of the underlying\nPoisson process. Two nonparametric estimators for $F$ are introduced which\ndepend only on the points of the Poisson process which generate non-empty cells\nand the actual cells corresponding to these points. The proposed estimators are\nproven to be strongly consistent, as the observation window expands unboundedly\nto the whole space. We also consider a stereological setting, where one is\ninterested in estimating the distribution function associated with the Poisson\nprocess of a higher dimensional Poisson-Laguerre tessellation, given that a\ncorresponding sectional Poisson-Laguerre tessellation is observed.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this paper, we consider statistical inference for Poisson-Laguerre\ntessellations in $\\mathbb{R}^d$. The object of interest is a distribution\nfunction $F$ which uniquely determines the intensity measure of the underlying\nPoisson process. Two nonparametric estimators for $F$ are introduced which\ndepend only on the points of the Poisson process which generate non-empty cells\nand the actual cells corresponding to these points. The proposed estimators are\nproven to be strongly consistent, as the observation window expands unboundedly\nto the whole space. We also consider a stereological setting, where one is\ninterested in estimating the distribution function associated with the Poisson\nprocess of a higher dimensional Poisson-Laguerre tessellation, given that a\ncorresponding sectional Poisson-Laguerre tessellation is observed."
                },
                "authors": [
                    {
                        "name": "Thomas van der Jagt"
                    },
                    {
                        "name": "Geurt Jongbloed"
                    },
                    {
                        "name": "Martina Vittorietti"
                    }
                ],
                "author_detail": {
                    "name": "Martina Vittorietti"
                },
                "author": "Martina Vittorietti",
                "arxiv_comment": "31 pages, 6 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.08810v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.08810v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "math.ST",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "math.ST",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.TH",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "62G05, 60D05",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.03093v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.03093v3",
                "updated": "2025-01-15T13:46:19Z",
                "updated_parsed": [
                    2025,
                    1,
                    15,
                    13,
                    46,
                    19,
                    2,
                    15,
                    0
                ],
                "published": "2024-09-04T21:46:18Z",
                "published_parsed": [
                    2024,
                    9,
                    4,
                    21,
                    46,
                    18,
                    2,
                    248,
                    0
                ],
                "title": "ASTER: Natural and Multi-language Unit Test Generation with LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ASTER: Natural and Multi-language Unit Test Generation with LLMs"
                },
                "summary": "Implementing automated unit tests is an important but time-consuming activity\nin software development. To assist developers in this task, many techniques for\nautomating unit test generation have been developed. However, despite this\neffort, usable tools exist for very few programming languages. Moreover,\nstudies have found that automatically generated tests suffer poor readability\nand do not resemble developer-written tests. In this work, we present a\nrigorous investigation of how large language models (LLMs) can help bridge the\ngap. We describe a generic pipeline that incorporates static analysis to guide\nLLMs in generating compilable and high-coverage test cases. We illustrate how\nthe pipeline can be applied to different programming languages, specifically\nJava and Python, and to complex software requiring environment mocking. We\nconducted an empirical study to assess the quality of the generated tests in\nterms of code coverage and test naturalness -- evaluating them on standard as\nwell as enterprise Java applications and a large Python benchmark. Our results\ndemonstrate that LLM-based test generation, when guided by static analysis, can\nbe competitive with, and even outperform, state-of-the-art test-generation\ntechniques in coverage achieved while also producing considerably more natural\ntest cases that developers find easy to understand. We also present the results\nof a user study, conducted with 161 professional developers, that highlights\nthe naturalness characteristics of the tests generated by our approach.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Implementing automated unit tests is an important but time-consuming activity\nin software development. To assist developers in this task, many techniques for\nautomating unit test generation have been developed. However, despite this\neffort, usable tools exist for very few programming languages. Moreover,\nstudies have found that automatically generated tests suffer poor readability\nand do not resemble developer-written tests. In this work, we present a\nrigorous investigation of how large language models (LLMs) can help bridge the\ngap. We describe a generic pipeline that incorporates static analysis to guide\nLLMs in generating compilable and high-coverage test cases. We illustrate how\nthe pipeline can be applied to different programming languages, specifically\nJava and Python, and to complex software requiring environment mocking. We\nconducted an empirical study to assess the quality of the generated tests in\nterms of code coverage and test naturalness -- evaluating them on standard as\nwell as enterprise Java applications and a large Python benchmark. Our results\ndemonstrate that LLM-based test generation, when guided by static analysis, can\nbe competitive with, and even outperform, state-of-the-art test-generation\ntechniques in coverage achieved while also producing considerably more natural\ntest cases that developers find easy to understand. We also present the results\nof a user study, conducted with 161 professional developers, that highlights\nthe naturalness characteristics of the tests generated by our approach."
                },
                "authors": [
                    {
                        "name": "Rangeet Pan"
                    },
                    {
                        "name": "Myeongsoo Kim"
                    },
                    {
                        "name": "Rahul Krishna"
                    },
                    {
                        "name": "Raju Pavuluri"
                    },
                    {
                        "name": "Saurabh Sinha"
                    }
                ],
                "author_detail": {
                    "name": "Saurabh Sinha"
                },
                "author": "Saurabh Sinha",
                "arxiv_comment": "Accepted at ICSE-SEIP, 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.03093v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.03093v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2403.12967v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2403.12967v2",
                "updated": "2025-01-15T13:26:44Z",
                "updated_parsed": [
                    2025,
                    1,
                    15,
                    13,
                    26,
                    44,
                    2,
                    15,
                    0
                ],
                "published": "2024-03-19T17:59:55Z",
                "published_parsed": [
                    2024,
                    3,
                    19,
                    17,
                    59,
                    55,
                    1,
                    79,
                    0
                ],
                "title": "The FLAMINGO project: the coupling between baryonic feedback and\n  cosmology in light of the $S_8$ tension",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The FLAMINGO project: the coupling between baryonic feedback and\n  cosmology in light of the $S_8$ tension"
                },
                "summary": "Large-scale structure surveys have reported measurements of the density of\nmatter, $\\Omega_\\mathrm{m}$, and the amplitude of clustering, $\\sigma_8$, that\nare in tension with the values inferred from observations of the cosmic\nmicrowave background. While this may be a sign of new physics that slows the\ngrowth of structure at late times, strong astrophysical feedback processes\ncould also be responsible. In this work, we argue that astrophysical processes\nare not independent of cosmology and that their coupling naturally leads to\nstronger baryonic feedback in cosmological models with suppressed structure\nformation or when combined with a mechanism that removes dark matter from\nhalos. We illustrate this with two well-motivated extensions of the Standard\nModel known to suppress structure formation: massive neutrinos and decaying\ndark matter. Our results, based on the FLAMINGO suite of hydrodynamical\nsimulations, show that the combined effect of baryonic and non-baryonic\nsuppression mechanisms is greater than the sum of its parts, particularly for\ndecaying dark matter. We also show that the dependence of baryonic feedback on\ncosmology can be modelled as a function of the ratio\n$f_\\mathrm{b}/c^2_\\mathrm{v}\\sim\nf_\\mathrm{b}/(\\Omega_\\mathrm{m}\\sigma_8)^{1/4}$ of the universal baryon\nfraction, $f_\\mathrm{b}$, to a velocity-based definition of halo concentration,\n$c^2_\\mathrm{v}$, giving an accurate fitting formula for the baryonic\nsuppression of the matter power spectrum. Although the combination of baryonic\nand non-baryonic suppression mechanisms can resolve the tension, the models\nwith neutrinos and decaying dark matter are challenged by constraints on the\nexpansion history.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large-scale structure surveys have reported measurements of the density of\nmatter, $\\Omega_\\mathrm{m}$, and the amplitude of clustering, $\\sigma_8$, that\nare in tension with the values inferred from observations of the cosmic\nmicrowave background. While this may be a sign of new physics that slows the\ngrowth of structure at late times, strong astrophysical feedback processes\ncould also be responsible. In this work, we argue that astrophysical processes\nare not independent of cosmology and that their coupling naturally leads to\nstronger baryonic feedback in cosmological models with suppressed structure\nformation or when combined with a mechanism that removes dark matter from\nhalos. We illustrate this with two well-motivated extensions of the Standard\nModel known to suppress structure formation: massive neutrinos and decaying\ndark matter. Our results, based on the FLAMINGO suite of hydrodynamical\nsimulations, show that the combined effect of baryonic and non-baryonic\nsuppression mechanisms is greater than the sum of its parts, particularly for\ndecaying dark matter. We also show that the dependence of baryonic feedback on\ncosmology can be modelled as a function of the ratio\n$f_\\mathrm{b}/c^2_\\mathrm{v}\\sim\nf_\\mathrm{b}/(\\Omega_\\mathrm{m}\\sigma_8)^{1/4}$ of the universal baryon\nfraction, $f_\\mathrm{b}$, to a velocity-based definition of halo concentration,\n$c^2_\\mathrm{v}$, giving an accurate fitting formula for the baryonic\nsuppression of the matter power spectrum. Although the combination of baryonic\nand non-baryonic suppression mechanisms can resolve the tension, the models\nwith neutrinos and decaying dark matter are challenged by constraints on the\nexpansion history."
                },
                "authors": [
                    {
                        "name": "Willem Elbers"
                    },
                    {
                        "name": "Carlos S. Frenk"
                    },
                    {
                        "name": "Adrian Jenkins"
                    },
                    {
                        "name": "Baojiu Li"
                    },
                    {
                        "name": "John C. Helly"
                    },
                    {
                        "name": "Roi Kugel"
                    },
                    {
                        "name": "Matthieu Schaller"
                    },
                    {
                        "name": "Joop Schaye"
                    },
                    {
                        "name": "Joey Braspenning"
                    },
                    {
                        "name": "Juliana Kwan"
                    },
                    {
                        "name": "Ian G. McCarthy"
                    },
                    {
                        "name": "Jaime Salcido"
                    },
                    {
                        "name": "Marcel P. van Daalen"
                    },
                    {
                        "name": "Bert Vandenbroucke"
                    },
                    {
                        "name": "Silvia Pascoli"
                    }
                ],
                "author_detail": {
                    "name": "Silvia Pascoli"
                },
                "author": "Silvia Pascoli",
                "arxiv_comment": "18 pages, 14 figures, MNRAS, accepted for publication",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2403.12967v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2403.12967v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.CO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.CO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "astro-ph.GA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.08786v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.08786v1",
                "updated": "2025-01-15T13:16:39Z",
                "updated_parsed": [
                    2025,
                    1,
                    15,
                    13,
                    16,
                    39,
                    2,
                    15,
                    0
                ],
                "published": "2025-01-15T13:16:39Z",
                "published_parsed": [
                    2025,
                    1,
                    15,
                    13,
                    16,
                    39,
                    2,
                    15,
                    0
                ],
                "title": "Differentiability and overlap concentration in optimal Bayesian\n  inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Differentiability and overlap concentration in optimal Bayesian\n  inference"
                },
                "summary": "In this short note, we consider models of optimal Bayesian inference of\nfinite-rank tensor products. We add to the model a linear channel parametrized\nby $h$. We show that at every interior differentiable point $h$ of the free\nenergy (associated with the model), the overlap concentrates at the gradient of\nthe free energy and the minimum mean-square error converges to a related limit.\nIn other words, the model is replica-symmetric at every differentiable point.\nAt any signal-to-noise ratio, such points $h$ form a full-measure set (hence\n$h=0$ belongs to the closure of these points). For a sufficiently low\nsignal-to-noise ratio, we show that every interior point is a differentiable\npoint.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this short note, we consider models of optimal Bayesian inference of\nfinite-rank tensor products. We add to the model a linear channel parametrized\nby $h$. We show that at every interior differentiable point $h$ of the free\nenergy (associated with the model), the overlap concentrates at the gradient of\nthe free energy and the minimum mean-square error converges to a related limit.\nIn other words, the model is replica-symmetric at every differentiable point.\nAt any signal-to-noise ratio, such points $h$ form a full-measure set (hence\n$h=0$ belongs to the closure of these points). For a sufficiently low\nsignal-to-noise ratio, we show that every interior point is a differentiable\npoint."
                },
                "authors": [
                    {
                        "name": "Hong-Bin Chen"
                    },
                    {
                        "name": "Victor Issa"
                    }
                ],
                "author_detail": {
                    "name": "Victor Issa"
                },
                "arxiv_affiliation": "ENS de Lyon",
                "author": "Victor Issa",
                "arxiv_comment": "17 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.08786v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.08786v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "math.PR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "math.PR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.ST",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.TH",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2312.17345v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2312.17345v2",
                "updated": "2025-01-15T12:46:07Z",
                "updated_parsed": [
                    2025,
                    1,
                    15,
                    12,
                    46,
                    7,
                    2,
                    15,
                    0
                ],
                "published": "2023-12-28T20:26:03Z",
                "published_parsed": [
                    2023,
                    12,
                    28,
                    20,
                    26,
                    3,
                    3,
                    362,
                    0
                ],
                "title": "3VL: Using Trees to Improve Vision-Language Models' Interpretability",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "3VL: Using Trees to Improve Vision-Language Models' Interpretability"
                },
                "summary": "Vision-Language models (VLMs) have proven to be effective at aligning image\nand text representations, producing superior zero-shot results when transferred\nto many downstream tasks. However, these representations suffer from some key\nshortcomings in understanding Compositional Language Concepts (CLC), such as\nrecognizing objects' attributes, states, and relations between different\nobjects. Moreover, VLMs typically have poor interpretability, making it\nchallenging to debug and mitigate compositional-understanding failures. In this\nwork, we introduce the architecture and training technique of Tree-augmented\nVision-Language (3VL) model accompanied by our proposed Anchor inference method\nand Differential Relevance (DiRe) interpretability tool. By expanding the text\nof an arbitrary image-text pair into a hierarchical tree structure using\nlanguage analysis tools, 3VL allows the induction of this structure into the\nvisual representation learned by the model, enhancing its interpretability and\ncompositional reasoning. Additionally, we show how Anchor, a simple technique\nfor text unification, can be used to filter nuisance factors while increasing\nCLC understanding performance, e.g., on the fundamental VL-Checklist benchmark.\nWe also show how DiRe, which performs a differential comparison between VLM\nrelevancy maps, enables us to generate compelling visualizations of the reasons\nfor a model's success or failure. Our code is available at:\nhttps://github.com/niryellinek/3VL.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Vision-Language models (VLMs) have proven to be effective at aligning image\nand text representations, producing superior zero-shot results when transferred\nto many downstream tasks. However, these representations suffer from some key\nshortcomings in understanding Compositional Language Concepts (CLC), such as\nrecognizing objects' attributes, states, and relations between different\nobjects. Moreover, VLMs typically have poor interpretability, making it\nchallenging to debug and mitigate compositional-understanding failures. In this\nwork, we introduce the architecture and training technique of Tree-augmented\nVision-Language (3VL) model accompanied by our proposed Anchor inference method\nand Differential Relevance (DiRe) interpretability tool. By expanding the text\nof an arbitrary image-text pair into a hierarchical tree structure using\nlanguage analysis tools, 3VL allows the induction of this structure into the\nvisual representation learned by the model, enhancing its interpretability and\ncompositional reasoning. Additionally, we show how Anchor, a simple technique\nfor text unification, can be used to filter nuisance factors while increasing\nCLC understanding performance, e.g., on the fundamental VL-Checklist benchmark.\nWe also show how DiRe, which performs a differential comparison between VLM\nrelevancy maps, enables us to generate compelling visualizations of the reasons\nfor a model's success or failure. Our code is available at:\nhttps://github.com/niryellinek/3VL."
                },
                "authors": [
                    {
                        "name": "Nir Yellinek"
                    },
                    {
                        "name": "Leonid Karlinsky"
                    },
                    {
                        "name": "Raja Giryes"
                    }
                ],
                "author_detail": {
                    "name": "Raja Giryes"
                },
                "author": "Raja Giryes",
                "arxiv_comment": "accepted to IEEE TIP",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2312.17345v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2312.17345v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.08769v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.08769v1",
                "updated": "2025-01-15T12:42:09Z",
                "updated_parsed": [
                    2025,
                    1,
                    15,
                    12,
                    42,
                    9,
                    2,
                    15,
                    0
                ],
                "published": "2025-01-15T12:42:09Z",
                "published_parsed": [
                    2025,
                    1,
                    15,
                    12,
                    42,
                    9,
                    2,
                    15,
                    0
                ],
                "title": "Enhanced Large Language Models for Effective Screening of Depression and\n  Anxiety",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Enhanced Large Language Models for Effective Screening of Depression and\n  Anxiety"
                },
                "summary": "Depressive and anxiety disorders are widespread, necessitating timely\nidentification and management. Recent advances in Large Language Models (LLMs)\noffer potential solutions, yet high costs and ethical concerns about training\ndata remain challenges. This paper introduces a pipeline for synthesizing\nclinical interviews, resulting in 1,157 interactive dialogues (PsyInterview),\nand presents EmoScan, an LLM-based emotional disorder screening system. EmoScan\ndistinguishes between coarse (e.g., anxiety or depressive disorders) and fine\ndisorders (e.g., major depressive disorders) and conducts high-quality\ninterviews. Evaluations showed that EmoScan exceeded the performance of base\nmodels and other LLMs like GPT-4 in screening emotional disorders\n(F1-score=0.7467). It also delivers superior explanations (BERTScore=0.9408)\nand demonstrates robust generalizability (F1-score of 0.67 on an external\ndataset). Furthermore, EmoScan outperforms baselines in interviewing skills, as\nvalidated by automated ratings and human evaluations. This work highlights the\nimportance of scalable data-generative pipelines for developing effective\nmental health LLM tools.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Depressive and anxiety disorders are widespread, necessitating timely\nidentification and management. Recent advances in Large Language Models (LLMs)\noffer potential solutions, yet high costs and ethical concerns about training\ndata remain challenges. This paper introduces a pipeline for synthesizing\nclinical interviews, resulting in 1,157 interactive dialogues (PsyInterview),\nand presents EmoScan, an LLM-based emotional disorder screening system. EmoScan\ndistinguishes between coarse (e.g., anxiety or depressive disorders) and fine\ndisorders (e.g., major depressive disorders) and conducts high-quality\ninterviews. Evaluations showed that EmoScan exceeded the performance of base\nmodels and other LLMs like GPT-4 in screening emotional disorders\n(F1-score=0.7467). It also delivers superior explanations (BERTScore=0.9408)\nand demonstrates robust generalizability (F1-score of 0.67 on an external\ndataset). Furthermore, EmoScan outperforms baselines in interviewing skills, as\nvalidated by automated ratings and human evaluations. This work highlights the\nimportance of scalable data-generative pipelines for developing effective\nmental health LLM tools."
                },
                "authors": [
                    {
                        "name": "June M. Liu"
                    },
                    {
                        "name": "Mengxia Gao"
                    },
                    {
                        "name": "Sahand Sabour"
                    },
                    {
                        "name": "Zhuang Chen"
                    },
                    {
                        "name": "Minlie Huang"
                    },
                    {
                        "name": "Tatia M. C. Lee"
                    }
                ],
                "author_detail": {
                    "name": "Tatia M. C. Lee"
                },
                "author": "Tatia M. C. Lee",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.08769v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.08769v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.08760v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.08760v1",
                "updated": "2025-01-15T12:25:56Z",
                "updated_parsed": [
                    2025,
                    1,
                    15,
                    12,
                    25,
                    56,
                    2,
                    15,
                    0
                ],
                "published": "2025-01-15T12:25:56Z",
                "published_parsed": [
                    2025,
                    1,
                    15,
                    12,
                    25,
                    56,
                    2,
                    15,
                    0
                ],
                "title": "Leveraging LLM Agents for Translating Network Configurations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Leveraging LLM Agents for Translating Network Configurations"
                },
                "summary": "Configuration translation is a critical and frequent task in network\noperations. When a network device is damaged or outdated, administrators need\nto replace it to maintain service continuity. The replacement devices may\noriginate from different vendors, necessitating configuration translation to\nensure seamless network operation. However, translating configurations manually\nis a labor-intensive and error-prone process. In this paper, we propose an\nintent-based framework for translating network configuration with Large\nLanguage Model (LLM) Agents. The core of our approach is an Intent-based\nRetrieval Augmented Generation (IRAG) module that systematically splits a\nconfiguration file into fragments, extracts intents, and generates accurate\ntranslations. We also design a two-stage verification method to validate the\nsyntax and semantics correctness of the translated configurations. We implement\nand evaluate the proposed method on real-world network configurations.\nExperimental results show that our method achieves 97.74% syntax correctness,\noutperforming state-of-the-art methods in translation accuracy.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Configuration translation is a critical and frequent task in network\noperations. When a network device is damaged or outdated, administrators need\nto replace it to maintain service continuity. The replacement devices may\noriginate from different vendors, necessitating configuration translation to\nensure seamless network operation. However, translating configurations manually\nis a labor-intensive and error-prone process. In this paper, we propose an\nintent-based framework for translating network configuration with Large\nLanguage Model (LLM) Agents. The core of our approach is an Intent-based\nRetrieval Augmented Generation (IRAG) module that systematically splits a\nconfiguration file into fragments, extracts intents, and generates accurate\ntranslations. We also design a two-stage verification method to validate the\nsyntax and semantics correctness of the translated configurations. We implement\nand evaluate the proposed method on real-world network configurations.\nExperimental results show that our method achieves 97.74% syntax correctness,\noutperforming state-of-the-art methods in translation accuracy."
                },
                "authors": [
                    {
                        "name": "Yunze Wei"
                    },
                    {
                        "name": "Xiaohui Xie"
                    },
                    {
                        "name": "Yiwei Zuo"
                    },
                    {
                        "name": "Tianshuo Hu"
                    },
                    {
                        "name": "Xinyi Chen"
                    },
                    {
                        "name": "Kaiwen Chi"
                    },
                    {
                        "name": "Yong Cui"
                    }
                ],
                "author_detail": {
                    "name": "Yong Cui"
                },
                "author": "Yong Cui",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.08760v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.08760v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.08748v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.08748v1",
                "updated": "2025-01-15T12:06:49Z",
                "updated_parsed": [
                    2025,
                    1,
                    15,
                    12,
                    6,
                    49,
                    2,
                    15,
                    0
                ],
                "published": "2025-01-15T12:06:49Z",
                "published_parsed": [
                    2025,
                    1,
                    15,
                    12,
                    6,
                    49,
                    2,
                    15,
                    0
                ],
                "title": "A Semi-Parametric Bayesian Spatial Model for Rainfall Events in\n  Geographically Complex Domains",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Semi-Parametric Bayesian Spatial Model for Rainfall Events in\n  Geographically Complex Domains"
                },
                "summary": "Environmental phenomena are influenced by complex interactions among various\nfactors. For instance, the amount of rainfall measured at different stations\nwithin a given area is shaped by atmospheric conditions, orography, and physics\nof water processes. Motivated by the need to analyze rainfall across complex\nspatial locations, we propose a flexible Bayesian semi-parametric model for\nspatially distributed data. This method effectively accounts for spatial\ncorrelation while incorporating dependencies on geographical characteristics in\na highly flexible manner. Indeed, using latent Gaussian processes, indexed by\nspatial coordinates and topographical features, the model integrates spatial\ndependencies and environmental characteristics within a nonparametric\nframework. Posterior inference is conducted using an efficient rejection-free\nMarkov Chain Monte Carlo algorithm, which eliminates the need for tuning\nparameter calibration, ensuring smoother and more reliable estimation. The\nmodel's flexibility is evaluated through a series of simulation studies,\ninvolving different rainfall and spatial correlation scenarios, to demonstrate\nits robustness across various conditions. We then apply the model to a large\ndataset of rainfall events collected from the Italian regions of Veneto and\nTrentino-Alto Adige, these areas are known for their complex orography and\ndiverse meteorological drivers. By analyzing this data, we generate detailed\nmaps that illustrate the mean and variance of rainfall and rainy days. The\nmethod is implemented in a new R package available on GitHub.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Environmental phenomena are influenced by complex interactions among various\nfactors. For instance, the amount of rainfall measured at different stations\nwithin a given area is shaped by atmospheric conditions, orography, and physics\nof water processes. Motivated by the need to analyze rainfall across complex\nspatial locations, we propose a flexible Bayesian semi-parametric model for\nspatially distributed data. This method effectively accounts for spatial\ncorrelation while incorporating dependencies on geographical characteristics in\na highly flexible manner. Indeed, using latent Gaussian processes, indexed by\nspatial coordinates and topographical features, the model integrates spatial\ndependencies and environmental characteristics within a nonparametric\nframework. Posterior inference is conducted using an efficient rejection-free\nMarkov Chain Monte Carlo algorithm, which eliminates the need for tuning\nparameter calibration, ensuring smoother and more reliable estimation. The\nmodel's flexibility is evaluated through a series of simulation studies,\ninvolving different rainfall and spatial correlation scenarios, to demonstrate\nits robustness across various conditions. We then apply the model to a large\ndataset of rainfall events collected from the Italian regions of Veneto and\nTrentino-Alto Adige, these areas are known for their complex orography and\ndiverse meteorological drivers. By analyzing this data, we generate detailed\nmaps that illustrate the mean and variance of rainfall and rainy days. The\nmethod is implemented in a new R package available on GitHub."
                },
                "authors": [
                    {
                        "name": "Paolo Onorati"
                    },
                    {
                        "name": "Antonio Canale"
                    }
                ],
                "author_detail": {
                    "name": "Antonio Canale"
                },
                "author": "Antonio Canale",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.08748v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.08748v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.AP",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.AP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.CO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.08109v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.08109v2",
                "updated": "2025-01-15T11:57:34Z",
                "updated_parsed": [
                    2025,
                    1,
                    15,
                    11,
                    57,
                    34,
                    2,
                    15,
                    0
                ],
                "published": "2024-12-11T05:31:39Z",
                "published_parsed": [
                    2024,
                    12,
                    11,
                    5,
                    31,
                    39,
                    2,
                    346,
                    0
                ],
                "title": "Unseen Horizons: Unveiling the Real Capability of LLM Code Generation\n  Beyond the Familiar",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Unseen Horizons: Unveiling the Real Capability of LLM Code Generation\n  Beyond the Familiar"
                },
                "summary": "Recently, large language models (LLMs) have shown strong potential in code\ngeneration tasks. However, there are still gaps before they can be fully\napplied in actual software development processes. Accurately assessing the code\ngeneration capabilities of large language models has become an important basis\nfor evaluating and improving the models. Some existing works have constructed\ndatasets to evaluate the capabilities of these models. However, the current\nevaluation process may encounter the illusion of \"Specialist in Familiarity\",\nprimarily due to three gaps: the exposure of target code, case timeliness, and\ndependency availability. The fundamental reason for these gaps is that the code\nin current datasets may have been extensively exposed and exercised during the\ntraining phase, and due to the continuous training and development of LLM,\ntheir timeliness has been severely compromised. The key to solve the problem is\nto, as much as possible, evaluate the LLMs using code that they have not\nencountered before. Thus, the fundamental idea in this paper is to draw on the\nconcept of code obfuscation, changing code at different levels while ensuring\nthe functionality and output. To this end, we build a code-obfuscation based\nbenchmark OBFUSEVAL. We first collect 1,354 raw cases from five real-world\nprojects, including function description and code. Then we use three-level\nstrategy (symbol, structure and semantic) to obfuscate descriptions, code and\ncontext dependencies. We evaluate four LLMs on OBFU- SEVAL and compared the\neffectiveness of different obfuscation strategy. We use official test suites of\nthese projects to evaluate the generated code. The results show that after\nobfuscation, the average decrease ratio of test pass rate can up to 62.5%.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recently, large language models (LLMs) have shown strong potential in code\ngeneration tasks. However, there are still gaps before they can be fully\napplied in actual software development processes. Accurately assessing the code\ngeneration capabilities of large language models has become an important basis\nfor evaluating and improving the models. Some existing works have constructed\ndatasets to evaluate the capabilities of these models. However, the current\nevaluation process may encounter the illusion of \"Specialist in Familiarity\",\nprimarily due to three gaps: the exposure of target code, case timeliness, and\ndependency availability. The fundamental reason for these gaps is that the code\nin current datasets may have been extensively exposed and exercised during the\ntraining phase, and due to the continuous training and development of LLM,\ntheir timeliness has been severely compromised. The key to solve the problem is\nto, as much as possible, evaluate the LLMs using code that they have not\nencountered before. Thus, the fundamental idea in this paper is to draw on the\nconcept of code obfuscation, changing code at different levels while ensuring\nthe functionality and output. To this end, we build a code-obfuscation based\nbenchmark OBFUSEVAL. We first collect 1,354 raw cases from five real-world\nprojects, including function description and code. Then we use three-level\nstrategy (symbol, structure and semantic) to obfuscate descriptions, code and\ncontext dependencies. We evaluate four LLMs on OBFU- SEVAL and compared the\neffectiveness of different obfuscation strategy. We use official test suites of\nthese projects to evaluate the generated code. The results show that after\nobfuscation, the average decrease ratio of test pass rate can up to 62.5%."
                },
                "authors": [
                    {
                        "name": "Yuanliang Zhang"
                    },
                    {
                        "name": "Yifan Xie"
                    },
                    {
                        "name": "Shanshan Li"
                    },
                    {
                        "name": "Ke Liu"
                    },
                    {
                        "name": "Chong Wang"
                    },
                    {
                        "name": "Zhouyang Jia"
                    },
                    {
                        "name": "Xiangbing Huang"
                    },
                    {
                        "name": "Jie Song"
                    },
                    {
                        "name": "Chaopeng Luo"
                    },
                    {
                        "name": "Zhizheng Zheng"
                    },
                    {
                        "name": "Rulin Xu"
                    },
                    {
                        "name": "Yitong Liu"
                    },
                    {
                        "name": "Si Zheng"
                    },
                    {
                        "name": "Xiangke Liao"
                    }
                ],
                "author_detail": {
                    "name": "Xiangke Liao"
                },
                "author": "Xiangke Liao",
                "arxiv_comment": "Accepted by the 47th International Conference on Software Engineering\n  (ICSE 2025)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.08109v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.08109v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.03655v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.03655v2",
                "updated": "2025-01-15T11:57:24Z",
                "updated_parsed": [
                    2025,
                    1,
                    15,
                    11,
                    57,
                    24,
                    2,
                    15,
                    0
                ],
                "published": "2024-12-04T19:00:01Z",
                "published_parsed": [
                    2024,
                    12,
                    4,
                    19,
                    0,
                    1,
                    2,
                    339,
                    0
                ],
                "title": "Stepping Up Superradiance Constraints on Axions",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Stepping Up Superradiance Constraints on Axions"
                },
                "summary": "Light feebly-coupled bosonic particles can efficiently extract the rotational\nenergy of rapidly spinning black holes on sub-astrophysical timescales via a\nphenomenon known as black hole superradiance. In the case of light axions, the\nfeeble self-interactions of these particles can lead to a non-linear coupled\nevolution of many superradiant quasi-bound states, dramatically altering the\nrate at which the black hole is spun down. In this work, we extend the study of\naxion superradiance to higher order states, solving for the first time the\ncoupled evolution of all states with $n \\leq 5$ in the fully relativistic limit\n(with $n$ being the principle quantum number). Using a Bayesian framework, we\nre-derive constraints on axions using the inferred spins of solar mass black\nholes, demonstrating that previously adopted limit-setting procedures have\nunderestimated current sensitivity to the axion decay constant $f_a$ by around\none order of magnitude, and that the inclusion to higher order states allows\none to reasonably capture the evolution of typical high-spin black holes across\na much wider range of parameter space, thereby allowing constraints to be\nextended to more massive axions. We conclude with an extensive discussion on\nthe systematics associated with spin inference from x-ray observations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Light feebly-coupled bosonic particles can efficiently extract the rotational\nenergy of rapidly spinning black holes on sub-astrophysical timescales via a\nphenomenon known as black hole superradiance. In the case of light axions, the\nfeeble self-interactions of these particles can lead to a non-linear coupled\nevolution of many superradiant quasi-bound states, dramatically altering the\nrate at which the black hole is spun down. In this work, we extend the study of\naxion superradiance to higher order states, solving for the first time the\ncoupled evolution of all states with $n \\leq 5$ in the fully relativistic limit\n(with $n$ being the principle quantum number). Using a Bayesian framework, we\nre-derive constraints on axions using the inferred spins of solar mass black\nholes, demonstrating that previously adopted limit-setting procedures have\nunderestimated current sensitivity to the axion decay constant $f_a$ by around\none order of magnitude, and that the inclusion to higher order states allows\none to reasonably capture the evolution of typical high-spin black holes across\na much wider range of parameter space, thereby allowing constraints to be\nextended to more massive axions. We conclude with an extensive discussion on\nthe systematics associated with spin inference from x-ray observations."
                },
                "authors": [
                    {
                        "name": "Samuel J. Witte"
                    },
                    {
                        "name": "Andrew Mummery"
                    }
                ],
                "author_detail": {
                    "name": "Andrew Mummery"
                },
                "author": "Andrew Mummery",
                "arxiv_comment": "v2: Included 311 & 511 states, generalized to include all scattering\n  permutations, fixed small typo in data file, added citations. Slight change\n  in evolution seen for some points in parameter space, conclusions unchanged",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.03655v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.03655v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "hep-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "hep-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "astro-ph.HE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "gr-qc",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.08533v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.08533v2",
                "updated": "2025-01-15T11:42:25Z",
                "updated_parsed": [
                    2025,
                    1,
                    15,
                    11,
                    42,
                    25,
                    2,
                    15,
                    0
                ],
                "published": "2024-12-11T16:46:17Z",
                "published_parsed": [
                    2024,
                    12,
                    11,
                    16,
                    46,
                    17,
                    2,
                    346,
                    0
                ],
                "title": "Rate accelerated inference for integrals of multivariate random\n  functions",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Rate accelerated inference for integrals of multivariate random\n  functions"
                },
                "summary": "The computation of integrals is a fundamental task in the analysis of\nfunctional data, which are typically considered as random elements in a space\nof squared integrable functions. Borrowing ideas from recent advances in the\nMonte Carlo integration literature, we propose effective unbiased estimation\nand inference procedures for integrals of uni- and multivariate random\nfunctions. Several applications to key problems in functional data analysis\ninvolving random design points are studied and illustrated. In the absence of\nnoise, the proposed estimates converge faster than the sample mean and the\nusual algorithms for numerical integration. Moreover, the proposed estimator\nfacilitates effective inference by generally providing better coverage with\nshorter confidence and prediction intervals, in both noisy and noiseless\nsetups.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The computation of integrals is a fundamental task in the analysis of\nfunctional data, which are typically considered as random elements in a space\nof squared integrable functions. Borrowing ideas from recent advances in the\nMonte Carlo integration literature, we propose effective unbiased estimation\nand inference procedures for integrals of uni- and multivariate random\nfunctions. Several applications to key problems in functional data analysis\ninvolving random design points are studied and illustrated. In the absence of\nnoise, the proposed estimates converge faster than the sample mean and the\nusual algorithms for numerical integration. Moreover, the proposed estimator\nfacilitates effective inference by generally providing better coverage with\nshorter confidence and prediction intervals, in both noisy and noiseless\nsetups."
                },
                "authors": [
                    {
                        "name": "Valentin Patilea"
                    },
                    {
                        "name": "Sunny G. W. Wang"
                    }
                ],
                "author_detail": {
                    "name": "Sunny G. W. Wang"
                },
                "author": "Sunny G. W. Wang",
                "arxiv_comment": "26 pages, Supplementary Material separately available upon request",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.08533v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.08533v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ME",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "62R10, 62G08, 62M99, 62-08",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.03268v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.03268v2",
                "updated": "2025-01-15T11:39:56Z",
                "updated_parsed": [
                    2025,
                    1,
                    15,
                    11,
                    39,
                    56,
                    2,
                    15,
                    0
                ],
                "published": "2024-10-04T09:39:17Z",
                "published_parsed": [
                    2024,
                    10,
                    4,
                    9,
                    39,
                    17,
                    4,
                    278,
                    0
                ],
                "title": "Narrative Player: Reviving Data Narratives with Visuals",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Narrative Player: Reviving Data Narratives with Visuals"
                },
                "summary": "Data-rich documents are commonly found across various fields such as\nbusiness, finance, and science. However, a general limitation of these\ndocuments for reading is their reliance on text to convey data and facts.\nVisual representation of text aids in providing a satisfactory reading\nexperience in comprehension and engagement. However, existing work emphasizes\npresenting the insights of local text context, rather than fully conveying data\nstories within the whole paragraphs and engaging readers. To provide readers\nwith satisfactory data stories, this paper presents Narrative Player, a novel\nmethod that automatically revives data narratives with consistent and\ncontextualized visuals. Specifically, it accepts a paragraph and corresponding\ndata table as input and leverages LLMs to characterize the clauses and extract\ncontextualized data facts. Subsequently, the facts are transformed into a\ncoherent visualization sequence with a carefully designed optimization-based\napproach. Animations are also assigned between adjacent visualizations to\nenable seamless transitions. Finally, the visualization sequence, transition\nanimations, and audio narration generated by text-to-speech technologies are\nrendered into a data video. The evaluation results showed that the\nautomatic-generated data videos were well-received by participants and experts\nfor enhancing reading.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Data-rich documents are commonly found across various fields such as\nbusiness, finance, and science. However, a general limitation of these\ndocuments for reading is their reliance on text to convey data and facts.\nVisual representation of text aids in providing a satisfactory reading\nexperience in comprehension and engagement. However, existing work emphasizes\npresenting the insights of local text context, rather than fully conveying data\nstories within the whole paragraphs and engaging readers. To provide readers\nwith satisfactory data stories, this paper presents Narrative Player, a novel\nmethod that automatically revives data narratives with consistent and\ncontextualized visuals. Specifically, it accepts a paragraph and corresponding\ndata table as input and leverages LLMs to characterize the clauses and extract\ncontextualized data facts. Subsequently, the facts are transformed into a\ncoherent visualization sequence with a carefully designed optimization-based\napproach. Animations are also assigned between adjacent visualizations to\nenable seamless transitions. Finally, the visualization sequence, transition\nanimations, and audio narration generated by text-to-speech technologies are\nrendered into a data video. The evaluation results showed that the\nautomatic-generated data videos were well-received by participants and experts\nfor enhancing reading."
                },
                "authors": [
                    {
                        "name": "Zekai Shao"
                    },
                    {
                        "name": "Leixian Shen"
                    },
                    {
                        "name": "Haotian Li"
                    },
                    {
                        "name": "Yi Shan"
                    },
                    {
                        "name": "Huamin Qu"
                    },
                    {
                        "name": "Yun Wang"
                    },
                    {
                        "name": "Siming Chen"
                    }
                ],
                "author_detail": {
                    "name": "Siming Chen"
                },
                "author": "Siming Chen",
                "arxiv_comment": "Accepted by IEEE TVCG",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.03268v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.03268v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2311.17696v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2311.17696v5",
                "updated": "2025-01-15T11:12:26Z",
                "updated_parsed": [
                    2025,
                    1,
                    15,
                    11,
                    12,
                    26,
                    2,
                    15,
                    0
                ],
                "published": "2023-11-29T15:02:46Z",
                "published_parsed": [
                    2023,
                    11,
                    29,
                    15,
                    2,
                    46,
                    2,
                    333,
                    0
                ],
                "title": "How to Build an AI Tutor That Can Adapt to Any Course Using Knowledge\n  Graph-Enhanced Retrieval-Augmented Generation (KG-RAG)",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "How to Build an AI Tutor That Can Adapt to Any Course Using Knowledge\n  Graph-Enhanced Retrieval-Augmented Generation (KG-RAG)"
                },
                "summary": "This paper introduces a novel framework for adaptable AI tutors using\nKnowledge Graph-enhanced Retrieval-Augmented Generation (KG-RAG). This approach\naddresses the critical challenges of information hallucination and limited\ncourse-specific adaptation prevalent in Large Language Model (LLM)-based\ntutoring systems. By integrating Knowledge Graphs (KGs) with RAG, we provide a\nstructured representation of course concepts and their interrelationships,\ngrounding the AI tutor's responses in relevant, validated material. We leverage\nQwen2.5, a powerful and cost-effective LLM, within our KG-RAG framework. A user\nstudy (n=50) demonstrated positive student feedback regarding answer relevance,\nease of use, and overall satisfaction. This KG-RAG framework offers a promising\npathway towards personalized learning experiences and broader access to\nhigh-quality education.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper introduces a novel framework for adaptable AI tutors using\nKnowledge Graph-enhanced Retrieval-Augmented Generation (KG-RAG). This approach\naddresses the critical challenges of information hallucination and limited\ncourse-specific adaptation prevalent in Large Language Model (LLM)-based\ntutoring systems. By integrating Knowledge Graphs (KGs) with RAG, we provide a\nstructured representation of course concepts and their interrelationships,\ngrounding the AI tutor's responses in relevant, validated material. We leverage\nQwen2.5, a powerful and cost-effective LLM, within our KG-RAG framework. A user\nstudy (n=50) demonstrated positive student feedback regarding answer relevance,\nease of use, and overall satisfaction. This KG-RAG framework offers a promising\npathway towards personalized learning experiences and broader access to\nhigh-quality education."
                },
                "authors": [
                    {
                        "name": "Chenxi Dong"
                    },
                    {
                        "name": "Yimin Yuan"
                    },
                    {
                        "name": "Kan Chen"
                    },
                    {
                        "name": "Shupei Cheng"
                    },
                    {
                        "name": "Chujie Wen"
                    }
                ],
                "author_detail": {
                    "name": "Chujie Wen"
                },
                "author": "Chujie Wen",
                "arxiv_comment": "6 pages, 5 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2311.17696v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2311.17696v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.08716v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.08716v1",
                "updated": "2025-01-15T10:57:55Z",
                "updated_parsed": [
                    2025,
                    1,
                    15,
                    10,
                    57,
                    55,
                    2,
                    15,
                    0
                ],
                "published": "2025-01-15T10:57:55Z",
                "published_parsed": [
                    2025,
                    1,
                    15,
                    10,
                    57,
                    55,
                    2,
                    15,
                    0
                ],
                "title": "The Inherent Limits of Pretrained LLMs: The Unexpected Convergence of\n  Instruction Tuning and In-Context Learning Capabilities",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Inherent Limits of Pretrained LLMs: The Unexpected Convergence of\n  Instruction Tuning and In-Context Learning Capabilities"
                },
                "summary": "Large Language Models (LLMs), trained on extensive web-scale corpora, have\ndemonstrated remarkable abilities across diverse tasks, especially as they are\nscaled up. Nevertheless, even state-of-the-art models struggle in certain\ncases, sometimes failing at problems solvable by young children, indicating\nthat traditional notions of task complexity are insufficient for explaining LLM\ncapabilities. However, exploring LLM capabilities is complicated by the fact\nthat most widely-used models are also \"instruction-tuned\" to respond\nappropriately to prompts. With the goal of disentangling the factors\ninfluencing LLM performance, we investigate whether instruction-tuned models\npossess fundamentally different capabilities from base models that are prompted\nusing in-context examples. Through extensive experiments across various model\nfamilies, scales and task types, which included instruction tuning 90 different\nLLMs, we demonstrate that the performance of instruction-tuned models is\nsignificantly correlated with the in-context performance of their base\ncounterparts. By clarifying what instruction-tuning contributes, we extend\nprior research into in-context learning, which suggests that base models use\npriors from pretraining data to solve tasks. Specifically, we extend this\nunderstanding to instruction-tuned models, suggesting that their pretraining\ndata similarly sets a limiting boundary on the tasks they can solve, with the\nadded influence of the instruction-tuning dataset.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs), trained on extensive web-scale corpora, have\ndemonstrated remarkable abilities across diverse tasks, especially as they are\nscaled up. Nevertheless, even state-of-the-art models struggle in certain\ncases, sometimes failing at problems solvable by young children, indicating\nthat traditional notions of task complexity are insufficient for explaining LLM\ncapabilities. However, exploring LLM capabilities is complicated by the fact\nthat most widely-used models are also \"instruction-tuned\" to respond\nappropriately to prompts. With the goal of disentangling the factors\ninfluencing LLM performance, we investigate whether instruction-tuned models\npossess fundamentally different capabilities from base models that are prompted\nusing in-context examples. Through extensive experiments across various model\nfamilies, scales and task types, which included instruction tuning 90 different\nLLMs, we demonstrate that the performance of instruction-tuned models is\nsignificantly correlated with the in-context performance of their base\ncounterparts. By clarifying what instruction-tuning contributes, we extend\nprior research into in-context learning, which suggests that base models use\npriors from pretraining data to solve tasks. Specifically, we extend this\nunderstanding to instruction-tuned models, suggesting that their pretraining\ndata similarly sets a limiting boundary on the tasks they can solve, with the\nadded influence of the instruction-tuning dataset."
                },
                "authors": [
                    {
                        "name": "Irina Bigoulaeva"
                    },
                    {
                        "name": "Harish Tayyar Madabushi"
                    },
                    {
                        "name": "Iryna Gurevych"
                    }
                ],
                "author_detail": {
                    "name": "Iryna Gurevych"
                },
                "author": "Iryna Gurevych",
                "arxiv_comment": "The code for this paper is available at:\n  https://github.com/UKPLab/arxiv2025-inherent-limits-plms",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.08716v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.08716v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.08306v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.08306v2",
                "updated": "2025-01-15T10:36:48Z",
                "updated_parsed": [
                    2025,
                    1,
                    15,
                    10,
                    36,
                    48,
                    2,
                    15,
                    0
                ],
                "published": "2024-07-11T08:54:38Z",
                "published_parsed": [
                    2024,
                    7,
                    11,
                    8,
                    54,
                    38,
                    3,
                    193,
                    0
                ],
                "title": "Let Network Decide What to Learn: Symbolic Music Understanding Model\n  Based on Large-scale Adversarial Pre-training",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Let Network Decide What to Learn: Symbolic Music Understanding Model\n  Based on Large-scale Adversarial Pre-training"
                },
                "summary": "As a crucial aspect of Music Information Retrieval (MIR), Symbolic Music\nUnderstanding (SMU) has garnered significant attention for its potential to\nassist both musicians and enthusiasts in learning and creating music. Recently,\npre-trained language models have been widely adopted in SMU due to the\nsubstantial similarities between symbolic music and natural language, as well\nas the ability of these models to leverage limited music data effectively.\nHowever, some studies have shown the common pre-trained methods like Mask\nLanguage Model (MLM) may introduce bias issues like racism discrimination in\nNatural Language Process (NLP) and affects the performance of downstream tasks,\nwhich also happens in SMU. This bias often arises when masked tokens cannot be\ninferred from their context, forcing the model to overfit the training set\ninstead of generalizing. To address this challenge, we propose\nAdversarial-MidiBERT for SMU, which adaptively determines what to mask during\nMLM via a masker network, rather than employing random masking. By avoiding the\nmasking of tokens that are difficult to infer from context, our model is better\nequipped to capture contextual structures and relationships, rather than merely\nconforming to the training data distribution. We evaluate our method across\nfour SMU tasks, and our approach demonstrates excellent performance in all\ncases. The code for our model is publicly available at\nhttps://github.com/RS2002/Adversarial-MidiBERT.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As a crucial aspect of Music Information Retrieval (MIR), Symbolic Music\nUnderstanding (SMU) has garnered significant attention for its potential to\nassist both musicians and enthusiasts in learning and creating music. Recently,\npre-trained language models have been widely adopted in SMU due to the\nsubstantial similarities between symbolic music and natural language, as well\nas the ability of these models to leverage limited music data effectively.\nHowever, some studies have shown the common pre-trained methods like Mask\nLanguage Model (MLM) may introduce bias issues like racism discrimination in\nNatural Language Process (NLP) and affects the performance of downstream tasks,\nwhich also happens in SMU. This bias often arises when masked tokens cannot be\ninferred from their context, forcing the model to overfit the training set\ninstead of generalizing. To address this challenge, we propose\nAdversarial-MidiBERT for SMU, which adaptively determines what to mask during\nMLM via a masker network, rather than employing random masking. By avoiding the\nmasking of tokens that are difficult to infer from context, our model is better\nequipped to capture contextual structures and relationships, rather than merely\nconforming to the training data distribution. We evaluate our method across\nfour SMU tasks, and our approach demonstrates excellent performance in all\ncases. The code for our model is publicly available at\nhttps://github.com/RS2002/Adversarial-MidiBERT."
                },
                "authors": [
                    {
                        "name": "Zijian Zhao"
                    }
                ],
                "author_detail": {
                    "name": "Zijian Zhao"
                },
                "author": "Zijian Zhao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.08306v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.08306v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SD",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.12935v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.12935v3",
                "updated": "2025-01-15T10:21:30Z",
                "updated_parsed": [
                    2025,
                    1,
                    15,
                    10,
                    21,
                    30,
                    2,
                    15,
                    0
                ],
                "published": "2024-08-23T09:33:48Z",
                "published_parsed": [
                    2024,
                    8,
                    23,
                    9,
                    33,
                    48,
                    4,
                    236,
                    0
                ],
                "title": "Trustworthy, Responsible, and Safe AI: A Comprehensive Architectural\n  Framework for AI Safety with Challenges and Mitigations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Trustworthy, Responsible, and Safe AI: A Comprehensive Architectural\n  Framework for AI Safety with Challenges and Mitigations"
                },
                "summary": "AI Safety is an emerging area of critical importance to the safe adoption and\ndeployment of AI systems. With the rapid proliferation of AI and especially\nwith the recent advancement of Generative AI (or GAI), the technology ecosystem\nbehind the design, development, adoption, and deployment of AI systems has\ndrastically changed, broadening the scope of AI Safety to address impacts on\npublic safety and national security. In this paper, we propose a novel\narchitectural framework for understanding and analyzing AI Safety; defining its\ncharacteristics from three perspectives: Trustworthy AI, Responsible AI, and\nSafe AI. We provide an extensive review of current research and advancements in\nAI safety from these perspectives, highlighting their key challenges and\nmitigation approaches. Through examples from state-of-the-art technologies,\nparticularly Large Language Models (LLMs), we present innovative mechanism,\nmethodologies, and techniques for designing and testing AI safety. Our goal is\nto promote advancement in AI safety research, and ultimately enhance people's\ntrust in digital transformation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AI Safety is an emerging area of critical importance to the safe adoption and\ndeployment of AI systems. With the rapid proliferation of AI and especially\nwith the recent advancement of Generative AI (or GAI), the technology ecosystem\nbehind the design, development, adoption, and deployment of AI systems has\ndrastically changed, broadening the scope of AI Safety to address impacts on\npublic safety and national security. In this paper, we propose a novel\narchitectural framework for understanding and analyzing AI Safety; defining its\ncharacteristics from three perspectives: Trustworthy AI, Responsible AI, and\nSafe AI. We provide an extensive review of current research and advancements in\nAI safety from these perspectives, highlighting their key challenges and\nmitigation approaches. Through examples from state-of-the-art technologies,\nparticularly Large Language Models (LLMs), we present innovative mechanism,\nmethodologies, and techniques for designing and testing AI safety. Our goal is\nto promote advancement in AI safety research, and ultimately enhance people's\ntrust in digital transformation."
                },
                "authors": [
                    {
                        "name": "Chen Chen"
                    },
                    {
                        "name": "Xueluan Gong"
                    },
                    {
                        "name": "Ziyao Liu"
                    },
                    {
                        "name": "Weifeng Jiang"
                    },
                    {
                        "name": "Si Qi Goh"
                    },
                    {
                        "name": "Kwok-Yan Lam"
                    }
                ],
                "author_detail": {
                    "name": "Kwok-Yan Lam"
                },
                "author": "Kwok-Yan Lam",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.12935v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.12935v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.05301v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.05301v2",
                "updated": "2025-01-15T09:42:42Z",
                "updated_parsed": [
                    2025,
                    1,
                    15,
                    9,
                    42,
                    42,
                    2,
                    15,
                    0
                ],
                "published": "2024-10-04T12:22:54Z",
                "published_parsed": [
                    2024,
                    10,
                    4,
                    12,
                    22,
                    54,
                    4,
                    278,
                    0
                ],
                "title": "Diffusion-based Unsupervised Audio-visual Speech Enhancement",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion-based Unsupervised Audio-visual Speech Enhancement"
                },
                "summary": "This paper proposes a new unsupervised audio-visual speech enhancement (AVSE)\napproach that combines a diffusion-based audio-visual speech generative model\nwith a non-negative matrix factorization (NMF) noise model. First, the\ndiffusion model is pre-trained on clean speech conditioned on corresponding\nvideo data to simulate the speech generative distribution. This pre-trained\nmodel is then paired with the NMF-based noise model to estimate clean speech\niteratively. Specifically, a diffusion-based posterior sampling approach is\nimplemented within the reverse diffusion process, where after each iteration, a\nspeech estimate is obtained and used to update the noise parameters.\nExperimental results confirm that the proposed AVSE approach not only\noutperforms its audio-only counterpart but also generalizes better than a\nrecent supervised-generative AVSE method. Additionally, the new inference\nalgorithm offers a better balance between inference speed and performance\ncompared to the previous diffusion-based method. Code and demo available at:\nhttps://jeaneudesayilo.github.io/fast_UdiffSE",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper proposes a new unsupervised audio-visual speech enhancement (AVSE)\napproach that combines a diffusion-based audio-visual speech generative model\nwith a non-negative matrix factorization (NMF) noise model. First, the\ndiffusion model is pre-trained on clean speech conditioned on corresponding\nvideo data to simulate the speech generative distribution. This pre-trained\nmodel is then paired with the NMF-based noise model to estimate clean speech\niteratively. Specifically, a diffusion-based posterior sampling approach is\nimplemented within the reverse diffusion process, where after each iteration, a\nspeech estimate is obtained and used to update the noise parameters.\nExperimental results confirm that the proposed AVSE approach not only\noutperforms its audio-only counterpart but also generalizes better than a\nrecent supervised-generative AVSE method. Additionally, the new inference\nalgorithm offers a better balance between inference speed and performance\ncompared to the previous diffusion-based method. Code and demo available at:\nhttps://jeaneudesayilo.github.io/fast_UdiffSE"
                },
                "authors": [
                    {
                        "name": "Jean-Eudes Ayilo"
                    },
                    {
                        "name": "Mostafa Sadeghi"
                    },
                    {
                        "name": "Romain Serizel"
                    },
                    {
                        "name": "Xavier Alameda-Pineda"
                    }
                ],
                "author_detail": {
                    "name": "Xavier Alameda-Pineda"
                },
                "arxiv_affiliation": "ROBOTLEARN",
                "author": "Xavier Alameda-Pineda",
                "arxiv_journal_ref": "International Conference on Acoustics Speech and Signal Processing\n  (ICASSP), IEEE, Apr 2025, Hyderabad, India",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.05301v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.05301v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SD",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.08686v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.08686v1",
                "updated": "2025-01-15T09:32:37Z",
                "updated_parsed": [
                    2025,
                    1,
                    15,
                    9,
                    32,
                    37,
                    2,
                    15,
                    0
                ],
                "published": "2025-01-15T09:32:37Z",
                "published_parsed": [
                    2025,
                    1,
                    15,
                    9,
                    32,
                    37,
                    2,
                    15,
                    0
                ],
                "title": "Knowledge Graph-based Retrieval-Augmented Generation for Schema Matching",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Knowledge Graph-based Retrieval-Augmented Generation for Schema Matching"
                },
                "summary": "Traditional similarity-based schema matching methods are incapable of\nresolving semantic ambiguities and conflicts in domain-specific complex mapping\nscenarios due to missing commonsense and domain-specific knowledge. The\nhallucination problem of large language models (LLMs) also makes it challenging\nfor LLM-based schema matching to address the above issues. Therefore, we\npropose a Knowledge Graph-based Retrieval-Augmented Generation model for Schema\nMatching, referred to as the KG-RAG4SM. In particular, KG-RAG4SM introduces\nnovel vector-based, graph traversal-based, and query-based graph retrievals, as\nwell as a hybrid approach and ranking schemes that identify the most relevant\nsubgraphs from external large knowledge graphs (KGs). We showcase that KG-based\nretrieval-augmented LLMs are capable of generating more accurate results for\ncomplex matching cases without any re-training. Our experimental results show\nthat KG-RAG4SM outperforms the LLM-based state-of-the-art (SOTA) methods (e.g.,\nJellyfish-8B) by 35.89% and 30.50% in terms of precision and F1 score on the\nMIMIC dataset, respectively; KG-RAG4SM with GPT-4o-mini outperforms the\npre-trained language model (PLM)-based SOTA methods (e.g., SMAT) by 69.20% and\n21.97% in terms of precision and F1 score on the Synthea dataset, respectively.\nThe results also demonstrate that our approach is more efficient in end-to-end\nschema matching, and scales to retrieve from large KGs. Our case studies on the\ndataset from the real-world schema matching scenario exhibit that the\nhallucination problem of LLMs for schema matching is well mitigated by our\nsolution.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Traditional similarity-based schema matching methods are incapable of\nresolving semantic ambiguities and conflicts in domain-specific complex mapping\nscenarios due to missing commonsense and domain-specific knowledge. The\nhallucination problem of large language models (LLMs) also makes it challenging\nfor LLM-based schema matching to address the above issues. Therefore, we\npropose a Knowledge Graph-based Retrieval-Augmented Generation model for Schema\nMatching, referred to as the KG-RAG4SM. In particular, KG-RAG4SM introduces\nnovel vector-based, graph traversal-based, and query-based graph retrievals, as\nwell as a hybrid approach and ranking schemes that identify the most relevant\nsubgraphs from external large knowledge graphs (KGs). We showcase that KG-based\nretrieval-augmented LLMs are capable of generating more accurate results for\ncomplex matching cases without any re-training. Our experimental results show\nthat KG-RAG4SM outperforms the LLM-based state-of-the-art (SOTA) methods (e.g.,\nJellyfish-8B) by 35.89% and 30.50% in terms of precision and F1 score on the\nMIMIC dataset, respectively; KG-RAG4SM with GPT-4o-mini outperforms the\npre-trained language model (PLM)-based SOTA methods (e.g., SMAT) by 69.20% and\n21.97% in terms of precision and F1 score on the Synthea dataset, respectively.\nThe results also demonstrate that our approach is more efficient in end-to-end\nschema matching, and scales to retrieve from large KGs. Our case studies on the\ndataset from the real-world schema matching scenario exhibit that the\nhallucination problem of LLMs for schema matching is well mitigated by our\nsolution."
                },
                "authors": [
                    {
                        "name": "Chuangtao Ma"
                    },
                    {
                        "name": "Sriom Chakrabarti"
                    },
                    {
                        "name": "Arijit Khan"
                    },
                    {
                        "name": "Bálint Molnár"
                    }
                ],
                "author_detail": {
                    "name": "Bálint Molnár"
                },
                "author": "Bálint Molnár",
                "arxiv_comment": "Under Review",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.08686v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.08686v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.15512v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.15512v3",
                "updated": "2025-01-15T09:12:02Z",
                "updated_parsed": [
                    2025,
                    1,
                    15,
                    9,
                    12,
                    2,
                    2,
                    15,
                    0
                ],
                "published": "2024-08-28T03:48:05Z",
                "published_parsed": [
                    2024,
                    8,
                    28,
                    3,
                    48,
                    5,
                    2,
                    241,
                    0
                ],
                "title": "Toward Automated Simulation Research Workflow through LLM Prompt\n  Engineering Design",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Toward Automated Simulation Research Workflow through LLM Prompt\n  Engineering Design"
                },
                "summary": "The advent of Large Language Models (LLMs) has created new opportunities for\nthe automation of scientific research spanning both experimental processes and\ncomputational simulations. This study explores the feasibility of constructing\nan autonomous simulation agent (ASA) powered by LLMs through prompt engineering\nand automated program design to automate the entire simulation research process\naccording to a human-provided research plan. This process includes experimental\ndesign, remote upload and simulation execution, data analysis, and report\ncompilation. Using a well-studied simulation problem of polymer chain\nconformations as a test case, we assessed the long-task completion and\nreliability of ASAs powered by different LLMs, including GPT-4o, Claude-3.5,\netc. Our findings revealed that ASA-GPT-4o achieved near-flawless execution on\ndesignated research missions, underscoring the potential of methods like ASA to\nachieve automation in simulation research processes to enhance research\nefficiency. The outlined automation can be iteratively performed for up to 20\ncycles without human intervention, illustrating the potential of ASA for\nlong-task workflow automation. Additionally, we discussed the intrinsic traits\nof ASA in managing extensive tasks, focusing on self-validation mechanisms, and\nthe balance between local attention and global oversight.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The advent of Large Language Models (LLMs) has created new opportunities for\nthe automation of scientific research spanning both experimental processes and\ncomputational simulations. This study explores the feasibility of constructing\nan autonomous simulation agent (ASA) powered by LLMs through prompt engineering\nand automated program design to automate the entire simulation research process\naccording to a human-provided research plan. This process includes experimental\ndesign, remote upload and simulation execution, data analysis, and report\ncompilation. Using a well-studied simulation problem of polymer chain\nconformations as a test case, we assessed the long-task completion and\nreliability of ASAs powered by different LLMs, including GPT-4o, Claude-3.5,\netc. Our findings revealed that ASA-GPT-4o achieved near-flawless execution on\ndesignated research missions, underscoring the potential of methods like ASA to\nachieve automation in simulation research processes to enhance research\nefficiency. The outlined automation can be iteratively performed for up to 20\ncycles without human intervention, illustrating the potential of ASA for\nlong-task workflow automation. Additionally, we discussed the intrinsic traits\nof ASA in managing extensive tasks, focusing on self-validation mechanisms, and\nthe balance between local attention and global oversight."
                },
                "authors": [
                    {
                        "name": "Zhihan Liu"
                    },
                    {
                        "name": "Yubo Chai"
                    },
                    {
                        "name": "Jianfeng Li"
                    }
                ],
                "author_detail": {
                    "name": "Jianfeng Li"
                },
                "author": "Jianfeng Li",
                "arxiv_doi": "10.1021/acs.jcim.4c01653",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1021/acs.jcim.4c01653",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2408.15512v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.15512v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "The source code and example results of ASA can be found at\n  https://github.com/zokaraa/autonomous_simulation_agent",
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "physics.chem-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.08670v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.08670v1",
                "updated": "2025-01-15T09:04:30Z",
                "updated_parsed": [
                    2025,
                    1,
                    15,
                    9,
                    4,
                    30,
                    2,
                    15,
                    0
                ],
                "published": "2025-01-15T09:04:30Z",
                "published_parsed": [
                    2025,
                    1,
                    15,
                    9,
                    4,
                    30,
                    2,
                    15,
                    0
                ],
                "title": "Augmenting Smart Contract Decompiler Output through Fine-grained\n  Dependency Analysis and LLM-facilitated Semantic Recovery",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Augmenting Smart Contract Decompiler Output through Fine-grained\n  Dependency Analysis and LLM-facilitated Semantic Recovery"
                },
                "summary": "Decompiler is a specialized type of reverse engineering tool extensively\nemployed in program analysis tasks, particularly in program comprehension and\nvulnerability detection. However, current Solidity smart contract decompilers\nface significant limitations in reconstructing the original source code. In\nparticular, the bottleneck of SOTA decompilers lies in inaccurate method\nidentification, incorrect variable type recovery, and missing contract\nattributes. These deficiencies hinder downstream tasks and understanding of the\nprogram logic. To address these challenges, we propose SmartHalo, a new\nframework that enhances decompiler output by combining static analysis (SA) and\nlarge language models (LLM). SmartHalo leverages the complementary strengths of\nSA's accuracy in control and data flow analysis and LLM's capability in\nsemantic prediction. More specifically, \\system{} constructs a new data\nstructure - Dependency Graph (DG), to extract semantic dependencies via static\nanalysis. Then, it takes DG to create prompts for LLM optimization. Finally,\nthe correctness of LLM outputs is validated through symbolic execution and\nformal verification. Evaluation on a dataset consisting of 465 randomly\nselected smart contract methods shows that SmartHalo significantly improves the\nquality of the decompiled code, compared to SOTA decompilers (e.g., Gigahorse).\nNotably, integrating GPT-4o with SmartHalo further enhances its performance,\nachieving precision rates of 87.39% for method boundaries, 90.39% for variable\ntypes, and 80.65% for contract attributes.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Decompiler is a specialized type of reverse engineering tool extensively\nemployed in program analysis tasks, particularly in program comprehension and\nvulnerability detection. However, current Solidity smart contract decompilers\nface significant limitations in reconstructing the original source code. In\nparticular, the bottleneck of SOTA decompilers lies in inaccurate method\nidentification, incorrect variable type recovery, and missing contract\nattributes. These deficiencies hinder downstream tasks and understanding of the\nprogram logic. To address these challenges, we propose SmartHalo, a new\nframework that enhances decompiler output by combining static analysis (SA) and\nlarge language models (LLM). SmartHalo leverages the complementary strengths of\nSA's accuracy in control and data flow analysis and LLM's capability in\nsemantic prediction. More specifically, \\system{} constructs a new data\nstructure - Dependency Graph (DG), to extract semantic dependencies via static\nanalysis. Then, it takes DG to create prompts for LLM optimization. Finally,\nthe correctness of LLM outputs is validated through symbolic execution and\nformal verification. Evaluation on a dataset consisting of 465 randomly\nselected smart contract methods shows that SmartHalo significantly improves the\nquality of the decompiled code, compared to SOTA decompilers (e.g., Gigahorse).\nNotably, integrating GPT-4o with SmartHalo further enhances its performance,\nachieving precision rates of 87.39% for method boundaries, 90.39% for variable\ntypes, and 80.65% for contract attributes."
                },
                "authors": [
                    {
                        "name": "Zeqin Liao"
                    },
                    {
                        "name": "Yuhong Nan"
                    },
                    {
                        "name": "Zixu Gao"
                    },
                    {
                        "name": "Henglong Liang"
                    },
                    {
                        "name": "Sicheng Hao"
                    },
                    {
                        "name": "Peifan Reng"
                    },
                    {
                        "name": "Zibin Zheng"
                    }
                ],
                "author_detail": {
                    "name": "Zibin Zheng"
                },
                "author": "Zibin Zheng",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.08670v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.08670v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.08662v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.08662v1",
                "updated": "2025-01-15T08:57:41Z",
                "updated_parsed": [
                    2025,
                    1,
                    15,
                    8,
                    57,
                    41,
                    2,
                    15,
                    0
                ],
                "published": "2025-01-15T08:57:41Z",
                "published_parsed": [
                    2025,
                    1,
                    15,
                    8,
                    57,
                    41,
                    2,
                    15,
                    0
                ],
                "title": "Product of Gaussian Mixture Diffusion Model for non-linear MRI Inversion",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Product of Gaussian Mixture Diffusion Model for non-linear MRI Inversion"
                },
                "summary": "Diffusion models have recently shown remarkable results in magnetic resonance\nimaging reconstruction. However, the employed networks typically are black-box\nestimators of the (smoothed) prior score with tens of millions of parameters,\nrestricting interpretability and increasing reconstruction time. Furthermore,\nparallel imaging reconstruction algorithms either rely on off-line coil\nsensitivity estimation, which is prone to misalignment and restricting sampling\ntrajectories, or perform per-coil reconstruction, making the computational cost\nproportional to the number of coils. To overcome this, we jointly reconstruct\nthe image and the coil sensitivities using the lightweight,\nparameter-efficient, and interpretable product of Gaussian mixture diffusion\nmodel as an image prior and a classical smoothness priors on the coil\nsensitivities. The proposed method delivers promising results while allowing\nfor fast inference and demonstrating robustness to contrast out-of-distribution\ndata and sampling trajectories, comparable to classical variational penalties\nsuch as total variation. Finally, the probabilistic formulation allows the\ncalculation of the posterior expectation and pixel-wise variance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion models have recently shown remarkable results in magnetic resonance\nimaging reconstruction. However, the employed networks typically are black-box\nestimators of the (smoothed) prior score with tens of millions of parameters,\nrestricting interpretability and increasing reconstruction time. Furthermore,\nparallel imaging reconstruction algorithms either rely on off-line coil\nsensitivity estimation, which is prone to misalignment and restricting sampling\ntrajectories, or perform per-coil reconstruction, making the computational cost\nproportional to the number of coils. To overcome this, we jointly reconstruct\nthe image and the coil sensitivities using the lightweight,\nparameter-efficient, and interpretable product of Gaussian mixture diffusion\nmodel as an image prior and a classical smoothness priors on the coil\nsensitivities. The proposed method delivers promising results while allowing\nfor fast inference and demonstrating robustness to contrast out-of-distribution\ndata and sampling trajectories, comparable to classical variational penalties\nsuch as total variation. Finally, the probabilistic formulation allows the\ncalculation of the posterior expectation and pixel-wise variance."
                },
                "authors": [
                    {
                        "name": "Laurenz Nagler"
                    },
                    {
                        "name": "Martin Zach"
                    },
                    {
                        "name": "Thomas Pock"
                    }
                ],
                "author_detail": {
                    "name": "Thomas Pock"
                },
                "author": "Thomas Pock",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.08662v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.08662v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.IV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.IV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.08284v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.08284v2",
                "updated": "2025-01-15T08:55:50Z",
                "updated_parsed": [
                    2025,
                    1,
                    15,
                    8,
                    55,
                    50,
                    2,
                    15,
                    0
                ],
                "published": "2025-01-14T18:00:07Z",
                "published_parsed": [
                    2025,
                    1,
                    14,
                    18,
                    0,
                    7,
                    1,
                    14,
                    0
                ],
                "title": "AfriHate: A Multilingual Collection of Hate Speech and Abusive Language\n  Datasets for African Languages",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AfriHate: A Multilingual Collection of Hate Speech and Abusive Language\n  Datasets for African Languages"
                },
                "summary": "Hate speech and abusive language are global phenomena that need\nsocio-cultural background knowledge to be understood, identified, and\nmoderated. However, in many regions of the Global South, there have been\nseveral documented occurrences of (1) absence of moderation and (2) censorship\ndue to the reliance on keyword spotting out of context. Further, high-profile\nindividuals have frequently been at the center of the moderation process, while\nlarge and targeted hate speech campaigns against minorities have been\noverlooked. These limitations are mainly due to the lack of high-quality data\nin the local languages and the failure to include local communities in the\ncollection, annotation, and moderation processes. To address this issue, we\npresent AfriHate: a multilingual collection of hate speech and abusive language\ndatasets in 15 African languages. Each instance in AfriHate is annotated by\nnative speakers familiar with the local culture. We report the challenges\nrelated to the construction of the datasets and present various classification\nbaseline results with and without using LLMs. The datasets, individual\nannotations, and hate speech and offensive language lexicons are available on\nhttps://github.com/AfriHate/AfriHate",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Hate speech and abusive language are global phenomena that need\nsocio-cultural background knowledge to be understood, identified, and\nmoderated. However, in many regions of the Global South, there have been\nseveral documented occurrences of (1) absence of moderation and (2) censorship\ndue to the reliance on keyword spotting out of context. Further, high-profile\nindividuals have frequently been at the center of the moderation process, while\nlarge and targeted hate speech campaigns against minorities have been\noverlooked. These limitations are mainly due to the lack of high-quality data\nin the local languages and the failure to include local communities in the\ncollection, annotation, and moderation processes. To address this issue, we\npresent AfriHate: a multilingual collection of hate speech and abusive language\ndatasets in 15 African languages. Each instance in AfriHate is annotated by\nnative speakers familiar with the local culture. We report the challenges\nrelated to the construction of the datasets and present various classification\nbaseline results with and without using LLMs. The datasets, individual\nannotations, and hate speech and offensive language lexicons are available on\nhttps://github.com/AfriHate/AfriHate"
                },
                "authors": [
                    {
                        "name": "Shamsuddeen Hassan Muhammad"
                    },
                    {
                        "name": "Idris Abdulmumin"
                    },
                    {
                        "name": "Abinew Ali Ayele"
                    },
                    {
                        "name": "David Ifeoluwa Adelani"
                    },
                    {
                        "name": "Ibrahim Said Ahmad"
                    },
                    {
                        "name": "Saminu Mohammad Aliyu"
                    },
                    {
                        "name": "Nelson Odhiambo Onyango"
                    },
                    {
                        "name": "Lilian D. A. Wanzare"
                    },
                    {
                        "name": "Samuel Rutunda"
                    },
                    {
                        "name": "Lukman Jibril Aliyu"
                    },
                    {
                        "name": "Esubalew Alemneh"
                    },
                    {
                        "name": "Oumaima Hourrane"
                    },
                    {
                        "name": "Hagos Tesfahun Gebremichael"
                    },
                    {
                        "name": "Elyas Abdi Ismail"
                    },
                    {
                        "name": "Meriem Beloucif"
                    },
                    {
                        "name": "Ebrahim Chekol Jibril"
                    },
                    {
                        "name": "Andiswa Bukula"
                    },
                    {
                        "name": "Rooweither Mabuya"
                    },
                    {
                        "name": "Salomey Osei"
                    },
                    {
                        "name": "Abigail Oppong"
                    },
                    {
                        "name": "Tadesse Destaw Belay"
                    },
                    {
                        "name": "Tadesse Kebede Guge"
                    },
                    {
                        "name": "Tesfa Tegegne Asfaw"
                    },
                    {
                        "name": "Chiamaka Ijeoma Chukwuneke"
                    },
                    {
                        "name": "Paul Röttger"
                    },
                    {
                        "name": "Seid Muhie Yimam"
                    },
                    {
                        "name": "Nedjma Ousidhoum"
                    }
                ],
                "author_detail": {
                    "name": "Nedjma Ousidhoum"
                },
                "author": "Nedjma Ousidhoum",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.08284v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.08284v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.08648v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.08648v1",
                "updated": "2025-01-15T08:24:03Z",
                "updated_parsed": [
                    2025,
                    1,
                    15,
                    8,
                    24,
                    3,
                    2,
                    15,
                    0
                ],
                "published": "2025-01-15T08:24:03Z",
                "published_parsed": [
                    2025,
                    1,
                    15,
                    8,
                    24,
                    3,
                    2,
                    15,
                    0
                ],
                "title": "MAGNET: Augmenting Generative Decoders with Representation Learning and\n  Infilling Capabilities",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MAGNET: Augmenting Generative Decoders with Representation Learning and\n  Infilling Capabilities"
                },
                "summary": "While originally designed for unidirectional generative modeling,\ndecoder-only large language models (LLMs) are increasingly being adapted for\nbidirectional modeling. However, unidirectional and bidirectional models are\ntypically trained separately with distinct objectives (generation and\nrepresentation learning, respectively). This separation overlooks the\nopportunity for developing a more versatile language model and for these\nobjectives to complement each other. In this work, we introduce MAGNET, an\nadaptation of decoder-only LLMs that enhances their ability to generate robust\nrepresentations and infill missing text spans, while preserving their knowledge\nand text generation capabilities. MAGNET employs three self-supervised training\nobjectives and introduces an attention mechanism that combines bidirectional\nand causal attention, enabling unified training across all objectives. Our\nresults demonstrate that LLMs adapted with MAGNET (1) surpass strong text\nencoders on token-level and sentence-level representation learning tasks, (2)\ngenerate contextually appropriate text infills by leveraging future context,\n(3) retain the ability for open-ended text generation without exhibiting\nrepetition problem, and (4) preserve the knowledge gained by the LLM during\npretraining.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While originally designed for unidirectional generative modeling,\ndecoder-only large language models (LLMs) are increasingly being adapted for\nbidirectional modeling. However, unidirectional and bidirectional models are\ntypically trained separately with distinct objectives (generation and\nrepresentation learning, respectively). This separation overlooks the\nopportunity for developing a more versatile language model and for these\nobjectives to complement each other. In this work, we introduce MAGNET, an\nadaptation of decoder-only LLMs that enhances their ability to generate robust\nrepresentations and infill missing text spans, while preserving their knowledge\nand text generation capabilities. MAGNET employs three self-supervised training\nobjectives and introduces an attention mechanism that combines bidirectional\nand causal attention, enabling unified training across all objectives. Our\nresults demonstrate that LLMs adapted with MAGNET (1) surpass strong text\nencoders on token-level and sentence-level representation learning tasks, (2)\ngenerate contextually appropriate text infills by leveraging future context,\n(3) retain the ability for open-ended text generation without exhibiting\nrepetition problem, and (4) preserve the knowledge gained by the LLM during\npretraining."
                },
                "authors": [
                    {
                        "name": "Savya Khosla"
                    },
                    {
                        "name": "Kushal Kafle"
                    },
                    {
                        "name": "Simon Jenni"
                    },
                    {
                        "name": "Handong Zhao"
                    },
                    {
                        "name": "John Collomosse"
                    },
                    {
                        "name": "Jing Shi"
                    }
                ],
                "author_detail": {
                    "name": "Jing Shi"
                },
                "author": "Jing Shi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.08648v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.08648v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2402.16705v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2402.16705v2",
                "updated": "2025-01-15T08:20:19Z",
                "updated_parsed": [
                    2025,
                    1,
                    15,
                    8,
                    20,
                    19,
                    2,
                    15,
                    0
                ],
                "published": "2024-02-26T16:21:53Z",
                "published_parsed": [
                    2024,
                    2,
                    26,
                    16,
                    21,
                    53,
                    0,
                    57,
                    0
                ],
                "title": "SelectIT: Selective Instruction Tuning for LLMs via Uncertainty-Aware\n  Self-Reflection",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SelectIT: Selective Instruction Tuning for LLMs via Uncertainty-Aware\n  Self-Reflection"
                },
                "summary": "Instruction tuning (IT) is crucial to tailoring large language models (LLMs)\ntowards human-centric interactions. Recent advancements have shown that the\ncareful selection of a small, high-quality subset of IT data can significantly\nenhance the performance of LLMs. Despite this, common approaches often rely on\nadditional models or data, which increases costs and limits widespread\nadoption. In this work, we propose a novel approach, termed SelectIT, that\ncapitalizes on the foundational capabilities of the LLM itself. Specifically,\nwe exploit the intrinsic uncertainty present in LLMs to more effectively select\nhigh-quality IT data, without the need for extra resources. Furthermore, we\nintroduce a curated IT dataset, the Selective Alpaca, created by applying\nSelectIT to the Alpaca-GPT4 dataset. Empirical results demonstrate that IT\nusing Selective Alpaca leads to substantial model ability enhancement. The\nrobustness of SelectIT has also been corroborated in various foundation models\nand domain-specific tasks. Our findings suggest that longer and more\ncomputationally intensive IT data may serve as superior sources of IT, offering\nvaluable insights for future research in this area. Data, code, and scripts are\nfreely available at https://github.com/Blue-Raincoat/SelectIT.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Instruction tuning (IT) is crucial to tailoring large language models (LLMs)\ntowards human-centric interactions. Recent advancements have shown that the\ncareful selection of a small, high-quality subset of IT data can significantly\nenhance the performance of LLMs. Despite this, common approaches often rely on\nadditional models or data, which increases costs and limits widespread\nadoption. In this work, we propose a novel approach, termed SelectIT, that\ncapitalizes on the foundational capabilities of the LLM itself. Specifically,\nwe exploit the intrinsic uncertainty present in LLMs to more effectively select\nhigh-quality IT data, without the need for extra resources. Furthermore, we\nintroduce a curated IT dataset, the Selective Alpaca, created by applying\nSelectIT to the Alpaca-GPT4 dataset. Empirical results demonstrate that IT\nusing Selective Alpaca leads to substantial model ability enhancement. The\nrobustness of SelectIT has also been corroborated in various foundation models\nand domain-specific tasks. Our findings suggest that longer and more\ncomputationally intensive IT data may serve as superior sources of IT, offering\nvaluable insights for future research in this area. Data, code, and scripts are\nfreely available at https://github.com/Blue-Raincoat/SelectIT."
                },
                "authors": [
                    {
                        "name": "Liangxin Liu"
                    },
                    {
                        "name": "Xuebo Liu"
                    },
                    {
                        "name": "Derek F. Wong"
                    },
                    {
                        "name": "Dongfang Li"
                    },
                    {
                        "name": "Ziyi Wang"
                    },
                    {
                        "name": "Baotian Hu"
                    },
                    {
                        "name": "Min Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Min Zhang"
                },
                "author": "Min Zhang",
                "arxiv_comment": "Accepted to NeurIPS 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2402.16705v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2402.16705v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.08645v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.08645v1",
                "updated": "2025-01-15T08:19:13Z",
                "updated_parsed": [
                    2025,
                    1,
                    15,
                    8,
                    19,
                    13,
                    2,
                    15,
                    0
                ],
                "published": "2025-01-15T08:19:13Z",
                "published_parsed": [
                    2025,
                    1,
                    15,
                    8,
                    19,
                    13,
                    2,
                    15,
                    0
                ],
                "title": "Learnable Sparsification of Die-to-Die Communication via Spike-Based\n  Encoding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Learnable Sparsification of Die-to-Die Communication via Spike-Based\n  Encoding"
                },
                "summary": "Efficient communication is central to both biological and artificial\nintelligence (AI) systems. In biological brains, the challenge of long-range\ncommunication across regions is addressed through sparse, spike-based\nsignaling, minimizing energy consumption and latency. In contrast, modern AI\nworkloads, which keep scaling ever larger across distributed compute systems,\nare increasingly constrained by bandwidth limitations, creating bottlenecks\nthat hinder scalability and energy efficiency. Inspired by the brain's\nefficient communication strategies, we propose SNAP, a hybrid neural network\narchitecture combining spiking neural networks (SNNs) and artificial neural\nnetworks (ANNs) to address these challenges. SNAP integrates SNNs at\nbandwidth-constrained regions, such as chip boundaries, where spike-based\nencoding reduces data transfer overhead. Within each chip, dense ANN\ncomputations are maintained to preserve high throughput, accuracy, and\nrobustness.\n  Historically, SNNs have faced difficulties scaling up, with limitations in\ntask-specific performance and reliance on specialized hardware to exploit\nsparsity. SNAP overcomes these barriers through an algorithm-architecture\nco-design leveraging learnable sparsity for die-to-die communication while\nlimiting spiking layers to specific network partitions. This composable design\nintegrates spike-based and non-spiking pathways, making it adaptable to diverse\ndeep learning workloads. Our evaluations on language processing and computer\nvision tasks demonstrate up to 5.3x energy efficiency improvements and 15.2x\nreductions in inference latency, outperforming both traditional SNNs and\nnon-spiking models. We find that as model resources scale, SNAP's improvement\nmargins grow. By addressing the critical bottleneck of inter-chip\ncommunication, SNAP offers a scalable, biologically inspired pathway to more\nefficient AI systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient communication is central to both biological and artificial\nintelligence (AI) systems. In biological brains, the challenge of long-range\ncommunication across regions is addressed through sparse, spike-based\nsignaling, minimizing energy consumption and latency. In contrast, modern AI\nworkloads, which keep scaling ever larger across distributed compute systems,\nare increasingly constrained by bandwidth limitations, creating bottlenecks\nthat hinder scalability and energy efficiency. Inspired by the brain's\nefficient communication strategies, we propose SNAP, a hybrid neural network\narchitecture combining spiking neural networks (SNNs) and artificial neural\nnetworks (ANNs) to address these challenges. SNAP integrates SNNs at\nbandwidth-constrained regions, such as chip boundaries, where spike-based\nencoding reduces data transfer overhead. Within each chip, dense ANN\ncomputations are maintained to preserve high throughput, accuracy, and\nrobustness.\n  Historically, SNNs have faced difficulties scaling up, with limitations in\ntask-specific performance and reliance on specialized hardware to exploit\nsparsity. SNAP overcomes these barriers through an algorithm-architecture\nco-design leveraging learnable sparsity for die-to-die communication while\nlimiting spiking layers to specific network partitions. This composable design\nintegrates spike-based and non-spiking pathways, making it adaptable to diverse\ndeep learning workloads. Our evaluations on language processing and computer\nvision tasks demonstrate up to 5.3x energy efficiency improvements and 15.2x\nreductions in inference latency, outperforming both traditional SNNs and\nnon-spiking models. We find that as model resources scale, SNAP's improvement\nmargins grow. By addressing the critical bottleneck of inter-chip\ncommunication, SNAP offers a scalable, biologically inspired pathway to more\nefficient AI systems."
                },
                "authors": [
                    {
                        "name": "Joshua Nardone"
                    },
                    {
                        "name": "Ruijie Zhu"
                    },
                    {
                        "name": "Joseph Callenes"
                    },
                    {
                        "name": "Mohammed E. Elbtity"
                    },
                    {
                        "name": "Ramtin Zand"
                    },
                    {
                        "name": "Jason Eshraghian"
                    }
                ],
                "author_detail": {
                    "name": "Jason Eshraghian"
                },
                "author": "Jason Eshraghian",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.08645v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.08645v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.08642v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.08642v1",
                "updated": "2025-01-15T08:11:23Z",
                "updated_parsed": [
                    2025,
                    1,
                    15,
                    8,
                    11,
                    23,
                    2,
                    15,
                    0
                ],
                "published": "2025-01-15T08:11:23Z",
                "published_parsed": [
                    2025,
                    1,
                    15,
                    8,
                    11,
                    23,
                    2,
                    15,
                    0
                ],
                "title": "Effects of pressure gradient histories on skin friction and mean flow of\n  high Reynolds number turbulent boundary layers over smooth and rough walls",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Effects of pressure gradient histories on skin friction and mean flow of\n  high Reynolds number turbulent boundary layers over smooth and rough walls"
                },
                "summary": "Experiments are conducted over smooth and rough walls to explore the\ninfluence of pressure gradient histories on skin friction and mean flow of\nturbulent boundary layers. Different pressure gradient histories are imposed on\nthe boundary layer through an aerofoil mounted in the freestream. Hot-wire\nmeasurements are taken at different freestream velocities downstream of the\naerofoil where the flow has locally recovered to zero pressure gradient but\nretains the history effects. Direct skin friction measurements are also made\nusing oil film interferometry for smooth walls and a floating element drag\nbalance for rough walls. The friction Reynolds number, $Re_\\tau$, varies\nbetween $3000$ and $27000$, depending both on the surface conditions and the\nfreestream velocity ensuring sufficient scale separation. Results align with\nprevious findings, showing that adverse pressure gradients just upstream of the\nmeasurement location increase wake strength and reduce the local skin friction\nwhile favourable pressure gradients suppress the wake and increase skin\nfriction. The roughness length scale, $y_0$, remains constant across different\npressure gradient histories for rough wall boundary layers. Inspired by\nprevious works, a new correlation is proposed to infer skin friction based on\nthe mean flow. The difference in skin friction between an arbitrary pressure\ngradient history and zero pressure gradient condition can be predicted using\nonly the local wake strength parameter ($\\Pi$), and the variations in wake\nstrength for different histories are related to a weighted integral of the\npressure gradient history normalised by local quantities. This allows us to\ndevelop a general correlation that can be used to infer skin friction for\nturbulent boundary layers experiencing arbitrary pressure-gradient histories.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Experiments are conducted over smooth and rough walls to explore the\ninfluence of pressure gradient histories on skin friction and mean flow of\nturbulent boundary layers. Different pressure gradient histories are imposed on\nthe boundary layer through an aerofoil mounted in the freestream. Hot-wire\nmeasurements are taken at different freestream velocities downstream of the\naerofoil where the flow has locally recovered to zero pressure gradient but\nretains the history effects. Direct skin friction measurements are also made\nusing oil film interferometry for smooth walls and a floating element drag\nbalance for rough walls. The friction Reynolds number, $Re_\\tau$, varies\nbetween $3000$ and $27000$, depending both on the surface conditions and the\nfreestream velocity ensuring sufficient scale separation. Results align with\nprevious findings, showing that adverse pressure gradients just upstream of the\nmeasurement location increase wake strength and reduce the local skin friction\nwhile favourable pressure gradients suppress the wake and increase skin\nfriction. The roughness length scale, $y_0$, remains constant across different\npressure gradient histories for rough wall boundary layers. Inspired by\nprevious works, a new correlation is proposed to infer skin friction based on\nthe mean flow. The difference in skin friction between an arbitrary pressure\ngradient history and zero pressure gradient condition can be predicted using\nonly the local wake strength parameter ($\\Pi$), and the variations in wake\nstrength for different histories are related to a weighted integral of the\npressure gradient history normalised by local quantities. This allows us to\ndevelop a general correlation that can be used to infer skin friction for\nturbulent boundary layers experiencing arbitrary pressure-gradient histories."
                },
                "authors": [
                    {
                        "name": "Thomas Preskett"
                    },
                    {
                        "name": "Marco Virgilio"
                    },
                    {
                        "name": "Prateek Jaiswal"
                    },
                    {
                        "name": "Bharathram Ganapathisubramani"
                    }
                ],
                "author_detail": {
                    "name": "Bharathram Ganapathisubramani"
                },
                "author": "Bharathram Ganapathisubramani",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.08642v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.08642v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.flu-dyn",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.flu-dyn",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.08639v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.08639v1",
                "updated": "2025-01-15T08:04:44Z",
                "updated_parsed": [
                    2025,
                    1,
                    15,
                    8,
                    4,
                    44,
                    2,
                    15,
                    0
                ],
                "published": "2025-01-15T08:04:44Z",
                "published_parsed": [
                    2025,
                    1,
                    15,
                    8,
                    4,
                    44,
                    2,
                    15,
                    0
                ],
                "title": "Detecting Wildfire Flame and Smoke through Edge Computing using Transfer\n  Learning Enhanced Deep Learning Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Detecting Wildfire Flame and Smoke through Edge Computing using Transfer\n  Learning Enhanced Deep Learning Models"
                },
                "summary": "Autonomous unmanned aerial vehicles (UAVs) integrated with edge computing\ncapabilities empower real-time data processing directly on the device,\ndramatically reducing latency in critical scenarios such as wildfire detection.\nThis study underscores Transfer Learning's (TL) significance in boosting the\nperformance of object detectors for identifying wildfire smoke and flames,\nespecially when trained on limited datasets, and investigates the impact TL has\non edge computing metrics. With the latter focusing how TL-enhanced You Only\nLook Once (YOLO) models perform in terms of inference time, power usage, and\nenergy consumption when using edge computing devices. This study utilizes the\nAerial Fire and Smoke Essential (AFSE) dataset as the target, with the Flame\nand Smoke Detection Dataset (FASDD) and the Microsoft Common Objects in Context\n(COCO) dataset serving as source datasets. We explore a two-stage cascaded TL\nmethod, utilizing D-Fire or FASDD as initial stage target datasets and AFSE as\nthe subsequent stage. Through fine-tuning, TL significantly enhances detection\nprecision, achieving up to 79.2% mean Average Precision (mAP@0.5), reduces\ntraining time, and increases model generalizability across the AFSE dataset.\nHowever, cascaded TL yielded no notable improvements and TL alone did not\nbenefit the edge computing metrics evaluated. Lastly, this work found that\nYOLOv5n remains a powerful model when lacking hardware acceleration, finding\nthat YOLOv5n can process images nearly twice as fast as its newer counterpart,\nYOLO11n. Overall, the results affirm TL's role in augmenting the accuracy of\nobject detectors while also illustrating that additional enhancements are\nneeded to improve edge computing performance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Autonomous unmanned aerial vehicles (UAVs) integrated with edge computing\ncapabilities empower real-time data processing directly on the device,\ndramatically reducing latency in critical scenarios such as wildfire detection.\nThis study underscores Transfer Learning's (TL) significance in boosting the\nperformance of object detectors for identifying wildfire smoke and flames,\nespecially when trained on limited datasets, and investigates the impact TL has\non edge computing metrics. With the latter focusing how TL-enhanced You Only\nLook Once (YOLO) models perform in terms of inference time, power usage, and\nenergy consumption when using edge computing devices. This study utilizes the\nAerial Fire and Smoke Essential (AFSE) dataset as the target, with the Flame\nand Smoke Detection Dataset (FASDD) and the Microsoft Common Objects in Context\n(COCO) dataset serving as source datasets. We explore a two-stage cascaded TL\nmethod, utilizing D-Fire or FASDD as initial stage target datasets and AFSE as\nthe subsequent stage. Through fine-tuning, TL significantly enhances detection\nprecision, achieving up to 79.2% mean Average Precision (mAP@0.5), reduces\ntraining time, and increases model generalizability across the AFSE dataset.\nHowever, cascaded TL yielded no notable improvements and TL alone did not\nbenefit the edge computing metrics evaluated. Lastly, this work found that\nYOLOv5n remains a powerful model when lacking hardware acceleration, finding\nthat YOLOv5n can process images nearly twice as fast as its newer counterpart,\nYOLO11n. Overall, the results affirm TL's role in augmenting the accuracy of\nobject detectors while also illustrating that additional enhancements are\nneeded to improve edge computing performance."
                },
                "authors": [
                    {
                        "name": "Giovanny Vazquez"
                    },
                    {
                        "name": "Shengjie Zhai"
                    },
                    {
                        "name": "Mei Yang"
                    }
                ],
                "author_detail": {
                    "name": "Mei Yang"
                },
                "author": "Mei Yang",
                "arxiv_comment": "11 pages, 7 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.08639v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.08639v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.IV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.12117v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.12117v3",
                "updated": "2025-01-15T08:03:55Z",
                "updated_parsed": [
                    2025,
                    1,
                    15,
                    8,
                    3,
                    55,
                    2,
                    15,
                    0
                ],
                "published": "2024-07-16T18:59:49Z",
                "published_parsed": [
                    2024,
                    7,
                    16,
                    18,
                    59,
                    49,
                    1,
                    198,
                    0
                ],
                "title": "MEMO: Fine-grained Tensor Management For Ultra-long Context LLM Training",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MEMO: Fine-grained Tensor Management For Ultra-long Context LLM Training"
                },
                "summary": "Nowadays, Large Language Models (LLMs) have been trained using extended\ncontext lengths to foster more creative applications. However, long context\ntraining poses great challenges considering the constraint of GPU memory. It\nnot only leads to substantial activation memory consumption during training,\nbut also incurs considerable memory fragmentation. To facilitate long context\ntraining, existing frameworks have adopted strategies such as recomputation and\nvarious forms of parallelisms. Nevertheless, these techniques rely on redundant\ncomputation or extensive communication, resulting in low Model FLOPS\nUtilization (MFU). In this paper, we propose MEMO, a novel LLM training\nframework designed for fine-grained activation memory management. Given the\nquadratic scaling of computation and linear scaling of memory with sequence\nlengths when using FlashAttention, we offload memory-consuming activations to\nCPU memory after each layer's forward pass and fetch them during the backward\npass. To maximize the swapping of activations without hindering computation,\nand to avoid exhausting limited CPU memory, we implement a token-wise\nactivation recomputation and swapping mechanism. Furthermore, we tackle the\nmemory fragmentation issue by employing a bi-level Mixed Integer Programming\n(MIP) approach, optimizing memory reuse across transformer layers. Empirical\nresults demonstrate that MEMO achieves an average of 1.97x and 1.80x MFU\ncompared to Megatron-LM and DeepSpeed, respectively. This improvement is\nattributed to MEMO's ability to minimize memory fragmentation, reduce\nrecomputation and intensive communication, and circumvent the delays associated\nwith the memory reorganization process due to fragmentation. By leveraging\nfine-grained activation memory management, MEMO facilitates efficient training\nof 7B LLM with 1 million sequence length on just 8 A800 GPUs, achieving an MFU\nof 52.30%.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Nowadays, Large Language Models (LLMs) have been trained using extended\ncontext lengths to foster more creative applications. However, long context\ntraining poses great challenges considering the constraint of GPU memory. It\nnot only leads to substantial activation memory consumption during training,\nbut also incurs considerable memory fragmentation. To facilitate long context\ntraining, existing frameworks have adopted strategies such as recomputation and\nvarious forms of parallelisms. Nevertheless, these techniques rely on redundant\ncomputation or extensive communication, resulting in low Model FLOPS\nUtilization (MFU). In this paper, we propose MEMO, a novel LLM training\nframework designed for fine-grained activation memory management. Given the\nquadratic scaling of computation and linear scaling of memory with sequence\nlengths when using FlashAttention, we offload memory-consuming activations to\nCPU memory after each layer's forward pass and fetch them during the backward\npass. To maximize the swapping of activations without hindering computation,\nand to avoid exhausting limited CPU memory, we implement a token-wise\nactivation recomputation and swapping mechanism. Furthermore, we tackle the\nmemory fragmentation issue by employing a bi-level Mixed Integer Programming\n(MIP) approach, optimizing memory reuse across transformer layers. Empirical\nresults demonstrate that MEMO achieves an average of 1.97x and 1.80x MFU\ncompared to Megatron-LM and DeepSpeed, respectively. This improvement is\nattributed to MEMO's ability to minimize memory fragmentation, reduce\nrecomputation and intensive communication, and circumvent the delays associated\nwith the memory reorganization process due to fragmentation. By leveraging\nfine-grained activation memory management, MEMO facilitates efficient training\nof 7B LLM with 1 million sequence length on just 8 A800 GPUs, achieving an MFU\nof 52.30%."
                },
                "authors": [
                    {
                        "name": "Pinxue Zhao"
                    },
                    {
                        "name": "Hailin Zhang"
                    },
                    {
                        "name": "Fangcheng Fu"
                    },
                    {
                        "name": "Xiaonan Nie"
                    },
                    {
                        "name": "Qibin Liu"
                    },
                    {
                        "name": "Fang Yang"
                    },
                    {
                        "name": "Yuanbo Peng"
                    },
                    {
                        "name": "Dian Jiao"
                    },
                    {
                        "name": "Shuaipeng Li"
                    },
                    {
                        "name": "Jinbao Xue"
                    },
                    {
                        "name": "Yangyu Tao"
                    },
                    {
                        "name": "Bin Cui"
                    }
                ],
                "author_detail": {
                    "name": "Bin Cui"
                },
                "author": "Bin Cui",
                "arxiv_doi": "10.1145/3709703",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3709703",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2407.12117v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.12117v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.06605v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.06605v2",
                "updated": "2025-01-15T08:01:51Z",
                "updated_parsed": [
                    2025,
                    1,
                    15,
                    8,
                    1,
                    51,
                    2,
                    15,
                    0
                ],
                "published": "2025-01-11T18:11:07Z",
                "published_parsed": [
                    2025,
                    1,
                    11,
                    18,
                    11,
                    7,
                    5,
                    11,
                    0
                ],
                "title": "RoboHorizon: An LLM-Assisted Multi-View World Model for Long-Horizon\n  Robotic Manipulation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RoboHorizon: An LLM-Assisted Multi-View World Model for Long-Horizon\n  Robotic Manipulation"
                },
                "summary": "Efficient control in long-horizon robotic manipulation is challenging due to\ncomplex representation and policy learning requirements. Model-based visual\nreinforcement learning (RL) has shown great potential in addressing these\nchallenges but still faces notable limitations, particularly in handling sparse\nrewards and complex visual features in long-horizon environments. To address\nthese limitations, we propose the Recognize-Sense-Plan-Act (RSPA) pipeline for\nlong-horizon tasks and further introduce RoboHorizon, an LLM-assisted\nmulti-view world model tailored for long-horizon robotic manipulation. In\nRoboHorizon, pre-trained LLMs generate dense reward structures for multi-stage\nsub-tasks based on task language instructions, enabling robots to better\nrecognize long-horizon tasks. Keyframe discovery is then integrated into the\nmulti-view masked autoencoder (MAE) architecture to enhance the robot's ability\nto sense critical task sequences, strengthening its multi-stage perception of\nlong-horizon processes. Leveraging these dense rewards and multi-view\nrepresentations, a robotic world model is constructed to efficiently plan\nlong-horizon tasks, enabling the robot to reliably act through RL algorithms.\nExperiments on two representative benchmarks, RLBench and FurnitureBench, show\nthat RoboHorizon outperforms state-of-the-art visual model-based RL methods,\nachieving a 23.35% improvement in task success rates on RLBench's 4\nshort-horizon tasks and a 29.23% improvement on 6 long-horizon tasks from\nRLBench and 3 furniture assembly tasks from FurnitureBench.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient control in long-horizon robotic manipulation is challenging due to\ncomplex representation and policy learning requirements. Model-based visual\nreinforcement learning (RL) has shown great potential in addressing these\nchallenges but still faces notable limitations, particularly in handling sparse\nrewards and complex visual features in long-horizon environments. To address\nthese limitations, we propose the Recognize-Sense-Plan-Act (RSPA) pipeline for\nlong-horizon tasks and further introduce RoboHorizon, an LLM-assisted\nmulti-view world model tailored for long-horizon robotic manipulation. In\nRoboHorizon, pre-trained LLMs generate dense reward structures for multi-stage\nsub-tasks based on task language instructions, enabling robots to better\nrecognize long-horizon tasks. Keyframe discovery is then integrated into the\nmulti-view masked autoencoder (MAE) architecture to enhance the robot's ability\nto sense critical task sequences, strengthening its multi-stage perception of\nlong-horizon processes. Leveraging these dense rewards and multi-view\nrepresentations, a robotic world model is constructed to efficiently plan\nlong-horizon tasks, enabling the robot to reliably act through RL algorithms.\nExperiments on two representative benchmarks, RLBench and FurnitureBench, show\nthat RoboHorizon outperforms state-of-the-art visual model-based RL methods,\nachieving a 23.35% improvement in task success rates on RLBench's 4\nshort-horizon tasks and a 29.23% improvement on 6 long-horizon tasks from\nRLBench and 3 furniture assembly tasks from FurnitureBench."
                },
                "authors": [
                    {
                        "name": "Zixuan Chen"
                    },
                    {
                        "name": "Jing Huo"
                    },
                    {
                        "name": "Yangtao Chen"
                    },
                    {
                        "name": "Yang Gao"
                    }
                ],
                "author_detail": {
                    "name": "Yang Gao"
                },
                "author": "Yang Gao",
                "arxiv_comment": "Under review",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.06605v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.06605v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.11344v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.11344v3",
                "updated": "2025-01-15T07:46:15Z",
                "updated_parsed": [
                    2025,
                    1,
                    15,
                    7,
                    46,
                    15,
                    2,
                    15,
                    0
                ],
                "published": "2024-11-18T07:33:10Z",
                "published_parsed": [
                    2024,
                    11,
                    18,
                    7,
                    33,
                    10,
                    0,
                    323,
                    0
                ],
                "title": "Mitigating Knowledge Conflicts in Language Model-Driven Question\n  Answering",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mitigating Knowledge Conflicts in Language Model-Driven Question\n  Answering"
                },
                "summary": "In the context of knowledge-driven seq-to-seq generation tasks, such as\ndocument-based question answering and document summarization systems, two\nfundamental knowledge sources play crucial roles: the inherent knowledge\nembedded within model parameters and the external knowledge obtained through\ncontext. Recent studies revealed a significant challenge: when there exists a\nmisalignment between the model's inherent knowledge and the ground truth\nanswers in training data, the system may exhibit problematic behaviors during\ninference, such as ignoring input context, or generating unfaithful content.\nOur investigation proposes a strategy to minimize hallucination by building\nexplicit connection between source inputs and generated outputs. We\nspecifically target a common hallucination pattern in question answering,\nexamining how the correspondence between entities and their contexts during\nmodel training influences the system's performance at inference time.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In the context of knowledge-driven seq-to-seq generation tasks, such as\ndocument-based question answering and document summarization systems, two\nfundamental knowledge sources play crucial roles: the inherent knowledge\nembedded within model parameters and the external knowledge obtained through\ncontext. Recent studies revealed a significant challenge: when there exists a\nmisalignment between the model's inherent knowledge and the ground truth\nanswers in training data, the system may exhibit problematic behaviors during\ninference, such as ignoring input context, or generating unfaithful content.\nOur investigation proposes a strategy to minimize hallucination by building\nexplicit connection between source inputs and generated outputs. We\nspecifically target a common hallucination pattern in question answering,\nexamining how the correspondence between entities and their contexts during\nmodel training influences the system's performance at inference time."
                },
                "authors": [
                    {
                        "name": "Han Cao"
                    },
                    {
                        "name": "Zhaoyang Zhang"
                    },
                    {
                        "name": "Xiangtian Li"
                    },
                    {
                        "name": "Chufan Wu"
                    },
                    {
                        "name": "Hansong Zhang"
                    },
                    {
                        "name": "Wenqing Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Wenqing Zhang"
                },
                "author": "Wenqing Zhang",
                "arxiv_comment": "revised version, more figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.11344v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.11344v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.08631v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.08631v1",
                "updated": "2025-01-15T07:36:19Z",
                "updated_parsed": [
                    2025,
                    1,
                    15,
                    7,
                    36,
                    19,
                    2,
                    15,
                    0
                ],
                "published": "2025-01-15T07:36:19Z",
                "published_parsed": [
                    2025,
                    1,
                    15,
                    7,
                    36,
                    19,
                    2,
                    15,
                    0
                ],
                "title": "SWSC: Shared Weight for Similar Channel in LLM",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SWSC: Shared Weight for Similar Channel in LLM"
                },
                "summary": "Large language models (LLMs) have spurred development in multiple industries.\nHowever, the growing number of their parameters brings substantial storage and\ncomputing burdens, making it essential to explore model compression techniques\nfor parameter reduction and easier deployment. We propose SWSC, an LLM\ncompression method based on the concept of Shared Weight for Similar Channel.\nIt uses the K-Means clustering algorithm to cluster model weights\nchannel-by-channel, generating clusters with highly similar vectors within\neach. A representative vector from each cluster is selected to approximately\nreplace all vectors in the cluster, significantly reducing the number of model\nweight parameters. However, approximate restoration will inevitably cause\ndamage to the performance of the model. To tackle this issue, we perform\nsingular value decomposition on the weight error values before and after\ncompression and retain the larger singular values and their corresponding\nsingular vectors to compensate for the accuracy. The experimental results show\nthat our method can effectively ensure the performance of the compressed LLM\neven under low-precision conditions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have spurred development in multiple industries.\nHowever, the growing number of their parameters brings substantial storage and\ncomputing burdens, making it essential to explore model compression techniques\nfor parameter reduction and easier deployment. We propose SWSC, an LLM\ncompression method based on the concept of Shared Weight for Similar Channel.\nIt uses the K-Means clustering algorithm to cluster model weights\nchannel-by-channel, generating clusters with highly similar vectors within\neach. A representative vector from each cluster is selected to approximately\nreplace all vectors in the cluster, significantly reducing the number of model\nweight parameters. However, approximate restoration will inevitably cause\ndamage to the performance of the model. To tackle this issue, we perform\nsingular value decomposition on the weight error values before and after\ncompression and retain the larger singular values and their corresponding\nsingular vectors to compensate for the accuracy. The experimental results show\nthat our method can effectively ensure the performance of the compressed LLM\neven under low-precision conditions."
                },
                "authors": [
                    {
                        "name": "Binrui Zeng"
                    },
                    {
                        "name": "Yongtao Tang"
                    },
                    {
                        "name": "Xiaodong Liu"
                    },
                    {
                        "name": "Xiaopeng Li"
                    }
                ],
                "author_detail": {
                    "name": "Xiaopeng Li"
                },
                "author": "Xiaopeng Li",
                "arxiv_comment": "5pages, 3 figures, work in progress",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.08631v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.08631v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.00453v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.00453v4",
                "updated": "2025-01-15T07:18:43Z",
                "updated_parsed": [
                    2025,
                    1,
                    15,
                    7,
                    18,
                    43,
                    2,
                    15,
                    0
                ],
                "published": "2024-11-01T09:05:47Z",
                "published_parsed": [
                    2024,
                    11,
                    1,
                    9,
                    5,
                    47,
                    4,
                    306,
                    0
                ],
                "title": "Diffusion Models as Network Optimizers: Explorations and Analysis",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion Models as Network Optimizers: Explorations and Analysis"
                },
                "summary": "Network optimization is a fundamental challenge in the Internet of Things\n(IoT) network, often characterized by complex features that make it difficult\nto solve these problems. Recently, generative diffusion models (GDMs) have\nemerged as a promising new approach to network optimization, with the potential\nto directly address these optimization problems. However, the application of\nGDMs in this field is still in its early stages, and there is a noticeable lack\nof theoretical research and empirical findings. In this study, we first explore\nthe intrinsic characteristics of generative models. Next, we provide a concise\ntheoretical proof and intuitive demonstration of the advantages of generative\nmodels over discriminative models in network optimization. Based on this\nexploration, we implement GDMs as optimizers aimed at learning high-quality\nsolution distributions for given inputs, sampling from these distributions\nduring inference to approximate or achieve optimal solutions. Specifically, we\nutilize denoising diffusion probabilistic models (DDPMs) and employ a\nclassifier-free guidance mechanism to manage conditional guidance based on\ninput parameters. We conduct extensive experiments across three challenging\nnetwork optimization problems. By investigating various model configurations\nand the principles of GDMs as optimizers, we demonstrate the ability to\novercome prediction errors and validate the convergence of generated solutions\nto optimal solutions. We provide code and data at\nhttps://github.com/qiyu3816/DiffSG.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Network optimization is a fundamental challenge in the Internet of Things\n(IoT) network, often characterized by complex features that make it difficult\nto solve these problems. Recently, generative diffusion models (GDMs) have\nemerged as a promising new approach to network optimization, with the potential\nto directly address these optimization problems. However, the application of\nGDMs in this field is still in its early stages, and there is a noticeable lack\nof theoretical research and empirical findings. In this study, we first explore\nthe intrinsic characteristics of generative models. Next, we provide a concise\ntheoretical proof and intuitive demonstration of the advantages of generative\nmodels over discriminative models in network optimization. Based on this\nexploration, we implement GDMs as optimizers aimed at learning high-quality\nsolution distributions for given inputs, sampling from these distributions\nduring inference to approximate or achieve optimal solutions. Specifically, we\nutilize denoising diffusion probabilistic models (DDPMs) and employ a\nclassifier-free guidance mechanism to manage conditional guidance based on\ninput parameters. We conduct extensive experiments across three challenging\nnetwork optimization problems. By investigating various model configurations\nand the principles of GDMs as optimizers, we demonstrate the ability to\novercome prediction errors and validate the convergence of generated solutions\nto optimal solutions. We provide code and data at\nhttps://github.com/qiyu3816/DiffSG."
                },
                "authors": [
                    {
                        "name": "Ruihuai Liang"
                    },
                    {
                        "name": "Bo Yang"
                    },
                    {
                        "name": "Pengyu Chen"
                    },
                    {
                        "name": "Xianjin Li"
                    },
                    {
                        "name": "Yifan Xue"
                    },
                    {
                        "name": "Zhiwen Yu"
                    },
                    {
                        "name": "Xuelin Cao"
                    },
                    {
                        "name": "Yan Zhang"
                    },
                    {
                        "name": "Mérouane Debbah"
                    },
                    {
                        "name": "H. Vincent Poor"
                    },
                    {
                        "name": "Chau Yuen"
                    }
                ],
                "author_detail": {
                    "name": "Chau Yuen"
                },
                "author": "Chau Yuen",
                "arxiv_doi": "10.1109/JIOT.2025.3528955",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1109/JIOT.2025.3528955",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2411.00453v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.00453v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_journal_ref": "IEEE Internet of Things Journal (2025)",
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.08626v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.08626v1",
                "updated": "2025-01-15T07:07:48Z",
                "updated_parsed": [
                    2025,
                    1,
                    15,
                    7,
                    7,
                    48,
                    2,
                    15,
                    0
                ],
                "published": "2025-01-15T07:07:48Z",
                "published_parsed": [
                    2025,
                    1,
                    15,
                    7,
                    7,
                    48,
                    2,
                    15,
                    0
                ],
                "title": "A Learning Algorithm That Attains the Human Optimum in a Repeated\n  Human-Machine Interaction Game",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Learning Algorithm That Attains the Human Optimum in a Repeated\n  Human-Machine Interaction Game"
                },
                "summary": "When humans interact with learning-based control systems, a common goal is to\nminimize a cost function known only to the human. For instance, an exoskeleton\nmay adapt its assistance in an effort to minimize the human's metabolic\ncost-of-transport. Conventional approaches to synthesizing the learning\nalgorithm solve an inverse problem to infer the human's cost. However, these\nproblems can be ill-posed, hard to solve, or sensitive to problem data. Here we\nshow a game-theoretic learning algorithm that works solely by observing human\nactions to find the cost minimum, avoiding the need to solve an inverse\nproblem. We evaluate the performance of our algorithm in an extensive set of\nhuman subjects experiments, demonstrating consistent convergence to the minimum\nof a prescribed human cost function in scalar and multidimensional\ninstantiations of the game. We conclude by outlining future directions for\ntheoretical and empirical extensions of our results.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "When humans interact with learning-based control systems, a common goal is to\nminimize a cost function known only to the human. For instance, an exoskeleton\nmay adapt its assistance in an effort to minimize the human's metabolic\ncost-of-transport. Conventional approaches to synthesizing the learning\nalgorithm solve an inverse problem to infer the human's cost. However, these\nproblems can be ill-posed, hard to solve, or sensitive to problem data. Here we\nshow a game-theoretic learning algorithm that works solely by observing human\nactions to find the cost minimum, avoiding the need to solve an inverse\nproblem. We evaluate the performance of our algorithm in an extensive set of\nhuman subjects experiments, demonstrating consistent convergence to the minimum\nof a prescribed human cost function in scalar and multidimensional\ninstantiations of the game. We conclude by outlining future directions for\ntheoretical and empirical extensions of our results."
                },
                "authors": [
                    {
                        "name": "Jason T. Isa"
                    },
                    {
                        "name": "Lillian J. Ratliff"
                    },
                    {
                        "name": "Samuel A. Burden"
                    }
                ],
                "author_detail": {
                    "name": "Samuel A. Burden"
                },
                "author": "Samuel A. Burden",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.08626v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.08626v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.GT",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.GT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.08618v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.08618v1",
                "updated": "2025-01-15T06:34:34Z",
                "updated_parsed": [
                    2025,
                    1,
                    15,
                    6,
                    34,
                    34,
                    2,
                    15,
                    0
                ],
                "published": "2025-01-15T06:34:34Z",
                "published_parsed": [
                    2025,
                    1,
                    15,
                    6,
                    34,
                    34,
                    2,
                    15,
                    0
                ],
                "title": "Disjoint Processing Mechanisms of Hierarchical and Linear Grammars in\n  Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Disjoint Processing Mechanisms of Hierarchical and Linear Grammars in\n  Large Language Models"
                },
                "summary": "All natural languages are structured hierarchically. In humans, this\nstructural restriction is neurologically coded: when two grammars are presented\nwith identical vocabularies, brain areas responsible for language processing\nare only sensitive to hierarchical grammars. Using large language models\n(LLMs), we investigate whether such functionally distinct hierarchical\nprocessing regions can arise solely from exposure to large-scale language\ndistributions. We generate inputs using English, Italian, Japanese, or nonce\nwords, varying the underlying grammars to conform to either hierarchical or\nlinear/positional rules. Using these grammars, we first observe that language\nmodels show distinct behaviors on hierarchical versus linearly structured\ninputs. Then, we find that the components responsible for processing\nhierarchical grammars are distinct from those that process linear grammars; we\ncausally verify this in ablation experiments. Finally, we observe that\nhierarchy-selective components are also active on nonce grammars; this suggests\nthat hierarchy sensitivity is not tied to meaning, nor in-distribution inputs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "All natural languages are structured hierarchically. In humans, this\nstructural restriction is neurologically coded: when two grammars are presented\nwith identical vocabularies, brain areas responsible for language processing\nare only sensitive to hierarchical grammars. Using large language models\n(LLMs), we investigate whether such functionally distinct hierarchical\nprocessing regions can arise solely from exposure to large-scale language\ndistributions. We generate inputs using English, Italian, Japanese, or nonce\nwords, varying the underlying grammars to conform to either hierarchical or\nlinear/positional rules. Using these grammars, we first observe that language\nmodels show distinct behaviors on hierarchical versus linearly structured\ninputs. Then, we find that the components responsible for processing\nhierarchical grammars are distinct from those that process linear grammars; we\ncausally verify this in ablation experiments. Finally, we observe that\nhierarchy-selective components are also active on nonce grammars; this suggests\nthat hierarchy sensitivity is not tied to meaning, nor in-distribution inputs."
                },
                "authors": [
                    {
                        "name": "Aruna Sankaranarayanan"
                    },
                    {
                        "name": "Dylan Hadfield-Menell"
                    },
                    {
                        "name": "Aaron Mueller"
                    }
                ],
                "author_detail": {
                    "name": "Aaron Mueller"
                },
                "author": "Aaron Mueller",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.08618v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.08618v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.08613v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.08613v1",
                "updated": "2025-01-15T06:22:35Z",
                "updated_parsed": [
                    2025,
                    1,
                    15,
                    6,
                    22,
                    35,
                    2,
                    15,
                    0
                ],
                "published": "2025-01-15T06:22:35Z",
                "published_parsed": [
                    2025,
                    1,
                    15,
                    6,
                    22,
                    35,
                    2,
                    15,
                    0
                ],
                "title": "Assessing the Alignment of FOL Closeness Metrics with Human Judgement",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Assessing the Alignment of FOL Closeness Metrics with Human Judgement"
                },
                "summary": "The recent successful paradigm of solving logical reasoning problems with\ntool-augmented large language models (LLMs) leverages translation of natural\nlanguage statements into First-Order Logic~(FOL) and external theorem provers.\nHowever, the correctness of FOL statements, comprising operators and text\npredicates, often goes unverified due to the lack of a reliable evaluation\nmetric for comparing generated and ground-truth FOLs. In this paper, we present\na comprehensive study of sensitivity of existing metrics and their alignment\nwith human judgement on FOL evaluation. Using ground-truth FOLs, we carefully\ndesigned various perturbations on the ground-truth to assess metric\nsensitivity. We sample FOL translation candidates for natural language\nstatements and measure the ranking alignment between automatic metrics and\nhuman annotators. Our empirical findings highlight oversensitivity in the\nn-gram metric BLEU for text perturbations, the semantic graph metric Smatch++\nfor structural perturbations, and FOL metric for operator perturbation. We also\nobserve a closer alignment between BertScore and human judgement. Additionally,\nwe show that combining metrics enhances both alignment and sensitivity compared\nto using individual metrics.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The recent successful paradigm of solving logical reasoning problems with\ntool-augmented large language models (LLMs) leverages translation of natural\nlanguage statements into First-Order Logic~(FOL) and external theorem provers.\nHowever, the correctness of FOL statements, comprising operators and text\npredicates, often goes unverified due to the lack of a reliable evaluation\nmetric for comparing generated and ground-truth FOLs. In this paper, we present\na comprehensive study of sensitivity of existing metrics and their alignment\nwith human judgement on FOL evaluation. Using ground-truth FOLs, we carefully\ndesigned various perturbations on the ground-truth to assess metric\nsensitivity. We sample FOL translation candidates for natural language\nstatements and measure the ranking alignment between automatic metrics and\nhuman annotators. Our empirical findings highlight oversensitivity in the\nn-gram metric BLEU for text perturbations, the semantic graph metric Smatch++\nfor structural perturbations, and FOL metric for operator perturbation. We also\nobserve a closer alignment between BertScore and human judgement. Additionally,\nwe show that combining metrics enhances both alignment and sensitivity compared\nto using individual metrics."
                },
                "authors": [
                    {
                        "name": "Ramya Keerthy Thatikonda"
                    },
                    {
                        "name": "Wray Buntine"
                    },
                    {
                        "name": "Ehsan Shareghi"
                    }
                ],
                "author_detail": {
                    "name": "Ehsan Shareghi"
                },
                "author": "Ehsan Shareghi",
                "arxiv_comment": "Code: https://github.com/RamyaKeerthy/AlignmentFOL",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.08613v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.08613v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.20061v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.20061v2",
                "updated": "2025-01-15T06:15:13Z",
                "updated_parsed": [
                    2025,
                    1,
                    15,
                    6,
                    15,
                    13,
                    2,
                    15,
                    0
                ],
                "published": "2024-12-28T07:30:05Z",
                "published_parsed": [
                    2024,
                    12,
                    28,
                    7,
                    30,
                    5,
                    5,
                    363,
                    0
                ],
                "title": "Comparative Analysis of Listwise Reranking with Large Language Models in\n  Limited-Resource Language Contexts",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Comparative Analysis of Listwise Reranking with Large Language Models in\n  Limited-Resource Language Contexts"
                },
                "summary": "Large Language Models (LLMs) have demonstrated significant effectiveness\nacross various NLP tasks, including text ranking. This study assesses the\nperformance of large language models (LLMs) in listwise reranking for\nlimited-resource African languages. We compare proprietary models RankGPT3.5,\nRank4o-mini, RankGPTo1-mini and RankClaude-sonnet in cross-lingual contexts.\nResults indicate that these LLMs significantly outperform traditional baseline\nmethods such as BM25-DT in most evaluation metrics, particularly in nDCG@10 and\nMRR@100. These findings highlight the potential of LLMs in enhancing reranking\ntasks for low-resource languages and offer insights into cost-effective\nsolutions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have demonstrated significant effectiveness\nacross various NLP tasks, including text ranking. This study assesses the\nperformance of large language models (LLMs) in listwise reranking for\nlimited-resource African languages. We compare proprietary models RankGPT3.5,\nRank4o-mini, RankGPTo1-mini and RankClaude-sonnet in cross-lingual contexts.\nResults indicate that these LLMs significantly outperform traditional baseline\nmethods such as BM25-DT in most evaluation metrics, particularly in nDCG@10 and\nMRR@100. These findings highlight the potential of LLMs in enhancing reranking\ntasks for low-resource languages and offer insights into cost-effective\nsolutions."
                },
                "authors": [
                    {
                        "name": "Yanxin Shen"
                    },
                    {
                        "name": "Lun Wang"
                    },
                    {
                        "name": "Chuanqi Shi"
                    },
                    {
                        "name": "Shaoshuai Du"
                    },
                    {
                        "name": "Yiyi Tao"
                    },
                    {
                        "name": "Yixian Shen"
                    },
                    {
                        "name": "Hang Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Hang Zhang"
                },
                "author": "Hang Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.20061v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.20061v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.01566v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.01566v3",
                "updated": "2025-01-15T06:10:16Z",
                "updated_parsed": [
                    2025,
                    1,
                    15,
                    6,
                    10,
                    16,
                    2,
                    15,
                    0
                ],
                "published": "2024-04-02T02:00:42Z",
                "published_parsed": [
                    2024,
                    4,
                    2,
                    2,
                    0,
                    42,
                    1,
                    93,
                    0
                ],
                "title": "Heterogeneous Treatment Effects and Causal Mechanisms",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Heterogeneous Treatment Effects and Causal Mechanisms"
                },
                "summary": "The credibility revolution advances the use of research designs that permit\nidentification and estimation of causal effects. However, understanding which\nmechanisms produce measured causal effects remains a challenge. A dominant\ncurrent approach to the quantitative evaluation of mechanisms relies on the\ndetection of heterogeneous treatment effects with respect to pre-treatment\ncovariates. This paper develops a framework to understand when the existence of\nsuch heterogeneous treatment effects can support inferences about the\nactivation of a mechanism. We show first that this design cannot provide\nevidence of mechanism activation without an additional, generally implicit,\nassumption. Further, even when this assumption is satisfied, if a measured\noutcome is produced by a non-linear transformation of a directly-affected\noutcome of theoretical interest, heterogeneous treatment effects are not\ninformative of mechanism activation. We provide novel guidance for\ninterpretation and research design in light of these findings.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The credibility revolution advances the use of research designs that permit\nidentification and estimation of causal effects. However, understanding which\nmechanisms produce measured causal effects remains a challenge. A dominant\ncurrent approach to the quantitative evaluation of mechanisms relies on the\ndetection of heterogeneous treatment effects with respect to pre-treatment\ncovariates. This paper develops a framework to understand when the existence of\nsuch heterogeneous treatment effects can support inferences about the\nactivation of a mechanism. We show first that this design cannot provide\nevidence of mechanism activation without an additional, generally implicit,\nassumption. Further, even when this assumption is satisfied, if a measured\noutcome is produced by a non-linear transformation of a directly-affected\noutcome of theoretical interest, heterogeneous treatment effects are not\ninformative of mechanism activation. We provide novel guidance for\ninterpretation and research design in light of these findings."
                },
                "authors": [
                    {
                        "name": "Jiawei Fu"
                    },
                    {
                        "name": "Tara Slough"
                    }
                ],
                "author_detail": {
                    "name": "Tara Slough"
                },
                "author": "Tara Slough",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.01566v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.01566v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "econ.EM",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "econ.EM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.08603v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.08603v1",
                "updated": "2025-01-15T06:00:50Z",
                "updated_parsed": [
                    2025,
                    1,
                    15,
                    6,
                    0,
                    50,
                    2,
                    15,
                    0
                ],
                "published": "2025-01-15T06:00:50Z",
                "published_parsed": [
                    2025,
                    1,
                    15,
                    6,
                    0,
                    50,
                    2,
                    15,
                    0
                ],
                "title": "Monte Carlo Tree Search for Comprehensive Exploration in LLM-Based\n  Automatic Heuristic Design",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Monte Carlo Tree Search for Comprehensive Exploration in LLM-Based\n  Automatic Heuristic Design"
                },
                "summary": "Handcrafting heuristics for solving complex planning tasks (e.g., NP-hard\ncombinatorial optimization (CO) problems) is a common practice but requires\nextensive domain knowledge. Recently, Large Language Model (LLM)-based\nautomatic heuristics design (AHD) methods have shown promise in generating\nhigh-quality heuristics without manual intervention. Existing LLM-based AHD\nmethods employ a population to maintain a fixed number of top-performing\nLLM-generated heuristics and introduce evolutionary computation (EC) to enhance\nthe population iteratively. However, the population-based procedure brings\ngreedy properties, often resulting in convergence to local optima. Instead, to\nmore comprehensively explore the space of heuristics, we propose using Monte\nCarlo Tree Search (MCTS) for LLM-based heuristic evolution while preserving all\nLLM-generated heuristics in a tree structure. With a novel thought-alignment\nprocess and an exploration-decay technique, the proposed MCTS-AHD method\ndelivers significantly higher-quality heuristics on various complex tasks. Our\ncode is available at https://github.com/zz1358m/MCTS-AHD-master.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Handcrafting heuristics for solving complex planning tasks (e.g., NP-hard\ncombinatorial optimization (CO) problems) is a common practice but requires\nextensive domain knowledge. Recently, Large Language Model (LLM)-based\nautomatic heuristics design (AHD) methods have shown promise in generating\nhigh-quality heuristics without manual intervention. Existing LLM-based AHD\nmethods employ a population to maintain a fixed number of top-performing\nLLM-generated heuristics and introduce evolutionary computation (EC) to enhance\nthe population iteratively. However, the population-based procedure brings\ngreedy properties, often resulting in convergence to local optima. Instead, to\nmore comprehensively explore the space of heuristics, we propose using Monte\nCarlo Tree Search (MCTS) for LLM-based heuristic evolution while preserving all\nLLM-generated heuristics in a tree structure. With a novel thought-alignment\nprocess and an exploration-decay technique, the proposed MCTS-AHD method\ndelivers significantly higher-quality heuristics on various complex tasks. Our\ncode is available at https://github.com/zz1358m/MCTS-AHD-master."
                },
                "authors": [
                    {
                        "name": "Zhi Zheng"
                    },
                    {
                        "name": "Zhuoliang Xie"
                    },
                    {
                        "name": "Zhenkun Wang"
                    },
                    {
                        "name": "Bryan Hooi"
                    }
                ],
                "author_detail": {
                    "name": "Bryan Hooi"
                },
                "author": "Bryan Hooi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.08603v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.08603v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.08600v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.08600v1",
                "updated": "2025-01-15T05:54:33Z",
                "updated_parsed": [
                    2025,
                    1,
                    15,
                    5,
                    54,
                    33,
                    2,
                    15,
                    0
                ],
                "published": "2025-01-15T05:54:33Z",
                "published_parsed": [
                    2025,
                    1,
                    15,
                    5,
                    54,
                    33,
                    2,
                    15,
                    0
                ],
                "title": "AutoRestTest: A Tool for Automated REST API Testing Using LLMs and MARL",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AutoRestTest: A Tool for Automated REST API Testing Using LLMs and MARL"
                },
                "summary": "As REST APIs have become widespread in modern web services, comprehensive\ntesting of these APIs has become increasingly crucial. Due to the vast search\nspace consisting of operations, parameters, and parameter values along with\ntheir complex dependencies and constraints, current testing tools suffer from\nlow code coverage, leading to suboptimal fault detection. To address this\nlimitation, we present a novel tool, AutoRestTest, which integrates the\nSemantic Operation Dependency Graph (SODG) with Multi-Agent Reinforcement\nLearning (MARL) and large language models (LLMs) for effective REST API\ntesting. AutoRestTest determines operation-dependent parameters using the SODG\nand employs five specialized agents (operation, parameter, value, dependency,\nand header) to identify dependencies of operations and generate operation\nsequences, parameter combinations, and values. AutoRestTest provides a\ncommand-line interface and continuous telemetry on successful operation count,\nunique server errors detected, and time elapsed. Upon completion, AutoRestTest\ngenerates a detailed report highlighting errors detected and operations\nexercised. In this paper, we introduce our tool and present preliminary\nresults.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As REST APIs have become widespread in modern web services, comprehensive\ntesting of these APIs has become increasingly crucial. Due to the vast search\nspace consisting of operations, parameters, and parameter values along with\ntheir complex dependencies and constraints, current testing tools suffer from\nlow code coverage, leading to suboptimal fault detection. To address this\nlimitation, we present a novel tool, AutoRestTest, which integrates the\nSemantic Operation Dependency Graph (SODG) with Multi-Agent Reinforcement\nLearning (MARL) and large language models (LLMs) for effective REST API\ntesting. AutoRestTest determines operation-dependent parameters using the SODG\nand employs five specialized agents (operation, parameter, value, dependency,\nand header) to identify dependencies of operations and generate operation\nsequences, parameter combinations, and values. AutoRestTest provides a\ncommand-line interface and continuous telemetry on successful operation count,\nunique server errors detected, and time elapsed. Upon completion, AutoRestTest\ngenerates a detailed report highlighting errors detected and operations\nexercised. In this paper, we introduce our tool and present preliminary\nresults."
                },
                "authors": [
                    {
                        "name": "Tyler Stennett"
                    },
                    {
                        "name": "Myeongsoo Kim"
                    },
                    {
                        "name": "Saurabh Sinha"
                    },
                    {
                        "name": "Alessandro Orso"
                    }
                ],
                "author_detail": {
                    "name": "Alessandro Orso"
                },
                "author": "Alessandro Orso",
                "arxiv_comment": "To be published in the 47th IEEE/ACM International Conference on\n  Software Engineering - Demonstration Track (ICSE-Demo 2025)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.08600v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.08600v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.08598v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.08598v1",
                "updated": "2025-01-15T05:51:20Z",
                "updated_parsed": [
                    2025,
                    1,
                    15,
                    5,
                    51,
                    20,
                    2,
                    15,
                    0
                ],
                "published": "2025-01-15T05:51:20Z",
                "published_parsed": [
                    2025,
                    1,
                    15,
                    5,
                    51,
                    20,
                    2,
                    15,
                    0
                ],
                "title": "LlamaRestTest: Effective REST API Testing with Small Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LlamaRestTest: Effective REST API Testing with Small Language Models"
                },
                "summary": "Modern web services rely heavily on REST APIs, typically documented using the\nOpenAPI specification. The widespread adoption of this standard has resulted in\nthe development of many black-box testing tools that generate tests based on\nthese specifications. Recent advancements in Natural Language Processing (NLP),\nparticularly with Large Language Models (LLMs), have enhanced REST API testing\nby extracting actionable rules and generating input values from the\nhuman-readable portions of the specification. However, these advancements\noverlook the potential of continuously refining the identified rules and test\ninputs based on server responses. To address this limitation, we present\nLlamaRestTest, a novel approach that employs two custom LLMs to generate\nrealistic test inputs and uncover parameter dependencies during the testing\nprocess by incorporating server responses. These LLMs are created by\nfine-tuning the Llama3-8b model, using mined datasets of REST API example\nvalues and inter-parameter dependencies. We evaluated LlamaRestTest on 12\nreal-world services (including popular services such as Spotify), comparing it\nagainst RESTGPT, a GPT-powered specification-enhancement tool, as well as\nseveral state-of-the-art REST API testing tools, including RESTler, MoRest,\nEvoMaster, and ARAT-RL. Our results show that fine-tuning enables smaller LLMs\nto outperform larger models in detecting actionable rules and generating inputs\nfor REST API testing. We evaluated configurations from the base Llama3-8B to\nfine-tuned versions and explored 2-bit, 4-bit, and 8-bit quantization for\nefficiency. LlamaRestTest surpasses state-of-the-art tools in code coverage and\nerror detection, even with RESTGPT-enhanced specifications, and an ablation\nstudy highlights the impact of its novel components.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Modern web services rely heavily on REST APIs, typically documented using the\nOpenAPI specification. The widespread adoption of this standard has resulted in\nthe development of many black-box testing tools that generate tests based on\nthese specifications. Recent advancements in Natural Language Processing (NLP),\nparticularly with Large Language Models (LLMs), have enhanced REST API testing\nby extracting actionable rules and generating input values from the\nhuman-readable portions of the specification. However, these advancements\noverlook the potential of continuously refining the identified rules and test\ninputs based on server responses. To address this limitation, we present\nLlamaRestTest, a novel approach that employs two custom LLMs to generate\nrealistic test inputs and uncover parameter dependencies during the testing\nprocess by incorporating server responses. These LLMs are created by\nfine-tuning the Llama3-8b model, using mined datasets of REST API example\nvalues and inter-parameter dependencies. We evaluated LlamaRestTest on 12\nreal-world services (including popular services such as Spotify), comparing it\nagainst RESTGPT, a GPT-powered specification-enhancement tool, as well as\nseveral state-of-the-art REST API testing tools, including RESTler, MoRest,\nEvoMaster, and ARAT-RL. Our results show that fine-tuning enables smaller LLMs\nto outperform larger models in detecting actionable rules and generating inputs\nfor REST API testing. We evaluated configurations from the base Llama3-8B to\nfine-tuned versions and explored 2-bit, 4-bit, and 8-bit quantization for\nefficiency. LlamaRestTest surpasses state-of-the-art tools in code coverage and\nerror detection, even with RESTGPT-enhanced specifications, and an ablation\nstudy highlights the impact of its novel components."
                },
                "authors": [
                    {
                        "name": "Myeongsoo Kim"
                    },
                    {
                        "name": "Saurabh Sinha"
                    },
                    {
                        "name": "Alessandro Orso"
                    }
                ],
                "author_detail": {
                    "name": "Alessandro Orso"
                },
                "author": "Alessandro Orso",
                "arxiv_comment": "To be published in the ACM International Conference on the\n  Foundations of Software Engineering (FSE 2025)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.08598v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.08598v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.08582v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.08582v1",
                "updated": "2025-01-15T05:07:06Z",
                "updated_parsed": [
                    2025,
                    1,
                    15,
                    5,
                    7,
                    6,
                    2,
                    15,
                    0
                ],
                "published": "2025-01-15T05:07:06Z",
                "published_parsed": [
                    2025,
                    1,
                    15,
                    5,
                    7,
                    6,
                    2,
                    15,
                    0
                ],
                "title": "LoRS: Efficient Low-Rank Adaptation for Sparse Large Language Model",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LoRS: Efficient Low-Rank Adaptation for Sparse Large Language Model"
                },
                "summary": "Existing low-rank adaptation (LoRA) methods face challenges on sparse large\nlanguage models (LLMs) due to the inability to maintain sparsity. Recent works\nintroduced methods that maintain sparsity by augmenting LoRA techniques with\nadditional masking mechanisms. Despite these successes, such approaches suffer\nfrom an increased memory and computation overhead, which affects efficiency of\nLoRA methods. In response to this limitation, we introduce LoRS, an innovative\nmethod designed to achieve both memory and computation efficiency when\nfine-tuning sparse LLMs. To mitigate the substantial memory and computation\ndemands associated with preserving sparsity, our approach incorporates\nstrategies of weight recompute and computational graph rearrangement. In\naddition, we also improve the effectiveness of LoRS through better adapter\ninitialization. These innovations lead to a notable reduction in memory and\ncomputation consumption during the fine-tuning phase, all while achieving\nperformance levels that outperform existing LoRA approaches.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Existing low-rank adaptation (LoRA) methods face challenges on sparse large\nlanguage models (LLMs) due to the inability to maintain sparsity. Recent works\nintroduced methods that maintain sparsity by augmenting LoRA techniques with\nadditional masking mechanisms. Despite these successes, such approaches suffer\nfrom an increased memory and computation overhead, which affects efficiency of\nLoRA methods. In response to this limitation, we introduce LoRS, an innovative\nmethod designed to achieve both memory and computation efficiency when\nfine-tuning sparse LLMs. To mitigate the substantial memory and computation\ndemands associated with preserving sparsity, our approach incorporates\nstrategies of weight recompute and computational graph rearrangement. In\naddition, we also improve the effectiveness of LoRS through better adapter\ninitialization. These innovations lead to a notable reduction in memory and\ncomputation consumption during the fine-tuning phase, all while achieving\nperformance levels that outperform existing LoRA approaches."
                },
                "authors": [
                    {
                        "name": "Yuxuan Hu"
                    },
                    {
                        "name": "Jing Zhang"
                    },
                    {
                        "name": "Xiaodong Chen"
                    },
                    {
                        "name": "Zhe Zhao"
                    },
                    {
                        "name": "Cuiping Li"
                    },
                    {
                        "name": "Hong Chen"
                    }
                ],
                "author_detail": {
                    "name": "Hong Chen"
                },
                "author": "Hong Chen",
                "arxiv_comment": "12 pages, 4 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.08582v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.08582v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.7",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.08579v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.08579v1",
                "updated": "2025-01-15T04:59:49Z",
                "updated_parsed": [
                    2025,
                    1,
                    15,
                    4,
                    59,
                    49,
                    2,
                    15,
                    0
                ],
                "published": "2025-01-15T04:59:49Z",
                "published_parsed": [
                    2025,
                    1,
                    15,
                    4,
                    59,
                    49,
                    2,
                    15,
                    0
                ],
                "title": "What Limits LLM-based Human Simulation: LLMs or Our Design?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "What Limits LLM-based Human Simulation: LLMs or Our Design?"
                },
                "summary": "We argue that advancing LLM-based human simulation requires addressing both\nLLM's inherent limitations and simulation framework design challenges. Recent\nstudies have revealed significant gaps between LLM-based human simulations and\nreal-world observations, highlighting these dual challenges. To address these\ngaps, we present a comprehensive analysis of LLM limitations and our design\nissues, proposing targeted solutions for both aspects. Furthermore, we explore\nfuture directions that address both challenges simultaneously, particularly in\ndata collection, LLM generation, and evaluation. To support further research in\nthis field, we provide a curated collection of LLM-based human simulation\nresources.\\footnote{https://github.com/Persdre/llm-human-simulation}",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We argue that advancing LLM-based human simulation requires addressing both\nLLM's inherent limitations and simulation framework design challenges. Recent\nstudies have revealed significant gaps between LLM-based human simulations and\nreal-world observations, highlighting these dual challenges. To address these\ngaps, we present a comprehensive analysis of LLM limitations and our design\nissues, proposing targeted solutions for both aspects. Furthermore, we explore\nfuture directions that address both challenges simultaneously, particularly in\ndata collection, LLM generation, and evaluation. To support further research in\nthis field, we provide a curated collection of LLM-based human simulation\nresources.\\footnote{https://github.com/Persdre/llm-human-simulation}"
                },
                "authors": [
                    {
                        "name": "Qian Wang"
                    },
                    {
                        "name": "Jiaying Wu"
                    },
                    {
                        "name": "Zhenheng Tang"
                    },
                    {
                        "name": "Bingqiao Luo"
                    },
                    {
                        "name": "Nuo Chen"
                    },
                    {
                        "name": "Wei Chen"
                    },
                    {
                        "name": "Bingsheng He"
                    }
                ],
                "author_detail": {
                    "name": "Bingsheng He"
                },
                "author": "Bingsheng He",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.08579v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.08579v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.10186v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.10186v2",
                "updated": "2025-01-15T04:55:04Z",
                "updated_parsed": [
                    2025,
                    1,
                    15,
                    4,
                    55,
                    4,
                    2,
                    15,
                    0
                ],
                "published": "2024-10-14T06:09:28Z",
                "published_parsed": [
                    2024,
                    10,
                    14,
                    6,
                    9,
                    28,
                    0,
                    288,
                    0
                ],
                "title": "A Possible Metal-Dominated Atmosphere Below the Thick Aerosols of GJ\n  1214 b Suggested by its JWST Panchromatic Transmission Spectrum",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Possible Metal-Dominated Atmosphere Below the Thick Aerosols of GJ\n  1214 b Suggested by its JWST Panchromatic Transmission Spectrum"
                },
                "summary": "GJ1214b is the archetype sub-Neptune for which thick aerosols have prevented\nus from constraining its atmospheric properties for over a decade. In this\nstudy, we leverage the panchromatic transmission spectrum of GJ1214b\nestablished by HST and JWST to investigate its atmospheric properties using a\nsuite of atmospheric radiative transfer, photochemistry, and aerosol\nmicrophysical models. We find that the combined HST, JWST/NIRSpec and JWST/MIRI\nspectrum can be well-explained by atmospheric models with an extremely high\nmetallicity of [M/H]$\\sim$3.5 and an extremely high haze production rate of\n$F_{\\rm haze}{\\sim}10^{-8}$--$10^{-7}$ g cm$^{-2}$ s$^{-1}$. Such high\natmospheric metallicity is suggested by the relatively strong CO2 feature\ncompared to the haze absorption feature or the CH4 feature in the NIRSpec-G395H\nbandpass of 2.5--5 $\\mu$m. The flat 5--12 $\\mu$m MIRI spectrum also suggests a\nsmall scale height with a high atmospheric metallicity that is needed to\nsuppress a prominent 6 $\\mu$m haze feature. We tested the sensitivity of our\ninterpretation to various assumptions for uncertain haze properties, such as\noptical constants and production rate, and all models tested here consistently\nsuggest extremely high metallicity. Thus, we conclude that GJ1214b likely has a\nmetal-dominated atmosphere where hydrogen is no longer the main atmospheric\nconstituent. We also find that different assumptions for the haze production\nrate lead to distinct inferences for the atmospheric C/O ratio. We stress the\nimportance of high precision follow-up observations to confirm the\nmetal-dominated atmosphere and to constrain the C/O ratio, which provides\nfurther insights on the planet formation process. The confirmation of the\nmetal-dominated atmosphere is particularly crucial, as it challenges the\nconventional understanding of interior structure and evolution of sub-Neptunes.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GJ1214b is the archetype sub-Neptune for which thick aerosols have prevented\nus from constraining its atmospheric properties for over a decade. In this\nstudy, we leverage the panchromatic transmission spectrum of GJ1214b\nestablished by HST and JWST to investigate its atmospheric properties using a\nsuite of atmospheric radiative transfer, photochemistry, and aerosol\nmicrophysical models. We find that the combined HST, JWST/NIRSpec and JWST/MIRI\nspectrum can be well-explained by atmospheric models with an extremely high\nmetallicity of [M/H]$\\sim$3.5 and an extremely high haze production rate of\n$F_{\\rm haze}{\\sim}10^{-8}$--$10^{-7}$ g cm$^{-2}$ s$^{-1}$. Such high\natmospheric metallicity is suggested by the relatively strong CO2 feature\ncompared to the haze absorption feature or the CH4 feature in the NIRSpec-G395H\nbandpass of 2.5--5 $\\mu$m. The flat 5--12 $\\mu$m MIRI spectrum also suggests a\nsmall scale height with a high atmospheric metallicity that is needed to\nsuppress a prominent 6 $\\mu$m haze feature. We tested the sensitivity of our\ninterpretation to various assumptions for uncertain haze properties, such as\noptical constants and production rate, and all models tested here consistently\nsuggest extremely high metallicity. Thus, we conclude that GJ1214b likely has a\nmetal-dominated atmosphere where hydrogen is no longer the main atmospheric\nconstituent. We also find that different assumptions for the haze production\nrate lead to distinct inferences for the atmospheric C/O ratio. We stress the\nimportance of high precision follow-up observations to confirm the\nmetal-dominated atmosphere and to constrain the C/O ratio, which provides\nfurther insights on the planet formation process. The confirmation of the\nmetal-dominated atmosphere is particularly crucial, as it challenges the\nconventional understanding of interior structure and evolution of sub-Neptunes."
                },
                "authors": [
                    {
                        "name": "Kazumasa Ohno"
                    },
                    {
                        "name": "Everett Schlawin"
                    },
                    {
                        "name": "Taylor J. Bell"
                    },
                    {
                        "name": "Matthew M. Murphy"
                    },
                    {
                        "name": "Thomas G. Beatty"
                    },
                    {
                        "name": "Luis Welbanks"
                    },
                    {
                        "name": "Thomas P. Greene"
                    },
                    {
                        "name": "Jonathan J. Fortney"
                    },
                    {
                        "name": "Vivien Parmentier"
                    },
                    {
                        "name": "Isaac R. Edelman"
                    },
                    {
                        "name": "Nishil Mehta"
                    },
                    {
                        "name": "Marcia J. Rieke"
                    }
                ],
                "author_detail": {
                    "name": "Marcia J. Rieke"
                },
                "author": "Marcia J. Rieke",
                "arxiv_comment": "22 pages, 10 figures, Published in ApJL, Please also see a companion\n  paper Schlawin et al. (2024)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.10186v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.10186v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.EP",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.EP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2402.18023v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2402.18023v3",
                "updated": "2025-01-15T04:47:36Z",
                "updated_parsed": [
                    2025,
                    1,
                    15,
                    4,
                    47,
                    36,
                    2,
                    15,
                    0
                ],
                "published": "2024-02-28T03:38:20Z",
                "published_parsed": [
                    2024,
                    2,
                    28,
                    3,
                    38,
                    20,
                    2,
                    59,
                    0
                ],
                "title": "Do Large Language Models Mirror Cognitive Language Processing?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Do Large Language Models Mirror Cognitive Language Processing?"
                },
                "summary": "Large Language Models (LLMs) have demonstrated remarkable abilities in text\ncomprehension and logical reasoning, indicating that the text representations\nlearned by LLMs can facilitate their language processing capabilities. In\nneuroscience, brain cognitive processing signals are typically utilized to\nstudy human language processing. Therefore, it is natural to ask how well the\ntext embeddings from LLMs align with the brain cognitive processing signals,\nand how training strategies affect the LLM-brain alignment? In this paper, we\nemploy Representational Similarity Analysis (RSA) to measure the alignment\nbetween 23 mainstream LLMs and fMRI signals of the brain to evaluate how\neffectively LLMs simulate cognitive language processing. We empirically\ninvestigate the impact of various factors (e.g., pre-training data size, model\nscaling, alignment training, and prompts) on such LLM-brain alignment.\nExperimental results indicate that pre-training data size and model scaling are\npositively correlated with LLM-brain similarity, and alignment training can\nsignificantly improve LLM-brain similarity. Explicit prompts contribute to the\nconsistency of LLMs with brain cognitive language processing, while nonsensical\nnoisy prompts may attenuate such alignment. Additionally, the performance of a\nwide range of LLM evaluations (e.g., MMLU, Chatbot Arena) is highly correlated\nwith the LLM-brain similarity.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have demonstrated remarkable abilities in text\ncomprehension and logical reasoning, indicating that the text representations\nlearned by LLMs can facilitate their language processing capabilities. In\nneuroscience, brain cognitive processing signals are typically utilized to\nstudy human language processing. Therefore, it is natural to ask how well the\ntext embeddings from LLMs align with the brain cognitive processing signals,\nand how training strategies affect the LLM-brain alignment? In this paper, we\nemploy Representational Similarity Analysis (RSA) to measure the alignment\nbetween 23 mainstream LLMs and fMRI signals of the brain to evaluate how\neffectively LLMs simulate cognitive language processing. We empirically\ninvestigate the impact of various factors (e.g., pre-training data size, model\nscaling, alignment training, and prompts) on such LLM-brain alignment.\nExperimental results indicate that pre-training data size and model scaling are\npositively correlated with LLM-brain similarity, and alignment training can\nsignificantly improve LLM-brain similarity. Explicit prompts contribute to the\nconsistency of LLMs with brain cognitive language processing, while nonsensical\nnoisy prompts may attenuate such alignment. Additionally, the performance of a\nwide range of LLM evaluations (e.g., MMLU, Chatbot Arena) is highly correlated\nwith the LLM-brain similarity."
                },
                "authors": [
                    {
                        "name": "Yuqi Ren"
                    },
                    {
                        "name": "Renren Jin"
                    },
                    {
                        "name": "Tongxuan Zhang"
                    },
                    {
                        "name": "Deyi Xiong"
                    }
                ],
                "author_detail": {
                    "name": "Deyi Xiong"
                },
                "author": "Deyi Xiong",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2402.18023v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2402.18023v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.08570v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.08570v1",
                "updated": "2025-01-15T04:32:41Z",
                "updated_parsed": [
                    2025,
                    1,
                    15,
                    4,
                    32,
                    41,
                    2,
                    15,
                    0
                ],
                "published": "2025-01-15T04:32:41Z",
                "published_parsed": [
                    2025,
                    1,
                    15,
                    4,
                    32,
                    41,
                    2,
                    15,
                    0
                ],
                "title": "Information Entropy Invariance: Enhancing Length Extrapolation in\n  Attention Mechanisms",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Information Entropy Invariance: Enhancing Length Extrapolation in\n  Attention Mechanisms"
                },
                "summary": "Improving the length extrapolation capabilities of Large Language Models\n(LLMs) remains a critical challenge in natural language processing. Many recent\nefforts have focused on modifying the scaled dot-product attention mechanism,\nand often introduce scaled temperatures without rigorous theoretical\njustification. To fill this gap, we introduce a novel approach based on\ninformation entropy invariance. We propose two new scaled temperatures to\nenhance length extrapolation. First, a training-free method InfoScale is\ndesigned for dot-product attention, and preserves focus on original tokens\nduring length extrapolation by ensuring information entropy remains consistent.\nSecond, we theoretically analyze the impact of scaling (CosScale) on cosine\nattention. Experimental data demonstrates that combining InfoScale and CosScale\nachieves state-of-the-art performance on the GAU-{\\alpha} model with a context\nwindow extended to 64 times the training length, and outperforms seven existing\nmethods. Our analysis reveals that significantly increasing CosScale\napproximates windowed attention, and highlights the significance of attention\nscore dilution as a key challenge in long-range context handling. The code and\ndata are available at https://github.com/HT-NEKO/InfoScale.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Improving the length extrapolation capabilities of Large Language Models\n(LLMs) remains a critical challenge in natural language processing. Many recent\nefforts have focused on modifying the scaled dot-product attention mechanism,\nand often introduce scaled temperatures without rigorous theoretical\njustification. To fill this gap, we introduce a novel approach based on\ninformation entropy invariance. We propose two new scaled temperatures to\nenhance length extrapolation. First, a training-free method InfoScale is\ndesigned for dot-product attention, and preserves focus on original tokens\nduring length extrapolation by ensuring information entropy remains consistent.\nSecond, we theoretically analyze the impact of scaling (CosScale) on cosine\nattention. Experimental data demonstrates that combining InfoScale and CosScale\nachieves state-of-the-art performance on the GAU-{\\alpha} model with a context\nwindow extended to 64 times the training length, and outperforms seven existing\nmethods. Our analysis reveals that significantly increasing CosScale\napproximates windowed attention, and highlights the significance of attention\nscore dilution as a key challenge in long-range context handling. The code and\ndata are available at https://github.com/HT-NEKO/InfoScale."
                },
                "authors": [
                    {
                        "name": "Kewei Li"
                    },
                    {
                        "name": "Yanwen Kong"
                    },
                    {
                        "name": "Yiping Xu"
                    },
                    {
                        "name": "Lan Huang"
                    },
                    {
                        "name": "Ruochi Zhang"
                    },
                    {
                        "name": "Fengfeng Zhou"
                    }
                ],
                "author_detail": {
                    "name": "Fengfeng Zhou"
                },
                "author": "Fengfeng Zhou",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.08570v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.08570v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.19213v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.19213v2",
                "updated": "2025-01-15T04:17:38Z",
                "updated_parsed": [
                    2025,
                    1,
                    15,
                    4,
                    17,
                    38,
                    2,
                    15,
                    0
                ],
                "published": "2024-05-29T15:56:33Z",
                "published_parsed": [
                    2024,
                    5,
                    29,
                    15,
                    56,
                    33,
                    2,
                    150,
                    0
                ],
                "title": "EdgeSight: Enabling Modeless and Cost-Efficient Inference at the Edge",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "EdgeSight: Enabling Modeless and Cost-Efficient Inference at the Edge"
                },
                "summary": "Traditional ML inference is evolving toward modeless inference, which\nabstracts the complexity of model selection from users, allowing the system to\nautomatically choose the most appropriate model for each request based on\naccuracy and resource requirements. While prior studies have focused on\nmodeless inference within data centers, this paper tackles the pressing need\nfor cost-efficient modeless inference at the edge -- particularly within its\nunique constraints of limited device memory, volatile network conditions, and\nrestricted power consumption.\n  To overcome these challenges, we propose EdgeSight, a system that provides\ncost-efficient EdgeSight serving for diverse DNNs at the edge. EdgeSight\nemploys an edge-data center (edge-DC) architecture, utilizing confidence\nscaling to reduce the number of model options while meeting diverse accuracy\nrequirements. Additionally, it supports lossy inference in volatile network\nenvironments. Our experimental results show that EdgeSight outperforms existing\nsystems by up to 1.6x in P99 latency for modeless services. Furthermore, our\nFPGA prototype demonstrates similar performance at certain accuracy levels,\nwith a power consumption reduction of up to 3.34x.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Traditional ML inference is evolving toward modeless inference, which\nabstracts the complexity of model selection from users, allowing the system to\nautomatically choose the most appropriate model for each request based on\naccuracy and resource requirements. While prior studies have focused on\nmodeless inference within data centers, this paper tackles the pressing need\nfor cost-efficient modeless inference at the edge -- particularly within its\nunique constraints of limited device memory, volatile network conditions, and\nrestricted power consumption.\n  To overcome these challenges, we propose EdgeSight, a system that provides\ncost-efficient EdgeSight serving for diverse DNNs at the edge. EdgeSight\nemploys an edge-data center (edge-DC) architecture, utilizing confidence\nscaling to reduce the number of model options while meeting diverse accuracy\nrequirements. Additionally, it supports lossy inference in volatile network\nenvironments. Our experimental results show that EdgeSight outperforms existing\nsystems by up to 1.6x in P99 latency for modeless services. Furthermore, our\nFPGA prototype demonstrates similar performance at certain accuracy levels,\nwith a power consumption reduction of up to 3.34x."
                },
                "authors": [
                    {
                        "name": "ChonLam Lao"
                    },
                    {
                        "name": "Jiaqi Gao"
                    },
                    {
                        "name": "Ganesh Ananthanarayanan"
                    },
                    {
                        "name": "Aditya Akella"
                    },
                    {
                        "name": "Minlan Yu"
                    }
                ],
                "author_detail": {
                    "name": "Minlan Yu"
                },
                "author": "Minlan Yu",
                "arxiv_comment": "12 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.19213v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.19213v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.08558v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.08558v1",
                "updated": "2025-01-15T03:49:08Z",
                "updated_parsed": [
                    2025,
                    1,
                    15,
                    3,
                    49,
                    8,
                    2,
                    15,
                    0
                ],
                "published": "2025-01-15T03:49:08Z",
                "published_parsed": [
                    2025,
                    1,
                    15,
                    3,
                    49,
                    8,
                    2,
                    15,
                    0
                ],
                "title": "LAMS: LLM-Driven Automatic Mode Switching for Assistive Teleoperation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LAMS: LLM-Driven Automatic Mode Switching for Assistive Teleoperation"
                },
                "summary": "Teleoperating high degrees-of-freedom (DoF) robotic manipulators via low-DoF\ncontrollers like joysticks often requires frequent switching between control\nmodes, where each mode maps controller movements to specific robot actions.\nManually performing this frequent switching can make teleoperation cumbersome\nand inefficient. On the other hand, existing automatic mode-switching\nsolutions, such as heuristic-based or learning-based methods, are often\ntask-specific and lack generalizability. In this paper, we introduce LLM-Driven\nAutomatic Mode Switching (LAMS), a novel approach that leverages Large Language\nModels (LLMs) to automatically switch control modes based on task context.\nUnlike existing methods, LAMS requires no prior task demonstrations and\nincrementally improves by integrating user-generated mode-switching examples.\nWe validate LAMS through an ablation study and a user study with 10\nparticipants on complex, long-horizon tasks, demonstrating that LAMS\neffectively reduces manual mode switches, is preferred over alternative\nmethods, and improves performance over time. The project website with\nsupplementary materials is at https://lams-assistance.github.io/.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Teleoperating high degrees-of-freedom (DoF) robotic manipulators via low-DoF\ncontrollers like joysticks often requires frequent switching between control\nmodes, where each mode maps controller movements to specific robot actions.\nManually performing this frequent switching can make teleoperation cumbersome\nand inefficient. On the other hand, existing automatic mode-switching\nsolutions, such as heuristic-based or learning-based methods, are often\ntask-specific and lack generalizability. In this paper, we introduce LLM-Driven\nAutomatic Mode Switching (LAMS), a novel approach that leverages Large Language\nModels (LLMs) to automatically switch control modes based on task context.\nUnlike existing methods, LAMS requires no prior task demonstrations and\nincrementally improves by integrating user-generated mode-switching examples.\nWe validate LAMS through an ablation study and a user study with 10\nparticipants on complex, long-horizon tasks, demonstrating that LAMS\neffectively reduces manual mode switches, is preferred over alternative\nmethods, and improves performance over time. The project website with\nsupplementary materials is at https://lams-assistance.github.io/."
                },
                "authors": [
                    {
                        "name": "Yiran Tao"
                    },
                    {
                        "name": "Jehan Yang"
                    },
                    {
                        "name": "Dan Ding"
                    },
                    {
                        "name": "Zackory Erickson"
                    }
                ],
                "author_detail": {
                    "name": "Zackory Erickson"
                },
                "author": "Zackory Erickson",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.08558v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.08558v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.04820v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.04820v2",
                "updated": "2025-01-15T03:43:22Z",
                "updated_parsed": [
                    2025,
                    1,
                    15,
                    3,
                    43,
                    22,
                    2,
                    15,
                    0
                ],
                "published": "2024-08-09T02:22:51Z",
                "published_parsed": [
                    2024,
                    8,
                    9,
                    2,
                    22,
                    51,
                    4,
                    222,
                    0
                ],
                "title": "Natural Language Outlines for Code: Literate Programming in the LLM Era",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Natural Language Outlines for Code: Literate Programming in the LLM Era"
                },
                "summary": "We propose using natural language outlines as a novel modality and\ninteraction surface for providing AI assistance to developers throughout the\nsoftware development process. An NL outline for a code function comprises\nmultiple statements written in concise prose, which partition the code and\nsummarize its main ideas in the style of literate programming. Crucially, we\nfind that modern LLMs can generate accurate and high-quality NL outlines in\npractice. Moreover, NL outlines enable a bidirectional sync between code and\nNL, allowing changes in one to be automatically reflected in the other. We\ndiscuss many use cases for NL outlines: they can accelerate understanding and\nnavigation of code and diffs, simplify code maintenance, augment code search,\nsteer code generation, and more. We then propose and compare multiple LLM\nprompting techniques for generating outlines and ask professional developers to\njudge outline quality. Finally, we present two case studies applying NL\noutlines toward code review and malware detection.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We propose using natural language outlines as a novel modality and\ninteraction surface for providing AI assistance to developers throughout the\nsoftware development process. An NL outline for a code function comprises\nmultiple statements written in concise prose, which partition the code and\nsummarize its main ideas in the style of literate programming. Crucially, we\nfind that modern LLMs can generate accurate and high-quality NL outlines in\npractice. Moreover, NL outlines enable a bidirectional sync between code and\nNL, allowing changes in one to be automatically reflected in the other. We\ndiscuss many use cases for NL outlines: they can accelerate understanding and\nnavigation of code and diffs, simplify code maintenance, augment code search,\nsteer code generation, and more. We then propose and compare multiple LLM\nprompting techniques for generating outlines and ask professional developers to\njudge outline quality. Finally, we present two case studies applying NL\noutlines toward code review and malware detection."
                },
                "authors": [
                    {
                        "name": "Kensen Shi"
                    },
                    {
                        "name": "Deniz Altınbüken"
                    },
                    {
                        "name": "Saswat Anand"
                    },
                    {
                        "name": "Mihai Christodorescu"
                    },
                    {
                        "name": "Katja Grünwedel"
                    },
                    {
                        "name": "Alexa Koenings"
                    },
                    {
                        "name": "Sai Naidu"
                    },
                    {
                        "name": "Anurag Pathak"
                    },
                    {
                        "name": "Marc Rasi"
                    },
                    {
                        "name": "Fredde Ribeiro"
                    },
                    {
                        "name": "Brandon Ruffin"
                    },
                    {
                        "name": "Siddhant Sanyam"
                    },
                    {
                        "name": "Maxim Tabachnyk"
                    },
                    {
                        "name": "Sara Toth"
                    },
                    {
                        "name": "Roy Tu"
                    },
                    {
                        "name": "Tobias Welp"
                    },
                    {
                        "name": "Pengcheng Yin"
                    },
                    {
                        "name": "Manzil Zaheer"
                    },
                    {
                        "name": "Satish Chandra"
                    },
                    {
                        "name": "Charles Sutton"
                    }
                ],
                "author_detail": {
                    "name": "Charles Sutton"
                },
                "author": "Charles Sutton",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.04820v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.04820v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.08553v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.08553v2",
                "updated": "2025-01-15T03:37:00Z",
                "updated_parsed": [
                    2025,
                    1,
                    15,
                    3,
                    37,
                    0,
                    2,
                    15,
                    0
                ],
                "published": "2024-08-16T06:37:59Z",
                "published_parsed": [
                    2024,
                    8,
                    16,
                    6,
                    37,
                    59,
                    4,
                    229,
                    0
                ],
                "title": "Improving the Ability of Pre-trained Language Model by Imparting Large\n  Language Model's Experience",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Improving the Ability of Pre-trained Language Model by Imparting Large\n  Language Model's Experience"
                },
                "summary": "Large Language Models (LLMs) and pre-trained Language Models (LMs) have\nachieved impressive success on many software engineering tasks (e.g., code\ncompletion and code generation). By leveraging huge existing code corpora\n(e.g., GitHub), these models can understand the patterns in source code and use\nthese patterns to predict code properties. However, LLMs under few-shot\nlearning perform poorly on non-generative tasks (e.g., fault localization and\nvulnerability localization), and fine-tuning LLMs is time-consuming and costly\nfor end users and small organizations. Furthermore, the performance of\nfine-tuning LMs for non-generative tasks is impressive, yet it heavily depends\non the amount and quality of data. As a result, the current lack of data and\nthe high cost of collecting it in real-world scenarios further limit the\napplicability of LMs. In this paper, we leverage the powerful generation\ncapabilities of LLMs to enhance pre-trained LMs. Specifically, we use LLMs to\ngenerate domain-specific data, thereby improving the performance of pre-trained\nLMs on the target tasks. We conduct experiments by combining different LLMs in\nour generation phase and introducing various LMs to learn from the\nLLM-generated data. Then, we compare the performance of these LMs before and\nafter learning the data. We find that LLM-generated data significantly enhances\nthe performance of LMs. The improvement can reach up to 58.36% for fault\nlocalization and up to 6.09% for clone detection.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) and pre-trained Language Models (LMs) have\nachieved impressive success on many software engineering tasks (e.g., code\ncompletion and code generation). By leveraging huge existing code corpora\n(e.g., GitHub), these models can understand the patterns in source code and use\nthese patterns to predict code properties. However, LLMs under few-shot\nlearning perform poorly on non-generative tasks (e.g., fault localization and\nvulnerability localization), and fine-tuning LLMs is time-consuming and costly\nfor end users and small organizations. Furthermore, the performance of\nfine-tuning LMs for non-generative tasks is impressive, yet it heavily depends\non the amount and quality of data. As a result, the current lack of data and\nthe high cost of collecting it in real-world scenarios further limit the\napplicability of LMs. In this paper, we leverage the powerful generation\ncapabilities of LLMs to enhance pre-trained LMs. Specifically, we use LLMs to\ngenerate domain-specific data, thereby improving the performance of pre-trained\nLMs on the target tasks. We conduct experiments by combining different LLMs in\nour generation phase and introducing various LMs to learn from the\nLLM-generated data. Then, we compare the performance of these LMs before and\nafter learning the data. We find that LLM-generated data significantly enhances\nthe performance of LMs. The improvement can reach up to 58.36% for fault\nlocalization and up to 6.09% for clone detection."
                },
                "authors": [
                    {
                        "name": "Xin Yin"
                    },
                    {
                        "name": "Chao Ni"
                    },
                    {
                        "name": "Xiaodan Xu"
                    },
                    {
                        "name": "Xinrui Li"
                    },
                    {
                        "name": "Xiaohu Yang"
                    }
                ],
                "author_detail": {
                    "name": "Xiaohu Yang"
                },
                "author": "Xiaohu Yang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.08553v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.08553v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.11514v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.11514v2",
                "updated": "2025-01-15T03:20:24Z",
                "updated_parsed": [
                    2025,
                    1,
                    15,
                    3,
                    20,
                    24,
                    2,
                    15,
                    0
                ],
                "published": "2024-06-17T13:21:23Z",
                "published_parsed": [
                    2024,
                    6,
                    17,
                    13,
                    21,
                    23,
                    0,
                    169,
                    0
                ],
                "title": "Counterfactual Debating with Preset Stances for Hallucination\n  Elimination of LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Counterfactual Debating with Preset Stances for Hallucination\n  Elimination of LLMs"
                },
                "summary": "Large Language Models (LLMs) excel in various natural language processing\ntasks but struggle with hallucination issues. Existing solutions have\nconsidered utilizing LLMs' inherent reasoning abilities to alleviate\nhallucination, such as self-correction and diverse sampling methods. However,\nthese methods often overtrust LLMs' initial answers due to inherent biases. The\nkey to alleviating this issue lies in overriding LLMs' inherent biases for\nanswer inspection. To this end, we propose a CounterFactual Multi-Agent Debate\n(CFMAD) framework. CFMAD presets the stances of LLMs to override their inherent\nbiases by compelling LLMs to generate justifications for a predetermined\nanswer's correctness. The LLMs with different predetermined stances are engaged\nwith a skeptical critic for counterfactual debate on the rationality of\ngenerated justifications. Finally, the debate process is evaluated by a\nthird-party judge to determine the final answer. Extensive experiments on four\ndatasets of three tasks demonstrate the superiority of CFMAD over existing\nmethods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) excel in various natural language processing\ntasks but struggle with hallucination issues. Existing solutions have\nconsidered utilizing LLMs' inherent reasoning abilities to alleviate\nhallucination, such as self-correction and diverse sampling methods. However,\nthese methods often overtrust LLMs' initial answers due to inherent biases. The\nkey to alleviating this issue lies in overriding LLMs' inherent biases for\nanswer inspection. To this end, we propose a CounterFactual Multi-Agent Debate\n(CFMAD) framework. CFMAD presets the stances of LLMs to override their inherent\nbiases by compelling LLMs to generate justifications for a predetermined\nanswer's correctness. The LLMs with different predetermined stances are engaged\nwith a skeptical critic for counterfactual debate on the rationality of\ngenerated justifications. Finally, the debate process is evaluated by a\nthird-party judge to determine the final answer. Extensive experiments on four\ndatasets of three tasks demonstrate the superiority of CFMAD over existing\nmethods."
                },
                "authors": [
                    {
                        "name": "Yi Fang"
                    },
                    {
                        "name": "Moxin Li"
                    },
                    {
                        "name": "Wenjie Wang"
                    },
                    {
                        "name": "Hui Lin"
                    },
                    {
                        "name": "Fuli Feng"
                    }
                ],
                "author_detail": {
                    "name": "Fuli Feng"
                },
                "author": "Fuli Feng",
                "arxiv_comment": "accepted by COLING 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.11514v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.11514v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.08549v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.08549v1",
                "updated": "2025-01-15T03:17:24Z",
                "updated_parsed": [
                    2025,
                    1,
                    15,
                    3,
                    17,
                    24,
                    2,
                    15,
                    0
                ],
                "published": "2025-01-15T03:17:24Z",
                "published_parsed": [
                    2025,
                    1,
                    15,
                    3,
                    17,
                    24,
                    2,
                    15,
                    0
                ],
                "title": "The Devil is in Temporal Token: High Quality Video Reasoning\n  Segmentation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Devil is in Temporal Token: High Quality Video Reasoning\n  Segmentation"
                },
                "summary": "Existing methods for Video Reasoning Segmentation rely heavily on a single\nspecial token to represent the object in the keyframe or the entire video,\ninadequately capturing spatial complexity and inter-frame motion. To overcome\nthese challenges, we propose VRS-HQ, an end-to-end video reasoning segmentation\napproach that leverages Multimodal Large Language Models (MLLMs) to inject rich\nspatiotemporal features into hierarchical tokens.Our key innovations include a\nTemporal Dynamic Aggregation (TDA) and a Token-driven Keyframe Selection (TKS).\nSpecifically, we design frame-level <SEG> and temporal-level <TAK> tokens that\nutilize MLLM's autoregressive learning to effectively capture both local and\nglobal information. Subsequently, we apply a similarity-based weighted fusion\nand frame selection strategy, then utilize SAM2 to perform keyframe\nsegmentation and propagation. To enhance keyframe localization accuracy, the\nTKS filters keyframes based on SAM2's occlusion scores during inference. VRS-HQ\nachieves state-of-the-art performance on ReVOS, surpassing VISA by\n5.9%/12.5%/9.1% in J&F scores across the three subsets. These results highlight\nthe strong temporal reasoning and segmentation capabilities of our method. Code\nand model weights will be released at VRS-HQ.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Existing methods for Video Reasoning Segmentation rely heavily on a single\nspecial token to represent the object in the keyframe or the entire video,\ninadequately capturing spatial complexity and inter-frame motion. To overcome\nthese challenges, we propose VRS-HQ, an end-to-end video reasoning segmentation\napproach that leverages Multimodal Large Language Models (MLLMs) to inject rich\nspatiotemporal features into hierarchical tokens.Our key innovations include a\nTemporal Dynamic Aggregation (TDA) and a Token-driven Keyframe Selection (TKS).\nSpecifically, we design frame-level <SEG> and temporal-level <TAK> tokens that\nutilize MLLM's autoregressive learning to effectively capture both local and\nglobal information. Subsequently, we apply a similarity-based weighted fusion\nand frame selection strategy, then utilize SAM2 to perform keyframe\nsegmentation and propagation. To enhance keyframe localization accuracy, the\nTKS filters keyframes based on SAM2's occlusion scores during inference. VRS-HQ\nachieves state-of-the-art performance on ReVOS, surpassing VISA by\n5.9%/12.5%/9.1% in J&F scores across the three subsets. These results highlight\nthe strong temporal reasoning and segmentation capabilities of our method. Code\nand model weights will be released at VRS-HQ."
                },
                "authors": [
                    {
                        "name": "Sitong Gong"
                    },
                    {
                        "name": "Yunzhi Zhuge"
                    },
                    {
                        "name": "Lu Zhang"
                    },
                    {
                        "name": "Zongxin Yang"
                    },
                    {
                        "name": "Pingping Zhang"
                    },
                    {
                        "name": "Huchuan Lu"
                    }
                ],
                "author_detail": {
                    "name": "Huchuan Lu"
                },
                "author": "Huchuan Lu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.08549v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.08549v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.01028v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.01028v4",
                "updated": "2025-01-15T03:02:22Z",
                "updated_parsed": [
                    2025,
                    1,
                    15,
                    3,
                    2,
                    22,
                    2,
                    15,
                    0
                ],
                "published": "2025-01-02T03:17:51Z",
                "published_parsed": [
                    2025,
                    1,
                    2,
                    3,
                    17,
                    51,
                    3,
                    2,
                    0
                ],
                "title": "KaLM-Embedding: Superior Training Data Brings A Stronger Embedding Model",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "KaLM-Embedding: Superior Training Data Brings A Stronger Embedding Model"
                },
                "summary": "As retrieval-augmented generation prevails in large language models,\nembedding models are becoming increasingly crucial. Despite the growing number\nof general embedding models, prior work often overlooks the critical role of\ntraining data quality. In this work, we introduce KaLM-Embedding, a general\nmultilingual embedding model that leverages a large quantity of cleaner, more\ndiverse, and domain-specific training data. Our model has been trained with key\ntechniques proven to enhance performance: (1) persona-based synthetic data to\ncreate diversified examples distilled from LLMs, (2) ranking consistency\nfiltering to remove less informative samples, and (3) semi-homogeneous task\nbatch sampling to improve training efficacy. Departing from traditional\nBERT-like architectures, we adopt Qwen2-0.5B as the pre-trained model,\nfacilitating the adaptation of auto-regressive language models for general\nembedding tasks. Extensive evaluations of the MTEB benchmark across multiple\nlanguages show that our model outperforms others of comparable size, setting a\nnew standard for multilingual embedding models with <1B parameters.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As retrieval-augmented generation prevails in large language models,\nembedding models are becoming increasingly crucial. Despite the growing number\nof general embedding models, prior work often overlooks the critical role of\ntraining data quality. In this work, we introduce KaLM-Embedding, a general\nmultilingual embedding model that leverages a large quantity of cleaner, more\ndiverse, and domain-specific training data. Our model has been trained with key\ntechniques proven to enhance performance: (1) persona-based synthetic data to\ncreate diversified examples distilled from LLMs, (2) ranking consistency\nfiltering to remove less informative samples, and (3) semi-homogeneous task\nbatch sampling to improve training efficacy. Departing from traditional\nBERT-like architectures, we adopt Qwen2-0.5B as the pre-trained model,\nfacilitating the adaptation of auto-regressive language models for general\nembedding tasks. Extensive evaluations of the MTEB benchmark across multiple\nlanguages show that our model outperforms others of comparable size, setting a\nnew standard for multilingual embedding models with <1B parameters."
                },
                "authors": [
                    {
                        "name": "Xinshuo Hu"
                    },
                    {
                        "name": "Zifei Shan"
                    },
                    {
                        "name": "Xinping Zhao"
                    },
                    {
                        "name": "Zetian Sun"
                    },
                    {
                        "name": "Zhenyu Liu"
                    },
                    {
                        "name": "Dongfang Li"
                    },
                    {
                        "name": "Shaolin Ye"
                    },
                    {
                        "name": "Xinyuan Wei"
                    },
                    {
                        "name": "Qian Chen"
                    },
                    {
                        "name": "Baotian Hu"
                    },
                    {
                        "name": "Haofen Wang"
                    },
                    {
                        "name": "Jun Yu"
                    },
                    {
                        "name": "Min Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Min Zhang"
                },
                "author": "Min Zhang",
                "arxiv_comment": "Technical Report. 23 pages, 6 figures, 10 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.01028v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.01028v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.08540v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.08540v1",
                "updated": "2025-01-15T03:00:57Z",
                "updated_parsed": [
                    2025,
                    1,
                    15,
                    3,
                    0,
                    57,
                    2,
                    15,
                    0
                ],
                "published": "2025-01-15T03:00:57Z",
                "published_parsed": [
                    2025,
                    1,
                    15,
                    3,
                    0,
                    57,
                    2,
                    15,
                    0
                ],
                "title": "Knowledge prompt chaining for semantic modeling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Knowledge prompt chaining for semantic modeling"
                },
                "summary": "The task of building semantics for structured data such as CSV, JSON, and XML\nfiles is highly relevant in the knowledge representation field. Even though we\nhave a vast of structured data on the internet, mapping them to domain\nontologies to build semantics for them is still very challenging as it requires\nthe construction model to understand and learn graph-structured knowledge.\nOtherwise, the task will require human beings' effort and cost. In this paper,\nwe proposed a novel automatic semantic modeling framework: Knowledge Prompt\nChaining. It can serialize the graph-structured knowledge and inject it into\nthe LLMs properly in a Prompt Chaining architecture. Through this knowledge\ninjection and prompting chaining, the model in our framework can learn the\nstructure information and latent space of the graph and generate the semantic\nlabels and semantic graphs following the chains' insturction naturally. Based\non experimental results, our method achieves better performance than existing\nleading techniques, despite using reduced structured input data.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The task of building semantics for structured data such as CSV, JSON, and XML\nfiles is highly relevant in the knowledge representation field. Even though we\nhave a vast of structured data on the internet, mapping them to domain\nontologies to build semantics for them is still very challenging as it requires\nthe construction model to understand and learn graph-structured knowledge.\nOtherwise, the task will require human beings' effort and cost. In this paper,\nwe proposed a novel automatic semantic modeling framework: Knowledge Prompt\nChaining. It can serialize the graph-structured knowledge and inject it into\nthe LLMs properly in a Prompt Chaining architecture. Through this knowledge\ninjection and prompting chaining, the model in our framework can learn the\nstructure information and latent space of the graph and generate the semantic\nlabels and semantic graphs following the chains' insturction naturally. Based\non experimental results, our method achieves better performance than existing\nleading techniques, despite using reduced structured input data."
                },
                "authors": [
                    {
                        "name": "Ning Pei Ding"
                    },
                    {
                        "name": "Jingge Du"
                    },
                    {
                        "name": "Zaiwen Feng"
                    }
                ],
                "author_detail": {
                    "name": "Zaiwen Feng"
                },
                "author": "Zaiwen Feng",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.08540v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.08540v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.08538v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.08538v1",
                "updated": "2025-01-15T02:56:50Z",
                "updated_parsed": [
                    2025,
                    1,
                    15,
                    2,
                    56,
                    50,
                    2,
                    15,
                    0
                ],
                "published": "2025-01-15T02:56:50Z",
                "published_parsed": [
                    2025,
                    1,
                    15,
                    2,
                    56,
                    50,
                    2,
                    15,
                    0
                ],
                "title": "Homophily-aware Heterogeneous Graph Contrastive Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Homophily-aware Heterogeneous Graph Contrastive Learning"
                },
                "summary": "Heterogeneous graph pre-training (HGP) has demonstrated remarkable\nperformance across various domains. However, the issue of heterophily in\nreal-world heterogeneous graphs (HGs) has been largely overlooked. To bridge\nthis research gap, we proposed a novel heterogeneous graph contrastive learning\nframework, termed HGMS, which leverages connection strength and multi-view\nself-expression to learn homophilous node representations. Specifically, we\ndesign a heterogeneous edge dropping augmentation strategy that enhances the\nhomophily of augmented views. Moreover, we introduce a multi-view\nself-expressive learning method to infer the homophily between nodes. In\npractice, we develop two approaches to solve the self-expressive matrix. The\nsolved self-expressive matrix serves as an additional augmented view to provide\nhomophilous information and is used to identify false negatives in contrastive\nloss. Extensive experimental results demonstrate the superiority of HGMS across\ndifferent downstream tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Heterogeneous graph pre-training (HGP) has demonstrated remarkable\nperformance across various domains. However, the issue of heterophily in\nreal-world heterogeneous graphs (HGs) has been largely overlooked. To bridge\nthis research gap, we proposed a novel heterogeneous graph contrastive learning\nframework, termed HGMS, which leverages connection strength and multi-view\nself-expression to learn homophilous node representations. Specifically, we\ndesign a heterogeneous edge dropping augmentation strategy that enhances the\nhomophily of augmented views. Moreover, we introduce a multi-view\nself-expressive learning method to infer the homophily between nodes. In\npractice, we develop two approaches to solve the self-expressive matrix. The\nsolved self-expressive matrix serves as an additional augmented view to provide\nhomophilous information and is used to identify false negatives in contrastive\nloss. Extensive experimental results demonstrate the superiority of HGMS across\ndifferent downstream tasks."
                },
                "authors": [
                    {
                        "name": "Haosen Wang"
                    },
                    {
                        "name": "Chenglong Shi"
                    },
                    {
                        "name": "Can Xu"
                    },
                    {
                        "name": "Surong Yan"
                    },
                    {
                        "name": "Pan Tang"
                    }
                ],
                "author_detail": {
                    "name": "Pan Tang"
                },
                "author": "Pan Tang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.08538v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.08538v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.08524v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.08524v1",
                "updated": "2025-01-15T02:26:21Z",
                "updated_parsed": [
                    2025,
                    1,
                    15,
                    2,
                    26,
                    21,
                    2,
                    15,
                    0
                ],
                "published": "2025-01-15T02:26:21Z",
                "published_parsed": [
                    2025,
                    1,
                    15,
                    2,
                    26,
                    21,
                    2,
                    15,
                    0
                ],
                "title": "Deep inference of simulated strong lenses in ground-based surveys",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Deep inference of simulated strong lenses in ground-based surveys"
                },
                "summary": "The large number of strong lenses discoverable in future astronomical surveys\nwill likely enhance the value of strong gravitational lensing as a cosmic probe\nof dark energy and dark matter. However, leveraging the increased statistical\npower of such large samples will require further development of automated lens\nmodeling techniques. We show that deep learning and simulation-based inference\n(SBI) methods produce informative and reliable estimates of parameter\nposteriors for strong lensing systems in ground-based surveys. We present the\nexamination and comparison of two approaches to lens parameter estimation for\nstrong galaxy-galaxy lenses -- Neural Posterior Estimation (NPE) and Bayesian\nNeural Networks (BNNs). We perform inference on 1-, 5-, and 12-parameter lens\nmodels for ground-based imaging data that mimics the Dark Energy Survey (DES).\nWe find that NPE outperforms BNNs, producing posterior distributions that are\nmore accurate, precise, and well-calibrated for most parameters. For the\n12-parameter NPE model, the calibration is consistently within $<$10\\% of\noptimal calibration for all parameters, while the BNN is rarely within 20\\% of\noptimal calibration for any of the parameters. Similarly, residuals for most of\nthe parameters are smaller (by up to an order of magnitude) with the NPE model\nthan the BNN model. This work takes important steps in the systematic\ncomparison of methods for different levels of model complexity.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The large number of strong lenses discoverable in future astronomical surveys\nwill likely enhance the value of strong gravitational lensing as a cosmic probe\nof dark energy and dark matter. However, leveraging the increased statistical\npower of such large samples will require further development of automated lens\nmodeling techniques. We show that deep learning and simulation-based inference\n(SBI) methods produce informative and reliable estimates of parameter\nposteriors for strong lensing systems in ground-based surveys. We present the\nexamination and comparison of two approaches to lens parameter estimation for\nstrong galaxy-galaxy lenses -- Neural Posterior Estimation (NPE) and Bayesian\nNeural Networks (BNNs). We perform inference on 1-, 5-, and 12-parameter lens\nmodels for ground-based imaging data that mimics the Dark Energy Survey (DES).\nWe find that NPE outperforms BNNs, producing posterior distributions that are\nmore accurate, precise, and well-calibrated for most parameters. For the\n12-parameter NPE model, the calibration is consistently within $<$10\\% of\noptimal calibration for all parameters, while the BNN is rarely within 20\\% of\noptimal calibration for any of the parameters. Similarly, residuals for most of\nthe parameters are smaller (by up to an order of magnitude) with the NPE model\nthan the BNN model. This work takes important steps in the systematic\ncomparison of methods for different levels of model complexity."
                },
                "authors": [
                    {
                        "name": "Jason Poh"
                    },
                    {
                        "name": "Ashwin Samudre"
                    },
                    {
                        "name": "Aleksandra Ćiprijanović"
                    },
                    {
                        "name": "Joshua Frieman"
                    },
                    {
                        "name": "Gourav Khullar"
                    },
                    {
                        "name": "Brian D. Nord"
                    }
                ],
                "author_detail": {
                    "name": "Brian D. Nord"
                },
                "author": "Brian D. Nord",
                "arxiv_comment": "Intended for submission to JCAP",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.08524v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.08524v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.IM",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.IM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "astro-ph.CO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.08523v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.08523v1",
                "updated": "2025-01-15T02:25:35Z",
                "updated_parsed": [
                    2025,
                    1,
                    15,
                    2,
                    25,
                    35,
                    2,
                    15,
                    0
                ],
                "published": "2025-01-15T02:25:35Z",
                "published_parsed": [
                    2025,
                    1,
                    15,
                    2,
                    25,
                    35,
                    2,
                    15,
                    0
                ],
                "title": "Doc-Guided Sent2Sent++: A Sent2Sent++ Agent with Doc-Guided memory for\n  Document-level Machine Translation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Doc-Guided Sent2Sent++: A Sent2Sent++ Agent with Doc-Guided memory for\n  Document-level Machine Translation"
                },
                "summary": "The field of artificial intelligence has witnessed significant advancements\nin natural language processing, largely attributed to the capabilities of Large\nLanguage Models (LLMs). These models form the backbone of Agents designed to\naddress long-context dependencies, particularly in Document-level Machine\nTranslation (DocMT). DocMT presents unique challenges, with quality,\nconsistency, and fluency being the key metrics for evaluation. Existing\napproaches, such as Doc2Doc and Doc2Sent, either omit sentences or compromise\nfluency. This paper introduces Doc-Guided Sent2Sent++, an Agent that employs an\nincremental sentence-level forced decoding strategy \\textbf{to ensure every\nsentence is translated while enhancing the fluency of adjacent sentences.} Our\nAgent leverages a Doc-Guided Memory, focusing solely on the summary and its\ntranslation, which we find to be an efficient approach to maintaining\nconsistency. Through extensive testing across multiple languages and domains,\nwe demonstrate that Sent2Sent++ outperforms other methods in terms of quality,\nconsistency, and fluency. The results indicate that, our approach has achieved\nsignificant improvements in metrics such as s-COMET, d-COMET, LTCR-$1_f$, and\ndocument-level perplexity (d-ppl). The contributions of this paper include a\ndetailed analysis of current DocMT research, the introduction of the\nSent2Sent++ decoding method, the Doc-Guided Memory mechanism, and validation of\nits effectiveness across languages and domains.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The field of artificial intelligence has witnessed significant advancements\nin natural language processing, largely attributed to the capabilities of Large\nLanguage Models (LLMs). These models form the backbone of Agents designed to\naddress long-context dependencies, particularly in Document-level Machine\nTranslation (DocMT). DocMT presents unique challenges, with quality,\nconsistency, and fluency being the key metrics for evaluation. Existing\napproaches, such as Doc2Doc and Doc2Sent, either omit sentences or compromise\nfluency. This paper introduces Doc-Guided Sent2Sent++, an Agent that employs an\nincremental sentence-level forced decoding strategy \\textbf{to ensure every\nsentence is translated while enhancing the fluency of adjacent sentences.} Our\nAgent leverages a Doc-Guided Memory, focusing solely on the summary and its\ntranslation, which we find to be an efficient approach to maintaining\nconsistency. Through extensive testing across multiple languages and domains,\nwe demonstrate that Sent2Sent++ outperforms other methods in terms of quality,\nconsistency, and fluency. The results indicate that, our approach has achieved\nsignificant improvements in metrics such as s-COMET, d-COMET, LTCR-$1_f$, and\ndocument-level perplexity (d-ppl). The contributions of this paper include a\ndetailed analysis of current DocMT research, the introduction of the\nSent2Sent++ decoding method, the Doc-Guided Memory mechanism, and validation of\nits effectiveness across languages and domains."
                },
                "authors": [
                    {
                        "name": "Jiaxin Guo"
                    },
                    {
                        "name": "Yuanchang Luo"
                    },
                    {
                        "name": "Daimeng Wei"
                    },
                    {
                        "name": "Ling Zhang"
                    },
                    {
                        "name": "Zongyao Li"
                    },
                    {
                        "name": "Hengchao Shang"
                    },
                    {
                        "name": "Zhiqiang Rao"
                    },
                    {
                        "name": "Shaojun Li"
                    },
                    {
                        "name": "Jinlong Yang"
                    },
                    {
                        "name": "Zhanglin Wu"
                    },
                    {
                        "name": "Hao Yang"
                    }
                ],
                "author_detail": {
                    "name": "Hao Yang"
                },
                "author": "Hao Yang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.08523v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.08523v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.03363v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.03363v2",
                "updated": "2025-01-15T01:51:55Z",
                "updated_parsed": [
                    2025,
                    1,
                    15,
                    1,
                    51,
                    55,
                    2,
                    15,
                    0
                ],
                "published": "2024-09-05T09:10:38Z",
                "published_parsed": [
                    2024,
                    9,
                    5,
                    9,
                    10,
                    38,
                    3,
                    249,
                    0
                ],
                "title": "Con-ReCall: Detecting Pre-training Data in LLMs via Contrastive Decoding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Con-ReCall: Detecting Pre-training Data in LLMs via Contrastive Decoding"
                },
                "summary": "The training data in large language models is key to their success, but it\nalso presents privacy and security risks, as it may contain sensitive\ninformation. Detecting pre-training data is crucial for mitigating these\nconcerns. Existing methods typically analyze target text in isolation or solely\nwith non-member contexts, overlooking potential insights from simultaneously\nconsidering both member and non-member contexts. While previous work suggested\nthat member contexts provide little information due to the minor distributional\nshift they induce, our analysis reveals that these subtle shifts can be\neffectively leveraged when contrasted with non-member contexts. In this paper,\nwe propose Con-ReCall, a novel approach that leverages the asymmetric\ndistributional shifts induced by member and non-member contexts through\ncontrastive decoding, amplifying subtle differences to enhance membership\ninference. Extensive empirical evaluations demonstrate that Con-ReCall achieves\nstate-of-the-art performance on the WikiMIA benchmark and is robust against\nvarious text manipulation techniques.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The training data in large language models is key to their success, but it\nalso presents privacy and security risks, as it may contain sensitive\ninformation. Detecting pre-training data is crucial for mitigating these\nconcerns. Existing methods typically analyze target text in isolation or solely\nwith non-member contexts, overlooking potential insights from simultaneously\nconsidering both member and non-member contexts. While previous work suggested\nthat member contexts provide little information due to the minor distributional\nshift they induce, our analysis reveals that these subtle shifts can be\neffectively leveraged when contrasted with non-member contexts. In this paper,\nwe propose Con-ReCall, a novel approach that leverages the asymmetric\ndistributional shifts induced by member and non-member contexts through\ncontrastive decoding, amplifying subtle differences to enhance membership\ninference. Extensive empirical evaluations demonstrate that Con-ReCall achieves\nstate-of-the-art performance on the WikiMIA benchmark and is robust against\nvarious text manipulation techniques."
                },
                "authors": [
                    {
                        "name": "Cheng Wang"
                    },
                    {
                        "name": "Yiwei Wang"
                    },
                    {
                        "name": "Bryan Hooi"
                    },
                    {
                        "name": "Yujun Cai"
                    },
                    {
                        "name": "Nanyun Peng"
                    },
                    {
                        "name": "Kai-Wei Chang"
                    }
                ],
                "author_detail": {
                    "name": "Kai-Wei Chang"
                },
                "author": "Kai-Wei Chang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.03363v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.03363v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.20166v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.20166v2",
                "updated": "2025-01-15T01:34:46Z",
                "updated_parsed": [
                    2025,
                    1,
                    15,
                    1,
                    34,
                    46,
                    2,
                    15,
                    0
                ],
                "published": "2024-12-28T14:38:16Z",
                "published_parsed": [
                    2024,
                    12,
                    28,
                    14,
                    38,
                    16,
                    5,
                    363,
                    0
                ],
                "title": "LoL-PIM: Long-Context LLM Decoding with Scalable DRAM-PIM System",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LoL-PIM: Long-Context LLM Decoding with Scalable DRAM-PIM System"
                },
                "summary": "The expansion of large language models (LLMs) with hundreds of billions of\nparameters presents significant challenges to computational resources,\nparticularly data movement and memory bandwidth. Long-context LLMs, which\nprocess sequences of tens of thousands of tokens, further increase the demand\non the memory system as the complexity in attention layers and key-value cache\nsizes is proportional to the context length. Processing-in-Memory (PIM)\nmaximizes memory bandwidth by moving compute to the data and can address the\nmemory bandwidth challenges; however, PIM is not necessarily scalable to\naccelerate long-context LLM because of limited per-module memory capacity and\nthe inflexibility of fixed-functional unit PIM architecture and static memory\nmanagement. In this work, we propose LoL-PIM which is a multi-node PIM\narchitecture that accelerates long context LLM through hardware-software\nco-design. In particular, we propose how pipeline parallelism can be exploited\nacross a multi-PIM module while a direct PIM access (DPA) controller (or DMA\nfor PIM) is proposed that enables dynamic PIM memory management and results in\nefficient PIM utilization across a diverse range of context length. We\ndeveloped an MLIR-based compiler for LoL-PIM extending a commercial PIM-based\ncompiler where the software modifications were implemented and evaluated, while\nthe hardware changes were modeled in the simulator. Our evaluations demonstrate\nthat LoL-PIM significantly improves throughput and reduces latency for\nlong-context LLM inference, outperforming both multi-GPU and GPU-PIM systems\n(up to 8.54x and 16.0x speedup, respectively), thereby enabling more efficient\ndeployment of LLMs in real-world applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The expansion of large language models (LLMs) with hundreds of billions of\nparameters presents significant challenges to computational resources,\nparticularly data movement and memory bandwidth. Long-context LLMs, which\nprocess sequences of tens of thousands of tokens, further increase the demand\non the memory system as the complexity in attention layers and key-value cache\nsizes is proportional to the context length. Processing-in-Memory (PIM)\nmaximizes memory bandwidth by moving compute to the data and can address the\nmemory bandwidth challenges; however, PIM is not necessarily scalable to\naccelerate long-context LLM because of limited per-module memory capacity and\nthe inflexibility of fixed-functional unit PIM architecture and static memory\nmanagement. In this work, we propose LoL-PIM which is a multi-node PIM\narchitecture that accelerates long context LLM through hardware-software\nco-design. In particular, we propose how pipeline parallelism can be exploited\nacross a multi-PIM module while a direct PIM access (DPA) controller (or DMA\nfor PIM) is proposed that enables dynamic PIM memory management and results in\nefficient PIM utilization across a diverse range of context length. We\ndeveloped an MLIR-based compiler for LoL-PIM extending a commercial PIM-based\ncompiler where the software modifications were implemented and evaluated, while\nthe hardware changes were modeled in the simulator. Our evaluations demonstrate\nthat LoL-PIM significantly improves throughput and reduces latency for\nlong-context LLM inference, outperforming both multi-GPU and GPU-PIM systems\n(up to 8.54x and 16.0x speedup, respectively), thereby enabling more efficient\ndeployment of LLMs in real-world applications."
                },
                "authors": [
                    {
                        "name": "Hyucksung Kwon"
                    },
                    {
                        "name": "Kyungmo Koo"
                    },
                    {
                        "name": "Janghyeon Kim"
                    },
                    {
                        "name": "Woongkyu Lee"
                    },
                    {
                        "name": "Minjae Lee"
                    },
                    {
                        "name": "Hyungdeok Lee"
                    },
                    {
                        "name": "Yousub Jung"
                    },
                    {
                        "name": "Jaehan Park"
                    },
                    {
                        "name": "Yosub Song"
                    },
                    {
                        "name": "Byeongsu Yang"
                    },
                    {
                        "name": "Haerang Choi"
                    },
                    {
                        "name": "Guhyun Kim"
                    },
                    {
                        "name": "Jongsoon Won"
                    },
                    {
                        "name": "Woojae Shin"
                    },
                    {
                        "name": "Changhyun Kim"
                    },
                    {
                        "name": "Gyeongcheol Shin"
                    },
                    {
                        "name": "Yongkee Kwon"
                    },
                    {
                        "name": "Ilkon Kim"
                    },
                    {
                        "name": "Euicheol Lim"
                    },
                    {
                        "name": "John Kim"
                    },
                    {
                        "name": "Jungwook Choi"
                    }
                ],
                "author_detail": {
                    "name": "Jungwook Choi"
                },
                "author": "Jungwook Choi",
                "arxiv_comment": "15 pages, 12 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.20166v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.20166v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.08501v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.08501v1",
                "updated": "2025-01-15T00:38:13Z",
                "updated_parsed": [
                    2025,
                    1,
                    15,
                    0,
                    38,
                    13,
                    2,
                    15,
                    0
                ],
                "published": "2025-01-15T00:38:13Z",
                "published_parsed": [
                    2025,
                    1,
                    15,
                    0,
                    38,
                    13,
                    2,
                    15,
                    0
                ],
                "title": "Scalable Bayesian Physics-Informed Kolmogorov-Arnold Networks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Scalable Bayesian Physics-Informed Kolmogorov-Arnold Networks"
                },
                "summary": "Uncertainty quantification (UQ) plays a pivotal role in scientific machine\nlearning, especially when surrogate models are used to approximate complex\nsystems. Although multilayer perceptions (MLPs) are commonly employed as\nsurrogates, they often suffer from overfitting due to their large number of\nparameters. Kolmogorov-Arnold networks (KANs) offer an alternative solution\nwith fewer parameters. However, gradient-based inference methods, such as\nHamiltonian Monte Carlo (HMC), may result in computational inefficiency when\napplied to KANs, especially for large-scale datasets, due to the high cost of\nback-propagation.To address these challenges, we propose a novel approach,\ncombining the dropout Tikhonov ensemble Kalman inversion (DTEKI) with Chebyshev\nKANs. This gradient-free method effectively mitigates overfitting and enhances\nnumerical stability. Additionally, we incorporate the active subspace method to\nreduce the parameter-space dimensionality, allowing us to improve the accuracy\nof predictions and obtain more reliable uncertainty estimates.Extensive\nexperiments demonstrate the efficacy of our approach in various test cases,\nincluding scenarios with large datasets and high noise levels. Our results show\nthat the new method achieves comparable or better accuracy, much higher\nefficiency as well as stability compared to HMC, in addition to scalability.\nMoreover, by leveraging the low-dimensional parameter subspace, our method\npreserves prediction accuracy while substantially reducing further the\ncomputational cost.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Uncertainty quantification (UQ) plays a pivotal role in scientific machine\nlearning, especially when surrogate models are used to approximate complex\nsystems. Although multilayer perceptions (MLPs) are commonly employed as\nsurrogates, they often suffer from overfitting due to their large number of\nparameters. Kolmogorov-Arnold networks (KANs) offer an alternative solution\nwith fewer parameters. However, gradient-based inference methods, such as\nHamiltonian Monte Carlo (HMC), may result in computational inefficiency when\napplied to KANs, especially for large-scale datasets, due to the high cost of\nback-propagation.To address these challenges, we propose a novel approach,\ncombining the dropout Tikhonov ensemble Kalman inversion (DTEKI) with Chebyshev\nKANs. This gradient-free method effectively mitigates overfitting and enhances\nnumerical stability. Additionally, we incorporate the active subspace method to\nreduce the parameter-space dimensionality, allowing us to improve the accuracy\nof predictions and obtain more reliable uncertainty estimates.Extensive\nexperiments demonstrate the efficacy of our approach in various test cases,\nincluding scenarios with large datasets and high noise levels. Our results show\nthat the new method achieves comparable or better accuracy, much higher\nefficiency as well as stability compared to HMC, in addition to scalability.\nMoreover, by leveraging the low-dimensional parameter subspace, our method\npreserves prediction accuracy while substantially reducing further the\ncomputational cost."
                },
                "authors": [
                    {
                        "name": "Zhiwei Gao"
                    },
                    {
                        "name": "George Em Karniadakis"
                    }
                ],
                "author_detail": {
                    "name": "George Em Karniadakis"
                },
                "author": "George Em Karniadakis",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.08501v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.08501v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "math.NA",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "math.NA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.20906v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.20906v4",
                "updated": "2025-01-15T00:10:57Z",
                "updated_parsed": [
                    2025,
                    1,
                    15,
                    0,
                    10,
                    57,
                    2,
                    15,
                    0
                ],
                "published": "2024-07-30T15:26:36Z",
                "published_parsed": [
                    2024,
                    7,
                    30,
                    15,
                    26,
                    36,
                    1,
                    212,
                    0
                ],
                "title": "Automated Review Generation Method Based on Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Automated Review Generation Method Based on Large Language Models"
                },
                "summary": "Literature research, vital for scientific work, faces the challenge of\nsurging information volumes exceeding researchers' processing capabilities. We\npresent an automated review generation method based on large language models\n(LLMs) to overcome efficiency bottlenecks and reduce cognitive load. Our\nstatistically validated evaluation framework demonstrates that the generated\nreviews match or exceed manual quality, offering broad applicability across\nresearch fields without requiring users' domain knowledge. Applied to propane\ndehydrogenation (PDH) catalysts, our method swiftly analyzed 343 articles,\naveraging seconds per article per LLM account, producing comprehensive reviews\nspanning 35 topics, with extended analysis of 1041 articles providing insights\ninto catalysts' properties. Through multi-layered quality control, we\neffectively mitigated LLMs' hallucinations, with expert verification confirming\naccuracy and citation integrity while demonstrating hallucination risks reduced\nto below 0.5\\% with 95\\% confidence. Released Windows application enables\none-click review generation, enhancing research productivity and literature\nrecommendation efficiency while setting the stage for broader scientific\nexplorations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Literature research, vital for scientific work, faces the challenge of\nsurging information volumes exceeding researchers' processing capabilities. We\npresent an automated review generation method based on large language models\n(LLMs) to overcome efficiency bottlenecks and reduce cognitive load. Our\nstatistically validated evaluation framework demonstrates that the generated\nreviews match or exceed manual quality, offering broad applicability across\nresearch fields without requiring users' domain knowledge. Applied to propane\ndehydrogenation (PDH) catalysts, our method swiftly analyzed 343 articles,\naveraging seconds per article per LLM account, producing comprehensive reviews\nspanning 35 topics, with extended analysis of 1041 articles providing insights\ninto catalysts' properties. Through multi-layered quality control, we\neffectively mitigated LLMs' hallucinations, with expert verification confirming\naccuracy and citation integrity while demonstrating hallucination risks reduced\nto below 0.5\\% with 95\\% confidence. Released Windows application enables\none-click review generation, enhancing research productivity and literature\nrecommendation efficiency while setting the stage for broader scientific\nexplorations."
                },
                "authors": [
                    {
                        "name": "Shican Wu"
                    },
                    {
                        "name": "Xiao Ma"
                    },
                    {
                        "name": "Dehui Luo"
                    },
                    {
                        "name": "Lulu Li"
                    },
                    {
                        "name": "Xiangcheng Shi"
                    },
                    {
                        "name": "Xin Chang"
                    },
                    {
                        "name": "Xiaoyun Lin"
                    },
                    {
                        "name": "Ran Luo"
                    },
                    {
                        "name": "Chunlei Pei"
                    },
                    {
                        "name": "Changying Du"
                    },
                    {
                        "name": "Zhi-Jian Zhao"
                    },
                    {
                        "name": "Jinlong Gong"
                    }
                ],
                "author_detail": {
                    "name": "Jinlong Gong"
                },
                "author": "Jinlong Gong",
                "arxiv_comment": "21 pages, 5 figures, 1 tables Code:\n  https://github.com/TJU-ECAT-AI/AutomaticReviewGeneration Data:\n  https://github.com/TJU-ECAT-AI/AutomaticReviewGenerationData This research\n  has been invited for a Short Oral presentation at the 18th ICC -\n  International Congress on Catalysis, taking place in Lyon, France from July\n  14-19, 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.20906v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.20906v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "physics.data-an",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.08496v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.08496v1",
                "updated": "2025-01-14T23:59:23Z",
                "updated_parsed": [
                    2025,
                    1,
                    14,
                    23,
                    59,
                    23,
                    1,
                    14,
                    0
                ],
                "published": "2025-01-14T23:59:23Z",
                "published_parsed": [
                    2025,
                    1,
                    14,
                    23,
                    59,
                    23,
                    1,
                    14,
                    0
                ],
                "title": "Quantifying the Importance of Data Alignment in Downstream Model\n  Performance",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Quantifying the Importance of Data Alignment in Downstream Model\n  Performance"
                },
                "summary": "Contrary to the conventional emphasis on dataset size, we explore the role of\ndata alignment -- an often overlooked aspect of data quality -- in training\ncapable Large Language Models (LLMs). To do so, we use the Task2Vec-based\nalignment coefficient, a quantitative measure of the similarity between two\ndatasets, to quantify the impact of alignment between training data and\nevaluation data on downstream performance. In particular, we conduct controlled\n\\textit{interventional} experiments for two settings: 1. the impact of\nincreased alignment coefficients between various pre-training (pt) against\nevaluation datasets, and 2. the impact of increased alignment coefficients\nbetween domain specific fine-tuning (ft) against domain specific evaluation.\nThe domain specific task we explore is Autoformalization -- the machine\ntranslation task between natural language and code for formal verification. In\nboth settings, we find a strong, predictable negative correlation between the\nalignment coefficient of a model's training and evaluation data and the model's\nloss/perplexity on the respective downstream task. These findings suggest a\nre-evaluation of LLM training approaches, demonstrating the relevance of data\nalignment compared to data quantity, especially in specialized downstream tasks\nsuch as Autoformalization.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Contrary to the conventional emphasis on dataset size, we explore the role of\ndata alignment -- an often overlooked aspect of data quality -- in training\ncapable Large Language Models (LLMs). To do so, we use the Task2Vec-based\nalignment coefficient, a quantitative measure of the similarity between two\ndatasets, to quantify the impact of alignment between training data and\nevaluation data on downstream performance. In particular, we conduct controlled\n\\textit{interventional} experiments for two settings: 1. the impact of\nincreased alignment coefficients between various pre-training (pt) against\nevaluation datasets, and 2. the impact of increased alignment coefficients\nbetween domain specific fine-tuning (ft) against domain specific evaluation.\nThe domain specific task we explore is Autoformalization -- the machine\ntranslation task between natural language and code for formal verification. In\nboth settings, we find a strong, predictable negative correlation between the\nalignment coefficient of a model's training and evaluation data and the model's\nloss/perplexity on the respective downstream task. These findings suggest a\nre-evaluation of LLM training approaches, demonstrating the relevance of data\nalignment compared to data quantity, especially in specialized downstream tasks\nsuch as Autoformalization."
                },
                "authors": [
                    {
                        "name": "Krrish Chawla"
                    },
                    {
                        "name": "Aryan Sahai"
                    },
                    {
                        "name": "Mario DePavia"
                    },
                    {
                        "name": "Sudharsan Sundar"
                    },
                    {
                        "name": "Brando Miranda"
                    }
                ],
                "author_detail": {
                    "name": "Brando Miranda"
                },
                "author": "Brando Miranda",
                "arxiv_journal_ref": "ICLR DMLR Data-centric Machine Learning Research (2024)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.08496v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.08496v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.08492v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.08492v1",
                "updated": "2025-01-14T23:39:50Z",
                "updated_parsed": [
                    2025,
                    1,
                    14,
                    23,
                    39,
                    50,
                    1,
                    14,
                    0
                ],
                "published": "2025-01-14T23:39:50Z",
                "published_parsed": [
                    2025,
                    1,
                    14,
                    23,
                    39,
                    50,
                    1,
                    14,
                    0
                ],
                "title": "Bayesian Sphere-on-Sphere Regression with Optimal Transport Maps",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Bayesian Sphere-on-Sphere Regression with Optimal Transport Maps"
                },
                "summary": "Spherical regression, where both covariate and response variables are defined\non the sphere, is a required form of data analysis in several scientific\ndisciplines, and has been the subject of substantial methodological development\nin recent years. Yet, it remains a challenging problem due to the complexities\ninvolved in constructing valid and expressive regression models between\nspherical domains, and the difficulties involved in quantifying uncertainty of\nestimated regression maps. To address these challenges, we propose casting\nspherical regression as a problem of optimal transport within a Bayesian\nframework. Through this approach, we obviate the need for directly\nparameterizing a spherical regression map, and are able to quantify uncertainty\non the inferred map. We derive posterior contraction rates for the proposed\nmodel under two different prior specifications and, in doing so, obtain a\nresult on the quantitative stability of optimal transport maps on the sphere,\none that may be useful in other contexts. The utility of our approach is\ndemonstrated empirically through a simulation study and through its application\nto real data.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Spherical regression, where both covariate and response variables are defined\non the sphere, is a required form of data analysis in several scientific\ndisciplines, and has been the subject of substantial methodological development\nin recent years. Yet, it remains a challenging problem due to the complexities\ninvolved in constructing valid and expressive regression models between\nspherical domains, and the difficulties involved in quantifying uncertainty of\nestimated regression maps. To address these challenges, we propose casting\nspherical regression as a problem of optimal transport within a Bayesian\nframework. Through this approach, we obviate the need for directly\nparameterizing a spherical regression map, and are able to quantify uncertainty\non the inferred map. We derive posterior contraction rates for the proposed\nmodel under two different prior specifications and, in doing so, obtain a\nresult on the quantitative stability of optimal transport maps on the sphere,\none that may be useful in other contexts. The utility of our approach is\ndemonstrated empirically through a simulation study and through its application\nto real data."
                },
                "authors": [
                    {
                        "name": "Tin Lok James Ng"
                    },
                    {
                        "name": "Kwok-Kun Kwong"
                    },
                    {
                        "name": "Jiakun Liu"
                    },
                    {
                        "name": "Andrew Zammit-Mangion"
                    }
                ],
                "author_detail": {
                    "name": "Andrew Zammit-Mangion"
                },
                "author": "Andrew Zammit-Mangion",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.08492v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.08492v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ME",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.ST",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.TH",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2402.11341v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2402.11341v2",
                "updated": "2025-01-14T23:35:27Z",
                "updated_parsed": [
                    2025,
                    1,
                    14,
                    23,
                    35,
                    27,
                    1,
                    14,
                    0
                ],
                "published": "2024-02-17T17:28:57Z",
                "published_parsed": [
                    2024,
                    2,
                    17,
                    17,
                    28,
                    57,
                    5,
                    48,
                    0
                ],
                "title": "Between- and Within-Cluster Spearman Rank Correlations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Between- and Within-Cluster Spearman Rank Correlations"
                },
                "summary": "Clustered data are common in practice. Clustering arises when subjects are\nmeasured repeatedly, or subjects are nested in groups (e.g., households,\nschools). It is often of interest to evaluate the correlation between two\nvariables with clustered data. There are three commonly used Pearson\ncorrelation coefficients (total, between-, and within-cluster), which together\nprovide an enriched perspective of the correlation. However, these Pearson\ncorrelation coefficients are sensitive to extreme values and skewed\ndistributions. They also vary with data transformation, which is arbitrary and\noften difficult to choose, and they are not applicable to ordered categorical\ndata. Current nonparametric correlation measures for clustered data are only\nfor the total correlation. Here we define population parameters for the\nbetween- and within-cluster Spearman rank correlations. The definitions are\nnatural extensions of the Pearson between- and within-cluster correlations to\nthe rank scale. We show that the total Spearman rank correlation approximates a\nlinear combination of the between- and within-cluster Spearman rank\ncorrelations, where the weights are functions of rank intraclass correlations\nof the two random variables. We also discuss the equivalence between the\nwithin-cluster Spearman rank correlation and the covariate-adjusted partial\nSpearman rank correlation. Furthermore, we describe estimation and inference\nfor the three Spearman rank correlations, conduct simulations to evaluate the\nperformance of our estimators, and illustrate their use with data from a\nlongitudinal biomarker study and a clustered randomized trial.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Clustered data are common in practice. Clustering arises when subjects are\nmeasured repeatedly, or subjects are nested in groups (e.g., households,\nschools). It is often of interest to evaluate the correlation between two\nvariables with clustered data. There are three commonly used Pearson\ncorrelation coefficients (total, between-, and within-cluster), which together\nprovide an enriched perspective of the correlation. However, these Pearson\ncorrelation coefficients are sensitive to extreme values and skewed\ndistributions. They also vary with data transformation, which is arbitrary and\noften difficult to choose, and they are not applicable to ordered categorical\ndata. Current nonparametric correlation measures for clustered data are only\nfor the total correlation. Here we define population parameters for the\nbetween- and within-cluster Spearman rank correlations. The definitions are\nnatural extensions of the Pearson between- and within-cluster correlations to\nthe rank scale. We show that the total Spearman rank correlation approximates a\nlinear combination of the between- and within-cluster Spearman rank\ncorrelations, where the weights are functions of rank intraclass correlations\nof the two random variables. We also discuss the equivalence between the\nwithin-cluster Spearman rank correlation and the covariate-adjusted partial\nSpearman rank correlation. Furthermore, we describe estimation and inference\nfor the three Spearman rank correlations, conduct simulations to evaluate the\nperformance of our estimators, and illustrate their use with data from a\nlongitudinal biomarker study and a clustered randomized trial."
                },
                "authors": [
                    {
                        "name": "Shengxin Tu"
                    },
                    {
                        "name": "Chun Li"
                    },
                    {
                        "name": "Bryan E. Shepherd"
                    }
                ],
                "author_detail": {
                    "name": "Bryan E. Shepherd"
                },
                "author": "Bryan E. Shepherd",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2402.11341v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2402.11341v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ME",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.08477v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.08477v1",
                "updated": "2025-01-14T22:40:30Z",
                "updated_parsed": [
                    2025,
                    1,
                    14,
                    22,
                    40,
                    30,
                    1,
                    14,
                    0
                ],
                "published": "2025-01-14T22:40:30Z",
                "published_parsed": [
                    2025,
                    1,
                    14,
                    22,
                    40,
                    30,
                    1,
                    14,
                    0
                ],
                "title": "On the Asymptotics of Importance Weighted Variational Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "On the Asymptotics of Importance Weighted Variational Inference"
                },
                "summary": "For complex latent variable models, the likelihood function is not available\nin closed form. In this context, a popular method to perform parameter\nestimation is Importance Weighted Variational Inference. It essentially\nmaximizes the expectation of the logarithm of an importance sampling estimate\nof the likelihood with respect to both the latent variable model parameters and\nthe importance distribution parameters, the expectation being itself with\nrespect to the importance samples. Despite its great empirical success in\nmachine learning, a theoretical analysis of the limit properties of the\nresulting estimates is still lacking. We fill this gap by establishing\nconsistency when both the Monte Carlo and the observed data sample sizes go to\ninfinity simultaneously. We also establish asymptotic normality and efficiency\nunder additional conditions relating the rate of growth between the Monte Carlo\nand the observed data samples sizes. We distinguish several regimes related to\nthe smoothness of the importance ratio.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "For complex latent variable models, the likelihood function is not available\nin closed form. In this context, a popular method to perform parameter\nestimation is Importance Weighted Variational Inference. It essentially\nmaximizes the expectation of the logarithm of an importance sampling estimate\nof the likelihood with respect to both the latent variable model parameters and\nthe importance distribution parameters, the expectation being itself with\nrespect to the importance samples. Despite its great empirical success in\nmachine learning, a theoretical analysis of the limit properties of the\nresulting estimates is still lacking. We fill this gap by establishing\nconsistency when both the Monte Carlo and the observed data sample sizes go to\ninfinity simultaneously. We also establish asymptotic normality and efficiency\nunder additional conditions relating the rate of growth between the Monte Carlo\nand the observed data samples sizes. We distinguish several regimes related to\nthe smoothness of the importance ratio."
                },
                "authors": [
                    {
                        "name": "Badr-Eddine Cherief-Abdellatif"
                    },
                    {
                        "name": "Randal Douc"
                    },
                    {
                        "name": "Arnaud Doucet"
                    },
                    {
                        "name": "Hugo Marival"
                    }
                ],
                "author_detail": {
                    "name": "Hugo Marival"
                },
                "author": "Hugo Marival",
                "arxiv_comment": "25 pages, 2 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.08477v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.08477v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "math.ST",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "math.ST",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.TH",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "62F12, 62F30, 62B10 (Primary) 60F05, 68T07 (Secondary)",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "G.3",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.08474v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.08474v1",
                "updated": "2025-01-14T22:38:55Z",
                "updated_parsed": [
                    2025,
                    1,
                    14,
                    22,
                    38,
                    55,
                    1,
                    14,
                    0
                ],
                "published": "2025-01-14T22:38:55Z",
                "published_parsed": [
                    2025,
                    1,
                    14,
                    22,
                    38,
                    55,
                    1,
                    14,
                    0
                ],
                "title": "The Theater Stage as Laboratory: Review of Real-Time Comedy LLM Systems\n  for Live Performance",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Theater Stage as Laboratory: Review of Real-Time Comedy LLM Systems\n  for Live Performance"
                },
                "summary": "In this position paper, we review the eclectic recent history of academic and\nartistic works involving computational systems for humor generation, and focus\nspecifically on live performance. We make the case that AI comedy should be\nevaluated in live conditions, in front of audiences sharing either physical or\nonline spaces, and under real-time constraints. We further suggest that\nimprovised comedy is therefore the perfect substrate for deploying and\nassessing computational humor systems. Using examples of successful AI-infused\nshows, we demonstrate that live performance raises three sets of challenges for\ncomputational humor generation: 1) questions around robotic embodiment,\nanthropomorphism and competition between humans and machines, 2) questions\naround comedic timing and the nature of audience interaction, and 3) questions\nabout the human interpretation of seemingly absurd AI-generated humor. We argue\nthat these questions impact the choice of methodologies for evaluating\ncomputational humor, as any such method needs to work around the constraints of\nlive audiences and performance spaces. These interrogations also highlight\ndifferent types of collaborative relationship of human comedians towards AI\ntools.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this position paper, we review the eclectic recent history of academic and\nartistic works involving computational systems for humor generation, and focus\nspecifically on live performance. We make the case that AI comedy should be\nevaluated in live conditions, in front of audiences sharing either physical or\nonline spaces, and under real-time constraints. We further suggest that\nimprovised comedy is therefore the perfect substrate for deploying and\nassessing computational humor systems. Using examples of successful AI-infused\nshows, we demonstrate that live performance raises three sets of challenges for\ncomputational humor generation: 1) questions around robotic embodiment,\nanthropomorphism and competition between humans and machines, 2) questions\naround comedic timing and the nature of audience interaction, and 3) questions\nabout the human interpretation of seemingly absurd AI-generated humor. We argue\nthat these questions impact the choice of methodologies for evaluating\ncomputational humor, as any such method needs to work around the constraints of\nlive audiences and performance spaces. These interrogations also highlight\ndifferent types of collaborative relationship of human comedians towards AI\ntools."
                },
                "authors": [
                    {
                        "name": "Piotr Wojciech Mirowski"
                    },
                    {
                        "name": "Boyd Branch"
                    },
                    {
                        "name": "Kory Wallace Mathewson"
                    }
                ],
                "author_detail": {
                    "name": "Kory Wallace Mathewson"
                },
                "author": "Kory Wallace Mathewson",
                "arxiv_comment": "8 pages, 1st Workshop on Computational Humor (CHum), COLING 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.08474v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.08474v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            }
        ]
    },
    {
        "keyword": "LLM Deployment",
        "arxiv_results": [
            {
                "id": "http://arxiv.org/abs/2501.09012v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.09012v1",
                "updated": "2025-01-15T18:56:22Z",
                "updated_parsed": [
                    2025,
                    1,
                    15,
                    18,
                    56,
                    22,
                    2,
                    15,
                    0
                ],
                "published": "2025-01-15T18:56:22Z",
                "published_parsed": [
                    2025,
                    1,
                    15,
                    18,
                    56,
                    22,
                    2,
                    15,
                    0
                ],
                "title": "Multimodal LLMs Can Reason about Aesthetics in Zero-Shot",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multimodal LLMs Can Reason about Aesthetics in Zero-Shot"
                },
                "summary": "We present the first study on how Multimodal LLMs' (MLLMs) reasoning ability\nshall be elicited to evaluate the aesthetics of artworks. To facilitate this\ninvestigation, we construct MM-StyleBench, a novel high-quality dataset for\nbenchmarking artistic stylization. We then develop a principled method for\nhuman preference modeling and perform a systematic correlation analysis between\nMLLMs' responses and human preference. Our experiments reveal an inherent\nhallucination issue of MLLMs in art evaluation, associated with response\nsubjectivity. ArtCoT is proposed, demonstrating that art-specific task\ndecomposition and the use of concrete language boost MLLMs' reasoning ability\nfor aesthetics. Our findings offer valuable insights into MLLMs for art and can\nbenefit a wide range of downstream applications, such as style transfer and\nartistic image generation. Code available at\nhttps://github.com/songrise/MLLM4Art.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present the first study on how Multimodal LLMs' (MLLMs) reasoning ability\nshall be elicited to evaluate the aesthetics of artworks. To facilitate this\ninvestigation, we construct MM-StyleBench, a novel high-quality dataset for\nbenchmarking artistic stylization. We then develop a principled method for\nhuman preference modeling and perform a systematic correlation analysis between\nMLLMs' responses and human preference. Our experiments reveal an inherent\nhallucination issue of MLLMs in art evaluation, associated with response\nsubjectivity. ArtCoT is proposed, demonstrating that art-specific task\ndecomposition and the use of concrete language boost MLLMs' reasoning ability\nfor aesthetics. Our findings offer valuable insights into MLLMs for art and can\nbenefit a wide range of downstream applications, such as style transfer and\nartistic image generation. Code available at\nhttps://github.com/songrise/MLLM4Art."
                },
                "authors": [
                    {
                        "name": "Ruixiang Jiang"
                    },
                    {
                        "name": "Changwen Chen"
                    }
                ],
                "author_detail": {
                    "name": "Changwen Chen"
                },
                "author": "Changwen Chen",
                "arxiv_comment": "WIP, Homepage https://github.com/songrise/MLLM4Art",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.09012v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.09012v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.09004v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.09004v1",
                "updated": "2025-01-15T18:37:08Z",
                "updated_parsed": [
                    2025,
                    1,
                    15,
                    18,
                    37,
                    8,
                    2,
                    15,
                    0
                ],
                "published": "2025-01-15T18:37:08Z",
                "published_parsed": [
                    2025,
                    1,
                    15,
                    18,
                    37,
                    8,
                    2,
                    15,
                    0
                ],
                "title": "Aegis2.0: A Diverse AI Safety Dataset and Risks Taxonomy for Alignment\n  of LLM Guardrails",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Aegis2.0: A Diverse AI Safety Dataset and Risks Taxonomy for Alignment\n  of LLM Guardrails"
                },
                "summary": "As Large Language Models (LLMs) and generative AI become increasingly\nwidespread, concerns about content safety have grown in parallel. Currently,\nthere is a clear lack of high-quality, human-annotated datasets that address\nthe full spectrum of LLM-related safety risks and are usable for commercial\napplications. To bridge this gap, we propose a comprehensive and adaptable\ntaxonomy for categorizing safety risks, structured into 12 top-level hazard\ncategories with an extension to 9 fine-grained subcategories. This taxonomy is\ndesigned to meet the diverse requirements of downstream users, offering more\ngranular and flexible tools for managing various risk types. Using a hybrid\ndata generation pipeline that combines human annotations with a multi-LLM\n\"jury\" system to assess the safety of responses, we obtain Aegis 2.0, a\ncarefully curated collection of 34,248 samples of human-LLM interactions,\nannotated according to our proposed taxonomy. To validate its effectiveness, we\ndemonstrate that several lightweight models, trained using parameter-efficient\ntechniques on Aegis 2.0, achieve performance competitive with leading safety\nmodels fully fine-tuned on much larger, non-commercial datasets. In addition,\nwe introduce a novel training blend that combines safety with topic following\ndata.This approach enhances the adaptability of guard models, enabling them to\ngeneralize to new risk categories defined during inference. We plan to\nopen-source Aegis 2.0 data and models to the research community to aid in the\nsafety guardrailing of LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As Large Language Models (LLMs) and generative AI become increasingly\nwidespread, concerns about content safety have grown in parallel. Currently,\nthere is a clear lack of high-quality, human-annotated datasets that address\nthe full spectrum of LLM-related safety risks and are usable for commercial\napplications. To bridge this gap, we propose a comprehensive and adaptable\ntaxonomy for categorizing safety risks, structured into 12 top-level hazard\ncategories with an extension to 9 fine-grained subcategories. This taxonomy is\ndesigned to meet the diverse requirements of downstream users, offering more\ngranular and flexible tools for managing various risk types. Using a hybrid\ndata generation pipeline that combines human annotations with a multi-LLM\n\"jury\" system to assess the safety of responses, we obtain Aegis 2.0, a\ncarefully curated collection of 34,248 samples of human-LLM interactions,\nannotated according to our proposed taxonomy. To validate its effectiveness, we\ndemonstrate that several lightweight models, trained using parameter-efficient\ntechniques on Aegis 2.0, achieve performance competitive with leading safety\nmodels fully fine-tuned on much larger, non-commercial datasets. In addition,\nwe introduce a novel training blend that combines safety with topic following\ndata.This approach enhances the adaptability of guard models, enabling them to\ngeneralize to new risk categories defined during inference. We plan to\nopen-source Aegis 2.0 data and models to the research community to aid in the\nsafety guardrailing of LLMs."
                },
                "authors": [
                    {
                        "name": "Shaona Ghosh"
                    },
                    {
                        "name": "Prasoon Varshney"
                    },
                    {
                        "name": "Makesh Narsimhan Sreedhar"
                    },
                    {
                        "name": "Aishwarya Padmakumar"
                    },
                    {
                        "name": "Traian Rebedea"
                    },
                    {
                        "name": "Jibin Rajan Varghese"
                    },
                    {
                        "name": "Christopher Parisien"
                    }
                ],
                "author_detail": {
                    "name": "Christopher Parisien"
                },
                "author": "Christopher Parisien",
                "arxiv_comment": "arXiv admin note: text overlap with arXiv:2404.05993",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.09004v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.09004v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.08102v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.08102v2",
                "updated": "2025-01-15T18:10:00Z",
                "updated_parsed": [
                    2025,
                    1,
                    15,
                    18,
                    10,
                    0,
                    2,
                    15,
                    0
                ],
                "published": "2025-01-14T13:19:47Z",
                "published_parsed": [
                    2025,
                    1,
                    14,
                    13,
                    19,
                    47,
                    1,
                    14,
                    0
                ],
                "title": "Consistency of Responses and Continuations Generated by Large Language\n  Models on Social Media",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Consistency of Responses and Continuations Generated by Large Language\n  Models on Social Media"
                },
                "summary": "Large Language Models (LLMs) demonstrate remarkable capabilities in text\ngeneration, yet their emotional consistency and semantic coherence in social\nmedia contexts remain insufficiently understood. This study investigates how\nLLMs handle emotional content and maintain semantic relationships through\ncontinuation and response tasks using two open-source models: Gemma and Llama.\nBy analyzing climate change discussions from Twitter and Reddit, we examine\nemotional transitions, intensity patterns, and semantic similarity between\nhuman-authored and LLM-generated content. Our findings reveal that while both\nmodels maintain high semantic coherence, they exhibit distinct emotional\npatterns: Gemma shows a tendency toward negative emotion amplification,\nparticularly anger, while maintaining certain positive emotions like optimism.\nLlama demonstrates superior emotional preservation across a broader spectrum of\naffects. Both models systematically generate responses with attenuated\nemotional intensity compared to human-authored content and show a bias toward\npositive emotions in response tasks. Additionally, both models maintain strong\nsemantic similarity with original texts, though performance varies between\ncontinuation and response tasks. These findings provide insights into LLMs'\nemotional and semantic processing capabilities, with implications for their\ndeployment in social media contexts and human-AI interaction design.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) demonstrate remarkable capabilities in text\ngeneration, yet their emotional consistency and semantic coherence in social\nmedia contexts remain insufficiently understood. This study investigates how\nLLMs handle emotional content and maintain semantic relationships through\ncontinuation and response tasks using two open-source models: Gemma and Llama.\nBy analyzing climate change discussions from Twitter and Reddit, we examine\nemotional transitions, intensity patterns, and semantic similarity between\nhuman-authored and LLM-generated content. Our findings reveal that while both\nmodels maintain high semantic coherence, they exhibit distinct emotional\npatterns: Gemma shows a tendency toward negative emotion amplification,\nparticularly anger, while maintaining certain positive emotions like optimism.\nLlama demonstrates superior emotional preservation across a broader spectrum of\naffects. Both models systematically generate responses with attenuated\nemotional intensity compared to human-authored content and show a bias toward\npositive emotions in response tasks. Additionally, both models maintain strong\nsemantic similarity with original texts, though performance varies between\ncontinuation and response tasks. These findings provide insights into LLMs'\nemotional and semantic processing capabilities, with implications for their\ndeployment in social media contexts and human-AI interaction design."
                },
                "authors": [
                    {
                        "name": "Wenlu Fan"
                    },
                    {
                        "name": "Yuqi Zhu"
                    },
                    {
                        "name": "Chenyang Wang"
                    },
                    {
                        "name": "Bin Wang"
                    },
                    {
                        "name": "Wentao Xu"
                    }
                ],
                "author_detail": {
                    "name": "Wentao Xu"
                },
                "author": "Wentao Xu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.08102v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.08102v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.08977v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.08977v1",
                "updated": "2025-01-15T17:47:57Z",
                "updated_parsed": [
                    2025,
                    1,
                    15,
                    17,
                    47,
                    57,
                    2,
                    15,
                    0
                ],
                "published": "2025-01-15T17:47:57Z",
                "published_parsed": [
                    2025,
                    1,
                    15,
                    17,
                    47,
                    57,
                    2,
                    15,
                    0
                ],
                "title": "Development and Validation of the Provider Documentation Summarization\n  Quality Instrument for Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Development and Validation of the Provider Documentation Summarization\n  Quality Instrument for Large Language Models"
                },
                "summary": "As Large Language Models (LLMs) are integrated into electronic health record\n(EHR) workflows, validated instruments are essential to evaluate their\nperformance before implementation. Existing instruments for provider\ndocumentation quality are often unsuitable for the complexities of\nLLM-generated text and lack validation on real-world data. The Provider\nDocumentation Summarization Quality Instrument (PDSQI-9) was developed to\nevaluate LLM-generated clinical summaries. Multi-document summaries were\ngenerated from real-world EHR data across multiple specialties using several\nLLMs (GPT-4o, Mixtral 8x7b, and Llama 3-8b). Validation included Pearson\ncorrelation for substantive validity, factor analysis and Cronbach's alpha for\nstructural validity, inter-rater reliability (ICC and Krippendorff's alpha) for\ngeneralizability, a semi-Delphi process for content validity, and comparisons\nof high- versus low-quality summaries for discriminant validity. Seven\nphysician raters evaluated 779 summaries and answered 8,329 questions,\nachieving over 80% power for inter-rater reliability. The PDSQI-9 demonstrated\nstrong internal consistency (Cronbach's alpha = 0.879; 95% CI: 0.867-0.891) and\nhigh inter-rater reliability (ICC = 0.867; 95% CI: 0.867-0.868), supporting\nstructural validity and generalizability. Factor analysis identified a 4-factor\nmodel explaining 58% of the variance, representing organization, clarity,\naccuracy, and utility. Substantive validity was supported by correlations\nbetween note length and scores for Succinct (rho = -0.200, p = 0.029) and\nOrganized (rho = -0.190, p = 0.037). Discriminant validity distinguished high-\nfrom low-quality summaries (p < 0.001). The PDSQI-9 demonstrates robust\nconstruct validity, supporting its use in clinical practice to evaluate\nLLM-generated summaries and facilitate safer integration of LLMs into\nhealthcare workflows.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As Large Language Models (LLMs) are integrated into electronic health record\n(EHR) workflows, validated instruments are essential to evaluate their\nperformance before implementation. Existing instruments for provider\ndocumentation quality are often unsuitable for the complexities of\nLLM-generated text and lack validation on real-world data. The Provider\nDocumentation Summarization Quality Instrument (PDSQI-9) was developed to\nevaluate LLM-generated clinical summaries. Multi-document summaries were\ngenerated from real-world EHR data across multiple specialties using several\nLLMs (GPT-4o, Mixtral 8x7b, and Llama 3-8b). Validation included Pearson\ncorrelation for substantive validity, factor analysis and Cronbach's alpha for\nstructural validity, inter-rater reliability (ICC and Krippendorff's alpha) for\ngeneralizability, a semi-Delphi process for content validity, and comparisons\nof high- versus low-quality summaries for discriminant validity. Seven\nphysician raters evaluated 779 summaries and answered 8,329 questions,\nachieving over 80% power for inter-rater reliability. The PDSQI-9 demonstrated\nstrong internal consistency (Cronbach's alpha = 0.879; 95% CI: 0.867-0.891) and\nhigh inter-rater reliability (ICC = 0.867; 95% CI: 0.867-0.868), supporting\nstructural validity and generalizability. Factor analysis identified a 4-factor\nmodel explaining 58% of the variance, representing organization, clarity,\naccuracy, and utility. Substantive validity was supported by correlations\nbetween note length and scores for Succinct (rho = -0.200, p = 0.029) and\nOrganized (rho = -0.190, p = 0.037). Discriminant validity distinguished high-\nfrom low-quality summaries (p < 0.001). The PDSQI-9 demonstrates robust\nconstruct validity, supporting its use in clinical practice to evaluate\nLLM-generated summaries and facilitate safer integration of LLMs into\nhealthcare workflows."
                },
                "authors": [
                    {
                        "name": "Emma Croxford"
                    },
                    {
                        "name": "Yanjun Gao"
                    },
                    {
                        "name": "Nicholas Pellegrino"
                    },
                    {
                        "name": "Karen K. Wong"
                    },
                    {
                        "name": "Graham Wills"
                    },
                    {
                        "name": "Elliot First"
                    },
                    {
                        "name": "Miranda Schnier"
                    },
                    {
                        "name": "Kyle Burton"
                    },
                    {
                        "name": "Cris G. Ebby"
                    },
                    {
                        "name": "Jillian Gorskic"
                    },
                    {
                        "name": "Matthew Kalscheur"
                    },
                    {
                        "name": "Samy Khalil"
                    },
                    {
                        "name": "Marie Pisani"
                    },
                    {
                        "name": "Tyler Rubeor"
                    },
                    {
                        "name": "Peter Stetson"
                    },
                    {
                        "name": "Frank Liao"
                    },
                    {
                        "name": "Cherodeep Goswami"
                    },
                    {
                        "name": "Brian Patterson"
                    },
                    {
                        "name": "Majid Afshar"
                    }
                ],
                "author_detail": {
                    "name": "Majid Afshar"
                },
                "author": "Majid Afshar",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.08977v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.08977v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.08974v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.08974v1",
                "updated": "2025-01-15T17:36:56Z",
                "updated_parsed": [
                    2025,
                    1,
                    15,
                    17,
                    36,
                    56,
                    2,
                    15,
                    0
                ],
                "published": "2025-01-15T17:36:56Z",
                "published_parsed": [
                    2025,
                    1,
                    15,
                    17,
                    36,
                    56,
                    2,
                    15,
                    0
                ],
                "title": "Learning to Extract Cross-Domain Aspects and Understanding Sentiments\n  Using Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Learning to Extract Cross-Domain Aspects and Understanding Sentiments\n  Using Large Language Models"
                },
                "summary": "Aspect-based sentiment analysis (ASBA) is a refined approach to sentiment\nanalysis that aims to extract and classify sentiments based on specific aspects\nor features of a product, service, or entity. Unlike traditional sentiment\nanalysis, which assigns a general sentiment score to entire reviews or texts,\nABSA focuses on breaking down the text into individual components or aspects\n(e.g., quality, price, service) and evaluating the sentiment towards each. This\nallows for a more granular level of understanding of customer opinions,\nenabling businesses to pinpoint specific areas of strength and improvement. The\nprocess involves several key steps, including aspect extraction, sentiment\nclassification, and aspect-level sentiment aggregation for a review paragraph\nor any other form that the users have provided. ABSA has significant\napplications in areas such as product reviews, social media monitoring,\ncustomer feedback analysis, and market research. By leveraging techniques from\nnatural language processing (NLP) and machine learning, ABSA facilitates the\nextraction of valuable insights, enabling companies to make data-driven\ndecisions that enhance customer satisfaction and optimize offerings. As ABSA\nevolves, it holds the potential to greatly improve personalized customer\nexperiences by providing a deeper understanding of sentiment across various\nproduct aspects. In this work, we have analyzed the strength of LLMs for a\ncomplete cross-domain aspect-based sentiment analysis with the aim of defining\nthe framework for certain products and using it for other similar situations.\nWe argue that it is possible to that at an effectiveness of 92\\% accuracy for\nthe Aspect Based Sentiment Analysis dataset of SemEval-2015 Task 12.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Aspect-based sentiment analysis (ASBA) is a refined approach to sentiment\nanalysis that aims to extract and classify sentiments based on specific aspects\nor features of a product, service, or entity. Unlike traditional sentiment\nanalysis, which assigns a general sentiment score to entire reviews or texts,\nABSA focuses on breaking down the text into individual components or aspects\n(e.g., quality, price, service) and evaluating the sentiment towards each. This\nallows for a more granular level of understanding of customer opinions,\nenabling businesses to pinpoint specific areas of strength and improvement. The\nprocess involves several key steps, including aspect extraction, sentiment\nclassification, and aspect-level sentiment aggregation for a review paragraph\nor any other form that the users have provided. ABSA has significant\napplications in areas such as product reviews, social media monitoring,\ncustomer feedback analysis, and market research. By leveraging techniques from\nnatural language processing (NLP) and machine learning, ABSA facilitates the\nextraction of valuable insights, enabling companies to make data-driven\ndecisions that enhance customer satisfaction and optimize offerings. As ABSA\nevolves, it holds the potential to greatly improve personalized customer\nexperiences by providing a deeper understanding of sentiment across various\nproduct aspects. In this work, we have analyzed the strength of LLMs for a\ncomplete cross-domain aspect-based sentiment analysis with the aim of defining\nthe framework for certain products and using it for other similar situations.\nWe argue that it is possible to that at an effectiveness of 92\\% accuracy for\nthe Aspect Based Sentiment Analysis dataset of SemEval-2015 Task 12."
                },
                "authors": [
                    {
                        "name": "Karukriti Kaushik Ghosh"
                    },
                    {
                        "name": "Chiranjib Sur"
                    }
                ],
                "author_detail": {
                    "name": "Chiranjib Sur"
                },
                "author": "Chiranjib Sur",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.08974v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.08974v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.05541v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.05541v2",
                "updated": "2025-01-15T17:23:23Z",
                "updated_parsed": [
                    2025,
                    1,
                    15,
                    17,
                    23,
                    23,
                    2,
                    15,
                    0
                ],
                "published": "2025-01-09T19:27:28Z",
                "published_parsed": [
                    2025,
                    1,
                    9,
                    19,
                    27,
                    28,
                    3,
                    9,
                    0
                ],
                "title": "Customizable LLM-Powered Chatbot for Behavioral Science Research",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Customizable LLM-Powered Chatbot for Behavioral Science Research"
                },
                "summary": "The rapid advancement of Artificial Intelligence has resulted in the advent\nof Large Language Models (LLMs) with the capacity to produce text that closely\nresembles human communication. These models have been seamlessly integrated\ninto diverse applications, enabling interactive and responsive communication\nacross multiple platforms. The potential utility of chatbots transcends these\ntraditional applications, particularly in research contexts, wherein they can\noffer valuable insights and facilitate the design of innovative experiments. In\nthis study, we present a Customizable LLM-Powered Chatbot (CLPC), a web-based\nchatbot system designed to assist in behavioral science research. The system is\nmeticulously designed to function as an experimental instrument rather than a\nconventional chatbot, necessitating users to input a username and experiment\ncode upon access. This setup facilitates precise data cross-referencing,\nthereby augmenting the integrity and applicability of the data collected for\nresearch purposes. It can be easily expanded to accommodate new basic events as\nneeded; and it allows researchers to integrate their own logging events without\nthe necessity of implementing a separate logging mechanism. It is worth noting\nthat our system was built to assist primarily behavioral science research but\nis not limited to it, it can easily be adapted to assist information retrieval\nresearch or interacting with chat bot agents in general.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rapid advancement of Artificial Intelligence has resulted in the advent\nof Large Language Models (LLMs) with the capacity to produce text that closely\nresembles human communication. These models have been seamlessly integrated\ninto diverse applications, enabling interactive and responsive communication\nacross multiple platforms. The potential utility of chatbots transcends these\ntraditional applications, particularly in research contexts, wherein they can\noffer valuable insights and facilitate the design of innovative experiments. In\nthis study, we present a Customizable LLM-Powered Chatbot (CLPC), a web-based\nchatbot system designed to assist in behavioral science research. The system is\nmeticulously designed to function as an experimental instrument rather than a\nconventional chatbot, necessitating users to input a username and experiment\ncode upon access. This setup facilitates precise data cross-referencing,\nthereby augmenting the integrity and applicability of the data collected for\nresearch purposes. It can be easily expanded to accommodate new basic events as\nneeded; and it allows researchers to integrate their own logging events without\nthe necessity of implementing a separate logging mechanism. It is worth noting\nthat our system was built to assist primarily behavioral science research but\nis not limited to it, it can easily be adapted to assist information retrieval\nresearch or interacting with chat bot agents in general."
                },
                "authors": [
                    {
                        "name": "Zenon Lamprou"
                    },
                    {
                        "name": "Yashar Moshfeghi"
                    }
                ],
                "author_detail": {
                    "name": "Yashar Moshfeghi"
                },
                "author": "Yashar Moshfeghi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.05541v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.05541v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.08951v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.08951v1",
                "updated": "2025-01-15T16:56:26Z",
                "updated_parsed": [
                    2025,
                    1,
                    15,
                    16,
                    56,
                    26,
                    2,
                    15,
                    0
                ],
                "published": "2025-01-15T16:56:26Z",
                "published_parsed": [
                    2025,
                    1,
                    15,
                    16,
                    56,
                    26,
                    2,
                    15,
                    0
                ],
                "title": "Analyzing the Ethical Logic of Six Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Analyzing the Ethical Logic of Six Large Language Models"
                },
                "summary": "This study examines the ethical reasoning of six prominent generative large\nlanguage models: OpenAI GPT-4o, Meta LLaMA 3.1, Perplexity, Anthropic Claude\n3.5 Sonnet, Google Gemini, and Mistral 7B. The research explores how these\nmodels articulate and apply ethical logic, particularly in response to moral\ndilemmas such as the Trolley Problem, and Heinz Dilemma. Departing from\ntraditional alignment studies, the study adopts an explainability-transparency\nframework, prompting models to explain their ethical reasoning. This approach\nis analyzed through three established ethical typologies: the\nconsequentialist-deontological analytic, Moral Foundations Theory, and the\nKohlberg Stages of Moral Development Model. Findings reveal that LLMs exhibit\nlargely convergent ethical logic, marked by a rationalist, consequentialist\nemphasis, with decisions often prioritizing harm minimization and fairness.\nDespite similarities in pre-training and model architecture, a mixture of\nnuanced and significant differences in ethical reasoning emerge across models,\nreflecting variations in fine-tuning and post-training processes. The models\nconsistently display erudition, caution, and self-awareness, presenting ethical\nreasoning akin to a graduate-level discourse in moral philosophy. In striking\nuniformity these systems all describe their ethical reasoning as more\nsophisticated than what is characteristic of typical human moral logic.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This study examines the ethical reasoning of six prominent generative large\nlanguage models: OpenAI GPT-4o, Meta LLaMA 3.1, Perplexity, Anthropic Claude\n3.5 Sonnet, Google Gemini, and Mistral 7B. The research explores how these\nmodels articulate and apply ethical logic, particularly in response to moral\ndilemmas such as the Trolley Problem, and Heinz Dilemma. Departing from\ntraditional alignment studies, the study adopts an explainability-transparency\nframework, prompting models to explain their ethical reasoning. This approach\nis analyzed through three established ethical typologies: the\nconsequentialist-deontological analytic, Moral Foundations Theory, and the\nKohlberg Stages of Moral Development Model. Findings reveal that LLMs exhibit\nlargely convergent ethical logic, marked by a rationalist, consequentialist\nemphasis, with decisions often prioritizing harm minimization and fairness.\nDespite similarities in pre-training and model architecture, a mixture of\nnuanced and significant differences in ethical reasoning emerge across models,\nreflecting variations in fine-tuning and post-training processes. The models\nconsistently display erudition, caution, and self-awareness, presenting ethical\nreasoning akin to a graduate-level discourse in moral philosophy. In striking\nuniformity these systems all describe their ethical reasoning as more\nsophisticated than what is characteristic of typical human moral logic."
                },
                "authors": [
                    {
                        "name": "W. Russell Neuman"
                    },
                    {
                        "name": "Chad Coleman"
                    },
                    {
                        "name": "Manan Shah"
                    }
                ],
                "author_detail": {
                    "name": "Manan Shah"
                },
                "author": "Manan Shah",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.08951v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.08951v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.08944v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.08944v1",
                "updated": "2025-01-15T16:46:32Z",
                "updated_parsed": [
                    2025,
                    1,
                    15,
                    16,
                    46,
                    32,
                    2,
                    15,
                    0
                ],
                "published": "2025-01-15T16:46:32Z",
                "published_parsed": [
                    2025,
                    1,
                    15,
                    16,
                    46,
                    32,
                    2,
                    15,
                    0
                ],
                "title": "Physical AI Agents: Integrating Cognitive Intelligence with Real-World\n  Action",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Physical AI Agents: Integrating Cognitive Intelligence with Real-World\n  Action"
                },
                "summary": "Vertical AI Agents are revolutionizing industries by delivering\ndomain-specific intelligence and tailored solutions. However, many sectors,\nsuch as manufacturing, healthcare, and logistics, demand AI systems capable of\nextending their intelligence into the physical world, interacting directly with\nobjects, environments, and dynamic conditions. This need has led to the\nemergence of Physical AI Agents--systems that integrate cognitive reasoning,\npowered by specialized LLMs, with precise physical actions to perform\nreal-world tasks.\n  This work introduces Physical AI Agents as an evolution of shared principles\nwith Vertical AI Agents, tailored for physical interaction. We propose a\nmodular architecture with three core blocks--perception, cognition, and\nactuation--offering a scalable framework for diverse industries. Additionally,\nwe present the Physical Retrieval Augmented Generation (Ph-RAG) design pattern,\nwhich connects physical intelligence to industry-specific LLMs for real-time\ndecision-making and reporting informed by physical context.\n  Through case studies, we demonstrate how Physical AI Agents and the Ph-RAG\nframework are transforming industries like autonomous vehicles, warehouse\nrobotics, healthcare, and manufacturing, offering businesses a pathway to\nintegrate embodied AI for operational efficiency and innovation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Vertical AI Agents are revolutionizing industries by delivering\ndomain-specific intelligence and tailored solutions. However, many sectors,\nsuch as manufacturing, healthcare, and logistics, demand AI systems capable of\nextending their intelligence into the physical world, interacting directly with\nobjects, environments, and dynamic conditions. This need has led to the\nemergence of Physical AI Agents--systems that integrate cognitive reasoning,\npowered by specialized LLMs, with precise physical actions to perform\nreal-world tasks.\n  This work introduces Physical AI Agents as an evolution of shared principles\nwith Vertical AI Agents, tailored for physical interaction. We propose a\nmodular architecture with three core blocks--perception, cognition, and\nactuation--offering a scalable framework for diverse industries. Additionally,\nwe present the Physical Retrieval Augmented Generation (Ph-RAG) design pattern,\nwhich connects physical intelligence to industry-specific LLMs for real-time\ndecision-making and reporting informed by physical context.\n  Through case studies, we demonstrate how Physical AI Agents and the Ph-RAG\nframework are transforming industries like autonomous vehicles, warehouse\nrobotics, healthcare, and manufacturing, offering businesses a pathway to\nintegrate embodied AI for operational efficiency and innovation."
                },
                "authors": [
                    {
                        "name": "Fouad Bousetouane"
                    }
                ],
                "author_detail": {
                    "name": "Fouad Bousetouane"
                },
                "author": "Fouad Bousetouane",
                "arxiv_comment": "27 pages, 3 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.08944v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.08944v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.MA",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.08925v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.08925v1",
                "updated": "2025-01-15T16:30:29Z",
                "updated_parsed": [
                    2025,
                    1,
                    15,
                    16,
                    30,
                    29,
                    2,
                    15,
                    0
                ],
                "published": "2025-01-15T16:30:29Z",
                "published_parsed": [
                    2025,
                    1,
                    15,
                    16,
                    30,
                    29,
                    2,
                    15,
                    0
                ],
                "title": "Disentangling Exploration of Large Language Models by Optimal\n  Exploitation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Disentangling Exploration of Large Language Models by Optimal\n  Exploitation"
                },
                "summary": "Exploration is a crucial skill for self-improvement and open-ended\nproblem-solving. However, it remains uncertain whether large language models\ncan effectively explore the state-space. Existing evaluations predominantly\nfocus on the trade-off between exploration and exploitation, often assessed in\nmulti-armed bandit problems. In contrast, this work isolates exploration as the\nsole objective, tasking the agent with delivering information that enhances\nfuture returns. For the evaluation, we propose to decompose missing rewards\ninto exploration and exploitation components by measuring the optimal\nachievable return for the states already explored. Our experiments with various\nLLMs reveal that most models struggle to sufficiently explore the state-space\nand that weak exploration is insufficient. We observe a positive correlation\nbetween model size and exploration performance, with larger models\ndemonstrating superior capabilities. Furthermore, we show that our\ndecomposition provides insights into differences in behaviors driven by agent\ninstructions during prompt engineering, offering a valuable tool for refining\nLLM performance in exploratory tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Exploration is a crucial skill for self-improvement and open-ended\nproblem-solving. However, it remains uncertain whether large language models\ncan effectively explore the state-space. Existing evaluations predominantly\nfocus on the trade-off between exploration and exploitation, often assessed in\nmulti-armed bandit problems. In contrast, this work isolates exploration as the\nsole objective, tasking the agent with delivering information that enhances\nfuture returns. For the evaluation, we propose to decompose missing rewards\ninto exploration and exploitation components by measuring the optimal\nachievable return for the states already explored. Our experiments with various\nLLMs reveal that most models struggle to sufficiently explore the state-space\nand that weak exploration is insufficient. We observe a positive correlation\nbetween model size and exploration performance, with larger models\ndemonstrating superior capabilities. Furthermore, we show that our\ndecomposition provides insights into differences in behaviors driven by agent\ninstructions during prompt engineering, offering a valuable tool for refining\nLLM performance in exploratory tasks."
                },
                "authors": [
                    {
                        "name": "Tim Grams"
                    },
                    {
                        "name": "Patrick Betz"
                    },
                    {
                        "name": "Christian Bartelt"
                    }
                ],
                "author_detail": {
                    "name": "Christian Bartelt"
                },
                "author": "Christian Bartelt",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.08925v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.08925v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.08913v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.08913v1",
                "updated": "2025-01-15T16:21:09Z",
                "updated_parsed": [
                    2025,
                    1,
                    15,
                    16,
                    21,
                    9,
                    2,
                    15,
                    0
                ],
                "published": "2025-01-15T16:21:09Z",
                "published_parsed": [
                    2025,
                    1,
                    15,
                    16,
                    21,
                    9,
                    2,
                    15,
                    0
                ],
                "title": "GenAI Content Detection Task 3: Cross-Domain Machine-Generated Text\n  Detection Challenge",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GenAI Content Detection Task 3: Cross-Domain Machine-Generated Text\n  Detection Challenge"
                },
                "summary": "Recently there have been many shared tasks targeting the detection of\ngenerated text from Large Language Models (LLMs). However, these shared tasks\ntend to focus either on cases where text is limited to one particular domain or\ncases where text can be from many domains, some of which may not be seen during\ntest time. In this shared task, using the newly released RAID benchmark, we aim\nto answer whether or not models can detect generated text from a large, yet\nfixed, number of domains and LLMs, all of which are seen during training. Over\nthe course of three months, our task was attempted by 9 teams with 23 detector\nsubmissions. We find that multiple participants were able to obtain accuracies\nof over 99% on machine-generated text from RAID while maintaining a 5% False\nPositive Rate -- suggesting that detectors are able to robustly detect text\nfrom many domains and models simultaneously. We discuss potential\ninterpretations of this result and provide directions for future research.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recently there have been many shared tasks targeting the detection of\ngenerated text from Large Language Models (LLMs). However, these shared tasks\ntend to focus either on cases where text is limited to one particular domain or\ncases where text can be from many domains, some of which may not be seen during\ntest time. In this shared task, using the newly released RAID benchmark, we aim\nto answer whether or not models can detect generated text from a large, yet\nfixed, number of domains and LLMs, all of which are seen during training. Over\nthe course of three months, our task was attempted by 9 teams with 23 detector\nsubmissions. We find that multiple participants were able to obtain accuracies\nof over 99% on machine-generated text from RAID while maintaining a 5% False\nPositive Rate -- suggesting that detectors are able to robustly detect text\nfrom many domains and models simultaneously. We discuss potential\ninterpretations of this result and provide directions for future research."
                },
                "authors": [
                    {
                        "name": "Liam Dugan"
                    },
                    {
                        "name": "Andrew Zhu"
                    },
                    {
                        "name": "Firoj Alam"
                    },
                    {
                        "name": "Preslav Nakov"
                    },
                    {
                        "name": "Marianna Apidianaki"
                    },
                    {
                        "name": "Chris Callison-Burch"
                    }
                ],
                "author_detail": {
                    "name": "Chris Callison-Burch"
                },
                "author": "Chris Callison-Burch",
                "arxiv_comment": "COLING 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.08913v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.08913v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.7",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.08897v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.08897v1",
                "updated": "2025-01-15T16:06:10Z",
                "updated_parsed": [
                    2025,
                    1,
                    15,
                    16,
                    6,
                    10,
                    2,
                    15,
                    0
                ],
                "published": "2025-01-15T16:06:10Z",
                "published_parsed": [
                    2025,
                    1,
                    15,
                    16,
                    6,
                    10,
                    2,
                    15,
                    0
                ],
                "title": "Leveraging Large Language Models as Knowledge-Driven Agents for Reliable\n  Retrosynthesis Planning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Leveraging Large Language Models as Knowledge-Driven Agents for Reliable\n  Retrosynthesis Planning"
                },
                "summary": "Identifying reliable synthesis pathways in materials chemistry is a complex\ntask, particularly in polymer science, due to the intricate and often\nnon-unique nomenclature of macromolecules. To address this challenge, we\npropose an agent system that integrates large language models (LLMs) and\nknowledge graphs (KGs). By leveraging LLMs' powerful capabilities for\nextracting and recognizing chemical substance names, and storing the extracted\ndata in a structured knowledge graph, our system fully automates the retrieval\nof relevant literatures, extraction of reaction data, database querying,\nconstruction of retrosynthetic pathway trees, further expansion through the\nretrieval of additional literature and recommendation of optimal reaction\npathways. A novel Multi-branched Reaction Pathway Search (MBRPS) algorithm\nenables the exploration of all pathways, with a particular focus on\nmulti-branched ones, helping LLMs overcome weak reasoning in multi-branched\npaths. This work represents the first attempt to develop a fully automated\nretrosynthesis planning agent tailored specially for macromolecules powered by\nLLMs. Applied to polyimide synthesis, our new approach constructs a\nretrosynthetic pathway tree with hundreds of pathways and recommends optimized\nroutes, including both known and novel pathways, demonstrating its\neffectiveness and potential for broader applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Identifying reliable synthesis pathways in materials chemistry is a complex\ntask, particularly in polymer science, due to the intricate and often\nnon-unique nomenclature of macromolecules. To address this challenge, we\npropose an agent system that integrates large language models (LLMs) and\nknowledge graphs (KGs). By leveraging LLMs' powerful capabilities for\nextracting and recognizing chemical substance names, and storing the extracted\ndata in a structured knowledge graph, our system fully automates the retrieval\nof relevant literatures, extraction of reaction data, database querying,\nconstruction of retrosynthetic pathway trees, further expansion through the\nretrieval of additional literature and recommendation of optimal reaction\npathways. A novel Multi-branched Reaction Pathway Search (MBRPS) algorithm\nenables the exploration of all pathways, with a particular focus on\nmulti-branched ones, helping LLMs overcome weak reasoning in multi-branched\npaths. This work represents the first attempt to develop a fully automated\nretrosynthesis planning agent tailored specially for macromolecules powered by\nLLMs. Applied to polyimide synthesis, our new approach constructs a\nretrosynthetic pathway tree with hundreds of pathways and recommends optimized\nroutes, including both known and novel pathways, demonstrating its\neffectiveness and potential for broader applications."
                },
                "authors": [
                    {
                        "name": "Qinyu Ma"
                    },
                    {
                        "name": "Yuhao Zhou"
                    },
                    {
                        "name": "Jianfeng Li"
                    }
                ],
                "author_detail": {
                    "name": "Jianfeng Li"
                },
                "author": "Jianfeng Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.08897v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.08897v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.08847v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.08847v1",
                "updated": "2025-01-15T14:59:00Z",
                "updated_parsed": [
                    2025,
                    1,
                    15,
                    14,
                    59,
                    0,
                    2,
                    15,
                    0
                ],
                "published": "2025-01-15T14:59:00Z",
                "published_parsed": [
                    2025,
                    1,
                    15,
                    14,
                    59,
                    0,
                    2,
                    15,
                    0
                ],
                "title": "Automatic tuning of communication protocols for vehicular ad hoc\n  networks using metaheuristics",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Automatic tuning of communication protocols for vehicular ad hoc\n  networks using metaheuristics"
                },
                "summary": "The emerging field of vehicular ad hoc networks (VANETs) deals with a set of\ncommunicating vehicles which are able to spontaneously interconnect without any\npre-existing infrastructure. In such kind of networks, it is crucial to make an\noptimal configuration of the communication protocols previously to the final\nnetwork deployment. This way, a human designer can obtain an optimal QoS of the\nnetwork beforehand. The problem we consider in this work lies in configuring\nthe File Transfer protocol Configuration (FTC) with the aim of optimizing the\ntransmission time, the number of lost packets, and the amount of data\ntransferred in realistic VANET scenarios. We face the FTC with five\nrepresentative state-of-the-art optimization techniques and compare their\nperformance. These algorithms are: Particle Swarm Optimization (PSO),\nDifferential Evolution (DE), Genetic Algorithm (GA), Evolutionary Strategy\n(ES), and Simulated Annealing (SA). For our tests, two typical environment\ninstances of VANETs for Urban and Highway scenarios have been defined. The\nexperiments using ns- 2 (a well-known realistic VANET simulator) reveal that\nPSO outperforms all the compared algorithms for both studied VANET instances.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The emerging field of vehicular ad hoc networks (VANETs) deals with a set of\ncommunicating vehicles which are able to spontaneously interconnect without any\npre-existing infrastructure. In such kind of networks, it is crucial to make an\noptimal configuration of the communication protocols previously to the final\nnetwork deployment. This way, a human designer can obtain an optimal QoS of the\nnetwork beforehand. The problem we consider in this work lies in configuring\nthe File Transfer protocol Configuration (FTC) with the aim of optimizing the\ntransmission time, the number of lost packets, and the amount of data\ntransferred in realistic VANET scenarios. We face the FTC with five\nrepresentative state-of-the-art optimization techniques and compare their\nperformance. These algorithms are: Particle Swarm Optimization (PSO),\nDifferential Evolution (DE), Genetic Algorithm (GA), Evolutionary Strategy\n(ES), and Simulated Annealing (SA). For our tests, two typical environment\ninstances of VANETs for Urban and Highway scenarios have been defined. The\nexperiments using ns- 2 (a well-known realistic VANET simulator) reveal that\nPSO outperforms all the compared algorithms for both studied VANET instances."
                },
                "authors": [
                    {
                        "name": "José García-Nieto"
                    },
                    {
                        "name": "Jamal Toutouh"
                    },
                    {
                        "name": "Enrique Alba"
                    }
                ],
                "author_detail": {
                    "name": "Enrique Alba"
                },
                "author": "Enrique Alba",
                "arxiv_doi": "10.1016/j.engappai.2010.01.012",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1016/j.engappai.2010.01.012",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2501.08847v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.08847v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_journal_ref": "Engineering Applications of Artificial Intelligence, 23(5),\n  795-805 (2010).",
                "arxiv_primary_category": {
                    "term": "cs.NE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.08841v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.08841v1",
                "updated": "2025-01-15T14:52:20Z",
                "updated_parsed": [
                    2025,
                    1,
                    15,
                    14,
                    52,
                    20,
                    2,
                    15,
                    0
                ],
                "published": "2025-01-15T14:52:20Z",
                "published_parsed": [
                    2025,
                    1,
                    15,
                    14,
                    52,
                    20,
                    2,
                    15,
                    0
                ],
                "title": "Exploring Task-Level Optimal Prompts for Visual In-Context Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Exploring Task-Level Optimal Prompts for Visual In-Context Learning"
                },
                "summary": "With the development of Vision Foundation Models (VFMs) in recent years,\nVisual In-Context Learning (VICL) has become a better choice compared to\nmodifying models in most scenarios. Different from retraining or fine-tuning\nmodel, VICL does not require modifications to the model's weights or\narchitecture, and only needs a prompt with demonstrations to teach VFM how to\nsolve tasks. Currently, significant computational cost for finding optimal\nprompts for every test sample hinders the deployment of VICL, as determining\nwhich demonstrations to use for constructing prompts is very costly. In this\npaper, however, we find a counterintuitive phenomenon that most test samples\nactually achieve optimal performance under the same prompts, and searching for\nsample-level prompts only costs more time but results in completely identical\nprompts. Therefore, we propose task-level prompting to reduce the cost of\nsearching for prompts during the inference stage and introduce two time-saving\nyet effective task-level prompt search strategies. Extensive experimental\nresults show that our proposed method can identify near-optimal prompts and\nreach the best VICL performance with a minimal cost that prior work has never\nachieved.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the development of Vision Foundation Models (VFMs) in recent years,\nVisual In-Context Learning (VICL) has become a better choice compared to\nmodifying models in most scenarios. Different from retraining or fine-tuning\nmodel, VICL does not require modifications to the model's weights or\narchitecture, and only needs a prompt with demonstrations to teach VFM how to\nsolve tasks. Currently, significant computational cost for finding optimal\nprompts for every test sample hinders the deployment of VICL, as determining\nwhich demonstrations to use for constructing prompts is very costly. In this\npaper, however, we find a counterintuitive phenomenon that most test samples\nactually achieve optimal performance under the same prompts, and searching for\nsample-level prompts only costs more time but results in completely identical\nprompts. Therefore, we propose task-level prompting to reduce the cost of\nsearching for prompts during the inference stage and introduce two time-saving\nyet effective task-level prompt search strategies. Extensive experimental\nresults show that our proposed method can identify near-optimal prompts and\nreach the best VICL performance with a minimal cost that prior work has never\nachieved."
                },
                "authors": [
                    {
                        "name": "Yan Zhu"
                    },
                    {
                        "name": "Huan Ma"
                    },
                    {
                        "name": "Changqing Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Changqing Zhang"
                },
                "author": "Changqing Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.08841v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.08841v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.08838v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.08838v1",
                "updated": "2025-01-15T14:47:02Z",
                "updated_parsed": [
                    2025,
                    1,
                    15,
                    14,
                    47,
                    2,
                    2,
                    15,
                    0
                ],
                "published": "2025-01-15T14:47:02Z",
                "published_parsed": [
                    2025,
                    1,
                    15,
                    14,
                    47,
                    2,
                    2,
                    15,
                    0
                ],
                "title": "ToMATO: Verbalizing the Mental States of Role-Playing LLMs for\n  Benchmarking Theory of Mind",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ToMATO: Verbalizing the Mental States of Role-Playing LLMs for\n  Benchmarking Theory of Mind"
                },
                "summary": "Existing Theory of Mind (ToM) benchmarks diverge from real-world scenarios in\nthree aspects: 1) they assess a limited range of mental states such as beliefs,\n2) false beliefs are not comprehensively explored, and 3) the diverse\npersonality traits of characters are overlooked. To address these challenges,\nwe introduce ToMATO, a new ToM benchmark formulated as multiple-choice QA over\nconversations. ToMATO is generated via LLM-LLM conversations featuring\ninformation asymmetry. By employing a prompting method that requires\nrole-playing LLMs to verbalize their thoughts before each utterance, we capture\nboth first- and second-order mental states across five categories: belief,\nintention, desire, emotion, and knowledge. These verbalized thoughts serve as\nanswers to questions designed to assess the mental states of characters within\nconversations. Furthermore, the information asymmetry introduced by hiding\nthoughts from others induces the generation of false beliefs about various\nmental states. Assigning distinct personality traits to LLMs further\ndiversifies both utterances and thoughts. ToMATO consists of 5.4k questions,\n753 conversations, and 15 personality trait patterns. Our analysis shows that\nthis dataset construction approach frequently generates false beliefs due to\nthe information asymmetry between role-playing LLMs, and effectively reflects\ndiverse personalities. We evaluate nine LLMs on ToMATO and find that even\nGPT-4o mini lags behind human performance, especially in understanding false\nbeliefs, and lacks robustness to various personality traits.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Existing Theory of Mind (ToM) benchmarks diverge from real-world scenarios in\nthree aspects: 1) they assess a limited range of mental states such as beliefs,\n2) false beliefs are not comprehensively explored, and 3) the diverse\npersonality traits of characters are overlooked. To address these challenges,\nwe introduce ToMATO, a new ToM benchmark formulated as multiple-choice QA over\nconversations. ToMATO is generated via LLM-LLM conversations featuring\ninformation asymmetry. By employing a prompting method that requires\nrole-playing LLMs to verbalize their thoughts before each utterance, we capture\nboth first- and second-order mental states across five categories: belief,\nintention, desire, emotion, and knowledge. These verbalized thoughts serve as\nanswers to questions designed to assess the mental states of characters within\nconversations. Furthermore, the information asymmetry introduced by hiding\nthoughts from others induces the generation of false beliefs about various\nmental states. Assigning distinct personality traits to LLMs further\ndiversifies both utterances and thoughts. ToMATO consists of 5.4k questions,\n753 conversations, and 15 personality trait patterns. Our analysis shows that\nthis dataset construction approach frequently generates false beliefs due to\nthe information asymmetry between role-playing LLMs, and effectively reflects\ndiverse personalities. We evaluate nine LLMs on ToMATO and find that even\nGPT-4o mini lags behind human performance, especially in understanding false\nbeliefs, and lacks robustness to various personality traits."
                },
                "authors": [
                    {
                        "name": "Kazutoshi Shinoda"
                    },
                    {
                        "name": "Nobukatsu Hojo"
                    },
                    {
                        "name": "Kyosuke Nishida"
                    },
                    {
                        "name": "Saki Mizuno"
                    },
                    {
                        "name": "Keita Suzuki"
                    },
                    {
                        "name": "Ryo Masumura"
                    },
                    {
                        "name": "Hiroaki Sugiyama"
                    },
                    {
                        "name": "Kuniko Saito"
                    }
                ],
                "author_detail": {
                    "name": "Kuniko Saito"
                },
                "author": "Kuniko Saito",
                "arxiv_comment": "Accepted by AAAI 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.08838v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.08838v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.11192v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.11192v2",
                "updated": "2025-01-15T14:38:01Z",
                "updated_parsed": [
                    2025,
                    1,
                    15,
                    14,
                    38,
                    1,
                    2,
                    15,
                    0
                ],
                "published": "2024-06-17T03:57:35Z",
                "published_parsed": [
                    2024,
                    6,
                    17,
                    3,
                    57,
                    35,
                    0,
                    169,
                    0
                ],
                "title": "Beyond Boundaries: Learning a Universal Entity Taxonomy across Datasets\n  and Languages for Open Named Entity Recognition",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Beyond Boundaries: Learning a Universal Entity Taxonomy across Datasets\n  and Languages for Open Named Entity Recognition"
                },
                "summary": "Open Named Entity Recognition (NER), which involves identifying arbitrary\ntypes of entities from arbitrary domains, remains challenging for Large\nLanguage Models (LLMs). Recent studies suggest that fine-tuning LLMs on\nextensive NER data can boost their performance. However, training directly on\nexisting datasets neglects their inconsistent entity definitions and redundant\ndata, limiting LLMs to dataset-specific learning and hindering out-of-domain\nadaptation. To address this, we present B2NERD, a compact dataset designed to\nguide LLMs' generalization in Open NER under a universal entity taxonomy.\nB2NERD is refined from 54 existing English and Chinese datasets using a\ntwo-step process. First, we detect inconsistent entity definitions across\ndatasets and clarify them by distinguishable label names to construct a\nuniversal taxonomy of 400+ entity types. Second, we address redundancy using a\ndata pruning strategy that selects fewer samples with greater category and\nsemantic diversity. Comprehensive evaluation shows that B2NERD significantly\nenhances LLMs' Open NER capabilities. Our B2NER models, trained on B2NERD,\noutperform GPT-4 by 6.8-12.0 F1 points and surpass previous methods in 3\nout-of-domain benchmarks across 15 datasets and 6 languages. The data, models,\nand code are publicly available at https://github.com/UmeanNever/B2NER.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Open Named Entity Recognition (NER), which involves identifying arbitrary\ntypes of entities from arbitrary domains, remains challenging for Large\nLanguage Models (LLMs). Recent studies suggest that fine-tuning LLMs on\nextensive NER data can boost their performance. However, training directly on\nexisting datasets neglects their inconsistent entity definitions and redundant\ndata, limiting LLMs to dataset-specific learning and hindering out-of-domain\nadaptation. To address this, we present B2NERD, a compact dataset designed to\nguide LLMs' generalization in Open NER under a universal entity taxonomy.\nB2NERD is refined from 54 existing English and Chinese datasets using a\ntwo-step process. First, we detect inconsistent entity definitions across\ndatasets and clarify them by distinguishable label names to construct a\nuniversal taxonomy of 400+ entity types. Second, we address redundancy using a\ndata pruning strategy that selects fewer samples with greater category and\nsemantic diversity. Comprehensive evaluation shows that B2NERD significantly\nenhances LLMs' Open NER capabilities. Our B2NER models, trained on B2NERD,\noutperform GPT-4 by 6.8-12.0 F1 points and surpass previous methods in 3\nout-of-domain benchmarks across 15 datasets and 6 languages. The data, models,\nand code are publicly available at https://github.com/UmeanNever/B2NER."
                },
                "authors": [
                    {
                        "name": "Yuming Yang"
                    },
                    {
                        "name": "Wantong Zhao"
                    },
                    {
                        "name": "Caishuang Huang"
                    },
                    {
                        "name": "Junjie Ye"
                    },
                    {
                        "name": "Xiao Wang"
                    },
                    {
                        "name": "Huiyuan Zheng"
                    },
                    {
                        "name": "Yang Nan"
                    },
                    {
                        "name": "Yuran Wang"
                    },
                    {
                        "name": "Xueying Xu"
                    },
                    {
                        "name": "Kaixin Huang"
                    },
                    {
                        "name": "Yunke Zhang"
                    },
                    {
                        "name": "Tao Gui"
                    },
                    {
                        "name": "Qi Zhang"
                    },
                    {
                        "name": "Xuanjing Huang"
                    }
                ],
                "author_detail": {
                    "name": "Xuanjing Huang"
                },
                "author": "Xuanjing Huang",
                "arxiv_comment": "Accepted at COLING 2025. Camera-ready version updated. Project page:\n  https://github.com/UmeanNever/B2NER",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.11192v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.11192v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.08821v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.08821v1",
                "updated": "2025-01-15T14:19:03Z",
                "updated_parsed": [
                    2025,
                    1,
                    15,
                    14,
                    19,
                    3,
                    2,
                    15,
                    0
                ],
                "published": "2025-01-15T14:19:03Z",
                "published_parsed": [
                    2025,
                    1,
                    15,
                    14,
                    19,
                    3,
                    2,
                    15,
                    0
                ],
                "title": "A Closer Look at the Learnability of Out-of-Distribution (OOD) Detection",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Closer Look at the Learnability of Out-of-Distribution (OOD) Detection"
                },
                "summary": "Machine learning algorithms often encounter different or\n\"out-of-distribution\" (OOD) data at deployment time, and OOD detection is\nfrequently employed to detect these examples. While it works reasonably well in\npractice, existing theoretical results on OOD detection are highly pessimistic.\nIn this work, we take a closer look at this problem, and make a distinction\nbetween uniform and non-uniform learnability, following PAC learning theory. We\ncharacterize under what conditions OOD detection is uniformly and non-uniformly\nlearnable, and we show that in several cases, non-uniform learnability turns a\nnumber of negative results into positive. In all cases where OOD detection is\nlearnable, we provide concrete learning algorithms and a sample-complexity\nanalysis.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Machine learning algorithms often encounter different or\n\"out-of-distribution\" (OOD) data at deployment time, and OOD detection is\nfrequently employed to detect these examples. While it works reasonably well in\npractice, existing theoretical results on OOD detection are highly pessimistic.\nIn this work, we take a closer look at this problem, and make a distinction\nbetween uniform and non-uniform learnability, following PAC learning theory. We\ncharacterize under what conditions OOD detection is uniformly and non-uniformly\nlearnable, and we show that in several cases, non-uniform learnability turns a\nnumber of negative results into positive. In all cases where OOD detection is\nlearnable, we provide concrete learning algorithms and a sample-complexity\nanalysis."
                },
                "authors": [
                    {
                        "name": "Konstantin Garov"
                    },
                    {
                        "name": "Kamalika Chaudhuri"
                    }
                ],
                "author_detail": {
                    "name": "Kamalika Chaudhuri"
                },
                "author": "Kamalika Chaudhuri",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.08821v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.08821v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.03093v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.03093v3",
                "updated": "2025-01-15T13:46:19Z",
                "updated_parsed": [
                    2025,
                    1,
                    15,
                    13,
                    46,
                    19,
                    2,
                    15,
                    0
                ],
                "published": "2024-09-04T21:46:18Z",
                "published_parsed": [
                    2024,
                    9,
                    4,
                    21,
                    46,
                    18,
                    2,
                    248,
                    0
                ],
                "title": "ASTER: Natural and Multi-language Unit Test Generation with LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ASTER: Natural and Multi-language Unit Test Generation with LLMs"
                },
                "summary": "Implementing automated unit tests is an important but time-consuming activity\nin software development. To assist developers in this task, many techniques for\nautomating unit test generation have been developed. However, despite this\neffort, usable tools exist for very few programming languages. Moreover,\nstudies have found that automatically generated tests suffer poor readability\nand do not resemble developer-written tests. In this work, we present a\nrigorous investigation of how large language models (LLMs) can help bridge the\ngap. We describe a generic pipeline that incorporates static analysis to guide\nLLMs in generating compilable and high-coverage test cases. We illustrate how\nthe pipeline can be applied to different programming languages, specifically\nJava and Python, and to complex software requiring environment mocking. We\nconducted an empirical study to assess the quality of the generated tests in\nterms of code coverage and test naturalness -- evaluating them on standard as\nwell as enterprise Java applications and a large Python benchmark. Our results\ndemonstrate that LLM-based test generation, when guided by static analysis, can\nbe competitive with, and even outperform, state-of-the-art test-generation\ntechniques in coverage achieved while also producing considerably more natural\ntest cases that developers find easy to understand. We also present the results\nof a user study, conducted with 161 professional developers, that highlights\nthe naturalness characteristics of the tests generated by our approach.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Implementing automated unit tests is an important but time-consuming activity\nin software development. To assist developers in this task, many techniques for\nautomating unit test generation have been developed. However, despite this\neffort, usable tools exist for very few programming languages. Moreover,\nstudies have found that automatically generated tests suffer poor readability\nand do not resemble developer-written tests. In this work, we present a\nrigorous investigation of how large language models (LLMs) can help bridge the\ngap. We describe a generic pipeline that incorporates static analysis to guide\nLLMs in generating compilable and high-coverage test cases. We illustrate how\nthe pipeline can be applied to different programming languages, specifically\nJava and Python, and to complex software requiring environment mocking. We\nconducted an empirical study to assess the quality of the generated tests in\nterms of code coverage and test naturalness -- evaluating them on standard as\nwell as enterprise Java applications and a large Python benchmark. Our results\ndemonstrate that LLM-based test generation, when guided by static analysis, can\nbe competitive with, and even outperform, state-of-the-art test-generation\ntechniques in coverage achieved while also producing considerably more natural\ntest cases that developers find easy to understand. We also present the results\nof a user study, conducted with 161 professional developers, that highlights\nthe naturalness characteristics of the tests generated by our approach."
                },
                "authors": [
                    {
                        "name": "Rangeet Pan"
                    },
                    {
                        "name": "Myeongsoo Kim"
                    },
                    {
                        "name": "Rahul Krishna"
                    },
                    {
                        "name": "Raju Pavuluri"
                    },
                    {
                        "name": "Saurabh Sinha"
                    }
                ],
                "author_detail": {
                    "name": "Saurabh Sinha"
                },
                "author": "Saurabh Sinha",
                "arxiv_comment": "Accepted at ICSE-SEIP, 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.03093v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.03093v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.08792v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.08792v1",
                "updated": "2025-01-15T13:36:57Z",
                "updated_parsed": [
                    2025,
                    1,
                    15,
                    13,
                    36,
                    57,
                    2,
                    15,
                    0
                ],
                "published": "2025-01-15T13:36:57Z",
                "published_parsed": [
                    2025,
                    1,
                    15,
                    13,
                    36,
                    57,
                    2,
                    15,
                    0
                ],
                "title": "First experimental results and optimization study of the portable\n  neutron-gamma imager GN-Vision",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "First experimental results and optimization study of the portable\n  neutron-gamma imager GN-Vision"
                },
                "summary": "GN-Vision is a compact, dual-modality imaging device designed to\nsimultaneously localize the spatial origin of $\\gamma$-ray and slow neutron\nsources, with potential applications in nuclear safety, security, and hadron\ntherapy. The system utilizes two position-sensitive detection planes, combining\nCompton imaging techniques for $\\gamma$-ray visualization with passive\ncollimation for imaging slow and thermal neutrons (energies below 100 eV). This\npaper presents the first experimental outcomes from the initial GN-Vision\nprototype, focused on the development of its neutron imaging capabilities.\nFollowing this experimental assessment, we explore the device$'$s performance\npotential and discuss several Monte Carlo simulation-based optimizations aimed\nat refining the neutron collimation system. These optimizations seek to improve\nreal-time imaging efficiency and cost-effectiveness, enhancing GN-Vision$'$s\napplicability for future practical deployments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GN-Vision is a compact, dual-modality imaging device designed to\nsimultaneously localize the spatial origin of $\\gamma$-ray and slow neutron\nsources, with potential applications in nuclear safety, security, and hadron\ntherapy. The system utilizes two position-sensitive detection planes, combining\nCompton imaging techniques for $\\gamma$-ray visualization with passive\ncollimation for imaging slow and thermal neutrons (energies below 100 eV). This\npaper presents the first experimental outcomes from the initial GN-Vision\nprototype, focused on the development of its neutron imaging capabilities.\nFollowing this experimental assessment, we explore the device$'$s performance\npotential and discuss several Monte Carlo simulation-based optimizations aimed\nat refining the neutron collimation system. These optimizations seek to improve\nreal-time imaging efficiency and cost-effectiveness, enhancing GN-Vision$'$s\napplicability for future practical deployments."
                },
                "authors": [
                    {
                        "name": "Jorge Lerendegui-Marco"
                    },
                    {
                        "name": "James Hallam"
                    },
                    {
                        "name": "Gastón Cisterna"
                    },
                    {
                        "name": "Andrea Sanchis-Moltó"
                    },
                    {
                        "name": "Javier Balibrea-Correa"
                    },
                    {
                        "name": "Víctor Babiano"
                    },
                    {
                        "name": "David Calvo"
                    },
                    {
                        "name": "Gabriel de la Fuente"
                    },
                    {
                        "name": "Bernardo Gameiro"
                    },
                    {
                        "name": "Ion Ladarescu"
                    },
                    {
                        "name": "Pablo Torres-Sánchez"
                    },
                    {
                        "name": "César Domingo-Pardo"
                    }
                ],
                "author_detail": {
                    "name": "César Domingo-Pardo"
                },
                "author": "César Domingo-Pardo",
                "arxiv_comment": "13 pages, 12 figures, submitted to App. Rad. Isot",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.08792v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.08792v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.ins-det",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.ins-det",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "nucl-ex",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.08769v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.08769v1",
                "updated": "2025-01-15T12:42:09Z",
                "updated_parsed": [
                    2025,
                    1,
                    15,
                    12,
                    42,
                    9,
                    2,
                    15,
                    0
                ],
                "published": "2025-01-15T12:42:09Z",
                "published_parsed": [
                    2025,
                    1,
                    15,
                    12,
                    42,
                    9,
                    2,
                    15,
                    0
                ],
                "title": "Enhanced Large Language Models for Effective Screening of Depression and\n  Anxiety",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Enhanced Large Language Models for Effective Screening of Depression and\n  Anxiety"
                },
                "summary": "Depressive and anxiety disorders are widespread, necessitating timely\nidentification and management. Recent advances in Large Language Models (LLMs)\noffer potential solutions, yet high costs and ethical concerns about training\ndata remain challenges. This paper introduces a pipeline for synthesizing\nclinical interviews, resulting in 1,157 interactive dialogues (PsyInterview),\nand presents EmoScan, an LLM-based emotional disorder screening system. EmoScan\ndistinguishes between coarse (e.g., anxiety or depressive disorders) and fine\ndisorders (e.g., major depressive disorders) and conducts high-quality\ninterviews. Evaluations showed that EmoScan exceeded the performance of base\nmodels and other LLMs like GPT-4 in screening emotional disorders\n(F1-score=0.7467). It also delivers superior explanations (BERTScore=0.9408)\nand demonstrates robust generalizability (F1-score of 0.67 on an external\ndataset). Furthermore, EmoScan outperforms baselines in interviewing skills, as\nvalidated by automated ratings and human evaluations. This work highlights the\nimportance of scalable data-generative pipelines for developing effective\nmental health LLM tools.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Depressive and anxiety disorders are widespread, necessitating timely\nidentification and management. Recent advances in Large Language Models (LLMs)\noffer potential solutions, yet high costs and ethical concerns about training\ndata remain challenges. This paper introduces a pipeline for synthesizing\nclinical interviews, resulting in 1,157 interactive dialogues (PsyInterview),\nand presents EmoScan, an LLM-based emotional disorder screening system. EmoScan\ndistinguishes between coarse (e.g., anxiety or depressive disorders) and fine\ndisorders (e.g., major depressive disorders) and conducts high-quality\ninterviews. Evaluations showed that EmoScan exceeded the performance of base\nmodels and other LLMs like GPT-4 in screening emotional disorders\n(F1-score=0.7467). It also delivers superior explanations (BERTScore=0.9408)\nand demonstrates robust generalizability (F1-score of 0.67 on an external\ndataset). Furthermore, EmoScan outperforms baselines in interviewing skills, as\nvalidated by automated ratings and human evaluations. This work highlights the\nimportance of scalable data-generative pipelines for developing effective\nmental health LLM tools."
                },
                "authors": [
                    {
                        "name": "June M. Liu"
                    },
                    {
                        "name": "Mengxia Gao"
                    },
                    {
                        "name": "Sahand Sabour"
                    },
                    {
                        "name": "Zhuang Chen"
                    },
                    {
                        "name": "Minlie Huang"
                    },
                    {
                        "name": "Tatia M. C. Lee"
                    }
                ],
                "author_detail": {
                    "name": "Tatia M. C. Lee"
                },
                "author": "Tatia M. C. Lee",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.08769v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.08769v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.08760v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.08760v1",
                "updated": "2025-01-15T12:25:56Z",
                "updated_parsed": [
                    2025,
                    1,
                    15,
                    12,
                    25,
                    56,
                    2,
                    15,
                    0
                ],
                "published": "2025-01-15T12:25:56Z",
                "published_parsed": [
                    2025,
                    1,
                    15,
                    12,
                    25,
                    56,
                    2,
                    15,
                    0
                ],
                "title": "Leveraging LLM Agents for Translating Network Configurations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Leveraging LLM Agents for Translating Network Configurations"
                },
                "summary": "Configuration translation is a critical and frequent task in network\noperations. When a network device is damaged or outdated, administrators need\nto replace it to maintain service continuity. The replacement devices may\noriginate from different vendors, necessitating configuration translation to\nensure seamless network operation. However, translating configurations manually\nis a labor-intensive and error-prone process. In this paper, we propose an\nintent-based framework for translating network configuration with Large\nLanguage Model (LLM) Agents. The core of our approach is an Intent-based\nRetrieval Augmented Generation (IRAG) module that systematically splits a\nconfiguration file into fragments, extracts intents, and generates accurate\ntranslations. We also design a two-stage verification method to validate the\nsyntax and semantics correctness of the translated configurations. We implement\nand evaluate the proposed method on real-world network configurations.\nExperimental results show that our method achieves 97.74% syntax correctness,\noutperforming state-of-the-art methods in translation accuracy.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Configuration translation is a critical and frequent task in network\noperations. When a network device is damaged or outdated, administrators need\nto replace it to maintain service continuity. The replacement devices may\noriginate from different vendors, necessitating configuration translation to\nensure seamless network operation. However, translating configurations manually\nis a labor-intensive and error-prone process. In this paper, we propose an\nintent-based framework for translating network configuration with Large\nLanguage Model (LLM) Agents. The core of our approach is an Intent-based\nRetrieval Augmented Generation (IRAG) module that systematically splits a\nconfiguration file into fragments, extracts intents, and generates accurate\ntranslations. We also design a two-stage verification method to validate the\nsyntax and semantics correctness of the translated configurations. We implement\nand evaluate the proposed method on real-world network configurations.\nExperimental results show that our method achieves 97.74% syntax correctness,\noutperforming state-of-the-art methods in translation accuracy."
                },
                "authors": [
                    {
                        "name": "Yunze Wei"
                    },
                    {
                        "name": "Xiaohui Xie"
                    },
                    {
                        "name": "Yiwei Zuo"
                    },
                    {
                        "name": "Tianshuo Hu"
                    },
                    {
                        "name": "Xinyi Chen"
                    },
                    {
                        "name": "Kaiwen Chi"
                    },
                    {
                        "name": "Yong Cui"
                    }
                ],
                "author_detail": {
                    "name": "Yong Cui"
                },
                "author": "Yong Cui",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.08760v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.08760v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.08109v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.08109v2",
                "updated": "2025-01-15T11:57:34Z",
                "updated_parsed": [
                    2025,
                    1,
                    15,
                    11,
                    57,
                    34,
                    2,
                    15,
                    0
                ],
                "published": "2024-12-11T05:31:39Z",
                "published_parsed": [
                    2024,
                    12,
                    11,
                    5,
                    31,
                    39,
                    2,
                    346,
                    0
                ],
                "title": "Unseen Horizons: Unveiling the Real Capability of LLM Code Generation\n  Beyond the Familiar",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Unseen Horizons: Unveiling the Real Capability of LLM Code Generation\n  Beyond the Familiar"
                },
                "summary": "Recently, large language models (LLMs) have shown strong potential in code\ngeneration tasks. However, there are still gaps before they can be fully\napplied in actual software development processes. Accurately assessing the code\ngeneration capabilities of large language models has become an important basis\nfor evaluating and improving the models. Some existing works have constructed\ndatasets to evaluate the capabilities of these models. However, the current\nevaluation process may encounter the illusion of \"Specialist in Familiarity\",\nprimarily due to three gaps: the exposure of target code, case timeliness, and\ndependency availability. The fundamental reason for these gaps is that the code\nin current datasets may have been extensively exposed and exercised during the\ntraining phase, and due to the continuous training and development of LLM,\ntheir timeliness has been severely compromised. The key to solve the problem is\nto, as much as possible, evaluate the LLMs using code that they have not\nencountered before. Thus, the fundamental idea in this paper is to draw on the\nconcept of code obfuscation, changing code at different levels while ensuring\nthe functionality and output. To this end, we build a code-obfuscation based\nbenchmark OBFUSEVAL. We first collect 1,354 raw cases from five real-world\nprojects, including function description and code. Then we use three-level\nstrategy (symbol, structure and semantic) to obfuscate descriptions, code and\ncontext dependencies. We evaluate four LLMs on OBFU- SEVAL and compared the\neffectiveness of different obfuscation strategy. We use official test suites of\nthese projects to evaluate the generated code. The results show that after\nobfuscation, the average decrease ratio of test pass rate can up to 62.5%.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recently, large language models (LLMs) have shown strong potential in code\ngeneration tasks. However, there are still gaps before they can be fully\napplied in actual software development processes. Accurately assessing the code\ngeneration capabilities of large language models has become an important basis\nfor evaluating and improving the models. Some existing works have constructed\ndatasets to evaluate the capabilities of these models. However, the current\nevaluation process may encounter the illusion of \"Specialist in Familiarity\",\nprimarily due to three gaps: the exposure of target code, case timeliness, and\ndependency availability. The fundamental reason for these gaps is that the code\nin current datasets may have been extensively exposed and exercised during the\ntraining phase, and due to the continuous training and development of LLM,\ntheir timeliness has been severely compromised. The key to solve the problem is\nto, as much as possible, evaluate the LLMs using code that they have not\nencountered before. Thus, the fundamental idea in this paper is to draw on the\nconcept of code obfuscation, changing code at different levels while ensuring\nthe functionality and output. To this end, we build a code-obfuscation based\nbenchmark OBFUSEVAL. We first collect 1,354 raw cases from five real-world\nprojects, including function description and code. Then we use three-level\nstrategy (symbol, structure and semantic) to obfuscate descriptions, code and\ncontext dependencies. We evaluate four LLMs on OBFU- SEVAL and compared the\neffectiveness of different obfuscation strategy. We use official test suites of\nthese projects to evaluate the generated code. The results show that after\nobfuscation, the average decrease ratio of test pass rate can up to 62.5%."
                },
                "authors": [
                    {
                        "name": "Yuanliang Zhang"
                    },
                    {
                        "name": "Yifan Xie"
                    },
                    {
                        "name": "Shanshan Li"
                    },
                    {
                        "name": "Ke Liu"
                    },
                    {
                        "name": "Chong Wang"
                    },
                    {
                        "name": "Zhouyang Jia"
                    },
                    {
                        "name": "Xiangbing Huang"
                    },
                    {
                        "name": "Jie Song"
                    },
                    {
                        "name": "Chaopeng Luo"
                    },
                    {
                        "name": "Zhizheng Zheng"
                    },
                    {
                        "name": "Rulin Xu"
                    },
                    {
                        "name": "Yitong Liu"
                    },
                    {
                        "name": "Si Zheng"
                    },
                    {
                        "name": "Xiangke Liao"
                    }
                ],
                "author_detail": {
                    "name": "Xiangke Liao"
                },
                "author": "Xiangke Liao",
                "arxiv_comment": "Accepted by the 47th International Conference on Software Engineering\n  (ICSE 2025)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.08109v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.08109v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.03268v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.03268v2",
                "updated": "2025-01-15T11:39:56Z",
                "updated_parsed": [
                    2025,
                    1,
                    15,
                    11,
                    39,
                    56,
                    2,
                    15,
                    0
                ],
                "published": "2024-10-04T09:39:17Z",
                "published_parsed": [
                    2024,
                    10,
                    4,
                    9,
                    39,
                    17,
                    4,
                    278,
                    0
                ],
                "title": "Narrative Player: Reviving Data Narratives with Visuals",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Narrative Player: Reviving Data Narratives with Visuals"
                },
                "summary": "Data-rich documents are commonly found across various fields such as\nbusiness, finance, and science. However, a general limitation of these\ndocuments for reading is their reliance on text to convey data and facts.\nVisual representation of text aids in providing a satisfactory reading\nexperience in comprehension and engagement. However, existing work emphasizes\npresenting the insights of local text context, rather than fully conveying data\nstories within the whole paragraphs and engaging readers. To provide readers\nwith satisfactory data stories, this paper presents Narrative Player, a novel\nmethod that automatically revives data narratives with consistent and\ncontextualized visuals. Specifically, it accepts a paragraph and corresponding\ndata table as input and leverages LLMs to characterize the clauses and extract\ncontextualized data facts. Subsequently, the facts are transformed into a\ncoherent visualization sequence with a carefully designed optimization-based\napproach. Animations are also assigned between adjacent visualizations to\nenable seamless transitions. Finally, the visualization sequence, transition\nanimations, and audio narration generated by text-to-speech technologies are\nrendered into a data video. The evaluation results showed that the\nautomatic-generated data videos were well-received by participants and experts\nfor enhancing reading.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Data-rich documents are commonly found across various fields such as\nbusiness, finance, and science. However, a general limitation of these\ndocuments for reading is their reliance on text to convey data and facts.\nVisual representation of text aids in providing a satisfactory reading\nexperience in comprehension and engagement. However, existing work emphasizes\npresenting the insights of local text context, rather than fully conveying data\nstories within the whole paragraphs and engaging readers. To provide readers\nwith satisfactory data stories, this paper presents Narrative Player, a novel\nmethod that automatically revives data narratives with consistent and\ncontextualized visuals. Specifically, it accepts a paragraph and corresponding\ndata table as input and leverages LLMs to characterize the clauses and extract\ncontextualized data facts. Subsequently, the facts are transformed into a\ncoherent visualization sequence with a carefully designed optimization-based\napproach. Animations are also assigned between adjacent visualizations to\nenable seamless transitions. Finally, the visualization sequence, transition\nanimations, and audio narration generated by text-to-speech technologies are\nrendered into a data video. The evaluation results showed that the\nautomatic-generated data videos were well-received by participants and experts\nfor enhancing reading."
                },
                "authors": [
                    {
                        "name": "Zekai Shao"
                    },
                    {
                        "name": "Leixian Shen"
                    },
                    {
                        "name": "Haotian Li"
                    },
                    {
                        "name": "Yi Shan"
                    },
                    {
                        "name": "Huamin Qu"
                    },
                    {
                        "name": "Yun Wang"
                    },
                    {
                        "name": "Siming Chen"
                    }
                ],
                "author_detail": {
                    "name": "Siming Chen"
                },
                "author": "Siming Chen",
                "arxiv_comment": "Accepted by IEEE TVCG",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.03268v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.03268v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.08737v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.08737v1",
                "updated": "2025-01-15T11:33:52Z",
                "updated_parsed": [
                    2025,
                    1,
                    15,
                    11,
                    33,
                    52,
                    2,
                    15,
                    0
                ],
                "published": "2025-01-15T11:33:52Z",
                "published_parsed": [
                    2025,
                    1,
                    15,
                    11,
                    33,
                    52,
                    2,
                    15,
                    0
                ],
                "title": "Resource-Constrained Federated Continual Learning: What Does Matter?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Resource-Constrained Federated Continual Learning: What Does Matter?"
                },
                "summary": "Federated Continual Learning (FCL) aims to enable sequentially\nprivacy-preserving model training on streams of incoming data that vary in edge\ndevices by preserving previous knowledge while adapting to new data. Current\nFCL literature focuses on restricted data privacy and access to previously seen\ndata while imposing no constraints on the training overhead. This is\nunreasonable for FCL applications in real-world scenarios, where edge devices\nare primarily constrained by resources such as storage, computational budget,\nand label rate. We revisit this problem with a large-scale benchmark and\nanalyze the performance of state-of-the-art FCL approaches under different\nresource-constrained settings. Various typical FCL techniques and six datasets\nin two incremental learning scenarios (Class-IL and Domain-IL) are involved in\nour experiments. Through extensive experiments amounting to a total of over\n1,000+ GPU hours, we find that, under limited resource-constrained settings,\nexisting FCL approaches, with no exception, fail to achieve the expected\nperformance. Our conclusions are consistent in the sensitivity analysis. This\nsuggests that most existing FCL methods are particularly too resource-dependent\nfor real-world deployment. Moreover, we study the performance of typical FCL\ntechniques with resource constraints and shed light on future research\ndirections in FCL.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Federated Continual Learning (FCL) aims to enable sequentially\nprivacy-preserving model training on streams of incoming data that vary in edge\ndevices by preserving previous knowledge while adapting to new data. Current\nFCL literature focuses on restricted data privacy and access to previously seen\ndata while imposing no constraints on the training overhead. This is\nunreasonable for FCL applications in real-world scenarios, where edge devices\nare primarily constrained by resources such as storage, computational budget,\nand label rate. We revisit this problem with a large-scale benchmark and\nanalyze the performance of state-of-the-art FCL approaches under different\nresource-constrained settings. Various typical FCL techniques and six datasets\nin two incremental learning scenarios (Class-IL and Domain-IL) are involved in\nour experiments. Through extensive experiments amounting to a total of over\n1,000+ GPU hours, we find that, under limited resource-constrained settings,\nexisting FCL approaches, with no exception, fail to achieve the expected\nperformance. Our conclusions are consistent in the sensitivity analysis. This\nsuggests that most existing FCL methods are particularly too resource-dependent\nfor real-world deployment. Moreover, we study the performance of typical FCL\ntechniques with resource constraints and shed light on future research\ndirections in FCL."
                },
                "authors": [
                    {
                        "name": "Yichen Li"
                    },
                    {
                        "name": "Yuying Wang"
                    },
                    {
                        "name": "Jiahua Dong"
                    },
                    {
                        "name": "Haozhao Wang"
                    },
                    {
                        "name": "Yining Qi"
                    },
                    {
                        "name": "Rui Zhang"
                    },
                    {
                        "name": "Ruixuan Li"
                    }
                ],
                "author_detail": {
                    "name": "Ruixuan Li"
                },
                "author": "Ruixuan Li",
                "arxiv_comment": "arXiv admin note: text overlap with arXiv:2303.11165 by other authors",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.08737v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.08737v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.07937v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.07937v2",
                "updated": "2025-01-15T11:18:58Z",
                "updated_parsed": [
                    2025,
                    1,
                    15,
                    11,
                    18,
                    58,
                    2,
                    15,
                    0
                ],
                "published": "2025-01-14T08:45:40Z",
                "published_parsed": [
                    2025,
                    1,
                    14,
                    8,
                    45,
                    40,
                    1,
                    14,
                    0
                ],
                "title": "Towards high resolution, validated and open global wind power\n  assessments",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Towards high resolution, validated and open global wind power\n  assessments"
                },
                "summary": "Wind power is expected to play a crucial role in future net-zero energy\nsystems, but wind power simulations to support deployment strategies vary\ndrastically in their results, hindering reliable design decisions. Therefore,\nwe present a transparent, open source, validated and evaluated, global wind\npower simulation tool ETHOS.RESKit.Wind with high spatial resolution and\ncustomizable designs for both onshore and offshore wind turbines. The tool\nprovides a comprehensive validation and calibration procedure using over 16\nmillion global measurements from metrerological masts and wind turbine sites.\nWe achieve a global average capacity factor mean error of 0.006 and Pearson\ncorrelation of 0.865. In addition, we evaluate its performance against several\naggregated and statistical sources of wind power generation. The release of\nETHOS.RESKit.Wind is a step towards a fully open source and open data approach\nto accurate wind power modeling by incorporating the most comprehensive\nsimulation advances in one model.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Wind power is expected to play a crucial role in future net-zero energy\nsystems, but wind power simulations to support deployment strategies vary\ndrastically in their results, hindering reliable design decisions. Therefore,\nwe present a transparent, open source, validated and evaluated, global wind\npower simulation tool ETHOS.RESKit.Wind with high spatial resolution and\ncustomizable designs for both onshore and offshore wind turbines. The tool\nprovides a comprehensive validation and calibration procedure using over 16\nmillion global measurements from metrerological masts and wind turbine sites.\nWe achieve a global average capacity factor mean error of 0.006 and Pearson\ncorrelation of 0.865. In addition, we evaluate its performance against several\naggregated and statistical sources of wind power generation. The release of\nETHOS.RESKit.Wind is a step towards a fully open source and open data approach\nto accurate wind power modeling by incorporating the most comprehensive\nsimulation advances in one model."
                },
                "authors": [
                    {
                        "name": "Edgar Ubaldo Peña-Sánchez"
                    },
                    {
                        "name": "Philipp Dunkel"
                    },
                    {
                        "name": "Christoph Winkler"
                    },
                    {
                        "name": "Heidi Heinrichs"
                    },
                    {
                        "name": "Florian Prinz"
                    },
                    {
                        "name": "Jann Weinand"
                    },
                    {
                        "name": "Rachel Maier"
                    },
                    {
                        "name": "Sebastian Dickler"
                    },
                    {
                        "name": "Shuying Chen"
                    },
                    {
                        "name": "Katharina Gruber"
                    },
                    {
                        "name": "Theresa Klütz"
                    },
                    {
                        "name": "Jochen Linßen"
                    },
                    {
                        "name": "Detlef Stolten"
                    }
                ],
                "author_detail": {
                    "name": "Detlef Stolten"
                },
                "author": "Detlef Stolten",
                "arxiv_comment": "Edgar Ubaldo Pe\\~na-S\\'anchez, Philipp Dunkel and Christoph Winkler\n  contributed euqally to the paper. Main article: 20 pages, 7 figures;\n  supplementary material: 20 pages, 10 figures. This version includes minor\n  updates: Fix appearance of toolname in online abstract. Fix figure numbering\n  in appendix",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.07937v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.07937v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.soc-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.soc-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2311.17696v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2311.17696v5",
                "updated": "2025-01-15T11:12:26Z",
                "updated_parsed": [
                    2025,
                    1,
                    15,
                    11,
                    12,
                    26,
                    2,
                    15,
                    0
                ],
                "published": "2023-11-29T15:02:46Z",
                "published_parsed": [
                    2023,
                    11,
                    29,
                    15,
                    2,
                    46,
                    2,
                    333,
                    0
                ],
                "title": "How to Build an AI Tutor That Can Adapt to Any Course Using Knowledge\n  Graph-Enhanced Retrieval-Augmented Generation (KG-RAG)",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "How to Build an AI Tutor That Can Adapt to Any Course Using Knowledge\n  Graph-Enhanced Retrieval-Augmented Generation (KG-RAG)"
                },
                "summary": "This paper introduces a novel framework for adaptable AI tutors using\nKnowledge Graph-enhanced Retrieval-Augmented Generation (KG-RAG). This approach\naddresses the critical challenges of information hallucination and limited\ncourse-specific adaptation prevalent in Large Language Model (LLM)-based\ntutoring systems. By integrating Knowledge Graphs (KGs) with RAG, we provide a\nstructured representation of course concepts and their interrelationships,\ngrounding the AI tutor's responses in relevant, validated material. We leverage\nQwen2.5, a powerful and cost-effective LLM, within our KG-RAG framework. A user\nstudy (n=50) demonstrated positive student feedback regarding answer relevance,\nease of use, and overall satisfaction. This KG-RAG framework offers a promising\npathway towards personalized learning experiences and broader access to\nhigh-quality education.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper introduces a novel framework for adaptable AI tutors using\nKnowledge Graph-enhanced Retrieval-Augmented Generation (KG-RAG). This approach\naddresses the critical challenges of information hallucination and limited\ncourse-specific adaptation prevalent in Large Language Model (LLM)-based\ntutoring systems. By integrating Knowledge Graphs (KGs) with RAG, we provide a\nstructured representation of course concepts and their interrelationships,\ngrounding the AI tutor's responses in relevant, validated material. We leverage\nQwen2.5, a powerful and cost-effective LLM, within our KG-RAG framework. A user\nstudy (n=50) demonstrated positive student feedback regarding answer relevance,\nease of use, and overall satisfaction. This KG-RAG framework offers a promising\npathway towards personalized learning experiences and broader access to\nhigh-quality education."
                },
                "authors": [
                    {
                        "name": "Chenxi Dong"
                    },
                    {
                        "name": "Yimin Yuan"
                    },
                    {
                        "name": "Kan Chen"
                    },
                    {
                        "name": "Shupei Cheng"
                    },
                    {
                        "name": "Chujie Wen"
                    }
                ],
                "author_detail": {
                    "name": "Chujie Wen"
                },
                "author": "Chujie Wen",
                "arxiv_comment": "6 pages, 5 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2311.17696v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2311.17696v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.08716v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.08716v1",
                "updated": "2025-01-15T10:57:55Z",
                "updated_parsed": [
                    2025,
                    1,
                    15,
                    10,
                    57,
                    55,
                    2,
                    15,
                    0
                ],
                "published": "2025-01-15T10:57:55Z",
                "published_parsed": [
                    2025,
                    1,
                    15,
                    10,
                    57,
                    55,
                    2,
                    15,
                    0
                ],
                "title": "The Inherent Limits of Pretrained LLMs: The Unexpected Convergence of\n  Instruction Tuning and In-Context Learning Capabilities",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Inherent Limits of Pretrained LLMs: The Unexpected Convergence of\n  Instruction Tuning and In-Context Learning Capabilities"
                },
                "summary": "Large Language Models (LLMs), trained on extensive web-scale corpora, have\ndemonstrated remarkable abilities across diverse tasks, especially as they are\nscaled up. Nevertheless, even state-of-the-art models struggle in certain\ncases, sometimes failing at problems solvable by young children, indicating\nthat traditional notions of task complexity are insufficient for explaining LLM\ncapabilities. However, exploring LLM capabilities is complicated by the fact\nthat most widely-used models are also \"instruction-tuned\" to respond\nappropriately to prompts. With the goal of disentangling the factors\ninfluencing LLM performance, we investigate whether instruction-tuned models\npossess fundamentally different capabilities from base models that are prompted\nusing in-context examples. Through extensive experiments across various model\nfamilies, scales and task types, which included instruction tuning 90 different\nLLMs, we demonstrate that the performance of instruction-tuned models is\nsignificantly correlated with the in-context performance of their base\ncounterparts. By clarifying what instruction-tuning contributes, we extend\nprior research into in-context learning, which suggests that base models use\npriors from pretraining data to solve tasks. Specifically, we extend this\nunderstanding to instruction-tuned models, suggesting that their pretraining\ndata similarly sets a limiting boundary on the tasks they can solve, with the\nadded influence of the instruction-tuning dataset.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs), trained on extensive web-scale corpora, have\ndemonstrated remarkable abilities across diverse tasks, especially as they are\nscaled up. Nevertheless, even state-of-the-art models struggle in certain\ncases, sometimes failing at problems solvable by young children, indicating\nthat traditional notions of task complexity are insufficient for explaining LLM\ncapabilities. However, exploring LLM capabilities is complicated by the fact\nthat most widely-used models are also \"instruction-tuned\" to respond\nappropriately to prompts. With the goal of disentangling the factors\ninfluencing LLM performance, we investigate whether instruction-tuned models\npossess fundamentally different capabilities from base models that are prompted\nusing in-context examples. Through extensive experiments across various model\nfamilies, scales and task types, which included instruction tuning 90 different\nLLMs, we demonstrate that the performance of instruction-tuned models is\nsignificantly correlated with the in-context performance of their base\ncounterparts. By clarifying what instruction-tuning contributes, we extend\nprior research into in-context learning, which suggests that base models use\npriors from pretraining data to solve tasks. Specifically, we extend this\nunderstanding to instruction-tuned models, suggesting that their pretraining\ndata similarly sets a limiting boundary on the tasks they can solve, with the\nadded influence of the instruction-tuning dataset."
                },
                "authors": [
                    {
                        "name": "Irina Bigoulaeva"
                    },
                    {
                        "name": "Harish Tayyar Madabushi"
                    },
                    {
                        "name": "Iryna Gurevych"
                    }
                ],
                "author_detail": {
                    "name": "Iryna Gurevych"
                },
                "author": "Iryna Gurevych",
                "arxiv_comment": "The code for this paper is available at:\n  https://github.com/UKPLab/arxiv2025-inherent-limits-plms",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.08716v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.08716v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.12935v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.12935v3",
                "updated": "2025-01-15T10:21:30Z",
                "updated_parsed": [
                    2025,
                    1,
                    15,
                    10,
                    21,
                    30,
                    2,
                    15,
                    0
                ],
                "published": "2024-08-23T09:33:48Z",
                "published_parsed": [
                    2024,
                    8,
                    23,
                    9,
                    33,
                    48,
                    4,
                    236,
                    0
                ],
                "title": "Trustworthy, Responsible, and Safe AI: A Comprehensive Architectural\n  Framework for AI Safety with Challenges and Mitigations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Trustworthy, Responsible, and Safe AI: A Comprehensive Architectural\n  Framework for AI Safety with Challenges and Mitigations"
                },
                "summary": "AI Safety is an emerging area of critical importance to the safe adoption and\ndeployment of AI systems. With the rapid proliferation of AI and especially\nwith the recent advancement of Generative AI (or GAI), the technology ecosystem\nbehind the design, development, adoption, and deployment of AI systems has\ndrastically changed, broadening the scope of AI Safety to address impacts on\npublic safety and national security. In this paper, we propose a novel\narchitectural framework for understanding and analyzing AI Safety; defining its\ncharacteristics from three perspectives: Trustworthy AI, Responsible AI, and\nSafe AI. We provide an extensive review of current research and advancements in\nAI safety from these perspectives, highlighting their key challenges and\nmitigation approaches. Through examples from state-of-the-art technologies,\nparticularly Large Language Models (LLMs), we present innovative mechanism,\nmethodologies, and techniques for designing and testing AI safety. Our goal is\nto promote advancement in AI safety research, and ultimately enhance people's\ntrust in digital transformation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AI Safety is an emerging area of critical importance to the safe adoption and\ndeployment of AI systems. With the rapid proliferation of AI and especially\nwith the recent advancement of Generative AI (or GAI), the technology ecosystem\nbehind the design, development, adoption, and deployment of AI systems has\ndrastically changed, broadening the scope of AI Safety to address impacts on\npublic safety and national security. In this paper, we propose a novel\narchitectural framework for understanding and analyzing AI Safety; defining its\ncharacteristics from three perspectives: Trustworthy AI, Responsible AI, and\nSafe AI. We provide an extensive review of current research and advancements in\nAI safety from these perspectives, highlighting their key challenges and\nmitigation approaches. Through examples from state-of-the-art technologies,\nparticularly Large Language Models (LLMs), we present innovative mechanism,\nmethodologies, and techniques for designing and testing AI safety. Our goal is\nto promote advancement in AI safety research, and ultimately enhance people's\ntrust in digital transformation."
                },
                "authors": [
                    {
                        "name": "Chen Chen"
                    },
                    {
                        "name": "Xueluan Gong"
                    },
                    {
                        "name": "Ziyao Liu"
                    },
                    {
                        "name": "Weifeng Jiang"
                    },
                    {
                        "name": "Si Qi Goh"
                    },
                    {
                        "name": "Kwok-Yan Lam"
                    }
                ],
                "author_detail": {
                    "name": "Kwok-Yan Lam"
                },
                "author": "Kwok-Yan Lam",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.12935v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.12935v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.08686v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.08686v1",
                "updated": "2025-01-15T09:32:37Z",
                "updated_parsed": [
                    2025,
                    1,
                    15,
                    9,
                    32,
                    37,
                    2,
                    15,
                    0
                ],
                "published": "2025-01-15T09:32:37Z",
                "published_parsed": [
                    2025,
                    1,
                    15,
                    9,
                    32,
                    37,
                    2,
                    15,
                    0
                ],
                "title": "Knowledge Graph-based Retrieval-Augmented Generation for Schema Matching",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Knowledge Graph-based Retrieval-Augmented Generation for Schema Matching"
                },
                "summary": "Traditional similarity-based schema matching methods are incapable of\nresolving semantic ambiguities and conflicts in domain-specific complex mapping\nscenarios due to missing commonsense and domain-specific knowledge. The\nhallucination problem of large language models (LLMs) also makes it challenging\nfor LLM-based schema matching to address the above issues. Therefore, we\npropose a Knowledge Graph-based Retrieval-Augmented Generation model for Schema\nMatching, referred to as the KG-RAG4SM. In particular, KG-RAG4SM introduces\nnovel vector-based, graph traversal-based, and query-based graph retrievals, as\nwell as a hybrid approach and ranking schemes that identify the most relevant\nsubgraphs from external large knowledge graphs (KGs). We showcase that KG-based\nretrieval-augmented LLMs are capable of generating more accurate results for\ncomplex matching cases without any re-training. Our experimental results show\nthat KG-RAG4SM outperforms the LLM-based state-of-the-art (SOTA) methods (e.g.,\nJellyfish-8B) by 35.89% and 30.50% in terms of precision and F1 score on the\nMIMIC dataset, respectively; KG-RAG4SM with GPT-4o-mini outperforms the\npre-trained language model (PLM)-based SOTA methods (e.g., SMAT) by 69.20% and\n21.97% in terms of precision and F1 score on the Synthea dataset, respectively.\nThe results also demonstrate that our approach is more efficient in end-to-end\nschema matching, and scales to retrieve from large KGs. Our case studies on the\ndataset from the real-world schema matching scenario exhibit that the\nhallucination problem of LLMs for schema matching is well mitigated by our\nsolution.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Traditional similarity-based schema matching methods are incapable of\nresolving semantic ambiguities and conflicts in domain-specific complex mapping\nscenarios due to missing commonsense and domain-specific knowledge. The\nhallucination problem of large language models (LLMs) also makes it challenging\nfor LLM-based schema matching to address the above issues. Therefore, we\npropose a Knowledge Graph-based Retrieval-Augmented Generation model for Schema\nMatching, referred to as the KG-RAG4SM. In particular, KG-RAG4SM introduces\nnovel vector-based, graph traversal-based, and query-based graph retrievals, as\nwell as a hybrid approach and ranking schemes that identify the most relevant\nsubgraphs from external large knowledge graphs (KGs). We showcase that KG-based\nretrieval-augmented LLMs are capable of generating more accurate results for\ncomplex matching cases without any re-training. Our experimental results show\nthat KG-RAG4SM outperforms the LLM-based state-of-the-art (SOTA) methods (e.g.,\nJellyfish-8B) by 35.89% and 30.50% in terms of precision and F1 score on the\nMIMIC dataset, respectively; KG-RAG4SM with GPT-4o-mini outperforms the\npre-trained language model (PLM)-based SOTA methods (e.g., SMAT) by 69.20% and\n21.97% in terms of precision and F1 score on the Synthea dataset, respectively.\nThe results also demonstrate that our approach is more efficient in end-to-end\nschema matching, and scales to retrieve from large KGs. Our case studies on the\ndataset from the real-world schema matching scenario exhibit that the\nhallucination problem of LLMs for schema matching is well mitigated by our\nsolution."
                },
                "authors": [
                    {
                        "name": "Chuangtao Ma"
                    },
                    {
                        "name": "Sriom Chakrabarti"
                    },
                    {
                        "name": "Arijit Khan"
                    },
                    {
                        "name": "Bálint Molnár"
                    }
                ],
                "author_detail": {
                    "name": "Bálint Molnár"
                },
                "author": "Bálint Molnár",
                "arxiv_comment": "Under Review",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.08686v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.08686v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.15512v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.15512v3",
                "updated": "2025-01-15T09:12:02Z",
                "updated_parsed": [
                    2025,
                    1,
                    15,
                    9,
                    12,
                    2,
                    2,
                    15,
                    0
                ],
                "published": "2024-08-28T03:48:05Z",
                "published_parsed": [
                    2024,
                    8,
                    28,
                    3,
                    48,
                    5,
                    2,
                    241,
                    0
                ],
                "title": "Toward Automated Simulation Research Workflow through LLM Prompt\n  Engineering Design",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Toward Automated Simulation Research Workflow through LLM Prompt\n  Engineering Design"
                },
                "summary": "The advent of Large Language Models (LLMs) has created new opportunities for\nthe automation of scientific research spanning both experimental processes and\ncomputational simulations. This study explores the feasibility of constructing\nan autonomous simulation agent (ASA) powered by LLMs through prompt engineering\nand automated program design to automate the entire simulation research process\naccording to a human-provided research plan. This process includes experimental\ndesign, remote upload and simulation execution, data analysis, and report\ncompilation. Using a well-studied simulation problem of polymer chain\nconformations as a test case, we assessed the long-task completion and\nreliability of ASAs powered by different LLMs, including GPT-4o, Claude-3.5,\netc. Our findings revealed that ASA-GPT-4o achieved near-flawless execution on\ndesignated research missions, underscoring the potential of methods like ASA to\nachieve automation in simulation research processes to enhance research\nefficiency. The outlined automation can be iteratively performed for up to 20\ncycles without human intervention, illustrating the potential of ASA for\nlong-task workflow automation. Additionally, we discussed the intrinsic traits\nof ASA in managing extensive tasks, focusing on self-validation mechanisms, and\nthe balance between local attention and global oversight.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The advent of Large Language Models (LLMs) has created new opportunities for\nthe automation of scientific research spanning both experimental processes and\ncomputational simulations. This study explores the feasibility of constructing\nan autonomous simulation agent (ASA) powered by LLMs through prompt engineering\nand automated program design to automate the entire simulation research process\naccording to a human-provided research plan. This process includes experimental\ndesign, remote upload and simulation execution, data analysis, and report\ncompilation. Using a well-studied simulation problem of polymer chain\nconformations as a test case, we assessed the long-task completion and\nreliability of ASAs powered by different LLMs, including GPT-4o, Claude-3.5,\netc. Our findings revealed that ASA-GPT-4o achieved near-flawless execution on\ndesignated research missions, underscoring the potential of methods like ASA to\nachieve automation in simulation research processes to enhance research\nefficiency. The outlined automation can be iteratively performed for up to 20\ncycles without human intervention, illustrating the potential of ASA for\nlong-task workflow automation. Additionally, we discussed the intrinsic traits\nof ASA in managing extensive tasks, focusing on self-validation mechanisms, and\nthe balance between local attention and global oversight."
                },
                "authors": [
                    {
                        "name": "Zhihan Liu"
                    },
                    {
                        "name": "Yubo Chai"
                    },
                    {
                        "name": "Jianfeng Li"
                    }
                ],
                "author_detail": {
                    "name": "Jianfeng Li"
                },
                "author": "Jianfeng Li",
                "arxiv_doi": "10.1021/acs.jcim.4c01653",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1021/acs.jcim.4c01653",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2408.15512v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.15512v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "The source code and example results of ASA can be found at\n  https://github.com/zokaraa/autonomous_simulation_agent",
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "physics.chem-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.08672v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.08672v1",
                "updated": "2025-01-15T09:04:56Z",
                "updated_parsed": [
                    2025,
                    1,
                    15,
                    9,
                    4,
                    56,
                    2,
                    15,
                    0
                ],
                "published": "2025-01-15T09:04:56Z",
                "published_parsed": [
                    2025,
                    1,
                    15,
                    9,
                    4,
                    56,
                    2,
                    15,
                    0
                ],
                "title": "GS-LIVO: Real-Time LiDAR, Inertial, and Visual Multi-sensor Fused\n  Odometry with Gaussian Mapping",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GS-LIVO: Real-Time LiDAR, Inertial, and Visual Multi-sensor Fused\n  Odometry with Gaussian Mapping"
                },
                "summary": "In recent years, 3D Gaussian splatting (3D-GS) has emerged as a novel scene\nrepresentation approach. However, existing vision-only 3D-GS methods often rely\non hand-crafted heuristics for point-cloud densification and face challenges in\nhandling occlusions and high GPU memory and computation consumption.\nLiDAR-Inertial-Visual (LIV) sensor configuration has demonstrated superior\nperformance in localization and dense mapping by leveraging complementary\nsensing characteristics: rich texture information from cameras, precise\ngeometric measurements from LiDAR, and high-frequency motion data from IMU.\nInspired by this, we propose a novel real-time Gaussian-based simultaneous\nlocalization and mapping (SLAM) system. Our map system comprises a global\nGaussian map and a sliding window of Gaussians, along with an IESKF-based\nodometry. The global Gaussian map consists of hash-indexed voxels organized in\na recursive octree, effectively covering sparse spatial volumes while adapting\nto different levels of detail and scales. The Gaussian map is initialized\nthrough multi-sensor fusion and optimized with photometric gradients. Our\nsystem incrementally maintains a sliding window of Gaussians, significantly\nreducing GPU computation and memory consumption by only optimizing the map\nwithin the sliding window. Moreover, we implement a tightly coupled\nmulti-sensor fusion odometry with an iterative error state Kalman filter\n(IESKF), leveraging real-time updating and rendering of the Gaussian map. Our\nsystem represents the first real-time Gaussian-based SLAM framework deployable\non resource-constrained embedded systems, demonstrated on the NVIDIA Jetson\nOrin NX platform. The framework achieves real-time performance while\nmaintaining robust multi-sensor fusion capabilities. All implementation\nalgorithms, hardware designs, and CAD models will be publicly available.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In recent years, 3D Gaussian splatting (3D-GS) has emerged as a novel scene\nrepresentation approach. However, existing vision-only 3D-GS methods often rely\non hand-crafted heuristics for point-cloud densification and face challenges in\nhandling occlusions and high GPU memory and computation consumption.\nLiDAR-Inertial-Visual (LIV) sensor configuration has demonstrated superior\nperformance in localization and dense mapping by leveraging complementary\nsensing characteristics: rich texture information from cameras, precise\ngeometric measurements from LiDAR, and high-frequency motion data from IMU.\nInspired by this, we propose a novel real-time Gaussian-based simultaneous\nlocalization and mapping (SLAM) system. Our map system comprises a global\nGaussian map and a sliding window of Gaussians, along with an IESKF-based\nodometry. The global Gaussian map consists of hash-indexed voxels organized in\na recursive octree, effectively covering sparse spatial volumes while adapting\nto different levels of detail and scales. The Gaussian map is initialized\nthrough multi-sensor fusion and optimized with photometric gradients. Our\nsystem incrementally maintains a sliding window of Gaussians, significantly\nreducing GPU computation and memory consumption by only optimizing the map\nwithin the sliding window. Moreover, we implement a tightly coupled\nmulti-sensor fusion odometry with an iterative error state Kalman filter\n(IESKF), leveraging real-time updating and rendering of the Gaussian map. Our\nsystem represents the first real-time Gaussian-based SLAM framework deployable\non resource-constrained embedded systems, demonstrated on the NVIDIA Jetson\nOrin NX platform. The framework achieves real-time performance while\nmaintaining robust multi-sensor fusion capabilities. All implementation\nalgorithms, hardware designs, and CAD models will be publicly available."
                },
                "authors": [
                    {
                        "name": "Sheng Hong"
                    },
                    {
                        "name": "Chunran Zheng"
                    },
                    {
                        "name": "Yishu Shen"
                    },
                    {
                        "name": "Changze Li"
                    },
                    {
                        "name": "Fu Zhang"
                    },
                    {
                        "name": "Tong Qin"
                    },
                    {
                        "name": "Shaojie Shen"
                    }
                ],
                "author_detail": {
                    "name": "Shaojie Shen"
                },
                "author": "Shaojie Shen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.08672v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.08672v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.08670v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.08670v1",
                "updated": "2025-01-15T09:04:30Z",
                "updated_parsed": [
                    2025,
                    1,
                    15,
                    9,
                    4,
                    30,
                    2,
                    15,
                    0
                ],
                "published": "2025-01-15T09:04:30Z",
                "published_parsed": [
                    2025,
                    1,
                    15,
                    9,
                    4,
                    30,
                    2,
                    15,
                    0
                ],
                "title": "Augmenting Smart Contract Decompiler Output through Fine-grained\n  Dependency Analysis and LLM-facilitated Semantic Recovery",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Augmenting Smart Contract Decompiler Output through Fine-grained\n  Dependency Analysis and LLM-facilitated Semantic Recovery"
                },
                "summary": "Decompiler is a specialized type of reverse engineering tool extensively\nemployed in program analysis tasks, particularly in program comprehension and\nvulnerability detection. However, current Solidity smart contract decompilers\nface significant limitations in reconstructing the original source code. In\nparticular, the bottleneck of SOTA decompilers lies in inaccurate method\nidentification, incorrect variable type recovery, and missing contract\nattributes. These deficiencies hinder downstream tasks and understanding of the\nprogram logic. To address these challenges, we propose SmartHalo, a new\nframework that enhances decompiler output by combining static analysis (SA) and\nlarge language models (LLM). SmartHalo leverages the complementary strengths of\nSA's accuracy in control and data flow analysis and LLM's capability in\nsemantic prediction. More specifically, \\system{} constructs a new data\nstructure - Dependency Graph (DG), to extract semantic dependencies via static\nanalysis. Then, it takes DG to create prompts for LLM optimization. Finally,\nthe correctness of LLM outputs is validated through symbolic execution and\nformal verification. Evaluation on a dataset consisting of 465 randomly\nselected smart contract methods shows that SmartHalo significantly improves the\nquality of the decompiled code, compared to SOTA decompilers (e.g., Gigahorse).\nNotably, integrating GPT-4o with SmartHalo further enhances its performance,\nachieving precision rates of 87.39% for method boundaries, 90.39% for variable\ntypes, and 80.65% for contract attributes.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Decompiler is a specialized type of reverse engineering tool extensively\nemployed in program analysis tasks, particularly in program comprehension and\nvulnerability detection. However, current Solidity smart contract decompilers\nface significant limitations in reconstructing the original source code. In\nparticular, the bottleneck of SOTA decompilers lies in inaccurate method\nidentification, incorrect variable type recovery, and missing contract\nattributes. These deficiencies hinder downstream tasks and understanding of the\nprogram logic. To address these challenges, we propose SmartHalo, a new\nframework that enhances decompiler output by combining static analysis (SA) and\nlarge language models (LLM). SmartHalo leverages the complementary strengths of\nSA's accuracy in control and data flow analysis and LLM's capability in\nsemantic prediction. More specifically, \\system{} constructs a new data\nstructure - Dependency Graph (DG), to extract semantic dependencies via static\nanalysis. Then, it takes DG to create prompts for LLM optimization. Finally,\nthe correctness of LLM outputs is validated through symbolic execution and\nformal verification. Evaluation on a dataset consisting of 465 randomly\nselected smart contract methods shows that SmartHalo significantly improves the\nquality of the decompiled code, compared to SOTA decompilers (e.g., Gigahorse).\nNotably, integrating GPT-4o with SmartHalo further enhances its performance,\nachieving precision rates of 87.39% for method boundaries, 90.39% for variable\ntypes, and 80.65% for contract attributes."
                },
                "authors": [
                    {
                        "name": "Zeqin Liao"
                    },
                    {
                        "name": "Yuhong Nan"
                    },
                    {
                        "name": "Zixu Gao"
                    },
                    {
                        "name": "Henglong Liang"
                    },
                    {
                        "name": "Sicheng Hao"
                    },
                    {
                        "name": "Peifan Reng"
                    },
                    {
                        "name": "Zibin Zheng"
                    }
                ],
                "author_detail": {
                    "name": "Zibin Zheng"
                },
                "author": "Zibin Zheng",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.08670v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.08670v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.08284v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.08284v2",
                "updated": "2025-01-15T08:55:50Z",
                "updated_parsed": [
                    2025,
                    1,
                    15,
                    8,
                    55,
                    50,
                    2,
                    15,
                    0
                ],
                "published": "2025-01-14T18:00:07Z",
                "published_parsed": [
                    2025,
                    1,
                    14,
                    18,
                    0,
                    7,
                    1,
                    14,
                    0
                ],
                "title": "AfriHate: A Multilingual Collection of Hate Speech and Abusive Language\n  Datasets for African Languages",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AfriHate: A Multilingual Collection of Hate Speech and Abusive Language\n  Datasets for African Languages"
                },
                "summary": "Hate speech and abusive language are global phenomena that need\nsocio-cultural background knowledge to be understood, identified, and\nmoderated. However, in many regions of the Global South, there have been\nseveral documented occurrences of (1) absence of moderation and (2) censorship\ndue to the reliance on keyword spotting out of context. Further, high-profile\nindividuals have frequently been at the center of the moderation process, while\nlarge and targeted hate speech campaigns against minorities have been\noverlooked. These limitations are mainly due to the lack of high-quality data\nin the local languages and the failure to include local communities in the\ncollection, annotation, and moderation processes. To address this issue, we\npresent AfriHate: a multilingual collection of hate speech and abusive language\ndatasets in 15 African languages. Each instance in AfriHate is annotated by\nnative speakers familiar with the local culture. We report the challenges\nrelated to the construction of the datasets and present various classification\nbaseline results with and without using LLMs. The datasets, individual\nannotations, and hate speech and offensive language lexicons are available on\nhttps://github.com/AfriHate/AfriHate",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Hate speech and abusive language are global phenomena that need\nsocio-cultural background knowledge to be understood, identified, and\nmoderated. However, in many regions of the Global South, there have been\nseveral documented occurrences of (1) absence of moderation and (2) censorship\ndue to the reliance on keyword spotting out of context. Further, high-profile\nindividuals have frequently been at the center of the moderation process, while\nlarge and targeted hate speech campaigns against minorities have been\noverlooked. These limitations are mainly due to the lack of high-quality data\nin the local languages and the failure to include local communities in the\ncollection, annotation, and moderation processes. To address this issue, we\npresent AfriHate: a multilingual collection of hate speech and abusive language\ndatasets in 15 African languages. Each instance in AfriHate is annotated by\nnative speakers familiar with the local culture. We report the challenges\nrelated to the construction of the datasets and present various classification\nbaseline results with and without using LLMs. The datasets, individual\nannotations, and hate speech and offensive language lexicons are available on\nhttps://github.com/AfriHate/AfriHate"
                },
                "authors": [
                    {
                        "name": "Shamsuddeen Hassan Muhammad"
                    },
                    {
                        "name": "Idris Abdulmumin"
                    },
                    {
                        "name": "Abinew Ali Ayele"
                    },
                    {
                        "name": "David Ifeoluwa Adelani"
                    },
                    {
                        "name": "Ibrahim Said Ahmad"
                    },
                    {
                        "name": "Saminu Mohammad Aliyu"
                    },
                    {
                        "name": "Nelson Odhiambo Onyango"
                    },
                    {
                        "name": "Lilian D. A. Wanzare"
                    },
                    {
                        "name": "Samuel Rutunda"
                    },
                    {
                        "name": "Lukman Jibril Aliyu"
                    },
                    {
                        "name": "Esubalew Alemneh"
                    },
                    {
                        "name": "Oumaima Hourrane"
                    },
                    {
                        "name": "Hagos Tesfahun Gebremichael"
                    },
                    {
                        "name": "Elyas Abdi Ismail"
                    },
                    {
                        "name": "Meriem Beloucif"
                    },
                    {
                        "name": "Ebrahim Chekol Jibril"
                    },
                    {
                        "name": "Andiswa Bukula"
                    },
                    {
                        "name": "Rooweither Mabuya"
                    },
                    {
                        "name": "Salomey Osei"
                    },
                    {
                        "name": "Abigail Oppong"
                    },
                    {
                        "name": "Tadesse Destaw Belay"
                    },
                    {
                        "name": "Tadesse Kebede Guge"
                    },
                    {
                        "name": "Tesfa Tegegne Asfaw"
                    },
                    {
                        "name": "Chiamaka Ijeoma Chukwuneke"
                    },
                    {
                        "name": "Paul Röttger"
                    },
                    {
                        "name": "Seid Muhie Yimam"
                    },
                    {
                        "name": "Nedjma Ousidhoum"
                    }
                ],
                "author_detail": {
                    "name": "Nedjma Ousidhoum"
                },
                "author": "Nedjma Ousidhoum",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.08284v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.08284v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.08648v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.08648v1",
                "updated": "2025-01-15T08:24:03Z",
                "updated_parsed": [
                    2025,
                    1,
                    15,
                    8,
                    24,
                    3,
                    2,
                    15,
                    0
                ],
                "published": "2025-01-15T08:24:03Z",
                "published_parsed": [
                    2025,
                    1,
                    15,
                    8,
                    24,
                    3,
                    2,
                    15,
                    0
                ],
                "title": "MAGNET: Augmenting Generative Decoders with Representation Learning and\n  Infilling Capabilities",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MAGNET: Augmenting Generative Decoders with Representation Learning and\n  Infilling Capabilities"
                },
                "summary": "While originally designed for unidirectional generative modeling,\ndecoder-only large language models (LLMs) are increasingly being adapted for\nbidirectional modeling. However, unidirectional and bidirectional models are\ntypically trained separately with distinct objectives (generation and\nrepresentation learning, respectively). This separation overlooks the\nopportunity for developing a more versatile language model and for these\nobjectives to complement each other. In this work, we introduce MAGNET, an\nadaptation of decoder-only LLMs that enhances their ability to generate robust\nrepresentations and infill missing text spans, while preserving their knowledge\nand text generation capabilities. MAGNET employs three self-supervised training\nobjectives and introduces an attention mechanism that combines bidirectional\nand causal attention, enabling unified training across all objectives. Our\nresults demonstrate that LLMs adapted with MAGNET (1) surpass strong text\nencoders on token-level and sentence-level representation learning tasks, (2)\ngenerate contextually appropriate text infills by leveraging future context,\n(3) retain the ability for open-ended text generation without exhibiting\nrepetition problem, and (4) preserve the knowledge gained by the LLM during\npretraining.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While originally designed for unidirectional generative modeling,\ndecoder-only large language models (LLMs) are increasingly being adapted for\nbidirectional modeling. However, unidirectional and bidirectional models are\ntypically trained separately with distinct objectives (generation and\nrepresentation learning, respectively). This separation overlooks the\nopportunity for developing a more versatile language model and for these\nobjectives to complement each other. In this work, we introduce MAGNET, an\nadaptation of decoder-only LLMs that enhances their ability to generate robust\nrepresentations and infill missing text spans, while preserving their knowledge\nand text generation capabilities. MAGNET employs three self-supervised training\nobjectives and introduces an attention mechanism that combines bidirectional\nand causal attention, enabling unified training across all objectives. Our\nresults demonstrate that LLMs adapted with MAGNET (1) surpass strong text\nencoders on token-level and sentence-level representation learning tasks, (2)\ngenerate contextually appropriate text infills by leveraging future context,\n(3) retain the ability for open-ended text generation without exhibiting\nrepetition problem, and (4) preserve the knowledge gained by the LLM during\npretraining."
                },
                "authors": [
                    {
                        "name": "Savya Khosla"
                    },
                    {
                        "name": "Kushal Kafle"
                    },
                    {
                        "name": "Simon Jenni"
                    },
                    {
                        "name": "Handong Zhao"
                    },
                    {
                        "name": "John Collomosse"
                    },
                    {
                        "name": "Jing Shi"
                    }
                ],
                "author_detail": {
                    "name": "Jing Shi"
                },
                "author": "Jing Shi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.08648v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.08648v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2402.16705v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2402.16705v2",
                "updated": "2025-01-15T08:20:19Z",
                "updated_parsed": [
                    2025,
                    1,
                    15,
                    8,
                    20,
                    19,
                    2,
                    15,
                    0
                ],
                "published": "2024-02-26T16:21:53Z",
                "published_parsed": [
                    2024,
                    2,
                    26,
                    16,
                    21,
                    53,
                    0,
                    57,
                    0
                ],
                "title": "SelectIT: Selective Instruction Tuning for LLMs via Uncertainty-Aware\n  Self-Reflection",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SelectIT: Selective Instruction Tuning for LLMs via Uncertainty-Aware\n  Self-Reflection"
                },
                "summary": "Instruction tuning (IT) is crucial to tailoring large language models (LLMs)\ntowards human-centric interactions. Recent advancements have shown that the\ncareful selection of a small, high-quality subset of IT data can significantly\nenhance the performance of LLMs. Despite this, common approaches often rely on\nadditional models or data, which increases costs and limits widespread\nadoption. In this work, we propose a novel approach, termed SelectIT, that\ncapitalizes on the foundational capabilities of the LLM itself. Specifically,\nwe exploit the intrinsic uncertainty present in LLMs to more effectively select\nhigh-quality IT data, without the need for extra resources. Furthermore, we\nintroduce a curated IT dataset, the Selective Alpaca, created by applying\nSelectIT to the Alpaca-GPT4 dataset. Empirical results demonstrate that IT\nusing Selective Alpaca leads to substantial model ability enhancement. The\nrobustness of SelectIT has also been corroborated in various foundation models\nand domain-specific tasks. Our findings suggest that longer and more\ncomputationally intensive IT data may serve as superior sources of IT, offering\nvaluable insights for future research in this area. Data, code, and scripts are\nfreely available at https://github.com/Blue-Raincoat/SelectIT.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Instruction tuning (IT) is crucial to tailoring large language models (LLMs)\ntowards human-centric interactions. Recent advancements have shown that the\ncareful selection of a small, high-quality subset of IT data can significantly\nenhance the performance of LLMs. Despite this, common approaches often rely on\nadditional models or data, which increases costs and limits widespread\nadoption. In this work, we propose a novel approach, termed SelectIT, that\ncapitalizes on the foundational capabilities of the LLM itself. Specifically,\nwe exploit the intrinsic uncertainty present in LLMs to more effectively select\nhigh-quality IT data, without the need for extra resources. Furthermore, we\nintroduce a curated IT dataset, the Selective Alpaca, created by applying\nSelectIT to the Alpaca-GPT4 dataset. Empirical results demonstrate that IT\nusing Selective Alpaca leads to substantial model ability enhancement. The\nrobustness of SelectIT has also been corroborated in various foundation models\nand domain-specific tasks. Our findings suggest that longer and more\ncomputationally intensive IT data may serve as superior sources of IT, offering\nvaluable insights for future research in this area. Data, code, and scripts are\nfreely available at https://github.com/Blue-Raincoat/SelectIT."
                },
                "authors": [
                    {
                        "name": "Liangxin Liu"
                    },
                    {
                        "name": "Xuebo Liu"
                    },
                    {
                        "name": "Derek F. Wong"
                    },
                    {
                        "name": "Dongfang Li"
                    },
                    {
                        "name": "Ziyi Wang"
                    },
                    {
                        "name": "Baotian Hu"
                    },
                    {
                        "name": "Min Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Min Zhang"
                },
                "author": "Min Zhang",
                "arxiv_comment": "Accepted to NeurIPS 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2402.16705v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2402.16705v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.12117v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.12117v3",
                "updated": "2025-01-15T08:03:55Z",
                "updated_parsed": [
                    2025,
                    1,
                    15,
                    8,
                    3,
                    55,
                    2,
                    15,
                    0
                ],
                "published": "2024-07-16T18:59:49Z",
                "published_parsed": [
                    2024,
                    7,
                    16,
                    18,
                    59,
                    49,
                    1,
                    198,
                    0
                ],
                "title": "MEMO: Fine-grained Tensor Management For Ultra-long Context LLM Training",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MEMO: Fine-grained Tensor Management For Ultra-long Context LLM Training"
                },
                "summary": "Nowadays, Large Language Models (LLMs) have been trained using extended\ncontext lengths to foster more creative applications. However, long context\ntraining poses great challenges considering the constraint of GPU memory. It\nnot only leads to substantial activation memory consumption during training,\nbut also incurs considerable memory fragmentation. To facilitate long context\ntraining, existing frameworks have adopted strategies such as recomputation and\nvarious forms of parallelisms. Nevertheless, these techniques rely on redundant\ncomputation or extensive communication, resulting in low Model FLOPS\nUtilization (MFU). In this paper, we propose MEMO, a novel LLM training\nframework designed for fine-grained activation memory management. Given the\nquadratic scaling of computation and linear scaling of memory with sequence\nlengths when using FlashAttention, we offload memory-consuming activations to\nCPU memory after each layer's forward pass and fetch them during the backward\npass. To maximize the swapping of activations without hindering computation,\nand to avoid exhausting limited CPU memory, we implement a token-wise\nactivation recomputation and swapping mechanism. Furthermore, we tackle the\nmemory fragmentation issue by employing a bi-level Mixed Integer Programming\n(MIP) approach, optimizing memory reuse across transformer layers. Empirical\nresults demonstrate that MEMO achieves an average of 1.97x and 1.80x MFU\ncompared to Megatron-LM and DeepSpeed, respectively. This improvement is\nattributed to MEMO's ability to minimize memory fragmentation, reduce\nrecomputation and intensive communication, and circumvent the delays associated\nwith the memory reorganization process due to fragmentation. By leveraging\nfine-grained activation memory management, MEMO facilitates efficient training\nof 7B LLM with 1 million sequence length on just 8 A800 GPUs, achieving an MFU\nof 52.30%.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Nowadays, Large Language Models (LLMs) have been trained using extended\ncontext lengths to foster more creative applications. However, long context\ntraining poses great challenges considering the constraint of GPU memory. It\nnot only leads to substantial activation memory consumption during training,\nbut also incurs considerable memory fragmentation. To facilitate long context\ntraining, existing frameworks have adopted strategies such as recomputation and\nvarious forms of parallelisms. Nevertheless, these techniques rely on redundant\ncomputation or extensive communication, resulting in low Model FLOPS\nUtilization (MFU). In this paper, we propose MEMO, a novel LLM training\nframework designed for fine-grained activation memory management. Given the\nquadratic scaling of computation and linear scaling of memory with sequence\nlengths when using FlashAttention, we offload memory-consuming activations to\nCPU memory after each layer's forward pass and fetch them during the backward\npass. To maximize the swapping of activations without hindering computation,\nand to avoid exhausting limited CPU memory, we implement a token-wise\nactivation recomputation and swapping mechanism. Furthermore, we tackle the\nmemory fragmentation issue by employing a bi-level Mixed Integer Programming\n(MIP) approach, optimizing memory reuse across transformer layers. Empirical\nresults demonstrate that MEMO achieves an average of 1.97x and 1.80x MFU\ncompared to Megatron-LM and DeepSpeed, respectively. This improvement is\nattributed to MEMO's ability to minimize memory fragmentation, reduce\nrecomputation and intensive communication, and circumvent the delays associated\nwith the memory reorganization process due to fragmentation. By leveraging\nfine-grained activation memory management, MEMO facilitates efficient training\nof 7B LLM with 1 million sequence length on just 8 A800 GPUs, achieving an MFU\nof 52.30%."
                },
                "authors": [
                    {
                        "name": "Pinxue Zhao"
                    },
                    {
                        "name": "Hailin Zhang"
                    },
                    {
                        "name": "Fangcheng Fu"
                    },
                    {
                        "name": "Xiaonan Nie"
                    },
                    {
                        "name": "Qibin Liu"
                    },
                    {
                        "name": "Fang Yang"
                    },
                    {
                        "name": "Yuanbo Peng"
                    },
                    {
                        "name": "Dian Jiao"
                    },
                    {
                        "name": "Shuaipeng Li"
                    },
                    {
                        "name": "Jinbao Xue"
                    },
                    {
                        "name": "Yangyu Tao"
                    },
                    {
                        "name": "Bin Cui"
                    }
                ],
                "author_detail": {
                    "name": "Bin Cui"
                },
                "author": "Bin Cui",
                "arxiv_doi": "10.1145/3709703",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3709703",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2407.12117v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.12117v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.06605v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.06605v2",
                "updated": "2025-01-15T08:01:51Z",
                "updated_parsed": [
                    2025,
                    1,
                    15,
                    8,
                    1,
                    51,
                    2,
                    15,
                    0
                ],
                "published": "2025-01-11T18:11:07Z",
                "published_parsed": [
                    2025,
                    1,
                    11,
                    18,
                    11,
                    7,
                    5,
                    11,
                    0
                ],
                "title": "RoboHorizon: An LLM-Assisted Multi-View World Model for Long-Horizon\n  Robotic Manipulation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RoboHorizon: An LLM-Assisted Multi-View World Model for Long-Horizon\n  Robotic Manipulation"
                },
                "summary": "Efficient control in long-horizon robotic manipulation is challenging due to\ncomplex representation and policy learning requirements. Model-based visual\nreinforcement learning (RL) has shown great potential in addressing these\nchallenges but still faces notable limitations, particularly in handling sparse\nrewards and complex visual features in long-horizon environments. To address\nthese limitations, we propose the Recognize-Sense-Plan-Act (RSPA) pipeline for\nlong-horizon tasks and further introduce RoboHorizon, an LLM-assisted\nmulti-view world model tailored for long-horizon robotic manipulation. In\nRoboHorizon, pre-trained LLMs generate dense reward structures for multi-stage\nsub-tasks based on task language instructions, enabling robots to better\nrecognize long-horizon tasks. Keyframe discovery is then integrated into the\nmulti-view masked autoencoder (MAE) architecture to enhance the robot's ability\nto sense critical task sequences, strengthening its multi-stage perception of\nlong-horizon processes. Leveraging these dense rewards and multi-view\nrepresentations, a robotic world model is constructed to efficiently plan\nlong-horizon tasks, enabling the robot to reliably act through RL algorithms.\nExperiments on two representative benchmarks, RLBench and FurnitureBench, show\nthat RoboHorizon outperforms state-of-the-art visual model-based RL methods,\nachieving a 23.35% improvement in task success rates on RLBench's 4\nshort-horizon tasks and a 29.23% improvement on 6 long-horizon tasks from\nRLBench and 3 furniture assembly tasks from FurnitureBench.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient control in long-horizon robotic manipulation is challenging due to\ncomplex representation and policy learning requirements. Model-based visual\nreinforcement learning (RL) has shown great potential in addressing these\nchallenges but still faces notable limitations, particularly in handling sparse\nrewards and complex visual features in long-horizon environments. To address\nthese limitations, we propose the Recognize-Sense-Plan-Act (RSPA) pipeline for\nlong-horizon tasks and further introduce RoboHorizon, an LLM-assisted\nmulti-view world model tailored for long-horizon robotic manipulation. In\nRoboHorizon, pre-trained LLMs generate dense reward structures for multi-stage\nsub-tasks based on task language instructions, enabling robots to better\nrecognize long-horizon tasks. Keyframe discovery is then integrated into the\nmulti-view masked autoencoder (MAE) architecture to enhance the robot's ability\nto sense critical task sequences, strengthening its multi-stage perception of\nlong-horizon processes. Leveraging these dense rewards and multi-view\nrepresentations, a robotic world model is constructed to efficiently plan\nlong-horizon tasks, enabling the robot to reliably act through RL algorithms.\nExperiments on two representative benchmarks, RLBench and FurnitureBench, show\nthat RoboHorizon outperforms state-of-the-art visual model-based RL methods,\nachieving a 23.35% improvement in task success rates on RLBench's 4\nshort-horizon tasks and a 29.23% improvement on 6 long-horizon tasks from\nRLBench and 3 furniture assembly tasks from FurnitureBench."
                },
                "authors": [
                    {
                        "name": "Zixuan Chen"
                    },
                    {
                        "name": "Jing Huo"
                    },
                    {
                        "name": "Yangtao Chen"
                    },
                    {
                        "name": "Yang Gao"
                    }
                ],
                "author_detail": {
                    "name": "Yang Gao"
                },
                "author": "Yang Gao",
                "arxiv_comment": "Under review",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.06605v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.06605v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.04239v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.04239v3",
                "updated": "2025-01-15T07:59:39Z",
                "updated_parsed": [
                    2025,
                    1,
                    15,
                    7,
                    59,
                    39,
                    2,
                    15,
                    0
                ],
                "published": "2025-01-08T02:32:48Z",
                "published_parsed": [
                    2025,
                    1,
                    8,
                    2,
                    32,
                    48,
                    2,
                    8,
                    0
                ],
                "title": "Dynamic Localisation of Spatial-Temporal Graph Neural Network",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Dynamic Localisation of Spatial-Temporal Graph Neural Network"
                },
                "summary": "Spatial-temporal data, fundamental to many intelligent applications, reveals\ndependencies indicating causal links between present measurements at specific\nlocations and historical data at the same or other locations. Within this\ncontext, adaptive spatial-temporal graph neural networks (ASTGNNs) have emerged\nas valuable tools for modelling these dependencies, especially through a\ndata-driven approach rather than pre-defined spatial graphs. While this\napproach offers higher accuracy, it presents increased computational demands.\nAddressing this challenge, this paper delves into the concept of localisation\nwithin ASTGNNs, introducing an innovative perspective that spatial dependencies\nshould be dynamically evolving over time. We introduce \\textit{DynAGS}, a\nlocalised ASTGNN framework aimed at maximising efficiency and accuracy in\ndistributed deployment. This framework integrates dynamic localisation,\ntime-evolving spatial graphs, and personalised localisation, all orchestrated\naround the Dynamic Graph Generator, a light-weighted central module leveraging\ncross attention. The central module can integrate historical information in a\nnode-independent manner to enhance the feature representation of nodes at the\ncurrent moment. This improved feature representation is then used to generate a\ndynamic sparse graph without the need for costly data exchanges, and it\nsupports personalised localisation. Performance assessments across two core\nASTGNN architectures and nine real-world datasets from various applications\nreveal that \\textit{DynAGS} outshines current benchmarks, underscoring that the\ndynamic modelling of spatial dependencies can drastically improve model\nexpressibility, flexibility, and system efficiency, especially in distributed\nsettings.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Spatial-temporal data, fundamental to many intelligent applications, reveals\ndependencies indicating causal links between present measurements at specific\nlocations and historical data at the same or other locations. Within this\ncontext, adaptive spatial-temporal graph neural networks (ASTGNNs) have emerged\nas valuable tools for modelling these dependencies, especially through a\ndata-driven approach rather than pre-defined spatial graphs. While this\napproach offers higher accuracy, it presents increased computational demands.\nAddressing this challenge, this paper delves into the concept of localisation\nwithin ASTGNNs, introducing an innovative perspective that spatial dependencies\nshould be dynamically evolving over time. We introduce \\textit{DynAGS}, a\nlocalised ASTGNN framework aimed at maximising efficiency and accuracy in\ndistributed deployment. This framework integrates dynamic localisation,\ntime-evolving spatial graphs, and personalised localisation, all orchestrated\naround the Dynamic Graph Generator, a light-weighted central module leveraging\ncross attention. The central module can integrate historical information in a\nnode-independent manner to enhance the feature representation of nodes at the\ncurrent moment. This improved feature representation is then used to generate a\ndynamic sparse graph without the need for costly data exchanges, and it\nsupports personalised localisation. Performance assessments across two core\nASTGNN architectures and nine real-world datasets from various applications\nreveal that \\textit{DynAGS} outshines current benchmarks, underscoring that the\ndynamic modelling of spatial dependencies can drastically improve model\nexpressibility, flexibility, and system efficiency, especially in distributed\nsettings."
                },
                "authors": [
                    {
                        "name": "Wenying Duan"
                    },
                    {
                        "name": "Shujun Guo"
                    },
                    {
                        "name": "Wei huang"
                    },
                    {
                        "name": "Hong Rao"
                    },
                    {
                        "name": "Xiaoxi He"
                    }
                ],
                "author_detail": {
                    "name": "Xiaoxi He"
                },
                "author": "Xiaoxi He",
                "arxiv_comment": "This paper was accepted by KDD'25",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.04239v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.04239v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.08631v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.08631v1",
                "updated": "2025-01-15T07:36:19Z",
                "updated_parsed": [
                    2025,
                    1,
                    15,
                    7,
                    36,
                    19,
                    2,
                    15,
                    0
                ],
                "published": "2025-01-15T07:36:19Z",
                "published_parsed": [
                    2025,
                    1,
                    15,
                    7,
                    36,
                    19,
                    2,
                    15,
                    0
                ],
                "title": "SWSC: Shared Weight for Similar Channel in LLM",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SWSC: Shared Weight for Similar Channel in LLM"
                },
                "summary": "Large language models (LLMs) have spurred development in multiple industries.\nHowever, the growing number of their parameters brings substantial storage and\ncomputing burdens, making it essential to explore model compression techniques\nfor parameter reduction and easier deployment. We propose SWSC, an LLM\ncompression method based on the concept of Shared Weight for Similar Channel.\nIt uses the K-Means clustering algorithm to cluster model weights\nchannel-by-channel, generating clusters with highly similar vectors within\neach. A representative vector from each cluster is selected to approximately\nreplace all vectors in the cluster, significantly reducing the number of model\nweight parameters. However, approximate restoration will inevitably cause\ndamage to the performance of the model. To tackle this issue, we perform\nsingular value decomposition on the weight error values before and after\ncompression and retain the larger singular values and their corresponding\nsingular vectors to compensate for the accuracy. The experimental results show\nthat our method can effectively ensure the performance of the compressed LLM\neven under low-precision conditions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have spurred development in multiple industries.\nHowever, the growing number of their parameters brings substantial storage and\ncomputing burdens, making it essential to explore model compression techniques\nfor parameter reduction and easier deployment. We propose SWSC, an LLM\ncompression method based on the concept of Shared Weight for Similar Channel.\nIt uses the K-Means clustering algorithm to cluster model weights\nchannel-by-channel, generating clusters with highly similar vectors within\neach. A representative vector from each cluster is selected to approximately\nreplace all vectors in the cluster, significantly reducing the number of model\nweight parameters. However, approximate restoration will inevitably cause\ndamage to the performance of the model. To tackle this issue, we perform\nsingular value decomposition on the weight error values before and after\ncompression and retain the larger singular values and their corresponding\nsingular vectors to compensate for the accuracy. The experimental results show\nthat our method can effectively ensure the performance of the compressed LLM\neven under low-precision conditions."
                },
                "authors": [
                    {
                        "name": "Binrui Zeng"
                    },
                    {
                        "name": "Yongtao Tang"
                    },
                    {
                        "name": "Xiaodong Liu"
                    },
                    {
                        "name": "Xiaopeng Li"
                    }
                ],
                "author_detail": {
                    "name": "Xiaopeng Li"
                },
                "author": "Xiaopeng Li",
                "arxiv_comment": "5pages, 3 figures, work in progress",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.08631v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.08631v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.08618v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.08618v1",
                "updated": "2025-01-15T06:34:34Z",
                "updated_parsed": [
                    2025,
                    1,
                    15,
                    6,
                    34,
                    34,
                    2,
                    15,
                    0
                ],
                "published": "2025-01-15T06:34:34Z",
                "published_parsed": [
                    2025,
                    1,
                    15,
                    6,
                    34,
                    34,
                    2,
                    15,
                    0
                ],
                "title": "Disjoint Processing Mechanisms of Hierarchical and Linear Grammars in\n  Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Disjoint Processing Mechanisms of Hierarchical and Linear Grammars in\n  Large Language Models"
                },
                "summary": "All natural languages are structured hierarchically. In humans, this\nstructural restriction is neurologically coded: when two grammars are presented\nwith identical vocabularies, brain areas responsible for language processing\nare only sensitive to hierarchical grammars. Using large language models\n(LLMs), we investigate whether such functionally distinct hierarchical\nprocessing regions can arise solely from exposure to large-scale language\ndistributions. We generate inputs using English, Italian, Japanese, or nonce\nwords, varying the underlying grammars to conform to either hierarchical or\nlinear/positional rules. Using these grammars, we first observe that language\nmodels show distinct behaviors on hierarchical versus linearly structured\ninputs. Then, we find that the components responsible for processing\nhierarchical grammars are distinct from those that process linear grammars; we\ncausally verify this in ablation experiments. Finally, we observe that\nhierarchy-selective components are also active on nonce grammars; this suggests\nthat hierarchy sensitivity is not tied to meaning, nor in-distribution inputs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "All natural languages are structured hierarchically. In humans, this\nstructural restriction is neurologically coded: when two grammars are presented\nwith identical vocabularies, brain areas responsible for language processing\nare only sensitive to hierarchical grammars. Using large language models\n(LLMs), we investigate whether such functionally distinct hierarchical\nprocessing regions can arise solely from exposure to large-scale language\ndistributions. We generate inputs using English, Italian, Japanese, or nonce\nwords, varying the underlying grammars to conform to either hierarchical or\nlinear/positional rules. Using these grammars, we first observe that language\nmodels show distinct behaviors on hierarchical versus linearly structured\ninputs. Then, we find that the components responsible for processing\nhierarchical grammars are distinct from those that process linear grammars; we\ncausally verify this in ablation experiments. Finally, we observe that\nhierarchy-selective components are also active on nonce grammars; this suggests\nthat hierarchy sensitivity is not tied to meaning, nor in-distribution inputs."
                },
                "authors": [
                    {
                        "name": "Aruna Sankaranarayanan"
                    },
                    {
                        "name": "Dylan Hadfield-Menell"
                    },
                    {
                        "name": "Aaron Mueller"
                    }
                ],
                "author_detail": {
                    "name": "Aaron Mueller"
                },
                "author": "Aaron Mueller",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.08618v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.08618v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.08613v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.08613v1",
                "updated": "2025-01-15T06:22:35Z",
                "updated_parsed": [
                    2025,
                    1,
                    15,
                    6,
                    22,
                    35,
                    2,
                    15,
                    0
                ],
                "published": "2025-01-15T06:22:35Z",
                "published_parsed": [
                    2025,
                    1,
                    15,
                    6,
                    22,
                    35,
                    2,
                    15,
                    0
                ],
                "title": "Assessing the Alignment of FOL Closeness Metrics with Human Judgement",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Assessing the Alignment of FOL Closeness Metrics with Human Judgement"
                },
                "summary": "The recent successful paradigm of solving logical reasoning problems with\ntool-augmented large language models (LLMs) leverages translation of natural\nlanguage statements into First-Order Logic~(FOL) and external theorem provers.\nHowever, the correctness of FOL statements, comprising operators and text\npredicates, often goes unverified due to the lack of a reliable evaluation\nmetric for comparing generated and ground-truth FOLs. In this paper, we present\na comprehensive study of sensitivity of existing metrics and their alignment\nwith human judgement on FOL evaluation. Using ground-truth FOLs, we carefully\ndesigned various perturbations on the ground-truth to assess metric\nsensitivity. We sample FOL translation candidates for natural language\nstatements and measure the ranking alignment between automatic metrics and\nhuman annotators. Our empirical findings highlight oversensitivity in the\nn-gram metric BLEU for text perturbations, the semantic graph metric Smatch++\nfor structural perturbations, and FOL metric for operator perturbation. We also\nobserve a closer alignment between BertScore and human judgement. Additionally,\nwe show that combining metrics enhances both alignment and sensitivity compared\nto using individual metrics.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The recent successful paradigm of solving logical reasoning problems with\ntool-augmented large language models (LLMs) leverages translation of natural\nlanguage statements into First-Order Logic~(FOL) and external theorem provers.\nHowever, the correctness of FOL statements, comprising operators and text\npredicates, often goes unverified due to the lack of a reliable evaluation\nmetric for comparing generated and ground-truth FOLs. In this paper, we present\na comprehensive study of sensitivity of existing metrics and their alignment\nwith human judgement on FOL evaluation. Using ground-truth FOLs, we carefully\ndesigned various perturbations on the ground-truth to assess metric\nsensitivity. We sample FOL translation candidates for natural language\nstatements and measure the ranking alignment between automatic metrics and\nhuman annotators. Our empirical findings highlight oversensitivity in the\nn-gram metric BLEU for text perturbations, the semantic graph metric Smatch++\nfor structural perturbations, and FOL metric for operator perturbation. We also\nobserve a closer alignment between BertScore and human judgement. Additionally,\nwe show that combining metrics enhances both alignment and sensitivity compared\nto using individual metrics."
                },
                "authors": [
                    {
                        "name": "Ramya Keerthy Thatikonda"
                    },
                    {
                        "name": "Wray Buntine"
                    },
                    {
                        "name": "Ehsan Shareghi"
                    }
                ],
                "author_detail": {
                    "name": "Ehsan Shareghi"
                },
                "author": "Ehsan Shareghi",
                "arxiv_comment": "Code: https://github.com/RamyaKeerthy/AlignmentFOL",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.08613v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.08613v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.20061v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.20061v2",
                "updated": "2025-01-15T06:15:13Z",
                "updated_parsed": [
                    2025,
                    1,
                    15,
                    6,
                    15,
                    13,
                    2,
                    15,
                    0
                ],
                "published": "2024-12-28T07:30:05Z",
                "published_parsed": [
                    2024,
                    12,
                    28,
                    7,
                    30,
                    5,
                    5,
                    363,
                    0
                ],
                "title": "Comparative Analysis of Listwise Reranking with Large Language Models in\n  Limited-Resource Language Contexts",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Comparative Analysis of Listwise Reranking with Large Language Models in\n  Limited-Resource Language Contexts"
                },
                "summary": "Large Language Models (LLMs) have demonstrated significant effectiveness\nacross various NLP tasks, including text ranking. This study assesses the\nperformance of large language models (LLMs) in listwise reranking for\nlimited-resource African languages. We compare proprietary models RankGPT3.5,\nRank4o-mini, RankGPTo1-mini and RankClaude-sonnet in cross-lingual contexts.\nResults indicate that these LLMs significantly outperform traditional baseline\nmethods such as BM25-DT in most evaluation metrics, particularly in nDCG@10 and\nMRR@100. These findings highlight the potential of LLMs in enhancing reranking\ntasks for low-resource languages and offer insights into cost-effective\nsolutions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have demonstrated significant effectiveness\nacross various NLP tasks, including text ranking. This study assesses the\nperformance of large language models (LLMs) in listwise reranking for\nlimited-resource African languages. We compare proprietary models RankGPT3.5,\nRank4o-mini, RankGPTo1-mini and RankClaude-sonnet in cross-lingual contexts.\nResults indicate that these LLMs significantly outperform traditional baseline\nmethods such as BM25-DT in most evaluation metrics, particularly in nDCG@10 and\nMRR@100. These findings highlight the potential of LLMs in enhancing reranking\ntasks for low-resource languages and offer insights into cost-effective\nsolutions."
                },
                "authors": [
                    {
                        "name": "Yanxin Shen"
                    },
                    {
                        "name": "Lun Wang"
                    },
                    {
                        "name": "Chuanqi Shi"
                    },
                    {
                        "name": "Shaoshuai Du"
                    },
                    {
                        "name": "Yiyi Tao"
                    },
                    {
                        "name": "Yixian Shen"
                    },
                    {
                        "name": "Hang Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Hang Zhang"
                },
                "author": "Hang Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.20061v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.20061v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.08603v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.08603v1",
                "updated": "2025-01-15T06:00:50Z",
                "updated_parsed": [
                    2025,
                    1,
                    15,
                    6,
                    0,
                    50,
                    2,
                    15,
                    0
                ],
                "published": "2025-01-15T06:00:50Z",
                "published_parsed": [
                    2025,
                    1,
                    15,
                    6,
                    0,
                    50,
                    2,
                    15,
                    0
                ],
                "title": "Monte Carlo Tree Search for Comprehensive Exploration in LLM-Based\n  Automatic Heuristic Design",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Monte Carlo Tree Search for Comprehensive Exploration in LLM-Based\n  Automatic Heuristic Design"
                },
                "summary": "Handcrafting heuristics for solving complex planning tasks (e.g., NP-hard\ncombinatorial optimization (CO) problems) is a common practice but requires\nextensive domain knowledge. Recently, Large Language Model (LLM)-based\nautomatic heuristics design (AHD) methods have shown promise in generating\nhigh-quality heuristics without manual intervention. Existing LLM-based AHD\nmethods employ a population to maintain a fixed number of top-performing\nLLM-generated heuristics and introduce evolutionary computation (EC) to enhance\nthe population iteratively. However, the population-based procedure brings\ngreedy properties, often resulting in convergence to local optima. Instead, to\nmore comprehensively explore the space of heuristics, we propose using Monte\nCarlo Tree Search (MCTS) for LLM-based heuristic evolution while preserving all\nLLM-generated heuristics in a tree structure. With a novel thought-alignment\nprocess and an exploration-decay technique, the proposed MCTS-AHD method\ndelivers significantly higher-quality heuristics on various complex tasks. Our\ncode is available at https://github.com/zz1358m/MCTS-AHD-master.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Handcrafting heuristics for solving complex planning tasks (e.g., NP-hard\ncombinatorial optimization (CO) problems) is a common practice but requires\nextensive domain knowledge. Recently, Large Language Model (LLM)-based\nautomatic heuristics design (AHD) methods have shown promise in generating\nhigh-quality heuristics without manual intervention. Existing LLM-based AHD\nmethods employ a population to maintain a fixed number of top-performing\nLLM-generated heuristics and introduce evolutionary computation (EC) to enhance\nthe population iteratively. However, the population-based procedure brings\ngreedy properties, often resulting in convergence to local optima. Instead, to\nmore comprehensively explore the space of heuristics, we propose using Monte\nCarlo Tree Search (MCTS) for LLM-based heuristic evolution while preserving all\nLLM-generated heuristics in a tree structure. With a novel thought-alignment\nprocess and an exploration-decay technique, the proposed MCTS-AHD method\ndelivers significantly higher-quality heuristics on various complex tasks. Our\ncode is available at https://github.com/zz1358m/MCTS-AHD-master."
                },
                "authors": [
                    {
                        "name": "Zhi Zheng"
                    },
                    {
                        "name": "Zhuoliang Xie"
                    },
                    {
                        "name": "Zhenkun Wang"
                    },
                    {
                        "name": "Bryan Hooi"
                    }
                ],
                "author_detail": {
                    "name": "Bryan Hooi"
                },
                "author": "Bryan Hooi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.08603v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.08603v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.08600v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.08600v1",
                "updated": "2025-01-15T05:54:33Z",
                "updated_parsed": [
                    2025,
                    1,
                    15,
                    5,
                    54,
                    33,
                    2,
                    15,
                    0
                ],
                "published": "2025-01-15T05:54:33Z",
                "published_parsed": [
                    2025,
                    1,
                    15,
                    5,
                    54,
                    33,
                    2,
                    15,
                    0
                ],
                "title": "AutoRestTest: A Tool for Automated REST API Testing Using LLMs and MARL",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AutoRestTest: A Tool for Automated REST API Testing Using LLMs and MARL"
                },
                "summary": "As REST APIs have become widespread in modern web services, comprehensive\ntesting of these APIs has become increasingly crucial. Due to the vast search\nspace consisting of operations, parameters, and parameter values along with\ntheir complex dependencies and constraints, current testing tools suffer from\nlow code coverage, leading to suboptimal fault detection. To address this\nlimitation, we present a novel tool, AutoRestTest, which integrates the\nSemantic Operation Dependency Graph (SODG) with Multi-Agent Reinforcement\nLearning (MARL) and large language models (LLMs) for effective REST API\ntesting. AutoRestTest determines operation-dependent parameters using the SODG\nand employs five specialized agents (operation, parameter, value, dependency,\nand header) to identify dependencies of operations and generate operation\nsequences, parameter combinations, and values. AutoRestTest provides a\ncommand-line interface and continuous telemetry on successful operation count,\nunique server errors detected, and time elapsed. Upon completion, AutoRestTest\ngenerates a detailed report highlighting errors detected and operations\nexercised. In this paper, we introduce our tool and present preliminary\nresults.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As REST APIs have become widespread in modern web services, comprehensive\ntesting of these APIs has become increasingly crucial. Due to the vast search\nspace consisting of operations, parameters, and parameter values along with\ntheir complex dependencies and constraints, current testing tools suffer from\nlow code coverage, leading to suboptimal fault detection. To address this\nlimitation, we present a novel tool, AutoRestTest, which integrates the\nSemantic Operation Dependency Graph (SODG) with Multi-Agent Reinforcement\nLearning (MARL) and large language models (LLMs) for effective REST API\ntesting. AutoRestTest determines operation-dependent parameters using the SODG\nand employs five specialized agents (operation, parameter, value, dependency,\nand header) to identify dependencies of operations and generate operation\nsequences, parameter combinations, and values. AutoRestTest provides a\ncommand-line interface and continuous telemetry on successful operation count,\nunique server errors detected, and time elapsed. Upon completion, AutoRestTest\ngenerates a detailed report highlighting errors detected and operations\nexercised. In this paper, we introduce our tool and present preliminary\nresults."
                },
                "authors": [
                    {
                        "name": "Tyler Stennett"
                    },
                    {
                        "name": "Myeongsoo Kim"
                    },
                    {
                        "name": "Saurabh Sinha"
                    },
                    {
                        "name": "Alessandro Orso"
                    }
                ],
                "author_detail": {
                    "name": "Alessandro Orso"
                },
                "author": "Alessandro Orso",
                "arxiv_comment": "To be published in the 47th IEEE/ACM International Conference on\n  Software Engineering - Demonstration Track (ICSE-Demo 2025)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.08600v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.08600v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.08599v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.08599v1",
                "updated": "2025-01-15T05:52:55Z",
                "updated_parsed": [
                    2025,
                    1,
                    15,
                    5,
                    52,
                    55,
                    2,
                    15,
                    0
                ],
                "published": "2025-01-15T05:52:55Z",
                "published_parsed": [
                    2025,
                    1,
                    15,
                    5,
                    52,
                    55,
                    2,
                    15,
                    0
                ],
                "title": "Double reflections Assisted RIS Deployment and Energy-efficient Group\n  Selection in mmWaves D2D Communication",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Double reflections Assisted RIS Deployment and Energy-efficient Group\n  Selection in mmWaves D2D Communication"
                },
                "summary": "Reconfigurable intelligent surfaces (RISs) offer a viable way to improve the\nperformance of multi-hop device-to-device (D2D) communication. However, due to\nthe substantial propagation and penetration losses of the millimeter waves\n(mmWaves), a direct line of sight (LoS) link and close proximity of a device\npair are required for a high data rate. Static obstacles like trees and\nbuildings can easily impede the direct LoS connectivity between a device pair.\nHence, RIS placement plays a crucial role in establishing an indirect LoS link\nbetween them. Therefore, in this work, we propose a set cover-based RIS\ndeployment strategy for both single and double RIS-assisted D2D communication.\nIn particular, we have demonstrated that permitting reflections via two\nconsecutive RISs can greatly lower the RIS density in the environment,\npreventing resource waste and enabling the service of more obstructed device\npairs. After the RIS deployment, for information transfer, we also propose an\nenergy-efficient group selection criteria. Moreover, we prove that sometimes\ndouble reflections are more beneficial than single reflection, which is\ncounter-intuitive. Numerical results show that our approach outperforms a\nrandom and a recent deployment strategy.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reconfigurable intelligent surfaces (RISs) offer a viable way to improve the\nperformance of multi-hop device-to-device (D2D) communication. However, due to\nthe substantial propagation and penetration losses of the millimeter waves\n(mmWaves), a direct line of sight (LoS) link and close proximity of a device\npair are required for a high data rate. Static obstacles like trees and\nbuildings can easily impede the direct LoS connectivity between a device pair.\nHence, RIS placement plays a crucial role in establishing an indirect LoS link\nbetween them. Therefore, in this work, we propose a set cover-based RIS\ndeployment strategy for both single and double RIS-assisted D2D communication.\nIn particular, we have demonstrated that permitting reflections via two\nconsecutive RISs can greatly lower the RIS density in the environment,\npreventing resource waste and enabling the service of more obstructed device\npairs. After the RIS deployment, for information transfer, we also propose an\nenergy-efficient group selection criteria. Moreover, we prove that sometimes\ndouble reflections are more beneficial than single reflection, which is\ncounter-intuitive. Numerical results show that our approach outperforms a\nrandom and a recent deployment strategy."
                },
                "authors": [
                    {
                        "name": "Lakshmikanta Sau"
                    },
                    {
                        "name": "Sasthi C. Ghosh"
                    }
                ],
                "author_detail": {
                    "name": "Sasthi C. Ghosh"
                },
                "author": "Sasthi C. Ghosh",
                "arxiv_comment": "Submitted for a possible journal publication",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.08599v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.08599v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.08598v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.08598v1",
                "updated": "2025-01-15T05:51:20Z",
                "updated_parsed": [
                    2025,
                    1,
                    15,
                    5,
                    51,
                    20,
                    2,
                    15,
                    0
                ],
                "published": "2025-01-15T05:51:20Z",
                "published_parsed": [
                    2025,
                    1,
                    15,
                    5,
                    51,
                    20,
                    2,
                    15,
                    0
                ],
                "title": "LlamaRestTest: Effective REST API Testing with Small Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LlamaRestTest: Effective REST API Testing with Small Language Models"
                },
                "summary": "Modern web services rely heavily on REST APIs, typically documented using the\nOpenAPI specification. The widespread adoption of this standard has resulted in\nthe development of many black-box testing tools that generate tests based on\nthese specifications. Recent advancements in Natural Language Processing (NLP),\nparticularly with Large Language Models (LLMs), have enhanced REST API testing\nby extracting actionable rules and generating input values from the\nhuman-readable portions of the specification. However, these advancements\noverlook the potential of continuously refining the identified rules and test\ninputs based on server responses. To address this limitation, we present\nLlamaRestTest, a novel approach that employs two custom LLMs to generate\nrealistic test inputs and uncover parameter dependencies during the testing\nprocess by incorporating server responses. These LLMs are created by\nfine-tuning the Llama3-8b model, using mined datasets of REST API example\nvalues and inter-parameter dependencies. We evaluated LlamaRestTest on 12\nreal-world services (including popular services such as Spotify), comparing it\nagainst RESTGPT, a GPT-powered specification-enhancement tool, as well as\nseveral state-of-the-art REST API testing tools, including RESTler, MoRest,\nEvoMaster, and ARAT-RL. Our results show that fine-tuning enables smaller LLMs\nto outperform larger models in detecting actionable rules and generating inputs\nfor REST API testing. We evaluated configurations from the base Llama3-8B to\nfine-tuned versions and explored 2-bit, 4-bit, and 8-bit quantization for\nefficiency. LlamaRestTest surpasses state-of-the-art tools in code coverage and\nerror detection, even with RESTGPT-enhanced specifications, and an ablation\nstudy highlights the impact of its novel components.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Modern web services rely heavily on REST APIs, typically documented using the\nOpenAPI specification. The widespread adoption of this standard has resulted in\nthe development of many black-box testing tools that generate tests based on\nthese specifications. Recent advancements in Natural Language Processing (NLP),\nparticularly with Large Language Models (LLMs), have enhanced REST API testing\nby extracting actionable rules and generating input values from the\nhuman-readable portions of the specification. However, these advancements\noverlook the potential of continuously refining the identified rules and test\ninputs based on server responses. To address this limitation, we present\nLlamaRestTest, a novel approach that employs two custom LLMs to generate\nrealistic test inputs and uncover parameter dependencies during the testing\nprocess by incorporating server responses. These LLMs are created by\nfine-tuning the Llama3-8b model, using mined datasets of REST API example\nvalues and inter-parameter dependencies. We evaluated LlamaRestTest on 12\nreal-world services (including popular services such as Spotify), comparing it\nagainst RESTGPT, a GPT-powered specification-enhancement tool, as well as\nseveral state-of-the-art REST API testing tools, including RESTler, MoRest,\nEvoMaster, and ARAT-RL. Our results show that fine-tuning enables smaller LLMs\nto outperform larger models in detecting actionable rules and generating inputs\nfor REST API testing. We evaluated configurations from the base Llama3-8B to\nfine-tuned versions and explored 2-bit, 4-bit, and 8-bit quantization for\nefficiency. LlamaRestTest surpasses state-of-the-art tools in code coverage and\nerror detection, even with RESTGPT-enhanced specifications, and an ablation\nstudy highlights the impact of its novel components."
                },
                "authors": [
                    {
                        "name": "Myeongsoo Kim"
                    },
                    {
                        "name": "Saurabh Sinha"
                    },
                    {
                        "name": "Alessandro Orso"
                    }
                ],
                "author_detail": {
                    "name": "Alessandro Orso"
                },
                "author": "Alessandro Orso",
                "arxiv_comment": "To be published in the ACM International Conference on the\n  Foundations of Software Engineering (FSE 2025)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.08598v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.08598v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2401.16626v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2401.16626v2",
                "updated": "2025-01-15T05:27:47Z",
                "updated_parsed": [
                    2025,
                    1,
                    15,
                    5,
                    27,
                    47,
                    2,
                    15,
                    0
                ],
                "published": "2024-01-29T23:50:40Z",
                "published_parsed": [
                    2024,
                    1,
                    29,
                    23,
                    50,
                    40,
                    0,
                    29,
                    0
                ],
                "title": "Implications of Zoning Ordinances for Rural Utility-Scale Solar\n  Deployment and Power System Decarbonization in the Great Lakes Region",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Implications of Zoning Ordinances for Rural Utility-Scale Solar\n  Deployment and Power System Decarbonization in the Great Lakes Region"
                },
                "summary": "Local zoning ordinances across the United States have the impact of\nrestricting development of energy infrastructure, including utility-scale solar\nphotovoltaics. While these ordinances may be developed for legitimate purposes\nto protect public health and safety, they could impede or increase costs of\npower sector decarbonization. We quantify the role of utility-scale solar\nzoning ordinances on power sector decarbonization across the Great Lakes region\n(Illinois, Indiana, Michigan, Minnesota, Ohio, and Wisconsin) by integrating\n6,300 rural community zoning ordinances into a power system planning model.\nRelative to no ordinances, solar zoning ordinances reduce total potential\ndeployment of solar PV by 52% (or 1.6 TW) across our region. Currently,\nhowever, the biggest zoning barrier to deployment is zoning ordinances which\nare silent on utility-scale solar. Deployment restrictions translate to up to 4\nGW greater investment needs and 5.6% greater PV investment costs to achieve a\n10% PV generation target. Starker shifts occur at the state level, e.g.\nWisconsin sees a 40% reduction in PV investments due to zoning restrictions.\nOur results underscore the need for planning that aligns local zoning laws with\nstate and regional goals.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Local zoning ordinances across the United States have the impact of\nrestricting development of energy infrastructure, including utility-scale solar\nphotovoltaics. While these ordinances may be developed for legitimate purposes\nto protect public health and safety, they could impede or increase costs of\npower sector decarbonization. We quantify the role of utility-scale solar\nzoning ordinances on power sector decarbonization across the Great Lakes region\n(Illinois, Indiana, Michigan, Minnesota, Ohio, and Wisconsin) by integrating\n6,300 rural community zoning ordinances into a power system planning model.\nRelative to no ordinances, solar zoning ordinances reduce total potential\ndeployment of solar PV by 52% (or 1.6 TW) across our region. Currently,\nhowever, the biggest zoning barrier to deployment is zoning ordinances which\nare silent on utility-scale solar. Deployment restrictions translate to up to 4\nGW greater investment needs and 5.6% greater PV investment costs to achieve a\n10% PV generation target. Starker shifts occur at the state level, e.g.\nWisconsin sees a 40% reduction in PV investments due to zoning restrictions.\nOur results underscore the need for planning that aligns local zoning laws with\nstate and regional goals."
                },
                "authors": [
                    {
                        "name": "Papa Yaw Owusu-Obeng"
                    },
                    {
                        "name": "Sarah Banas Mills"
                    },
                    {
                        "name": "Michael T. Craig"
                    }
                ],
                "author_detail": {
                    "name": "Michael T. Craig"
                },
                "author": "Michael T. Craig",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2401.16626v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2401.16626v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.08591v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.08591v1",
                "updated": "2025-01-15T05:20:01Z",
                "updated_parsed": [
                    2025,
                    1,
                    15,
                    5,
                    20,
                    1,
                    2,
                    15,
                    0
                ],
                "published": "2025-01-15T05:20:01Z",
                "published_parsed": [
                    2025,
                    1,
                    15,
                    5,
                    20,
                    1,
                    2,
                    15,
                    0
                ],
                "title": "OpenMLDB: A Real-Time Relational Data Feature Computation System for\n  Online ML",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "OpenMLDB: A Real-Time Relational Data Feature Computation System for\n  Online ML"
                },
                "summary": "Efficient and consistent feature computation is crucial for a wide range of\nonline ML applications. Typically, feature computation is divided into two\ndistinct phases, i.e., offline stage for model training and online stage for\nmodel serving. These phases often rely on execution engines with different\ninterface languages and function implementations, causing significant\ninconsistencies. Moreover, many online ML features involve complex time-series\ncomputations (e.g., functions over varied-length table windows) that differ\nfrom standard streaming and analytical queries. Existing data processing\nsystems (e.g., Spark, Flink, DuckDB) often incur multi-second latencies for\nthese computations, making them unsuitable for real-time online ML applications\nthat demand timely feature updates.\n  This paper presents OpenMLDB, a feature computation system deployed in\n4Paradigm's SageOne platform and over 100 real scenarios. Technically, OpenMLDB\nfirst employs a unified query plan generator for consistent computation results\nacross the offline and online stages, significantly reducing feature deployment\noverhead. Second, OpenMLDB provides an online execution engine that resolves\nperformance bottlenecks caused by long window computations (via\npre-aggregation) and multi-table window unions (via data self-adjusting). It\nalso provides a high-performance offline execution engine with window parallel\noptimization and time-aware data skew resolving. Third, OpenMLDB features a\ncompact data format and stream-focused indexing to maximize memory usage and\naccelerate data access. Evaluations in testing and real workloads reveal\nsignificant performance improvements and resource savings compared to the\nbaseline systems. The open community of OpenMLDB now has over 150 contributors\nand gained 1.6k stars on GitHub.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient and consistent feature computation is crucial for a wide range of\nonline ML applications. Typically, feature computation is divided into two\ndistinct phases, i.e., offline stage for model training and online stage for\nmodel serving. These phases often rely on execution engines with different\ninterface languages and function implementations, causing significant\ninconsistencies. Moreover, many online ML features involve complex time-series\ncomputations (e.g., functions over varied-length table windows) that differ\nfrom standard streaming and analytical queries. Existing data processing\nsystems (e.g., Spark, Flink, DuckDB) often incur multi-second latencies for\nthese computations, making them unsuitable for real-time online ML applications\nthat demand timely feature updates.\n  This paper presents OpenMLDB, a feature computation system deployed in\n4Paradigm's SageOne platform and over 100 real scenarios. Technically, OpenMLDB\nfirst employs a unified query plan generator for consistent computation results\nacross the offline and online stages, significantly reducing feature deployment\noverhead. Second, OpenMLDB provides an online execution engine that resolves\nperformance bottlenecks caused by long window computations (via\npre-aggregation) and multi-table window unions (via data self-adjusting). It\nalso provides a high-performance offline execution engine with window parallel\noptimization and time-aware data skew resolving. Third, OpenMLDB features a\ncompact data format and stream-focused indexing to maximize memory usage and\naccelerate data access. Evaluations in testing and real workloads reveal\nsignificant performance improvements and resource savings compared to the\nbaseline systems. The open community of OpenMLDB now has over 150 contributors\nand gained 1.6k stars on GitHub."
                },
                "authors": [
                    {
                        "name": "Xuanhe Zhou"
                    },
                    {
                        "name": "Wei Zhou"
                    },
                    {
                        "name": "Liguo Qi"
                    },
                    {
                        "name": "Hao Zhang"
                    },
                    {
                        "name": "Dihao Chen"
                    },
                    {
                        "name": "Bingsheng He"
                    },
                    {
                        "name": "Mian Lu"
                    },
                    {
                        "name": "Guoliang Li"
                    },
                    {
                        "name": "Fan Wu"
                    },
                    {
                        "name": "Yuqiang Chen"
                    }
                ],
                "author_detail": {
                    "name": "Yuqiang Chen"
                },
                "author": "Yuqiang Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.08591v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.08591v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.08586v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.08586v1",
                "updated": "2025-01-15T05:15:01Z",
                "updated_parsed": [
                    2025,
                    1,
                    15,
                    5,
                    15,
                    1,
                    2,
                    15,
                    0
                ],
                "published": "2025-01-15T05:15:01Z",
                "published_parsed": [
                    2025,
                    1,
                    15,
                    5,
                    15,
                    1,
                    2,
                    15,
                    0
                ],
                "title": "First-Ever Deployment of a SiPM-on-Tile Calorimeter in a Collider: A\n  Parasitic Test with 200 GeV $pp$ Collisions at RHIC",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "First-Ever Deployment of a SiPM-on-Tile Calorimeter in a Collider: A\n  Parasitic Test with 200 GeV $pp$ Collisions at RHIC"
                },
                "summary": "We describe the testing of a prototype SiPM-on-tile iron-scintillator\ncalorimeter at the Relativistic Heavy Ion Collider (RHIC) during its 200 GeV\n$pp$ run in 2024. The prototype, measuring $20 \\times 20 \\, \\text{cm}^{2}$ and\n24 radiation lengths in depth, was positioned in the STAR experimental hall,\napproximately 8 m from the interaction point and 65 cm from the beam line,\ncovering a pseudorapidity range of about $3.1<\\eta<3.4$. By using the dark\ncurrent of a reference SiPM as a radiation monitor, we estimate that the\nprototype was exposed to a fluence of about $10^{10}$ 1-MeV\n$n_{\\mathrm{eq}}$/cm$^2$. Channel-by-channel calibration was performed in a\ndata-driven way with the signature from minimum-ionizing particles during\nbeam-on conditions. A Geant4 detector simulation, with inputs from the Pythia8\nevent generator, describes measurements of energy spectra and hit\nmultiplicities reasonably well. These results mark the first deployment,\ncommissioning, calibration, and long-term operation of a SiPM-on-tile\ncalorimeter in a collider environment. This experimental campaign will guide\ndetector designs and operational strategies for the ePIC detector at the future\nEIC, as well as other applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We describe the testing of a prototype SiPM-on-tile iron-scintillator\ncalorimeter at the Relativistic Heavy Ion Collider (RHIC) during its 200 GeV\n$pp$ run in 2024. The prototype, measuring $20 \\times 20 \\, \\text{cm}^{2}$ and\n24 radiation lengths in depth, was positioned in the STAR experimental hall,\napproximately 8 m from the interaction point and 65 cm from the beam line,\ncovering a pseudorapidity range of about $3.1<\\eta<3.4$. By using the dark\ncurrent of a reference SiPM as a radiation monitor, we estimate that the\nprototype was exposed to a fluence of about $10^{10}$ 1-MeV\n$n_{\\mathrm{eq}}$/cm$^2$. Channel-by-channel calibration was performed in a\ndata-driven way with the signature from minimum-ionizing particles during\nbeam-on conditions. A Geant4 detector simulation, with inputs from the Pythia8\nevent generator, describes measurements of energy spectra and hit\nmultiplicities reasonably well. These results mark the first deployment,\ncommissioning, calibration, and long-term operation of a SiPM-on-tile\ncalorimeter in a collider environment. This experimental campaign will guide\ndetector designs and operational strategies for the ePIC detector at the future\nEIC, as well as other applications."
                },
                "authors": [
                    {
                        "name": "Weibin Zhang"
                    },
                    {
                        "name": "Sean Preins"
                    },
                    {
                        "name": "Jiajun Huang"
                    },
                    {
                        "name": "Sebouh J. Paul"
                    },
                    {
                        "name": "Ryan Milton"
                    },
                    {
                        "name": "Miguel Rodriguez"
                    },
                    {
                        "name": "Peter Carney"
                    },
                    {
                        "name": "Ryan Tsiao"
                    },
                    {
                        "name": "Yousef Abdelkadous"
                    },
                    {
                        "name": "Miguel Arratia"
                    }
                ],
                "author_detail": {
                    "name": "Miguel Arratia"
                },
                "author": "Miguel Arratia",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.08586v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.08586v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.ins-det",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.ins-det",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "hep-ex",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "nucl-ex",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.08582v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.08582v1",
                "updated": "2025-01-15T05:07:06Z",
                "updated_parsed": [
                    2025,
                    1,
                    15,
                    5,
                    7,
                    6,
                    2,
                    15,
                    0
                ],
                "published": "2025-01-15T05:07:06Z",
                "published_parsed": [
                    2025,
                    1,
                    15,
                    5,
                    7,
                    6,
                    2,
                    15,
                    0
                ],
                "title": "LoRS: Efficient Low-Rank Adaptation for Sparse Large Language Model",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LoRS: Efficient Low-Rank Adaptation for Sparse Large Language Model"
                },
                "summary": "Existing low-rank adaptation (LoRA) methods face challenges on sparse large\nlanguage models (LLMs) due to the inability to maintain sparsity. Recent works\nintroduced methods that maintain sparsity by augmenting LoRA techniques with\nadditional masking mechanisms. Despite these successes, such approaches suffer\nfrom an increased memory and computation overhead, which affects efficiency of\nLoRA methods. In response to this limitation, we introduce LoRS, an innovative\nmethod designed to achieve both memory and computation efficiency when\nfine-tuning sparse LLMs. To mitigate the substantial memory and computation\ndemands associated with preserving sparsity, our approach incorporates\nstrategies of weight recompute and computational graph rearrangement. In\naddition, we also improve the effectiveness of LoRS through better adapter\ninitialization. These innovations lead to a notable reduction in memory and\ncomputation consumption during the fine-tuning phase, all while achieving\nperformance levels that outperform existing LoRA approaches.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Existing low-rank adaptation (LoRA) methods face challenges on sparse large\nlanguage models (LLMs) due to the inability to maintain sparsity. Recent works\nintroduced methods that maintain sparsity by augmenting LoRA techniques with\nadditional masking mechanisms. Despite these successes, such approaches suffer\nfrom an increased memory and computation overhead, which affects efficiency of\nLoRA methods. In response to this limitation, we introduce LoRS, an innovative\nmethod designed to achieve both memory and computation efficiency when\nfine-tuning sparse LLMs. To mitigate the substantial memory and computation\ndemands associated with preserving sparsity, our approach incorporates\nstrategies of weight recompute and computational graph rearrangement. In\naddition, we also improve the effectiveness of LoRS through better adapter\ninitialization. These innovations lead to a notable reduction in memory and\ncomputation consumption during the fine-tuning phase, all while achieving\nperformance levels that outperform existing LoRA approaches."
                },
                "authors": [
                    {
                        "name": "Yuxuan Hu"
                    },
                    {
                        "name": "Jing Zhang"
                    },
                    {
                        "name": "Xiaodong Chen"
                    },
                    {
                        "name": "Zhe Zhao"
                    },
                    {
                        "name": "Cuiping Li"
                    },
                    {
                        "name": "Hong Chen"
                    }
                ],
                "author_detail": {
                    "name": "Hong Chen"
                },
                "author": "Hong Chen",
                "arxiv_comment": "12 pages, 4 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.08582v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.08582v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.7",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.08579v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.08579v1",
                "updated": "2025-01-15T04:59:49Z",
                "updated_parsed": [
                    2025,
                    1,
                    15,
                    4,
                    59,
                    49,
                    2,
                    15,
                    0
                ],
                "published": "2025-01-15T04:59:49Z",
                "published_parsed": [
                    2025,
                    1,
                    15,
                    4,
                    59,
                    49,
                    2,
                    15,
                    0
                ],
                "title": "What Limits LLM-based Human Simulation: LLMs or Our Design?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "What Limits LLM-based Human Simulation: LLMs or Our Design?"
                },
                "summary": "We argue that advancing LLM-based human simulation requires addressing both\nLLM's inherent limitations and simulation framework design challenges. Recent\nstudies have revealed significant gaps between LLM-based human simulations and\nreal-world observations, highlighting these dual challenges. To address these\ngaps, we present a comprehensive analysis of LLM limitations and our design\nissues, proposing targeted solutions for both aspects. Furthermore, we explore\nfuture directions that address both challenges simultaneously, particularly in\ndata collection, LLM generation, and evaluation. To support further research in\nthis field, we provide a curated collection of LLM-based human simulation\nresources.\\footnote{https://github.com/Persdre/llm-human-simulation}",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We argue that advancing LLM-based human simulation requires addressing both\nLLM's inherent limitations and simulation framework design challenges. Recent\nstudies have revealed significant gaps between LLM-based human simulations and\nreal-world observations, highlighting these dual challenges. To address these\ngaps, we present a comprehensive analysis of LLM limitations and our design\nissues, proposing targeted solutions for both aspects. Furthermore, we explore\nfuture directions that address both challenges simultaneously, particularly in\ndata collection, LLM generation, and evaluation. To support further research in\nthis field, we provide a curated collection of LLM-based human simulation\nresources.\\footnote{https://github.com/Persdre/llm-human-simulation}"
                },
                "authors": [
                    {
                        "name": "Qian Wang"
                    },
                    {
                        "name": "Jiaying Wu"
                    },
                    {
                        "name": "Zhenheng Tang"
                    },
                    {
                        "name": "Bingqiao Luo"
                    },
                    {
                        "name": "Nuo Chen"
                    },
                    {
                        "name": "Wei Chen"
                    },
                    {
                        "name": "Bingsheng He"
                    }
                ],
                "author_detail": {
                    "name": "Bingsheng He"
                },
                "author": "Bingsheng He",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.08579v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.08579v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.08576v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.08576v1",
                "updated": "2025-01-15T04:52:15Z",
                "updated_parsed": [
                    2025,
                    1,
                    15,
                    4,
                    52,
                    15,
                    2,
                    15,
                    0
                ],
                "published": "2025-01-15T04:52:15Z",
                "published_parsed": [
                    2025,
                    1,
                    15,
                    4,
                    52,
                    15,
                    2,
                    15,
                    0
                ],
                "title": "Intelligent Reflecting Surfaces Aided Wireless Network: Deployment\n  Architectures and Solutions",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Intelligent Reflecting Surfaces Aided Wireless Network: Deployment\n  Architectures and Solutions"
                },
                "summary": "Intelligent reflecting surfaces (IRSs) have emerged as a transformative\ntechnology for wireless networks by improving coverage, capacity, and energy\nefficiency through intelligent manipulation of wireless propagation\nenvironments. This paper provides a comprehensive study on the deployment and\ncoordination of IRSs for wireless networks. By addressing both single- and\nmulti-reflection IRS architectures, we examine their deployment strategies\nacross diverse scenarios, including point-to-point, point-to-multipoint, and\npoint-to-area setups. For the single-reflection case, we highlight the\ntrade-offs between passive and active IRS architectures in terms of beamforming\ngain, coverage extension, and spatial multiplexing. For the multi-reflection\ncase, we discuss practical strategies to optimize IRS deployment and element\nallocation, balancing cooperative beamforming gains and path loss. The paper\nfurther discusses practical challenges in IRS implementation, including\nenvironmental conditions, system compatibility, and hardware limitations.\nNumerical results and field tests validate the effectiveness of IRS-aided\nwireless networks and demonstrate their capacity and coverage improvements.\nLastly, promising research directions, including movable IRSs, near-field\ndeployments, and network-level optimization, are outlined to guide future\ninvestigations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Intelligent reflecting surfaces (IRSs) have emerged as a transformative\ntechnology for wireless networks by improving coverage, capacity, and energy\nefficiency through intelligent manipulation of wireless propagation\nenvironments. This paper provides a comprehensive study on the deployment and\ncoordination of IRSs for wireless networks. By addressing both single- and\nmulti-reflection IRS architectures, we examine their deployment strategies\nacross diverse scenarios, including point-to-point, point-to-multipoint, and\npoint-to-area setups. For the single-reflection case, we highlight the\ntrade-offs between passive and active IRS architectures in terms of beamforming\ngain, coverage extension, and spatial multiplexing. For the multi-reflection\ncase, we discuss practical strategies to optimize IRS deployment and element\nallocation, balancing cooperative beamforming gains and path loss. The paper\nfurther discusses practical challenges in IRS implementation, including\nenvironmental conditions, system compatibility, and hardware limitations.\nNumerical results and field tests validate the effectiveness of IRS-aided\nwireless networks and demonstrate their capacity and coverage improvements.\nLastly, promising research directions, including movable IRSs, near-field\ndeployments, and network-level optimization, are outlined to guide future\ninvestigations."
                },
                "authors": [
                    {
                        "name": "Qingqing Wu"
                    },
                    {
                        "name": "Guangji Chen"
                    },
                    {
                        "name": "Qiaoyan Peng"
                    },
                    {
                        "name": "Wen Chen"
                    },
                    {
                        "name": "Yifei Yuan"
                    },
                    {
                        "name": "Zhenqiao Cheng"
                    },
                    {
                        "name": "Jianwu Dou"
                    },
                    {
                        "name": "Zhiyong Zhao"
                    },
                    {
                        "name": "Ping Li"
                    }
                ],
                "author_detail": {
                    "name": "Ping Li"
                },
                "author": "Ping Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.08576v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.08576v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SP",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2402.18023v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2402.18023v3",
                "updated": "2025-01-15T04:47:36Z",
                "updated_parsed": [
                    2025,
                    1,
                    15,
                    4,
                    47,
                    36,
                    2,
                    15,
                    0
                ],
                "published": "2024-02-28T03:38:20Z",
                "published_parsed": [
                    2024,
                    2,
                    28,
                    3,
                    38,
                    20,
                    2,
                    59,
                    0
                ],
                "title": "Do Large Language Models Mirror Cognitive Language Processing?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Do Large Language Models Mirror Cognitive Language Processing?"
                },
                "summary": "Large Language Models (LLMs) have demonstrated remarkable abilities in text\ncomprehension and logical reasoning, indicating that the text representations\nlearned by LLMs can facilitate their language processing capabilities. In\nneuroscience, brain cognitive processing signals are typically utilized to\nstudy human language processing. Therefore, it is natural to ask how well the\ntext embeddings from LLMs align with the brain cognitive processing signals,\nand how training strategies affect the LLM-brain alignment? In this paper, we\nemploy Representational Similarity Analysis (RSA) to measure the alignment\nbetween 23 mainstream LLMs and fMRI signals of the brain to evaluate how\neffectively LLMs simulate cognitive language processing. We empirically\ninvestigate the impact of various factors (e.g., pre-training data size, model\nscaling, alignment training, and prompts) on such LLM-brain alignment.\nExperimental results indicate that pre-training data size and model scaling are\npositively correlated with LLM-brain similarity, and alignment training can\nsignificantly improve LLM-brain similarity. Explicit prompts contribute to the\nconsistency of LLMs with brain cognitive language processing, while nonsensical\nnoisy prompts may attenuate such alignment. Additionally, the performance of a\nwide range of LLM evaluations (e.g., MMLU, Chatbot Arena) is highly correlated\nwith the LLM-brain similarity.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have demonstrated remarkable abilities in text\ncomprehension and logical reasoning, indicating that the text representations\nlearned by LLMs can facilitate their language processing capabilities. In\nneuroscience, brain cognitive processing signals are typically utilized to\nstudy human language processing. Therefore, it is natural to ask how well the\ntext embeddings from LLMs align with the brain cognitive processing signals,\nand how training strategies affect the LLM-brain alignment? In this paper, we\nemploy Representational Similarity Analysis (RSA) to measure the alignment\nbetween 23 mainstream LLMs and fMRI signals of the brain to evaluate how\neffectively LLMs simulate cognitive language processing. We empirically\ninvestigate the impact of various factors (e.g., pre-training data size, model\nscaling, alignment training, and prompts) on such LLM-brain alignment.\nExperimental results indicate that pre-training data size and model scaling are\npositively correlated with LLM-brain similarity, and alignment training can\nsignificantly improve LLM-brain similarity. Explicit prompts contribute to the\nconsistency of LLMs with brain cognitive language processing, while nonsensical\nnoisy prompts may attenuate such alignment. Additionally, the performance of a\nwide range of LLM evaluations (e.g., MMLU, Chatbot Arena) is highly correlated\nwith the LLM-brain similarity."
                },
                "authors": [
                    {
                        "name": "Yuqi Ren"
                    },
                    {
                        "name": "Renren Jin"
                    },
                    {
                        "name": "Tongxuan Zhang"
                    },
                    {
                        "name": "Deyi Xiong"
                    }
                ],
                "author_detail": {
                    "name": "Deyi Xiong"
                },
                "author": "Deyi Xiong",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2402.18023v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2402.18023v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.08570v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.08570v1",
                "updated": "2025-01-15T04:32:41Z",
                "updated_parsed": [
                    2025,
                    1,
                    15,
                    4,
                    32,
                    41,
                    2,
                    15,
                    0
                ],
                "published": "2025-01-15T04:32:41Z",
                "published_parsed": [
                    2025,
                    1,
                    15,
                    4,
                    32,
                    41,
                    2,
                    15,
                    0
                ],
                "title": "Information Entropy Invariance: Enhancing Length Extrapolation in\n  Attention Mechanisms",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Information Entropy Invariance: Enhancing Length Extrapolation in\n  Attention Mechanisms"
                },
                "summary": "Improving the length extrapolation capabilities of Large Language Models\n(LLMs) remains a critical challenge in natural language processing. Many recent\nefforts have focused on modifying the scaled dot-product attention mechanism,\nand often introduce scaled temperatures without rigorous theoretical\njustification. To fill this gap, we introduce a novel approach based on\ninformation entropy invariance. We propose two new scaled temperatures to\nenhance length extrapolation. First, a training-free method InfoScale is\ndesigned for dot-product attention, and preserves focus on original tokens\nduring length extrapolation by ensuring information entropy remains consistent.\nSecond, we theoretically analyze the impact of scaling (CosScale) on cosine\nattention. Experimental data demonstrates that combining InfoScale and CosScale\nachieves state-of-the-art performance on the GAU-{\\alpha} model with a context\nwindow extended to 64 times the training length, and outperforms seven existing\nmethods. Our analysis reveals that significantly increasing CosScale\napproximates windowed attention, and highlights the significance of attention\nscore dilution as a key challenge in long-range context handling. The code and\ndata are available at https://github.com/HT-NEKO/InfoScale.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Improving the length extrapolation capabilities of Large Language Models\n(LLMs) remains a critical challenge in natural language processing. Many recent\nefforts have focused on modifying the scaled dot-product attention mechanism,\nand often introduce scaled temperatures without rigorous theoretical\njustification. To fill this gap, we introduce a novel approach based on\ninformation entropy invariance. We propose two new scaled temperatures to\nenhance length extrapolation. First, a training-free method InfoScale is\ndesigned for dot-product attention, and preserves focus on original tokens\nduring length extrapolation by ensuring information entropy remains consistent.\nSecond, we theoretically analyze the impact of scaling (CosScale) on cosine\nattention. Experimental data demonstrates that combining InfoScale and CosScale\nachieves state-of-the-art performance on the GAU-{\\alpha} model with a context\nwindow extended to 64 times the training length, and outperforms seven existing\nmethods. Our analysis reveals that significantly increasing CosScale\napproximates windowed attention, and highlights the significance of attention\nscore dilution as a key challenge in long-range context handling. The code and\ndata are available at https://github.com/HT-NEKO/InfoScale."
                },
                "authors": [
                    {
                        "name": "Kewei Li"
                    },
                    {
                        "name": "Yanwen Kong"
                    },
                    {
                        "name": "Yiping Xu"
                    },
                    {
                        "name": "Lan Huang"
                    },
                    {
                        "name": "Ruochi Zhang"
                    },
                    {
                        "name": "Fengfeng Zhou"
                    }
                ],
                "author_detail": {
                    "name": "Fengfeng Zhou"
                },
                "author": "Fengfeng Zhou",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.08570v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.08570v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.08566v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.08566v1",
                "updated": "2025-01-15T04:17:48Z",
                "updated_parsed": [
                    2025,
                    1,
                    15,
                    4,
                    17,
                    48,
                    2,
                    15,
                    0
                ],
                "published": "2025-01-15T04:17:48Z",
                "published_parsed": [
                    2025,
                    1,
                    15,
                    4,
                    17,
                    48,
                    2,
                    15,
                    0
                ],
                "title": "Towards Lightweight and Stable Zero-shot TTS with Self-distilled\n  Representation Disentanglement",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Towards Lightweight and Stable Zero-shot TTS with Self-distilled\n  Representation Disentanglement"
                },
                "summary": "Zero-shot Text-To-Speech (TTS) synthesis shows great promise for personalized\nvoice customization through voice cloning. However, current methods for\nachieving zero-shot TTS heavily rely on large model scales and extensive\ntraining datasets to ensure satisfactory performance and generalizability\nacross various speakers. This raises concerns regarding both deployment costs\nand data security. In this paper, we present a lightweight and stable zero-shot\nTTS system. We introduce a novel TTS architecture designed to effectively model\nlinguistic content and various speaker attributes from source speech and prompt\nspeech, respectively. Furthermore, we present a two-stage self-distillation\nframework that constructs parallel data pairs for effectively disentangling\nlinguistic content and speakers from the perspective of training data.\nExtensive experiments show that our system exhibits excellent performance and\nsuperior stability on the zero-shot TTS tasks. Moreover, it shows markedly\nsuperior computational efficiency, with RTFs of 0.13 and 0.012 on the CPU and\nGPU, respectively.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Zero-shot Text-To-Speech (TTS) synthesis shows great promise for personalized\nvoice customization through voice cloning. However, current methods for\nachieving zero-shot TTS heavily rely on large model scales and extensive\ntraining datasets to ensure satisfactory performance and generalizability\nacross various speakers. This raises concerns regarding both deployment costs\nand data security. In this paper, we present a lightweight and stable zero-shot\nTTS system. We introduce a novel TTS architecture designed to effectively model\nlinguistic content and various speaker attributes from source speech and prompt\nspeech, respectively. Furthermore, we present a two-stage self-distillation\nframework that constructs parallel data pairs for effectively disentangling\nlinguistic content and speakers from the perspective of training data.\nExtensive experiments show that our system exhibits excellent performance and\nsuperior stability on the zero-shot TTS tasks. Moreover, it shows markedly\nsuperior computational efficiency, with RTFs of 0.13 and 0.012 on the CPU and\nGPU, respectively."
                },
                "authors": [
                    {
                        "name": "Qianniu Chen"
                    },
                    {
                        "name": "Xiaoyang Hao"
                    },
                    {
                        "name": "Bowen Li"
                    },
                    {
                        "name": "Yue Liu"
                    },
                    {
                        "name": "Li Lu"
                    }
                ],
                "author_detail": {
                    "name": "Li Lu"
                },
                "author": "Li Lu",
                "arxiv_comment": "5 pages,4 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.08566v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.08566v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SD",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.10516v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.10516v3",
                "updated": "2025-01-15T04:10:22Z",
                "updated_parsed": [
                    2025,
                    1,
                    15,
                    4,
                    10,
                    22,
                    2,
                    15,
                    0
                ],
                "published": "2024-12-13T19:22:31Z",
                "published_parsed": [
                    2024,
                    12,
                    13,
                    19,
                    22,
                    31,
                    4,
                    348,
                    0
                ],
                "title": "CHIPS-FF: Evaluating Universal Machine Learning Force Fields for\n  Material Properties",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CHIPS-FF: Evaluating Universal Machine Learning Force Fields for\n  Material Properties"
                },
                "summary": "In this work, we introduce CHIPS-FF (Computational High-Performance\nInfrastructure for Predictive Simulation-based Force Fields), a universal,\nopen-source benchmarking platform for machine learning force fields (MLFFs).\nThis platform provides robust evaluation beyond conventional metrics such as\nenergy, focusing on complex properties including elastic constants, phonon\nspectra, defect formation energies, surface energies, and interfacial and\namorphous phase properties. Utilizing 16 graph-based MLFF models including\nALIGNN-FF, CHGNet, MatGL, MACE, SevenNet, ORB, MatterSim and OMat24, the\nCHIPS-FF workflow integrates the Atomic Simulation Environment (ASE) with\nJARVIS-Tools to facilitate automated high-throughput simulations. Our framework\nis tested on a set of 104 materials, including metals, semiconductors and\ninsulators representative of those used in semiconductor components, with each\nMLFF evaluated for convergence, accuracy, and computational cost. Additionally,\nwe evaluate the force-prediction accuracy of these models for close to 2\nmillion atomic structures. By offering a streamlined, flexible benchmarking\ninfrastructure, CHIPS-FF aims to guide the development and deployment of MLFFs\nfor real-world semiconductor applications, bridging the gap between quantum\nmechanical simulations and large-scale device modeling.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this work, we introduce CHIPS-FF (Computational High-Performance\nInfrastructure for Predictive Simulation-based Force Fields), a universal,\nopen-source benchmarking platform for machine learning force fields (MLFFs).\nThis platform provides robust evaluation beyond conventional metrics such as\nenergy, focusing on complex properties including elastic constants, phonon\nspectra, defect formation energies, surface energies, and interfacial and\namorphous phase properties. Utilizing 16 graph-based MLFF models including\nALIGNN-FF, CHGNet, MatGL, MACE, SevenNet, ORB, MatterSim and OMat24, the\nCHIPS-FF workflow integrates the Atomic Simulation Environment (ASE) with\nJARVIS-Tools to facilitate automated high-throughput simulations. Our framework\nis tested on a set of 104 materials, including metals, semiconductors and\ninsulators representative of those used in semiconductor components, with each\nMLFF evaluated for convergence, accuracy, and computational cost. Additionally,\nwe evaluate the force-prediction accuracy of these models for close to 2\nmillion atomic structures. By offering a streamlined, flexible benchmarking\ninfrastructure, CHIPS-FF aims to guide the development and deployment of MLFFs\nfor real-world semiconductor applications, bridging the gap between quantum\nmechanical simulations and large-scale device modeling."
                },
                "authors": [
                    {
                        "name": "Daniel Wines"
                    },
                    {
                        "name": "Kamal Choudhary"
                    }
                ],
                "author_detail": {
                    "name": "Kamal Choudhary"
                },
                "author": "Kamal Choudhary",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.10516v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.10516v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cond-mat.mtrl-sci",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cond-mat.mtrl-sci",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "physics.comp-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.08558v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.08558v1",
                "updated": "2025-01-15T03:49:08Z",
                "updated_parsed": [
                    2025,
                    1,
                    15,
                    3,
                    49,
                    8,
                    2,
                    15,
                    0
                ],
                "published": "2025-01-15T03:49:08Z",
                "published_parsed": [
                    2025,
                    1,
                    15,
                    3,
                    49,
                    8,
                    2,
                    15,
                    0
                ],
                "title": "LAMS: LLM-Driven Automatic Mode Switching for Assistive Teleoperation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LAMS: LLM-Driven Automatic Mode Switching for Assistive Teleoperation"
                },
                "summary": "Teleoperating high degrees-of-freedom (DoF) robotic manipulators via low-DoF\ncontrollers like joysticks often requires frequent switching between control\nmodes, where each mode maps controller movements to specific robot actions.\nManually performing this frequent switching can make teleoperation cumbersome\nand inefficient. On the other hand, existing automatic mode-switching\nsolutions, such as heuristic-based or learning-based methods, are often\ntask-specific and lack generalizability. In this paper, we introduce LLM-Driven\nAutomatic Mode Switching (LAMS), a novel approach that leverages Large Language\nModels (LLMs) to automatically switch control modes based on task context.\nUnlike existing methods, LAMS requires no prior task demonstrations and\nincrementally improves by integrating user-generated mode-switching examples.\nWe validate LAMS through an ablation study and a user study with 10\nparticipants on complex, long-horizon tasks, demonstrating that LAMS\neffectively reduces manual mode switches, is preferred over alternative\nmethods, and improves performance over time. The project website with\nsupplementary materials is at https://lams-assistance.github.io/.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Teleoperating high degrees-of-freedom (DoF) robotic manipulators via low-DoF\ncontrollers like joysticks often requires frequent switching between control\nmodes, where each mode maps controller movements to specific robot actions.\nManually performing this frequent switching can make teleoperation cumbersome\nand inefficient. On the other hand, existing automatic mode-switching\nsolutions, such as heuristic-based or learning-based methods, are often\ntask-specific and lack generalizability. In this paper, we introduce LLM-Driven\nAutomatic Mode Switching (LAMS), a novel approach that leverages Large Language\nModels (LLMs) to automatically switch control modes based on task context.\nUnlike existing methods, LAMS requires no prior task demonstrations and\nincrementally improves by integrating user-generated mode-switching examples.\nWe validate LAMS through an ablation study and a user study with 10\nparticipants on complex, long-horizon tasks, demonstrating that LAMS\neffectively reduces manual mode switches, is preferred over alternative\nmethods, and improves performance over time. The project website with\nsupplementary materials is at https://lams-assistance.github.io/."
                },
                "authors": [
                    {
                        "name": "Yiran Tao"
                    },
                    {
                        "name": "Jehan Yang"
                    },
                    {
                        "name": "Dan Ding"
                    },
                    {
                        "name": "Zackory Erickson"
                    }
                ],
                "author_detail": {
                    "name": "Zackory Erickson"
                },
                "author": "Zackory Erickson",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.08558v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.08558v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.04820v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.04820v2",
                "updated": "2025-01-15T03:43:22Z",
                "updated_parsed": [
                    2025,
                    1,
                    15,
                    3,
                    43,
                    22,
                    2,
                    15,
                    0
                ],
                "published": "2024-08-09T02:22:51Z",
                "published_parsed": [
                    2024,
                    8,
                    9,
                    2,
                    22,
                    51,
                    4,
                    222,
                    0
                ],
                "title": "Natural Language Outlines for Code: Literate Programming in the LLM Era",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Natural Language Outlines for Code: Literate Programming in the LLM Era"
                },
                "summary": "We propose using natural language outlines as a novel modality and\ninteraction surface for providing AI assistance to developers throughout the\nsoftware development process. An NL outline for a code function comprises\nmultiple statements written in concise prose, which partition the code and\nsummarize its main ideas in the style of literate programming. Crucially, we\nfind that modern LLMs can generate accurate and high-quality NL outlines in\npractice. Moreover, NL outlines enable a bidirectional sync between code and\nNL, allowing changes in one to be automatically reflected in the other. We\ndiscuss many use cases for NL outlines: they can accelerate understanding and\nnavigation of code and diffs, simplify code maintenance, augment code search,\nsteer code generation, and more. We then propose and compare multiple LLM\nprompting techniques for generating outlines and ask professional developers to\njudge outline quality. Finally, we present two case studies applying NL\noutlines toward code review and malware detection.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We propose using natural language outlines as a novel modality and\ninteraction surface for providing AI assistance to developers throughout the\nsoftware development process. An NL outline for a code function comprises\nmultiple statements written in concise prose, which partition the code and\nsummarize its main ideas in the style of literate programming. Crucially, we\nfind that modern LLMs can generate accurate and high-quality NL outlines in\npractice. Moreover, NL outlines enable a bidirectional sync between code and\nNL, allowing changes in one to be automatically reflected in the other. We\ndiscuss many use cases for NL outlines: they can accelerate understanding and\nnavigation of code and diffs, simplify code maintenance, augment code search,\nsteer code generation, and more. We then propose and compare multiple LLM\nprompting techniques for generating outlines and ask professional developers to\njudge outline quality. Finally, we present two case studies applying NL\noutlines toward code review and malware detection."
                },
                "authors": [
                    {
                        "name": "Kensen Shi"
                    },
                    {
                        "name": "Deniz Altınbüken"
                    },
                    {
                        "name": "Saswat Anand"
                    },
                    {
                        "name": "Mihai Christodorescu"
                    },
                    {
                        "name": "Katja Grünwedel"
                    },
                    {
                        "name": "Alexa Koenings"
                    },
                    {
                        "name": "Sai Naidu"
                    },
                    {
                        "name": "Anurag Pathak"
                    },
                    {
                        "name": "Marc Rasi"
                    },
                    {
                        "name": "Fredde Ribeiro"
                    },
                    {
                        "name": "Brandon Ruffin"
                    },
                    {
                        "name": "Siddhant Sanyam"
                    },
                    {
                        "name": "Maxim Tabachnyk"
                    },
                    {
                        "name": "Sara Toth"
                    },
                    {
                        "name": "Roy Tu"
                    },
                    {
                        "name": "Tobias Welp"
                    },
                    {
                        "name": "Pengcheng Yin"
                    },
                    {
                        "name": "Manzil Zaheer"
                    },
                    {
                        "name": "Satish Chandra"
                    },
                    {
                        "name": "Charles Sutton"
                    }
                ],
                "author_detail": {
                    "name": "Charles Sutton"
                },
                "author": "Charles Sutton",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.04820v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.04820v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.08553v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.08553v2",
                "updated": "2025-01-15T03:37:00Z",
                "updated_parsed": [
                    2025,
                    1,
                    15,
                    3,
                    37,
                    0,
                    2,
                    15,
                    0
                ],
                "published": "2024-08-16T06:37:59Z",
                "published_parsed": [
                    2024,
                    8,
                    16,
                    6,
                    37,
                    59,
                    4,
                    229,
                    0
                ],
                "title": "Improving the Ability of Pre-trained Language Model by Imparting Large\n  Language Model's Experience",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Improving the Ability of Pre-trained Language Model by Imparting Large\n  Language Model's Experience"
                },
                "summary": "Large Language Models (LLMs) and pre-trained Language Models (LMs) have\nachieved impressive success on many software engineering tasks (e.g., code\ncompletion and code generation). By leveraging huge existing code corpora\n(e.g., GitHub), these models can understand the patterns in source code and use\nthese patterns to predict code properties. However, LLMs under few-shot\nlearning perform poorly on non-generative tasks (e.g., fault localization and\nvulnerability localization), and fine-tuning LLMs is time-consuming and costly\nfor end users and small organizations. Furthermore, the performance of\nfine-tuning LMs for non-generative tasks is impressive, yet it heavily depends\non the amount and quality of data. As a result, the current lack of data and\nthe high cost of collecting it in real-world scenarios further limit the\napplicability of LMs. In this paper, we leverage the powerful generation\ncapabilities of LLMs to enhance pre-trained LMs. Specifically, we use LLMs to\ngenerate domain-specific data, thereby improving the performance of pre-trained\nLMs on the target tasks. We conduct experiments by combining different LLMs in\nour generation phase and introducing various LMs to learn from the\nLLM-generated data. Then, we compare the performance of these LMs before and\nafter learning the data. We find that LLM-generated data significantly enhances\nthe performance of LMs. The improvement can reach up to 58.36% for fault\nlocalization and up to 6.09% for clone detection.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) and pre-trained Language Models (LMs) have\nachieved impressive success on many software engineering tasks (e.g., code\ncompletion and code generation). By leveraging huge existing code corpora\n(e.g., GitHub), these models can understand the patterns in source code and use\nthese patterns to predict code properties. However, LLMs under few-shot\nlearning perform poorly on non-generative tasks (e.g., fault localization and\nvulnerability localization), and fine-tuning LLMs is time-consuming and costly\nfor end users and small organizations. Furthermore, the performance of\nfine-tuning LMs for non-generative tasks is impressive, yet it heavily depends\non the amount and quality of data. As a result, the current lack of data and\nthe high cost of collecting it in real-world scenarios further limit the\napplicability of LMs. In this paper, we leverage the powerful generation\ncapabilities of LLMs to enhance pre-trained LMs. Specifically, we use LLMs to\ngenerate domain-specific data, thereby improving the performance of pre-trained\nLMs on the target tasks. We conduct experiments by combining different LLMs in\nour generation phase and introducing various LMs to learn from the\nLLM-generated data. Then, we compare the performance of these LMs before and\nafter learning the data. We find that LLM-generated data significantly enhances\nthe performance of LMs. The improvement can reach up to 58.36% for fault\nlocalization and up to 6.09% for clone detection."
                },
                "authors": [
                    {
                        "name": "Xin Yin"
                    },
                    {
                        "name": "Chao Ni"
                    },
                    {
                        "name": "Xiaodan Xu"
                    },
                    {
                        "name": "Xinrui Li"
                    },
                    {
                        "name": "Xiaohu Yang"
                    }
                ],
                "author_detail": {
                    "name": "Xiaohu Yang"
                },
                "author": "Xiaohu Yang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.08553v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.08553v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.11514v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.11514v2",
                "updated": "2025-01-15T03:20:24Z",
                "updated_parsed": [
                    2025,
                    1,
                    15,
                    3,
                    20,
                    24,
                    2,
                    15,
                    0
                ],
                "published": "2024-06-17T13:21:23Z",
                "published_parsed": [
                    2024,
                    6,
                    17,
                    13,
                    21,
                    23,
                    0,
                    169,
                    0
                ],
                "title": "Counterfactual Debating with Preset Stances for Hallucination\n  Elimination of LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Counterfactual Debating with Preset Stances for Hallucination\n  Elimination of LLMs"
                },
                "summary": "Large Language Models (LLMs) excel in various natural language processing\ntasks but struggle with hallucination issues. Existing solutions have\nconsidered utilizing LLMs' inherent reasoning abilities to alleviate\nhallucination, such as self-correction and diverse sampling methods. However,\nthese methods often overtrust LLMs' initial answers due to inherent biases. The\nkey to alleviating this issue lies in overriding LLMs' inherent biases for\nanswer inspection. To this end, we propose a CounterFactual Multi-Agent Debate\n(CFMAD) framework. CFMAD presets the stances of LLMs to override their inherent\nbiases by compelling LLMs to generate justifications for a predetermined\nanswer's correctness. The LLMs with different predetermined stances are engaged\nwith a skeptical critic for counterfactual debate on the rationality of\ngenerated justifications. Finally, the debate process is evaluated by a\nthird-party judge to determine the final answer. Extensive experiments on four\ndatasets of three tasks demonstrate the superiority of CFMAD over existing\nmethods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) excel in various natural language processing\ntasks but struggle with hallucination issues. Existing solutions have\nconsidered utilizing LLMs' inherent reasoning abilities to alleviate\nhallucination, such as self-correction and diverse sampling methods. However,\nthese methods often overtrust LLMs' initial answers due to inherent biases. The\nkey to alleviating this issue lies in overriding LLMs' inherent biases for\nanswer inspection. To this end, we propose a CounterFactual Multi-Agent Debate\n(CFMAD) framework. CFMAD presets the stances of LLMs to override their inherent\nbiases by compelling LLMs to generate justifications for a predetermined\nanswer's correctness. The LLMs with different predetermined stances are engaged\nwith a skeptical critic for counterfactual debate on the rationality of\ngenerated justifications. Finally, the debate process is evaluated by a\nthird-party judge to determine the final answer. Extensive experiments on four\ndatasets of three tasks demonstrate the superiority of CFMAD over existing\nmethods."
                },
                "authors": [
                    {
                        "name": "Yi Fang"
                    },
                    {
                        "name": "Moxin Li"
                    },
                    {
                        "name": "Wenjie Wang"
                    },
                    {
                        "name": "Hui Lin"
                    },
                    {
                        "name": "Fuli Feng"
                    }
                ],
                "author_detail": {
                    "name": "Fuli Feng"
                },
                "author": "Fuli Feng",
                "arxiv_comment": "accepted by COLING 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.11514v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.11514v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.01028v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.01028v4",
                "updated": "2025-01-15T03:02:22Z",
                "updated_parsed": [
                    2025,
                    1,
                    15,
                    3,
                    2,
                    22,
                    2,
                    15,
                    0
                ],
                "published": "2025-01-02T03:17:51Z",
                "published_parsed": [
                    2025,
                    1,
                    2,
                    3,
                    17,
                    51,
                    3,
                    2,
                    0
                ],
                "title": "KaLM-Embedding: Superior Training Data Brings A Stronger Embedding Model",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "KaLM-Embedding: Superior Training Data Brings A Stronger Embedding Model"
                },
                "summary": "As retrieval-augmented generation prevails in large language models,\nembedding models are becoming increasingly crucial. Despite the growing number\nof general embedding models, prior work often overlooks the critical role of\ntraining data quality. In this work, we introduce KaLM-Embedding, a general\nmultilingual embedding model that leverages a large quantity of cleaner, more\ndiverse, and domain-specific training data. Our model has been trained with key\ntechniques proven to enhance performance: (1) persona-based synthetic data to\ncreate diversified examples distilled from LLMs, (2) ranking consistency\nfiltering to remove less informative samples, and (3) semi-homogeneous task\nbatch sampling to improve training efficacy. Departing from traditional\nBERT-like architectures, we adopt Qwen2-0.5B as the pre-trained model,\nfacilitating the adaptation of auto-regressive language models for general\nembedding tasks. Extensive evaluations of the MTEB benchmark across multiple\nlanguages show that our model outperforms others of comparable size, setting a\nnew standard for multilingual embedding models with <1B parameters.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As retrieval-augmented generation prevails in large language models,\nembedding models are becoming increasingly crucial. Despite the growing number\nof general embedding models, prior work often overlooks the critical role of\ntraining data quality. In this work, we introduce KaLM-Embedding, a general\nmultilingual embedding model that leverages a large quantity of cleaner, more\ndiverse, and domain-specific training data. Our model has been trained with key\ntechniques proven to enhance performance: (1) persona-based synthetic data to\ncreate diversified examples distilled from LLMs, (2) ranking consistency\nfiltering to remove less informative samples, and (3) semi-homogeneous task\nbatch sampling to improve training efficacy. Departing from traditional\nBERT-like architectures, we adopt Qwen2-0.5B as the pre-trained model,\nfacilitating the adaptation of auto-regressive language models for general\nembedding tasks. Extensive evaluations of the MTEB benchmark across multiple\nlanguages show that our model outperforms others of comparable size, setting a\nnew standard for multilingual embedding models with <1B parameters."
                },
                "authors": [
                    {
                        "name": "Xinshuo Hu"
                    },
                    {
                        "name": "Zifei Shan"
                    },
                    {
                        "name": "Xinping Zhao"
                    },
                    {
                        "name": "Zetian Sun"
                    },
                    {
                        "name": "Zhenyu Liu"
                    },
                    {
                        "name": "Dongfang Li"
                    },
                    {
                        "name": "Shaolin Ye"
                    },
                    {
                        "name": "Xinyuan Wei"
                    },
                    {
                        "name": "Qian Chen"
                    },
                    {
                        "name": "Baotian Hu"
                    },
                    {
                        "name": "Haofen Wang"
                    },
                    {
                        "name": "Jun Yu"
                    },
                    {
                        "name": "Min Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Min Zhang"
                },
                "author": "Min Zhang",
                "arxiv_comment": "Technical Report. 23 pages, 6 figures, 10 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.01028v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.01028v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.08540v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.08540v1",
                "updated": "2025-01-15T03:00:57Z",
                "updated_parsed": [
                    2025,
                    1,
                    15,
                    3,
                    0,
                    57,
                    2,
                    15,
                    0
                ],
                "published": "2025-01-15T03:00:57Z",
                "published_parsed": [
                    2025,
                    1,
                    15,
                    3,
                    0,
                    57,
                    2,
                    15,
                    0
                ],
                "title": "Knowledge prompt chaining for semantic modeling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Knowledge prompt chaining for semantic modeling"
                },
                "summary": "The task of building semantics for structured data such as CSV, JSON, and XML\nfiles is highly relevant in the knowledge representation field. Even though we\nhave a vast of structured data on the internet, mapping them to domain\nontologies to build semantics for them is still very challenging as it requires\nthe construction model to understand and learn graph-structured knowledge.\nOtherwise, the task will require human beings' effort and cost. In this paper,\nwe proposed a novel automatic semantic modeling framework: Knowledge Prompt\nChaining. It can serialize the graph-structured knowledge and inject it into\nthe LLMs properly in a Prompt Chaining architecture. Through this knowledge\ninjection and prompting chaining, the model in our framework can learn the\nstructure information and latent space of the graph and generate the semantic\nlabels and semantic graphs following the chains' insturction naturally. Based\non experimental results, our method achieves better performance than existing\nleading techniques, despite using reduced structured input data.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The task of building semantics for structured data such as CSV, JSON, and XML\nfiles is highly relevant in the knowledge representation field. Even though we\nhave a vast of structured data on the internet, mapping them to domain\nontologies to build semantics for them is still very challenging as it requires\nthe construction model to understand and learn graph-structured knowledge.\nOtherwise, the task will require human beings' effort and cost. In this paper,\nwe proposed a novel automatic semantic modeling framework: Knowledge Prompt\nChaining. It can serialize the graph-structured knowledge and inject it into\nthe LLMs properly in a Prompt Chaining architecture. Through this knowledge\ninjection and prompting chaining, the model in our framework can learn the\nstructure information and latent space of the graph and generate the semantic\nlabels and semantic graphs following the chains' insturction naturally. Based\non experimental results, our method achieves better performance than existing\nleading techniques, despite using reduced structured input data."
                },
                "authors": [
                    {
                        "name": "Ning Pei Ding"
                    },
                    {
                        "name": "Jingge Du"
                    },
                    {
                        "name": "Zaiwen Feng"
                    }
                ],
                "author_detail": {
                    "name": "Zaiwen Feng"
                },
                "author": "Zaiwen Feng",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.08540v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.08540v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.08523v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.08523v1",
                "updated": "2025-01-15T02:25:35Z",
                "updated_parsed": [
                    2025,
                    1,
                    15,
                    2,
                    25,
                    35,
                    2,
                    15,
                    0
                ],
                "published": "2025-01-15T02:25:35Z",
                "published_parsed": [
                    2025,
                    1,
                    15,
                    2,
                    25,
                    35,
                    2,
                    15,
                    0
                ],
                "title": "Doc-Guided Sent2Sent++: A Sent2Sent++ Agent with Doc-Guided memory for\n  Document-level Machine Translation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Doc-Guided Sent2Sent++: A Sent2Sent++ Agent with Doc-Guided memory for\n  Document-level Machine Translation"
                },
                "summary": "The field of artificial intelligence has witnessed significant advancements\nin natural language processing, largely attributed to the capabilities of Large\nLanguage Models (LLMs). These models form the backbone of Agents designed to\naddress long-context dependencies, particularly in Document-level Machine\nTranslation (DocMT). DocMT presents unique challenges, with quality,\nconsistency, and fluency being the key metrics for evaluation. Existing\napproaches, such as Doc2Doc and Doc2Sent, either omit sentences or compromise\nfluency. This paper introduces Doc-Guided Sent2Sent++, an Agent that employs an\nincremental sentence-level forced decoding strategy \\textbf{to ensure every\nsentence is translated while enhancing the fluency of adjacent sentences.} Our\nAgent leverages a Doc-Guided Memory, focusing solely on the summary and its\ntranslation, which we find to be an efficient approach to maintaining\nconsistency. Through extensive testing across multiple languages and domains,\nwe demonstrate that Sent2Sent++ outperforms other methods in terms of quality,\nconsistency, and fluency. The results indicate that, our approach has achieved\nsignificant improvements in metrics such as s-COMET, d-COMET, LTCR-$1_f$, and\ndocument-level perplexity (d-ppl). The contributions of this paper include a\ndetailed analysis of current DocMT research, the introduction of the\nSent2Sent++ decoding method, the Doc-Guided Memory mechanism, and validation of\nits effectiveness across languages and domains.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The field of artificial intelligence has witnessed significant advancements\nin natural language processing, largely attributed to the capabilities of Large\nLanguage Models (LLMs). These models form the backbone of Agents designed to\naddress long-context dependencies, particularly in Document-level Machine\nTranslation (DocMT). DocMT presents unique challenges, with quality,\nconsistency, and fluency being the key metrics for evaluation. Existing\napproaches, such as Doc2Doc and Doc2Sent, either omit sentences or compromise\nfluency. This paper introduces Doc-Guided Sent2Sent++, an Agent that employs an\nincremental sentence-level forced decoding strategy \\textbf{to ensure every\nsentence is translated while enhancing the fluency of adjacent sentences.} Our\nAgent leverages a Doc-Guided Memory, focusing solely on the summary and its\ntranslation, which we find to be an efficient approach to maintaining\nconsistency. Through extensive testing across multiple languages and domains,\nwe demonstrate that Sent2Sent++ outperforms other methods in terms of quality,\nconsistency, and fluency. The results indicate that, our approach has achieved\nsignificant improvements in metrics such as s-COMET, d-COMET, LTCR-$1_f$, and\ndocument-level perplexity (d-ppl). The contributions of this paper include a\ndetailed analysis of current DocMT research, the introduction of the\nSent2Sent++ decoding method, the Doc-Guided Memory mechanism, and validation of\nits effectiveness across languages and domains."
                },
                "authors": [
                    {
                        "name": "Jiaxin Guo"
                    },
                    {
                        "name": "Yuanchang Luo"
                    },
                    {
                        "name": "Daimeng Wei"
                    },
                    {
                        "name": "Ling Zhang"
                    },
                    {
                        "name": "Zongyao Li"
                    },
                    {
                        "name": "Hengchao Shang"
                    },
                    {
                        "name": "Zhiqiang Rao"
                    },
                    {
                        "name": "Shaojun Li"
                    },
                    {
                        "name": "Jinlong Yang"
                    },
                    {
                        "name": "Zhanglin Wu"
                    },
                    {
                        "name": "Hao Yang"
                    }
                ],
                "author_detail": {
                    "name": "Hao Yang"
                },
                "author": "Hao Yang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.08523v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.08523v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.03363v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.03363v2",
                "updated": "2025-01-15T01:51:55Z",
                "updated_parsed": [
                    2025,
                    1,
                    15,
                    1,
                    51,
                    55,
                    2,
                    15,
                    0
                ],
                "published": "2024-09-05T09:10:38Z",
                "published_parsed": [
                    2024,
                    9,
                    5,
                    9,
                    10,
                    38,
                    3,
                    249,
                    0
                ],
                "title": "Con-ReCall: Detecting Pre-training Data in LLMs via Contrastive Decoding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Con-ReCall: Detecting Pre-training Data in LLMs via Contrastive Decoding"
                },
                "summary": "The training data in large language models is key to their success, but it\nalso presents privacy and security risks, as it may contain sensitive\ninformation. Detecting pre-training data is crucial for mitigating these\nconcerns. Existing methods typically analyze target text in isolation or solely\nwith non-member contexts, overlooking potential insights from simultaneously\nconsidering both member and non-member contexts. While previous work suggested\nthat member contexts provide little information due to the minor distributional\nshift they induce, our analysis reveals that these subtle shifts can be\neffectively leveraged when contrasted with non-member contexts. In this paper,\nwe propose Con-ReCall, a novel approach that leverages the asymmetric\ndistributional shifts induced by member and non-member contexts through\ncontrastive decoding, amplifying subtle differences to enhance membership\ninference. Extensive empirical evaluations demonstrate that Con-ReCall achieves\nstate-of-the-art performance on the WikiMIA benchmark and is robust against\nvarious text manipulation techniques.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The training data in large language models is key to their success, but it\nalso presents privacy and security risks, as it may contain sensitive\ninformation. Detecting pre-training data is crucial for mitigating these\nconcerns. Existing methods typically analyze target text in isolation or solely\nwith non-member contexts, overlooking potential insights from simultaneously\nconsidering both member and non-member contexts. While previous work suggested\nthat member contexts provide little information due to the minor distributional\nshift they induce, our analysis reveals that these subtle shifts can be\neffectively leveraged when contrasted with non-member contexts. In this paper,\nwe propose Con-ReCall, a novel approach that leverages the asymmetric\ndistributional shifts induced by member and non-member contexts through\ncontrastive decoding, amplifying subtle differences to enhance membership\ninference. Extensive empirical evaluations demonstrate that Con-ReCall achieves\nstate-of-the-art performance on the WikiMIA benchmark and is robust against\nvarious text manipulation techniques."
                },
                "authors": [
                    {
                        "name": "Cheng Wang"
                    },
                    {
                        "name": "Yiwei Wang"
                    },
                    {
                        "name": "Bryan Hooi"
                    },
                    {
                        "name": "Yujun Cai"
                    },
                    {
                        "name": "Nanyun Peng"
                    },
                    {
                        "name": "Kai-Wei Chang"
                    }
                ],
                "author_detail": {
                    "name": "Kai-Wei Chang"
                },
                "author": "Kai-Wei Chang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.03363v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.03363v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.20166v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.20166v2",
                "updated": "2025-01-15T01:34:46Z",
                "updated_parsed": [
                    2025,
                    1,
                    15,
                    1,
                    34,
                    46,
                    2,
                    15,
                    0
                ],
                "published": "2024-12-28T14:38:16Z",
                "published_parsed": [
                    2024,
                    12,
                    28,
                    14,
                    38,
                    16,
                    5,
                    363,
                    0
                ],
                "title": "LoL-PIM: Long-Context LLM Decoding with Scalable DRAM-PIM System",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LoL-PIM: Long-Context LLM Decoding with Scalable DRAM-PIM System"
                },
                "summary": "The expansion of large language models (LLMs) with hundreds of billions of\nparameters presents significant challenges to computational resources,\nparticularly data movement and memory bandwidth. Long-context LLMs, which\nprocess sequences of tens of thousands of tokens, further increase the demand\non the memory system as the complexity in attention layers and key-value cache\nsizes is proportional to the context length. Processing-in-Memory (PIM)\nmaximizes memory bandwidth by moving compute to the data and can address the\nmemory bandwidth challenges; however, PIM is not necessarily scalable to\naccelerate long-context LLM because of limited per-module memory capacity and\nthe inflexibility of fixed-functional unit PIM architecture and static memory\nmanagement. In this work, we propose LoL-PIM which is a multi-node PIM\narchitecture that accelerates long context LLM through hardware-software\nco-design. In particular, we propose how pipeline parallelism can be exploited\nacross a multi-PIM module while a direct PIM access (DPA) controller (or DMA\nfor PIM) is proposed that enables dynamic PIM memory management and results in\nefficient PIM utilization across a diverse range of context length. We\ndeveloped an MLIR-based compiler for LoL-PIM extending a commercial PIM-based\ncompiler where the software modifications were implemented and evaluated, while\nthe hardware changes were modeled in the simulator. Our evaluations demonstrate\nthat LoL-PIM significantly improves throughput and reduces latency for\nlong-context LLM inference, outperforming both multi-GPU and GPU-PIM systems\n(up to 8.54x and 16.0x speedup, respectively), thereby enabling more efficient\ndeployment of LLMs in real-world applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The expansion of large language models (LLMs) with hundreds of billions of\nparameters presents significant challenges to computational resources,\nparticularly data movement and memory bandwidth. Long-context LLMs, which\nprocess sequences of tens of thousands of tokens, further increase the demand\non the memory system as the complexity in attention layers and key-value cache\nsizes is proportional to the context length. Processing-in-Memory (PIM)\nmaximizes memory bandwidth by moving compute to the data and can address the\nmemory bandwidth challenges; however, PIM is not necessarily scalable to\naccelerate long-context LLM because of limited per-module memory capacity and\nthe inflexibility of fixed-functional unit PIM architecture and static memory\nmanagement. In this work, we propose LoL-PIM which is a multi-node PIM\narchitecture that accelerates long context LLM through hardware-software\nco-design. In particular, we propose how pipeline parallelism can be exploited\nacross a multi-PIM module while a direct PIM access (DPA) controller (or DMA\nfor PIM) is proposed that enables dynamic PIM memory management and results in\nefficient PIM utilization across a diverse range of context length. We\ndeveloped an MLIR-based compiler for LoL-PIM extending a commercial PIM-based\ncompiler where the software modifications were implemented and evaluated, while\nthe hardware changes were modeled in the simulator. Our evaluations demonstrate\nthat LoL-PIM significantly improves throughput and reduces latency for\nlong-context LLM inference, outperforming both multi-GPU and GPU-PIM systems\n(up to 8.54x and 16.0x speedup, respectively), thereby enabling more efficient\ndeployment of LLMs in real-world applications."
                },
                "authors": [
                    {
                        "name": "Hyucksung Kwon"
                    },
                    {
                        "name": "Kyungmo Koo"
                    },
                    {
                        "name": "Janghyeon Kim"
                    },
                    {
                        "name": "Woongkyu Lee"
                    },
                    {
                        "name": "Minjae Lee"
                    },
                    {
                        "name": "Hyungdeok Lee"
                    },
                    {
                        "name": "Yousub Jung"
                    },
                    {
                        "name": "Jaehan Park"
                    },
                    {
                        "name": "Yosub Song"
                    },
                    {
                        "name": "Byeongsu Yang"
                    },
                    {
                        "name": "Haerang Choi"
                    },
                    {
                        "name": "Guhyun Kim"
                    },
                    {
                        "name": "Jongsoon Won"
                    },
                    {
                        "name": "Woojae Shin"
                    },
                    {
                        "name": "Changhyun Kim"
                    },
                    {
                        "name": "Gyeongcheol Shin"
                    },
                    {
                        "name": "Yongkee Kwon"
                    },
                    {
                        "name": "Ilkon Kim"
                    },
                    {
                        "name": "Euicheol Lim"
                    },
                    {
                        "name": "John Kim"
                    },
                    {
                        "name": "Jungwook Choi"
                    }
                ],
                "author_detail": {
                    "name": "Jungwook Choi"
                },
                "author": "Jungwook Choi",
                "arxiv_comment": "15 pages, 12 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.20166v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.20166v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.08504v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.08504v1",
                "updated": "2025-01-15T00:54:12Z",
                "updated_parsed": [
                    2025,
                    1,
                    15,
                    0,
                    54,
                    12,
                    2,
                    15,
                    0
                ],
                "published": "2025-01-15T00:54:12Z",
                "published_parsed": [
                    2025,
                    1,
                    15,
                    0,
                    54,
                    12,
                    2,
                    15,
                    0
                ],
                "title": "SuperSAM: Crafting a SAM Supernetwork via Structured Pruning and\n  Unstructured Parameter Prioritization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SuperSAM: Crafting a SAM Supernetwork via Structured Pruning and\n  Unstructured Parameter Prioritization"
                },
                "summary": "Neural Architecture Search (NAS) is a powerful approach of automating the\ndesign of efficient neural architectures. In contrast to traditional NAS\nmethods, recently proposed one-shot NAS methods prove to be more efficient in\nperforming NAS. One-shot NAS works by generating a singular weight-sharing\nsupernetwork that acts as a search space (container) of subnetworks. Despite\nits achievements, designing the one-shot search space remains a major\nchallenge. In this work we propose a search space design strategy for Vision\nTransformer (ViT)-based architectures. In particular, we convert the Segment\nAnything Model (SAM) into a weight-sharing supernetwork called SuperSAM. Our\napproach involves automating the search space design via layer-wise structured\npruning and parameter prioritization. While the structured pruning applies\nprobabilistic removal of certain transformer layers, parameter prioritization\nperforms weight reordering and slicing of MLP-blocks in the remaining layers.\nWe train supernetworks on several datasets using the sandwich rule. For\ndeployment, we enhance subnetwork discovery by utilizing a program autotuner to\nidentify efficient subnetworks within the search space. The resulting\nsubnetworks are 30-70% smaller in size compared to the original pre-trained SAM\nViT-B, yet outperform the pretrained model. Our work introduces a new and\neffective method for ViT NAS search-space design.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Neural Architecture Search (NAS) is a powerful approach of automating the\ndesign of efficient neural architectures. In contrast to traditional NAS\nmethods, recently proposed one-shot NAS methods prove to be more efficient in\nperforming NAS. One-shot NAS works by generating a singular weight-sharing\nsupernetwork that acts as a search space (container) of subnetworks. Despite\nits achievements, designing the one-shot search space remains a major\nchallenge. In this work we propose a search space design strategy for Vision\nTransformer (ViT)-based architectures. In particular, we convert the Segment\nAnything Model (SAM) into a weight-sharing supernetwork called SuperSAM. Our\napproach involves automating the search space design via layer-wise structured\npruning and parameter prioritization. While the structured pruning applies\nprobabilistic removal of certain transformer layers, parameter prioritization\nperforms weight reordering and slicing of MLP-blocks in the remaining layers.\nWe train supernetworks on several datasets using the sandwich rule. For\ndeployment, we enhance subnetwork discovery by utilizing a program autotuner to\nidentify efficient subnetworks within the search space. The resulting\nsubnetworks are 30-70% smaller in size compared to the original pre-trained SAM\nViT-B, yet outperform the pretrained model. Our work introduces a new and\neffective method for ViT NAS search-space design."
                },
                "authors": [
                    {
                        "name": "Waqwoya Abebe"
                    },
                    {
                        "name": "Sadegh Jafari"
                    },
                    {
                        "name": "Sixing Yu"
                    },
                    {
                        "name": "Akash Dutta"
                    },
                    {
                        "name": "Jan Strube"
                    },
                    {
                        "name": "Nathan R. Tallent"
                    },
                    {
                        "name": "Luanzheng Guo"
                    },
                    {
                        "name": "Pablo Munoz"
                    },
                    {
                        "name": "Ali Jannesari"
                    }
                ],
                "author_detail": {
                    "name": "Ali Jannesari"
                },
                "author": "Ali Jannesari",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.08504v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.08504v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.20906v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.20906v4",
                "updated": "2025-01-15T00:10:57Z",
                "updated_parsed": [
                    2025,
                    1,
                    15,
                    0,
                    10,
                    57,
                    2,
                    15,
                    0
                ],
                "published": "2024-07-30T15:26:36Z",
                "published_parsed": [
                    2024,
                    7,
                    30,
                    15,
                    26,
                    36,
                    1,
                    212,
                    0
                ],
                "title": "Automated Review Generation Method Based on Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Automated Review Generation Method Based on Large Language Models"
                },
                "summary": "Literature research, vital for scientific work, faces the challenge of\nsurging information volumes exceeding researchers' processing capabilities. We\npresent an automated review generation method based on large language models\n(LLMs) to overcome efficiency bottlenecks and reduce cognitive load. Our\nstatistically validated evaluation framework demonstrates that the generated\nreviews match or exceed manual quality, offering broad applicability across\nresearch fields without requiring users' domain knowledge. Applied to propane\ndehydrogenation (PDH) catalysts, our method swiftly analyzed 343 articles,\naveraging seconds per article per LLM account, producing comprehensive reviews\nspanning 35 topics, with extended analysis of 1041 articles providing insights\ninto catalysts' properties. Through multi-layered quality control, we\neffectively mitigated LLMs' hallucinations, with expert verification confirming\naccuracy and citation integrity while demonstrating hallucination risks reduced\nto below 0.5\\% with 95\\% confidence. Released Windows application enables\none-click review generation, enhancing research productivity and literature\nrecommendation efficiency while setting the stage for broader scientific\nexplorations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Literature research, vital for scientific work, faces the challenge of\nsurging information volumes exceeding researchers' processing capabilities. We\npresent an automated review generation method based on large language models\n(LLMs) to overcome efficiency bottlenecks and reduce cognitive load. Our\nstatistically validated evaluation framework demonstrates that the generated\nreviews match or exceed manual quality, offering broad applicability across\nresearch fields without requiring users' domain knowledge. Applied to propane\ndehydrogenation (PDH) catalysts, our method swiftly analyzed 343 articles,\naveraging seconds per article per LLM account, producing comprehensive reviews\nspanning 35 topics, with extended analysis of 1041 articles providing insights\ninto catalysts' properties. Through multi-layered quality control, we\neffectively mitigated LLMs' hallucinations, with expert verification confirming\naccuracy and citation integrity while demonstrating hallucination risks reduced\nto below 0.5\\% with 95\\% confidence. Released Windows application enables\none-click review generation, enhancing research productivity and literature\nrecommendation efficiency while setting the stage for broader scientific\nexplorations."
                },
                "authors": [
                    {
                        "name": "Shican Wu"
                    },
                    {
                        "name": "Xiao Ma"
                    },
                    {
                        "name": "Dehui Luo"
                    },
                    {
                        "name": "Lulu Li"
                    },
                    {
                        "name": "Xiangcheng Shi"
                    },
                    {
                        "name": "Xin Chang"
                    },
                    {
                        "name": "Xiaoyun Lin"
                    },
                    {
                        "name": "Ran Luo"
                    },
                    {
                        "name": "Chunlei Pei"
                    },
                    {
                        "name": "Changying Du"
                    },
                    {
                        "name": "Zhi-Jian Zhao"
                    },
                    {
                        "name": "Jinlong Gong"
                    }
                ],
                "author_detail": {
                    "name": "Jinlong Gong"
                },
                "author": "Jinlong Gong",
                "arxiv_comment": "21 pages, 5 figures, 1 tables Code:\n  https://github.com/TJU-ECAT-AI/AutomaticReviewGeneration Data:\n  https://github.com/TJU-ECAT-AI/AutomaticReviewGenerationData This research\n  has been invited for a Short Oral presentation at the 18th ICC -\n  International Congress on Catalysis, taking place in Lyon, France from July\n  14-19, 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.20906v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.20906v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "physics.data-an",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.08496v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.08496v1",
                "updated": "2025-01-14T23:59:23Z",
                "updated_parsed": [
                    2025,
                    1,
                    14,
                    23,
                    59,
                    23,
                    1,
                    14,
                    0
                ],
                "published": "2025-01-14T23:59:23Z",
                "published_parsed": [
                    2025,
                    1,
                    14,
                    23,
                    59,
                    23,
                    1,
                    14,
                    0
                ],
                "title": "Quantifying the Importance of Data Alignment in Downstream Model\n  Performance",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Quantifying the Importance of Data Alignment in Downstream Model\n  Performance"
                },
                "summary": "Contrary to the conventional emphasis on dataset size, we explore the role of\ndata alignment -- an often overlooked aspect of data quality -- in training\ncapable Large Language Models (LLMs). To do so, we use the Task2Vec-based\nalignment coefficient, a quantitative measure of the similarity between two\ndatasets, to quantify the impact of alignment between training data and\nevaluation data on downstream performance. In particular, we conduct controlled\n\\textit{interventional} experiments for two settings: 1. the impact of\nincreased alignment coefficients between various pre-training (pt) against\nevaluation datasets, and 2. the impact of increased alignment coefficients\nbetween domain specific fine-tuning (ft) against domain specific evaluation.\nThe domain specific task we explore is Autoformalization -- the machine\ntranslation task between natural language and code for formal verification. In\nboth settings, we find a strong, predictable negative correlation between the\nalignment coefficient of a model's training and evaluation data and the model's\nloss/perplexity on the respective downstream task. These findings suggest a\nre-evaluation of LLM training approaches, demonstrating the relevance of data\nalignment compared to data quantity, especially in specialized downstream tasks\nsuch as Autoformalization.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Contrary to the conventional emphasis on dataset size, we explore the role of\ndata alignment -- an often overlooked aspect of data quality -- in training\ncapable Large Language Models (LLMs). To do so, we use the Task2Vec-based\nalignment coefficient, a quantitative measure of the similarity between two\ndatasets, to quantify the impact of alignment between training data and\nevaluation data on downstream performance. In particular, we conduct controlled\n\\textit{interventional} experiments for two settings: 1. the impact of\nincreased alignment coefficients between various pre-training (pt) against\nevaluation datasets, and 2. the impact of increased alignment coefficients\nbetween domain specific fine-tuning (ft) against domain specific evaluation.\nThe domain specific task we explore is Autoformalization -- the machine\ntranslation task between natural language and code for formal verification. In\nboth settings, we find a strong, predictable negative correlation between the\nalignment coefficient of a model's training and evaluation data and the model's\nloss/perplexity on the respective downstream task. These findings suggest a\nre-evaluation of LLM training approaches, demonstrating the relevance of data\nalignment compared to data quantity, especially in specialized downstream tasks\nsuch as Autoformalization."
                },
                "authors": [
                    {
                        "name": "Krrish Chawla"
                    },
                    {
                        "name": "Aryan Sahai"
                    },
                    {
                        "name": "Mario DePavia"
                    },
                    {
                        "name": "Sudharsan Sundar"
                    },
                    {
                        "name": "Brando Miranda"
                    }
                ],
                "author_detail": {
                    "name": "Brando Miranda"
                },
                "author": "Brando Miranda",
                "arxiv_journal_ref": "ICLR DMLR Data-centric Machine Learning Research (2024)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.08496v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.08496v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2403.12176v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2403.12176v3",
                "updated": "2025-01-14T22:44:14Z",
                "updated_parsed": [
                    2025,
                    1,
                    14,
                    22,
                    44,
                    14,
                    1,
                    14,
                    0
                ],
                "published": "2024-03-18T18:49:20Z",
                "published_parsed": [
                    2024,
                    3,
                    18,
                    18,
                    49,
                    20,
                    0,
                    78,
                    0
                ],
                "title": "Safety Implications of Explainable Artificial Intelligence in End-to-End\n  Autonomous Driving",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Safety Implications of Explainable Artificial Intelligence in End-to-End\n  Autonomous Driving"
                },
                "summary": "The end-to-end learning pipeline is gradually creating a paradigm shift in\nthe ongoing development of highly autonomous vehicles, largely due to advances\nin deep learning, the availability of large-scale training datasets, and\nimprovements in integrated sensor devices. However, a lack of explainability in\nreal-time decisions with contemporary learning methods impedes user trust and\nattenuates the widespread deployment and commercialization of such vehicles.\nMoreover, the issue is exacerbated when these cars are involved in or cause\ntraffic accidents. Consequently, explainability in end-to-end autonomous\ndriving is essential to build trust in vehicular automation. With that said,\nautomotive researchers have not yet rigorously explored safety benefits and\nconsequences of explanations in end-to-end autonomous driving. This paper aims\nto bridge the gaps between these topics and seeks to answer the following\nresearch question: What are safety implications of explanations in end-to-end\nautonomous driving? In this regard, we first revisit established safety and\nexplainability concepts in end-to-end driving. Furthermore, we present three\ncritical case studies and show the pivotal role of explanations in enhancing\nself-driving safety. Finally, we describe insights from empirical studies and\nreveal potential value, limitations, and caveats of practical explainable AI\nmethods with respect to their safety assurance in end-to-end driving.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The end-to-end learning pipeline is gradually creating a paradigm shift in\nthe ongoing development of highly autonomous vehicles, largely due to advances\nin deep learning, the availability of large-scale training datasets, and\nimprovements in integrated sensor devices. However, a lack of explainability in\nreal-time decisions with contemporary learning methods impedes user trust and\nattenuates the widespread deployment and commercialization of such vehicles.\nMoreover, the issue is exacerbated when these cars are involved in or cause\ntraffic accidents. Consequently, explainability in end-to-end autonomous\ndriving is essential to build trust in vehicular automation. With that said,\nautomotive researchers have not yet rigorously explored safety benefits and\nconsequences of explanations in end-to-end autonomous driving. This paper aims\nto bridge the gaps between these topics and seeks to answer the following\nresearch question: What are safety implications of explanations in end-to-end\nautonomous driving? In this regard, we first revisit established safety and\nexplainability concepts in end-to-end driving. Furthermore, we present three\ncritical case studies and show the pivotal role of explanations in enhancing\nself-driving safety. Finally, we describe insights from empirical studies and\nreveal potential value, limitations, and caveats of practical explainable AI\nmethods with respect to their safety assurance in end-to-end driving."
                },
                "authors": [
                    {
                        "name": "Shahin Atakishiyev"
                    },
                    {
                        "name": "Mohammad Salameh"
                    },
                    {
                        "name": "Randy Goebel"
                    }
                ],
                "author_detail": {
                    "name": "Randy Goebel"
                },
                "author": "Randy Goebel",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2403.12176v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2403.12176v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.08474v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.08474v1",
                "updated": "2025-01-14T22:38:55Z",
                "updated_parsed": [
                    2025,
                    1,
                    14,
                    22,
                    38,
                    55,
                    1,
                    14,
                    0
                ],
                "published": "2025-01-14T22:38:55Z",
                "published_parsed": [
                    2025,
                    1,
                    14,
                    22,
                    38,
                    55,
                    1,
                    14,
                    0
                ],
                "title": "The Theater Stage as Laboratory: Review of Real-Time Comedy LLM Systems\n  for Live Performance",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Theater Stage as Laboratory: Review of Real-Time Comedy LLM Systems\n  for Live Performance"
                },
                "summary": "In this position paper, we review the eclectic recent history of academic and\nartistic works involving computational systems for humor generation, and focus\nspecifically on live performance. We make the case that AI comedy should be\nevaluated in live conditions, in front of audiences sharing either physical or\nonline spaces, and under real-time constraints. We further suggest that\nimprovised comedy is therefore the perfect substrate for deploying and\nassessing computational humor systems. Using examples of successful AI-infused\nshows, we demonstrate that live performance raises three sets of challenges for\ncomputational humor generation: 1) questions around robotic embodiment,\nanthropomorphism and competition between humans and machines, 2) questions\naround comedic timing and the nature of audience interaction, and 3) questions\nabout the human interpretation of seemingly absurd AI-generated humor. We argue\nthat these questions impact the choice of methodologies for evaluating\ncomputational humor, as any such method needs to work around the constraints of\nlive audiences and performance spaces. These interrogations also highlight\ndifferent types of collaborative relationship of human comedians towards AI\ntools.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this position paper, we review the eclectic recent history of academic and\nartistic works involving computational systems for humor generation, and focus\nspecifically on live performance. We make the case that AI comedy should be\nevaluated in live conditions, in front of audiences sharing either physical or\nonline spaces, and under real-time constraints. We further suggest that\nimprovised comedy is therefore the perfect substrate for deploying and\nassessing computational humor systems. Using examples of successful AI-infused\nshows, we demonstrate that live performance raises three sets of challenges for\ncomputational humor generation: 1) questions around robotic embodiment,\nanthropomorphism and competition between humans and machines, 2) questions\naround comedic timing and the nature of audience interaction, and 3) questions\nabout the human interpretation of seemingly absurd AI-generated humor. We argue\nthat these questions impact the choice of methodologies for evaluating\ncomputational humor, as any such method needs to work around the constraints of\nlive audiences and performance spaces. These interrogations also highlight\ndifferent types of collaborative relationship of human comedians towards AI\ntools."
                },
                "authors": [
                    {
                        "name": "Piotr Wojciech Mirowski"
                    },
                    {
                        "name": "Boyd Branch"
                    },
                    {
                        "name": "Kory Wallace Mathewson"
                    }
                ],
                "author_detail": {
                    "name": "Kory Wallace Mathewson"
                },
                "author": "Kory Wallace Mathewson",
                "arxiv_comment": "8 pages, 1st Workshop on Computational Humor (CHum), COLING 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.08474v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.08474v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.08460v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.08460v1",
                "updated": "2025-01-14T22:09:06Z",
                "updated_parsed": [
                    2025,
                    1,
                    14,
                    22,
                    9,
                    6,
                    1,
                    14,
                    0
                ],
                "published": "2025-01-14T22:09:06Z",
                "published_parsed": [
                    2025,
                    1,
                    14,
                    22,
                    9,
                    6,
                    1,
                    14,
                    0
                ],
                "title": "Towards Zero-Shot & Explainable Video Description by Reasoning over\n  Graphs of Events in Space and Time",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Towards Zero-Shot & Explainable Video Description by Reasoning over\n  Graphs of Events in Space and Time"
                },
                "summary": "In the current era of Machine Learning, Transformers have become the de facto\napproach across a variety of domains, such as computer vision and natural\nlanguage processing. Transformer-based solutions are the backbone of current\nstate-of-the-art methods for language generation, image and video\nclassification, segmentation, action and object recognition, among many others.\nInterestingly enough, while these state-of-the-art methods produce impressive\nresults in their respective domains, the problem of understanding the\nrelationship between vision and language is still beyond our reach. In this\nwork, we propose a common ground between vision and language based on events in\nspace and time in an explainable and programmatic way, to connect\nlearning-based vision and language state of the art models and provide a\nsolution to the long standing problem of describing videos in natural language.\nWe validate that our algorithmic approach is able to generate coherent, rich\nand relevant textual descriptions on videos collected from a variety of\ndatasets, using both standard metrics (e.g. Bleu, ROUGE) and the modern\nLLM-as-a-Jury approach.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In the current era of Machine Learning, Transformers have become the de facto\napproach across a variety of domains, such as computer vision and natural\nlanguage processing. Transformer-based solutions are the backbone of current\nstate-of-the-art methods for language generation, image and video\nclassification, segmentation, action and object recognition, among many others.\nInterestingly enough, while these state-of-the-art methods produce impressive\nresults in their respective domains, the problem of understanding the\nrelationship between vision and language is still beyond our reach. In this\nwork, we propose a common ground between vision and language based on events in\nspace and time in an explainable and programmatic way, to connect\nlearning-based vision and language state of the art models and provide a\nsolution to the long standing problem of describing videos in natural language.\nWe validate that our algorithmic approach is able to generate coherent, rich\nand relevant textual descriptions on videos collected from a variety of\ndatasets, using both standard metrics (e.g. Bleu, ROUGE) and the modern\nLLM-as-a-Jury approach."
                },
                "authors": [
                    {
                        "name": "Mihai Masala"
                    },
                    {
                        "name": "Marius Leordeanu"
                    }
                ],
                "author_detail": {
                    "name": "Marius Leordeanu"
                },
                "author": "Marius Leordeanu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.08460v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.08460v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.11186v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.11186v4",
                "updated": "2025-01-14T22:07:53Z",
                "updated_parsed": [
                    2025,
                    1,
                    14,
                    22,
                    7,
                    53,
                    1,
                    14,
                    0
                ],
                "published": "2024-07-15T19:17:31Z",
                "published_parsed": [
                    2024,
                    7,
                    15,
                    19,
                    17,
                    31,
                    0,
                    197,
                    0
                ],
                "title": "Empowering Persian LLMs for Instruction Following: A Novel Dataset and\n  Training Approach",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Empowering Persian LLMs for Instruction Following: A Novel Dataset and\n  Training Approach"
                },
                "summary": "Instruction-tuned large language models have demonstrated remarkable\ncapabilities in following human instructions across various domains. However,\ntheir proficiency remains notably deficient in many low-resource languages. To\naddress this challenge, we begin by introducing FarsInstruct a comprehensive\ninstruction dataset designed to enhance the instruction following ability of\nlarge language models specifically for the Persian language a significant yet\nunderrepresented language globally. FarsInstruct encompasses a wide range of\ntask types and datasets, each containing a mix of straightforward to complex\nmanual written instructions, as well as translations from the Public Pool of\nPrompts, ensuring a rich linguistic and cultural representation. Furthermore,\nwe introduce Co-CoLA, a framework designed to enhance the multi-task\nadaptability of LoRA-tuned models. Through extensive experimental analyses, our\nstudy showcases the effectiveness of the FarsInstruct dataset coupled with\ntraining by the Co-CoLA framework, in improving the performance of large\nlanguage models within the Persian context. As of the current writing,\nFarsInstruct comprises 197 templates across 21 distinct datasets, and we intend\nto update it consistently, thus augmenting its applicability.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Instruction-tuned large language models have demonstrated remarkable\ncapabilities in following human instructions across various domains. However,\ntheir proficiency remains notably deficient in many low-resource languages. To\naddress this challenge, we begin by introducing FarsInstruct a comprehensive\ninstruction dataset designed to enhance the instruction following ability of\nlarge language models specifically for the Persian language a significant yet\nunderrepresented language globally. FarsInstruct encompasses a wide range of\ntask types and datasets, each containing a mix of straightforward to complex\nmanual written instructions, as well as translations from the Public Pool of\nPrompts, ensuring a rich linguistic and cultural representation. Furthermore,\nwe introduce Co-CoLA, a framework designed to enhance the multi-task\nadaptability of LoRA-tuned models. Through extensive experimental analyses, our\nstudy showcases the effectiveness of the FarsInstruct dataset coupled with\ntraining by the Co-CoLA framework, in improving the performance of large\nlanguage models within the Persian context. As of the current writing,\nFarsInstruct comprises 197 templates across 21 distinct datasets, and we intend\nto update it consistently, thus augmenting its applicability."
                },
                "authors": [
                    {
                        "name": "Hojjat Mokhtarabadi"
                    },
                    {
                        "name": "Ziba Zamani"
                    },
                    {
                        "name": "Abbas Maazallahi"
                    },
                    {
                        "name": "Mohammad Hossein Manshaei"
                    }
                ],
                "author_detail": {
                    "name": "Mohammad Hossein Manshaei"
                },
                "author": "Mohammad Hossein Manshaei",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.11186v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.11186v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.08457v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.08457v1",
                "updated": "2025-01-14T22:02:38Z",
                "updated_parsed": [
                    2025,
                    1,
                    14,
                    22,
                    2,
                    38,
                    1,
                    14,
                    0
                ],
                "published": "2025-01-14T22:02:38Z",
                "published_parsed": [
                    2025,
                    1,
                    14,
                    22,
                    2,
                    38,
                    1,
                    14,
                    0
                ],
                "title": "Large Language Models For Text Classification: Case Study And\n  Comprehensive Review",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models For Text Classification: Case Study And\n  Comprehensive Review"
                },
                "summary": "Unlocking the potential of Large Language Models (LLMs) in data\nclassification represents a promising frontier in natural language processing.\nIn this work, we evaluate the performance of different LLMs in comparison with\nstate-of-the-art deep-learning and machine-learning models, in two different\nclassification scenarios: i) the classification of employees' working locations\nbased on job reviews posted online (multiclass classification), and 2) the\nclassification of news articles as fake or not (binary classification). Our\nanalysis encompasses a diverse range of language models differentiating in\nsize, quantization, and architecture. We explore the impact of alternative\nprompting techniques and evaluate the models based on the weighted F1-score.\nAlso, we examine the trade-off between performance (F1-score) and time\n(inference response time) for each language model to provide a more nuanced\nunderstanding of each model's practical applicability. Our work reveals\nsignificant variations in model responses based on the prompting strategies. We\nfind that LLMs, particularly Llama3 and GPT-4, can outperform traditional\nmethods in complex classification tasks, such as multiclass classification,\nthough at the cost of longer inference times. In contrast, simpler ML models\noffer better performance-to-time trade-offs in simpler binary classification\ntasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Unlocking the potential of Large Language Models (LLMs) in data\nclassification represents a promising frontier in natural language processing.\nIn this work, we evaluate the performance of different LLMs in comparison with\nstate-of-the-art deep-learning and machine-learning models, in two different\nclassification scenarios: i) the classification of employees' working locations\nbased on job reviews posted online (multiclass classification), and 2) the\nclassification of news articles as fake or not (binary classification). Our\nanalysis encompasses a diverse range of language models differentiating in\nsize, quantization, and architecture. We explore the impact of alternative\nprompting techniques and evaluate the models based on the weighted F1-score.\nAlso, we examine the trade-off between performance (F1-score) and time\n(inference response time) for each language model to provide a more nuanced\nunderstanding of each model's practical applicability. Our work reveals\nsignificant variations in model responses based on the prompting strategies. We\nfind that LLMs, particularly Llama3 and GPT-4, can outperform traditional\nmethods in complex classification tasks, such as multiclass classification,\nthough at the cost of longer inference times. In contrast, simpler ML models\noffer better performance-to-time trade-offs in simpler binary classification\ntasks."
                },
                "authors": [
                    {
                        "name": "Arina Kostina"
                    },
                    {
                        "name": "Marios D. Dikaiakos"
                    },
                    {
                        "name": "Dimosthenis Stefanidis"
                    },
                    {
                        "name": "George Pallis"
                    }
                ],
                "author_detail": {
                    "name": "George Pallis"
                },
                "author": "George Pallis",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.08457v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.08457v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.03205v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.03205v3",
                "updated": "2025-01-14T21:58:47Z",
                "updated_parsed": [
                    2025,
                    1,
                    14,
                    21,
                    58,
                    47,
                    1,
                    14,
                    0
                ],
                "published": "2024-12-04T10:44:50Z",
                "published_parsed": [
                    2024,
                    12,
                    4,
                    10,
                    44,
                    50,
                    2,
                    339,
                    0
                ],
                "title": "U-MATH: A University-Level Benchmark for Evaluating Mathematical Skills\n  in LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "U-MATH: A University-Level Benchmark for Evaluating Mathematical Skills\n  in LLMs"
                },
                "summary": "The current evaluation of mathematical skills in LLMs is limited, as existing\nbenchmarks are either relatively small, primarily focus on elementary and\nhigh-school problems, or lack diversity in topics. Additionally, the inclusion\nof visual elements in tasks remains largely under-explored.\n  To address these gaps, we introduce U-MATH, a novel benchmark of 1,100\nunpublished open-ended university-level problems sourced from teaching\nmaterials. It is balanced across six core subjects, with 20% of multimodal\nproblems. Given the open-ended nature of U-MATH problems, we employ an LLM to\njudge the correctness of generated solutions. To this end, we release\n$\\mu$-MATH, a dataset to evaluate the LLMs' capabilities in judging solutions.\n  The evaluation of general domain, math-specific, and multimodal LLMs\nhighlights the challenges presented by U-MATH. Our findings reveal that LLMs\nachieve a maximum accuracy of only 63% on text-based tasks, with even lower 45%\non visual problems. The solution assessment proves challenging for LLMs, with\nthe best LLM judge having an F1-score of 80% on $\\mu$-MATH.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The current evaluation of mathematical skills in LLMs is limited, as existing\nbenchmarks are either relatively small, primarily focus on elementary and\nhigh-school problems, or lack diversity in topics. Additionally, the inclusion\nof visual elements in tasks remains largely under-explored.\n  To address these gaps, we introduce U-MATH, a novel benchmark of 1,100\nunpublished open-ended university-level problems sourced from teaching\nmaterials. It is balanced across six core subjects, with 20% of multimodal\nproblems. Given the open-ended nature of U-MATH problems, we employ an LLM to\njudge the correctness of generated solutions. To this end, we release\n$\\mu$-MATH, a dataset to evaluate the LLMs' capabilities in judging solutions.\n  The evaluation of general domain, math-specific, and multimodal LLMs\nhighlights the challenges presented by U-MATH. Our findings reveal that LLMs\nachieve a maximum accuracy of only 63% on text-based tasks, with even lower 45%\non visual problems. The solution assessment proves challenging for LLMs, with\nthe best LLM judge having an F1-score of 80% on $\\mu$-MATH."
                },
                "authors": [
                    {
                        "name": "Konstantin Chernyshev"
                    },
                    {
                        "name": "Vitaliy Polshkov"
                    },
                    {
                        "name": "Ekaterina Artemova"
                    },
                    {
                        "name": "Alex Myasnikov"
                    },
                    {
                        "name": "Vlad Stepanov"
                    },
                    {
                        "name": "Alexei Miasnikov"
                    },
                    {
                        "name": "Sergei Tilga"
                    }
                ],
                "author_detail": {
                    "name": "Sergei Tilga"
                },
                "author": "Sergei Tilga",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.03205v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.03205v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.08454v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.08454v1",
                "updated": "2025-01-14T21:55:37Z",
                "updated_parsed": [
                    2025,
                    1,
                    14,
                    21,
                    55,
                    37,
                    1,
                    14,
                    0
                ],
                "published": "2025-01-14T21:55:37Z",
                "published_parsed": [
                    2025,
                    1,
                    14,
                    21,
                    55,
                    37,
                    1,
                    14,
                    0
                ],
                "title": "Tag&Tab: Pretraining Data Detection in Large Language Models Using\n  Keyword-Based Membership Inference Attack",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Tag&Tab: Pretraining Data Detection in Large Language Models Using\n  Keyword-Based Membership Inference Attack"
                },
                "summary": "Large language models (LLMs) have become essential digital task assistance\ntools. Their training relies heavily on the collection of vast amounts of data,\nwhich may include copyright-protected or sensitive information. Recent studies\non the detection of pretraining data in LLMs have primarily focused on\nsentence-level or paragraph-level membership inference attacks (MIAs), usually\ninvolving probability analysis of the target model prediction tokens. However,\nthe proposed methods often demonstrate poor performance, specifically in terms\nof accuracy, failing to account for the semantic importance of textual content\nand word significance. To address these shortcomings, we propose Tag&Tab, a\nnovel approach for detecting data that has been used as part of the LLM\npretraining. Our method leverages advanced natural language processing (NLP)\ntechniques to tag keywords in the input text - a process we term Tagging. Then,\nthe LLM is used to obtain the probabilities of these keywords and calculate\ntheir average log-likelihood to determine input text membership, a process we\nrefer to as Tabbing. Our experiments on three benchmark datasets (BookMIA,\nMIMIR, and the Pile) and several open-source LLMs of varying sizes demonstrate\nan average increase in the AUC scores ranging from 4.1% to 12.1% over\nstate-of-the-art methods. Tag&Tab not only sets a new standard for data leakage\ndetection in LLMs, but its outstanding performance is a testament to the\nimportance of words in MIAs on LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have become essential digital task assistance\ntools. Their training relies heavily on the collection of vast amounts of data,\nwhich may include copyright-protected or sensitive information. Recent studies\non the detection of pretraining data in LLMs have primarily focused on\nsentence-level or paragraph-level membership inference attacks (MIAs), usually\ninvolving probability analysis of the target model prediction tokens. However,\nthe proposed methods often demonstrate poor performance, specifically in terms\nof accuracy, failing to account for the semantic importance of textual content\nand word significance. To address these shortcomings, we propose Tag&Tab, a\nnovel approach for detecting data that has been used as part of the LLM\npretraining. Our method leverages advanced natural language processing (NLP)\ntechniques to tag keywords in the input text - a process we term Tagging. Then,\nthe LLM is used to obtain the probabilities of these keywords and calculate\ntheir average log-likelihood to determine input text membership, a process we\nrefer to as Tabbing. Our experiments on three benchmark datasets (BookMIA,\nMIMIR, and the Pile) and several open-source LLMs of varying sizes demonstrate\nan average increase in the AUC scores ranging from 4.1% to 12.1% over\nstate-of-the-art methods. Tag&Tab not only sets a new standard for data leakage\ndetection in LLMs, but its outstanding performance is a testament to the\nimportance of words in MIAs on LLMs."
                },
                "authors": [
                    {
                        "name": "Sagiv Antebi"
                    },
                    {
                        "name": "Edan Habler"
                    },
                    {
                        "name": "Asaf Shabtai"
                    },
                    {
                        "name": "Yuval Elovici"
                    }
                ],
                "author_detail": {
                    "name": "Yuval Elovici"
                },
                "author": "Yuval Elovici",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.08454v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.08454v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.08449v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.08449v1",
                "updated": "2025-01-14T21:38:01Z",
                "updated_parsed": [
                    2025,
                    1,
                    14,
                    21,
                    38,
                    1,
                    1,
                    14,
                    0
                ],
                "published": "2025-01-14T21:38:01Z",
                "published_parsed": [
                    2025,
                    1,
                    14,
                    21,
                    38,
                    1,
                    1,
                    14,
                    0
                ],
                "title": "A Refreshment Stirred, Not Shaken (II): Invariant-Preserving Deployments\n  of Differential Privacy for the US Decennial Census",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Refreshment Stirred, Not Shaken (II): Invariant-Preserving Deployments\n  of Differential Privacy for the US Decennial Census"
                },
                "summary": "Through the lens of the system of differential privacy specifications\ndeveloped in Part I of a trio of articles, this second paper examines two\nstatistical disclosure control (SDC) methods for the United States Decennial\nCensus: the Permutation Swapping Algorithm (PSA), which is similar to the 2010\nCensus's disclosure avoidance system (DAS), and the TopDown Algorithm (TDA),\nwhich was used in the 2020 DAS. To varying degrees, both methods leave\nunaltered some statistics of the confidential data $\\unicode{x2013}$ which are\ncalled the method's invariants $\\unicode{x2013}$ and hence neither can be\nreadily reconciled with differential privacy (DP), at least as it was\noriginally conceived. Nevertheless, we establish that the PSA satisfies\n$\\varepsilon$-DP subject to the invariants it necessarily induces, thereby\nshowing that this traditional SDC method can in fact still be understood within\nour more-general system of DP specifications. By a similar modification to\n$\\rho$-zero concentrated DP, we also provide a DP specification for the TDA.\nFinally, as a point of comparison, we consider the counterfactual scenario in\nwhich the PSA was adopted for the 2020 Census, resulting in a reduction in the\nnominal privacy loss, but at the cost of releasing many more invariants.\nTherefore, while our results explicate the mathematical guarantees of SDC\nprovided by the PSA, the TDA and the 2020 DAS in general, care must be taken in\ntheir translation to actual privacy protection $\\unicode{x2013}$ just as is the\ncase for any DP deployment.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Through the lens of the system of differential privacy specifications\ndeveloped in Part I of a trio of articles, this second paper examines two\nstatistical disclosure control (SDC) methods for the United States Decennial\nCensus: the Permutation Swapping Algorithm (PSA), which is similar to the 2010\nCensus's disclosure avoidance system (DAS), and the TopDown Algorithm (TDA),\nwhich was used in the 2020 DAS. To varying degrees, both methods leave\nunaltered some statistics of the confidential data $\\unicode{x2013}$ which are\ncalled the method's invariants $\\unicode{x2013}$ and hence neither can be\nreadily reconciled with differential privacy (DP), at least as it was\noriginally conceived. Nevertheless, we establish that the PSA satisfies\n$\\varepsilon$-DP subject to the invariants it necessarily induces, thereby\nshowing that this traditional SDC method can in fact still be understood within\nour more-general system of DP specifications. By a similar modification to\n$\\rho$-zero concentrated DP, we also provide a DP specification for the TDA.\nFinally, as a point of comparison, we consider the counterfactual scenario in\nwhich the PSA was adopted for the 2020 Census, resulting in a reduction in the\nnominal privacy loss, but at the cost of releasing many more invariants.\nTherefore, while our results explicate the mathematical guarantees of SDC\nprovided by the PSA, the TDA and the 2020 DAS in general, care must be taken in\ntheir translation to actual privacy protection $\\unicode{x2013}$ just as is the\ncase for any DP deployment."
                },
                "authors": [
                    {
                        "name": "James Bailie"
                    },
                    {
                        "name": "Ruobin Gong"
                    },
                    {
                        "name": "Xiao-Li Meng"
                    }
                ],
                "author_detail": {
                    "name": "Xiao-Li Meng"
                },
                "author": "Xiao-Li Meng",
                "arxiv_comment": "48 pages, 2 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.08449v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.08449v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.08439v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.08439v1",
                "updated": "2025-01-14T21:05:34Z",
                "updated_parsed": [
                    2025,
                    1,
                    14,
                    21,
                    5,
                    34,
                    1,
                    14,
                    0
                ],
                "published": "2025-01-14T21:05:34Z",
                "published_parsed": [
                    2025,
                    1,
                    14,
                    21,
                    5,
                    34,
                    1,
                    14,
                    0
                ],
                "title": "Revisiting Continuous p-Hub Location Problems with the L1 Metric",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Revisiting Continuous p-Hub Location Problems with the L1 Metric"
                },
                "summary": "Motivated by emerging urban applications in commercial, public sector, and\nhumanitarian logistics, we revisit continuous $p$-hub location problems in\nwhich several facilities must be located in a continuous space such that the\nexpected minimum Manhattan travel distance from a random service provider to a\nrandom customer through exactly one hub facility is minimized. In this paper,\nwe begin by deriving closed-form results for a one-dimensional case and\ntwo-dimensional cases with up to two hubs. Subsequently, a simulation-based\napproximation method is proposed for more complex two-dimensional scenarios\nwith more than two hubs. Moreover, an extended problem with multiple service\nproviders is analyzed to reflect real-life service settings. Finally, we apply\nour model and approximation method using publicly available data as a case\nstudy to optimize the deployment of public-access automated external\ndefibrillators in Virginia Beach.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Motivated by emerging urban applications in commercial, public sector, and\nhumanitarian logistics, we revisit continuous $p$-hub location problems in\nwhich several facilities must be located in a continuous space such that the\nexpected minimum Manhattan travel distance from a random service provider to a\nrandom customer through exactly one hub facility is minimized. In this paper,\nwe begin by deriving closed-form results for a one-dimensional case and\ntwo-dimensional cases with up to two hubs. Subsequently, a simulation-based\napproximation method is proposed for more complex two-dimensional scenarios\nwith more than two hubs. Moreover, an extended problem with multiple service\nproviders is analyzed to reflect real-life service settings. Finally, we apply\nour model and approximation method using publicly available data as a case\nstudy to optimize the deployment of public-access automated external\ndefibrillators in Virginia Beach."
                },
                "authors": [
                    {
                        "name": "Yifan Wu"
                    },
                    {
                        "name": "Joseph Geunes"
                    },
                    {
                        "name": "Xiaofeng Nie"
                    }
                ],
                "author_detail": {
                    "name": "Xiaofeng Nie"
                },
                "author": "Xiaofeng Nie",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.08439v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.08439v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "math.OC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "math.OC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.08421v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.08421v1",
                "updated": "2025-01-14T20:24:12Z",
                "updated_parsed": [
                    2025,
                    1,
                    14,
                    20,
                    24,
                    12,
                    1,
                    14,
                    0
                ],
                "published": "2025-01-14T20:24:12Z",
                "published_parsed": [
                    2025,
                    1,
                    14,
                    20,
                    24,
                    12,
                    1,
                    14,
                    0
                ],
                "title": "SEAL: Speaker Error Correction using Acoustic-conditioned Large Language\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SEAL: Speaker Error Correction using Acoustic-conditioned Large Language\n  Models"
                },
                "summary": "Speaker Diarization (SD) is a crucial component of modern end-to-end ASR\npipelines. Traditional SD systems, which are typically audio-based and operate\nindependently of ASR, often introduce speaker errors, particularly during\nspeaker transitions and overlapping speech. Recently, language models including\nfine-tuned large language models (LLMs) have shown to be effective as a\nsecond-pass speaker error corrector by leveraging lexical context in the\ntranscribed output. In this work, we introduce a novel acoustic conditioning\napproach to provide more fine-grained information from the acoustic diarizer to\nthe LLM. We also show that a simpler constrained decoding strategy reduces LLM\nhallucinations, while avoiding complicated post-processing. Our approach\nsignificantly reduces the speaker error rates by 24-43% across Fisher,\nCallhome, and RT03-CTS datasets, compared to the first-pass Acoustic SD.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Speaker Diarization (SD) is a crucial component of modern end-to-end ASR\npipelines. Traditional SD systems, which are typically audio-based and operate\nindependently of ASR, often introduce speaker errors, particularly during\nspeaker transitions and overlapping speech. Recently, language models including\nfine-tuned large language models (LLMs) have shown to be effective as a\nsecond-pass speaker error corrector by leveraging lexical context in the\ntranscribed output. In this work, we introduce a novel acoustic conditioning\napproach to provide more fine-grained information from the acoustic diarizer to\nthe LLM. We also show that a simpler constrained decoding strategy reduces LLM\nhallucinations, while avoiding complicated post-processing. Our approach\nsignificantly reduces the speaker error rates by 24-43% across Fisher,\nCallhome, and RT03-CTS datasets, compared to the first-pass Acoustic SD."
                },
                "authors": [
                    {
                        "name": "Anurag Kumar"
                    },
                    {
                        "name": "Rohit Paturi"
                    },
                    {
                        "name": "Amber Afshan"
                    },
                    {
                        "name": "Sundararajan Srinivasan"
                    }
                ],
                "author_detail": {
                    "name": "Sundararajan Srinivasan"
                },
                "author": "Sundararajan Srinivasan",
                "arxiv_comment": "Accepted at ICASSP 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.08421v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.08421v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.AS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.08413v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.08413v1",
                "updated": "2025-01-14T20:08:16Z",
                "updated_parsed": [
                    2025,
                    1,
                    14,
                    20,
                    8,
                    16,
                    1,
                    14,
                    0
                ],
                "published": "2025-01-14T20:08:16Z",
                "published_parsed": [
                    2025,
                    1,
                    14,
                    20,
                    8,
                    16,
                    1,
                    14,
                    0
                ],
                "title": "Ensemble of Large Language Models for Curated Labeling and Rating of\n  Free-text Data",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Ensemble of Large Language Models for Curated Labeling and Rating of\n  Free-text Data"
                },
                "summary": "Free-text responses are commonly collected in psychological studies,\nproviding rich qualitative insights that quantitative measures may not capture.\nLabeling curated topics of research interest in free-text data by multiple\ntrained human coders is typically labor-intensive and time-consuming. Though\nlarge language models (LLMs) excel in language processing, LLM-assisted\nlabeling techniques relying on closed-source LLMs cannot be directly applied to\nfree-text data, without explicit consent for external use.\n  In this study, we propose a framework of assembling locally-deployable LLMs\nto enhance the labeling of predetermined topics in free-text data under privacy\nconstraints. Analogous to annotation by multiple human raters, this framework\nleverages the heterogeneity of diverse open-source LLMs. The ensemble approach\nseeks a balance between the agreement and disagreement across LLMs, guided by a\nrelevancy scoring methodology that utilizes embedding distances between topic\ndescriptions and LLMs' reasoning. We evaluated the ensemble approach using both\npublicly accessible Reddit data from eating disorder related forums, and\nfree-text responses from eating disorder patients, both complemented by human\nannotations.\n  We found that: (1) there is heterogeneity in the performance of labeling\namong same-sized LLMs, with some showing low sensitivity but high precision,\nwhile others exhibit high sensitivity but low precision. (2) Compared to\nindividual LLMs, the ensemble of LLMs achieved the highest accuracy and optimal\nprecision-sensitivity trade-off in predicting human annotations. (3) The\nrelevancy scores across LLMs showed greater agreement than dichotomous labels,\nindicating that the relevancy scoring method effectively mitigates the\nheterogeneity in LLMs' labeling.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Free-text responses are commonly collected in psychological studies,\nproviding rich qualitative insights that quantitative measures may not capture.\nLabeling curated topics of research interest in free-text data by multiple\ntrained human coders is typically labor-intensive and time-consuming. Though\nlarge language models (LLMs) excel in language processing, LLM-assisted\nlabeling techniques relying on closed-source LLMs cannot be directly applied to\nfree-text data, without explicit consent for external use.\n  In this study, we propose a framework of assembling locally-deployable LLMs\nto enhance the labeling of predetermined topics in free-text data under privacy\nconstraints. Analogous to annotation by multiple human raters, this framework\nleverages the heterogeneity of diverse open-source LLMs. The ensemble approach\nseeks a balance between the agreement and disagreement across LLMs, guided by a\nrelevancy scoring methodology that utilizes embedding distances between topic\ndescriptions and LLMs' reasoning. We evaluated the ensemble approach using both\npublicly accessible Reddit data from eating disorder related forums, and\nfree-text responses from eating disorder patients, both complemented by human\nannotations.\n  We found that: (1) there is heterogeneity in the performance of labeling\namong same-sized LLMs, with some showing low sensitivity but high precision,\nwhile others exhibit high sensitivity but low precision. (2) Compared to\nindividual LLMs, the ensemble of LLMs achieved the highest accuracy and optimal\nprecision-sensitivity trade-off in predicting human annotations. (3) The\nrelevancy scores across LLMs showed greater agreement than dichotomous labels,\nindicating that the relevancy scoring method effectively mitigates the\nheterogeneity in LLMs' labeling."
                },
                "authors": [
                    {
                        "name": "Jiaxing Qiu"
                    },
                    {
                        "name": "Dongliang Guo"
                    },
                    {
                        "name": "Papini Natalie"
                    },
                    {
                        "name": "Peace Noelle"
                    },
                    {
                        "name": "Levinson Cheri"
                    },
                    {
                        "name": "Teague R. Henry"
                    }
                ],
                "author_detail": {
                    "name": "Teague R. Henry"
                },
                "author": "Teague R. Henry",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.08413v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.08413v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.08406v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.08406v1",
                "updated": "2025-01-14T19:53:58Z",
                "updated_parsed": [
                    2025,
                    1,
                    14,
                    19,
                    53,
                    58,
                    1,
                    14,
                    0
                ],
                "published": "2025-01-14T19:53:58Z",
                "published_parsed": [
                    2025,
                    1,
                    14,
                    19,
                    53,
                    58,
                    1,
                    14,
                    0
                ],
                "title": "OptiChat: Bridging Optimization Models and Practitioners with Large\n  Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "OptiChat: Bridging Optimization Models and Practitioners with Large\n  Language Models"
                },
                "summary": "Optimization models have been applied to solve a wide variety of\ndecision-making problems. These models are usually developed by optimization\nexperts but are used by practitioners without optimization expertise in various\napplication domains. As a result, practitioners often struggle to interact with\nand draw useful conclusions from optimization models independently. To fill\nthis gap, we introduce OptiChat, a natural language dialogue system designed to\nhelp practitioners interpret model formulation, diagnose infeasibility, analyze\nsensitivity, retrieve information, evaluate modifications, and provide\ncounterfactual explanations. By augmenting large language models (LLMs) with\nfunctional calls and code generation tailored for optimization models, we\nenable seamless interaction and minimize the risk of hallucinations in\nOptiChat. We develop a new dataset to evaluate OptiChat's performance in\nexplaining optimization models. Experiments demonstrate that OptiChat\neffectively bridges the gap between optimization models and practitioners,\ndelivering autonomous, accurate, and instant responses.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Optimization models have been applied to solve a wide variety of\ndecision-making problems. These models are usually developed by optimization\nexperts but are used by practitioners without optimization expertise in various\napplication domains. As a result, practitioners often struggle to interact with\nand draw useful conclusions from optimization models independently. To fill\nthis gap, we introduce OptiChat, a natural language dialogue system designed to\nhelp practitioners interpret model formulation, diagnose infeasibility, analyze\nsensitivity, retrieve information, evaluate modifications, and provide\ncounterfactual explanations. By augmenting large language models (LLMs) with\nfunctional calls and code generation tailored for optimization models, we\nenable seamless interaction and minimize the risk of hallucinations in\nOptiChat. We develop a new dataset to evaluate OptiChat's performance in\nexplaining optimization models. Experiments demonstrate that OptiChat\neffectively bridges the gap between optimization models and practitioners,\ndelivering autonomous, accurate, and instant responses."
                },
                "authors": [
                    {
                        "name": "Hao Chen"
                    },
                    {
                        "name": "Gonzalo Esteban Constante-Flores"
                    },
                    {
                        "name": "Krishna Sri Ipsit Mantri"
                    },
                    {
                        "name": "Sai Madhukiran Kompalli"
                    },
                    {
                        "name": "Akshdeep Singh Ahluwalia"
                    },
                    {
                        "name": "Can Li"
                    }
                ],
                "author_detail": {
                    "name": "Can Li"
                },
                "author": "Can Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.08406v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.08406v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.OC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.08328v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.08328v1",
                "updated": "2025-01-14T18:59:03Z",
                "updated_parsed": [
                    2025,
                    1,
                    14,
                    18,
                    59,
                    3,
                    1,
                    14,
                    0
                ],
                "published": "2025-01-14T18:59:03Z",
                "published_parsed": [
                    2025,
                    1,
                    14,
                    18,
                    59,
                    3,
                    1,
                    14,
                    0
                ],
                "title": "PokerBench: Training Large Language Models to become Professional Poker\n  Players",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PokerBench: Training Large Language Models to become Professional Poker\n  Players"
                },
                "summary": "We introduce PokerBench - a benchmark for evaluating the poker-playing\nabilities of large language models (LLMs). As LLMs excel in traditional NLP\ntasks, their application to complex, strategic games like poker poses a new\nchallenge. Poker, an incomplete information game, demands a multitude of skills\nsuch as mathematics, reasoning, planning, strategy, and a deep understanding of\ngame theory and human psychology. This makes Poker the ideal next frontier for\nlarge language models. PokerBench consists of a comprehensive compilation of\n11,000 most important scenarios, split between pre-flop and post-flop play,\ndeveloped in collaboration with trained poker players. We evaluate prominent\nmodels including GPT-4, ChatGPT 3.5, and various Llama and Gemma series models,\nfinding that all state-of-the-art LLMs underperform in playing optimal poker.\nHowever, after fine-tuning, these models show marked improvements. We validate\nPokerBench by having models with different scores compete with each other,\ndemonstrating that higher scores on PokerBench lead to higher win rates in\nactual poker games. Through gameplay between our fine-tuned model and GPT-4, we\nalso identify limitations of simple supervised fine-tuning for learning optimal\nplaying strategy, suggesting the need for more advanced methodologies for\neffectively training language models to excel in games. PokerBench thus\npresents a unique benchmark for a quick and reliable evaluation of the\npoker-playing ability of LLMs as well as a comprehensive benchmark to study the\nprogress of LLMs in complex game-playing scenarios. The dataset and code will\nbe made available at: \\url{https://github.com/pokerllm/pokerbench}.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce PokerBench - a benchmark for evaluating the poker-playing\nabilities of large language models (LLMs). As LLMs excel in traditional NLP\ntasks, their application to complex, strategic games like poker poses a new\nchallenge. Poker, an incomplete information game, demands a multitude of skills\nsuch as mathematics, reasoning, planning, strategy, and a deep understanding of\ngame theory and human psychology. This makes Poker the ideal next frontier for\nlarge language models. PokerBench consists of a comprehensive compilation of\n11,000 most important scenarios, split between pre-flop and post-flop play,\ndeveloped in collaboration with trained poker players. We evaluate prominent\nmodels including GPT-4, ChatGPT 3.5, and various Llama and Gemma series models,\nfinding that all state-of-the-art LLMs underperform in playing optimal poker.\nHowever, after fine-tuning, these models show marked improvements. We validate\nPokerBench by having models with different scores compete with each other,\ndemonstrating that higher scores on PokerBench lead to higher win rates in\nactual poker games. Through gameplay between our fine-tuned model and GPT-4, we\nalso identify limitations of simple supervised fine-tuning for learning optimal\nplaying strategy, suggesting the need for more advanced methodologies for\neffectively training language models to excel in games. PokerBench thus\npresents a unique benchmark for a quick and reliable evaluation of the\npoker-playing ability of LLMs as well as a comprehensive benchmark to study the\nprogress of LLMs in complex game-playing scenarios. The dataset and code will\nbe made available at: \\url{https://github.com/pokerllm/pokerbench}."
                },
                "authors": [
                    {
                        "name": "Richard Zhuang"
                    },
                    {
                        "name": "Akshat Gupta"
                    },
                    {
                        "name": "Richard Yang"
                    },
                    {
                        "name": "Aniket Rahane"
                    },
                    {
                        "name": "Zhengyu Li"
                    },
                    {
                        "name": "Gopala Anumanchipalli"
                    }
                ],
                "author_detail": {
                    "name": "Gopala Anumanchipalli"
                },
                "author": "Gopala Anumanchipalli",
                "arxiv_comment": "AAAI 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.08328v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.08328v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.GT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.08324v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.08324v1",
                "updated": "2025-01-14T18:56:33Z",
                "updated_parsed": [
                    2025,
                    1,
                    14,
                    18,
                    56,
                    33,
                    1,
                    14,
                    0
                ],
                "published": "2025-01-14T18:56:33Z",
                "published_parsed": [
                    2025,
                    1,
                    14,
                    18,
                    56,
                    33,
                    1,
                    14,
                    0
                ],
                "title": "ADAM-1: AI and Bioinformatics for Alzheimer's Detection and\n  Microbiome-Clinical Data Integrations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ADAM-1: AI and Bioinformatics for Alzheimer's Detection and\n  Microbiome-Clinical Data Integrations"
                },
                "summary": "The Alzheimer's Disease Analysis Model Generation 1 (ADAM) is a multi-agent\nlarge language model (LLM) framework designed to integrate and analyze\nmulti-modal data, including microbiome profiles, clinical datasets, and\nexternal knowledge bases, to enhance the understanding and detection of\nAlzheimer's disease (AD). By leveraging retrieval-augmented generation (RAG)\ntechniques along with its multi-agent architecture, ADAM-1 synthesizes insights\nfrom diverse data sources and contextualizes findings using literature-driven\nevidence. Comparative evaluation against XGBoost revealed similar mean F1\nscores but significantly reduced variance for ADAM-1, highlighting its\nrobustness and consistency, particularly in small laboratory datasets. While\ncurrently tailored for binary classification tasks, future iterations aim to\nincorporate additional data modalities, such as neuroimaging and biomarkers, to\nbroaden the scalability and applicability for Alzheimer's research and\ndiagnostics.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Alzheimer's Disease Analysis Model Generation 1 (ADAM) is a multi-agent\nlarge language model (LLM) framework designed to integrate and analyze\nmulti-modal data, including microbiome profiles, clinical datasets, and\nexternal knowledge bases, to enhance the understanding and detection of\nAlzheimer's disease (AD). By leveraging retrieval-augmented generation (RAG)\ntechniques along with its multi-agent architecture, ADAM-1 synthesizes insights\nfrom diverse data sources and contextualizes findings using literature-driven\nevidence. Comparative evaluation against XGBoost revealed similar mean F1\nscores but significantly reduced variance for ADAM-1, highlighting its\nrobustness and consistency, particularly in small laboratory datasets. While\ncurrently tailored for binary classification tasks, future iterations aim to\nincorporate additional data modalities, such as neuroimaging and biomarkers, to\nbroaden the scalability and applicability for Alzheimer's research and\ndiagnostics."
                },
                "authors": [
                    {
                        "name": "Ziyuan Huang"
                    },
                    {
                        "name": "Vishaldeep Kaur Sekhon"
                    },
                    {
                        "name": "Ouyang Guo"
                    },
                    {
                        "name": "Mark Newman"
                    },
                    {
                        "name": "Roozbeh Sadeghian"
                    },
                    {
                        "name": "Maria L. Vaida"
                    },
                    {
                        "name": "Cynthia Jo"
                    },
                    {
                        "name": "Doyle Ward"
                    },
                    {
                        "name": "Vanni Bucci"
                    },
                    {
                        "name": "John P. Haran"
                    }
                ],
                "author_detail": {
                    "name": "John P. Haran"
                },
                "author": "John P. Haran",
                "arxiv_comment": "16 pages, 16 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.08324v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.08324v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "68T07",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.08322v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.08322v1",
                "updated": "2025-01-14T18:55:35Z",
                "updated_parsed": [
                    2025,
                    1,
                    14,
                    18,
                    55,
                    35,
                    1,
                    14,
                    0
                ],
                "published": "2025-01-14T18:55:35Z",
                "published_parsed": [
                    2025,
                    1,
                    14,
                    18,
                    55,
                    35,
                    1,
                    14,
                    0
                ],
                "title": "Exploring Robustness of Multilingual LLMs on Real-World Noisy Data",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Exploring Robustness of Multilingual LLMs on Real-World Noisy Data"
                },
                "summary": "Large Language Models (LLMs) are trained on Web data that might contain\nspelling errors made by humans. But do they become robust to similar real-world\nnoise? In this paper, we investigate the effect of real-world spelling mistakes\non the performance of 9 language models, with parameters ranging from 0.2B to\n13B, in 3 different NLP tasks, namely Natural Language Inference (NLI), Name\nEntity Recognition (NER), and Intent Classification (IC). We perform our\nexperiments on 6 different languages and build a dictionary of real-world noise\nfor them using the Wikipedia edit history. We show that the performance gap of\nthe studied models on the clean and noisy test data averaged across all the\ndatasets and languages ranges from 2.3 to 4.3 absolute percentage points. In\naddition, mT5 models, in general, show more robustness compared to BLOOM,\nFalcon, and BERT-like models. In particular, mT5 (13B), was the most robust on\naverage overall, across the 3 tasks, and in 4 of the 6 languages.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) are trained on Web data that might contain\nspelling errors made by humans. But do they become robust to similar real-world\nnoise? In this paper, we investigate the effect of real-world spelling mistakes\non the performance of 9 language models, with parameters ranging from 0.2B to\n13B, in 3 different NLP tasks, namely Natural Language Inference (NLI), Name\nEntity Recognition (NER), and Intent Classification (IC). We perform our\nexperiments on 6 different languages and build a dictionary of real-world noise\nfor them using the Wikipedia edit history. We show that the performance gap of\nthe studied models on the clean and noisy test data averaged across all the\ndatasets and languages ranges from 2.3 to 4.3 absolute percentage points. In\naddition, mT5 models, in general, show more robustness compared to BLOOM,\nFalcon, and BERT-like models. In particular, mT5 (13B), was the most robust on\naverage overall, across the 3 tasks, and in 4 of the 6 languages."
                },
                "authors": [
                    {
                        "name": "Amirhossein Aliakbarzadeh"
                    },
                    {
                        "name": "Lucie Flek"
                    },
                    {
                        "name": "Akbar Karimi"
                    }
                ],
                "author_detail": {
                    "name": "Akbar Karimi"
                },
                "author": "Akbar Karimi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.08322v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.08322v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.08319v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.08319v1",
                "updated": "2025-01-14T18:53:00Z",
                "updated_parsed": [
                    2025,
                    1,
                    14,
                    18,
                    53,
                    0,
                    1,
                    14,
                    0
                ],
                "published": "2025-01-14T18:53:00Z",
                "published_parsed": [
                    2025,
                    1,
                    14,
                    18,
                    53,
                    0,
                    1,
                    14,
                    0
                ],
                "title": "Enhancing Automated Interpretability with Output-Centric Feature\n  Descriptions",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Enhancing Automated Interpretability with Output-Centric Feature\n  Descriptions"
                },
                "summary": "Automated interpretability pipelines generate natural language descriptions\nfor the concepts represented by features in large language models (LLMs), such\nas plants or the first word in a sentence. These descriptions are derived using\ninputs that activate the feature, which may be a dimension or a direction in\nthe model's representation space. However, identifying activating inputs is\ncostly, and the mechanistic role of a feature in model behavior is determined\nboth by how inputs cause a feature to activate and by how feature activation\naffects outputs. Using steering evaluations, we reveal that current pipelines\nprovide descriptions that fail to capture the causal effect of the feature on\noutputs. To fix this, we propose efficient, output-centric methods for\nautomatically generating feature descriptions. These methods use the tokens\nweighted higher after feature stimulation or the highest weight tokens after\napplying the vocabulary \"unembedding\" head directly to the feature. Our\noutput-centric descriptions better capture the causal effect of a feature on\nmodel outputs than input-centric descriptions, but combining the two leads to\nthe best performance on both input and output evaluations. Lastly, we show that\noutput-centric descriptions can be used to find inputs that activate features\npreviously thought to be \"dead\".",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Automated interpretability pipelines generate natural language descriptions\nfor the concepts represented by features in large language models (LLMs), such\nas plants or the first word in a sentence. These descriptions are derived using\ninputs that activate the feature, which may be a dimension or a direction in\nthe model's representation space. However, identifying activating inputs is\ncostly, and the mechanistic role of a feature in model behavior is determined\nboth by how inputs cause a feature to activate and by how feature activation\naffects outputs. Using steering evaluations, we reveal that current pipelines\nprovide descriptions that fail to capture the causal effect of the feature on\noutputs. To fix this, we propose efficient, output-centric methods for\nautomatically generating feature descriptions. These methods use the tokens\nweighted higher after feature stimulation or the highest weight tokens after\napplying the vocabulary \"unembedding\" head directly to the feature. Our\noutput-centric descriptions better capture the causal effect of a feature on\nmodel outputs than input-centric descriptions, but combining the two leads to\nthe best performance on both input and output evaluations. Lastly, we show that\noutput-centric descriptions can be used to find inputs that activate features\npreviously thought to be \"dead\"."
                },
                "authors": [
                    {
                        "name": "Yoav Gur-Arieh"
                    },
                    {
                        "name": "Roy Mayan"
                    },
                    {
                        "name": "Chen Agassy"
                    },
                    {
                        "name": "Atticus Geiger"
                    },
                    {
                        "name": "Mor Geva"
                    }
                ],
                "author_detail": {
                    "name": "Mor Geva"
                },
                "author": "Mor Geva",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.08319v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.08319v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.08306v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.08306v1",
                "updated": "2025-01-14T18:44:35Z",
                "updated_parsed": [
                    2025,
                    1,
                    14,
                    18,
                    44,
                    35,
                    1,
                    14,
                    0
                ],
                "published": "2025-01-14T18:44:35Z",
                "published_parsed": [
                    2025,
                    1,
                    14,
                    18,
                    44,
                    35,
                    1,
                    14,
                    0
                ],
                "title": "Path Loss Prediction Using Machine Learning with Extended Features",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Path Loss Prediction Using Machine Learning with Extended Features"
                },
                "summary": "Wireless communications rely on path loss modeling, which is most effective\nwhen it includes the physical details of the propagation environment. Acquiring\nthis data has historically been challenging, but geographic information system\ndata is becoming increasingly available with higher resolution and accuracy.\nAccess to such details enables propagation models to more accurately predict\ncoverage and minimize interference in wireless deployments. Machine\nlearning-based modeling can significantly support this effort, with\nfeature-based approaches allowing for accurate, efficient, and scalable\npropagation modeling. Building on previous work, we introduce an extended set\nof features that improves prediction accuracy while, most importantly,\nmaintaining model generalization across a broad range of environments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Wireless communications rely on path loss modeling, which is most effective\nwhen it includes the physical details of the propagation environment. Acquiring\nthis data has historically been challenging, but geographic information system\ndata is becoming increasingly available with higher resolution and accuracy.\nAccess to such details enables propagation models to more accurately predict\ncoverage and minimize interference in wireless deployments. Machine\nlearning-based modeling can significantly support this effort, with\nfeature-based approaches allowing for accurate, efficient, and scalable\npropagation modeling. Building on previous work, we introduce an extended set\nof features that improves prediction accuracy while, most importantly,\nmaintaining model generalization across a broad range of environments."
                },
                "authors": [
                    {
                        "name": "Jonathan Ethier"
                    },
                    {
                        "name": "Mathieu Chateauvert"
                    },
                    {
                        "name": "Ryan G. Dempsey"
                    },
                    {
                        "name": "Alexis Bose"
                    }
                ],
                "author_detail": {
                    "name": "Alexis Bose"
                },
                "author": "Alexis Bose",
                "arxiv_comment": "4 pages, 4 figures, conference paper",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.08306v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.08306v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.08292v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.08292v1",
                "updated": "2025-01-14T18:13:08Z",
                "updated_parsed": [
                    2025,
                    1,
                    14,
                    18,
                    13,
                    8,
                    1,
                    14,
                    0
                ],
                "published": "2025-01-14T18:13:08Z",
                "published_parsed": [
                    2025,
                    1,
                    14,
                    18,
                    13,
                    8,
                    1,
                    14,
                    0
                ],
                "title": "HALoGEN: Fantastic LLM Hallucinations and Where to Find Them",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "HALoGEN: Fantastic LLM Hallucinations and Where to Find Them"
                },
                "summary": "Despite their impressive ability to generate high-quality and fluent text,\ngenerative large language models (LLMs) also produce hallucinations: statements\nthat are misaligned with established world knowledge or provided input context.\nHowever, measuring hallucination can be challenging, as having humans verify\nmodel generations on-the-fly is both expensive and time-consuming. In this\nwork, we release HALoGEN, a comprehensive hallucination benchmark consisting\nof: (1) 10,923 prompts for generative models spanning nine domains including\nprogramming, scientific attribution, and summarization, and (2) automatic\nhigh-precision verifiers for each use case that decompose LLM generations into\natomic units, and verify each unit against a high-quality knowledge source. We\nuse this framework to evaluate ~150,000 generations from 14 language models,\nfinding that even the best-performing models are riddled with hallucinations\n(sometimes up to 86% of generated atomic facts depending on the domain). We\nfurther define a novel error classification for LLM hallucinations based on\nwhether they likely stem from incorrect recollection of training data (Type A\nerrors), or incorrect knowledge in training data (Type B errors), or are\nfabrication (Type C errors). We hope our framework provides a foundation to\nenable the principled study of why generative models hallucinate, and advances\nthe development of trustworthy large language models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Despite their impressive ability to generate high-quality and fluent text,\ngenerative large language models (LLMs) also produce hallucinations: statements\nthat are misaligned with established world knowledge or provided input context.\nHowever, measuring hallucination can be challenging, as having humans verify\nmodel generations on-the-fly is both expensive and time-consuming. In this\nwork, we release HALoGEN, a comprehensive hallucination benchmark consisting\nof: (1) 10,923 prompts for generative models spanning nine domains including\nprogramming, scientific attribution, and summarization, and (2) automatic\nhigh-precision verifiers for each use case that decompose LLM generations into\natomic units, and verify each unit against a high-quality knowledge source. We\nuse this framework to evaluate ~150,000 generations from 14 language models,\nfinding that even the best-performing models are riddled with hallucinations\n(sometimes up to 86% of generated atomic facts depending on the domain). We\nfurther define a novel error classification for LLM hallucinations based on\nwhether they likely stem from incorrect recollection of training data (Type A\nerrors), or incorrect knowledge in training data (Type B errors), or are\nfabrication (Type C errors). We hope our framework provides a foundation to\nenable the principled study of why generative models hallucinate, and advances\nthe development of trustworthy large language models."
                },
                "authors": [
                    {
                        "name": "Abhilasha Ravichander"
                    },
                    {
                        "name": "Shrusti Ghela"
                    },
                    {
                        "name": "David Wadden"
                    },
                    {
                        "name": "Yejin Choi"
                    }
                ],
                "author_detail": {
                    "name": "Yejin Choi"
                },
                "author": "Yejin Choi",
                "arxiv_comment": "Preprint",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.08292v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.08292v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.08276v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.08276v1",
                "updated": "2025-01-14T17:50:06Z",
                "updated_parsed": [
                    2025,
                    1,
                    14,
                    17,
                    50,
                    6,
                    1,
                    14,
                    0
                ],
                "published": "2025-01-14T17:50:06Z",
                "published_parsed": [
                    2025,
                    1,
                    14,
                    17,
                    50,
                    6,
                    1,
                    14,
                    0
                ],
                "title": "Exploring Robustness of LLMs to Sociodemographically-Conditioned\n  Paraphrasing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Exploring Robustness of LLMs to Sociodemographically-Conditioned\n  Paraphrasing"
                },
                "summary": "Large Language Models (LLMs) have shown impressive performance in various NLP\ntasks. However, there are concerns about their reliability in different domains\nof linguistic variations. Many works have proposed robustness evaluation\nmeasures for local adversarial attacks, but we need globally robust models\nunbiased to different language styles. We take a broader approach to explore a\nwider range of variations across sociodemographic dimensions to perform\nstructured reliability tests on the reasoning capacity of language models. We\nextend the SocialIQA dataset to create diverse paraphrased sets conditioned on\nsociodemographic styles. The assessment aims to provide a deeper understanding\nof LLMs in (a) their capability of generating demographic paraphrases with\nengineered prompts and (b) their reasoning capabilities in real-world, complex\nlanguage scenarios. We also explore measures such as perplexity,\nexplainability, and ATOMIC performance of paraphrases for fine-grained\nreliability analysis of LLMs on these sets. We find that demographic-specific\nparaphrasing significantly impacts the performance of language models,\nindicating that the subtleties of language variations remain a significant\nchallenge. The code and dataset will be made available for reproducibility and\nfuture research.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have shown impressive performance in various NLP\ntasks. However, there are concerns about their reliability in different domains\nof linguistic variations. Many works have proposed robustness evaluation\nmeasures for local adversarial attacks, but we need globally robust models\nunbiased to different language styles. We take a broader approach to explore a\nwider range of variations across sociodemographic dimensions to perform\nstructured reliability tests on the reasoning capacity of language models. We\nextend the SocialIQA dataset to create diverse paraphrased sets conditioned on\nsociodemographic styles. The assessment aims to provide a deeper understanding\nof LLMs in (a) their capability of generating demographic paraphrases with\nengineered prompts and (b) their reasoning capabilities in real-world, complex\nlanguage scenarios. We also explore measures such as perplexity,\nexplainability, and ATOMIC performance of paraphrases for fine-grained\nreliability analysis of LLMs on these sets. We find that demographic-specific\nparaphrasing significantly impacts the performance of language models,\nindicating that the subtleties of language variations remain a significant\nchallenge. The code and dataset will be made available for reproducibility and\nfuture research."
                },
                "authors": [
                    {
                        "name": "Pulkit Arora"
                    },
                    {
                        "name": "Akbar Karimi"
                    },
                    {
                        "name": "Lucie Flek"
                    }
                ],
                "author_detail": {
                    "name": "Lucie Flek"
                },
                "author": "Lucie Flek",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.08276v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.08276v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.06693v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.06693v2",
                "updated": "2025-01-14T17:29:06Z",
                "updated_parsed": [
                    2025,
                    1,
                    14,
                    17,
                    29,
                    6,
                    1,
                    14,
                    0
                ],
                "published": "2025-01-12T03:01:15Z",
                "published_parsed": [
                    2025,
                    1,
                    12,
                    3,
                    1,
                    15,
                    6,
                    12,
                    0
                ],
                "title": "Vid2Sim: Realistic and Interactive Simulation from Video for Urban\n  Navigation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Vid2Sim: Realistic and Interactive Simulation from Video for Urban\n  Navigation"
                },
                "summary": "Sim-to-real gap has long posed a significant challenge for robot learning in\nsimulation, preventing the deployment of learned models in the real world.\nPrevious work has primarily focused on domain randomization and system\nidentification to mitigate this gap. However, these methods are often limited\nby the inherent constraints of the simulation and graphics engines. In this\nwork, we propose Vid2Sim, a novel framework that effectively bridges the\nsim2real gap through a scalable and cost-efficient real2sim pipeline for neural\n3D scene reconstruction and simulation. Given a monocular video as input,\nVid2Sim can generate photorealistic and physically interactable 3D simulation\nenvironments to enable the reinforcement learning of visual navigation agents\nin complex urban environments. Extensive experiments demonstrate that Vid2Sim\nsignificantly improves the performance of urban navigation in the digital twins\nand real world by 31.2% and 68.3% in success rate compared with agents trained\nwith prior simulation methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Sim-to-real gap has long posed a significant challenge for robot learning in\nsimulation, preventing the deployment of learned models in the real world.\nPrevious work has primarily focused on domain randomization and system\nidentification to mitigate this gap. However, these methods are often limited\nby the inherent constraints of the simulation and graphics engines. In this\nwork, we propose Vid2Sim, a novel framework that effectively bridges the\nsim2real gap through a scalable and cost-efficient real2sim pipeline for neural\n3D scene reconstruction and simulation. Given a monocular video as input,\nVid2Sim can generate photorealistic and physically interactable 3D simulation\nenvironments to enable the reinforcement learning of visual navigation agents\nin complex urban environments. Extensive experiments demonstrate that Vid2Sim\nsignificantly improves the performance of urban navigation in the digital twins\nand real world by 31.2% and 68.3% in success rate compared with agents trained\nwith prior simulation methods."
                },
                "authors": [
                    {
                        "name": "Ziyang Xie"
                    },
                    {
                        "name": "Zhizheng Liu"
                    },
                    {
                        "name": "Zhenghao Peng"
                    },
                    {
                        "name": "Wayne Wu"
                    },
                    {
                        "name": "Bolei Zhou"
                    }
                ],
                "author_detail": {
                    "name": "Bolei Zhou"
                },
                "author": "Bolei Zhou",
                "arxiv_comment": "Project page: https://metadriverse.github.io/vid2sim/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.06693v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.06693v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.08262v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.08262v1",
                "updated": "2025-01-14T17:21:16Z",
                "updated_parsed": [
                    2025,
                    1,
                    14,
                    17,
                    21,
                    16,
                    1,
                    14,
                    0
                ],
                "published": "2025-01-14T17:21:16Z",
                "published_parsed": [
                    2025,
                    1,
                    14,
                    17,
                    21,
                    16,
                    1,
                    14,
                    0
                ],
                "title": "Addressing the sustainable AI trilemma: a case study on LLM agents and\n  RAG",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Addressing the sustainable AI trilemma: a case study on LLM agents and\n  RAG"
                },
                "summary": "Large language models (LLMs) have demonstrated significant capabilities, but\ntheir widespread deployment and more advanced applications raise critical\nsustainability challenges, particularly in inference energy consumption. We\npropose the concept of the Sustainable AI Trilemma, highlighting the tensions\nbetween AI capability, digital equity, and environmental sustainability.\nThrough a systematic case study of LLM agents and retrieval-augmented\ngeneration (RAG), we analyze the energy costs embedded in memory module designs\nand introduce novel metrics to quantify the trade-offs between energy\nconsumption and system performance. Our experimental results reveal significant\nenergy inefficiencies in current memory-augmented frameworks and demonstrate\nthat resource-constrained environments face disproportionate efficiency\npenalties. Our findings challenge the prevailing LLM-centric paradigm in agent\ndesign and provide practical insights for developing more sustainable AI\nsystems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have demonstrated significant capabilities, but\ntheir widespread deployment and more advanced applications raise critical\nsustainability challenges, particularly in inference energy consumption. We\npropose the concept of the Sustainable AI Trilemma, highlighting the tensions\nbetween AI capability, digital equity, and environmental sustainability.\nThrough a systematic case study of LLM agents and retrieval-augmented\ngeneration (RAG), we analyze the energy costs embedded in memory module designs\nand introduce novel metrics to quantify the trade-offs between energy\nconsumption and system performance. Our experimental results reveal significant\nenergy inefficiencies in current memory-augmented frameworks and demonstrate\nthat resource-constrained environments face disproportionate efficiency\npenalties. Our findings challenge the prevailing LLM-centric paradigm in agent\ndesign and provide practical insights for developing more sustainable AI\nsystems."
                },
                "authors": [
                    {
                        "name": "Hui Wu"
                    },
                    {
                        "name": "Xiaoyang Wang"
                    },
                    {
                        "name": "Zhong Fan"
                    }
                ],
                "author_detail": {
                    "name": "Zhong Fan"
                },
                "author": "Zhong Fan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.08262v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.08262v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.02748v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.02748v3",
                "updated": "2025-01-14T17:20:04Z",
                "updated_parsed": [
                    2025,
                    1,
                    14,
                    17,
                    20,
                    4,
                    1,
                    14,
                    0
                ],
                "published": "2024-10-03T17:57:01Z",
                "published_parsed": [
                    2024,
                    10,
                    3,
                    17,
                    57,
                    1,
                    3,
                    277,
                    0
                ],
                "title": "CriSPO: Multi-Aspect Critique-Suggestion-guided Automatic Prompt\n  Optimization for Text Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CriSPO: Multi-Aspect Critique-Suggestion-guided Automatic Prompt\n  Optimization for Text Generation"
                },
                "summary": "Existing automatic prompt engineering methods are typically designed for\ndiscriminative tasks, where new task prompts are iteratively refined with\nlimited feedback from a single metric reflecting a single aspect. However,\nthese approaches are suboptimal for generative tasks, which require more\nnuanced guidance beyond a single numeric metric to improve the prompt and\noptimize multiple aspects of the generated text. To address these challenges,\nwe propose a novel multi-aspect Critique-Suggestion-guided automatic Prompt\nOptimization (CriSPO) approach. CriSPO introduces a critique-suggestion module\nas its core component. This module spontaneously discovers aspects, and\ncompares generated and reference texts across these aspects, providing specific\nsuggestions for prompt modification. These clear critiques and actionable\nsuggestions guide a receptive optimizer module to make more substantial\nchanges, exploring a broader and more effective search space. To further\nimprove CriSPO with multi-metric optimization, we introduce an Automatic Suffix\nTuning (AST) extension to enhance the performance of task prompts across\nmultiple metrics. We evaluate CriSPO on 4 state-of-the-art LLMs across 4\nsummarization and 5 QA datasets. Extensive experiments show 3-4% ROUGE score\nimprovement on summarization and substantial improvement of various metrics on\nQA. Code available at https://github.com/amazon-science/crispo",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Existing automatic prompt engineering methods are typically designed for\ndiscriminative tasks, where new task prompts are iteratively refined with\nlimited feedback from a single metric reflecting a single aspect. However,\nthese approaches are suboptimal for generative tasks, which require more\nnuanced guidance beyond a single numeric metric to improve the prompt and\noptimize multiple aspects of the generated text. To address these challenges,\nwe propose a novel multi-aspect Critique-Suggestion-guided automatic Prompt\nOptimization (CriSPO) approach. CriSPO introduces a critique-suggestion module\nas its core component. This module spontaneously discovers aspects, and\ncompares generated and reference texts across these aspects, providing specific\nsuggestions for prompt modification. These clear critiques and actionable\nsuggestions guide a receptive optimizer module to make more substantial\nchanges, exploring a broader and more effective search space. To further\nimprove CriSPO with multi-metric optimization, we introduce an Automatic Suffix\nTuning (AST) extension to enhance the performance of task prompts across\nmultiple metrics. We evaluate CriSPO on 4 state-of-the-art LLMs across 4\nsummarization and 5 QA datasets. Extensive experiments show 3-4% ROUGE score\nimprovement on summarization and substantial improvement of various metrics on\nQA. Code available at https://github.com/amazon-science/crispo"
                },
                "authors": [
                    {
                        "name": "Han He"
                    },
                    {
                        "name": "Qianchu Liu"
                    },
                    {
                        "name": "Lei Xu"
                    },
                    {
                        "name": "Chaitanya Shivade"
                    },
                    {
                        "name": "Yi Zhang"
                    },
                    {
                        "name": "Sundararajan Srinivasan"
                    },
                    {
                        "name": "Katrin Kirchhoff"
                    }
                ],
                "author_detail": {
                    "name": "Katrin Kirchhoff"
                },
                "author": "Katrin Kirchhoff",
                "arxiv_comment": "Accepted to AAAI-2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.02748v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.02748v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.06208v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.06208v2",
                "updated": "2025-01-14T17:18:27Z",
                "updated_parsed": [
                    2025,
                    1,
                    14,
                    17,
                    18,
                    27,
                    1,
                    14,
                    0
                ],
                "published": "2024-10-08T17:08:52Z",
                "published_parsed": [
                    2024,
                    10,
                    8,
                    17,
                    8,
                    52,
                    1,
                    282,
                    0
                ],
                "title": "A Physical Layer Security Framework for IRS-Assisted Integrated Sensing\n  and Semantic Communication Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Physical Layer Security Framework for IRS-Assisted Integrated Sensing\n  and Semantic Communication Systems"
                },
                "summary": "In this paper, we propose a physical layer security (PLS) framework for an\nintelligent reflecting surface (IRS)-assisted integrated sensing and semantic\ncommunication (ISASC) system, where a multi-antenna dual-functional semantic\nbase station (BS) serves multiple semantic communication users (SCUs) and\nmonitors a potentially malicious sensing target (MST) in the presence of an\neavesdropper (EVE). Both MST and EVE attempt to wiretap information from the\nsignals transmitted to the SCUs. The deployment of the IRS not only enhances\nPLS by directing a strong beam towards the SCUs, but also improves the\nlocalization information for the target without disclosing information about\nthe SCUs. To further strengthen PLS, we employ joint artificial noise (AN) and\ndedicated sensing signal (DSS), in addition to wiretap coding. To evaluate\nsensing accuracy, we derive the Cramer-Rao bound (CRB) for estimating the\ndirection of arrival (DoA), and to assess the PLS level of the ISASC system, we\ndetermine a closed-form expression for the semantic secrecy rate (SSR). To\nachieve an optimal trade-off between these two competing objectives, we\nformulate a multi-objective optimization problem (MOOP) for the joint design of\nthe BS's beamforming (BF) vectors and the IRS's phase shift vector. To tackle\nthis MOOP problem, the $\\epsilon$-constraint method is employed, followed by an\nalternating optimization (AO)-based algorithm that leverages the classical\nsuccessive convex approximation (SCA) and semidefinite relaxation (SDR)\ntechniques. Simulation results demonstrate that the proposed scheme outperforms\nthe baseline schemes, achieving a superior trade-off between SSR and CRB.\nSpecifically, our proposed approach improves the sensing accuracy by 5 dB\ncompared to the commonly adopted maximal ratio transmission (MRT) approach.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this paper, we propose a physical layer security (PLS) framework for an\nintelligent reflecting surface (IRS)-assisted integrated sensing and semantic\ncommunication (ISASC) system, where a multi-antenna dual-functional semantic\nbase station (BS) serves multiple semantic communication users (SCUs) and\nmonitors a potentially malicious sensing target (MST) in the presence of an\neavesdropper (EVE). Both MST and EVE attempt to wiretap information from the\nsignals transmitted to the SCUs. The deployment of the IRS not only enhances\nPLS by directing a strong beam towards the SCUs, but also improves the\nlocalization information for the target without disclosing information about\nthe SCUs. To further strengthen PLS, we employ joint artificial noise (AN) and\ndedicated sensing signal (DSS), in addition to wiretap coding. To evaluate\nsensing accuracy, we derive the Cramer-Rao bound (CRB) for estimating the\ndirection of arrival (DoA), and to assess the PLS level of the ISASC system, we\ndetermine a closed-form expression for the semantic secrecy rate (SSR). To\nachieve an optimal trade-off between these two competing objectives, we\nformulate a multi-objective optimization problem (MOOP) for the joint design of\nthe BS's beamforming (BF) vectors and the IRS's phase shift vector. To tackle\nthis MOOP problem, the $\\epsilon$-constraint method is employed, followed by an\nalternating optimization (AO)-based algorithm that leverages the classical\nsuccessive convex approximation (SCA) and semidefinite relaxation (SDR)\ntechniques. Simulation results demonstrate that the proposed scheme outperforms\nthe baseline schemes, achieving a superior trade-off between SSR and CRB.\nSpecifically, our proposed approach improves the sensing accuracy by 5 dB\ncompared to the commonly adopted maximal ratio transmission (MRT) approach."
                },
                "authors": [
                    {
                        "name": "Hamid Amiriara"
                    },
                    {
                        "name": "Mahtab Mirmohseni"
                    },
                    {
                        "name": "Ahmed Elzanaty"
                    },
                    {
                        "name": "Yi Ma"
                    },
                    {
                        "name": "Rahim Tafazolli"
                    }
                ],
                "author_detail": {
                    "name": "Rahim Tafazolli"
                },
                "author": "Rahim Tafazolli",
                "arxiv_comment": "Part of this paper has been accepted at the 2025 IEEE Wireless\n  Communications and Networking Conference, March 2025, Milan, Italy",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.06208v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.06208v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SP",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.08365v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.08365v1",
                "updated": "2025-01-14T17:18:05Z",
                "updated_parsed": [
                    2025,
                    1,
                    14,
                    17,
                    18,
                    5,
                    1,
                    14,
                    0
                ],
                "published": "2025-01-14T17:18:05Z",
                "published_parsed": [
                    2025,
                    1,
                    14,
                    17,
                    18,
                    5,
                    1,
                    14,
                    0
                ],
                "title": "Towards Best Practices for Open Datasets for LLM Training",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Towards Best Practices for Open Datasets for LLM Training"
                },
                "summary": "Many AI companies are training their large language models (LLMs) on data\nwithout the permission of the copyright owners. The permissibility of doing so\nvaries by jurisdiction: in countries like the EU and Japan, this is allowed\nunder certain restrictions, while in the United States, the legal landscape is\nmore ambiguous. Regardless of the legal status, concerns from creative\nproducers have led to several high-profile copyright lawsuits, and the threat\nof litigation is commonly cited as a reason for the recent trend towards\nminimizing the information shared about training datasets by both corporate and\npublic interest actors. This trend in limiting data information causes harm by\nhindering transparency, accountability, and innovation in the broader ecosystem\nby denying researchers, auditors, and impacted individuals access to the\ninformation needed to understand AI models.\n  While this could be mitigated by training language models on open access and\npublic domain data, at the time of writing, there are no such models (trained\nat a meaningful scale) due to the substantial technical and sociological\nchallenges in assembling the necessary corpus. These challenges include\nincomplete and unreliable metadata, the cost and complexity of digitizing\nphysical records, and the diverse set of legal and technical skills required to\nensure relevance and responsibility in a quickly changing landscape. Building\ntowards a future where AI systems can be trained on openly licensed data that\nis responsibly curated and governed requires collaboration across legal,\ntechnical, and policy domains, along with investments in metadata standards,\ndigitization, and fostering a culture of openness.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Many AI companies are training their large language models (LLMs) on data\nwithout the permission of the copyright owners. The permissibility of doing so\nvaries by jurisdiction: in countries like the EU and Japan, this is allowed\nunder certain restrictions, while in the United States, the legal landscape is\nmore ambiguous. Regardless of the legal status, concerns from creative\nproducers have led to several high-profile copyright lawsuits, and the threat\nof litigation is commonly cited as a reason for the recent trend towards\nminimizing the information shared about training datasets by both corporate and\npublic interest actors. This trend in limiting data information causes harm by\nhindering transparency, accountability, and innovation in the broader ecosystem\nby denying researchers, auditors, and impacted individuals access to the\ninformation needed to understand AI models.\n  While this could be mitigated by training language models on open access and\npublic domain data, at the time of writing, there are no such models (trained\nat a meaningful scale) due to the substantial technical and sociological\nchallenges in assembling the necessary corpus. These challenges include\nincomplete and unreliable metadata, the cost and complexity of digitizing\nphysical records, and the diverse set of legal and technical skills required to\nensure relevance and responsibility in a quickly changing landscape. Building\ntowards a future where AI systems can be trained on openly licensed data that\nis responsibly curated and governed requires collaboration across legal,\ntechnical, and policy domains, along with investments in metadata standards,\ndigitization, and fostering a culture of openness."
                },
                "authors": [
                    {
                        "name": "Stefan Baack"
                    },
                    {
                        "name": "Stella Biderman"
                    },
                    {
                        "name": "Kasia Odrozek"
                    },
                    {
                        "name": "Aviya Skowron"
                    },
                    {
                        "name": "Ayah Bdeir"
                    },
                    {
                        "name": "Jillian Bommarito"
                    },
                    {
                        "name": "Jennifer Ding"
                    },
                    {
                        "name": "Maximilian Gahntz"
                    },
                    {
                        "name": "Paul Keller"
                    },
                    {
                        "name": "Pierre-Carl Langlais"
                    },
                    {
                        "name": "Greg Lindahl"
                    },
                    {
                        "name": "Sebastian Majstorovic"
                    },
                    {
                        "name": "Nik Marda"
                    },
                    {
                        "name": "Guilherme Penedo"
                    },
                    {
                        "name": "Maarten Van Segbroeck"
                    },
                    {
                        "name": "Jennifer Wang"
                    },
                    {
                        "name": "Leandro von Werra"
                    },
                    {
                        "name": "Mitchell Baker"
                    },
                    {
                        "name": "Julie Belião"
                    },
                    {
                        "name": "Kasia Chmielinski"
                    },
                    {
                        "name": "Marzieh Fadaee"
                    },
                    {
                        "name": "Lisa Gutermuth"
                    },
                    {
                        "name": "Hynek Kydlíček"
                    },
                    {
                        "name": "Greg Leppert"
                    },
                    {
                        "name": "EM Lewis-Jong"
                    },
                    {
                        "name": "Solana Larsen"
                    },
                    {
                        "name": "Shayne Longpre"
                    },
                    {
                        "name": "Angela Oduor Lungati"
                    },
                    {
                        "name": "Cullen Miller"
                    },
                    {
                        "name": "Victor Miller"
                    },
                    {
                        "name": "Max Ryabinin"
                    },
                    {
                        "name": "Kathleen Siminyu"
                    },
                    {
                        "name": "Andrew Strait"
                    },
                    {
                        "name": "Mark Surman"
                    },
                    {
                        "name": "Anna Tumadóttir"
                    },
                    {
                        "name": "Maurice Weber"
                    },
                    {
                        "name": "Rebecca Weiss"
                    },
                    {
                        "name": "Lee White"
                    },
                    {
                        "name": "Thomas Wolf"
                    }
                ],
                "author_detail": {
                    "name": "Thomas Wolf"
                },
                "author": "Thomas Wolf",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.08365v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.08365v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.08246v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.08246v1",
                "updated": "2025-01-14T16:32:01Z",
                "updated_parsed": [
                    2025,
                    1,
                    14,
                    16,
                    32,
                    1,
                    1,
                    14,
                    0
                ],
                "published": "2025-01-14T16:32:01Z",
                "published_parsed": [
                    2025,
                    1,
                    14,
                    16,
                    32,
                    1,
                    1,
                    14,
                    0
                ],
                "title": "Text-Diffusion Red-Teaming of Large Language Models: Unveiling Harmful\n  Behaviors with Proximity Constraints",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Text-Diffusion Red-Teaming of Large Language Models: Unveiling Harmful\n  Behaviors with Proximity Constraints"
                },
                "summary": "Recent work has proposed automated red-teaming methods for testing the\nvulnerabilities of a given target large language model (LLM). These methods use\nred-teaming LLMs to uncover inputs that induce harmful behavior in a target\nLLM. In this paper, we study red-teaming strategies that enable a targeted\nsecurity assessment. We propose an optimization framework for red-teaming with\nproximity constraints, where the discovered prompts must be similar to\nreference prompts from a given dataset. This dataset serves as a template for\nthe discovered prompts, anchoring the search for test-cases to specific topics,\nwriting styles, or types of harmful behavior. We show that established\nauto-regressive model architectures do not perform well in this setting. We\ntherefore introduce a black-box red-teaming method inspired by text-diffusion\nmodels: Diffusion for Auditing and Red-Teaming (DART). DART modifies the\nreference prompt by perturbing it in the embedding space, directly controlling\nthe amount of change introduced. We systematically evaluate our method by\ncomparing its effectiveness with established methods based on model fine-tuning\nand zero- and few-shot prompting. Our results show that DART is significantly\nmore effective at discovering harmful inputs in close proximity to the\nreference prompt.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent work has proposed automated red-teaming methods for testing the\nvulnerabilities of a given target large language model (LLM). These methods use\nred-teaming LLMs to uncover inputs that induce harmful behavior in a target\nLLM. In this paper, we study red-teaming strategies that enable a targeted\nsecurity assessment. We propose an optimization framework for red-teaming with\nproximity constraints, where the discovered prompts must be similar to\nreference prompts from a given dataset. This dataset serves as a template for\nthe discovered prompts, anchoring the search for test-cases to specific topics,\nwriting styles, or types of harmful behavior. We show that established\nauto-regressive model architectures do not perform well in this setting. We\ntherefore introduce a black-box red-teaming method inspired by text-diffusion\nmodels: Diffusion for Auditing and Red-Teaming (DART). DART modifies the\nreference prompt by perturbing it in the embedding space, directly controlling\nthe amount of change introduced. We systematically evaluate our method by\ncomparing its effectiveness with established methods based on model fine-tuning\nand zero- and few-shot prompting. Our results show that DART is significantly\nmore effective at discovering harmful inputs in close proximity to the\nreference prompt."
                },
                "authors": [
                    {
                        "name": "Jonathan Nöther"
                    },
                    {
                        "name": "Adish Singla"
                    },
                    {
                        "name": "Goran Radanović"
                    }
                ],
                "author_detail": {
                    "name": "Goran Radanović"
                },
                "author": "Goran Radanović",
                "arxiv_comment": "This is an extended version of a paper published at AAAI 25",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.08246v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.08246v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.08243v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.08243v1",
                "updated": "2025-01-14T16:30:10Z",
                "updated_parsed": [
                    2025,
                    1,
                    14,
                    16,
                    30,
                    10,
                    1,
                    14,
                    0
                ],
                "published": "2025-01-14T16:30:10Z",
                "published_parsed": [
                    2025,
                    1,
                    14,
                    16,
                    30,
                    10,
                    1,
                    14,
                    0
                ],
                "title": "Engineering LLM Powered Multi-agent Framework for Autonomous CloudOps",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Engineering LLM Powered Multi-agent Framework for Autonomous CloudOps"
                },
                "summary": "Cloud Operations (CloudOps) is a rapidly growing field focused on the\nautomated management and optimization of cloud infrastructure which is\nessential for organizations navigating increasingly complex cloud environments.\nMontyCloud Inc. is one of the major companies in the CloudOps domain that\nleverages autonomous bots to manage cloud compliance, security, and continuous\noperations. To make the platform more accessible and effective to the\ncustomers, we leveraged the use of GenAI.\n  Developing a GenAI-based solution for autonomous CloudOps for the existing\nMontyCloud system presented us with various challenges such as i) diverse data\nsources; ii) orchestration of multiple processes; and iii) handling complex\nworkflows to automate routine tasks. To this end, we developed MOYA, a\nmulti-agent framework that leverages GenAI and balances autonomy with the\nnecessary human control. This framework integrates various internal and\nexternal systems and is optimized for factors like task orchestration,\nsecurity, and error mitigation while producing accurate, reliable, and relevant\ninsights by utilizing Retrieval Augmented Generation (RAG). Evaluations of our\nmulti-agent system with the help of practitioners as well as using automated\nchecks demonstrate enhanced accuracy, responsiveness, and effectiveness over\nnon-agentic approaches across complex workflows.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cloud Operations (CloudOps) is a rapidly growing field focused on the\nautomated management and optimization of cloud infrastructure which is\nessential for organizations navigating increasingly complex cloud environments.\nMontyCloud Inc. is one of the major companies in the CloudOps domain that\nleverages autonomous bots to manage cloud compliance, security, and continuous\noperations. To make the platform more accessible and effective to the\ncustomers, we leveraged the use of GenAI.\n  Developing a GenAI-based solution for autonomous CloudOps for the existing\nMontyCloud system presented us with various challenges such as i) diverse data\nsources; ii) orchestration of multiple processes; and iii) handling complex\nworkflows to automate routine tasks. To this end, we developed MOYA, a\nmulti-agent framework that leverages GenAI and balances autonomy with the\nnecessary human control. This framework integrates various internal and\nexternal systems and is optimized for factors like task orchestration,\nsecurity, and error mitigation while producing accurate, reliable, and relevant\ninsights by utilizing Retrieval Augmented Generation (RAG). Evaluations of our\nmulti-agent system with the help of practitioners as well as using automated\nchecks demonstrate enhanced accuracy, responsiveness, and effectiveness over\nnon-agentic approaches across complex workflows."
                },
                "authors": [
                    {
                        "name": "Kannan Parthasarathy"
                    },
                    {
                        "name": "Karthik Vaidhyanathan"
                    },
                    {
                        "name": "Rudra Dhar"
                    },
                    {
                        "name": "Venkat Krishnamachari"
                    },
                    {
                        "name": "Basil Muhammed"
                    },
                    {
                        "name": "Adyansh Kakran"
                    },
                    {
                        "name": "Sreemaee Akshathala"
                    },
                    {
                        "name": "Shrikara Arun"
                    },
                    {
                        "name": "Sumant Dubey"
                    },
                    {
                        "name": "Mohan Veerubhotla"
                    },
                    {
                        "name": "Amey Karan"
                    }
                ],
                "author_detail": {
                    "name": "Amey Karan"
                },
                "author": "Amey Karan",
                "arxiv_comment": "The paper has been accepted as full paper to CAIN 2025\n  (https://conf.researchr.org/home/cain-2025), co-located with ICSE 2025\n  (https://conf.researchr.org/home/icse-2025). The paper was submitted to CAIN\n  for review on 9 November 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.08243v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.08243v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.14831v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.14831v3",
                "updated": "2025-01-14T16:17:49Z",
                "updated_parsed": [
                    2025,
                    1,
                    14,
                    16,
                    17,
                    49,
                    1,
                    14,
                    0
                ],
                "published": "2024-05-23T17:47:55Z",
                "published_parsed": [
                    2024,
                    5,
                    23,
                    17,
                    47,
                    55,
                    3,
                    144,
                    0
                ],
                "title": "HippoRAG: Neurobiologically Inspired Long-Term Memory for Large Language\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "HippoRAG: Neurobiologically Inspired Long-Term Memory for Large Language\n  Models"
                },
                "summary": "In order to thrive in hostile and ever-changing natural environments,\nmammalian brains evolved to store large amounts of knowledge about the world\nand continually integrate new information while avoiding catastrophic\nforgetting. Despite the impressive accomplishments, large language models\n(LLMs), even with retrieval-augmented generation (RAG), still struggle to\nefficiently and effectively integrate a large amount of new experiences after\npre-training. In this work, we introduce HippoRAG, a novel retrieval framework\ninspired by the hippocampal indexing theory of human long-term memory to enable\ndeeper and more efficient knowledge integration over new experiences. HippoRAG\nsynergistically orchestrates LLMs, knowledge graphs, and the Personalized\nPageRank algorithm to mimic the different roles of neocortex and hippocampus in\nhuman memory. We compare HippoRAG with existing RAG methods on multi-hop\nquestion answering and show that our method outperforms the state-of-the-art\nmethods remarkably, by up to 20%. Single-step retrieval with HippoRAG achieves\ncomparable or better performance than iterative retrieval like IRCoT while\nbeing 10-30 times cheaper and 6-13 times faster, and integrating HippoRAG into\nIRCoT brings further substantial gains. Finally, we show that our method can\ntackle new types of scenarios that are out of reach of existing methods. Code\nand data are available at https://github.com/OSU-NLP-Group/HippoRAG.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In order to thrive in hostile and ever-changing natural environments,\nmammalian brains evolved to store large amounts of knowledge about the world\nand continually integrate new information while avoiding catastrophic\nforgetting. Despite the impressive accomplishments, large language models\n(LLMs), even with retrieval-augmented generation (RAG), still struggle to\nefficiently and effectively integrate a large amount of new experiences after\npre-training. In this work, we introduce HippoRAG, a novel retrieval framework\ninspired by the hippocampal indexing theory of human long-term memory to enable\ndeeper and more efficient knowledge integration over new experiences. HippoRAG\nsynergistically orchestrates LLMs, knowledge graphs, and the Personalized\nPageRank algorithm to mimic the different roles of neocortex and hippocampus in\nhuman memory. We compare HippoRAG with existing RAG methods on multi-hop\nquestion answering and show that our method outperforms the state-of-the-art\nmethods remarkably, by up to 20%. Single-step retrieval with HippoRAG achieves\ncomparable or better performance than iterative retrieval like IRCoT while\nbeing 10-30 times cheaper and 6-13 times faster, and integrating HippoRAG into\nIRCoT brings further substantial gains. Finally, we show that our method can\ntackle new types of scenarios that are out of reach of existing methods. Code\nand data are available at https://github.com/OSU-NLP-Group/HippoRAG."
                },
                "authors": [
                    {
                        "name": "Bernal Jiménez Gutiérrez"
                    },
                    {
                        "name": "Yiheng Shu"
                    },
                    {
                        "name": "Yu Gu"
                    },
                    {
                        "name": "Michihiro Yasunaga"
                    },
                    {
                        "name": "Yu Su"
                    }
                ],
                "author_detail": {
                    "name": "Yu Su"
                },
                "author": "Yu Su",
                "arxiv_comment": "NeurIPS 2024. Code and data:\n  https://github.com/OSU-NLP-Group/HippoRAG",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.14831v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.14831v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.10729v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.10729v2",
                "updated": "2025-01-14T16:17:00Z",
                "updated_parsed": [
                    2025,
                    1,
                    14,
                    16,
                    17,
                    0,
                    1,
                    14,
                    0
                ],
                "published": "2024-06-15T20:04:06Z",
                "published_parsed": [
                    2024,
                    6,
                    15,
                    20,
                    4,
                    6,
                    5,
                    167,
                    0
                ],
                "title": "A Comprehensive Survey of Foundation Models in Medicine",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Comprehensive Survey of Foundation Models in Medicine"
                },
                "summary": "Foundation models (FMs) are large-scale deep learning models that are\ndeveloped using large datasets and self-supervised learning methods. These\nmodels serve as a base for different downstream tasks, including healthcare.\nFMs have been adopted with great success across various domains within\nhealthcare. Existing healthcare-based surveys have not yet included all of\nthese domains. Therefore, we provide a detailed survey of FMs in healthcare. We\nfocus on the history, learning strategies, flagship models, applications, and\nchallenges of FMs. We explore how FMs such as the BERT and GPT families are\nreshaping various healthcare domains, including clinical large language models,\nmedical image analysis, and omics. Furthermore, we provide a detailed taxonomy\nof healthcare applications facilitated by FMs, such as clinical NLP, medical\ncomputer vision, graph learning, and other biology-related tasks. Despite the\npromising opportunities FMs provide, they also have several associated\nchallenges, which are explained in detail. We also outline open research issues\nand potential lessons learned to provide researchers and practitioners with\ninsights into the capabilities of FMs in healthcare to advance their deployment\nand mitigate associated risks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Foundation models (FMs) are large-scale deep learning models that are\ndeveloped using large datasets and self-supervised learning methods. These\nmodels serve as a base for different downstream tasks, including healthcare.\nFMs have been adopted with great success across various domains within\nhealthcare. Existing healthcare-based surveys have not yet included all of\nthese domains. Therefore, we provide a detailed survey of FMs in healthcare. We\nfocus on the history, learning strategies, flagship models, applications, and\nchallenges of FMs. We explore how FMs such as the BERT and GPT families are\nreshaping various healthcare domains, including clinical large language models,\nmedical image analysis, and omics. Furthermore, we provide a detailed taxonomy\nof healthcare applications facilitated by FMs, such as clinical NLP, medical\ncomputer vision, graph learning, and other biology-related tasks. Despite the\npromising opportunities FMs provide, they also have several associated\nchallenges, which are explained in detail. We also outline open research issues\nand potential lessons learned to provide researchers and practitioners with\ninsights into the capabilities of FMs in healthcare to advance their deployment\nand mitigate associated risks."
                },
                "authors": [
                    {
                        "name": "Wasif Khan"
                    },
                    {
                        "name": "Seowung Leem"
                    },
                    {
                        "name": "Kyle B. See"
                    },
                    {
                        "name": "Joshua K. Wong"
                    },
                    {
                        "name": "Shaoting Zhang"
                    },
                    {
                        "name": "Ruogu Fang"
                    }
                ],
                "author_detail": {
                    "name": "Ruogu Fang"
                },
                "author": "Ruogu Fang",
                "arxiv_comment": "Currently under review in IEEE REVIEWS IN BIOMEDICAL ENGINEERING",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.10729v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.10729v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.08219v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.08219v1",
                "updated": "2025-01-14T16:02:33Z",
                "updated_parsed": [
                    2025,
                    1,
                    14,
                    16,
                    2,
                    33,
                    1,
                    14,
                    0
                ],
                "published": "2025-01-14T16:02:33Z",
                "published_parsed": [
                    2025,
                    1,
                    14,
                    16,
                    2,
                    33,
                    1,
                    14,
                    0
                ],
                "title": "Investigating Energy Efficiency and Performance Trade-offs in LLM\n  Inference Across Tasks and DVFS Settings",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Investigating Energy Efficiency and Performance Trade-offs in LLM\n  Inference Across Tasks and DVFS Settings"
                },
                "summary": "Large language models (LLMs) have shown significant improvements in many\nnatural language processing (NLP) tasks, accelerating their rapid adoption\nacross many industries. These models are resource-intensive, requiring\nextensive computational resources both during training and inference, leading\nto increased energy consumption and negative environmental impact. As their\nadoption accelerates, the sustainability of LLMs has become a critical issue,\nnecessitating strategies to optimize their runtime efficiency without\ncompromising performance. Hence, it is imperative to identify the parameters\nthat significantly influence the performance and energy efficiency of LLMs. To\nthat end, in this work, we investigate the effect of important parameters on\nthe performance and energy efficiency of LLMs during inference and examine\ntheir trade-offs.\n  First, we analyze how different types of models with varying numbers of\nparameters and architectures perform on tasks like text generation, question\nanswering, and summarization by benchmarking LLMs such as Falcon-7B,\nMistral-7B-v0.1, T5-3B, GPT-2, GPT-J-6B, and GPT-Neo-2.7B. Second, we study\ninput and output sequence characteristics such as sequence length concerning\nenergy consumption, performance, and throughput. Finally, we explore the impact\nof hardware-based power-saving techniques, i.e., Dynamic Voltage Frequency\nScaling (DVFS), on the models' latency and energy efficiency. Our extensive\nbenchmarking and statistical analysis reveal many interesting findings,\nuncovering how specific optimizations can reduce energy consumption while\nmaintaining throughput and accuracy. This study provides actionable insights\nfor researchers and practitioners to design energy-efficient LLM inference\nsystems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have shown significant improvements in many\nnatural language processing (NLP) tasks, accelerating their rapid adoption\nacross many industries. These models are resource-intensive, requiring\nextensive computational resources both during training and inference, leading\nto increased energy consumption and negative environmental impact. As their\nadoption accelerates, the sustainability of LLMs has become a critical issue,\nnecessitating strategies to optimize their runtime efficiency without\ncompromising performance. Hence, it is imperative to identify the parameters\nthat significantly influence the performance and energy efficiency of LLMs. To\nthat end, in this work, we investigate the effect of important parameters on\nthe performance and energy efficiency of LLMs during inference and examine\ntheir trade-offs.\n  First, we analyze how different types of models with varying numbers of\nparameters and architectures perform on tasks like text generation, question\nanswering, and summarization by benchmarking LLMs such as Falcon-7B,\nMistral-7B-v0.1, T5-3B, GPT-2, GPT-J-6B, and GPT-Neo-2.7B. Second, we study\ninput and output sequence characteristics such as sequence length concerning\nenergy consumption, performance, and throughput. Finally, we explore the impact\nof hardware-based power-saving techniques, i.e., Dynamic Voltage Frequency\nScaling (DVFS), on the models' latency and energy efficiency. Our extensive\nbenchmarking and statistical analysis reveal many interesting findings,\nuncovering how specific optimizations can reduce energy consumption while\nmaintaining throughput and accuracy. This study provides actionable insights\nfor researchers and practitioners to design energy-efficient LLM inference\nsystems."
                },
                "authors": [
                    {
                        "name": "Paul Joe Maliakel"
                    },
                    {
                        "name": "Shashikant Ilager"
                    },
                    {
                        "name": "Ivona Brandic"
                    }
                ],
                "author_detail": {
                    "name": "Ivona Brandic"
                },
                "author": "Ivona Brandic",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.08219v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.08219v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.14012v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.14012v2",
                "updated": "2025-01-14T15:58:02Z",
                "updated_parsed": [
                    2025,
                    1,
                    14,
                    15,
                    58,
                    2,
                    1,
                    14,
                    0
                ],
                "published": "2024-11-21T10:54:35Z",
                "published_parsed": [
                    2024,
                    11,
                    21,
                    10,
                    54,
                    35,
                    3,
                    326,
                    0
                ],
                "title": "Logic Augmented Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Logic Augmented Generation"
                },
                "summary": "Semantic Knowledge Graphs (SKG) face challenges with scalability,\nflexibility, contextual understanding, and handling unstructured or ambiguous\ninformation. However, they offer formal and structured knowledge enabling\nhighly interpretable and reliable results by means of reasoning and querying.\nLarge Language Models (LLMs) overcome those limitations making them suitable in\nopen-ended tasks and unstructured environments. Nevertheless, LLMs are neither\ninterpretable nor reliable. To solve the dichotomy between LLMs and SKGs we\nenvision Logic Augmented Generation (LAG) that combines the benefits of the two\nworlds. LAG uses LLMs as Reactive Continuous Knowledge Graphs that can generate\npotentially infinite relations and tacit knowledge on-demand. SKGs are key for\ninjecting a discrete heuristic dimension with clear logical and factual\nboundaries. We exemplify LAG in two tasks of collective intelligence, i.e.,\nmedical diagnostics and climate projections. Understanding the properties and\nlimitations of LAG, which are still mostly unknown, is of utmost importance for\nenabling a variety of tasks involving tacit knowledge in order to provide\ninterpretable and effective results.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Semantic Knowledge Graphs (SKG) face challenges with scalability,\nflexibility, contextual understanding, and handling unstructured or ambiguous\ninformation. However, they offer formal and structured knowledge enabling\nhighly interpretable and reliable results by means of reasoning and querying.\nLarge Language Models (LLMs) overcome those limitations making them suitable in\nopen-ended tasks and unstructured environments. Nevertheless, LLMs are neither\ninterpretable nor reliable. To solve the dichotomy between LLMs and SKGs we\nenvision Logic Augmented Generation (LAG) that combines the benefits of the two\nworlds. LAG uses LLMs as Reactive Continuous Knowledge Graphs that can generate\npotentially infinite relations and tacit knowledge on-demand. SKGs are key for\ninjecting a discrete heuristic dimension with clear logical and factual\nboundaries. We exemplify LAG in two tasks of collective intelligence, i.e.,\nmedical diagnostics and climate projections. Understanding the properties and\nlimitations of LAG, which are still mostly unknown, is of utmost importance for\nenabling a variety of tasks involving tacit knowledge in order to provide\ninterpretable and effective results."
                },
                "authors": [
                    {
                        "name": "Aldo Gangemi"
                    },
                    {
                        "name": "Andrea Giovanni Nuzzolese"
                    }
                ],
                "author_detail": {
                    "name": "Andrea Giovanni Nuzzolese"
                },
                "author": "Andrea Giovanni Nuzzolese",
                "arxiv_comment": "10 pages, 2 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.14012v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.14012v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.08208v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.08208v1",
                "updated": "2025-01-14T15:46:39Z",
                "updated_parsed": [
                    2025,
                    1,
                    14,
                    15,
                    46,
                    39,
                    1,
                    14,
                    0
                ],
                "published": "2025-01-14T15:46:39Z",
                "published_parsed": [
                    2025,
                    1,
                    14,
                    15,
                    46,
                    39,
                    1,
                    14,
                    0
                ],
                "title": "ASTRID -- An Automated and Scalable TRIaD for the Evaluation of\n  RAG-based Clinical Question Answering Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ASTRID -- An Automated and Scalable TRIaD for the Evaluation of\n  RAG-based Clinical Question Answering Systems"
                },
                "summary": "Large Language Models (LLMs) have shown impressive potential in clinical\nquestion answering (QA), with Retrieval Augmented Generation (RAG) emerging as\na leading approach for ensuring the factual accuracy of model responses.\nHowever, current automated RAG metrics perform poorly in clinical and\nconversational use cases. Using clinical human evaluations of responses is\nexpensive, unscalable, and not conducive to the continuous iterative\ndevelopment of RAG systems. To address these challenges, we introduce ASTRID -\nan Automated and Scalable TRIaD for evaluating clinical QA systems leveraging\nRAG - consisting of three metrics: Context Relevance (CR), Refusal Accuracy\n(RA), and Conversational Faithfulness (CF). Our novel evaluation metric, CF, is\ndesigned to better capture the faithfulness of a model's response to the\nknowledge base without penalising conversational elements. To validate our\ntriad, we curate a dataset of over 200 real-world patient questions posed to an\nLLM-based QA agent during surgical follow-up for cataract surgery - the highest\nvolume operation in the world - augmented with clinician-selected questions for\nemergency, clinical, and non-clinical out-of-domain scenarios. We demonstrate\nthat CF can predict human ratings of faithfulness better than existing\ndefinitions for conversational use cases. Furthermore, we show that evaluation\nusing our triad consisting of CF, RA, and CR exhibits alignment with clinician\nassessment for inappropriate, harmful, or unhelpful responses. Finally, using\nnine different LLMs, we demonstrate that the three metrics can closely agree\nwith human evaluations, highlighting the potential of these metrics for use in\nLLM-driven automated evaluation pipelines. We also publish the prompts and\ndatasets for these experiments, providing valuable resources for further\nresearch and development.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have shown impressive potential in clinical\nquestion answering (QA), with Retrieval Augmented Generation (RAG) emerging as\na leading approach for ensuring the factual accuracy of model responses.\nHowever, current automated RAG metrics perform poorly in clinical and\nconversational use cases. Using clinical human evaluations of responses is\nexpensive, unscalable, and not conducive to the continuous iterative\ndevelopment of RAG systems. To address these challenges, we introduce ASTRID -\nan Automated and Scalable TRIaD for evaluating clinical QA systems leveraging\nRAG - consisting of three metrics: Context Relevance (CR), Refusal Accuracy\n(RA), and Conversational Faithfulness (CF). Our novel evaluation metric, CF, is\ndesigned to better capture the faithfulness of a model's response to the\nknowledge base without penalising conversational elements. To validate our\ntriad, we curate a dataset of over 200 real-world patient questions posed to an\nLLM-based QA agent during surgical follow-up for cataract surgery - the highest\nvolume operation in the world - augmented with clinician-selected questions for\nemergency, clinical, and non-clinical out-of-domain scenarios. We demonstrate\nthat CF can predict human ratings of faithfulness better than existing\ndefinitions for conversational use cases. Furthermore, we show that evaluation\nusing our triad consisting of CF, RA, and CR exhibits alignment with clinician\nassessment for inappropriate, harmful, or unhelpful responses. Finally, using\nnine different LLMs, we demonstrate that the three metrics can closely agree\nwith human evaluations, highlighting the potential of these metrics for use in\nLLM-driven automated evaluation pipelines. We also publish the prompts and\ndatasets for these experiments, providing valuable resources for further\nresearch and development."
                },
                "authors": [
                    {
                        "name": "Mohita Chowdhury"
                    },
                    {
                        "name": "Yajie Vera He"
                    },
                    {
                        "name": "Aisling Higham"
                    },
                    {
                        "name": "Ernest Lim"
                    }
                ],
                "author_detail": {
                    "name": "Ernest Lim"
                },
                "author": "Ernest Lim",
                "arxiv_comment": "29 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.08208v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.08208v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.08203v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.08203v1",
                "updated": "2025-01-14T15:38:41Z",
                "updated_parsed": [
                    2025,
                    1,
                    14,
                    15,
                    38,
                    41,
                    1,
                    14,
                    0
                ],
                "published": "2025-01-14T15:38:41Z",
                "published_parsed": [
                    2025,
                    1,
                    14,
                    15,
                    38,
                    41,
                    1,
                    14,
                    0
                ],
                "title": "ArithmAttack: Evaluating Robustness of LLMs to Noisy Context in Math\n  Problem Solving",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ArithmAttack: Evaluating Robustness of LLMs to Noisy Context in Math\n  Problem Solving"
                },
                "summary": "While Large Language Models (LLMs) have shown impressive capabilities in math\nproblem-solving tasks, their robustness to noisy inputs is not well-studied. In\nthis work, we propose ArithmAttack to examine how robust the LLMs are when they\nencounter noisy prompts that contain extra noise in the form of punctuation\nmarks. While being easy to implement, ArithmAttack does not cause any\ninformation loss since words are not added or deleted from the context. We\nevaluate the robustness of seven LLMs, including LLama3, Mistral, and\nMathstral, on noisy GSM8K and MultiArith datasets. Our experiments suggest that\nall the studied models show vulnerability to such noise, with more noise\nleading to poorer performances.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While Large Language Models (LLMs) have shown impressive capabilities in math\nproblem-solving tasks, their robustness to noisy inputs is not well-studied. In\nthis work, we propose ArithmAttack to examine how robust the LLMs are when they\nencounter noisy prompts that contain extra noise in the form of punctuation\nmarks. While being easy to implement, ArithmAttack does not cause any\ninformation loss since words are not added or deleted from the context. We\nevaluate the robustness of seven LLMs, including LLama3, Mistral, and\nMathstral, on noisy GSM8K and MultiArith datasets. Our experiments suggest that\nall the studied models show vulnerability to such noise, with more noise\nleading to poorer performances."
                },
                "authors": [
                    {
                        "name": "Zain Ul Abedin"
                    },
                    {
                        "name": "Shahzeb Qamar"
                    },
                    {
                        "name": "Lucie Flek"
                    },
                    {
                        "name": "Akbar Karimi"
                    }
                ],
                "author_detail": {
                    "name": "Akbar Karimi"
                },
                "author": "Akbar Karimi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.08203v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.08203v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.03565v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.03565v3",
                "updated": "2025-01-14T15:30:50Z",
                "updated_parsed": [
                    2025,
                    1,
                    14,
                    15,
                    30,
                    50,
                    1,
                    14,
                    0
                ],
                "published": "2024-04-04T16:20:34Z",
                "published_parsed": [
                    2024,
                    4,
                    4,
                    16,
                    20,
                    34,
                    3,
                    95,
                    0
                ],
                "title": "Personalized LLM Response Generation with Parameterized Memory Injection",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Personalized LLM Response Generation with Parameterized Memory Injection"
                },
                "summary": "Large Language Models (LLMs) have exhibited remarkable proficiency in\ncomprehending and generating natural language. On the other hand, personalized\nLLM response generation holds the potential to offer substantial benefits for\nindividuals in critical areas such as medical. Existing research has explored\nmemory-augmented methods to prompt the LLM with pre-stored user-specific\nknowledge for personalized response generation in terms of new queries. We\ncontend that such paradigm is unable to perceive fine-granularity information.\nIn this study, we propose a novel \\textbf{M}emory-\\textbf{i}njected approach\nusing parameter-efficient fine-tuning (PEFT) and along with a Bayesian\nOptimisation searching strategy to achieve \\textbf{L}LM\n\\textbf{P}ersonalization(\\textbf{MiLP}).",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have exhibited remarkable proficiency in\ncomprehending and generating natural language. On the other hand, personalized\nLLM response generation holds the potential to offer substantial benefits for\nindividuals in critical areas such as medical. Existing research has explored\nmemory-augmented methods to prompt the LLM with pre-stored user-specific\nknowledge for personalized response generation in terms of new queries. We\ncontend that such paradigm is unable to perceive fine-granularity information.\nIn this study, we propose a novel \\textbf{M}emory-\\textbf{i}njected approach\nusing parameter-efficient fine-tuning (PEFT) and along with a Bayesian\nOptimisation searching strategy to achieve \\textbf{L}LM\n\\textbf{P}ersonalization(\\textbf{MiLP})."
                },
                "authors": [
                    {
                        "name": "Kai Zhang"
                    },
                    {
                        "name": "Yejin Kim"
                    },
                    {
                        "name": "Xiaozhong Liu"
                    }
                ],
                "author_detail": {
                    "name": "Xiaozhong Liu"
                },
                "author": "Xiaozhong Liu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.03565v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.03565v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            }
        ]
    }
]