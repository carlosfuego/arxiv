[
    {
        "keyword": "kv cache",
        "arxiv_results": [
            {
                "id": "http://arxiv.org/abs/2503.23367v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.23367v3",
                "updated": "2025-07-08T12:34:10Z",
                "updated_parsed": [
                    2025,
                    7,
                    8,
                    12,
                    34,
                    10,
                    1,
                    189,
                    0
                ],
                "published": "2025-03-30T08:51:19Z",
                "published_parsed": [
                    2025,
                    3,
                    30,
                    8,
                    51,
                    19,
                    6,
                    89,
                    0
                ],
                "title": "FastVAR: Linear Visual Autoregressive Modeling via Cached Token Pruning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FastVAR: Linear Visual Autoregressive Modeling via Cached Token Pruning"
                },
                "summary": "Visual Autoregressive (VAR) modeling has gained popularity for its shift\ntowards next-scale prediction. However, existing VAR paradigms process the\nentire token map at each scale step, leading to the complexity and runtime\nscaling dramatically with image resolution. To address this challenge, we\npropose FastVAR, a post-training acceleration method for efficient resolution\nscaling with VARs. Our key finding is that the majority of latency arises from\nthe large-scale step where most tokens have already converged. Leveraging this\nobservation, we develop the cached token pruning strategy that only forwards\npivotal tokens for scale-specific modeling while using cached tokens from\nprevious scale steps to restore the pruned slots. This significantly reduces\nthe number of forwarded tokens and improves the efficiency at larger\nresolutions. Experiments show the proposed FastVAR can further speedup\nFlashAttention-accelerated VAR by 2.7$\\times$ with negligible performance drop\nof <1%. We further extend FastVAR to zero-shot generation of higher resolution\nimages. In particular, FastVAR can generate one 2K image with 15GB memory\nfootprints in 1.5s on a single NVIDIA 3090 GPU. Code is available at\nhttps://github.com/csguoh/FastVAR.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Visual Autoregressive (VAR) modeling has gained popularity for its shift\ntowards next-scale prediction. However, existing VAR paradigms process the\nentire token map at each scale step, leading to the complexity and runtime\nscaling dramatically with image resolution. To address this challenge, we\npropose FastVAR, a post-training acceleration method for efficient resolution\nscaling with VARs. Our key finding is that the majority of latency arises from\nthe large-scale step where most tokens have already converged. Leveraging this\nobservation, we develop the cached token pruning strategy that only forwards\npivotal tokens for scale-specific modeling while using cached tokens from\nprevious scale steps to restore the pruned slots. This significantly reduces\nthe number of forwarded tokens and improves the efficiency at larger\nresolutions. Experiments show the proposed FastVAR can further speedup\nFlashAttention-accelerated VAR by 2.7$\\times$ with negligible performance drop\nof <1%. We further extend FastVAR to zero-shot generation of higher resolution\nimages. In particular, FastVAR can generate one 2K image with 15GB memory\nfootprints in 1.5s on a single NVIDIA 3090 GPU. Code is available at\nhttps://github.com/csguoh/FastVAR."
                },
                "authors": [
                    {
                        "name": "Hang Guo"
                    },
                    {
                        "name": "Yawei Li"
                    },
                    {
                        "name": "Taolin Zhang"
                    },
                    {
                        "name": "Jiangshan Wang"
                    },
                    {
                        "name": "Tao Dai"
                    },
                    {
                        "name": "Shu-Tao Xia"
                    },
                    {
                        "name": "Luca Benini"
                    }
                ],
                "author_detail": {
                    "name": "Luca Benini"
                },
                "author": "Luca Benini",
                "arxiv_comment": "ICCV2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.23367v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.23367v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.17616v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.17616v4",
                "updated": "2025-07-08T07:10:06Z",
                "updated_parsed": [
                    2025,
                    7,
                    8,
                    7,
                    10,
                    6,
                    1,
                    189,
                    0
                ],
                "published": "2024-11-26T17:28:10Z",
                "published_parsed": [
                    2024,
                    11,
                    26,
                    17,
                    28,
                    10,
                    1,
                    331,
                    0
                ],
                "title": "Towards Stabilized and Efficient Diffusion Transformers through\n  Long-Skip-Connections with Spectral Constraints",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Towards Stabilized and Efficient Diffusion Transformers through\n  Long-Skip-Connections with Spectral Constraints"
                },
                "summary": "Diffusion Transformers (DiT) have emerged as a powerful architecture for\nimage and video generation, offering superior quality and scalability. However,\ntheir practical application suffers from inherent dynamic feature instability,\nleading to error amplification during cached inference. Through systematic\nanalysis, we identify the absence of long-range feature preservation mechanisms\nas the root cause of unstable feature propagation and perturbation sensitivity.\nTo this end, we propose Skip-DiT, an image and video generative DiT variant\nenhanced with Long-Skip-Connections (LSCs) - the key efficiency component in\nU-Nets. Theoretical spectral norm and visualization analysis demonstrate how\nLSCs stabilize feature dynamics. Skip-DiT architecture and its stabilized\ndynamic feature enable an efficient statical caching mechanism that reuses deep\nfeatures across timesteps while updating shallow components. Extensive\nexperiments across the image and video generation tasks demonstrate that\nSkip-DiT achieves: (1) 4.4 times training acceleration and faster convergence,\n(2) 1.5-2 times inference acceleration with negligible quality loss and high\nfidelity to the original output, outperforming existing DiT caching methods\nacross various quantitative metrics. Our findings establish\nLong-Skip-Connections as critical architectural components for stable and\nefficient diffusion transformers. Codes are provided in the\nhttps://github.com/OpenSparseLLMs/Skip-DiT.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion Transformers (DiT) have emerged as a powerful architecture for\nimage and video generation, offering superior quality and scalability. However,\ntheir practical application suffers from inherent dynamic feature instability,\nleading to error amplification during cached inference. Through systematic\nanalysis, we identify the absence of long-range feature preservation mechanisms\nas the root cause of unstable feature propagation and perturbation sensitivity.\nTo this end, we propose Skip-DiT, an image and video generative DiT variant\nenhanced with Long-Skip-Connections (LSCs) - the key efficiency component in\nU-Nets. Theoretical spectral norm and visualization analysis demonstrate how\nLSCs stabilize feature dynamics. Skip-DiT architecture and its stabilized\ndynamic feature enable an efficient statical caching mechanism that reuses deep\nfeatures across timesteps while updating shallow components. Extensive\nexperiments across the image and video generation tasks demonstrate that\nSkip-DiT achieves: (1) 4.4 times training acceleration and faster convergence,\n(2) 1.5-2 times inference acceleration with negligible quality loss and high\nfidelity to the original output, outperforming existing DiT caching methods\nacross various quantitative metrics. Our findings establish\nLong-Skip-Connections as critical architectural components for stable and\nefficient diffusion transformers. Codes are provided in the\nhttps://github.com/OpenSparseLLMs/Skip-DiT."
                },
                "authors": [
                    {
                        "name": "Guanjie Chen"
                    },
                    {
                        "name": "Xinyu Zhao"
                    },
                    {
                        "name": "Yucheng Zhou"
                    },
                    {
                        "name": "Xiaoye Qu"
                    },
                    {
                        "name": "Tianlong Chen"
                    },
                    {
                        "name": "Yu Cheng"
                    }
                ],
                "author_detail": {
                    "name": "Yu Cheng"
                },
                "author": "Yu Cheng",
                "arxiv_comment": "17 pages, 8 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.17616v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.17616v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2306.03622v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2306.03622v3",
                "updated": "2025-07-08T02:15:07Z",
                "updated_parsed": [
                    2025,
                    7,
                    8,
                    2,
                    15,
                    7,
                    1,
                    189,
                    0
                ],
                "published": "2023-06-06T12:19:05Z",
                "published_parsed": [
                    2023,
                    6,
                    6,
                    12,
                    19,
                    5,
                    1,
                    157,
                    0
                ],
                "title": "Torpor: GPU-Enabled Serverless Computing for Low-Latency,\n  Resource-Efficient Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Torpor: GPU-Enabled Serverless Computing for Low-Latency,\n  Resource-Efficient Inference"
                },
                "summary": "Serverless computing offers a compelling cloud model for online inference\nservices. However, existing serverless platforms lack efficient support for\nGPUs, hindering their ability to deliver high-performance inference. In this\npaper, we present Torpor, a serverless platform for GPU-efficient, low-latency\ninference. To enable efficient sharing of a node's GPUs among numerous\ninference functions, Torpor maintains models in main memory and dynamically\nswaps them onto GPUs upon request arrivals (i.e., late binding with model\nswapping). Torpor uses various techniques, including asynchronous API\nredirection, GPU runtime sharing, pipelined model execution, and efficient GPU\nmemory management, to minimize latency overhead caused by model swapping.\nAdditionally, we design an interference-aware request scheduling algorithm that\nutilizes high-speed GPU interconnects to meet latency service-level objectives\n(SLOs) for individual inference functions. We have implemented Torpor and\nevaluated its performance in a production environment. Utilizing late binding\nand model swapping, Torpor can concurrently serve hundreds of inference\nfunctions on a worker node with 4 GPUs, while achieving latency performance\ncomparable to native execution, where each model is cached exclusively on a\nGPU. Pilot deployment in a leading commercial serverless cloud shows that\nTorpor reduces the GPU provisioning cost by 70% and 65% for users and the\nplatform, respectively.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Serverless computing offers a compelling cloud model for online inference\nservices. However, existing serverless platforms lack efficient support for\nGPUs, hindering their ability to deliver high-performance inference. In this\npaper, we present Torpor, a serverless platform for GPU-efficient, low-latency\ninference. To enable efficient sharing of a node's GPUs among numerous\ninference functions, Torpor maintains models in main memory and dynamically\nswaps them onto GPUs upon request arrivals (i.e., late binding with model\nswapping). Torpor uses various techniques, including asynchronous API\nredirection, GPU runtime sharing, pipelined model execution, and efficient GPU\nmemory management, to minimize latency overhead caused by model swapping.\nAdditionally, we design an interference-aware request scheduling algorithm that\nutilizes high-speed GPU interconnects to meet latency service-level objectives\n(SLOs) for individual inference functions. We have implemented Torpor and\nevaluated its performance in a production environment. Utilizing late binding\nand model swapping, Torpor can concurrently serve hundreds of inference\nfunctions on a worker node with 4 GPUs, while achieving latency performance\ncomparable to native execution, where each model is cached exclusively on a\nGPU. Pilot deployment in a leading commercial serverless cloud shows that\nTorpor reduces the GPU provisioning cost by 70% and 65% for users and the\nplatform, respectively."
                },
                "authors": [
                    {
                        "name": "Minchen Yu"
                    },
                    {
                        "name": "Ao Wang"
                    },
                    {
                        "name": "Dong Chen"
                    },
                    {
                        "name": "Haoxuan Yu"
                    },
                    {
                        "name": "Xiaonan Luo"
                    },
                    {
                        "name": "Zhuohao Li"
                    },
                    {
                        "name": "Wei Wang"
                    },
                    {
                        "name": "Ruichuan Chen"
                    },
                    {
                        "name": "Dapeng Nie"
                    },
                    {
                        "name": "Haoran Yang"
                    },
                    {
                        "name": "Yu Ding"
                    }
                ],
                "author_detail": {
                    "name": "Yu Ding"
                },
                "author": "Yu Ding",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2306.03622v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2306.03622v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.01827v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.01827v2",
                "updated": "2025-07-08T00:51:16Z",
                "updated_parsed": [
                    2025,
                    7,
                    8,
                    0,
                    51,
                    16,
                    1,
                    189,
                    0
                ],
                "published": "2024-12-02T18:59:53Z",
                "published_parsed": [
                    2024,
                    12,
                    2,
                    18,
                    59,
                    53,
                    0,
                    337,
                    0
                ],
                "title": "RandAR: Decoder-only Autoregressive Visual Generation in Random Orders",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RandAR: Decoder-only Autoregressive Visual Generation in Random Orders"
                },
                "summary": "We introduce RandAR, a decoder-only visual autoregressive (AR) model capable\nof generating images in arbitrary token orders. Unlike previous decoder-only AR\nmodels that rely on a predefined generation order, RandAR removes this\ninductive bias, unlocking new capabilities in decoder-only generation. Our\nessential design enables random order by inserting a \"position instruction\ntoken\" before each image token to be predicted, representing the spatial\nlocation of the next image token. Trained on randomly permuted token sequences\n-- a more challenging task than fixed-order generation, RandAR achieves\ncomparable performance to its conventional raster-order counterpart. More\nimportantly, decoder-only transformers trained from random orders acquire new\ncapabilities. For the efficiency bottleneck of AR models, RandAR adopts\nparallel decoding with KV-Cache at inference time, enjoying 2.5x acceleration\nwithout sacrificing generation quality. Additionally, RandAR supports\ninpainting, outpainting and resolution extrapolation in a zero-shot manner. We\nhope RandAR inspires new directions for decoder-only visual generation models\nand broadens their applications across diverse scenarios. Our project page is\nat https://rand-ar.github.io/.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce RandAR, a decoder-only visual autoregressive (AR) model capable\nof generating images in arbitrary token orders. Unlike previous decoder-only AR\nmodels that rely on a predefined generation order, RandAR removes this\ninductive bias, unlocking new capabilities in decoder-only generation. Our\nessential design enables random order by inserting a \"position instruction\ntoken\" before each image token to be predicted, representing the spatial\nlocation of the next image token. Trained on randomly permuted token sequences\n-- a more challenging task than fixed-order generation, RandAR achieves\ncomparable performance to its conventional raster-order counterpart. More\nimportantly, decoder-only transformers trained from random orders acquire new\ncapabilities. For the efficiency bottleneck of AR models, RandAR adopts\nparallel decoding with KV-Cache at inference time, enjoying 2.5x acceleration\nwithout sacrificing generation quality. Additionally, RandAR supports\ninpainting, outpainting and resolution extrapolation in a zero-shot manner. We\nhope RandAR inspires new directions for decoder-only visual generation models\nand broadens their applications across diverse scenarios. Our project page is\nat https://rand-ar.github.io/."
                },
                "authors": [
                    {
                        "name": "Ziqi Pang"
                    },
                    {
                        "name": "Tianyuan Zhang"
                    },
                    {
                        "name": "Fujun Luan"
                    },
                    {
                        "name": "Yunze Man"
                    },
                    {
                        "name": "Hao Tan"
                    },
                    {
                        "name": "Kai Zhang"
                    },
                    {
                        "name": "William T. Freeman"
                    },
                    {
                        "name": "Yu-Xiong Wang"
                    }
                ],
                "author_detail": {
                    "name": "Yu-Xiong Wang"
                },
                "author": "Yu-Xiong Wang",
                "arxiv_comment": "Project page: https://rand-ar.github.io/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.01827v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.01827v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.05240v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.05240v1",
                "updated": "2025-07-07T17:49:41Z",
                "updated_parsed": [
                    2025,
                    7,
                    7,
                    17,
                    49,
                    41,
                    0,
                    188,
                    0
                ],
                "published": "2025-07-07T17:49:41Z",
                "published_parsed": [
                    2025,
                    7,
                    7,
                    17,
                    49,
                    41,
                    0,
                    188,
                    0
                ],
                "title": "StreamVLN: Streaming Vision-and-Language Navigation via SlowFast Context\n  Modeling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "StreamVLN: Streaming Vision-and-Language Navigation via SlowFast Context\n  Modeling"
                },
                "summary": "Vision-and-Language Navigation (VLN) in real-world settings requires agents\nto process continuous visual streams and generate actions with low latency\ngrounded in language instructions. While Video-based Large Language Models\n(Video-LLMs) have driven recent progress, current VLN methods based on\nVideo-LLM often face trade-offs among fine-grained visual understanding,\nlong-term context modeling and computational efficiency. We introduce\nStreamVLN, a streaming VLN framework that employs a hybrid slow-fast context\nmodeling strategy to support multi-modal reasoning over interleaved vision,\nlanguage and action inputs. The fast-streaming dialogue context facilitates\nresponsive action generation through a sliding-window of active dialogues,\nwhile the slow-updating memory context compresses historical visual states\nusing a 3D-aware token pruning strategy. With this slow-fast design, StreamVLN\nachieves coherent multi-turn dialogue through efficient KV cache reuse,\nsupporting long video streams with bounded context size and inference cost.\nExperiments on VLN-CE benchmarks demonstrate state-of-the-art performance with\nstable low latency, ensuring robustness and efficiency in real-world\ndeployment. The project page is:\n\\href{https://streamvln.github.io/}{https://streamvln.github.io/}.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Vision-and-Language Navigation (VLN) in real-world settings requires agents\nto process continuous visual streams and generate actions with low latency\ngrounded in language instructions. While Video-based Large Language Models\n(Video-LLMs) have driven recent progress, current VLN methods based on\nVideo-LLM often face trade-offs among fine-grained visual understanding,\nlong-term context modeling and computational efficiency. We introduce\nStreamVLN, a streaming VLN framework that employs a hybrid slow-fast context\nmodeling strategy to support multi-modal reasoning over interleaved vision,\nlanguage and action inputs. The fast-streaming dialogue context facilitates\nresponsive action generation through a sliding-window of active dialogues,\nwhile the slow-updating memory context compresses historical visual states\nusing a 3D-aware token pruning strategy. With this slow-fast design, StreamVLN\nachieves coherent multi-turn dialogue through efficient KV cache reuse,\nsupporting long video streams with bounded context size and inference cost.\nExperiments on VLN-CE benchmarks demonstrate state-of-the-art performance with\nstable low latency, ensuring robustness and efficiency in real-world\ndeployment. The project page is:\n\\href{https://streamvln.github.io/}{https://streamvln.github.io/}."
                },
                "authors": [
                    {
                        "name": "Meng Wei"
                    },
                    {
                        "name": "Chenyang Wan"
                    },
                    {
                        "name": "Xiqian Yu"
                    },
                    {
                        "name": "Tai Wang"
                    },
                    {
                        "name": "Yuqiang Yang"
                    },
                    {
                        "name": "Xiaohan Mao"
                    },
                    {
                        "name": "Chenming Zhu"
                    },
                    {
                        "name": "Wenzhe Cai"
                    },
                    {
                        "name": "Hanqing Wang"
                    },
                    {
                        "name": "Yilun Chen"
                    },
                    {
                        "name": "Xihui Liu"
                    },
                    {
                        "name": "Jiangmiao Pang"
                    }
                ],
                "author_detail": {
                    "name": "Jiangmiao Pang"
                },
                "author": "Jiangmiao Pang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.05240v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.05240v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.04967v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.04967v1",
                "updated": "2025-07-07T13:10:01Z",
                "updated_parsed": [
                    2025,
                    7,
                    7,
                    13,
                    10,
                    1,
                    0,
                    188,
                    0
                ],
                "published": "2025-07-07T13:10:01Z",
                "published_parsed": [
                    2025,
                    7,
                    7,
                    13,
                    10,
                    1,
                    0,
                    188,
                    0
                ],
                "title": "The Case for Instance-Optimized LLMs in OLAP Databases",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Case for Instance-Optimized LLMs in OLAP Databases"
                },
                "summary": "Large Language Models (LLMs) can enhance analytics systems with powerful data\nsummarization, cleaning, and semantic transformation capabilities. However,\ndeploying LLMs at scale -- processing millions to billions of rows -- remains\nprohibitively expensive in computation and memory. We present IOLM-DB, a novel\nsystem that makes LLM-enhanced database queries practical through\nquery-specific model optimization. Instead of using general-purpose LLMs,\nIOLM-DB generates lightweight, specialized models tailored to each query's\nspecific needs using representative data samples. IOLM-DB reduces model\nfootprints by up to 76% and increases throughput by up to 3.31$\\times$ while\nmaintaining accuracy through aggressive compression techniques, including\nquantization, sparsification, and structural pruning. We further show how our\napproach enables higher parallelism on existing hardware and seamlessly\nsupports caching and batching strategies to reduce overheads. Our prototype\ndemonstrates that leveraging LLM queries inside analytics systems is feasible\nat scale, opening new possibilities for future OLAP applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) can enhance analytics systems with powerful data\nsummarization, cleaning, and semantic transformation capabilities. However,\ndeploying LLMs at scale -- processing millions to billions of rows -- remains\nprohibitively expensive in computation and memory. We present IOLM-DB, a novel\nsystem that makes LLM-enhanced database queries practical through\nquery-specific model optimization. Instead of using general-purpose LLMs,\nIOLM-DB generates lightweight, specialized models tailored to each query's\nspecific needs using representative data samples. IOLM-DB reduces model\nfootprints by up to 76% and increases throughput by up to 3.31$\\times$ while\nmaintaining accuracy through aggressive compression techniques, including\nquantization, sparsification, and structural pruning. We further show how our\napproach enables higher parallelism on existing hardware and seamlessly\nsupports caching and batching strategies to reduce overheads. Our prototype\ndemonstrates that leveraging LLM queries inside analytics systems is feasible\nat scale, opening new possibilities for future OLAP applications."
                },
                "authors": [
                    {
                        "name": "Bardia Mohammadi"
                    },
                    {
                        "name": "Laurent Bindschaedler"
                    }
                ],
                "author_detail": {
                    "name": "Laurent Bindschaedler"
                },
                "author": "Laurent Bindschaedler",
                "arxiv_journal_ref": "27th International Workshop on Design, Optimization, Languages and\n  Analytical Processing of Big Data 2025. CEUR-WS",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.04967v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.04967v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.14374v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.14374v2",
                "updated": "2025-07-07T09:25:21Z",
                "updated_parsed": [
                    2025,
                    7,
                    7,
                    9,
                    25,
                    21,
                    0,
                    188,
                    0
                ],
                "published": "2025-04-19T18:25:20Z",
                "published_parsed": [
                    2025,
                    4,
                    19,
                    18,
                    25,
                    20,
                    5,
                    109,
                    0
                ],
                "title": "A fast MPI-based Distributed Hash-Table as Surrogate Model demonstrated\n  in a coupled reactive transport HPC simulation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A fast MPI-based Distributed Hash-Table as Surrogate Model demonstrated\n  in a coupled reactive transport HPC simulation"
                },
                "summary": "Surrogate models can play a pivotal role in enhancing performance in\ncontemporary High-Performance Computing applications. Cache-based surrogates\nuse already calculated simulation results to interpolate or extrapolate further\nsimulation output values. But this approach only pays off if the access time to\nretrieve the needed values is much faster than the actual simulation. While the\nmost existing key-value stores use a Client-Server architecture with dedicated\nstorage nodes, this is not the most suitable architecture for HPC applications.\nInstead, we propose a distributed architecture where the parallel processes\noffer a part of their available memory to build a shared distributed hash table\nbased on MPI. This paper presents three DHT approaches with the special\nrequirements of HPC applications in mind. The presented lock-free design\noutperforms both DHT versions which use explicit synchronization by\ncoarse-grained resp. fine-grained locking. The lock-free DHT shows very good\nscaling regarding read and write performance. The runtime of a coupled reactive\ntransport simulation was improved between 14% and 42% using the lock-free DHT\nas a surrogate model.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Surrogate models can play a pivotal role in enhancing performance in\ncontemporary High-Performance Computing applications. Cache-based surrogates\nuse already calculated simulation results to interpolate or extrapolate further\nsimulation output values. But this approach only pays off if the access time to\nretrieve the needed values is much faster than the actual simulation. While the\nmost existing key-value stores use a Client-Server architecture with dedicated\nstorage nodes, this is not the most suitable architecture for HPC applications.\nInstead, we propose a distributed architecture where the parallel processes\noffer a part of their available memory to build a shared distributed hash table\nbased on MPI. This paper presents three DHT approaches with the special\nrequirements of HPC applications in mind. The presented lock-free design\noutperforms both DHT versions which use explicit synchronization by\ncoarse-grained resp. fine-grained locking. The lock-free DHT shows very good\nscaling regarding read and write performance. The runtime of a coupled reactive\ntransport simulation was improved between 14% and 42% using the lock-free DHT\nas a surrogate model."
                },
                "authors": [
                    {
                        "name": "Max LÃ¼bke"
                    },
                    {
                        "name": "Marco De Lucia"
                    },
                    {
                        "name": "Stefan Petri"
                    },
                    {
                        "name": "Bettina Schnor"
                    }
                ],
                "author_detail": {
                    "name": "Bettina Schnor"
                },
                "author": "Bettina Schnor",
                "arxiv_doi": "10.1007/978-3-031-97635-3_28",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1007/978-3-031-97635-3_28",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2504.14374v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.14374v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "Long version, 15 pages, 6 figures; Short version (8 pages) included\n  in the proceedings of \"25th International Conference on Computational\n  Science\" (ICCS25)",
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.04697v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.04697v1",
                "updated": "2025-07-07T06:33:59Z",
                "updated_parsed": [
                    2025,
                    7,
                    7,
                    6,
                    33,
                    59,
                    0,
                    188,
                    0
                ],
                "published": "2025-07-07T06:33:59Z",
                "published_parsed": [
                    2025,
                    7,
                    7,
                    6,
                    33,
                    59,
                    0,
                    188,
                    0
                ],
                "title": "Performance Evaluation of General Purpose Large Language Models for\n  Basic Linear Algebra Subprograms Code Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Performance Evaluation of General Purpose Large Language Models for\n  Basic Linear Algebra Subprograms Code Generation"
                },
                "summary": "Generative AI technology based on Large Language Models (LLM) has been\ndeveloped and applied to assist or automatically generate program codes. In\nthis paper, we evaluate the capability of existing general LLMs for Basic\nLinear Algebra Subprograms (BLAS) code generation for CPUs. We use two LLMs\nprovided by OpenAI: GPT-4.1, a Generative Pre-trained Transformer (GPT) model,\nand o4-mini, one of the o-series of Reasoning models. Both have been released\nin April 2025. For the routines from level-1 to 3 BLAS, we tried to generate\n(1) C code without optimization from routine name only, (2) C code with basic\nperformance optimizations (thread parallelization, SIMD vectorization, and\ncache blocking) from routine name only, and (3) C code with basic performance\noptimizations based on Fortran reference code. As a result, we found that\ncorrect code can be generated in many cases even when only routine name are\ngiven. We also confirmed that thread parallelization with OpenMP, SIMD\nvectorization, and cache blocking can be implemented to some extent, and that\nthe code is faster than the reference code.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Generative AI technology based on Large Language Models (LLM) has been\ndeveloped and applied to assist or automatically generate program codes. In\nthis paper, we evaluate the capability of existing general LLMs for Basic\nLinear Algebra Subprograms (BLAS) code generation for CPUs. We use two LLMs\nprovided by OpenAI: GPT-4.1, a Generative Pre-trained Transformer (GPT) model,\nand o4-mini, one of the o-series of Reasoning models. Both have been released\nin April 2025. For the routines from level-1 to 3 BLAS, we tried to generate\n(1) C code without optimization from routine name only, (2) C code with basic\nperformance optimizations (thread parallelization, SIMD vectorization, and\ncache blocking) from routine name only, and (3) C code with basic performance\noptimizations based on Fortran reference code. As a result, we found that\ncorrect code can be generated in many cases even when only routine name are\ngiven. We also confirmed that thread parallelization with OpenMP, SIMD\nvectorization, and cache blocking can be implemented to some extent, and that\nthe code is faster than the reference code."
                },
                "authors": [
                    {
                        "name": "Daichi Mukunoki"
                    },
                    {
                        "name": "Shun-ichiro Hayashi"
                    },
                    {
                        "name": "Tetsuya Hoshino"
                    },
                    {
                        "name": "Takahiro Katagiri"
                    }
                ],
                "author_detail": {
                    "name": "Takahiro Katagiri"
                },
                "author": "Takahiro Katagiri",
                "arxiv_comment": "8 pages, 6 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.04697v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.04697v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.04416v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.04416v1",
                "updated": "2025-07-06T15:08:49Z",
                "updated_parsed": [
                    2025,
                    7,
                    6,
                    15,
                    8,
                    49,
                    6,
                    187,
                    0
                ],
                "published": "2025-07-06T15:08:49Z",
                "published_parsed": [
                    2025,
                    7,
                    6,
                    15,
                    8,
                    49,
                    6,
                    187,
                    0
                ],
                "title": "RAT: Bridging RNN Efficiency and Attention Accuracy in Language Modeling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RAT: Bridging RNN Efficiency and Attention Accuracy in Language Modeling"
                },
                "summary": "Transformers have become the cornerstone of modern large-scale language\nmodels; however, their dependence on softmax attention poses a major\ncomputational bottleneck, particularly in long-context settings. In this work,\nrather than following prevalent approaches such as linear attention (or SSMs)\nand local attention, we introduce an intermediate design called \\rat between\nrecurrence and attention mechanisms. It partitions the input into chunks,\napplies a simple linear recurrence within each chunk to capture local\ndependencies, and then performs softmax attention across chunks to model\nlong-range interactions. By adjusting the size of the chunk, \\rat enables\nflexible trade-offs, combining the strengths of RNN and attention. Empirically,\nwith a chunk size of 16, the \\rat layer achieves a \\(7\\times\\) improvement in\ntraining speed with 100K token sequences and \\(9\\times\\) in generation at 4K\nsequence length, while maintaining similar or sometimes even better accuracy\ncompared to standard attention. We demonstrate this by training 1.3B parameter\nmodels from scratch and performing large-scale evaluations, including short-\nand long-context benchmarks, as well as supervised fine-tuning~(SFT). We\nfurther propose a hybrid architecture that interleaves \\rat with local\nattention. By combining efficient long-range modeling with strong local\ninteractions, this hybrid design not only improves inference speed and reduces\ncache memory usage compared to attention, but also consistently enhances\nperformance, for example, achieving an average 1 point gain in commonsense\nreasoning tasks, up to 4 points on code tasks, and a 1 point Rouge-L increase\nin a summarization SFT task. Code is available at\nhttps://github.com/CLAIRE-Labo/RAT",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Transformers have become the cornerstone of modern large-scale language\nmodels; however, their dependence on softmax attention poses a major\ncomputational bottleneck, particularly in long-context settings. In this work,\nrather than following prevalent approaches such as linear attention (or SSMs)\nand local attention, we introduce an intermediate design called \\rat between\nrecurrence and attention mechanisms. It partitions the input into chunks,\napplies a simple linear recurrence within each chunk to capture local\ndependencies, and then performs softmax attention across chunks to model\nlong-range interactions. By adjusting the size of the chunk, \\rat enables\nflexible trade-offs, combining the strengths of RNN and attention. Empirically,\nwith a chunk size of 16, the \\rat layer achieves a \\(7\\times\\) improvement in\ntraining speed with 100K token sequences and \\(9\\times\\) in generation at 4K\nsequence length, while maintaining similar or sometimes even better accuracy\ncompared to standard attention. We demonstrate this by training 1.3B parameter\nmodels from scratch and performing large-scale evaluations, including short-\nand long-context benchmarks, as well as supervised fine-tuning~(SFT). We\nfurther propose a hybrid architecture that interleaves \\rat with local\nattention. By combining efficient long-range modeling with strong local\ninteractions, this hybrid design not only improves inference speed and reduces\ncache memory usage compared to attention, but also consistently enhances\nperformance, for example, achieving an average 1 point gain in commonsense\nreasoning tasks, up to 4 points on code tasks, and a 1 point Rouge-L increase\nin a summarization SFT task. Code is available at\nhttps://github.com/CLAIRE-Labo/RAT"
                },
                "authors": [
                    {
                        "name": "Xiuying Wei"
                    },
                    {
                        "name": "Anunay Yadav"
                    },
                    {
                        "name": "Razvan Pascanu"
                    },
                    {
                        "name": "Caglar Gulcehre"
                    }
                ],
                "author_detail": {
                    "name": "Caglar Gulcehre"
                },
                "author": "Caglar Gulcehre",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.04416v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.04416v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.01110v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.01110v2",
                "updated": "2025-07-05T15:51:57Z",
                "updated_parsed": [
                    2025,
                    7,
                    5,
                    15,
                    51,
                    57,
                    5,
                    186,
                    0
                ],
                "published": "2025-07-01T18:12:43Z",
                "published_parsed": [
                    2025,
                    7,
                    1,
                    18,
                    12,
                    43,
                    1,
                    182,
                    0
                ],
                "title": "A LoD of Gaussians: Unified Training and Rendering for Ultra-Large Scale\n  Reconstruction with External Memory",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A LoD of Gaussians: Unified Training and Rendering for Ultra-Large Scale\n  Reconstruction with External Memory"
                },
                "summary": "Gaussian Splatting has emerged as a high-performance technique for novel view\nsynthesis, enabling real-time rendering and high-quality reconstruction of\nsmall scenes. However, scaling to larger environments has so far relied on\npartitioning the scene into chunks -- a strategy that introduces artifacts at\nchunk boundaries, complicates training across varying scales, and is poorly\nsuited to unstructured scenarios such as city-scale flyovers combined with\nstreet-level views. Moreover, rendering remains fundamentally limited by GPU\nmemory, as all visible chunks must reside in VRAM simultaneously. We introduce\nA LoD of Gaussians, a framework for training and rendering ultra-large-scale\nGaussian scenes on a single consumer-grade GPU -- without partitioning. Our\nmethod stores the full scene out-of-core (e.g., in CPU memory) and trains a\nLevel-of-Detail (LoD) representation directly, dynamically streaming only the\nrelevant Gaussians. A hybrid data structure combining Gaussian hierarchies with\nSequential Point Trees enables efficient, view-dependent LoD selection, while a\nlightweight caching and view scheduling system exploits temporal coherence to\nsupport real-time streaming and rendering. Together, these innovations enable\nseamless multi-scale reconstruction and interactive visualization of complex\nscenes -- from broad aerial views to fine-grained ground-level details.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Gaussian Splatting has emerged as a high-performance technique for novel view\nsynthesis, enabling real-time rendering and high-quality reconstruction of\nsmall scenes. However, scaling to larger environments has so far relied on\npartitioning the scene into chunks -- a strategy that introduces artifacts at\nchunk boundaries, complicates training across varying scales, and is poorly\nsuited to unstructured scenarios such as city-scale flyovers combined with\nstreet-level views. Moreover, rendering remains fundamentally limited by GPU\nmemory, as all visible chunks must reside in VRAM simultaneously. We introduce\nA LoD of Gaussians, a framework for training and rendering ultra-large-scale\nGaussian scenes on a single consumer-grade GPU -- without partitioning. Our\nmethod stores the full scene out-of-core (e.g., in CPU memory) and trains a\nLevel-of-Detail (LoD) representation directly, dynamically streaming only the\nrelevant Gaussians. A hybrid data structure combining Gaussian hierarchies with\nSequential Point Trees enables efficient, view-dependent LoD selection, while a\nlightweight caching and view scheduling system exploits temporal coherence to\nsupport real-time streaming and rendering. Together, these innovations enable\nseamless multi-scale reconstruction and interactive visualization of complex\nscenes -- from broad aerial views to fine-grained ground-level details."
                },
                "authors": [
                    {
                        "name": "Felix Windisch"
                    },
                    {
                        "name": "Lukas Radl"
                    },
                    {
                        "name": "Thomas KÃ¶hler"
                    },
                    {
                        "name": "Michael Steiner"
                    },
                    {
                        "name": "Dieter Schmalstieg"
                    },
                    {
                        "name": "Markus Steinberger"
                    }
                ],
                "author_detail": {
                    "name": "Markus Steinberger"
                },
                "author": "Markus Steinberger",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.01110v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.01110v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.GR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.GR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.05344v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.05344v2",
                "updated": "2025-07-05T15:40:51Z",
                "updated_parsed": [
                    2025,
                    7,
                    5,
                    15,
                    40,
                    51,
                    5,
                    186,
                    0
                ],
                "published": "2025-06-05T17:59:55Z",
                "published_parsed": [
                    2025,
                    6,
                    5,
                    17,
                    59,
                    55,
                    3,
                    156,
                    0
                ],
                "title": "SparseMM: Head Sparsity Emerges from Visual Concept Responses in MLLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SparseMM: Head Sparsity Emerges from Visual Concept Responses in MLLMs"
                },
                "summary": "Multimodal Large Language Models (MLLMs) are commonly derived by extending\npre-trained Large Language Models (LLMs) with visual capabilities. In this\nwork, we investigate how MLLMs process visual inputs by analyzing their\nattention mechanisms. We reveal a surprising sparsity phenomenon: only a small\nsubset (approximately less than 5%) of attention heads in LLMs actively\ncontribute to visual understanding, termed visual heads. To identify these\nheads efficiently, we design a training-free framework that quantifies\nhead-level visual relevance through targeted response analysis. Building on\nthis discovery, we introduce SparseMM, a KV-Cache optimization strategy that\nallocates asymmetric computation budgets to heads in LLMs based on their visual\nscores, leveraging the sparity of visual heads for accelerating the inference\nof MLLMs. Compared with prior KV-Cache acceleration methods that ignore the\nparticularity of visual, SparseMM prioritizes stress and retaining visual\nsemantics during decoding. Extensive evaluations across mainstream multimodal\nbenchmarks demonstrate that SparseMM achieves superior accuracy-efficiency\ntrade-offs. Notably, SparseMM delivers 1.38x real-time acceleration and 52%\nmemory reduction during generation while maintaining performance parity on\nefficiency test. Our project is open sourced at\nhttps://github.com/CR400AF-A/SparseMM.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multimodal Large Language Models (MLLMs) are commonly derived by extending\npre-trained Large Language Models (LLMs) with visual capabilities. In this\nwork, we investigate how MLLMs process visual inputs by analyzing their\nattention mechanisms. We reveal a surprising sparsity phenomenon: only a small\nsubset (approximately less than 5%) of attention heads in LLMs actively\ncontribute to visual understanding, termed visual heads. To identify these\nheads efficiently, we design a training-free framework that quantifies\nhead-level visual relevance through targeted response analysis. Building on\nthis discovery, we introduce SparseMM, a KV-Cache optimization strategy that\nallocates asymmetric computation budgets to heads in LLMs based on their visual\nscores, leveraging the sparity of visual heads for accelerating the inference\nof MLLMs. Compared with prior KV-Cache acceleration methods that ignore the\nparticularity of visual, SparseMM prioritizes stress and retaining visual\nsemantics during decoding. Extensive evaluations across mainstream multimodal\nbenchmarks demonstrate that SparseMM achieves superior accuracy-efficiency\ntrade-offs. Notably, SparseMM delivers 1.38x real-time acceleration and 52%\nmemory reduction during generation while maintaining performance parity on\nefficiency test. Our project is open sourced at\nhttps://github.com/CR400AF-A/SparseMM."
                },
                "authors": [
                    {
                        "name": "Jiahui Wang"
                    },
                    {
                        "name": "Zuyan Liu"
                    },
                    {
                        "name": "Yongming Rao"
                    },
                    {
                        "name": "Jiwen Lu"
                    }
                ],
                "author_detail": {
                    "name": "Jiwen Lu"
                },
                "author": "Jiwen Lu",
                "arxiv_comment": "Accepted to ICCV 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.05344v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.05344v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.00901v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.00901v2",
                "updated": "2025-07-05T13:37:48Z",
                "updated_parsed": [
                    2025,
                    7,
                    5,
                    13,
                    37,
                    48,
                    5,
                    186,
                    0
                ],
                "published": "2025-05-01T22:32:29Z",
                "published_parsed": [
                    2025,
                    5,
                    1,
                    22,
                    32,
                    29,
                    3,
                    121,
                    0
                ],
                "title": "Heterogeneous Memory Benchmarking Toolkit",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Heterogeneous Memory Benchmarking Toolkit"
                },
                "summary": "This paper presents an open-source kernel-level heterogeneous memory\ncharacterization framework (MemScope) for embedded systems. MemScope enables\nprecise characterization of the temporal behavior of available memory modules\nunder configurable contention stress scenarios. MemScope leverages kernel-level\ncontrol over physical memory allocation, cache maintenance, CPU state,\ninterrupts, and I/O device activity to accurately benchmark heterogeneous\nmemory subsystems. This gives us the privilege to directly map pieces of\ncontiguous physical memory and instantiate allocators, allowing us to finely\ncontrol cores to create and eliminate interference. Additionally, we can\nminimize noise and interruptions, guaranteeing more consistent and precise\nresults compared to equivalent user-space solutions. Running our Framework on a\nXilinx Zynq UltraScale+ ZCU102 CPU-FPGA platform demonstrates its capability to\nprecisely benchmark bandwidth and latency across various memory types,\nincluding PL-side DRAM and BRAM, in a multi-core system.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper presents an open-source kernel-level heterogeneous memory\ncharacterization framework (MemScope) for embedded systems. MemScope enables\nprecise characterization of the temporal behavior of available memory modules\nunder configurable contention stress scenarios. MemScope leverages kernel-level\ncontrol over physical memory allocation, cache maintenance, CPU state,\ninterrupts, and I/O device activity to accurately benchmark heterogeneous\nmemory subsystems. This gives us the privilege to directly map pieces of\ncontiguous physical memory and instantiate allocators, allowing us to finely\ncontrol cores to create and eliminate interference. Additionally, we can\nminimize noise and interruptions, guaranteeing more consistent and precise\nresults compared to equivalent user-space solutions. Running our Framework on a\nXilinx Zynq UltraScale+ ZCU102 CPU-FPGA platform demonstrates its capability to\nprecisely benchmark bandwidth and latency across various memory types,\nincluding PL-side DRAM and BRAM, in a multi-core system."
                },
                "authors": [
                    {
                        "name": "Golsana Ghaemi"
                    },
                    {
                        "name": "Gabriel Franco"
                    },
                    {
                        "name": "Kazem Taram"
                    },
                    {
                        "name": "Renato Mancuso"
                    }
                ],
                "author_detail": {
                    "name": "Renato Mancuso"
                },
                "author": "Renato Mancuso",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.00901v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.00901v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.03980v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.03980v1",
                "updated": "2025-07-05T10:11:37Z",
                "updated_parsed": [
                    2025,
                    7,
                    5,
                    10,
                    11,
                    37,
                    5,
                    186,
                    0
                ],
                "published": "2025-07-05T10:11:37Z",
                "published_parsed": [
                    2025,
                    7,
                    5,
                    10,
                    11,
                    37,
                    5,
                    186,
                    0
                ],
                "title": "Combination generators with optimal cache utilization and communication\n  free parallel execution",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Combination generators with optimal cache utilization and communication\n  free parallel execution"
                },
                "summary": "We introduce an efficient and elegant combination generator for producing all\ncombinations of size less than or equal to K, designed for exhaustive\ngeneration and combinatorial optimization tasks. This generator can be\nimplemented to achieve what we define as optimal efficiency: constant amortized\ntime, optimal cache utilization, embarrassingly parallel execution, and a\nrecursive structure compatible with pruning-based search. These properties are\ndifficult to satisfy simultaneously in existing generators. For example,\nclassical Gray code or lexicographic generators are typically list-based and\nsequentially defined, making them difficult to vectorized, inefficient in cache\nusage, and inherently hard to parallelize. Generators based on unranking\nmethods, while easy to parallelize, are non-recursive. These limitations reduce\ntheir applicability in our target applications, where both computational\nefficiency and recursion are crucial. We adapt Bird's algebra of\nprogramming-style calculation to derive our algorithms, a formalism for\ndeveloping correct-by-construction programs from specifications. As a result,\nall generators in this paper are first formulated in their clearest\nspecification, and efficient definitions are derived constructively through\nequational reasoning, resulting in concise and elegant divide-and-conquer\ndefinitions. Beyond presenting a combination generator, we extend our approach\nto construct generators for K-permutations, nested combinations of\ncombinations, and nested permutation-combination structures. To the best of our\nknowledge, the literature has not previously reported generators for these\nnested structures. We also develop sequential variants that produce\nconfigurations in Gray code-compatible orders -- such as the revolving door\nordering -- which are particularly useful for constructing nested generators.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce an efficient and elegant combination generator for producing all\ncombinations of size less than or equal to K, designed for exhaustive\ngeneration and combinatorial optimization tasks. This generator can be\nimplemented to achieve what we define as optimal efficiency: constant amortized\ntime, optimal cache utilization, embarrassingly parallel execution, and a\nrecursive structure compatible with pruning-based search. These properties are\ndifficult to satisfy simultaneously in existing generators. For example,\nclassical Gray code or lexicographic generators are typically list-based and\nsequentially defined, making them difficult to vectorized, inefficient in cache\nusage, and inherently hard to parallelize. Generators based on unranking\nmethods, while easy to parallelize, are non-recursive. These limitations reduce\ntheir applicability in our target applications, where both computational\nefficiency and recursion are crucial. We adapt Bird's algebra of\nprogramming-style calculation to derive our algorithms, a formalism for\ndeveloping correct-by-construction programs from specifications. As a result,\nall generators in this paper are first formulated in their clearest\nspecification, and efficient definitions are derived constructively through\nequational reasoning, resulting in concise and elegant divide-and-conquer\ndefinitions. Beyond presenting a combination generator, we extend our approach\nto construct generators for K-permutations, nested combinations of\ncombinations, and nested permutation-combination structures. To the best of our\nknowledge, the literature has not previously reported generators for these\nnested structures. We also develop sequential variants that produce\nconfigurations in Gray code-compatible orders -- such as the revolving door\nordering -- which are particularly useful for constructing nested generators."
                },
                "authors": [
                    {
                        "name": "Xi He"
                    },
                    {
                        "name": "Max. A. Little"
                    }
                ],
                "author_detail": {
                    "name": "Max. A. Little"
                },
                "author": "Max. A. Little",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.03980v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.03980v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DM",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.03919v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.03919v1",
                "updated": "2025-07-05T06:55:45Z",
                "updated_parsed": [
                    2025,
                    7,
                    5,
                    6,
                    55,
                    45,
                    5,
                    186,
                    0
                ],
                "published": "2025-07-05T06:55:45Z",
                "published_parsed": [
                    2025,
                    7,
                    5,
                    6,
                    55,
                    45,
                    5,
                    186,
                    0
                ],
                "title": "PFCS: Prime Factorization Cache System for Deterministic Data\n  Relationship Discovery",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PFCS: Prime Factorization Cache System for Deterministic Data\n  Relationship Discovery"
                },
                "summary": "Cache systems fundamentally limit modern computing performance due to their\ninability to precisely capture data relationships. While achieving 85-92% hit\nrates, traditional systems rely on statistical heuristics that cannot guarantee\nrelationship discovery, leading to suboptimal prefetching and resource waste.\nWe present PFCS (Prime Factorization Cache System), which leverages the\nmathematical uniqueness of prime factorization to achieve deterministic\nrelationship discovery with zero false positives. PFCS assigns unique primes to\ndata elements and represents relationships as composite numbers, enabling the\nrecovery of perfect relationships through factorization. A comprehensive\nevaluation across database, ML, and HPC workloads demonstrates an average\nperformance improvement of x 6.2, 98.9% hit rates, and a 38% power reduction\ncompared to state-of-the-art systems. The mathematical foundation provides\nformal guarantees impossible with approximation-based approaches, establishing\na new paradigm for cache system design",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cache systems fundamentally limit modern computing performance due to their\ninability to precisely capture data relationships. While achieving 85-92% hit\nrates, traditional systems rely on statistical heuristics that cannot guarantee\nrelationship discovery, leading to suboptimal prefetching and resource waste.\nWe present PFCS (Prime Factorization Cache System), which leverages the\nmathematical uniqueness of prime factorization to achieve deterministic\nrelationship discovery with zero false positives. PFCS assigns unique primes to\ndata elements and represents relationships as composite numbers, enabling the\nrecovery of perfect relationships through factorization. A comprehensive\nevaluation across database, ML, and HPC workloads demonstrates an average\nperformance improvement of x 6.2, 98.9% hit rates, and a 38% power reduction\ncompared to state-of-the-art systems. The mathematical foundation provides\nformal guarantees impossible with approximation-based approaches, establishing\na new paradigm for cache system design"
                },
                "authors": [
                    {
                        "name": "Duy Le"
                    }
                ],
                "author_detail": {
                    "name": "Duy Le"
                },
                "author": "Duy Le",
                "arxiv_comment": "6 pages, 3 figures, 3 algorithms",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.03919v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.03919v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.06483v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.06483v3",
                "updated": "2025-07-05T01:08:40Z",
                "updated_parsed": [
                    2025,
                    7,
                    5,
                    1,
                    8,
                    40,
                    5,
                    186,
                    0
                ],
                "published": "2024-06-10T17:22:17Z",
                "published_parsed": [
                    2024,
                    6,
                    10,
                    17,
                    22,
                    17,
                    0,
                    162,
                    0
                ],
                "title": "A Taxonomy and Comparative Analysis of IPv4 Identifier Selection\n  Correctness, Security, and Performance",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Taxonomy and Comparative Analysis of IPv4 Identifier Selection\n  Correctness, Security, and Performance"
                },
                "summary": "The battle for a more secure Internet is waged on many fronts, including the\nmost basic of networking protocols. Our focus is the IPv4 Identifier (IPID), an\nIPv4 header field as old as the Internet with an equally long history as an\nexploited side channel for scanning network properties, inferring off-path\nconnections, and poisoning DNS caches. This article taxonomizes the 25-year\nhistory of IPID-based exploits and the corresponding changes to IPID selection\nmethods. By mathematically analyzing these methods' correctness and security\nand empirically evaluating their performance, we reveal recommendations for\nbest practice as well as shortcomings of current operating system\nimplementations, emphasizing the value of systematic evaluations in network\nsecurity.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The battle for a more secure Internet is waged on many fronts, including the\nmost basic of networking protocols. Our focus is the IPv4 Identifier (IPID), an\nIPv4 header field as old as the Internet with an equally long history as an\nexploited side channel for scanning network properties, inferring off-path\nconnections, and poisoning DNS caches. This article taxonomizes the 25-year\nhistory of IPID-based exploits and the corresponding changes to IPID selection\nmethods. By mathematically analyzing these methods' correctness and security\nand empirically evaluating their performance, we reveal recommendations for\nbest practice as well as shortcomings of current operating system\nimplementations, emphasizing the value of systematic evaluations in network\nsecurity."
                },
                "authors": [
                    {
                        "name": "Joshua J. Daymude"
                    },
                    {
                        "name": "Antonio M. Espinoza"
                    },
                    {
                        "name": "Sean Bergen"
                    },
                    {
                        "name": "Benjamin Mixon-Baca"
                    },
                    {
                        "name": "Jeffrey Knockel"
                    },
                    {
                        "name": "Jedidiah R. Crandall"
                    }
                ],
                "author_detail": {
                    "name": "Jedidiah R. Crandall"
                },
                "author": "Jedidiah R. Crandall",
                "arxiv_comment": "36 pages, 11 figures, 2 tables, 1 algorithm",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.06483v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.06483v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.03812v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.03812v1",
                "updated": "2025-07-04T21:09:51Z",
                "updated_parsed": [
                    2025,
                    7,
                    4,
                    21,
                    9,
                    51,
                    4,
                    185,
                    0
                ],
                "published": "2025-07-04T21:09:51Z",
                "published_parsed": [
                    2025,
                    7,
                    4,
                    21,
                    9,
                    51,
                    4,
                    185,
                    0
                ],
                "title": "Memory- and compute-optimized geometric multigrid GMGPolar for\n  curvilinear coordinate representations -- Applications to fusion plasma",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Memory- and compute-optimized geometric multigrid GMGPolar for\n  curvilinear coordinate representations -- Applications to fusion plasma"
                },
                "summary": "Tokamak fusion reactors are actively studied as a means of realizing energy\nproduction from plasma fusion. However, due to the substantial cost and time\nrequired to construct fusion reactors and run physical experiments, numerical\nexperiments are indispensable for understanding plasma physics inside tokamaks,\nsupporting the design and engineering phase, and optimizing future reactor\ndesigns. Geometric multigrid methods are optimal solvers for many problems that\narise from the discretization of partial differential equations. It has been\nshown that the multigrid solver GMGPolar solves the 2D gyrokinetic Poisson\nequation in linear complexity and with only small memory requirements compared\nto other state-of-the-art solvers. In this paper, we present a completely\nrefactored and object-oriented version of GMGPolar which offers two different\nmatrix-free implementations. Among other things, we leverage the\nSherman-Morrison formula to solve cyclic tridiagonal systems from circular line\nsolvers without additional fill-in and we apply reordering to optimize cache\naccess of circular and radial smoothing operations. With the Give approach,\nmemory requirements are further reduced and speedups of four to seven are\nobtained for usual test cases. For the Take approach, speedups of 16 to 18 can\nbe attained.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Tokamak fusion reactors are actively studied as a means of realizing energy\nproduction from plasma fusion. However, due to the substantial cost and time\nrequired to construct fusion reactors and run physical experiments, numerical\nexperiments are indispensable for understanding plasma physics inside tokamaks,\nsupporting the design and engineering phase, and optimizing future reactor\ndesigns. Geometric multigrid methods are optimal solvers for many problems that\narise from the discretization of partial differential equations. It has been\nshown that the multigrid solver GMGPolar solves the 2D gyrokinetic Poisson\nequation in linear complexity and with only small memory requirements compared\nto other state-of-the-art solvers. In this paper, we present a completely\nrefactored and object-oriented version of GMGPolar which offers two different\nmatrix-free implementations. Among other things, we leverage the\nSherman-Morrison formula to solve cyclic tridiagonal systems from circular line\nsolvers without additional fill-in and we apply reordering to optimize cache\naccess of circular and radial smoothing operations. With the Give approach,\nmemory requirements are further reduced and speedups of four to seven are\nobtained for usual test cases. For the Take approach, speedups of 16 to 18 can\nbe attained."
                },
                "authors": [
                    {
                        "name": "Julian Litz"
                    },
                    {
                        "name": "Philippe Leleux"
                    },
                    {
                        "name": "Carola Kruse"
                    },
                    {
                        "name": "Joscha Gedicke"
                    },
                    {
                        "name": "Martin J. KÃ¼hn"
                    }
                ],
                "author_detail": {
                    "name": "Martin J. KÃ¼hn"
                },
                "author": "Martin J. KÃ¼hn",
                "arxiv_comment": "29 pages, 10 figures, 4 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.03812v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.03812v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.MS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.MS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "physics.plasm-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "68Q25, 65Y20, 65Y05, 65N55, 65N06, 65B99",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.03445v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.03445v1",
                "updated": "2025-07-04T10:01:10Z",
                "updated_parsed": [
                    2025,
                    7,
                    4,
                    10,
                    1,
                    10,
                    4,
                    185,
                    0
                ],
                "published": "2025-07-04T10:01:10Z",
                "published_parsed": [
                    2025,
                    7,
                    4,
                    10,
                    1,
                    10,
                    4,
                    185,
                    0
                ],
                "title": "Quantum Algorithm for the Fixed-Radius Neighbor Search",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Quantum Algorithm for the Fixed-Radius Neighbor Search"
                },
                "summary": "The neighbor search is a computationally demanding problem, usually both\ntime- and memory-consuming. The main problem of this kind of algorithms is the\nlong execution time due to cache misses. In this work, we propose a quantum\nalgorithm for the Fixed RAdius Neighbor Search problem (FRANS) based on the\nfixed-point version of Grover's algorithm. We derive an efficient circuit for\nsolving the FRANS with linear query complexity with the number of particles\n$N$. The quantum circuit returns the list of all the neighbors' pairs within\nthe fixed radius, together with their distance, avoiding the slow down given by\ncache miss. We explicitly write the Grover's operator and analyze its gate\ncomplexity. The whole algorithm has complexity of\n$\\mathcal{O}(M^{\\frac{1}{2}}N^{2})$ in the worst-case scenario, where $M$ is\nthe number of neighboring pairs, and uses $\\mathcal{O}(\\log N)$ number of\nqubits. By employing extra ancilla qubits the depth of the circuit can be\nbrought down to $\\mathcal{O}(N\\log N)$ at the cost of $\\mathcal{O}(N)$ qubits\nfor unstructured dataset, or $\\mathcal{O}(\\text{poly}(\\log N))$ qubits for\nstructured datasets. Finally we assess the resilience of the model to the\nreadout error, suggesting an error correction-free strategy to check the\naccuracy of the results.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The neighbor search is a computationally demanding problem, usually both\ntime- and memory-consuming. The main problem of this kind of algorithms is the\nlong execution time due to cache misses. In this work, we propose a quantum\nalgorithm for the Fixed RAdius Neighbor Search problem (FRANS) based on the\nfixed-point version of Grover's algorithm. We derive an efficient circuit for\nsolving the FRANS with linear query complexity with the number of particles\n$N$. The quantum circuit returns the list of all the neighbors' pairs within\nthe fixed radius, together with their distance, avoiding the slow down given by\ncache miss. We explicitly write the Grover's operator and analyze its gate\ncomplexity. The whole algorithm has complexity of\n$\\mathcal{O}(M^{\\frac{1}{2}}N^{2})$ in the worst-case scenario, where $M$ is\nthe number of neighboring pairs, and uses $\\mathcal{O}(\\log N)$ number of\nqubits. By employing extra ancilla qubits the depth of the circuit can be\nbrought down to $\\mathcal{O}(N\\log N)$ at the cost of $\\mathcal{O}(N)$ qubits\nfor unstructured dataset, or $\\mathcal{O}(\\text{poly}(\\log N))$ qubits for\nstructured datasets. Finally we assess the resilience of the model to the\nreadout error, suggesting an error correction-free strategy to check the\naccuracy of the results."
                },
                "authors": [
                    {
                        "name": "Luca Cappelli"
                    },
                    {
                        "name": "Claudio Sanavio"
                    },
                    {
                        "name": "Alessandro Andrea Zecchi"
                    },
                    {
                        "name": "Giuseppe Murante"
                    },
                    {
                        "name": "Sauro Succi"
                    }
                ],
                "author_detail": {
                    "name": "Sauro Succi"
                },
                "author": "Sauro Succi",
                "arxiv_comment": "17 pages, 8 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.03445v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.03445v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "quant-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "quant-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.03396v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.03396v1",
                "updated": "2025-07-04T09:03:18Z",
                "updated_parsed": [
                    2025,
                    7,
                    4,
                    9,
                    3,
                    18,
                    4,
                    185,
                    0
                ],
                "published": "2025-07-04T09:03:18Z",
                "published_parsed": [
                    2025,
                    7,
                    4,
                    9,
                    3,
                    18,
                    4,
                    185,
                    0
                ],
                "title": "Numerical investigation of the effect of high voltage frequency on the\n  density of RONS species in the air atmospheric pressure gas discharge",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Numerical investigation of the effect of high voltage frequency on the\n  density of RONS species in the air atmospheric pressure gas discharge"
                },
                "summary": "In the last few decades, studies in various fields of plasma technology have\nexpanded and its application in different processes has increased. Therefore,\nthe achievement of a desirable and practical plasma with specific\ncharacteristics is of particular importance. The frequency of the applied\nvoltage is one of the important factors that play a role in the physical and\nchemical characteristics. In this research, changes in the density of active\nspecies produced in an electrical discharge using a dielectric barrier and air\nworking gas have been investigated from a frequency of 500 Hz to 500 kHz, and\nby applying a constant voltage of 2 kV, have been investigated. For this\npurpose, 87 different reactions with specific collision cross-sections were\ndefined in COMSOL Multiphysics. Other parameters, including current-voltage\nwaveform, electric field, and species densitywere evaluated. The results show\nthat under completely identical conditions, the electron temperature\ndistribution changes with increasing applied frequency, and the density of\nreactive oxygen and nitrogen species RONS decreases, but O shows an increasing\ntrend. It should be noted that the simulation results are in good agreement\nwith previous experimental and simulation reports. These results offer valuable\ninsights into optimizing plasma parameters for different applications,\npotentially resulting in better treatment outcomes across a range of\ntherapeutic domains.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In the last few decades, studies in various fields of plasma technology have\nexpanded and its application in different processes has increased. Therefore,\nthe achievement of a desirable and practical plasma with specific\ncharacteristics is of particular importance. The frequency of the applied\nvoltage is one of the important factors that play a role in the physical and\nchemical characteristics. In this research, changes in the density of active\nspecies produced in an electrical discharge using a dielectric barrier and air\nworking gas have been investigated from a frequency of 500 Hz to 500 kHz, and\nby applying a constant voltage of 2 kV, have been investigated. For this\npurpose, 87 different reactions with specific collision cross-sections were\ndefined in COMSOL Multiphysics. Other parameters, including current-voltage\nwaveform, electric field, and species densitywere evaluated. The results show\nthat under completely identical conditions, the electron temperature\ndistribution changes with increasing applied frequency, and the density of\nreactive oxygen and nitrogen species RONS decreases, but O shows an increasing\ntrend. It should be noted that the simulation results are in good agreement\nwith previous experimental and simulation reports. These results offer valuable\ninsights into optimizing plasma parameters for different applications,\npotentially resulting in better treatment outcomes across a range of\ntherapeutic domains."
                },
                "authors": [
                    {
                        "name": "Fariborz Momtazzadeh"
                    },
                    {
                        "name": "Farshad Sohbatzadeh"
                    },
                    {
                        "name": "Hamed Soltani Ahmadi"
                    },
                    {
                        "name": "Ramin Mehrabifard"
                    }
                ],
                "author_detail": {
                    "name": "Ramin Mehrabifard"
                },
                "author": "Ramin Mehrabifard",
                "arxiv_doi": "10.1007/S40042-025-01392-9.",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1007/S40042-025-01392-9.",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2507.03396v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.03396v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.plasm-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.plasm-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.14051v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.14051v3",
                "updated": "2025-07-04T06:49:31Z",
                "updated_parsed": [
                    2025,
                    7,
                    4,
                    6,
                    49,
                    31,
                    4,
                    185,
                    0
                ],
                "published": "2025-04-18T19:46:54Z",
                "published_parsed": [
                    2025,
                    4,
                    18,
                    19,
                    46,
                    54,
                    4,
                    108,
                    0
                ],
                "title": "CAOTE: KV Cache Eviction for LLMs via Attention Output Error-Based Token\n  Selection",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CAOTE: KV Cache Eviction for LLMs via Attention Output Error-Based Token\n  Selection"
                },
                "summary": "While long context support of large language models has extended their\nabilities, it also incurs challenges in memory and compute which becomes\ncrucial bottlenecks in resource-restricted devices. Token eviction, a widely\nadopted post-training methodology designed to alleviate the bottlenecks by\nevicting less important tokens from the cache, typically uses attention scores\nas proxy metrics for token importance. However, one major limitation of\nattention score as a token-wise importance metrics is that it lacks the\ninformation about contribution of tokens to the attention output. In this\npaper, we propose a simple eviction criterion based on the contribution of\ncached tokens to attention outputs. Our method, CAOTE, optimizes for eviction\nerror due to token eviction, by seamlessly integrating attention scores and\nvalue vectors. This is the first method which uses value vector information on\ntop of attention-based eviction scores. Additionally, CAOTE can act as a\nmeta-heuristic method with flexible usage with any token eviction method. We\nshow that CAOTE, when combined with the state-of-the-art attention score-based\nmethods, always improves accuracies on the downstream task, indicating the\nimportance of leveraging information from values during token eviction process.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While long context support of large language models has extended their\nabilities, it also incurs challenges in memory and compute which becomes\ncrucial bottlenecks in resource-restricted devices. Token eviction, a widely\nadopted post-training methodology designed to alleviate the bottlenecks by\nevicting less important tokens from the cache, typically uses attention scores\nas proxy metrics for token importance. However, one major limitation of\nattention score as a token-wise importance metrics is that it lacks the\ninformation about contribution of tokens to the attention output. In this\npaper, we propose a simple eviction criterion based on the contribution of\ncached tokens to attention outputs. Our method, CAOTE, optimizes for eviction\nerror due to token eviction, by seamlessly integrating attention scores and\nvalue vectors. This is the first method which uses value vector information on\ntop of attention-based eviction scores. Additionally, CAOTE can act as a\nmeta-heuristic method with flexible usage with any token eviction method. We\nshow that CAOTE, when combined with the state-of-the-art attention score-based\nmethods, always improves accuracies on the downstream task, indicating the\nimportance of leveraging information from values during token eviction process."
                },
                "authors": [
                    {
                        "name": "Raghavv Goel"
                    },
                    {
                        "name": "Junyoung Park"
                    },
                    {
                        "name": "Mukul Gagrani"
                    },
                    {
                        "name": "Dalton Jones"
                    },
                    {
                        "name": "Matthew Morse"
                    },
                    {
                        "name": "Harper Langston"
                    },
                    {
                        "name": "Mingu Lee"
                    },
                    {
                        "name": "Chris Lott"
                    }
                ],
                "author_detail": {
                    "name": "Chris Lott"
                },
                "author": "Chris Lott",
                "arxiv_comment": "14 pages, 3 figures, 10 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.14051v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.14051v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.15431v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.15431v3",
                "updated": "2025-07-04T06:36:38Z",
                "updated_parsed": [
                    2025,
                    7,
                    4,
                    6,
                    36,
                    38,
                    4,
                    185,
                    0
                ],
                "published": "2025-05-21T12:11:53Z",
                "published_parsed": [
                    2025,
                    5,
                    21,
                    12,
                    11,
                    53,
                    2,
                    141,
                    0
                ],
                "title": "Hunyuan-TurboS: Advancing Large Language Models through\n  Mamba-Transformer Synergy and Adaptive Chain-of-Thought",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Hunyuan-TurboS: Advancing Large Language Models through\n  Mamba-Transformer Synergy and Adaptive Chain-of-Thought"
                },
                "summary": "As Large Language Models (LLMs) rapidly advance, we introduce Hunyuan-TurboS,\na novel large hybrid Transformer-Mamba Mixture of Experts (MoE) model. It\nsynergistically combines Mamba's long-sequence processing efficiency with\nTransformer's superior contextual understanding. Hunyuan-TurboS features an\nadaptive long-short chain-of-thought (CoT) mechanism, dynamically switching\nbetween rapid responses for simple queries and deep \"thinking\" modes for\ncomplex problems, optimizing computational resources. Architecturally, this 56B\nactivated (560B total) parameter model employs 128 layers (Mamba2, Attention,\nFFN) with an innovative AMF/MF block pattern. Faster Mamba2 ensures linear\ncomplexity, Grouped-Query Attention minimizes KV cache, and FFNs use an MoE\nstructure. Pre-trained on 16T high-quality tokens, it supports a 256K context\nlength and is the first industry-deployed large-scale Mamba model. Our\ncomprehensive post-training strategy enhances capabilities via Supervised\nFine-Tuning (3M instructions), a novel Adaptive Long-short CoT Fusion method,\nMulti-round Deliberation Learning for iterative improvement, and a two-stage\nLarge-scale Reinforcement Learning process targeting STEM and general\ninstruction-following. Evaluations show strong performance: overall top 7 rank\non LMSYS Chatbot Arena with a score of 1356, outperforming leading models like\nGemini-2.0-Flash-001 (1352) and o4-mini-2025-04-16 (1345). TurboS also achieves\nan average of 77.9% across 23 automated benchmarks. Hunyuan-TurboS balances\nhigh performance and efficiency, offering substantial capabilities at lower\ninference costs than many reasoning models, establishing a new paradigm for\nefficient large-scale pre-trained models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As Large Language Models (LLMs) rapidly advance, we introduce Hunyuan-TurboS,\na novel large hybrid Transformer-Mamba Mixture of Experts (MoE) model. It\nsynergistically combines Mamba's long-sequence processing efficiency with\nTransformer's superior contextual understanding. Hunyuan-TurboS features an\nadaptive long-short chain-of-thought (CoT) mechanism, dynamically switching\nbetween rapid responses for simple queries and deep \"thinking\" modes for\ncomplex problems, optimizing computational resources. Architecturally, this 56B\nactivated (560B total) parameter model employs 128 layers (Mamba2, Attention,\nFFN) with an innovative AMF/MF block pattern. Faster Mamba2 ensures linear\ncomplexity, Grouped-Query Attention minimizes KV cache, and FFNs use an MoE\nstructure. Pre-trained on 16T high-quality tokens, it supports a 256K context\nlength and is the first industry-deployed large-scale Mamba model. Our\ncomprehensive post-training strategy enhances capabilities via Supervised\nFine-Tuning (3M instructions), a novel Adaptive Long-short CoT Fusion method,\nMulti-round Deliberation Learning for iterative improvement, and a two-stage\nLarge-scale Reinforcement Learning process targeting STEM and general\ninstruction-following. Evaluations show strong performance: overall top 7 rank\non LMSYS Chatbot Arena with a score of 1356, outperforming leading models like\nGemini-2.0-Flash-001 (1352) and o4-mini-2025-04-16 (1345). TurboS also achieves\nan average of 77.9% across 23 automated benchmarks. Hunyuan-TurboS balances\nhigh performance and efficiency, offering substantial capabilities at lower\ninference costs than many reasoning models, establishing a new paradigm for\nefficient large-scale pre-trained models."
                },
                "authors": [
                    {
                        "name": "Tencent Hunyuan Team"
                    },
                    {
                        "name": "Ao Liu"
                    },
                    {
                        "name": "Botong Zhou"
                    },
                    {
                        "name": "Can Xu"
                    },
                    {
                        "name": "Chayse Zhou"
                    },
                    {
                        "name": "ChenChen Zhang"
                    },
                    {
                        "name": "Chengcheng Xu"
                    },
                    {
                        "name": "Chenhao Wang"
                    },
                    {
                        "name": "Decheng Wu"
                    },
                    {
                        "name": "Dengpeng Wu"
                    },
                    {
                        "name": "Dian Jiao"
                    },
                    {
                        "name": "Dong Du"
                    },
                    {
                        "name": "Dong Wang"
                    },
                    {
                        "name": "Feng Zhang"
                    },
                    {
                        "name": "Fengzong Lian"
                    },
                    {
                        "name": "Guanghui Xu"
                    },
                    {
                        "name": "Guanwei Zhang"
                    },
                    {
                        "name": "Hai Wang"
                    },
                    {
                        "name": "Haipeng Luo"
                    },
                    {
                        "name": "Han Hu"
                    },
                    {
                        "name": "Huilin Xu"
                    },
                    {
                        "name": "Jiajia Wu"
                    },
                    {
                        "name": "Jianchen Zhu"
                    },
                    {
                        "name": "Jianfeng Yan"
                    },
                    {
                        "name": "Jiaqi Zhu"
                    },
                    {
                        "name": "Jihong Zhang"
                    },
                    {
                        "name": "Jinbao Xue"
                    },
                    {
                        "name": "Jun Xia"
                    },
                    {
                        "name": "Junqiang Zheng"
                    },
                    {
                        "name": "Kai Liu"
                    },
                    {
                        "name": "Kai Zhang"
                    },
                    {
                        "name": "Kai Zheng"
                    },
                    {
                        "name": "Kejiao Li"
                    },
                    {
                        "name": "Keyao Wang"
                    },
                    {
                        "name": "Lan Jiang"
                    },
                    {
                        "name": "Lixin Liu"
                    },
                    {
                        "name": "Lulu Wu"
                    },
                    {
                        "name": "Mengyuan Huang"
                    },
                    {
                        "name": "Peijie Yu"
                    },
                    {
                        "name": "Peiqi Wang"
                    },
                    {
                        "name": "Qian Wang"
                    },
                    {
                        "name": "Qianbiao Xiang"
                    },
                    {
                        "name": "Qibin Liu"
                    },
                    {
                        "name": "Qingfeng Sun"
                    },
                    {
                        "name": "Richard Guo"
                    },
                    {
                        "name": "Ruobing Xie"
                    },
                    {
                        "name": "Saiyong Yang"
                    },
                    {
                        "name": "Shaohua Chen"
                    },
                    {
                        "name": "Shihui Hu"
                    },
                    {
                        "name": "Shuai Li"
                    },
                    {
                        "name": "Shuaipeng Li"
                    },
                    {
                        "name": "Shuang Chen"
                    },
                    {
                        "name": "Suncong Zheng"
                    },
                    {
                        "name": "Tao Yang"
                    },
                    {
                        "name": "Tian Zhang"
                    },
                    {
                        "name": "Tinghao Yu"
                    },
                    {
                        "name": "Weidong Han"
                    },
                    {
                        "name": "Weijie Liu"
                    },
                    {
                        "name": "Weijin Zhou"
                    },
                    {
                        "name": "Weikang Wang"
                    },
                    {
                        "name": "Wesleye Chen"
                    },
                    {
                        "name": "Xiao Feng"
                    },
                    {
                        "name": "Xiaoqin Ren"
                    },
                    {
                        "name": "Xingwu Sun"
                    },
                    {
                        "name": "Xiong Kuang"
                    },
                    {
                        "name": "Xuemeng Huang"
                    },
                    {
                        "name": "Xun Cao"
                    },
                    {
                        "name": "Yanfeng Chen"
                    },
                    {
                        "name": "Yang Du"
                    },
                    {
                        "name": "Zhen Yang"
                    },
                    {
                        "name": "Yangyu Tao"
                    },
                    {
                        "name": "Yaping Deng"
                    },
                    {
                        "name": "Yi Shen"
                    },
                    {
                        "name": "Yigeng Hong"
                    },
                    {
                        "name": "Yiqi Chen"
                    },
                    {
                        "name": "Yiqing Huang"
                    },
                    {
                        "name": "Yuchi Deng"
                    },
                    {
                        "name": "Yue Mao"
                    },
                    {
                        "name": "Yulong Wang"
                    },
                    {
                        "name": "Yuyuan Zeng"
                    },
                    {
                        "name": "Zenan Xu"
                    },
                    {
                        "name": "Zhanhui Kang"
                    },
                    {
                        "name": "Zhe Zhao"
                    },
                    {
                        "name": "ZhenXiang Yan"
                    },
                    {
                        "name": "Zheng Fang"
                    },
                    {
                        "name": "Zhichao Hu"
                    },
                    {
                        "name": "Zhongzhi Chen"
                    },
                    {
                        "name": "Zhuoyu Li"
                    },
                    {
                        "name": "Zongwei Li"
                    },
                    {
                        "name": "Alex Yan"
                    },
                    {
                        "name": "Ande Liang"
                    },
                    {
                        "name": "Baitong Liu"
                    },
                    {
                        "name": "Beiping Pan"
                    },
                    {
                        "name": "Bin Xing"
                    },
                    {
                        "name": "Binghong Wu"
                    },
                    {
                        "name": "Bingxin Qu"
                    },
                    {
                        "name": "Bolin Ni"
                    },
                    {
                        "name": "Boyu Wu"
                    },
                    {
                        "name": "Chen Li"
                    },
                    {
                        "name": "Cheng Jiang"
                    },
                    {
                        "name": "Cheng Zhang"
                    },
                    {
                        "name": "Chengjun Liu"
                    },
                    {
                        "name": "Chengxu Yang"
                    },
                    {
                        "name": "Chengzhong Xu"
                    },
                    {
                        "name": "Chiyu Wang"
                    },
                    {
                        "name": "Chong Zha"
                    },
                    {
                        "name": "Daisy Yi"
                    },
                    {
                        "name": "Di Wang"
                    },
                    {
                        "name": "Fanyang Lu"
                    },
                    {
                        "name": "Fei Chen"
                    },
                    {
                        "name": "Feifei Liu"
                    },
                    {
                        "name": "Feng Zheng"
                    },
                    {
                        "name": "Guanghua Yu"
                    },
                    {
                        "name": "Guiyang Li"
                    },
                    {
                        "name": "Guohua Wang"
                    },
                    {
                        "name": "Haisheng Lin"
                    },
                    {
                        "name": "Han Liu"
                    },
                    {
                        "name": "Han Wang"
                    },
                    {
                        "name": "Hao Fei"
                    },
                    {
                        "name": "Hao Lu"
                    },
                    {
                        "name": "Haoqing Jiang"
                    },
                    {
                        "name": "Haoran Sun"
                    },
                    {
                        "name": "Haotian Zhu"
                    },
                    {
                        "name": "Huangjin Dai"
                    },
                    {
                        "name": "Huankui Chen"
                    },
                    {
                        "name": "Huawen Feng"
                    },
                    {
                        "name": "Huihui Cai"
                    },
                    {
                        "name": "Huxin Peng"
                    },
                    {
                        "name": "Jackson Lv"
                    },
                    {
                        "name": "Jiacheng Shi"
                    },
                    {
                        "name": "Jiahao Bu"
                    },
                    {
                        "name": "Jianbo Li"
                    },
                    {
                        "name": "Jianglu Hu"
                    },
                    {
                        "name": "Jiangtao Guan"
                    },
                    {
                        "name": "Jianing Xu"
                    },
                    {
                        "name": "Jianwei Cai"
                    },
                    {
                        "name": "Jiarong Zhang"
                    },
                    {
                        "name": "Jiawei Song"
                    },
                    {
                        "name": "Jie Jiang"
                    },
                    {
                        "name": "Jie Liu"
                    },
                    {
                        "name": "Jieneng Yang"
                    },
                    {
                        "name": "Jihong Zhang"
                    },
                    {
                        "name": "Jin lv"
                    },
                    {
                        "name": "Jing Zhao"
                    },
                    {
                        "name": "Jinjian Li"
                    },
                    {
                        "name": "Jinxing Liu"
                    },
                    {
                        "name": "Jun Zhao"
                    },
                    {
                        "name": "Juntao Guo"
                    },
                    {
                        "name": "Kai Wang"
                    },
                    {
                        "name": "Kan Wu"
                    },
                    {
                        "name": "Lei Fu"
                    },
                    {
                        "name": "Lei He"
                    },
                    {
                        "name": "Lei Wang"
                    },
                    {
                        "name": "Li Liu"
                    },
                    {
                        "name": "Liang Dong"
                    },
                    {
                        "name": "Liya Zhan"
                    },
                    {
                        "name": "Long Cheng"
                    },
                    {
                        "name": "Long Xu"
                    },
                    {
                        "name": "Mao Zheng"
                    },
                    {
                        "name": "Meng Liu"
                    },
                    {
                        "name": "Mengkang Hu"
                    },
                    {
                        "name": "Nanli Chen"
                    },
                    {
                        "name": "Peirui Chen"
                    },
                    {
                        "name": "Peng He"
                    },
                    {
                        "name": "Pengju Pan"
                    },
                    {
                        "name": "Pengzhi Wei"
                    },
                    {
                        "name": "Qi Yang"
                    },
                    {
                        "name": "Qi Yi"
                    },
                    {
                        "name": "Roberts Wang"
                    },
                    {
                        "name": "Rongpeng Chen"
                    },
                    {
                        "name": "Rui Sun"
                    },
                    {
                        "name": "Rui Yang"
                    },
                    {
                        "name": "Ruibin Chen"
                    },
                    {
                        "name": "Ruixu Zhou"
                    },
                    {
                        "name": "Shaofeng Zhang"
                    },
                    {
                        "name": "Sheng Zhang"
                    },
                    {
                        "name": "Shihao Xu"
                    },
                    {
                        "name": "Shuaishuai Chang"
                    },
                    {
                        "name": "Shulin Liu"
                    },
                    {
                        "name": "SiQi Wang"
                    },
                    {
                        "name": "Songjia Feng"
                    },
                    {
                        "name": "Songling Yuan"
                    },
                    {
                        "name": "Tao Zhang"
                    },
                    {
                        "name": "Tianjiao Lang"
                    },
                    {
                        "name": "Tongkai Li"
                    },
                    {
                        "name": "Wei Deng"
                    },
                    {
                        "name": "Wei Li"
                    },
                    {
                        "name": "Weichao Wang"
                    },
                    {
                        "name": "Weigang Zhang"
                    },
                    {
                        "name": "Weixuan Sun"
                    },
                    {
                        "name": "Wen Ouyang"
                    },
                    {
                        "name": "Wenxiang Jiao"
                    },
                    {
                        "name": "Wenzhi Sun"
                    },
                    {
                        "name": "Wenzhuo Jia"
                    },
                    {
                        "name": "Xiang Zhang"
                    },
                    {
                        "name": "Xiangyu He"
                    },
                    {
                        "name": "Xianshun Ren"
                    },
                    {
                        "name": "XiaoYing Zhu"
                    },
                    {
                        "name": "Xiaolong Guo"
                    },
                    {
                        "name": "Xiaoxue Li"
                    },
                    {
                        "name": "Xiaoyu Ma"
                    },
                    {
                        "name": "Xican Lu"
                    },
                    {
                        "name": "Xinhua Feng"
                    },
                    {
                        "name": "Xinting Huang"
                    },
                    {
                        "name": "Xinyu Guan"
                    },
                    {
                        "name": "Xirui Li"
                    },
                    {
                        "name": "Xu Zhang"
                    },
                    {
                        "name": "Xudong Gao"
                    },
                    {
                        "name": "Xun Luo"
                    },
                    {
                        "name": "Xuxiang Qi"
                    },
                    {
                        "name": "Yangkun Chen"
                    },
                    {
                        "name": "Yangyu Tao"
                    },
                    {
                        "name": "Yanling Xiao"
                    },
                    {
                        "name": "Yantao Mai"
                    },
                    {
                        "name": "Yanze Chen"
                    },
                    {
                        "name": "Yao Ding"
                    },
                    {
                        "name": "Yeting Yang"
                    },
                    {
                        "name": "YiFan Song"
                    },
                    {
                        "name": "Yifan Yang"
                    },
                    {
                        "name": "Yijiao Zhu"
                    },
                    {
                        "name": "Yinhe Wu"
                    },
                    {
                        "name": "Yixian Liu"
                    },
                    {
                        "name": "Yong Yang"
                    },
                    {
                        "name": "Yuanjun Cai"
                    },
                    {
                        "name": "Yuanlin Tu"
                    },
                    {
                        "name": "Yue Zhang"
                    },
                    {
                        "name": "Yufei Huang"
                    },
                    {
                        "name": "Yuhang Zhou"
                    },
                    {
                        "name": "Yuhao Jiang"
                    },
                    {
                        "name": "Yuhong Liu"
                    },
                    {
                        "name": "Yuhui Hu"
                    },
                    {
                        "name": "Yujin Lin"
                    },
                    {
                        "name": "Yun Yang"
                    },
                    {
                        "name": "Yunhao Wang"
                    },
                    {
                        "name": "Yusong Zhang"
                    },
                    {
                        "name": "Zekun Wu"
                    },
                    {
                        "name": "Zelong Zhang"
                    },
                    {
                        "name": "Zhan Yu"
                    },
                    {
                        "name": "Zhaoliang Yang"
                    },
                    {
                        "name": "Zhe Zhao"
                    },
                    {
                        "name": "Zheng Li"
                    },
                    {
                        "name": "Zhenyu Huang"
                    },
                    {
                        "name": "Zhiguang Liu"
                    },
                    {
                        "name": "Zhijiang Xu"
                    },
                    {
                        "name": "Zhiqing Kui"
                    },
                    {
                        "name": "Zhiyin Zeng"
                    },
                    {
                        "name": "Zhiyuan Xiong"
                    },
                    {
                        "name": "Zhuo Han"
                    },
                    {
                        "name": "Zifan Wu"
                    },
                    {
                        "name": "Zigang Geng"
                    },
                    {
                        "name": "Zilong Zhao"
                    },
                    {
                        "name": "Ziyan Tang"
                    },
                    {
                        "name": "Ziyuan Zhu"
                    },
                    {
                        "name": "Zonglei Zhu"
                    },
                    {
                        "name": "Zhijiang Xu"
                    }
                ],
                "author_detail": {
                    "name": "Zhijiang Xu"
                },
                "author": "Zhijiang Xu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.15431v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.15431v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.03231v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.03231v1",
                "updated": "2025-07-04T00:16:15Z",
                "updated_parsed": [
                    2025,
                    7,
                    4,
                    0,
                    16,
                    15,
                    4,
                    185,
                    0
                ],
                "published": "2025-07-04T00:16:15Z",
                "published_parsed": [
                    2025,
                    7,
                    4,
                    0,
                    16,
                    15,
                    4,
                    185,
                    0
                ],
                "title": "Robust and Efficient Embedded Convex Optimization through First-Order\n  Adaptive Caching",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Robust and Efficient Embedded Convex Optimization through First-Order\n  Adaptive Caching"
                },
                "summary": "Recent advances in Model Predictive Control (MPC) leveraging a combination of\nfirst-order methods, such as the Alternating Direction Method of Multipliers\n(ADMM), and offline precomputation and caching of select operations, have\nexcitingly enabled real-time MPC on microcontrollers. Unfortunately, these\napproaches require the use of fixed hyperparameters, limiting their\nadaptability and overall performance. In this work, we introduce First-Order\nAdaptive Caching, which precomputes not only select matrix operations but also\ntheir sensitivities to hyperparameter variations, enabling online\nhyperparameter updates without full recomputation of the cache. We demonstrate\nthe effectiveness of our approach on a number of dynamic quadrotor tasks,\nachieving up to a 63.4% reduction in ADMM iterations over the use of optimized\nfixed hyperparameters and approaching 70% of the performance of a full cache\nrecomputation, while reducing the computational cost from O(n^3) to O(n^2)\ncomplexity. This performance enables us to perform figure-eight trajectories on\na 27g tiny quadrotor under wind disturbances. We release our implementation\nopen-source for the benefit of the wider robotics community.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advances in Model Predictive Control (MPC) leveraging a combination of\nfirst-order methods, such as the Alternating Direction Method of Multipliers\n(ADMM), and offline precomputation and caching of select operations, have\nexcitingly enabled real-time MPC on microcontrollers. Unfortunately, these\napproaches require the use of fixed hyperparameters, limiting their\nadaptability and overall performance. In this work, we introduce First-Order\nAdaptive Caching, which precomputes not only select matrix operations but also\ntheir sensitivities to hyperparameter variations, enabling online\nhyperparameter updates without full recomputation of the cache. We demonstrate\nthe effectiveness of our approach on a number of dynamic quadrotor tasks,\nachieving up to a 63.4% reduction in ADMM iterations over the use of optimized\nfixed hyperparameters and approaching 70% of the performance of a full cache\nrecomputation, while reducing the computational cost from O(n^3) to O(n^2)\ncomplexity. This performance enables us to perform figure-eight trajectories on\na 27g tiny quadrotor under wind disturbances. We release our implementation\nopen-source for the benefit of the wider robotics community."
                },
                "authors": [
                    {
                        "name": "Ishaan Mahajan"
                    },
                    {
                        "name": "Brian Plancher"
                    }
                ],
                "author_detail": {
                    "name": "Brian Plancher"
                },
                "author": "Brian Plancher",
                "arxiv_comment": "Accepted to IROS 2025, 7 pages, 4 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.03231v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.03231v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.OC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.03153v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.03153v1",
                "updated": "2025-07-03T20:20:33Z",
                "updated_parsed": [
                    2025,
                    7,
                    3,
                    20,
                    20,
                    33,
                    3,
                    184,
                    0
                ],
                "published": "2025-07-03T20:20:33Z",
                "published_parsed": [
                    2025,
                    7,
                    3,
                    20,
                    20,
                    33,
                    3,
                    184,
                    0
                ],
                "title": "HGCA: Hybrid GPU-CPU Attention for Long Context LLM Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "HGCA: Hybrid GPU-CPU Attention for Long Context LLM Inference"
                },
                "summary": "Scaling inference for large language models (LLMs) is increasingly\nconstrained by limited GPU memory, especially due to growing key-value (KV)\ncaches required for long-context generation. While existing approaches offload\nKV caches to CPU memory or apply sparse attention to reduce GPU load, they\noften underutilize CPU compute resources and compromise accuracy. We present\nHGCA, a hybrid CPU-GPU attention mechanism that enables scalable,\nhigh-throughput LLM inference with near-full attention quality. HGCA performs\ndense attention on recently generated KV entries retained in GPU memory and\nparallel sparse attention on selected, salient KV entries in CPU memory. The\nattention outputs are efficiently merged using log-sum-exp fusion, minimizing\nPCIe transfer overhead. HGCA also introduces a finegrained, per-head\nsparsification strategy optimized for CPU execution, preserving contextual\nrelevance while reducing computation. Our implementation seamlessly integrates\ninto existing LLM frameworks without requiring model retraining. Experiments\nacross diverse models and workloads show that HGCA achieves superior\nscalability, supports longer sequences and larger batch sizes, and outperforms\nexisting sparse attention baselines in both performance and accuracy -- all on\ncommodity GPU hardware.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Scaling inference for large language models (LLMs) is increasingly\nconstrained by limited GPU memory, especially due to growing key-value (KV)\ncaches required for long-context generation. While existing approaches offload\nKV caches to CPU memory or apply sparse attention to reduce GPU load, they\noften underutilize CPU compute resources and compromise accuracy. We present\nHGCA, a hybrid CPU-GPU attention mechanism that enables scalable,\nhigh-throughput LLM inference with near-full attention quality. HGCA performs\ndense attention on recently generated KV entries retained in GPU memory and\nparallel sparse attention on selected, salient KV entries in CPU memory. The\nattention outputs are efficiently merged using log-sum-exp fusion, minimizing\nPCIe transfer overhead. HGCA also introduces a finegrained, per-head\nsparsification strategy optimized for CPU execution, preserving contextual\nrelevance while reducing computation. Our implementation seamlessly integrates\ninto existing LLM frameworks without requiring model retraining. Experiments\nacross diverse models and workloads show that HGCA achieves superior\nscalability, supports longer sequences and larger batch sizes, and outperforms\nexisting sparse attention baselines in both performance and accuracy -- all on\ncommodity GPU hardware."
                },
                "authors": [
                    {
                        "name": "Weishu Deng"
                    },
                    {
                        "name": "Yujie Yang"
                    },
                    {
                        "name": "Peiran Du"
                    },
                    {
                        "name": "Lingfeng Xiang"
                    },
                    {
                        "name": "Zhen Lin"
                    },
                    {
                        "name": "Chen Zhong"
                    },
                    {
                        "name": "Song Jiang"
                    },
                    {
                        "name": "Hui Lu"
                    },
                    {
                        "name": "Jia Rao"
                    }
                ],
                "author_detail": {
                    "name": "Jia Rao"
                },
                "author": "Jia Rao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.03153v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.03153v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.02860v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.02860v1",
                "updated": "2025-07-03T17:59:54Z",
                "updated_parsed": [
                    2025,
                    7,
                    3,
                    17,
                    59,
                    54,
                    3,
                    184,
                    0
                ],
                "published": "2025-07-03T17:59:54Z",
                "published_parsed": [
                    2025,
                    7,
                    3,
                    17,
                    59,
                    54,
                    3,
                    184,
                    0
                ],
                "title": "Less is Enough: Training-Free Video Diffusion Acceleration via\n  Runtime-Adaptive Caching",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Less is Enough: Training-Free Video Diffusion Acceleration via\n  Runtime-Adaptive Caching"
                },
                "summary": "Video generation models have demonstrated remarkable performance, yet their\nbroader adoption remains constrained by slow inference speeds and substantial\ncomputational costs, primarily due to the iterative nature of the denoising\nprocess. Addressing this bottleneck is essential for democratizing advanced\nvideo synthesis technologies and enabling their integration into real-world\napplications. This work proposes EasyCache, a training-free acceleration\nframework for video diffusion models. EasyCache introduces a lightweight,\nruntime-adaptive caching mechanism that dynamically reuses previously computed\ntransformation vectors, avoiding redundant computations during inference.\nUnlike prior approaches, EasyCache requires no offline profiling,\npre-computation, or extensive parameter tuning. We conduct comprehensive\nstudies on various large-scale video generation models, including OpenSora,\nWan2.1, and HunyuanVideo. Our method achieves leading acceleration performance,\nreducing inference time by up to 2.1-3.3$\\times$ compared to the original\nbaselines while maintaining high visual fidelity with a significant up to 36%\nPSNR improvement compared to the previous SOTA method. This improvement makes\nour EasyCache a efficient and highly accessible solution for high-quality video\ngeneration in both research and practical applications. The code is available\nat https://github.com/H-EmbodVis/EasyCache.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Video generation models have demonstrated remarkable performance, yet their\nbroader adoption remains constrained by slow inference speeds and substantial\ncomputational costs, primarily due to the iterative nature of the denoising\nprocess. Addressing this bottleneck is essential for democratizing advanced\nvideo synthesis technologies and enabling their integration into real-world\napplications. This work proposes EasyCache, a training-free acceleration\nframework for video diffusion models. EasyCache introduces a lightweight,\nruntime-adaptive caching mechanism that dynamically reuses previously computed\ntransformation vectors, avoiding redundant computations during inference.\nUnlike prior approaches, EasyCache requires no offline profiling,\npre-computation, or extensive parameter tuning. We conduct comprehensive\nstudies on various large-scale video generation models, including OpenSora,\nWan2.1, and HunyuanVideo. Our method achieves leading acceleration performance,\nreducing inference time by up to 2.1-3.3$\\times$ compared to the original\nbaselines while maintaining high visual fidelity with a significant up to 36%\nPSNR improvement compared to the previous SOTA method. This improvement makes\nour EasyCache a efficient and highly accessible solution for high-quality video\ngeneration in both research and practical applications. The code is available\nat https://github.com/H-EmbodVis/EasyCache."
                },
                "authors": [
                    {
                        "name": "Xin Zhou"
                    },
                    {
                        "name": "Dingkang Liang"
                    },
                    {
                        "name": "Kaijin Chen"
                    },
                    {
                        "name": "Tianrui Feng"
                    },
                    {
                        "name": "Xiwu Chen"
                    },
                    {
                        "name": "Hongkai Lin"
                    },
                    {
                        "name": "Yikang Ding"
                    },
                    {
                        "name": "Feiyang Tan"
                    },
                    {
                        "name": "Hengshuang Zhao"
                    },
                    {
                        "name": "Xiang Bai"
                    }
                ],
                "author_detail": {
                    "name": "Xiang Bai"
                },
                "author": "Xiang Bai",
                "arxiv_comment": "The code is made available at\n  https://github.com/H-EmbodVis/EasyCache. Project page:\n  https://h-embodvis.github.io/EasyCache/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.02860v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.02860v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2312.04789v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2312.04789v2",
                "updated": "2025-07-03T17:11:28Z",
                "updated_parsed": [
                    2025,
                    7,
                    3,
                    17,
                    11,
                    28,
                    3,
                    184,
                    0
                ],
                "published": "2023-12-08T02:03:55Z",
                "published_parsed": [
                    2023,
                    12,
                    8,
                    2,
                    3,
                    55,
                    4,
                    342,
                    0
                ],
                "title": "HybridTier: an Adaptive and Lightweight CXL-Memory Tiering System",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "HybridTier: an Adaptive and Lightweight CXL-Memory Tiering System"
                },
                "summary": "Modern workloads are demanding increasingly larger memory capacity. Compute\nExpress Link (CXL)-based memory tiering has emerged as a promising solution for\naddressing this problem by utilizing traditional DRAM alongside slow-tier CXL\nmemory devices. We analyze prior tiering systems and observe two challenges for\nhigh-performance memory tiering: adapting to skewed but dynamically varying\ndata hotness distributions while minimizing memory and cache overhead due to\ntiering.\n  To address these challenges, we propose HybridTier, an adaptive and\nlightweight tiering system for CXL memory. HybridTier tracks both long-term\ndata access frequency and short-term access momentum \\emph{simultaneously} to\naccurately capture and adapt to shifting hotness distributions. HybridTier\nreduces the metadata memory overhead by tracking data accesses\n\\emph{probabilistically}, obtaining higher memory efficiency by trading off a\nsmall amount of tracking inaccuracy that has a negligible impact on application\nperformance. To reduce cache overhead, HybridTier uses lightweight data\nstructures that optimize for data locality to track data hotness. Our\nevaluations show that HybridTier outperforms prior systems by up to $91\\%$\n($19\\%$ geomean), incurring $2.0-7.8\\times$ less memory overhead and\n$1.7-3.5\\times$ less cache misses.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Modern workloads are demanding increasingly larger memory capacity. Compute\nExpress Link (CXL)-based memory tiering has emerged as a promising solution for\naddressing this problem by utilizing traditional DRAM alongside slow-tier CXL\nmemory devices. We analyze prior tiering systems and observe two challenges for\nhigh-performance memory tiering: adapting to skewed but dynamically varying\ndata hotness distributions while minimizing memory and cache overhead due to\ntiering.\n  To address these challenges, we propose HybridTier, an adaptive and\nlightweight tiering system for CXL memory. HybridTier tracks both long-term\ndata access frequency and short-term access momentum \\emph{simultaneously} to\naccurately capture and adapt to shifting hotness distributions. HybridTier\nreduces the metadata memory overhead by tracking data accesses\n\\emph{probabilistically}, obtaining higher memory efficiency by trading off a\nsmall amount of tracking inaccuracy that has a negligible impact on application\nperformance. To reduce cache overhead, HybridTier uses lightweight data\nstructures that optimize for data locality to track data hotness. Our\nevaluations show that HybridTier outperforms prior systems by up to $91\\%$\n($19\\%$ geomean), incurring $2.0-7.8\\times$ less memory overhead and\n$1.7-3.5\\times$ less cache misses."
                },
                "authors": [
                    {
                        "name": "Kevin Song"
                    },
                    {
                        "name": "Jiacheng Yang"
                    },
                    {
                        "name": "Zixuan Wang"
                    },
                    {
                        "name": "Jishen Zhao"
                    },
                    {
                        "name": "Sihang Liu"
                    },
                    {
                        "name": "Gennady Pekhimenko"
                    }
                ],
                "author_detail": {
                    "name": "Gennady Pekhimenko"
                },
                "author": "Gennady Pekhimenko",
                "arxiv_doi": "10.1145/3676642.3736119",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3676642.3736119",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2312.04789v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2312.04789v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "Appears in the Proceedings of the 30th ACM International Conference\n  on Architectural Support for Programming Languages and Operating Systems,\n  Volume 3 (ASPLOS 25)",
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.OS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.05693v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.05693v3",
                "updated": "2025-07-03T16:06:35Z",
                "updated_parsed": [
                    2025,
                    7,
                    3,
                    16,
                    6,
                    35,
                    3,
                    184,
                    0
                ],
                "published": "2024-12-07T16:41:54Z",
                "published_parsed": [
                    2024,
                    12,
                    7,
                    16,
                    41,
                    54,
                    5,
                    342,
                    0
                ],
                "title": "Batch-Max: Higher LLM Throughput using Larger Batch Sizes and KV Cache\n  Compression",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Batch-Max: Higher LLM Throughput using Larger Batch Sizes and KV Cache\n  Compression"
                },
                "summary": "Several works have developed eviction policies to remove key-value (KV) pairs\nfrom the KV cache for more efficient inference. The focus has been on\ncompressing the KV cache after the input prompt has been processed for faster\ntoken generation. In settings with limited GPU memory, and when the input\ncontext is longer than the generation length, we show that by also compressing\nthe KV cache during the input processing phase, larger batch sizes can be used\nresulting in significantly higher throughput while still maintaining the\noriginal model's accuracy.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Several works have developed eviction policies to remove key-value (KV) pairs\nfrom the KV cache for more efficient inference. The focus has been on\ncompressing the KV cache after the input prompt has been processed for faster\ntoken generation. In settings with limited GPU memory, and when the input\ncontext is longer than the generation length, we show that by also compressing\nthe KV cache during the input processing phase, larger batch sizes can be used\nresulting in significantly higher throughput while still maintaining the\noriginal model's accuracy."
                },
                "authors": [
                    {
                        "name": "Michael R. Metel"
                    },
                    {
                        "name": "Boxing Chen"
                    },
                    {
                        "name": "Mehdi Rezagholizadeh"
                    }
                ],
                "author_detail": {
                    "name": "Mehdi Rezagholizadeh"
                },
                "author": "Mehdi Rezagholizadeh",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.05693v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.05693v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.02659v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.02659v1",
                "updated": "2025-07-03T14:20:41Z",
                "updated_parsed": [
                    2025,
                    7,
                    3,
                    14,
                    20,
                    41,
                    3,
                    184,
                    0
                ],
                "published": "2025-07-03T14:20:41Z",
                "published_parsed": [
                    2025,
                    7,
                    3,
                    14,
                    20,
                    41,
                    3,
                    184,
                    0
                ],
                "title": "OmniDraft: A Cross-vocabulary, Online Adaptive Drafter for On-device\n  Speculative Decoding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "OmniDraft: A Cross-vocabulary, Online Adaptive Drafter for On-device\n  Speculative Decoding"
                },
                "summary": "Speculative decoding generally dictates having a small, efficient draft model\nthat is either pretrained or distilled offline to a particular target model\nseries, for instance, Llama or Qwen models. However, within online deployment\nsettings, there are two major challenges: 1) usage of a target model that is\nincompatible with the draft model; 2) expectation of latency improvements over\nusage and time. In this work, we propose OmniDraft, a unified framework that\nenables a single draft model to operate with any target model and adapt\ndynamically to user data. We introduce an online n-gram cache with hybrid\ndistillation fine-tuning to address the cross-vocabulary mismatch across draft\nand target models; and further improve decoding speed by leveraging adaptive\ndrafting techniques. OmniDraft is particularly suitable for on-device LLM\napplications where model cost, efficiency and user customization are the major\npoints of contention. This further highlights the need to tackle the above\nchallenges and motivates the \\textit{``one drafter for all''} paradigm. We\nshowcase the proficiency of the OmniDraft framework by performing online\nlearning on math reasoning, coding and text generation tasks. Notably,\nOmniDraft enables a single Llama-68M model to pair with various target models\nincluding Vicuna-7B, Qwen2-7B and Llama3-8B models for speculative decoding;\nand additionally provides up to 1.5-2x speedup.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Speculative decoding generally dictates having a small, efficient draft model\nthat is either pretrained or distilled offline to a particular target model\nseries, for instance, Llama or Qwen models. However, within online deployment\nsettings, there are two major challenges: 1) usage of a target model that is\nincompatible with the draft model; 2) expectation of latency improvements over\nusage and time. In this work, we propose OmniDraft, a unified framework that\nenables a single draft model to operate with any target model and adapt\ndynamically to user data. We introduce an online n-gram cache with hybrid\ndistillation fine-tuning to address the cross-vocabulary mismatch across draft\nand target models; and further improve decoding speed by leveraging adaptive\ndrafting techniques. OmniDraft is particularly suitable for on-device LLM\napplications where model cost, efficiency and user customization are the major\npoints of contention. This further highlights the need to tackle the above\nchallenges and motivates the \\textit{``one drafter for all''} paradigm. We\nshowcase the proficiency of the OmniDraft framework by performing online\nlearning on math reasoning, coding and text generation tasks. Notably,\nOmniDraft enables a single Llama-68M model to pair with various target models\nincluding Vicuna-7B, Qwen2-7B and Llama3-8B models for speculative decoding;\nand additionally provides up to 1.5-2x speedup."
                },
                "authors": [
                    {
                        "name": "Ramchalam Kinattinkara Ramakrishnan"
                    },
                    {
                        "name": "Zhaocong Yuan"
                    },
                    {
                        "name": "Shaojie Zhuo"
                    },
                    {
                        "name": "Chen Feng"
                    },
                    {
                        "name": "Yicheng Lin"
                    },
                    {
                        "name": "Chenzheng Su"
                    },
                    {
                        "name": "Xiaopeng Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Xiaopeng Zhang"
                },
                "author": "Xiaopeng Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.02659v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.02659v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.21817v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.21817v3",
                "updated": "2025-07-03T08:22:27Z",
                "updated_parsed": [
                    2025,
                    7,
                    3,
                    8,
                    22,
                    27,
                    3,
                    184,
                    0
                ],
                "published": "2025-03-26T04:16:48Z",
                "published_parsed": [
                    2025,
                    3,
                    26,
                    4,
                    16,
                    48,
                    2,
                    85,
                    0
                ],
                "title": "Skip-Vision: Efficient and Scalable Acceleration of Vision-Language\n  Models via Adaptive Token Skipping",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Skip-Vision: Efficient and Scalable Acceleration of Vision-Language\n  Models via Adaptive Token Skipping"
                },
                "summary": "Transformer-based models have driven significant advancements in Multimodal\nLarge Language Models (MLLMs), yet their computational costs surge drastically\nwhen scaling resolution, training data, and model parameters. A key bottleneck\nstems from the proliferation of visual tokens required for fine-grained image\nunderstanding. We propose Skip-Vision, a unified framework addressing both\ntraining and inference inefficiencies in vision-language models. On top of\nconventional token compression approaches, our method introduces two\ncomplementary acceleration strategies. For training acceleration, we observe\nthat Feed-Forward Network (FFN) computations on visual tokens induce marginal\nfeature updates. This motivates our Skip-FFN strategy, which bypasses FFN\nlayers for redundant visual tokens. For inference acceleration, we design a\nselective KV-cache removal mechanism that prunes the skipped key-value pairs\nduring decoding while preserving model performance. Experimental results\ndemonstrate that Skip-Vision reduces training time by up to 35\\%, inference\nFLOPs by 75\\%, and latency by 45\\%, while achieving comparable or superior\nperformance to existing methods. Our work provides a practical solution for\nscaling high-performance MLLMs with enhanced efficiency.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Transformer-based models have driven significant advancements in Multimodal\nLarge Language Models (MLLMs), yet their computational costs surge drastically\nwhen scaling resolution, training data, and model parameters. A key bottleneck\nstems from the proliferation of visual tokens required for fine-grained image\nunderstanding. We propose Skip-Vision, a unified framework addressing both\ntraining and inference inefficiencies in vision-language models. On top of\nconventional token compression approaches, our method introduces two\ncomplementary acceleration strategies. For training acceleration, we observe\nthat Feed-Forward Network (FFN) computations on visual tokens induce marginal\nfeature updates. This motivates our Skip-FFN strategy, which bypasses FFN\nlayers for redundant visual tokens. For inference acceleration, we design a\nselective KV-cache removal mechanism that prunes the skipped key-value pairs\nduring decoding while preserving model performance. Experimental results\ndemonstrate that Skip-Vision reduces training time by up to 35\\%, inference\nFLOPs by 75\\%, and latency by 45\\%, while achieving comparable or superior\nperformance to existing methods. Our work provides a practical solution for\nscaling high-performance MLLMs with enhanced efficiency."
                },
                "authors": [
                    {
                        "name": "Weili Zeng"
                    },
                    {
                        "name": "Ziyuan Huang"
                    },
                    {
                        "name": "Kaixiang Ji"
                    },
                    {
                        "name": "Yichao Yan"
                    }
                ],
                "author_detail": {
                    "name": "Yichao Yan"
                },
                "author": "Yichao Yan",
                "arxiv_comment": "Accepted by ICCV2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.21817v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.21817v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.02397v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.02397v1",
                "updated": "2025-07-03T07:49:18Z",
                "updated_parsed": [
                    2025,
                    7,
                    3,
                    7,
                    49,
                    18,
                    3,
                    184,
                    0
                ],
                "published": "2025-07-03T07:49:18Z",
                "published_parsed": [
                    2025,
                    7,
                    3,
                    7,
                    49,
                    18,
                    3,
                    184,
                    0
                ],
                "title": "Direct Reconstruction of Terahertz-driven Subcycle Electron Emission\n  Dynamics",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Direct Reconstruction of Terahertz-driven Subcycle Electron Emission\n  Dynamics"
                },
                "summary": "While field-driven electron emission is theoretically understood down to the\nsubcycle regime, its direct experimental temporal characterization using\nlong-wavelength terahertz (THz) fields remains elusive. Here, by driving a\ngraphite tip with phase-stable quasi-single-cycle THz pulses, we reveal\ndistinct subcycle electron emission dynamics including: (1) At a\ncarrier-envelope phase (CEP) near zero, spectral peaks scale linearly with THz\nfield strength, characteristic of subcycle emission; (2) At the opposite CEP,\ndominant deceleration fields generate stationary low-energy peaks. Crucially,\nwe develop a pump-probe-free, direct reconstruction method extracting electron\npulse profiles solely from measured energy spectra, obtaining durations from\n97.3 to 114.3 fs as the field increases (191-290 kV/cm). Phase-resolved\nsimulations further reveal a 71.2% modulation in the cutoff energy and a\nnear-total (99.7%) suppression of the emission current. This work not only\nvalidates the Fowler-Nordheim model under THz excitation but also establishes a\ngeneral framework for the direct temporal characterization of subcycle electron\nemission, opening pathways for precise electron control in ultrafast electron\nsources and lightwave nanoelectronics.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While field-driven electron emission is theoretically understood down to the\nsubcycle regime, its direct experimental temporal characterization using\nlong-wavelength terahertz (THz) fields remains elusive. Here, by driving a\ngraphite tip with phase-stable quasi-single-cycle THz pulses, we reveal\ndistinct subcycle electron emission dynamics including: (1) At a\ncarrier-envelope phase (CEP) near zero, spectral peaks scale linearly with THz\nfield strength, characteristic of subcycle emission; (2) At the opposite CEP,\ndominant deceleration fields generate stationary low-energy peaks. Crucially,\nwe develop a pump-probe-free, direct reconstruction method extracting electron\npulse profiles solely from measured energy spectra, obtaining durations from\n97.3 to 114.3 fs as the field increases (191-290 kV/cm). Phase-resolved\nsimulations further reveal a 71.2% modulation in the cutoff energy and a\nnear-total (99.7%) suppression of the emission current. This work not only\nvalidates the Fowler-Nordheim model under THz excitation but also establishes a\ngeneral framework for the direct temporal characterization of subcycle electron\nemission, opening pathways for precise electron control in ultrafast electron\nsources and lightwave nanoelectronics."
                },
                "authors": [
                    {
                        "name": "Jiakang Mao"
                    },
                    {
                        "name": "Yushan Zeng"
                    },
                    {
                        "name": "Hongyang Li"
                    },
                    {
                        "name": "Liwei Song"
                    },
                    {
                        "name": "Ye Tian"
                    },
                    {
                        "name": "Ruxin Li"
                    }
                ],
                "author_detail": {
                    "name": "Ruxin Li"
                },
                "author": "Ruxin Li",
                "arxiv_comment": "16 pages, 5 figures, references added",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.02397v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.02397v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.optics",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.optics",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.22618v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.22618v3",
                "updated": "2025-07-03T04:51:05Z",
                "updated_parsed": [
                    2025,
                    7,
                    3,
                    4,
                    51,
                    5,
                    3,
                    184,
                    0
                ],
                "published": "2025-05-28T17:39:15Z",
                "published_parsed": [
                    2025,
                    5,
                    28,
                    17,
                    39,
                    15,
                    2,
                    148,
                    0
                ],
                "title": "Fast-dLLM: Training-free Acceleration of Diffusion LLM by Enabling KV\n  Cache and Parallel Decoding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Fast-dLLM: Training-free Acceleration of Diffusion LLM by Enabling KV\n  Cache and Parallel Decoding"
                },
                "summary": "Diffusion-based large language models (Diffusion LLMs) have shown promise for\nnon-autoregressive text generation with parallel decoding capabilities.\nHowever, the practical inference speed of open-sourced Diffusion LLMs often\nlags behind autoregressive models due to the lack of Key-Value (KV) Cache and\nquality degradation when decoding multiple tokens simultaneously. To bridge\nthis gap, we introduce a novel block-wise approximate KV Cache mechanism\ntailored for bidirectional diffusion models, enabling cache reuse with\nnegligible performance drop. Additionally, we identify the root cause of\ngeneration quality degradation in parallel decoding as the disruption of token\ndependencies under the conditional independence assumption. To address this, we\npropose a confidence-aware parallel decoding strategy that selectively decodes\ntokens exceeding a confidence threshold, mitigating dependency violations and\nmaintaining generation quality. Experimental results on LLaDA and Dream models\nacross multiple LLM benchmarks demonstrate up to \\textbf{27.6$\\times$\nthroughput} improvement with minimal accuracy loss, closing the performance gap\nwith autoregressive models and paving the way for practical deployment of\nDiffusion LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion-based large language models (Diffusion LLMs) have shown promise for\nnon-autoregressive text generation with parallel decoding capabilities.\nHowever, the practical inference speed of open-sourced Diffusion LLMs often\nlags behind autoregressive models due to the lack of Key-Value (KV) Cache and\nquality degradation when decoding multiple tokens simultaneously. To bridge\nthis gap, we introduce a novel block-wise approximate KV Cache mechanism\ntailored for bidirectional diffusion models, enabling cache reuse with\nnegligible performance drop. Additionally, we identify the root cause of\ngeneration quality degradation in parallel decoding as the disruption of token\ndependencies under the conditional independence assumption. To address this, we\npropose a confidence-aware parallel decoding strategy that selectively decodes\ntokens exceeding a confidence threshold, mitigating dependency violations and\nmaintaining generation quality. Experimental results on LLaDA and Dream models\nacross multiple LLM benchmarks demonstrate up to \\textbf{27.6$\\times$\nthroughput} improvement with minimal accuracy loss, closing the performance gap\nwith autoregressive models and paving the way for practical deployment of\nDiffusion LLMs."
                },
                "authors": [
                    {
                        "name": "Chengyue Wu"
                    },
                    {
                        "name": "Hao Zhang"
                    },
                    {
                        "name": "Shuchen Xue"
                    },
                    {
                        "name": "Zhijian Liu"
                    },
                    {
                        "name": "Shizhe Diao"
                    },
                    {
                        "name": "Ligeng Zhu"
                    },
                    {
                        "name": "Ping Luo"
                    },
                    {
                        "name": "Song Han"
                    },
                    {
                        "name": "Enze Xie"
                    }
                ],
                "author_detail": {
                    "name": "Enze Xie"
                },
                "author": "Enze Xie",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.22618v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.22618v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.02227v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.02227v1",
                "updated": "2025-07-03T01:22:57Z",
                "updated_parsed": [
                    2025,
                    7,
                    3,
                    1,
                    22,
                    57,
                    3,
                    184,
                    0
                ],
                "published": "2025-07-03T01:22:57Z",
                "published_parsed": [
                    2025,
                    7,
                    3,
                    1,
                    22,
                    57,
                    3,
                    184,
                    0
                ],
                "title": "PhysicsCorrect: A Training-Free Approach for Stable Neural PDE\n  Simulations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PhysicsCorrect: A Training-Free Approach for Stable Neural PDE\n  Simulations"
                },
                "summary": "Neural networks have emerged as powerful surrogates for solving partial\ndifferential equations (PDEs), offering significant computational speedups over\ntraditional methods. However, these models suffer from a critical limitation:\nerror accumulation during long-term rollouts, where small inaccuracies compound\nexponentially, eventually causing complete divergence from physically valid\nsolutions. We present PhysicsCorrect, a training-free correction framework that\nenforces PDE consistency at each prediction step by formulating correction as a\nlinearized inverse problem based on PDE residuals. Our key innovation is an\nefficient caching strategy that precomputes the Jacobian and its pseudoinverse\nduring an offline warm-up phase, reducing computational overhead by two orders\nof magnitude compared to standard correction approaches. Across three\nrepresentative PDE systems -- Navier-Stokes fluid dynamics, wave equations, and\nthe chaotic Kuramoto-Sivashinsky equation -- PhysicsCorrect reduces prediction\nerrors by up to 100x while adding negligible inference time (under 5\\%). The\nframework integrates seamlessly with diverse architectures including Fourier\nNeural Operators, UNets, and Vision Transformers, effectively transforming\nunstable neural surrogates into reliable simulation tools that bridge the gap\nbetween deep learning's computational efficiency and the physical fidelity\ndemanded by practical scientific applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Neural networks have emerged as powerful surrogates for solving partial\ndifferential equations (PDEs), offering significant computational speedups over\ntraditional methods. However, these models suffer from a critical limitation:\nerror accumulation during long-term rollouts, where small inaccuracies compound\nexponentially, eventually causing complete divergence from physically valid\nsolutions. We present PhysicsCorrect, a training-free correction framework that\nenforces PDE consistency at each prediction step by formulating correction as a\nlinearized inverse problem based on PDE residuals. Our key innovation is an\nefficient caching strategy that precomputes the Jacobian and its pseudoinverse\nduring an offline warm-up phase, reducing computational overhead by two orders\nof magnitude compared to standard correction approaches. Across three\nrepresentative PDE systems -- Navier-Stokes fluid dynamics, wave equations, and\nthe chaotic Kuramoto-Sivashinsky equation -- PhysicsCorrect reduces prediction\nerrors by up to 100x while adding negligible inference time (under 5\\%). The\nframework integrates seamlessly with diverse architectures including Fourier\nNeural Operators, UNets, and Vision Transformers, effectively transforming\nunstable neural surrogates into reliable simulation tools that bridge the gap\nbetween deep learning's computational efficiency and the physical fidelity\ndemanded by practical scientific applications."
                },
                "authors": [
                    {
                        "name": "Xinquan Huang"
                    },
                    {
                        "name": "Paris Perdikaris"
                    }
                ],
                "author_detail": {
                    "name": "Paris Perdikaris"
                },
                "author": "Paris Perdikaris",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.02227v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.02227v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.01652v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.01652v1",
                "updated": "2025-07-02T12:27:06Z",
                "updated_parsed": [
                    2025,
                    7,
                    2,
                    12,
                    27,
                    6,
                    2,
                    183,
                    0
                ],
                "published": "2025-07-02T12:27:06Z",
                "published_parsed": [
                    2025,
                    7,
                    2,
                    12,
                    27,
                    6,
                    2,
                    183,
                    0
                ],
                "title": "Autoregressive Image Generation with Linear Complexity: A Spatial-Aware\n  Decay Perspective",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Autoregressive Image Generation with Linear Complexity: A Spatial-Aware\n  Decay Perspective"
                },
                "summary": "Autoregressive (AR) models have garnered significant attention in image\ngeneration for their ability to effectively capture both local and global\nstructures within visual data. However, prevalent AR models predominantly rely\non the transformer architectures, which are beset by quadratic computational\ncomplexity concerning input sequence length and substantial memory overhead due\nto the necessity of maintaining key-value caches. Although linear attention\nmechanisms have successfully reduced this burden in language models, our\ninitial experiments reveal that they significantly degrade image generation\nquality because of their inability to capture critical long-range dependencies\nin visual data. We propose Linear Attention with Spatial-Aware Decay (LASAD), a\nnovel attention mechanism that explicitly preserves genuine 2D spatial\nrelationships within the flattened image sequences by computing\nposition-dependent decay factors based on true 2D spatial location rather than\n1D sequence positions. Based on this mechanism, we present LASADGen, an\nautoregressive image generator that enables selective attention to relevant\nspatial contexts with linear complexity. Experiments on ImageNet show LASADGen\nachieves state-of-the-art image generation performance and computational\nefficiency, bridging the gap between linear attention's efficiency and spatial\nunderstanding needed for high-quality generation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Autoregressive (AR) models have garnered significant attention in image\ngeneration for their ability to effectively capture both local and global\nstructures within visual data. However, prevalent AR models predominantly rely\non the transformer architectures, which are beset by quadratic computational\ncomplexity concerning input sequence length and substantial memory overhead due\nto the necessity of maintaining key-value caches. Although linear attention\nmechanisms have successfully reduced this burden in language models, our\ninitial experiments reveal that they significantly degrade image generation\nquality because of their inability to capture critical long-range dependencies\nin visual data. We propose Linear Attention with Spatial-Aware Decay (LASAD), a\nnovel attention mechanism that explicitly preserves genuine 2D spatial\nrelationships within the flattened image sequences by computing\nposition-dependent decay factors based on true 2D spatial location rather than\n1D sequence positions. Based on this mechanism, we present LASADGen, an\nautoregressive image generator that enables selective attention to relevant\nspatial contexts with linear complexity. Experiments on ImageNet show LASADGen\nachieves state-of-the-art image generation performance and computational\nefficiency, bridging the gap between linear attention's efficiency and spatial\nunderstanding needed for high-quality generation."
                },
                "authors": [
                    {
                        "name": "Yuxin Mao"
                    },
                    {
                        "name": "Zhen Qin"
                    },
                    {
                        "name": "Jinxing Zhou"
                    },
                    {
                        "name": "Hui Deng"
                    },
                    {
                        "name": "Xuyang Shen"
                    },
                    {
                        "name": "Bin Fan"
                    },
                    {
                        "name": "Jing Zhang"
                    },
                    {
                        "name": "Yiran Zhong"
                    },
                    {
                        "name": "Yuchao Dai"
                    }
                ],
                "author_detail": {
                    "name": "Yuchao Dai"
                },
                "author": "Yuchao Dai",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.01652v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.01652v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2212.10318v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2212.10318v4",
                "updated": "2025-07-02T10:16:58Z",
                "updated_parsed": [
                    2025,
                    7,
                    2,
                    10,
                    16,
                    58,
                    2,
                    183,
                    0
                ],
                "published": "2022-12-20T15:09:30Z",
                "published_parsed": [
                    2022,
                    12,
                    20,
                    15,
                    9,
                    30,
                    1,
                    354,
                    0
                ],
                "title": "Learned-Database Systems Security",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Learned-Database Systems Security"
                },
                "summary": "A learned database system uses machine learning (ML) internally to improve\nperformance. We can expect such systems to be vulnerable to some adversarial-ML\nattacks. Often, the learned component is shared between mutually-distrusting\nusers or processes, much like microarchitectural resources such as caches,\npotentially giving rise to highly-realistic attacker models. However, compared\nto attacks on other ML-based systems, attackers face a level of indirection as\nthey cannot interact directly with the learned model. Additionally, the\ndifference between the attack surface of learned and non-learned versions of\nthe same system is often subtle. These factors obfuscate the de-facto risks\nthat the incorporation of ML carries. We analyze the root causes of\npotentially-increased attack surface in learned database systems and develop a\nframework for identifying vulnerabilities that stem from the use of ML. We\napply our framework to a broad set of learned components currently being\nexplored in the database community. To empirically validate the vulnerabilities\nsurfaced by our framework, we choose 3 of them and implement and evaluate\nexploits against these. We show that the use of ML cause leakage of past\nqueries in a database, enable a poisoning attack that causes exponential memory\nblowup in an index structure and crashes it in seconds, and enable index users\nto snoop on each others' key distributions by timing queries over their own\nkeys. We find that adversarial ML is an universal threat against learned\ncomponents in database systems, point to open research gaps in our\nunderstanding of learned-systems security, and conclude by discussing\nmitigations, while noting that data leakage is inherent in systems whose\nlearned component is shared between multiple parties.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A learned database system uses machine learning (ML) internally to improve\nperformance. We can expect such systems to be vulnerable to some adversarial-ML\nattacks. Often, the learned component is shared between mutually-distrusting\nusers or processes, much like microarchitectural resources such as caches,\npotentially giving rise to highly-realistic attacker models. However, compared\nto attacks on other ML-based systems, attackers face a level of indirection as\nthey cannot interact directly with the learned model. Additionally, the\ndifference between the attack surface of learned and non-learned versions of\nthe same system is often subtle. These factors obfuscate the de-facto risks\nthat the incorporation of ML carries. We analyze the root causes of\npotentially-increased attack surface in learned database systems and develop a\nframework for identifying vulnerabilities that stem from the use of ML. We\napply our framework to a broad set of learned components currently being\nexplored in the database community. To empirically validate the vulnerabilities\nsurfaced by our framework, we choose 3 of them and implement and evaluate\nexploits against these. We show that the use of ML cause leakage of past\nqueries in a database, enable a poisoning attack that causes exponential memory\nblowup in an index structure and crashes it in seconds, and enable index users\nto snoop on each others' key distributions by timing queries over their own\nkeys. We find that adversarial ML is an universal threat against learned\ncomponents in database systems, point to open research gaps in our\nunderstanding of learned-systems security, and conclude by discussing\nmitigations, while noting that data leakage is inherent in systems whose\nlearned component is shared between multiple parties."
                },
                "authors": [
                    {
                        "name": "Roei Schuster"
                    },
                    {
                        "name": "Jin Peng Zhou"
                    },
                    {
                        "name": "Thorsten Eisenhofer"
                    },
                    {
                        "name": "Paul Grubbs"
                    },
                    {
                        "name": "Nicolas Papernot"
                    }
                ],
                "author_detail": {
                    "name": "Nicolas Papernot"
                },
                "author": "Nicolas Papernot",
                "arxiv_comment": "Accepted at TMLR",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2212.10318v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2212.10318v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.01465v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.01465v1",
                "updated": "2025-07-02T08:24:50Z",
                "updated_parsed": [
                    2025,
                    7,
                    2,
                    8,
                    24,
                    50,
                    2,
                    183,
                    0
                ],
                "published": "2025-07-02T08:24:50Z",
                "published_parsed": [
                    2025,
                    7,
                    2,
                    8,
                    24,
                    50,
                    2,
                    183,
                    0
                ],
                "title": "A new efficient RPKI Design",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A new efficient RPKI Design"
                },
                "summary": "Resource Public Key Infrastructure (RPKI) is a critical security mechanism\nfor BGP, but the complexity of its architecture is a growing concern as its\nadoption scales. Current RPKI design heavily reuses legacy PKI components, such\nas X.509 EE-certificates, ASN.1 encoding, and XML-based repository protocols,\nall these introduce excessive cryptographic validation, redundant metadata, and\ninefficiencies in both storage and processing. We show that these design\nchoices, although based on established standards, create significant\nperformance bottlenecks, increase the vulnerability surface, and hinder\nscalability for wide-scale Internet deployment.\n  In this paper, we perform the first systematic analysis of the root causes of\ncomplexity in RPKI's design and experimentally quantify their real-world\nimpact. We show that over 70% of validation time in RPKI relying parties is\nspent on certificate parsing and signature verification, much of it\nunnecessary. Building on this insight, we introduce the improved RPKI (iRPKI),\na backwards-compatible redesign that preserves all security guarantees while\nsubstantially reducing protocol overhead. iRPKI eliminates EE-certificates and\nROA signatures, merges revocation and integrity objects, replaces verbose\nencodings with Protobuf, and restructures repository metadata for more\nefficient access. We experimentally demonstrate that our implementation of\niRPKI in the Routinator validator achieves a 20x speed-up of processing time,\n18x improvement of bandwidth requirements and 8x reduction in cache memory\nfootprint, while also eliminating classes of vulnerabilities that have led to\nat least 10 vulnerabilities in RPKI software. iRPKI significantly increases the\nfeasibility of deploying RPKI at scale in the Internet, and especially in\nconstrained environments. Our design may be deployed incrementally without\nimpacting existing operations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Resource Public Key Infrastructure (RPKI) is a critical security mechanism\nfor BGP, but the complexity of its architecture is a growing concern as its\nadoption scales. Current RPKI design heavily reuses legacy PKI components, such\nas X.509 EE-certificates, ASN.1 encoding, and XML-based repository protocols,\nall these introduce excessive cryptographic validation, redundant metadata, and\ninefficiencies in both storage and processing. We show that these design\nchoices, although based on established standards, create significant\nperformance bottlenecks, increase the vulnerability surface, and hinder\nscalability for wide-scale Internet deployment.\n  In this paper, we perform the first systematic analysis of the root causes of\ncomplexity in RPKI's design and experimentally quantify their real-world\nimpact. We show that over 70% of validation time in RPKI relying parties is\nspent on certificate parsing and signature verification, much of it\nunnecessary. Building on this insight, we introduce the improved RPKI (iRPKI),\na backwards-compatible redesign that preserves all security guarantees while\nsubstantially reducing protocol overhead. iRPKI eliminates EE-certificates and\nROA signatures, merges revocation and integrity objects, replaces verbose\nencodings with Protobuf, and restructures repository metadata for more\nefficient access. We experimentally demonstrate that our implementation of\niRPKI in the Routinator validator achieves a 20x speed-up of processing time,\n18x improvement of bandwidth requirements and 8x reduction in cache memory\nfootprint, while also eliminating classes of vulnerabilities that have led to\nat least 10 vulnerabilities in RPKI software. iRPKI significantly increases the\nfeasibility of deploying RPKI at scale in the Internet, and especially in\nconstrained environments. Our design may be deployed incrementally without\nimpacting existing operations."
                },
                "authors": [
                    {
                        "name": "Haya Schulmann"
                    },
                    {
                        "name": "Niklas Vogel"
                    }
                ],
                "author_detail": {
                    "name": "Niklas Vogel"
                },
                "author": "Niklas Vogel",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.01465v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.01465v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.01438v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.01438v1",
                "updated": "2025-07-02T07:47:28Z",
                "updated_parsed": [
                    2025,
                    7,
                    2,
                    7,
                    47,
                    28,
                    2,
                    183,
                    0
                ],
                "published": "2025-07-02T07:47:28Z",
                "published_parsed": [
                    2025,
                    7,
                    2,
                    7,
                    47,
                    28,
                    2,
                    183,
                    0
                ],
                "title": "EdgeLoRA: An Efficient Multi-Tenant LLM Serving System on Edge Devices",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "EdgeLoRA: An Efficient Multi-Tenant LLM Serving System on Edge Devices"
                },
                "summary": "Large Language Models (LLMs) have gained significant attention due to their\nversatility across a wide array of applications. Fine-tuning LLMs with\nparameter-efficient adapters, such as Low-Rank Adaptation (LoRA), enables these\nmodels to efficiently adapt to downstream tasks without extensive retraining.\nDeploying fine-tuned LLMs on multi-tenant edge devices offers substantial\nbenefits, such as reduced latency, enhanced privacy, and personalized\nresponses. However, serving LLMs efficiently on resource-constrained edge\ndevices presents critical challenges, including the complexity of adapter\nselection for different tasks and memory overhead from frequent adapter\nswapping. Moreover, given the multiple requests in multi-tenant settings,\nprocessing requests sequentially results in underutilization of computational\nresources and increased latency. This paper introduces EdgeLoRA, an efficient\nsystem for serving LLMs on edge devices in multi-tenant environments. EdgeLoRA\nincorporates three key innovations: (1) an adaptive adapter selection mechanism\nto streamline the adapter configuration process; (2) heterogeneous memory\nmanagement, leveraging intelligent adapter caching and pooling to mitigate\nmemory operation overhead; and (3) batch LoRA inference, enabling efficient\nbatch processing to significantly reduce computational latency. Comprehensive\nevaluations using the Llama3.1-8B model demonstrate that EdgeLoRA significantly\noutperforms the status quo (i.e., llama.cpp) in terms of both latency and\nthroughput. The results demonstrate that EdgeLoRA can achieve up to a 4 times\nboost in throughput. Even more impressively, it can serve several orders of\nmagnitude more adapters simultaneously. These results highlight EdgeLoRA's\npotential to transform edge deployment of LLMs in multi-tenant scenarios,\noffering a scalable and efficient solution for resource-constrained\nenvironments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have gained significant attention due to their\nversatility across a wide array of applications. Fine-tuning LLMs with\nparameter-efficient adapters, such as Low-Rank Adaptation (LoRA), enables these\nmodels to efficiently adapt to downstream tasks without extensive retraining.\nDeploying fine-tuned LLMs on multi-tenant edge devices offers substantial\nbenefits, such as reduced latency, enhanced privacy, and personalized\nresponses. However, serving LLMs efficiently on resource-constrained edge\ndevices presents critical challenges, including the complexity of adapter\nselection for different tasks and memory overhead from frequent adapter\nswapping. Moreover, given the multiple requests in multi-tenant settings,\nprocessing requests sequentially results in underutilization of computational\nresources and increased latency. This paper introduces EdgeLoRA, an efficient\nsystem for serving LLMs on edge devices in multi-tenant environments. EdgeLoRA\nincorporates three key innovations: (1) an adaptive adapter selection mechanism\nto streamline the adapter configuration process; (2) heterogeneous memory\nmanagement, leveraging intelligent adapter caching and pooling to mitigate\nmemory operation overhead; and (3) batch LoRA inference, enabling efficient\nbatch processing to significantly reduce computational latency. Comprehensive\nevaluations using the Llama3.1-8B model demonstrate that EdgeLoRA significantly\noutperforms the status quo (i.e., llama.cpp) in terms of both latency and\nthroughput. The results demonstrate that EdgeLoRA can achieve up to a 4 times\nboost in throughput. Even more impressively, it can serve several orders of\nmagnitude more adapters simultaneously. These results highlight EdgeLoRA's\npotential to transform edge deployment of LLMs in multi-tenant scenarios,\noffering a scalable and efficient solution for resource-constrained\nenvironments."
                },
                "authors": [
                    {
                        "name": "Zheyu Shen"
                    },
                    {
                        "name": "Yexiao He"
                    },
                    {
                        "name": "Ziyao Wang"
                    },
                    {
                        "name": "Yuning Zhang"
                    },
                    {
                        "name": "Guoheng Sun"
                    },
                    {
                        "name": "Wanghao Ye"
                    },
                    {
                        "name": "Ang Li"
                    }
                ],
                "author_detail": {
                    "name": "Ang Li"
                },
                "author": "Ang Li",
                "arxiv_doi": "10.1145/3711875.3729141",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3711875.3729141",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2507.01438v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.01438v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.20187v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.20187v2",
                "updated": "2025-07-02T05:12:29Z",
                "updated_parsed": [
                    2025,
                    7,
                    2,
                    5,
                    12,
                    29,
                    2,
                    183,
                    0
                ],
                "published": "2025-06-25T07:26:42Z",
                "published_parsed": [
                    2025,
                    6,
                    25,
                    7,
                    26,
                    42,
                    2,
                    176,
                    0
                ],
                "title": "Breaking the Boundaries of Long-Context LLM Inference: Adaptive KV\n  Management on a Single Commodity GPU",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Breaking the Boundaries of Long-Context LLM Inference: Adaptive KV\n  Management on a Single Commodity GPU"
                },
                "summary": "Advanced Large Language Models (LLMs) have achieved impressive performance\nacross a wide range of complex and long-context natural language tasks.\nHowever, performing long-context LLM inference locally on a commodity GPU (a\nPC) with privacy concerns remains challenging due to the increasing memory\ndemands of the key-value (KV) cache. Existing systems typically identify\nimportant tokens and selectively offload their KV data to GPU and CPU memory.\nThe KV data needs to be offloaded to disk due to the limited memory on a\ncommodity GPU, but the process is bottlenecked by token importance evaluation\noverhead and the disk's low bandwidth. In this paper, we present LeoAM, the\nfirst efficient importance-aware long-context LLM inference system for a single\ncommodity GPU with adaptive hierarchical GPU-CPU-Disk KV management. Our system\nemploys an adaptive KV management strategy that partitions KV data into\nvariable-sized chunks based on the skewed distribution of attention weights\nacross different layers to reduce computational and additional transmission\noverheads. Moreover, we propose a lightweight KV abstract method, which\nminimizes transmission latency by storing and extracting the KV abstract of\neach chunk on disk instead of the full KV data. LeoAM also leverages the\ndynamic compression and pipeline techniques to further accelerate inference.\nExperimental results demonstrate that LongInfer achieves an average inference\nlatency speedup of 3.46x, while maintaining comparable LLM response quality. In\nscenarios with larger batch sizes, it achieves up to a 5.47x speedup.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Advanced Large Language Models (LLMs) have achieved impressive performance\nacross a wide range of complex and long-context natural language tasks.\nHowever, performing long-context LLM inference locally on a commodity GPU (a\nPC) with privacy concerns remains challenging due to the increasing memory\ndemands of the key-value (KV) cache. Existing systems typically identify\nimportant tokens and selectively offload their KV data to GPU and CPU memory.\nThe KV data needs to be offloaded to disk due to the limited memory on a\ncommodity GPU, but the process is bottlenecked by token importance evaluation\noverhead and the disk's low bandwidth. In this paper, we present LeoAM, the\nfirst efficient importance-aware long-context LLM inference system for a single\ncommodity GPU with adaptive hierarchical GPU-CPU-Disk KV management. Our system\nemploys an adaptive KV management strategy that partitions KV data into\nvariable-sized chunks based on the skewed distribution of attention weights\nacross different layers to reduce computational and additional transmission\noverheads. Moreover, we propose a lightweight KV abstract method, which\nminimizes transmission latency by storing and extracting the KV abstract of\neach chunk on disk instead of the full KV data. LeoAM also leverages the\ndynamic compression and pipeline techniques to further accelerate inference.\nExperimental results demonstrate that LongInfer achieves an average inference\nlatency speedup of 3.46x, while maintaining comparable LLM response quality. In\nscenarios with larger batch sizes, it achieves up to a 5.47x speedup."
                },
                "authors": [
                    {
                        "name": "He Sun"
                    },
                    {
                        "name": "Li Li"
                    },
                    {
                        "name": "Mingjun Xiao"
                    },
                    {
                        "name": "Chengzhong Xu"
                    }
                ],
                "author_detail": {
                    "name": "Chengzhong Xu"
                },
                "author": "Chengzhong Xu",
                "arxiv_comment": "15 pages, 23 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.20187v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.20187v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.OS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.OS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "68M20",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "C.4",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.02006v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.02006v1",
                "updated": "2025-07-02T00:35:43Z",
                "updated_parsed": [
                    2025,
                    7,
                    2,
                    0,
                    35,
                    43,
                    2,
                    183,
                    0
                ],
                "published": "2025-07-02T00:35:43Z",
                "published_parsed": [
                    2025,
                    7,
                    2,
                    0,
                    35,
                    43,
                    2,
                    183,
                    0
                ],
                "title": "AIRES: Accelerating Out-of-Core GCNs via Algorithm-System Co-Design",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AIRES: Accelerating Out-of-Core GCNs via Algorithm-System Co-Design"
                },
                "summary": "Graph convolutional networks (GCNs) are fundamental in various scientific\napplications, ranging from biomedical protein-protein interactions (PPI) to\nlarge-scale recommendation systems. An essential component for modeling graph\nstructures in GCNs is sparse general matrix-matrix multiplication (SpGEMM). As\nthe size of graph data continues to scale up, SpGEMMs are often conducted in an\nout-of-core fashion due to limited GPU memory space in resource-constrained\nsystems. Albeit recent efforts that aim to alleviate the memory constraints of\nout-of-core SpGEMM through either GPU feature caching, hybrid CPU-GPU memory\nlayout, or performing the computation in sparse format, current systems suffer\nfrom both high I/O latency and GPU under-utilization issues.\n  In this paper, we first identify the problems of existing systems, where\nsparse format data alignment and memory allocation are the main performance\nbottlenecks, and propose AIRES, a novel algorithm-system co-design solution to\naccelerate out-of-core SpGEMM computation for GCNs. Specifically, from the\nalgorithm angle, AIRES proposes to alleviate the data alignment issues on the\nblock level for matrices in sparse formats and develops a tiling algorithm to\nfacilitate row block-wise alignment. On the system level, AIRES employs a\nthree-phase dynamic scheduling that features a dual-way data transfer strategy\nutilizing a tiered memory system: integrating GPU memory, GPU Direct Storage\n(GDS), and host memory to reduce I/O latency and improve throughput.\nEvaluations show that AIRES significantly outperforms the state-of-the-art\nmethods, achieving up to 1.8x lower latency in real-world graph processing\nbenchmarks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Graph convolutional networks (GCNs) are fundamental in various scientific\napplications, ranging from biomedical protein-protein interactions (PPI) to\nlarge-scale recommendation systems. An essential component for modeling graph\nstructures in GCNs is sparse general matrix-matrix multiplication (SpGEMM). As\nthe size of graph data continues to scale up, SpGEMMs are often conducted in an\nout-of-core fashion due to limited GPU memory space in resource-constrained\nsystems. Albeit recent efforts that aim to alleviate the memory constraints of\nout-of-core SpGEMM through either GPU feature caching, hybrid CPU-GPU memory\nlayout, or performing the computation in sparse format, current systems suffer\nfrom both high I/O latency and GPU under-utilization issues.\n  In this paper, we first identify the problems of existing systems, where\nsparse format data alignment and memory allocation are the main performance\nbottlenecks, and propose AIRES, a novel algorithm-system co-design solution to\naccelerate out-of-core SpGEMM computation for GCNs. Specifically, from the\nalgorithm angle, AIRES proposes to alleviate the data alignment issues on the\nblock level for matrices in sparse formats and develops a tiling algorithm to\nfacilitate row block-wise alignment. On the system level, AIRES employs a\nthree-phase dynamic scheduling that features a dual-way data transfer strategy\nutilizing a tiered memory system: integrating GPU memory, GPU Direct Storage\n(GDS), and host memory to reduce I/O latency and improve throughput.\nEvaluations show that AIRES significantly outperforms the state-of-the-art\nmethods, achieving up to 1.8x lower latency in real-world graph processing\nbenchmarks."
                },
                "authors": [
                    {
                        "name": "Shakya Jayakody"
                    },
                    {
                        "name": "Youpeng Zhao"
                    },
                    {
                        "name": "Jun Wang"
                    }
                ],
                "author_detail": {
                    "name": "Jun Wang"
                },
                "author": "Jun Wang",
                "arxiv_comment": "36th IEEE International Conference on Application-Specific Systems,\n  Architectures and Processors. (Accepted)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.02006v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.02006v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.01216v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.01216v1",
                "updated": "2025-07-01T22:27:21Z",
                "updated_parsed": [
                    2025,
                    7,
                    1,
                    22,
                    27,
                    21,
                    1,
                    182,
                    0
                ],
                "published": "2025-07-01T22:27:21Z",
                "published_parsed": [
                    2025,
                    7,
                    1,
                    22,
                    27,
                    21,
                    1,
                    182,
                    0
                ],
                "title": "PAE MobiLLM: Privacy-Aware and Efficient LLM Fine-Tuning on the Mobile\n  Device via Additive Side-Tuning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PAE MobiLLM: Privacy-Aware and Efficient LLM Fine-Tuning on the Mobile\n  Device via Additive Side-Tuning"
                },
                "summary": "There is a huge gap between numerous intriguing applications fostered by\non-device large language model (LLM) fine-tuning (FT) from fresh mobile data\nand the limited resources of a mobile device. While existing server-assisted\nmethods (e.g., split learning or side-tuning) may enable LLM FT on the local\nmobile device, they suffer from heavy communication burdens of activation\ntransmissions, and may disclose data, labels or fine-tuned models to the\nserver. To address those issues, we develop PAE MobiLLM, a privacy-aware and\nefficient LLM FT method which can be deployed on the mobile device via\nserver-assisted additive side-tuning. To further accelerate FT convergence and\nimprove computing efficiency, PAE MobiLLM integrates activation caching on the\nserver side, which allows the server to reuse historical activations and saves\nthe mobile device from repeatedly computing forward passes for the recurring\ndata samples. Besides, to reduce communication cost, PAE MobiLLM develops a\none-token (i.e., ``pivot'' token) activation shortcut that transmits only a\nsingle activation dimension instead of full activation matrices to guide the\nside network tuning. Last but not least, PAE MobiLLM introduces the additive\nadapter side-network design which makes the server train the adapter modules\nbased on device-defined prediction differences rather than raw ground-truth\nlabels. In this way, the server can only assist device-defined side-network\ncomputing, and learn nothing about data, labels or fine-tuned models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "There is a huge gap between numerous intriguing applications fostered by\non-device large language model (LLM) fine-tuning (FT) from fresh mobile data\nand the limited resources of a mobile device. While existing server-assisted\nmethods (e.g., split learning or side-tuning) may enable LLM FT on the local\nmobile device, they suffer from heavy communication burdens of activation\ntransmissions, and may disclose data, labels or fine-tuned models to the\nserver. To address those issues, we develop PAE MobiLLM, a privacy-aware and\nefficient LLM FT method which can be deployed on the mobile device via\nserver-assisted additive side-tuning. To further accelerate FT convergence and\nimprove computing efficiency, PAE MobiLLM integrates activation caching on the\nserver side, which allows the server to reuse historical activations and saves\nthe mobile device from repeatedly computing forward passes for the recurring\ndata samples. Besides, to reduce communication cost, PAE MobiLLM develops a\none-token (i.e., ``pivot'' token) activation shortcut that transmits only a\nsingle activation dimension instead of full activation matrices to guide the\nside network tuning. Last but not least, PAE MobiLLM introduces the additive\nadapter side-network design which makes the server train the adapter modules\nbased on device-defined prediction differences rather than raw ground-truth\nlabels. In this way, the server can only assist device-defined side-network\ncomputing, and learn nothing about data, labels or fine-tuned models."
                },
                "authors": [
                    {
                        "name": "Xingke Yang"
                    },
                    {
                        "name": "Liang Li"
                    },
                    {
                        "name": "Zhiyi Wan"
                    },
                    {
                        "name": "Sicong Li"
                    },
                    {
                        "name": "Hao Wang"
                    },
                    {
                        "name": "Xiaoqi Qi"
                    },
                    {
                        "name": "Jiang Liu"
                    },
                    {
                        "name": "Tomoaki Ohtsuki"
                    },
                    {
                        "name": "Xin Fu"
                    },
                    {
                        "name": "Miao Pan"
                    }
                ],
                "author_detail": {
                    "name": "Miao Pan"
                },
                "author": "Miao Pan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.01216v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.01216v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.15682v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.15682v2",
                "updated": "2025-07-01T21:27:40Z",
                "updated_parsed": [
                    2025,
                    7,
                    1,
                    21,
                    27,
                    40,
                    1,
                    182,
                    0
                ],
                "published": "2025-06-18T17:59:50Z",
                "published_parsed": [
                    2025,
                    6,
                    18,
                    17,
                    59,
                    50,
                    2,
                    169,
                    0
                ],
                "title": "Evolutionary Caching to Accelerate Your Off-the-Shelf Diffusion Model",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Evolutionary Caching to Accelerate Your Off-the-Shelf Diffusion Model"
                },
                "summary": "Diffusion-based image generation models excel at producing high-quality\nsynthetic content, but suffer from slow and computationally expensive\ninference. Prior work has attempted to mitigate this by caching and reusing\nfeatures within diffusion transformers across inference steps. These methods,\nhowever, often rely on rigid heuristics that result in limited acceleration or\npoor generalization across architectures. We propose Evolutionary Caching to\nAccelerate Diffusion models (ECAD), a genetic algorithm that learns efficient,\nper-model, caching schedules forming a Pareto frontier, using only a small set\nof calibration prompts. ECAD requires no modifications to network parameters or\nreference images. It offers significant inference speedups, enables\nfine-grained control over the quality-latency trade-off, and adapts seamlessly\nto different diffusion models. Notably, ECAD's learned schedules can generalize\neffectively to resolutions and model variants not seen during calibration. We\nevaluate ECAD on PixArt-alpha, PixArt-Sigma, and FLUX-1$.$dev using multiple\nmetrics (FID, CLIP, Image Reward) across diverse benchmarks (COCO, MJHQ-30k,\nPartiPrompts), demonstrating consistent improvements over previous approaches.\nOn PixArt-alpha, ECAD identifies a schedule that outperforms the previous\nstate-of-the-art method by 4.47 COCO FID while increasing inference speedup\nfrom 2.35x to 2.58x. Our results establish ECAD as a scalable and generalizable\napproach for accelerating diffusion inference. Our project website is available\nat https://aniaggarwal.github.io/ecad and our code is available at\nhttps://github.com/aniaggarwal/ecad.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion-based image generation models excel at producing high-quality\nsynthetic content, but suffer from slow and computationally expensive\ninference. Prior work has attempted to mitigate this by caching and reusing\nfeatures within diffusion transformers across inference steps. These methods,\nhowever, often rely on rigid heuristics that result in limited acceleration or\npoor generalization across architectures. We propose Evolutionary Caching to\nAccelerate Diffusion models (ECAD), a genetic algorithm that learns efficient,\nper-model, caching schedules forming a Pareto frontier, using only a small set\nof calibration prompts. ECAD requires no modifications to network parameters or\nreference images. It offers significant inference speedups, enables\nfine-grained control over the quality-latency trade-off, and adapts seamlessly\nto different diffusion models. Notably, ECAD's learned schedules can generalize\neffectively to resolutions and model variants not seen during calibration. We\nevaluate ECAD on PixArt-alpha, PixArt-Sigma, and FLUX-1$.$dev using multiple\nmetrics (FID, CLIP, Image Reward) across diverse benchmarks (COCO, MJHQ-30k,\nPartiPrompts), demonstrating consistent improvements over previous approaches.\nOn PixArt-alpha, ECAD identifies a schedule that outperforms the previous\nstate-of-the-art method by 4.47 COCO FID while increasing inference speedup\nfrom 2.35x to 2.58x. Our results establish ECAD as a scalable and generalizable\napproach for accelerating diffusion inference. Our project website is available\nat https://aniaggarwal.github.io/ecad and our code is available at\nhttps://github.com/aniaggarwal/ecad."
                },
                "authors": [
                    {
                        "name": "Anirud Aggarwal"
                    },
                    {
                        "name": "Abhinav Shrivastava"
                    },
                    {
                        "name": "Matthew Gwilliam"
                    }
                ],
                "author_detail": {
                    "name": "Matthew Gwilliam"
                },
                "author": "Matthew Gwilliam",
                "arxiv_comment": "29 pages, 22 figures, 9 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.15682v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.15682v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.01154v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.01154v1",
                "updated": "2025-07-01T19:28:37Z",
                "updated_parsed": [
                    2025,
                    7,
                    1,
                    19,
                    28,
                    37,
                    1,
                    182,
                    0
                ],
                "published": "2025-07-01T19:28:37Z",
                "published_parsed": [
                    2025,
                    7,
                    1,
                    19,
                    28,
                    37,
                    1,
                    182,
                    0
                ],
                "title": "FlashDP: Private Training Large Language Models with Efficient DP-SGD",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FlashDP: Private Training Large Language Models with Efficient DP-SGD"
                },
                "summary": "As large language models (LLMs) increasingly underpin technological\nadvancements, the privacy of their training data emerges as a critical concern.\nDifferential Privacy (DP) serves as a rigorous mechanism to protect this data,\nyet its integration via Differentially Private Stochastic Gradient Descent\n(DP-SGD) introduces substantial challenges, primarily due to the complexities\nof per-sample gradient clipping. Current explicit methods, such as Opacus,\nnecessitate extensive storage for per-sample gradients, significantly inflating\nmemory requirements. Conversely, implicit methods like GhostClip reduce storage\nneeds by recalculating gradients multiple times, which leads to inefficiencies\ndue to redundant computations. This paper introduces FlashDP, an innovative\ncache-friendly per-layer DP-SGD that consolidates necessary operations into a\nsingle task, calculating gradients only once in a fused manner. This approach\nnot only diminishes memory movement by up to \\textbf{50\\%} but also cuts down\nredundant computations by \\textbf{20\\%}, compared to previous methods.\nConsequently, FlashDP does not increase memory demands and achieves a\n\\textbf{90\\%} throughput compared to the Non-DP method on a four-A100 system\nduring the pre-training of the Llama-13B model, while maintaining parity with\nstandard per-layer clipped DP-SGD in terms of accuracy. These advancements\nestablish FlashDP as a pivotal development for efficient and privacy-preserving\ntraining of LLMs. FlashDP's code has been open-sourced in\nhttps://github.com/kaustpradalab/flashdp.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As large language models (LLMs) increasingly underpin technological\nadvancements, the privacy of their training data emerges as a critical concern.\nDifferential Privacy (DP) serves as a rigorous mechanism to protect this data,\nyet its integration via Differentially Private Stochastic Gradient Descent\n(DP-SGD) introduces substantial challenges, primarily due to the complexities\nof per-sample gradient clipping. Current explicit methods, such as Opacus,\nnecessitate extensive storage for per-sample gradients, significantly inflating\nmemory requirements. Conversely, implicit methods like GhostClip reduce storage\nneeds by recalculating gradients multiple times, which leads to inefficiencies\ndue to redundant computations. This paper introduces FlashDP, an innovative\ncache-friendly per-layer DP-SGD that consolidates necessary operations into a\nsingle task, calculating gradients only once in a fused manner. This approach\nnot only diminishes memory movement by up to \\textbf{50\\%} but also cuts down\nredundant computations by \\textbf{20\\%}, compared to previous methods.\nConsequently, FlashDP does not increase memory demands and achieves a\n\\textbf{90\\%} throughput compared to the Non-DP method on a four-A100 system\nduring the pre-training of the Llama-13B model, while maintaining parity with\nstandard per-layer clipped DP-SGD in terms of accuracy. These advancements\nestablish FlashDP as a pivotal development for efficient and privacy-preserving\ntraining of LLMs. FlashDP's code has been open-sourced in\nhttps://github.com/kaustpradalab/flashdp."
                },
                "authors": [
                    {
                        "name": "Liangyu Wang"
                    },
                    {
                        "name": "Junxiao Wang"
                    },
                    {
                        "name": "Jie Ren"
                    },
                    {
                        "name": "Zihang Xiang"
                    },
                    {
                        "name": "David E. Keyes"
                    },
                    {
                        "name": "Di Wang"
                    }
                ],
                "author_detail": {
                    "name": "Di Wang"
                },
                "author": "Di Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.01154v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.01154v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.00929v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.00929v1",
                "updated": "2025-07-01T16:36:23Z",
                "updated_parsed": [
                    2025,
                    7,
                    1,
                    16,
                    36,
                    23,
                    1,
                    182,
                    0
                ],
                "published": "2025-07-01T16:36:23Z",
                "published_parsed": [
                    2025,
                    7,
                    1,
                    16,
                    36,
                    23,
                    1,
                    182,
                    0
                ],
                "title": "Integrating nano- and micrometer-scale energy deposition models for\n  mechanistic prediction of radiation-induced DNA damage and cell survival",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Integrating nano- and micrometer-scale energy deposition models for\n  mechanistic prediction of radiation-induced DNA damage and cell survival"
                },
                "summary": "We present an integrated modeling framework that combines the Generalized\nStochastic Microdosimetric Model (GSM2), used to predict cell survival\nfractions, with MINAS-TIRITH, a fast and efficient Geant4 DNA-based tool for\nsimulating radiation-induced DNA damage in cell populations. This approach\nenables the generation of spatially and structurally resolved double-strand\nbreak (DSB) distributions, capturing key features such as damage complexity and\nchromosome specificity. A novel application of the DBSCAN clustering algorithm\nis introduced to group DSBs at the micrometer scale. This allows the\nidentification of physical aggregates of DNA damage and their association with\nsubnuclear domains, providing a direct link to the cell survival probability as\npredicted by \\gsm.\n  The model was validated using experimental data from HUVEC cells irradiated\nwith 220 kV X-rays and H460 cells exposed to protons over a wide linear energy\ntransfer (LET) range, from approximately 4 keV/{\\mu}m to over 20 keV/{\\mu}m.\nResults show excellent agreement between simulations and experimental survival\nprobabilities, making this one of the first consistent multi-scale models to\nbridge nanodosimetric and microdosimetric representations of radiation with\nbiological outcomes such as cell survival.\n  By incorporating the inherent stochastic nature of radiation-matter\ninteractions, this framework effectively connects the physical properties of\nthe radiation field to the biological response at the cellular level. Its\naccuracy across various radiation types and energies supports its potential for\nuse in biologically optimized radiotherapy.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present an integrated modeling framework that combines the Generalized\nStochastic Microdosimetric Model (GSM2), used to predict cell survival\nfractions, with MINAS-TIRITH, a fast and efficient Geant4 DNA-based tool for\nsimulating radiation-induced DNA damage in cell populations. This approach\nenables the generation of spatially and structurally resolved double-strand\nbreak (DSB) distributions, capturing key features such as damage complexity and\nchromosome specificity. A novel application of the DBSCAN clustering algorithm\nis introduced to group DSBs at the micrometer scale. This allows the\nidentification of physical aggregates of DNA damage and their association with\nsubnuclear domains, providing a direct link to the cell survival probability as\npredicted by \\gsm.\n  The model was validated using experimental data from HUVEC cells irradiated\nwith 220 kV X-rays and H460 cells exposed to protons over a wide linear energy\ntransfer (LET) range, from approximately 4 keV/{\\mu}m to over 20 keV/{\\mu}m.\nResults show excellent agreement between simulations and experimental survival\nprobabilities, making this one of the first consistent multi-scale models to\nbridge nanodosimetric and microdosimetric representations of radiation with\nbiological outcomes such as cell survival.\n  By incorporating the inherent stochastic nature of radiation-matter\ninteractions, this framework effectively connects the physical properties of\nthe radiation field to the biological response at the cellular level. Its\naccuracy across various radiation types and energies supports its potential for\nuse in biologically optimized radiotherapy."
                },
                "authors": [
                    {
                        "name": "Giulio Bordieri"
                    },
                    {
                        "name": "Marta Missiggia"
                    },
                    {
                        "name": "Gianluca Lattanzi"
                    },
                    {
                        "name": "Carmen Villagrasa"
                    },
                    {
                        "name": "Yann Perrot"
                    },
                    {
                        "name": "Francesco G. Cordoni"
                    }
                ],
                "author_detail": {
                    "name": "Francesco G. Cordoni"
                },
                "author": "Francesco G. Cordoni",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.00929v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.00929v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.bio-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.bio-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.00797v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.00797v1",
                "updated": "2025-07-01T14:30:31Z",
                "updated_parsed": [
                    2025,
                    7,
                    1,
                    14,
                    30,
                    31,
                    1,
                    182,
                    0
                ],
                "published": "2025-07-01T14:30:31Z",
                "published_parsed": [
                    2025,
                    7,
                    1,
                    14,
                    30,
                    31,
                    1,
                    182,
                    0
                ],
                "title": "VEDA: Efficient LLM Generation Through Voting-based KV Cache Eviction\n  and Dataflow-flexible Accelerator",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "VEDA: Efficient LLM Generation Through Voting-based KV Cache Eviction\n  and Dataflow-flexible Accelerator"
                },
                "summary": "Large Language Models (LLMs) excel in natural language processing tasks but\npose significant computational and memory challenges for edge deployment due to\ntheir intensive resource demands. This work addresses the efficiency of LLM\ninference by algorithm-hardware-dataflow tri-optimizations. We propose a novel\nvoting-based KV cache eviction algorithm, balancing hardware efficiency and\nalgorithm accuracy by adaptively identifying unimportant kv vectors. From a\ndataflow perspective, we introduce a flexible-product dataflow and a runtime\nreconfigurable PE array for matrix-vector multiplication. The proposed approach\neffectively handles the diverse dimensional requirements and solves the\nchallenges of incrementally varying sequence lengths. Additionally, an\nelement-serial scheduling scheme is proposed for nonlinear operations, such as\nsoftmax and layer normalization (layernorm). Results demonstrate a substantial\nreduction in latency, accompanied by a significant decrease in hardware\ncomplexity, from O(N) to O(1). The proposed solution is realized in a\ncustom-designed accelerator, VEDA, which outperforms existing hardware\nplatforms. This research represents a significant advancement in LLM inference\non resource-constrained edge devices, facilitating real-time processing,\nenhancing data privacy, and enabling model customization.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) excel in natural language processing tasks but\npose significant computational and memory challenges for edge deployment due to\ntheir intensive resource demands. This work addresses the efficiency of LLM\ninference by algorithm-hardware-dataflow tri-optimizations. We propose a novel\nvoting-based KV cache eviction algorithm, balancing hardware efficiency and\nalgorithm accuracy by adaptively identifying unimportant kv vectors. From a\ndataflow perspective, we introduce a flexible-product dataflow and a runtime\nreconfigurable PE array for matrix-vector multiplication. The proposed approach\neffectively handles the diverse dimensional requirements and solves the\nchallenges of incrementally varying sequence lengths. Additionally, an\nelement-serial scheduling scheme is proposed for nonlinear operations, such as\nsoftmax and layer normalization (layernorm). Results demonstrate a substantial\nreduction in latency, accompanied by a significant decrease in hardware\ncomplexity, from O(N) to O(1). The proposed solution is realized in a\ncustom-designed accelerator, VEDA, which outperforms existing hardware\nplatforms. This research represents a significant advancement in LLM inference\non resource-constrained edge devices, facilitating real-time processing,\nenhancing data privacy, and enabling model customization."
                },
                "authors": [
                    {
                        "name": "Zhican Wang"
                    },
                    {
                        "name": "Hongxiang Fan"
                    },
                    {
                        "name": "Haroon Waris"
                    },
                    {
                        "name": "Gang Wang"
                    },
                    {
                        "name": "Zhenyu Li"
                    },
                    {
                        "name": "Jianfei Jiang"
                    },
                    {
                        "name": "Yanan Sun"
                    },
                    {
                        "name": "Guanghui He"
                    }
                ],
                "author_detail": {
                    "name": "Guanghui He"
                },
                "author": "Guanghui He",
                "arxiv_comment": "DAC 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.00797v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.00797v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.00727v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.00727v1",
                "updated": "2025-07-01T13:17:46Z",
                "updated_parsed": [
                    2025,
                    7,
                    1,
                    13,
                    17,
                    46,
                    1,
                    182,
                    0
                ],
                "published": "2025-07-01T13:17:46Z",
                "published_parsed": [
                    2025,
                    7,
                    1,
                    13,
                    17,
                    46,
                    1,
                    182,
                    0
                ],
                "title": "On Hierarchical Coded Caching with Offline Users",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "On Hierarchical Coded Caching with Offline Users"
                },
                "summary": "This paper studies a two-layer hierarchical network in which some users are\noffline during the content delivery phase. A two-layer hierarchical network\nconsists of a single server connected to multiple cache-aided mirror sites, and\neach mirror site is connected to a distinct set of cache-aided users. A scheme\nfor such a hierarchical system with offline users has been proposed recently\nbut considered a special case where all mirror caches have zero memory, which\nis a significant limitation. We propose an array known as a hierarchical\nhotplug placement delivery array (HHPDA), which describes the placement and\ndelivery phases of a coded caching scheme for a general two-layer hierarchical\nnetwork with offline users. Further, we construct a class of HHPDAs using\ncombinatorial t-designs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper studies a two-layer hierarchical network in which some users are\noffline during the content delivery phase. A two-layer hierarchical network\nconsists of a single server connected to multiple cache-aided mirror sites, and\neach mirror site is connected to a distinct set of cache-aided users. A scheme\nfor such a hierarchical system with offline users has been proposed recently\nbut considered a special case where all mirror caches have zero memory, which\nis a significant limitation. We propose an array known as a hierarchical\nhotplug placement delivery array (HHPDA), which describes the placement and\ndelivery phases of a coded caching scheme for a general two-layer hierarchical\nnetwork with offline users. Further, we construct a class of HHPDAs using\ncombinatorial t-designs."
                },
                "authors": [
                    {
                        "name": "Rashid Ummer N. T."
                    },
                    {
                        "name": "B. Sundar Rajan"
                    }
                ],
                "author_detail": {
                    "name": "B. Sundar Rajan"
                },
                "author": "B. Sundar Rajan",
                "arxiv_comment": "A short version of this is accepted for presentation in 2025 IEEE\n  Information Theory Workshop; 8 pages, one figure",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.00727v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.00727v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IT",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.00716v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.00716v1",
                "updated": "2025-07-01T12:51:09Z",
                "updated_parsed": [
                    2025,
                    7,
                    1,
                    12,
                    51,
                    9,
                    1,
                    182,
                    0
                ],
                "published": "2025-07-01T12:51:09Z",
                "published_parsed": [
                    2025,
                    7,
                    1,
                    12,
                    51,
                    9,
                    1,
                    182,
                    0
                ],
                "title": "Accelerating Loading WebGraphs in ParaGrapher",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Accelerating Loading WebGraphs in ParaGrapher"
                },
                "summary": "ParaGrapher is a graph loading API and library that enables graph processing\nframeworks to load large-scale compressed graphs with minimal overhead. This\ncapability accelerates the design and implementation of new high-performance\ngraph algorithms and their evaluation on a wide range of graphs and across\ndifferent frameworks. However, our previous study identified two major\nlimitations in ParaGrapher: inefficient utilization of high-bandwidth storage\nand reduced decompression bandwidth due to increased compression ratios. To\naddress these limitations, we present two optimizations for ParaGrapher in this\npaper. To improve storage utilization, particularly for high-bandwidth storage,\nwe introduce ParaGrapher-FUSE (PG-Fuse) a filesystem based on the FUSE\n(Filesystem in User Space). PG-Fuse optimizes storage access by increasing the\nsize of requested blocks, reducing the number of calls to the underlying\nfilesystem, and caching the received blocks in memory for future calls. To\nimprove the decompression bandwidth, we introduce CompBin, a compact binary\nrepresentation of the CSR format. CompBin facilitates direct accesses to\nneighbors while preventing storage usage for unused bytes. Our evaluation on 12\nreal-world and synthetic graphs with up to 128 billion edges shows that PG-Fuse\nand CompBin achieve up to 7.6 and 21.8 times speedup, respectively.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ParaGrapher is a graph loading API and library that enables graph processing\nframeworks to load large-scale compressed graphs with minimal overhead. This\ncapability accelerates the design and implementation of new high-performance\ngraph algorithms and their evaluation on a wide range of graphs and across\ndifferent frameworks. However, our previous study identified two major\nlimitations in ParaGrapher: inefficient utilization of high-bandwidth storage\nand reduced decompression bandwidth due to increased compression ratios. To\naddress these limitations, we present two optimizations for ParaGrapher in this\npaper. To improve storage utilization, particularly for high-bandwidth storage,\nwe introduce ParaGrapher-FUSE (PG-Fuse) a filesystem based on the FUSE\n(Filesystem in User Space). PG-Fuse optimizes storage access by increasing the\nsize of requested blocks, reducing the number of calls to the underlying\nfilesystem, and caching the received blocks in memory for future calls. To\nimprove the decompression bandwidth, we introduce CompBin, a compact binary\nrepresentation of the CSR format. CompBin facilitates direct accesses to\nneighbors while preventing storage usage for unused bytes. Our evaluation on 12\nreal-world and synthetic graphs with up to 128 billion edges shows that PG-Fuse\nand CompBin achieve up to 7.6 and 21.8 times speedup, respectively."
                },
                "authors": [
                    {
                        "name": "Mohsen Koohi Esfahani"
                    }
                ],
                "author_detail": {
                    "name": "Mohsen Koohi Esfahani"
                },
                "author": "Mohsen Koohi Esfahani",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.00716v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.00716v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.00715v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.00715v1",
                "updated": "2025-07-01T12:42:06Z",
                "updated_parsed": [
                    2025,
                    7,
                    1,
                    12,
                    42,
                    6,
                    1,
                    182,
                    0
                ],
                "published": "2025-07-01T12:42:06Z",
                "published_parsed": [
                    2025,
                    7,
                    1,
                    12,
                    42,
                    6,
                    1,
                    182,
                    0
                ],
                "title": "EARN: Efficient Inference Acceleration for LLM-based Generative\n  Recommendation by Register Tokens",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "EARN: Efficient Inference Acceleration for LLM-based Generative\n  Recommendation by Register Tokens"
                },
                "summary": "Large Language Model-based generative recommendation (LLMRec) has achieved\nnotable success, but it suffers from high inference latency due to massive\ncomputational overhead and memory pressure of KV Cache. Existing KV Cache\nreduction methods face critical limitations: cache compression offers marginal\nacceleration given recommendation tasks' short decoding steps, while prompt\ncompression risks discarding vital interaction history. Through systematic\nanalysis of attention patterns in LLMRec, we uncover two pivotal insights: 1)\nlayer-wise attention sparsity inversion where early layers retain dense\ninformative patterns while later layers exhibit high redundancy, and 2) dual\nattention sinks phenomenon where attention scores concentrate on both head and\ntail tokens of input sequences. Motivated by these insights, we propose EARN,\nan efficient inference framework that leverages the early layers to compress\ninformation into register tokens placed at the input sequence boundaries, then\nfocuses solely on these tokens in the subsequent layers. Extensive experiments\non three datasets, two LLMRec methods and two LLM architectures demonstrate\nEARN's superiority, achieving up to 3.79x speedup and 80.8% KV Cache reduction\nwith better accuracy than the general finetuning approach. Our work bridges the\nefficiency-effectiveness gap in LLMRec, offering practical deployment\nadvantages for industrial scenarios.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Model-based generative recommendation (LLMRec) has achieved\nnotable success, but it suffers from high inference latency due to massive\ncomputational overhead and memory pressure of KV Cache. Existing KV Cache\nreduction methods face critical limitations: cache compression offers marginal\nacceleration given recommendation tasks' short decoding steps, while prompt\ncompression risks discarding vital interaction history. Through systematic\nanalysis of attention patterns in LLMRec, we uncover two pivotal insights: 1)\nlayer-wise attention sparsity inversion where early layers retain dense\ninformative patterns while later layers exhibit high redundancy, and 2) dual\nattention sinks phenomenon where attention scores concentrate on both head and\ntail tokens of input sequences. Motivated by these insights, we propose EARN,\nan efficient inference framework that leverages the early layers to compress\ninformation into register tokens placed at the input sequence boundaries, then\nfocuses solely on these tokens in the subsequent layers. Extensive experiments\non three datasets, two LLMRec methods and two LLM architectures demonstrate\nEARN's superiority, achieving up to 3.79x speedup and 80.8% KV Cache reduction\nwith better accuracy than the general finetuning approach. Our work bridges the\nefficiency-effectiveness gap in LLMRec, offering practical deployment\nadvantages for industrial scenarios."
                },
                "authors": [
                    {
                        "name": "Chaoqun Yang"
                    },
                    {
                        "name": "Xinyu Lin"
                    },
                    {
                        "name": "Wenjie Wang"
                    },
                    {
                        "name": "Yongqi Li"
                    },
                    {
                        "name": "Teng Sun"
                    },
                    {
                        "name": "Xianjing Han"
                    },
                    {
                        "name": "Tat-Seng Chua"
                    }
                ],
                "author_detail": {
                    "name": "Tat-Seng Chua"
                },
                "author": "Tat-Seng Chua",
                "arxiv_comment": "Accepted by KDD 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.00715v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.00715v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.00614v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.00614v1",
                "updated": "2025-07-01T09:47:38Z",
                "updated_parsed": [
                    2025,
                    7,
                    1,
                    9,
                    47,
                    38,
                    1,
                    182,
                    0
                ],
                "published": "2025-07-01T09:47:38Z",
                "published_parsed": [
                    2025,
                    7,
                    1,
                    9,
                    47,
                    38,
                    1,
                    182,
                    0
                ],
                "title": "Structural, dielectric, and ferroelectric characteristics of the\n  low-temperature sintered 65PMN-35PT sample for electroceramic applications",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Structural, dielectric, and ferroelectric characteristics of the\n  low-temperature sintered 65PMN-35PT sample for electroceramic applications"
                },
                "summary": "A single-phase 65PMN-35PT ceramic was synthesized at a relatively low\ntemperature (875 oC) using a modified columbite method. X-ray diffraction\nanalysis confirmed the single-phase formation of perovskite 65PMN-35PT with a\ntetragonal structure. Morphological studies indicated that the sample consisted\nof small grains with a size of about 2 micro-m. The dielectric properties of\nthe material demonstrate its relaxor behavior near the ferroelectric transition\ntemperature, TC = 457 K. The saturation and remnant polarization values of\napproximately 25.9 and 20.1 micro-C cm-2 were achieved for an electrically\npoled sample. Additionally, the poling induced a negative internal electric\nfield of about -0.2 kV cm-1 was detected due to the presence of ferroelectric\nnano-grains in this bulk 65PMN-35PT sample. These observed characteristics of\nthe pyrochlore-free 65PMN-35PT ceramic are similar to those of its\nsingle-crystal counterpart.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A single-phase 65PMN-35PT ceramic was synthesized at a relatively low\ntemperature (875 oC) using a modified columbite method. X-ray diffraction\nanalysis confirmed the single-phase formation of perovskite 65PMN-35PT with a\ntetragonal structure. Morphological studies indicated that the sample consisted\nof small grains with a size of about 2 micro-m. The dielectric properties of\nthe material demonstrate its relaxor behavior near the ferroelectric transition\ntemperature, TC = 457 K. The saturation and remnant polarization values of\napproximately 25.9 and 20.1 micro-C cm-2 were achieved for an electrically\npoled sample. Additionally, the poling induced a negative internal electric\nfield of about -0.2 kV cm-1 was detected due to the presence of ferroelectric\nnano-grains in this bulk 65PMN-35PT sample. These observed characteristics of\nthe pyrochlore-free 65PMN-35PT ceramic are similar to those of its\nsingle-crystal counterpart."
                },
                "authors": [
                    {
                        "name": "B. Ramachandran"
                    },
                    {
                        "name": "N. Sudarshan"
                    },
                    {
                        "name": "G. Mangamma"
                    },
                    {
                        "name": "M. S. Ramachandra Rao"
                    }
                ],
                "author_detail": {
                    "name": "M. S. Ramachandra Rao"
                },
                "author": "M. S. Ramachandra Rao",
                "arxiv_doi": "10.1007/s10832-025-00423-y",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1007/s10832-025-00423-y",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2507.00614v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.00614v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "9 pages, 7 figures, 1 Table and Accepted for publication in Journal\n  of Electroceramics",
                "arxiv_primary_category": {
                    "term": "cond-mat.mtrl-sci",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cond-mat.mtrl-sci",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.00462v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.00462v1",
                "updated": "2025-07-01T06:22:00Z",
                "updated_parsed": [
                    2025,
                    7,
                    1,
                    6,
                    22,
                    0,
                    1,
                    182,
                    0
                ],
                "published": "2025-07-01T06:22:00Z",
                "published_parsed": [
                    2025,
                    7,
                    1,
                    6,
                    22,
                    0,
                    1,
                    182,
                    0
                ],
                "title": "Unleashing the Potential of All Test Samples: Mean-Shift Guided\n  Test-Time Adaptation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Unleashing the Potential of All Test Samples: Mean-Shift Guided\n  Test-Time Adaptation"
                },
                "summary": "Visual-language models (VLMs) like CLIP exhibit strong generalization but\nstruggle with distribution shifts at test time. Existing training-free\ntest-time adaptation (TTA) methods operate strictly within CLIP's original\nfeature space, relying on high-confidence samples while overlooking the\npotential of low-confidence ones. We propose MS-TTA, a training-free approach\nthat enhances feature representations beyond CLIP's space using a single-step\nk-nearest neighbors (kNN) Mean-Shift. By refining all test samples, MS-TTA\nimproves feature compactness and class separability, leading to more stable\nadaptation. Additionally, a cache of refined embeddings further enhances\ninference by providing Mean Shift enhanced logits. Extensive evaluations on OOD\nand cross-dataset benchmarks demonstrate that MS-TTA consistently outperforms\nstate-of-the-art training-free TTA methods, achieving robust adaptation without\nrequiring additional training.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Visual-language models (VLMs) like CLIP exhibit strong generalization but\nstruggle with distribution shifts at test time. Existing training-free\ntest-time adaptation (TTA) methods operate strictly within CLIP's original\nfeature space, relying on high-confidence samples while overlooking the\npotential of low-confidence ones. We propose MS-TTA, a training-free approach\nthat enhances feature representations beyond CLIP's space using a single-step\nk-nearest neighbors (kNN) Mean-Shift. By refining all test samples, MS-TTA\nimproves feature compactness and class separability, leading to more stable\nadaptation. Additionally, a cache of refined embeddings further enhances\ninference by providing Mean Shift enhanced logits. Extensive evaluations on OOD\nand cross-dataset benchmarks demonstrate that MS-TTA consistently outperforms\nstate-of-the-art training-free TTA methods, achieving robust adaptation without\nrequiring additional training."
                },
                "authors": [
                    {
                        "name": "Jizhou Han"
                    },
                    {
                        "name": "Chenhao Ding"
                    },
                    {
                        "name": "SongLin Dong"
                    },
                    {
                        "name": "Yuhang He"
                    },
                    {
                        "name": "Xinyuan Gao"
                    },
                    {
                        "name": "Yihong Gong"
                    }
                ],
                "author_detail": {
                    "name": "Yihong Gong"
                },
                "author": "Yihong Gong",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.00462v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.00462v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.12036v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.12036v3",
                "updated": "2025-07-01T05:46:31Z",
                "updated_parsed": [
                    2025,
                    7,
                    1,
                    5,
                    46,
                    31,
                    1,
                    182,
                    0
                ],
                "published": "2025-05-23T00:01:52Z",
                "published_parsed": [
                    2025,
                    5,
                    23,
                    0,
                    1,
                    52,
                    4,
                    143,
                    0
                ],
                "title": "A Minimalist Method for Fine-tuning Text-to-Image Diffusion Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Minimalist Method for Fine-tuning Text-to-Image Diffusion Models"
                },
                "summary": "Recent work uses reinforcement learning (RL) to fine-tune text-to-image\ndiffusion models, improving text-image alignment and sample quality. However,\nexisting approaches introduce unnecessary complexity: they cache the full\nsampling trajectory, depend on differentiable reward models or large preference\ndatasets, or require specialized guidance techniques. Motivated by the \"golden\nnoise\" hypothesis -- that certain initial noise samples can consistently yield\nsuperior alignment -- we introduce Noise PPO, a minimalist RL algorithm that\nleaves the pre-trained diffusion model entirely frozen and learns a\nprompt-conditioned initial noise generator. Our approach requires no trajectory\nstorage, reward backpropagation, or complex guidance tricks. Extensive\nexperiments show that optimizing the initial noise distribution consistently\nimproves alignment and sample quality over the original model, with the most\nsignificant gains at low inference steps. As the number of inference steps\nincreases, the benefit of noise optimization diminishes but remains present.\nThese findings clarify the scope and limitations of the golden noise hypothesis\nand reinforce the practical value of minimalist RL fine-tuning for diffusion\nmodels.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent work uses reinforcement learning (RL) to fine-tune text-to-image\ndiffusion models, improving text-image alignment and sample quality. However,\nexisting approaches introduce unnecessary complexity: they cache the full\nsampling trajectory, depend on differentiable reward models or large preference\ndatasets, or require specialized guidance techniques. Motivated by the \"golden\nnoise\" hypothesis -- that certain initial noise samples can consistently yield\nsuperior alignment -- we introduce Noise PPO, a minimalist RL algorithm that\nleaves the pre-trained diffusion model entirely frozen and learns a\nprompt-conditioned initial noise generator. Our approach requires no trajectory\nstorage, reward backpropagation, or complex guidance tricks. Extensive\nexperiments show that optimizing the initial noise distribution consistently\nimproves alignment and sample quality over the original model, with the most\nsignificant gains at low inference steps. As the number of inference steps\nincreases, the benefit of noise optimization diminishes but remains present.\nThese findings clarify the scope and limitations of the golden noise hypothesis\nand reinforce the practical value of minimalist RL fine-tuning for diffusion\nmodels."
                },
                "authors": [
                    {
                        "name": "Yanting Miao"
                    },
                    {
                        "name": "William Loh"
                    },
                    {
                        "name": "Pacal Poupart"
                    },
                    {
                        "name": "Suraj Kothawade"
                    }
                ],
                "author_detail": {
                    "name": "Suraj Kothawade"
                },
                "author": "Suraj Kothawade",
                "arxiv_comment": "17 pages, 6 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.12036v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.12036v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.14051v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.14051v2",
                "updated": "2025-06-30T19:01:18Z",
                "updated_parsed": [
                    2025,
                    6,
                    30,
                    19,
                    1,
                    18,
                    0,
                    181,
                    0
                ],
                "published": "2025-02-19T19:12:46Z",
                "published_parsed": [
                    2025,
                    2,
                    19,
                    19,
                    12,
                    46,
                    2,
                    50,
                    0
                ],
                "title": "RocketKV: Accelerating Long-Context LLM Inference via Two-Stage KV Cache\n  Compression",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RocketKV: Accelerating Long-Context LLM Inference via Two-Stage KV Cache\n  Compression"
                },
                "summary": "Transformer-based Large Language Models rely critically on the KV cache to\nefficiently handle extended contexts during the decode phase. Yet, the size of\nthe KV cache grows proportionally with the input length, burdening both memory\nbandwidth and capacity as decoding progresses. To address this challenge, we\npresent RocketKV, a training-free KV cache compression strategy containing two\nconsecutive stages. In the first stage, it performs coarse-grain permanent KV\ncache eviction on the input sequence tokens. In the second stage, it adopts a\nhybrid sparse attention method to conduct fine-grain top-k sparse attention,\napproximating the attention scores by leveraging both head and sequence\ndimensionality reductions. We show that RocketKV provides a compression ratio\nof up to 400$\\times$, end-to-end speedup of up to 3.7$\\times$ as well as peak\nmemory reduction of up to 32.6% in the decode phase on an NVIDIA A100 GPU\ncompared to the full KV cache baseline, while achieving negligible accuracy\nloss on a variety of long-context tasks. We also propose a variant of RocketKV\nfor multi-turn scenarios, which consistently outperforms other existing methods\nand achieves accuracy nearly on par with an oracle top-k attention scheme.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Transformer-based Large Language Models rely critically on the KV cache to\nefficiently handle extended contexts during the decode phase. Yet, the size of\nthe KV cache grows proportionally with the input length, burdening both memory\nbandwidth and capacity as decoding progresses. To address this challenge, we\npresent RocketKV, a training-free KV cache compression strategy containing two\nconsecutive stages. In the first stage, it performs coarse-grain permanent KV\ncache eviction on the input sequence tokens. In the second stage, it adopts a\nhybrid sparse attention method to conduct fine-grain top-k sparse attention,\napproximating the attention scores by leveraging both head and sequence\ndimensionality reductions. We show that RocketKV provides a compression ratio\nof up to 400$\\times$, end-to-end speedup of up to 3.7$\\times$ as well as peak\nmemory reduction of up to 32.6% in the decode phase on an NVIDIA A100 GPU\ncompared to the full KV cache baseline, while achieving negligible accuracy\nloss on a variety of long-context tasks. We also propose a variant of RocketKV\nfor multi-turn scenarios, which consistently outperforms other existing methods\nand achieves accuracy nearly on par with an oracle top-k attention scheme."
                },
                "authors": [
                    {
                        "name": "Payman Behnam"
                    },
                    {
                        "name": "Yaosheng Fu"
                    },
                    {
                        "name": "Ritchie Zhao"
                    },
                    {
                        "name": "Po-An Tsai"
                    },
                    {
                        "name": "Zhiding Yu"
                    },
                    {
                        "name": "Alexey Tumanov"
                    }
                ],
                "author_detail": {
                    "name": "Alexey Tumanov"
                },
                "author": "Alexey Tumanov",
                "arxiv_comment": "ICML 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.14051v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.14051v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.24060v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.24060v1",
                "updated": "2025-06-30T17:07:59Z",
                "updated_parsed": [
                    2025,
                    6,
                    30,
                    17,
                    7,
                    59,
                    0,
                    181,
                    0
                ],
                "published": "2025-06-30T17:07:59Z",
                "published_parsed": [
                    2025,
                    6,
                    30,
                    17,
                    7,
                    59,
                    0,
                    181,
                    0
                ],
                "title": "Combinatorial Multi-Access Coded Caching with Private Caches under\n  Intersecting Index Constraints",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Combinatorial Multi-Access Coded Caching with Private Caches under\n  Intersecting Index Constraints"
                },
                "summary": "We consider the coded caching system where each user, equipped with a private\ncache, accesses a distinct r-subset of access caches. A central server housing\na library of files populates both private and access caches using uncoded\nplacement. In this work, we focus on a constrained indexing regime, referred to\nas the intersection class, in which the sets used to index the demands of each\nuser must have a nonempty intersection. This regime models resource-limited IoT\nscenarios such as edge-assisted IoT systems, where devices with small private\ncaches connect to a small number of shared caches. We provide a necessary and\nsufficient condition under which the system parameters fall within this\nintersection class. Under this condition, we propose a centralized coded\ncaching scheme and characterize its rate-memory trade-off. Next, we define a\nuniform-intersection subclass and establish a condition under which the system\nbelongs to this subclass. Within this subclass, the proposed scheme has a\nregular structure, with each transmission benefiting the same number of users,\nand we characterize its rate-memory trade-off. Additionally, we derive an index\ncoding-based lower bound on the minimum achievable worst-case rate under\nuncoded placement. Finally, we provide numerical comparisons between the rate\nof the proposed scheme, the new lower bound, and bounds from the original work.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We consider the coded caching system where each user, equipped with a private\ncache, accesses a distinct r-subset of access caches. A central server housing\na library of files populates both private and access caches using uncoded\nplacement. In this work, we focus on a constrained indexing regime, referred to\nas the intersection class, in which the sets used to index the demands of each\nuser must have a nonempty intersection. This regime models resource-limited IoT\nscenarios such as edge-assisted IoT systems, where devices with small private\ncaches connect to a small number of shared caches. We provide a necessary and\nsufficient condition under which the system parameters fall within this\nintersection class. Under this condition, we propose a centralized coded\ncaching scheme and characterize its rate-memory trade-off. Next, we define a\nuniform-intersection subclass and establish a condition under which the system\nbelongs to this subclass. Within this subclass, the proposed scheme has a\nregular structure, with each transmission benefiting the same number of users,\nand we characterize its rate-memory trade-off. Additionally, we derive an index\ncoding-based lower bound on the minimum achievable worst-case rate under\nuncoded placement. Finally, we provide numerical comparisons between the rate\nof the proposed scheme, the new lower bound, and bounds from the original work."
                },
                "authors": [
                    {
                        "name": "Dhruv Pratap Singh"
                    },
                    {
                        "name": "Anjana A. Mahesh"
                    },
                    {
                        "name": "B. Sundar Rajan"
                    }
                ],
                "author_detail": {
                    "name": "B. Sundar Rajan"
                },
                "author": "B. Sundar Rajan",
                "arxiv_comment": "9 pages and 3 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.24060v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.24060v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IT",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.05118v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.05118v4",
                "updated": "2025-06-30T16:23:35Z",
                "updated_parsed": [
                    2025,
                    6,
                    30,
                    16,
                    23,
                    35,
                    0,
                    181,
                    0
                ],
                "published": "2024-05-08T15:16:02Z",
                "published_parsed": [
                    2024,
                    5,
                    8,
                    15,
                    16,
                    2,
                    2,
                    129,
                    0
                ],
                "title": "Full Version: (De/Re)-Composition of Data-Parallel Computations via\n  Multi-Dimensional Homomorphisms",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Full Version: (De/Re)-Composition of Data-Parallel Computations via\n  Multi-Dimensional Homomorphisms"
                },
                "summary": "We formally introduce a systematic (de/re)-composition approach, based on the\nalgebraic formalism of \"Multi-Dimensional Homomorphisms (MDHs)\". Our approach\nis designed as general enough to be applicable to a wide range of data-parallel\ncomputations and for various kinds of target parallel architectures. To\nefficiently target the deep and complex memory and core hierarchies of\ncontemporary architectures, we exploit our introduced (de/re)-composition\napproach for a correct-by-construction, parametrized cache blocking and\nparallelization strategy. We show that our approach is powerful enough to\nexpress, in the same formalism, the (de/re)-composition strategies of different\nclasses of state-of-the-art approaches (scheduling-based, polyhedral, etc), and\nwe demonstrate that the parameters of our strategies enable systematically\ngenerating code that can be fully automatically optimized (auto-tuned) for the\nparticular target architecture and characteristics of the input and output data\n(e.g., their sizes and memory layouts). Particularly, our experiments confirm\nthat via auto-tuning, we achieve higher performance than state-of-the-art\napproaches, including hand-optimized solutions provided by vendors (such as\nNVIDIA cuBLAS/cuDNN and Intel oneMKL/oneDNN), on real-world data sets and for a\nvariety of data-parallel computations, including: linear algebra routines,\nstencil and quantum chemistry computations, data mining algorithms, and\ncomputations that recently gained high attention due to their relevance for\ndeep learning.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We formally introduce a systematic (de/re)-composition approach, based on the\nalgebraic formalism of \"Multi-Dimensional Homomorphisms (MDHs)\". Our approach\nis designed as general enough to be applicable to a wide range of data-parallel\ncomputations and for various kinds of target parallel architectures. To\nefficiently target the deep and complex memory and core hierarchies of\ncontemporary architectures, we exploit our introduced (de/re)-composition\napproach for a correct-by-construction, parametrized cache blocking and\nparallelization strategy. We show that our approach is powerful enough to\nexpress, in the same formalism, the (de/re)-composition strategies of different\nclasses of state-of-the-art approaches (scheduling-based, polyhedral, etc), and\nwe demonstrate that the parameters of our strategies enable systematically\ngenerating code that can be fully automatically optimized (auto-tuned) for the\nparticular target architecture and characteristics of the input and output data\n(e.g., their sizes and memory layouts). Particularly, our experiments confirm\nthat via auto-tuning, we achieve higher performance than state-of-the-art\napproaches, including hand-optimized solutions provided by vendors (such as\nNVIDIA cuBLAS/cuDNN and Intel oneMKL/oneDNN), on real-world data sets and for a\nvariety of data-parallel computations, including: linear algebra routines,\nstencil and quantum chemistry computations, data mining algorithms, and\ncomputations that recently gained high attention due to their relevance for\ndeep learning."
                },
                "authors": [
                    {
                        "name": "Ari Rasch"
                    }
                ],
                "author_detail": {
                    "name": "Ari Rasch"
                },
                "author": "Ari Rasch",
                "arxiv_doi": "10.1145/3665643",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3665643",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2405.05118v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.05118v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "A short version of this paper is published at ACM TOPLAS and\n  presented at PLDI'24",
                "arxiv_journal_ref": "ACM Trans. Program. Lang. Syst. (May 2024)",
                "arxiv_primary_category": {
                    "term": "cs.PL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.PL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.23809v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.23809v1",
                "updated": "2025-06-30T12:55:59Z",
                "updated_parsed": [
                    2025,
                    6,
                    30,
                    12,
                    55,
                    59,
                    0,
                    181,
                    0
                ],
                "published": "2025-06-30T12:55:59Z",
                "published_parsed": [
                    2025,
                    6,
                    30,
                    12,
                    55,
                    59,
                    0,
                    181,
                    0
                ],
                "title": "Large-scale Neural Network Quantum States for ab initio Quantum\n  Chemistry Simulations on Fugaku",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large-scale Neural Network Quantum States for ab initio Quantum\n  Chemistry Simulations on Fugaku"
                },
                "summary": "Solving quantum many-body problems is one of the fundamental challenges in\nquantum chemistry. While neural network quantum states (NQS) have emerged as a\npromising computational tool, its training process incurs exponentially growing\ncomputational demands, becoming prohibitively expensive for large-scale\nmolecular systems and creating fundamental scalability barriers for real-world\napplications. To address above challenges, we present \\ours, a high-performance\nNQS training framework for \\textit{ab initio} electronic structure\ncalculations. First, we propose a scalable sampling parallelism strategy with\nmulti-layers workload division and hybrid sampling scheme, which break the\nscalability barriers for large-scale NQS training. Then, we introduce\nmulti-level parallelism local energy parallelism, enabling more efficient local\nenergy computation. Last, we employ cache-centric optimization for\ntransformer-based \\textit{ansatz} and incorporate it with sampling parallelism\nstrategy, which further speedup up the NQS training and achieve stable memory\nfootprint at scale. Experiments demonstrate that \\ours accelerate NQS training\nwith up to 8.41x speedup and attains a parallel efficiency up to 95.8\\% when\nscaling to 1,536 nodes.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Solving quantum many-body problems is one of the fundamental challenges in\nquantum chemistry. While neural network quantum states (NQS) have emerged as a\npromising computational tool, its training process incurs exponentially growing\ncomputational demands, becoming prohibitively expensive for large-scale\nmolecular systems and creating fundamental scalability barriers for real-world\napplications. To address above challenges, we present \\ours, a high-performance\nNQS training framework for \\textit{ab initio} electronic structure\ncalculations. First, we propose a scalable sampling parallelism strategy with\nmulti-layers workload division and hybrid sampling scheme, which break the\nscalability barriers for large-scale NQS training. Then, we introduce\nmulti-level parallelism local energy parallelism, enabling more efficient local\nenergy computation. Last, we employ cache-centric optimization for\ntransformer-based \\textit{ansatz} and incorporate it with sampling parallelism\nstrategy, which further speedup up the NQS training and achieve stable memory\nfootprint at scale. Experiments demonstrate that \\ours accelerate NQS training\nwith up to 8.41x speedup and attains a parallel efficiency up to 95.8\\% when\nscaling to 1,536 nodes."
                },
                "authors": [
                    {
                        "name": "Hongtao Xu"
                    },
                    {
                        "name": "Zibo Wu"
                    },
                    {
                        "name": "Mingzhen Li"
                    },
                    {
                        "name": "Weile Jia"
                    }
                ],
                "author_detail": {
                    "name": "Weile Jia"
                },
                "author": "Weile Jia",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.23809v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.23809v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.02236v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.02236v2",
                "updated": "2025-06-30T05:54:40Z",
                "updated_parsed": [
                    2025,
                    6,
                    30,
                    5,
                    54,
                    40,
                    0,
                    181,
                    0
                ],
                "published": "2025-03-04T03:18:56Z",
                "published_parsed": [
                    2025,
                    3,
                    4,
                    3,
                    18,
                    56,
                    1,
                    63,
                    0
                ],
                "title": "VQ-LLM: High-performance Code Generation for Vector Quantization\n  Augmented LLM Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "VQ-LLM: High-performance Code Generation for Vector Quantization\n  Augmented LLM Inference"
                },
                "summary": "In this work, we design and implement VQ-LLM, an efficient fused Vector\nQuantization (VQ) kernel generation framework. We first introduce a software\nabstraction called codebook cache to optimize codebook access efficiency and\nsupport the integration of VQ with various computations. The codebook cache\nadaptively stores different entries across the GPU's memory hierarchy,\nincluding off-chip global memory, on-chip shared memory, and registers.\nCentered around the codebook cache, we design an efficient computation engine\nthat optimizes memory traffic during computations involving codebooks. This\ncompute engine adopts the codebook-centric dataflow and fusion optimizations.\nAdditionally, we provide adaptive heuristics to tailor parameter selection in\nour optimizations to diverse VQ configurations. Our optimizations achieve an\naverage latency reduction of 46.13% compared to unoptimized versions. Compared\nto existing open-source implementations, our methods decrease latency by 64.36%\nto 99.1%. A final comparison with state-of-the-art element-wise quantization\nmethods like AWQ and KVQuant shows that our VQ-LLM is practically viable,\nachieving latencies close or even better latencies to those at equivalent\nbit-widths, potentially offering greater accuracy.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this work, we design and implement VQ-LLM, an efficient fused Vector\nQuantization (VQ) kernel generation framework. We first introduce a software\nabstraction called codebook cache to optimize codebook access efficiency and\nsupport the integration of VQ with various computations. The codebook cache\nadaptively stores different entries across the GPU's memory hierarchy,\nincluding off-chip global memory, on-chip shared memory, and registers.\nCentered around the codebook cache, we design an efficient computation engine\nthat optimizes memory traffic during computations involving codebooks. This\ncompute engine adopts the codebook-centric dataflow and fusion optimizations.\nAdditionally, we provide adaptive heuristics to tailor parameter selection in\nour optimizations to diverse VQ configurations. Our optimizations achieve an\naverage latency reduction of 46.13% compared to unoptimized versions. Compared\nto existing open-source implementations, our methods decrease latency by 64.36%\nto 99.1%. A final comparison with state-of-the-art element-wise quantization\nmethods like AWQ and KVQuant shows that our VQ-LLM is practically viable,\nachieving latencies close or even better latencies to those at equivalent\nbit-widths, potentially offering greater accuracy."
                },
                "authors": [
                    {
                        "name": "Zihan Liu"
                    },
                    {
                        "name": "Xinhao Luo"
                    },
                    {
                        "name": "Junxian Guo"
                    },
                    {
                        "name": "Wentao Ni"
                    },
                    {
                        "name": "Yangjie Zhou"
                    },
                    {
                        "name": "Yue Guan"
                    },
                    {
                        "name": "Cong Guo"
                    },
                    {
                        "name": "Weihao Cui"
                    },
                    {
                        "name": "Yu Feng"
                    },
                    {
                        "name": "Minyi Guo"
                    },
                    {
                        "name": "Yuhao Zhu"
                    },
                    {
                        "name": "Minjia Zhang"
                    },
                    {
                        "name": "Jingwen Leng"
                    },
                    {
                        "name": "Chen Jin"
                    }
                ],
                "author_detail": {
                    "name": "Chen Jin"
                },
                "author": "Chen Jin",
                "arxiv_doi": "10.1109/HPCA61900.2025.00112",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1109/HPCA61900.2025.00112",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2503.02236v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.02236v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.12494v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.12494v2",
                "updated": "2025-06-30T05:45:43Z",
                "updated_parsed": [
                    2025,
                    6,
                    30,
                    5,
                    45,
                    43,
                    0,
                    181,
                    0
                ],
                "published": "2025-06-14T13:16:31Z",
                "published_parsed": [
                    2025,
                    6,
                    14,
                    13,
                    16,
                    31,
                    5,
                    165,
                    0
                ],
                "title": "FlexRAG: A Flexible and Comprehensive Framework for Retrieval-Augmented\n  Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FlexRAG: A Flexible and Comprehensive Framework for Retrieval-Augmented\n  Generation"
                },
                "summary": "Retrieval-Augmented Generation (RAG) plays a pivotal role in modern large\nlanguage model applications, with numerous existing frameworks offering a wide\nrange of functionalities to facilitate the development of RAG systems. However,\nwe have identified several persistent challenges in these frameworks, including\ndifficulties in algorithm reproduction and sharing, lack of new techniques, and\nhigh system overhead. To address these limitations, we introduce\n\\textbf{FlexRAG}, an open-source framework specifically designed for research\nand prototyping. FlexRAG supports text-based, multimodal, and network-based\nRAG, providing comprehensive lifecycle support alongside efficient asynchronous\nprocessing and persistent caching capabilities. By offering a robust and\nflexible solution, FlexRAG enables researchers to rapidly develop, deploy, and\nshare advanced RAG systems. Our toolkit and resources are available at\n\\href{https://github.com/ictnlp/FlexRAG}{https://github.com/ictnlp/FlexRAG}.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Retrieval-Augmented Generation (RAG) plays a pivotal role in modern large\nlanguage model applications, with numerous existing frameworks offering a wide\nrange of functionalities to facilitate the development of RAG systems. However,\nwe have identified several persistent challenges in these frameworks, including\ndifficulties in algorithm reproduction and sharing, lack of new techniques, and\nhigh system overhead. To address these limitations, we introduce\n\\textbf{FlexRAG}, an open-source framework specifically designed for research\nand prototyping. FlexRAG supports text-based, multimodal, and network-based\nRAG, providing comprehensive lifecycle support alongside efficient asynchronous\nprocessing and persistent caching capabilities. By offering a robust and\nflexible solution, FlexRAG enables researchers to rapidly develop, deploy, and\nshare advanced RAG systems. Our toolkit and resources are available at\n\\href{https://github.com/ictnlp/FlexRAG}{https://github.com/ictnlp/FlexRAG}."
                },
                "authors": [
                    {
                        "name": "Zhuocheng Zhang"
                    },
                    {
                        "name": "Yang Feng"
                    },
                    {
                        "name": "Min Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Min Zhang"
                },
                "author": "Min Zhang",
                "arxiv_comment": "Accepted by ACL 2025 Demo",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.12494v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.12494v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.02922v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.02922v2",
                "updated": "2025-06-30T05:21:58Z",
                "updated_parsed": [
                    2025,
                    6,
                    30,
                    5,
                    21,
                    58,
                    0,
                    181,
                    0
                ],
                "published": "2025-05-05T18:01:17Z",
                "published_parsed": [
                    2025,
                    5,
                    5,
                    18,
                    1,
                    17,
                    0,
                    125,
                    0
                ],
                "title": "RetroInfer: A Vector-Storage Approach for Scalable Long-Context LLM\n  Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RetroInfer: A Vector-Storage Approach for Scalable Long-Context LLM\n  Inference"
                },
                "summary": "The growing context lengths of large language models (LLMs) pose significant\nchallenges for efficient inference, primarily due to GPU memory and bandwidth\nconstraints. We present RetroInfer, a novel system that reconceptualizes the\nkey-value (KV) cache as a vector storage system which exploits the inherent\nattention sparsity to accelerate long-context LLM inference. At its core is the\nwave index, an Attention-aWare VEctor index that enables efficient and accurate\nretrieval of critical tokens through techniques such as tripartite attention\napproximation, accuracy-bounded attention estimation, and segmented clustering.\nComplementing this is the wave buffer, which coordinates KV cache placement and\noverlaps computation and data transfer across GPU and CPU to sustain high\nthroughput. Unlike prior sparsity-based methods that struggle with token\nselection and hardware coordination, RetroInfer delivers robust performance\nwithout compromising model accuracy. Experiments on long-context benchmarks\nshow up to 4.5X speedup over full attention within GPU memory limits and up to\n10.5X over sparse attention baselines when KV cache is extended to CPU memory,\nall while preserving full-attention-level accuracy.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The growing context lengths of large language models (LLMs) pose significant\nchallenges for efficient inference, primarily due to GPU memory and bandwidth\nconstraints. We present RetroInfer, a novel system that reconceptualizes the\nkey-value (KV) cache as a vector storage system which exploits the inherent\nattention sparsity to accelerate long-context LLM inference. At its core is the\nwave index, an Attention-aWare VEctor index that enables efficient and accurate\nretrieval of critical tokens through techniques such as tripartite attention\napproximation, accuracy-bounded attention estimation, and segmented clustering.\nComplementing this is the wave buffer, which coordinates KV cache placement and\noverlaps computation and data transfer across GPU and CPU to sustain high\nthroughput. Unlike prior sparsity-based methods that struggle with token\nselection and hardware coordination, RetroInfer delivers robust performance\nwithout compromising model accuracy. Experiments on long-context benchmarks\nshow up to 4.5X speedup over full attention within GPU memory limits and up to\n10.5X over sparse attention baselines when KV cache is extended to CPU memory,\nall while preserving full-attention-level accuracy."
                },
                "authors": [
                    {
                        "name": "Yaoqi Chen"
                    },
                    {
                        "name": "Jinkai Zhang"
                    },
                    {
                        "name": "Baotong Lu"
                    },
                    {
                        "name": "Qianxi Zhang"
                    },
                    {
                        "name": "Chengruidong Zhang"
                    },
                    {
                        "name": "Jingjia Luo"
                    },
                    {
                        "name": "Di Liu"
                    },
                    {
                        "name": "Huiqiang Jiang"
                    },
                    {
                        "name": "Qi Chen"
                    },
                    {
                        "name": "Jing Liu"
                    },
                    {
                        "name": "Bailu Ding"
                    },
                    {
                        "name": "Xiao Yan"
                    },
                    {
                        "name": "Jiawei Jiang"
                    },
                    {
                        "name": "Chen Chen"
                    },
                    {
                        "name": "Mingxing Zhang"
                    },
                    {
                        "name": "Yuqing Yang"
                    },
                    {
                        "name": "Fan Yang"
                    },
                    {
                        "name": "Mao Yang"
                    }
                ],
                "author_detail": {
                    "name": "Mao Yang"
                },
                "author": "Mao Yang",
                "arxiv_comment": "17 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.02922v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.02922v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.23488v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.23488v1",
                "updated": "2025-06-30T03:22:32Z",
                "updated_parsed": [
                    2025,
                    6,
                    30,
                    3,
                    22,
                    32,
                    0,
                    181,
                    0
                ],
                "published": "2025-06-30T03:22:32Z",
                "published_parsed": [
                    2025,
                    6,
                    30,
                    3,
                    22,
                    32,
                    0,
                    181,
                    0
                ],
                "title": "Generative AI-enhanced Low-Altitude UAV-Mounted Stacked Intelligent\n  Metasurfaces",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Generative AI-enhanced Low-Altitude UAV-Mounted Stacked Intelligent\n  Metasurfaces"
                },
                "summary": "Wireless communication systems face significant challenges in meeting the\nincreasing demands for higher data rates and more reliable connectivity in\ncomplex environments. Stacked intelligent metasurfaces (SIMs) have emerged as a\npromising technology for realizing wave-domain signal processing, with mobile\nSIMs offering superior communication performance compared to their fixed\ncounterparts. In this paper, we investigate a novel unmanned aerial vehicle\n(UAV)-mounted SIMs (UAV-SIMs) assisted communication system within the\nlow-altitude economy (LAE) networks paradigm, where UAVs function as both base\nstations that cache SIM-processed data and mobile platforms that flexibly\ndeploy SIMs to enhance uplink communications from ground users. To maximize\nnetwork capacity, we formulate a UAV-SIM-based joint optimization problem\n(USBJOP) that comprehensively addresses three critical aspects: the association\nbetween UAV-SIMs and users, the three-dimensional positioning of UAV-SIMs, and\nthe phase shifts across multiple SIM layers. Due to the inherent non-convexity\nand NP-hardness of USBJOP, we decompose it into three sub-optimization\nproblems, \\textit{i.e.}, association between UAV-SIMs and users optimization\nproblem (AUUOP), UAV location optimization problem (ULOP), and UAV-SIM phase\nshifts optimization problem (USPSOP), and solve them using an alternating\noptimization strategy. Specifically, we transform AUUOP and ULOP into convex\nforms solvable by the CVX tool, while addressing USPSOP through a generative\nartificial intelligence (GAI)-based hybrid optimization algorithm. Simulations\ndemonstrate that our proposed approach significantly outperforms benchmark\nschemes, achieving approximately 1.5 times higher network capacity compared to\nsuboptimal alternatives. Additionally, our proposed GAI method reduces the\nalgorithm runtime by 10\\% while maintaining solution quality.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Wireless communication systems face significant challenges in meeting the\nincreasing demands for higher data rates and more reliable connectivity in\ncomplex environments. Stacked intelligent metasurfaces (SIMs) have emerged as a\npromising technology for realizing wave-domain signal processing, with mobile\nSIMs offering superior communication performance compared to their fixed\ncounterparts. In this paper, we investigate a novel unmanned aerial vehicle\n(UAV)-mounted SIMs (UAV-SIMs) assisted communication system within the\nlow-altitude economy (LAE) networks paradigm, where UAVs function as both base\nstations that cache SIM-processed data and mobile platforms that flexibly\ndeploy SIMs to enhance uplink communications from ground users. To maximize\nnetwork capacity, we formulate a UAV-SIM-based joint optimization problem\n(USBJOP) that comprehensively addresses three critical aspects: the association\nbetween UAV-SIMs and users, the three-dimensional positioning of UAV-SIMs, and\nthe phase shifts across multiple SIM layers. Due to the inherent non-convexity\nand NP-hardness of USBJOP, we decompose it into three sub-optimization\nproblems, \\textit{i.e.}, association between UAV-SIMs and users optimization\nproblem (AUUOP), UAV location optimization problem (ULOP), and UAV-SIM phase\nshifts optimization problem (USPSOP), and solve them using an alternating\noptimization strategy. Specifically, we transform AUUOP and ULOP into convex\nforms solvable by the CVX tool, while addressing USPSOP through a generative\nartificial intelligence (GAI)-based hybrid optimization algorithm. Simulations\ndemonstrate that our proposed approach significantly outperforms benchmark\nschemes, achieving approximately 1.5 times higher network capacity compared to\nsuboptimal alternatives. Additionally, our proposed GAI method reduces the\nalgorithm runtime by 10\\% while maintaining solution quality."
                },
                "authors": [
                    {
                        "name": "Geng Sun"
                    },
                    {
                        "name": "Mingzhe Fan"
                    },
                    {
                        "name": "Lei Zhang"
                    },
                    {
                        "name": "Hongyang Pan"
                    },
                    {
                        "name": "Jiahui Li"
                    },
                    {
                        "name": "Chuang Zhang"
                    },
                    {
                        "name": "Linyao Li"
                    },
                    {
                        "name": "Changyuan Zhao"
                    },
                    {
                        "name": "Chau Yuen"
                    }
                ],
                "author_detail": {
                    "name": "Chau Yuen"
                },
                "author": "Chau Yuen",
                "arxiv_comment": "This paper has been already submitted to TCCN",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.23488v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.23488v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.23405v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.23405v1",
                "updated": "2025-06-29T21:55:58Z",
                "updated_parsed": [
                    2025,
                    6,
                    29,
                    21,
                    55,
                    58,
                    6,
                    180,
                    0
                ],
                "published": "2025-06-29T21:55:58Z",
                "published_parsed": [
                    2025,
                    6,
                    29,
                    21,
                    55,
                    58,
                    6,
                    180,
                    0
                ],
                "title": "CMOS+X: Stacking Persistent Embedded Memories based on Oxide Transistors\n  upon GPGPU Platforms",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CMOS+X: Stacking Persistent Embedded Memories based on Oxide Transistors\n  upon GPGPU Platforms"
                },
                "summary": "In contemporary general-purpose graphics processing units (GPGPUs), the\ncontinued increase in raw arithmetic throughput is constrained by the\ncapabilities of the register file (single-cycle) and last-level cache (high\nbandwidth), which require the delivery of operands at a cadence demanded by\nwide single-instruction multiple-data (SIMD) lanes. Enhancing the capacity,\ndensity, or bandwidth of these memories can unlock substantial performance\ngains; however, the recent stagnation of SRAM bit-cell scaling leads to\ninequivalent losses in compute density.\n  To address the challenges posed by SRAM's scaling and leakage power\nconsumption, this paper explores the potential CMOS+X integration of amorphous\noxide semiconductor (AOS) transistors in capacitive, persistent memory\ntopologies (e.g., 1T1C eDRAM, 2T0C/3T0C Gain Cell) as alternative cells in\nmulti-ported and high-bandwidth banked GPGPU memories. A detailed study of the\ndensity and energy tradeoffs of back-end-of-line (BEOL) integrated memories\nutilizing monolithic 3D (M3D)-integrated multiplexed arrays is conducted, while\naccounting for the macro-level limitations of integrating AOS candidate\nstructures proposed by the device community (an aspect often overlooked in\nprior work). By exploiting the short lifetime of register operands, we propose\na multi-ported AOS gain-cell capable of delivering 3x the read ports in ~76% of\nthe footprint of SRAM with over 70% lower standby power, enabling enhancements\nto compute capacity, such as larger warp sizes or processor counts. Benchmarks\nrun on a validated NVIDIA Ampere-class GPU model, using a modified version of\nAccel-Sim, demonstrate improvements of up to 5.2x the performance per watt and\nan average 8% higher geometric mean instruction per cycle (IPC) on various\ncompute- and memory-bound tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In contemporary general-purpose graphics processing units (GPGPUs), the\ncontinued increase in raw arithmetic throughput is constrained by the\ncapabilities of the register file (single-cycle) and last-level cache (high\nbandwidth), which require the delivery of operands at a cadence demanded by\nwide single-instruction multiple-data (SIMD) lanes. Enhancing the capacity,\ndensity, or bandwidth of these memories can unlock substantial performance\ngains; however, the recent stagnation of SRAM bit-cell scaling leads to\ninequivalent losses in compute density.\n  To address the challenges posed by SRAM's scaling and leakage power\nconsumption, this paper explores the potential CMOS+X integration of amorphous\noxide semiconductor (AOS) transistors in capacitive, persistent memory\ntopologies (e.g., 1T1C eDRAM, 2T0C/3T0C Gain Cell) as alternative cells in\nmulti-ported and high-bandwidth banked GPGPU memories. A detailed study of the\ndensity and energy tradeoffs of back-end-of-line (BEOL) integrated memories\nutilizing monolithic 3D (M3D)-integrated multiplexed arrays is conducted, while\naccounting for the macro-level limitations of integrating AOS candidate\nstructures proposed by the device community (an aspect often overlooked in\nprior work). By exploiting the short lifetime of register operands, we propose\na multi-ported AOS gain-cell capable of delivering 3x the read ports in ~76% of\nthe footprint of SRAM with over 70% lower standby power, enabling enhancements\nto compute capacity, such as larger warp sizes or processor counts. Benchmarks\nrun on a validated NVIDIA Ampere-class GPU model, using a modified version of\nAccel-Sim, demonstrate improvements of up to 5.2x the performance per watt and\nan average 8% higher geometric mean instruction per cycle (IPC) on various\ncompute- and memory-bound tasks."
                },
                "authors": [
                    {
                        "name": "Faaiq Waqar"
                    },
                    {
                        "name": "Ming-Yen Lee"
                    },
                    {
                        "name": "Seongwon Yoon"
                    },
                    {
                        "name": "Seongkwang Lim"
                    },
                    {
                        "name": "Shimeng Yu"
                    }
                ],
                "author_detail": {
                    "name": "Shimeng Yu"
                },
                "author": "Shimeng Yu",
                "arxiv_comment": "14 pages, 18 figures, 4 tables, 4 equations",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.23405v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.23405v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.ET",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.ET",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "B.8.2; B.3.1",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.01988v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.01988v1",
                "updated": "2025-06-28T13:02:17Z",
                "updated_parsed": [
                    2025,
                    6,
                    28,
                    13,
                    2,
                    17,
                    5,
                    179,
                    0
                ],
                "published": "2025-06-28T13:02:17Z",
                "published_parsed": [
                    2025,
                    6,
                    28,
                    13,
                    2,
                    17,
                    5,
                    179,
                    0
                ],
                "title": "Scaling Out Chip Interconnect Networks with Implicit Sequence Numbers",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Scaling Out Chip Interconnect Networks with Implicit Sequence Numbers"
                },
                "summary": "As AI models outpace the capabilities of single processors, interconnects\nacross chips have become a critical enabler for scalable computing. These\nprocessors exchange massive amounts of data at cache-line granularity,\nprompting the adoption of new interconnect protocols like CXL, NVLink, and\nUALink, designed for high bandwidth and small payloads. However, the increasing\ntransfer rates of these protocols heighten susceptibility to errors. While\nmechanisms like Cyclic Redundancy Check (CRC) and Forward Error Correction\n(FEC) are standard for reliable data transmission, scaling chip interconnects\nto multi-node configurations introduces new challenges, particularly in\nmanaging silently dropped flits in switching devices. This paper introduces\nImplicit Sequence Number (ISN), a novel mechanism that ensures precise flit\ndrop detection and in-order delivery without adding header overhead.\nAdditionally, we propose Reliability Extended Link (RXL), an extension of CXL\nthat incorporates ISN to support scalable, reliable multi-node interconnects\nwhile maintaining compatibility with the existing flit structure. By elevating\nCRC to a transport-layer mechanism for end-to-end data and sequence integrity,\nand relying on FEC for link-layer error correction and detection, RXL delivers\nrobust reliability and scalability without compromising bandwidth efficiency.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As AI models outpace the capabilities of single processors, interconnects\nacross chips have become a critical enabler for scalable computing. These\nprocessors exchange massive amounts of data at cache-line granularity,\nprompting the adoption of new interconnect protocols like CXL, NVLink, and\nUALink, designed for high bandwidth and small payloads. However, the increasing\ntransfer rates of these protocols heighten susceptibility to errors. While\nmechanisms like Cyclic Redundancy Check (CRC) and Forward Error Correction\n(FEC) are standard for reliable data transmission, scaling chip interconnects\nto multi-node configurations introduces new challenges, particularly in\nmanaging silently dropped flits in switching devices. This paper introduces\nImplicit Sequence Number (ISN), a novel mechanism that ensures precise flit\ndrop detection and in-order delivery without adding header overhead.\nAdditionally, we propose Reliability Extended Link (RXL), an extension of CXL\nthat incorporates ISN to support scalable, reliable multi-node interconnects\nwhile maintaining compatibility with the existing flit structure. By elevating\nCRC to a transport-layer mechanism for end-to-end data and sequence integrity,\nand relying on FEC for link-layer error correction and detection, RXL delivers\nrobust reliability and scalability without compromising bandwidth efficiency."
                },
                "authors": [
                    {
                        "name": "Giyong Jung"
                    },
                    {
                        "name": "Saeid Gorgin"
                    },
                    {
                        "name": "John Kim"
                    },
                    {
                        "name": "Jungrae Kim"
                    }
                ],
                "author_detail": {
                    "name": "Jungrae Kim"
                },
                "author": "Jungrae Kim",
                "arxiv_comment": "12 pages, 8 figures. This paper is accepted for [2025 The\n  International Conference for High Performance Computing, Networking, Storage\n  and Analysis (SC)]",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.01988v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.01988v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.22791v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.22791v1",
                "updated": "2025-06-28T07:25:12Z",
                "updated_parsed": [
                    2025,
                    6,
                    28,
                    7,
                    25,
                    12,
                    5,
                    179,
                    0
                ],
                "published": "2025-06-28T07:25:12Z",
                "published_parsed": [
                    2025,
                    6,
                    28,
                    7,
                    25,
                    12,
                    5,
                    179,
                    0
                ],
                "title": "ContextCache: Context-Aware Semantic Cache for Multi-Turn Queries in\n  Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ContextCache: Context-Aware Semantic Cache for Multi-Turn Queries in\n  Large Language Models"
                },
                "summary": "Semantic caching significantly reduces computational costs and improves\nefficiency by storing and reusing large language model (LLM) responses.\nHowever, existing systems rely primarily on matching individual queries,\nlacking awareness of multi-turn dialogue contexts, which leads to incorrect\ncache hits when similar queries appear in different conversational settings.\nThis demonstration introduces ContextCache, a context-aware semantic caching\nsystem for multi-turn dialogues. ContextCache employs a two-stage retrieval\narchitecture that first executes vector-based retrieval on the current query to\nidentify potential matches and then integrates current and historical dialogue\nrepresentations through self-attention mechanisms for precise contextual\nmatching. Evaluation of real-world conversations shows that ContextCache\nimproves precision and recall compared to existing methods. Additionally,\ncached responses exhibit approximately 10 times lower latency than direct LLM\ninvocation, enabling significant computational cost reductions for LLM\nconversational applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Semantic caching significantly reduces computational costs and improves\nefficiency by storing and reusing large language model (LLM) responses.\nHowever, existing systems rely primarily on matching individual queries,\nlacking awareness of multi-turn dialogue contexts, which leads to incorrect\ncache hits when similar queries appear in different conversational settings.\nThis demonstration introduces ContextCache, a context-aware semantic caching\nsystem for multi-turn dialogues. ContextCache employs a two-stage retrieval\narchitecture that first executes vector-based retrieval on the current query to\nidentify potential matches and then integrates current and historical dialogue\nrepresentations through self-attention mechanisms for precise contextual\nmatching. Evaluation of real-world conversations shows that ContextCache\nimproves precision and recall compared to existing methods. Additionally,\ncached responses exhibit approximately 10 times lower latency than direct LLM\ninvocation, enabling significant computational cost reductions for LLM\nconversational applications."
                },
                "authors": [
                    {
                        "name": "Jianxin Yan"
                    },
                    {
                        "name": "Wangze Ni"
                    },
                    {
                        "name": "Lei Chen"
                    },
                    {
                        "name": "Xuemin Lin"
                    },
                    {
                        "name": "Peng Cheng"
                    },
                    {
                        "name": "Zhan Qin"
                    },
                    {
                        "name": "Kui Ren"
                    }
                ],
                "author_detail": {
                    "name": "Kui Ren"
                },
                "author": "Kui Ren",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.22791v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.22791v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.12593v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.12593v4",
                "updated": "2025-06-28T06:24:44Z",
                "updated_parsed": [
                    2025,
                    6,
                    28,
                    6,
                    24,
                    44,
                    5,
                    179,
                    0
                ],
                "published": "2024-06-18T13:25:18Z",
                "published_parsed": [
                    2024,
                    6,
                    18,
                    13,
                    25,
                    18,
                    1,
                    170,
                    0
                ],
                "title": "PromptDSI: Prompt-based Rehearsal-free Continual Learning for Document\n  Retrieval",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PromptDSI: Prompt-based Rehearsal-free Continual Learning for Document\n  Retrieval"
                },
                "summary": "Differentiable Search Index (DSI) utilizes pre-trained language models to\nperform indexing and document retrieval via end-to-end learning without relying\non external indexes. However, DSI requires full re-training to index new\ndocuments, causing significant computational inefficiencies. Continual learning\n(CL) offers a solution by enabling the model to incrementally update without\nfull re-training. Existing CL solutions in document retrieval rely on memory\nbuffers or generative models for rehearsal, which is infeasible when accessing\nprevious training data is restricted due to privacy concerns. To this end, we\nintroduce PromptDSI, a prompt-based, rehearsal-free continual learning approach\nfor document retrieval. PromptDSI follows the Prompt-based Continual Learning\n(PCL) framework, using learnable prompts to efficiently index new documents\nwithout accessing previous documents or queries. To improve retrieval latency,\nwe remove the initial forward pass of PCL, which otherwise greatly increases\ntraining and inference time, with a negligible trade-off in performance.\nAdditionally, we introduce a novel topic-aware prompt pool that employs neural\ntopic embeddings as fixed keys, eliminating the instability of prompt key\noptimization while maintaining competitive performance with existing PCL prompt\npools. In a challenging rehearsal-free continual learning setup, we demonstrate\nthat PromptDSI variants outperform rehearsal-based baselines, match the strong\ncache-based baseline in mitigating forgetting, and significantly improving\nretrieval performance on new corpora.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Differentiable Search Index (DSI) utilizes pre-trained language models to\nperform indexing and document retrieval via end-to-end learning without relying\non external indexes. However, DSI requires full re-training to index new\ndocuments, causing significant computational inefficiencies. Continual learning\n(CL) offers a solution by enabling the model to incrementally update without\nfull re-training. Existing CL solutions in document retrieval rely on memory\nbuffers or generative models for rehearsal, which is infeasible when accessing\nprevious training data is restricted due to privacy concerns. To this end, we\nintroduce PromptDSI, a prompt-based, rehearsal-free continual learning approach\nfor document retrieval. PromptDSI follows the Prompt-based Continual Learning\n(PCL) framework, using learnable prompts to efficiently index new documents\nwithout accessing previous documents or queries. To improve retrieval latency,\nwe remove the initial forward pass of PCL, which otherwise greatly increases\ntraining and inference time, with a negligible trade-off in performance.\nAdditionally, we introduce a novel topic-aware prompt pool that employs neural\ntopic embeddings as fixed keys, eliminating the instability of prompt key\noptimization while maintaining competitive performance with existing PCL prompt\npools. In a challenging rehearsal-free continual learning setup, we demonstrate\nthat PromptDSI variants outperform rehearsal-based baselines, match the strong\ncache-based baseline in mitigating forgetting, and significantly improving\nretrieval performance on new corpora."
                },
                "authors": [
                    {
                        "name": "Tuan-Luc Huynh"
                    },
                    {
                        "name": "Thuy-Trang Vu"
                    },
                    {
                        "name": "Weiqing Wang"
                    },
                    {
                        "name": "Yinwei Wei"
                    },
                    {
                        "name": "Trung Le"
                    },
                    {
                        "name": "Dragan Gasevic"
                    },
                    {
                        "name": "Yuan-Fang Li"
                    },
                    {
                        "name": "Thanh-Toan Do"
                    }
                ],
                "author_detail": {
                    "name": "Thanh-Toan Do"
                },
                "author": "Thanh-Toan Do",
                "arxiv_comment": "ECML PKDD 2025 Research track. Camera-ready version. Code is\n  available at https://github.com/LouisDo2108/PromptDSI",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.12593v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.12593v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.05460v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.05460v4",
                "updated": "2025-06-28T03:53:17Z",
                "updated_parsed": [
                    2025,
                    6,
                    28,
                    3,
                    53,
                    17,
                    5,
                    179,
                    0
                ],
                "published": "2024-12-25T10:11:31Z",
                "published_parsed": [
                    2024,
                    12,
                    25,
                    10,
                    11,
                    31,
                    2,
                    360,
                    0
                ],
                "title": "Efficiently Serving Large Multimodal Models Using EPD Disaggregation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficiently Serving Large Multimodal Models Using EPD Disaggregation"
                },
                "summary": "Large Multimodal Models (LMMs) extend Large Language Models (LLMs) by\nhandling diverse inputs such as images, audio, and video, but at the cost of\nadding a multimodal encoding stage that increases both computational and memory\noverhead. This step negatively affects key Service Level Objectives (SLOs),\nsuch as time to first token (TTFT) and time per output token (TPOT). We\nintroduce Encode-Prefill-Decode (EPD) Disaggregation, a novel framework that\nseparates the encoding, prefill, and decode stages onto dedicated resources.\nUnlike current systems, which bundle encoding and prefill together, our\napproach decouples these steps, unlocking new opportunities and optimizations.\nThese include a mechanism to cache multimedia tokens for efficient transfer, a\nnovel way to parallelize the encoding load within a request, a module for\noptimal resource allocation for disaggregated serving, and a novel\nrole-switching method to handle changing workload characteristics. Experimental\nevaluations with popular LMMs show substantial gains in memory efficiency (up\nto 15x lower peak memory utilization), batch sizes (up to 22x larger), 10x more\nimages per request, and 2.2x larger KV caches. Furthermore, it leads to\nsignificant improvements in SLO attainment (up to 90-100% improvement) and TTFT\n(up to 71% reduction), compared to systems that do not disaggregate. The code\nis available at https://github.com/vbdi/epdserve.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Multimodal Models (LMMs) extend Large Language Models (LLMs) by\nhandling diverse inputs such as images, audio, and video, but at the cost of\nadding a multimodal encoding stage that increases both computational and memory\noverhead. This step negatively affects key Service Level Objectives (SLOs),\nsuch as time to first token (TTFT) and time per output token (TPOT). We\nintroduce Encode-Prefill-Decode (EPD) Disaggregation, a novel framework that\nseparates the encoding, prefill, and decode stages onto dedicated resources.\nUnlike current systems, which bundle encoding and prefill together, our\napproach decouples these steps, unlocking new opportunities and optimizations.\nThese include a mechanism to cache multimedia tokens for efficient transfer, a\nnovel way to parallelize the encoding load within a request, a module for\noptimal resource allocation for disaggregated serving, and a novel\nrole-switching method to handle changing workload characteristics. Experimental\nevaluations with popular LMMs show substantial gains in memory efficiency (up\nto 15x lower peak memory utilization), batch sizes (up to 22x larger), 10x more\nimages per request, and 2.2x larger KV caches. Furthermore, it leads to\nsignificant improvements in SLO attainment (up to 90-100% improvement) and TTFT\n(up to 71% reduction), compared to systems that do not disaggregate. The code\nis available at https://github.com/vbdi/epdserve."
                },
                "authors": [
                    {
                        "name": "Gursimran Singh"
                    },
                    {
                        "name": "Xinglu Wang"
                    },
                    {
                        "name": "Yifan Hu"
                    },
                    {
                        "name": "Timothy Yu"
                    },
                    {
                        "name": "Linzi Xing"
                    },
                    {
                        "name": "Wei Jiang"
                    },
                    {
                        "name": "Zhefeng Wang"
                    },
                    {
                        "name": "Xiaolong Bai"
                    },
                    {
                        "name": "Yi Li"
                    },
                    {
                        "name": "Ying Xiong"
                    },
                    {
                        "name": "Yong Zhang"
                    },
                    {
                        "name": "Zhenan Fan"
                    }
                ],
                "author_detail": {
                    "name": "Zhenan Fan"
                },
                "author": "Zhenan Fan",
                "arxiv_comment": "17 pages, 12 figures, 9 tables",
                "arxiv_journal_ref": "International Conference on Machine Proceedings of the 42nd\n  International Conference on Machine Learning, Vancouver, Canada. PMLR 267,\n  2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.05460v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.05460v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.22396v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.22396v1",
                "updated": "2025-06-27T17:10:32Z",
                "updated_parsed": [
                    2025,
                    6,
                    27,
                    17,
                    10,
                    32,
                    4,
                    178,
                    0
                ],
                "published": "2025-06-27T17:10:32Z",
                "published_parsed": [
                    2025,
                    6,
                    27,
                    17,
                    10,
                    32,
                    4,
                    178,
                    0
                ],
                "title": "QuickSilver -- Speeding up LLM Inference through Dynamic Token Halting,\n  KV Skipping, Contextual Token Fusion, and Adaptive Matryoshka Quantization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "QuickSilver -- Speeding up LLM Inference through Dynamic Token Halting,\n  KV Skipping, Contextual Token Fusion, and Adaptive Matryoshka Quantization"
                },
                "summary": "Inference accounts for the majority of latency and energy consumption in\nlarge language model (LLM) deployments, often exceeding 90% of total cost.\nWhile training-time efficiency has seen extensive progress, runtime\noptimization remains a key bottleneck, particularly under autoregressive\ndecoding. Existing approaches -- such as pruning, quantization, early exits,\nand speculative decoding -- often require retraining, architectural changes, or\ndisrupt decoding compatibility. We introduce QuickSilver, a modular,\ntoken-level framework that enables semantic adaptivity at inference time\nwithout altering model weights or structure. QuickSilver integrates four\nsynergistic mechanisms:\n  (i) Dynamic Token Halting, which halts computation for tokens with converged\nrepresentations; (ii) KV Cache Skipping, which selectively suppresses memory\nwrites to reduce attention overhead; and (iii) Contextual Token Fusion, which\ncollapses redundant tokens into shared paths to shrink sequence length.\n  Unlike speculative decoding or MoE routing, QuickSilver operates entirely on\nfrozen, dense models and requires no auxiliary networks. Applied to GPT-2 and\nLlama-2 across WikiText-103 and C4, QuickSilver achieves up to 39.6% FLOP\nreduction with negligible perplexity degradation (<=0.2).",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Inference accounts for the majority of latency and energy consumption in\nlarge language model (LLM) deployments, often exceeding 90% of total cost.\nWhile training-time efficiency has seen extensive progress, runtime\noptimization remains a key bottleneck, particularly under autoregressive\ndecoding. Existing approaches -- such as pruning, quantization, early exits,\nand speculative decoding -- often require retraining, architectural changes, or\ndisrupt decoding compatibility. We introduce QuickSilver, a modular,\ntoken-level framework that enables semantic adaptivity at inference time\nwithout altering model weights or structure. QuickSilver integrates four\nsynergistic mechanisms:\n  (i) Dynamic Token Halting, which halts computation for tokens with converged\nrepresentations; (ii) KV Cache Skipping, which selectively suppresses memory\nwrites to reduce attention overhead; and (iii) Contextual Token Fusion, which\ncollapses redundant tokens into shared paths to shrink sequence length.\n  Unlike speculative decoding or MoE routing, QuickSilver operates entirely on\nfrozen, dense models and requires no auxiliary networks. Applied to GPT-2 and\nLlama-2 across WikiText-103 and C4, QuickSilver achieves up to 39.6% FLOP\nreduction with negligible perplexity degradation (<=0.2)."
                },
                "authors": [
                    {
                        "name": "Danush Khanna"
                    },
                    {
                        "name": "Aditya Kumar Guru"
                    },
                    {
                        "name": "Srivarshinee Sridhar"
                    },
                    {
                        "name": "Zidan Ahmed"
                    },
                    {
                        "name": "Rubhav Bahirwani"
                    },
                    {
                        "name": "Meetu Malhotra"
                    },
                    {
                        "name": "Vinija Jain"
                    },
                    {
                        "name": "Aman Chadha"
                    },
                    {
                        "name": "Amitava Das"
                    },
                    {
                        "name": "Kripabandhu Ghosh"
                    }
                ],
                "author_detail": {
                    "name": "Kripabandhu Ghosh"
                },
                "author": "Kripabandhu Ghosh",
                "arxiv_comment": "Preprint. Under submission",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.22396v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.22396v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.0; I.2.7",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.22033v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.22033v1",
                "updated": "2025-06-27T09:27:04Z",
                "updated_parsed": [
                    2025,
                    6,
                    27,
                    9,
                    27,
                    4,
                    4,
                    178,
                    0
                ],
                "published": "2025-06-27T09:27:04Z",
                "published_parsed": [
                    2025,
                    6,
                    27,
                    9,
                    27,
                    4,
                    4,
                    178,
                    0
                ],
                "title": "SiPipe: Bridging the CPU-GPU Utilization Gap for Efficient\n  Pipeline-Parallel LLM Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SiPipe: Bridging the CPU-GPU Utilization Gap for Efficient\n  Pipeline-Parallel LLM Inference"
                },
                "summary": "As inference workloads for large language models (LLMs) scale to meet growing\nuser demand, pipeline parallelism (PP) has become a widely adopted strategy for\nmulti-GPU deployment, particularly in cross-node setups, to improve key-value\n(KV) cache capacity and inference throughput. However, PP suffers from inherent\ninefficiencies caused by three types of execution bubbles-load-imbalance,\nintra-stage, and inter-stage-which limit pipeline saturation. We present\nSiPipe, a heterogeneous pipeline design that improves throughput by leveraging\nunderutilized CPU resources to offload auxiliary computation and communication.\nSiPipe incorporates three key techniques-CPU sampling, a token-safe execution\nmodel, and structure-aware transmission-to mitigate pipeline bubbles and\nimprove execution efficiency. Across diverse LLMs, SiPipe achieves up to 2.1\ntimes higher throughput, 43% lower per-token latency, and up to 23% higher\naverage GPU utilization compared to the state-of-the-art vLLM under the same PP\nconfiguration, demonstrating its generality across LLMs and deployment\nscenarios.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As inference workloads for large language models (LLMs) scale to meet growing\nuser demand, pipeline parallelism (PP) has become a widely adopted strategy for\nmulti-GPU deployment, particularly in cross-node setups, to improve key-value\n(KV) cache capacity and inference throughput. However, PP suffers from inherent\ninefficiencies caused by three types of execution bubbles-load-imbalance,\nintra-stage, and inter-stage-which limit pipeline saturation. We present\nSiPipe, a heterogeneous pipeline design that improves throughput by leveraging\nunderutilized CPU resources to offload auxiliary computation and communication.\nSiPipe incorporates three key techniques-CPU sampling, a token-safe execution\nmodel, and structure-aware transmission-to mitigate pipeline bubbles and\nimprove execution efficiency. Across diverse LLMs, SiPipe achieves up to 2.1\ntimes higher throughput, 43% lower per-token latency, and up to 23% higher\naverage GPU utilization compared to the state-of-the-art vLLM under the same PP\nconfiguration, demonstrating its generality across LLMs and deployment\nscenarios."
                },
                "authors": [
                    {
                        "name": "Yongchao He"
                    },
                    {
                        "name": "Bohan Zhao"
                    },
                    {
                        "name": "Zheng Cao"
                    }
                ],
                "author_detail": {
                    "name": "Zheng Cao"
                },
                "author": "Zheng Cao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.22033v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.22033v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.00299v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.00299v3",
                "updated": "2025-06-27T09:14:02Z",
                "updated_parsed": [
                    2025,
                    6,
                    27,
                    9,
                    14,
                    2,
                    4,
                    178,
                    0
                ],
                "published": "2025-02-01T03:49:47Z",
                "published_parsed": [
                    2025,
                    2,
                    1,
                    3,
                    49,
                    47,
                    5,
                    32,
                    0
                ],
                "title": "ChunkKV: Semantic-Preserving KV Cache Compression for Efficient\n  Long-Context LLM Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ChunkKV: Semantic-Preserving KV Cache Compression for Efficient\n  Long-Context LLM Inference"
                },
                "summary": "Large Language Models (LLMs) require significant GPU memory when processing\nlong texts, with the key value (KV) cache consuming up to 70\\% of total memory\nduring inference. Although existing compression methods reduce memory by\nevaluating the importance of individual tokens, they overlook critical semantic\nrelationships between tokens, resulting in fragmented context and degraded\nperformance. We introduce ChunkKV, which fundamentally reimagines KV cache\ncompression by treating semantic chunks - rather than isolated tokens - as\nbasic compression units. This approach preserves complete linguistic structures\nand contextual integrity, ensuring that essential meaning is retained even\nunder aggressive compression. Our innovation includes a novel layer-wise index\nreuse technique that exploits the higher cross-layer similarity of preserved\nindices in ChunkKV, reducing computational overhead and improving throughput by\n26.5\\%. Comprehensive evaluations on challenging benchmarks: LongBench,\nNeedle-In-A-HayStack, GSM8K, and JailbreakV demonstrate that ChunkKV\noutperforms state-of-the-art methods by up to 8.7\\% in precision while\nmaintaining the same compression ratio. These results confirm that\nsemantic-aware compression significantly enhances both efficiency and\nperformance for long-context LLM inference, providing a simple yet effective\nsolution to the memory bottleneck problem.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) require significant GPU memory when processing\nlong texts, with the key value (KV) cache consuming up to 70\\% of total memory\nduring inference. Although existing compression methods reduce memory by\nevaluating the importance of individual tokens, they overlook critical semantic\nrelationships between tokens, resulting in fragmented context and degraded\nperformance. We introduce ChunkKV, which fundamentally reimagines KV cache\ncompression by treating semantic chunks - rather than isolated tokens - as\nbasic compression units. This approach preserves complete linguistic structures\nand contextual integrity, ensuring that essential meaning is retained even\nunder aggressive compression. Our innovation includes a novel layer-wise index\nreuse technique that exploits the higher cross-layer similarity of preserved\nindices in ChunkKV, reducing computational overhead and improving throughput by\n26.5\\%. Comprehensive evaluations on challenging benchmarks: LongBench,\nNeedle-In-A-HayStack, GSM8K, and JailbreakV demonstrate that ChunkKV\noutperforms state-of-the-art methods by up to 8.7\\% in precision while\nmaintaining the same compression ratio. These results confirm that\nsemantic-aware compression significantly enhances both efficiency and\nperformance for long-context LLM inference, providing a simple yet effective\nsolution to the memory bottleneck problem."
                },
                "authors": [
                    {
                        "name": "Xiang Liu"
                    },
                    {
                        "name": "Zhenheng Tang"
                    },
                    {
                        "name": "Peijie Dong"
                    },
                    {
                        "name": "Zeyu Li"
                    },
                    {
                        "name": "Yue Liu"
                    },
                    {
                        "name": "Bo Li"
                    },
                    {
                        "name": "Xuming Hu"
                    },
                    {
                        "name": "Xiaowen Chu"
                    }
                ],
                "author_detail": {
                    "name": "Xiaowen Chu"
                },
                "author": "Xiaowen Chu",
                "arxiv_comment": "41 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.00299v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.00299v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.21901v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.21901v1",
                "updated": "2025-06-27T04:38:20Z",
                "updated_parsed": [
                    2025,
                    6,
                    27,
                    4,
                    38,
                    20,
                    4,
                    178,
                    0
                ],
                "published": "2025-06-27T04:38:20Z",
                "published_parsed": [
                    2025,
                    6,
                    27,
                    4,
                    38,
                    20,
                    4,
                    178,
                    0
                ],
                "title": "A Survey of LLM Inference Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Survey of LLM Inference Systems"
                },
                "summary": "The past few years has witnessed specialized large language model (LLM)\ninference systems, such as vLLM, SGLang, Mooncake, and DeepFlow, alongside\nrapid LLM adoption via services like ChatGPT. Driving these system design\nefforts is the unique autoregressive nature of LLM request processing,\nmotivating new techniques for achieving high performance while preserving high\ninference quality over high-volume and high-velocity workloads. While many of\nthese techniques are discussed across the literature, they have not been\nanalyzed under the framework of a complete inference system, nor have the\nsystems themselves been analyzed and compared.\n  In this survey, we review these techniques, starting from operators and\nalgorithms for request processing, then moving on to techniques for model\noptimization and execution, including kernel design, batching, and scheduling,\nbefore ending with techniques for memory management, including paged memory,\neviction and offloading techniques, quantization, and cache persistence.\nThrough these discussions, we show that these techniques fundamentally rely on\nload prediction, adaptive mechanisms, and cost reduction in order to overcome\nthe challenges introduced by autoregressive generation and achieve the goals of\nthe system. We then discuss how these techniques can be combined to form\nsingle-replica and multi-replica inference systems, including disaggregated\ninference systems that offer more control over resource allocation and\nserverless systems that can be deployed over shared hardware infrastructure. We\nend with a discussion of remaining challenges.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The past few years has witnessed specialized large language model (LLM)\ninference systems, such as vLLM, SGLang, Mooncake, and DeepFlow, alongside\nrapid LLM adoption via services like ChatGPT. Driving these system design\nefforts is the unique autoregressive nature of LLM request processing,\nmotivating new techniques for achieving high performance while preserving high\ninference quality over high-volume and high-velocity workloads. While many of\nthese techniques are discussed across the literature, they have not been\nanalyzed under the framework of a complete inference system, nor have the\nsystems themselves been analyzed and compared.\n  In this survey, we review these techniques, starting from operators and\nalgorithms for request processing, then moving on to techniques for model\noptimization and execution, including kernel design, batching, and scheduling,\nbefore ending with techniques for memory management, including paged memory,\neviction and offloading techniques, quantization, and cache persistence.\nThrough these discussions, we show that these techniques fundamentally rely on\nload prediction, adaptive mechanisms, and cost reduction in order to overcome\nthe challenges introduced by autoregressive generation and achieve the goals of\nthe system. We then discuss how these techniques can be combined to form\nsingle-replica and multi-replica inference systems, including disaggregated\ninference systems that offer more control over resource allocation and\nserverless systems that can be deployed over shared hardware infrastructure. We\nend with a discussion of remaining challenges."
                },
                "authors": [
                    {
                        "name": "James Pan"
                    },
                    {
                        "name": "Guoliang Li"
                    }
                ],
                "author_detail": {
                    "name": "Guoliang Li"
                },
                "author": "Guoliang Li",
                "arxiv_comment": "25 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.21901v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.21901v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.15294v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.15294v3",
                "updated": "2025-06-27T03:43:24Z",
                "updated_parsed": [
                    2025,
                    6,
                    27,
                    3,
                    43,
                    24,
                    4,
                    178,
                    0
                ],
                "published": "2025-02-21T08:40:07Z",
                "published_parsed": [
                    2025,
                    2,
                    21,
                    8,
                    40,
                    7,
                    4,
                    52,
                    0
                ],
                "title": "Round Attention: A Novel Round-Level Attention Mechanism to Accelerate\n  LLM Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Round Attention: A Novel Round-Level Attention Mechanism to Accelerate\n  LLM Inference"
                },
                "summary": "The increasing context window size in large language models (LLMs) has\nimproved their ability to handle complex, long-text tasks. However, as the\nconversation rounds continue, it is required to store a large amount of KV\ncache in GPU memory, which significantly affects the efficiency and even\navailability of the model serving systems. This paper analyzes dialogue data\nfrom real users on the granularity of round and discovers that the LLM\ninference manifests a watershed layer, after which the distribution of\nround-level attention shows notable similarity. Based on this, we propose Round\nAttention - a novel round-level attention mechanism that selectively processes\nthe KV cache of top-k relevant rounds, where k is dynamically determined\nthrough the attention matrix in the watershed layer. Theoretical analysis\ndemonstrates that our method reduces memory usage by 54\\% to 82\\%, while\nexperimental results confirm that loading sparse critical-round KV cache\nmaintains answer accuracy without performance degradation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The increasing context window size in large language models (LLMs) has\nimproved their ability to handle complex, long-text tasks. However, as the\nconversation rounds continue, it is required to store a large amount of KV\ncache in GPU memory, which significantly affects the efficiency and even\navailability of the model serving systems. This paper analyzes dialogue data\nfrom real users on the granularity of round and discovers that the LLM\ninference manifests a watershed layer, after which the distribution of\nround-level attention shows notable similarity. Based on this, we propose Round\nAttention - a novel round-level attention mechanism that selectively processes\nthe KV cache of top-k relevant rounds, where k is dynamically determined\nthrough the attention matrix in the watershed layer. Theoretical analysis\ndemonstrates that our method reduces memory usage by 54\\% to 82\\%, while\nexperimental results confirm that loading sparse critical-round KV cache\nmaintains answer accuracy without performance degradation."
                },
                "authors": [
                    {
                        "name": "Yaohua Tang"
                    },
                    {
                        "name": "Zhicheng Hu"
                    },
                    {
                        "name": "Kun Cheng"
                    },
                    {
                        "name": "Fan Mo"
                    },
                    {
                        "name": "Qiheng Lv"
                    },
                    {
                        "name": "Hua Wang"
                    },
                    {
                        "name": "Zhi Chen"
                    }
                ],
                "author_detail": {
                    "name": "Zhi Chen"
                },
                "author": "Zhi Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.15294v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.15294v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.21710v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.21710v1",
                "updated": "2025-06-26T18:51:04Z",
                "updated_parsed": [
                    2025,
                    6,
                    26,
                    18,
                    51,
                    4,
                    3,
                    177,
                    0
                ],
                "published": "2025-06-26T18:51:04Z",
                "published_parsed": [
                    2025,
                    6,
                    26,
                    18,
                    51,
                    4,
                    3,
                    177,
                    0
                ],
                "title": "FOCUS: Internal MLLM Representations for Efficient Fine-Grained Visual\n  Question Answering",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FOCUS: Internal MLLM Representations for Efficient Fine-Grained Visual\n  Question Answering"
                },
                "summary": "While Multimodal Large Language Models (MLLMs) offer strong perception and\nreasoning capabilities for image-text input, Visual Question Answering (VQA)\nfocusing on small image details still remains a challenge. Although visual\ncropping techniques seem promising, recent approaches have several limitations:\nthe need for task-specific fine-tuning, low efficiency due to uninformed\nexhaustive search, or incompatibility with efficient attention implementations.\nWe address these shortcomings by proposing a training-free visual cropping\nmethod, dubbed FOCUS, that leverages MLLM-internal representations to guide the\nsearch for the most relevant image region. This is accomplished in four steps:\nfirst, we identify the target object(s) in the VQA prompt; second, we compute\nan object relevance map using the key-value (KV) cache; third, we propose and\nrank relevant image regions based on the map; and finally, we perform the\nfine-grained VQA task using the top-ranked region. As a result of this informed\nsearch strategy, FOCUS achieves strong performance across four fine-grained VQA\ndatasets and two types of MLLMs. It outperforms three popular visual cropping\nmethods in both accuracy and efficiency, and matches the best-performing\nbaseline, ZoomEye, while requiring 3 - 6.5 x less compute.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While Multimodal Large Language Models (MLLMs) offer strong perception and\nreasoning capabilities for image-text input, Visual Question Answering (VQA)\nfocusing on small image details still remains a challenge. Although visual\ncropping techniques seem promising, recent approaches have several limitations:\nthe need for task-specific fine-tuning, low efficiency due to uninformed\nexhaustive search, or incompatibility with efficient attention implementations.\nWe address these shortcomings by proposing a training-free visual cropping\nmethod, dubbed FOCUS, that leverages MLLM-internal representations to guide the\nsearch for the most relevant image region. This is accomplished in four steps:\nfirst, we identify the target object(s) in the VQA prompt; second, we compute\nan object relevance map using the key-value (KV) cache; third, we propose and\nrank relevant image regions based on the map; and finally, we perform the\nfine-grained VQA task using the top-ranked region. As a result of this informed\nsearch strategy, FOCUS achieves strong performance across four fine-grained VQA\ndatasets and two types of MLLMs. It outperforms three popular visual cropping\nmethods in both accuracy and efficiency, and matches the best-performing\nbaseline, ZoomEye, while requiring 3 - 6.5 x less compute."
                },
                "authors": [
                    {
                        "name": "Liangyu Zhong"
                    },
                    {
                        "name": "Fabio Rosenthal"
                    },
                    {
                        "name": "Joachim Sicking"
                    },
                    {
                        "name": "Fabian HÃ¼ger"
                    },
                    {
                        "name": "Thorsten Bagdonat"
                    },
                    {
                        "name": "Hanno Gottschalk"
                    },
                    {
                        "name": "Leo Schwinn"
                    }
                ],
                "author_detail": {
                    "name": "Leo Schwinn"
                },
                "author": "Leo Schwinn",
                "arxiv_comment": "Preprint. Under review",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.21710v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.21710v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.01805v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.01805v2",
                "updated": "2025-06-26T18:40:55Z",
                "updated_parsed": [
                    2025,
                    6,
                    26,
                    18,
                    40,
                    55,
                    3,
                    177,
                    0
                ],
                "published": "2025-01-03T13:32:57Z",
                "published_parsed": [
                    2025,
                    1,
                    3,
                    13,
                    32,
                    57,
                    4,
                    3,
                    0
                ],
                "title": "End-to-End Long Document Summarization using Gradient Caching",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "End-to-End Long Document Summarization using Gradient Caching"
                },
                "summary": "Training transformer-based encoder-decoder models for long document\nsummarization poses a significant challenge due to the quadratic memory\nconsumption during training. Several approaches have been proposed to extend\nthe input length at test time, but training with these approaches is still\ndifficult, requiring truncation of input documents and causing a mismatch\nbetween training and test conditions. In this work, we propose CachED (Gradient\n$\\textbf{Cach}$ing for $\\textbf{E}$ncoder-$\\textbf{D}$ecoder models), an\napproach that enables end-to-end training of existing transformer-based\nencoder-decoder models, using the entire document without truncation.\nSpecifically, we apply non-overlapping sliding windows to input documents,\nfollowed by fusion in decoder. During backpropagation, the gradients are cached\nat the decoder and are passed through the encoder in chunks by re-computing the\nhidden vectors, similar to gradient checkpointing. In the experiments on long\ndocument summarization, we extend BART to CachED BART, processing more than\n500K tokens during training and achieving superior performance without using\nany additional parameters.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Training transformer-based encoder-decoder models for long document\nsummarization poses a significant challenge due to the quadratic memory\nconsumption during training. Several approaches have been proposed to extend\nthe input length at test time, but training with these approaches is still\ndifficult, requiring truncation of input documents and causing a mismatch\nbetween training and test conditions. In this work, we propose CachED (Gradient\n$\\textbf{Cach}$ing for $\\textbf{E}$ncoder-$\\textbf{D}$ecoder models), an\napproach that enables end-to-end training of existing transformer-based\nencoder-decoder models, using the entire document without truncation.\nSpecifically, we apply non-overlapping sliding windows to input documents,\nfollowed by fusion in decoder. During backpropagation, the gradients are cached\nat the decoder and are passed through the encoder in chunks by re-computing the\nhidden vectors, similar to gradient checkpointing. In the experiments on long\ndocument summarization, we extend BART to CachED BART, processing more than\n500K tokens during training and achieving superior performance without using\nany additional parameters."
                },
                "authors": [
                    {
                        "name": "Rohit Saxena"
                    },
                    {
                        "name": "Hao Tang"
                    },
                    {
                        "name": "Frank Keller"
                    }
                ],
                "author_detail": {
                    "name": "Frank Keller"
                },
                "author": "Frank Keller",
                "arxiv_comment": "Accepted to Transactions of the Association for Computational\n  Linguistics (TACL 2025); Pre MIT Press version",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.01805v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.01805v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.19686v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.19686v2",
                "updated": "2025-06-26T17:18:54Z",
                "updated_parsed": [
                    2025,
                    6,
                    26,
                    17,
                    18,
                    54,
                    3,
                    177,
                    0
                ],
                "published": "2025-06-24T14:55:43Z",
                "published_parsed": [
                    2025,
                    6,
                    24,
                    14,
                    55,
                    43,
                    1,
                    175,
                    0
                ],
                "title": "From Memories to Maps: Mechanisms of In-Context Reinforcement Learning\n  in Transformers",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "From Memories to Maps: Mechanisms of In-Context Reinforcement Learning\n  in Transformers"
                },
                "summary": "Humans and animals show remarkable learning efficiency, adapting to new\nenvironments with minimal experience. This capability is not well captured by\nstandard reinforcement learning algorithms that rely on incremental value\nupdates. Rapid adaptation likely depends on episodic memory -- the ability to\nretrieve specific past experiences to guide decisions in novel contexts.\nTransformers provide a useful setting for studying these questions because of\ntheir ability to learn rapidly in-context and because their key-value\narchitecture resembles episodic memory systems in the brain. We train a\ntransformer to in-context reinforcement learn in a distribution of planning\ntasks inspired by rodent behavior. We then characterize the learning algorithms\nthat emerge in the model. We first find that representation learning is\nsupported by in-context structure learning and cross-context alignment, where\nrepresentations are aligned across environments with different sensory stimuli.\nWe next demonstrate that the reinforcement learning strategies developed by the\nmodel are not interpretable as standard model-free or model-based planning.\nInstead, we show that in-context reinforcement learning is supported by caching\nintermediate computations within the model's memory tokens, which are then\naccessed at decision time. Overall, we find that memory may serve as a\ncomputational resource, storing both raw experience and cached computations to\nsupport flexible behavior. Furthermore, the representations developed in the\nmodel resemble computations associated with the hippocampal-entorhinal system\nin the brain, suggesting that our findings may be relevant for natural\ncognition. Taken together, our work offers a mechanistic hypothesis for the\nrapid adaptation that underlies in-context learning in artificial and natural\nsettings.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Humans and animals show remarkable learning efficiency, adapting to new\nenvironments with minimal experience. This capability is not well captured by\nstandard reinforcement learning algorithms that rely on incremental value\nupdates. Rapid adaptation likely depends on episodic memory -- the ability to\nretrieve specific past experiences to guide decisions in novel contexts.\nTransformers provide a useful setting for studying these questions because of\ntheir ability to learn rapidly in-context and because their key-value\narchitecture resembles episodic memory systems in the brain. We train a\ntransformer to in-context reinforcement learn in a distribution of planning\ntasks inspired by rodent behavior. We then characterize the learning algorithms\nthat emerge in the model. We first find that representation learning is\nsupported by in-context structure learning and cross-context alignment, where\nrepresentations are aligned across environments with different sensory stimuli.\nWe next demonstrate that the reinforcement learning strategies developed by the\nmodel are not interpretable as standard model-free or model-based planning.\nInstead, we show that in-context reinforcement learning is supported by caching\nintermediate computations within the model's memory tokens, which are then\naccessed at decision time. Overall, we find that memory may serve as a\ncomputational resource, storing both raw experience and cached computations to\nsupport flexible behavior. Furthermore, the representations developed in the\nmodel resemble computations associated with the hippocampal-entorhinal system\nin the brain, suggesting that our findings may be relevant for natural\ncognition. Taken together, our work offers a mechanistic hypothesis for the\nrapid adaptation that underlies in-context learning in artificial and natural\nsettings."
                },
                "authors": [
                    {
                        "name": "Ching Fang"
                    },
                    {
                        "name": "Kanaka Rajan"
                    }
                ],
                "author_detail": {
                    "name": "Kanaka Rajan"
                },
                "author": "Kanaka Rajan",
                "arxiv_comment": "Updates: added other funding sources; formatted title correctly",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.19686v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.19686v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.21236v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.21236v1",
                "updated": "2025-06-26T13:22:30Z",
                "updated_parsed": [
                    2025,
                    6,
                    26,
                    13,
                    22,
                    30,
                    3,
                    177,
                    0
                ],
                "published": "2025-06-26T13:22:30Z",
                "published_parsed": [
                    2025,
                    6,
                    26,
                    13,
                    22,
                    30,
                    3,
                    177,
                    0
                ],
                "title": "Measurements, simulations, and models of the point-spread function of\n  electron-beam lithography",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Measurements, simulations, and models of the point-spread function of\n  electron-beam lithography"
                },
                "summary": "When a sample is exposed using electron-beam lithography, the electrons\nscatter deep and far in the substrate, resulting in unwanted deposition of dose\nat both the nano- and the microscale. This proximity effect can be mitigated by\nproximity effect correction provided that accurate and validated models of the\npoint-spread function of the electron scattering are available. Most works so\nfar considered a double-Gaussian model of the electron point-spread function,\nwhich is very inaccurate for modern electron-beam writers with high\nacceleration voltages. We present measurements of the process point-spread\nfunction for chemically semi-amplified resist on silicon and indium phosphide\nsubstrates using a 150 kV electron-beam lithography system. We find that the\ndouble-Gaussian model deviates from experiments by up to four orders of\nmagnitude. We propose instead a model comprising the sum of a power-law and a\nGaussian, which is in excellent agreement with simulations of the electron\nscattering obtained by a Monte Carlo method. We apply the power-law plus\nGaussian model to quantify the electron scattering and proximity effect\ncorrection parameters across material stacks, processing, and voltages from 5\nkV to 150 kV. We find that the power-law term remains remarkably constant,\nwhereas the long-range dose contributions and the clearing dose are\nsignificantly affected by the substrate and the acceleration voltage.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "When a sample is exposed using electron-beam lithography, the electrons\nscatter deep and far in the substrate, resulting in unwanted deposition of dose\nat both the nano- and the microscale. This proximity effect can be mitigated by\nproximity effect correction provided that accurate and validated models of the\npoint-spread function of the electron scattering are available. Most works so\nfar considered a double-Gaussian model of the electron point-spread function,\nwhich is very inaccurate for modern electron-beam writers with high\nacceleration voltages. We present measurements of the process point-spread\nfunction for chemically semi-amplified resist on silicon and indium phosphide\nsubstrates using a 150 kV electron-beam lithography system. We find that the\ndouble-Gaussian model deviates from experiments by up to four orders of\nmagnitude. We propose instead a model comprising the sum of a power-law and a\nGaussian, which is in excellent agreement with simulations of the electron\nscattering obtained by a Monte Carlo method. We apply the power-law plus\nGaussian model to quantify the electron scattering and proximity effect\ncorrection parameters across material stacks, processing, and voltages from 5\nkV to 150 kV. We find that the power-law term remains remarkably constant,\nwhereas the long-range dose contributions and the clearing dose are\nsignificantly affected by the substrate and the acceleration voltage."
                },
                "authors": [
                    {
                        "name": "Nikolaj B. Hougs"
                    },
                    {
                        "name": "Kristian S. Knudsen"
                    },
                    {
                        "name": "Marcus Albrechtsen"
                    },
                    {
                        "name": "Taichi Suhara"
                    },
                    {
                        "name": "Christian A. Rosiek"
                    },
                    {
                        "name": "SÃ¸ren Stobbe"
                    }
                ],
                "author_detail": {
                    "name": "SÃ¸ren Stobbe"
                },
                "author": "SÃ¸ren Stobbe",
                "arxiv_comment": "Main; 15 pages, 7 figures. Supporting; 5 pages, 2 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.21236v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.21236v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.app-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.app-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.21184v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.21184v1",
                "updated": "2025-06-26T12:43:43Z",
                "updated_parsed": [
                    2025,
                    6,
                    26,
                    12,
                    43,
                    43,
                    3,
                    177,
                    0
                ],
                "published": "2025-06-26T12:43:43Z",
                "published_parsed": [
                    2025,
                    6,
                    26,
                    12,
                    43,
                    43,
                    3,
                    177,
                    0
                ],
                "title": "Task-Aware KV Compression For Cost-Effective Long Video Understanding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Task-Aware KV Compression For Cost-Effective Long Video Understanding"
                },
                "summary": "Long-video understanding (LVU) remains a severe challenge for existing\nmultimodal large language models (MLLMs), primarily due to the prohibitive\ncomputational cost. Recent approaches have explored KV compression to mitigate\nthis issue, but they often suffer from significant information loss at high\ncompression ratios. In this paper, we introduce Video-X^2L, which flexibly\npreserves critical video information for each LVU task. Video-X^2L involves two\nkey operations. The first one is called bi-level KV compression. During the\nMLLM's pre-filling stage, Video-X^2L generates two types of compressed KVs:\nlow-compression KVs (L-KVs) to capture fine-grained video details and\nhigh-compression KVs (H-KVs) to offer compact video representations. The second\none is called selective KV re-loading. During the MLLM's decoding stage,\nVideo-X^2L selectively re-loads L-KVs for the most critical video chunks while\nusing H-KVs for other less important ones. This allows the MLLM to fully\nutilize task-specific information while maintaining the overall compactness.\nVideo-X^2L is simple yet effective: it is free from additional training and\ndirectly compatible with existing KV-compressible MLLMs. We evaluate Video-X^2L\nwith a variety of popular LVU benchmarks, including VideoMME, MLVU,\nLongVideoBench, and VNBench. Our experiment result shows that Video-X^2L\noutperforms existing KV-compression methods by a huge advantage while\nsubstantially saving the computation cost.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Long-video understanding (LVU) remains a severe challenge for existing\nmultimodal large language models (MLLMs), primarily due to the prohibitive\ncomputational cost. Recent approaches have explored KV compression to mitigate\nthis issue, but they often suffer from significant information loss at high\ncompression ratios. In this paper, we introduce Video-X^2L, which flexibly\npreserves critical video information for each LVU task. Video-X^2L involves two\nkey operations. The first one is called bi-level KV compression. During the\nMLLM's pre-filling stage, Video-X^2L generates two types of compressed KVs:\nlow-compression KVs (L-KVs) to capture fine-grained video details and\nhigh-compression KVs (H-KVs) to offer compact video representations. The second\none is called selective KV re-loading. During the MLLM's decoding stage,\nVideo-X^2L selectively re-loads L-KVs for the most critical video chunks while\nusing H-KVs for other less important ones. This allows the MLLM to fully\nutilize task-specific information while maintaining the overall compactness.\nVideo-X^2L is simple yet effective: it is free from additional training and\ndirectly compatible with existing KV-compressible MLLMs. We evaluate Video-X^2L\nwith a variety of popular LVU benchmarks, including VideoMME, MLVU,\nLongVideoBench, and VNBench. Our experiment result shows that Video-X^2L\noutperforms existing KV-compression methods by a huge advantage while\nsubstantially saving the computation cost."
                },
                "authors": [
                    {
                        "name": "Minghao Qin"
                    },
                    {
                        "name": "Yan Shu"
                    },
                    {
                        "name": "Peitian Zhang"
                    },
                    {
                        "name": "Kun Lun"
                    },
                    {
                        "name": "Huaying Yuan"
                    },
                    {
                        "name": "Juenjie Zhou"
                    },
                    {
                        "name": "Shitao Xiao"
                    },
                    {
                        "name": "Bo Zhao"
                    },
                    {
                        "name": "Zheng Liu"
                    }
                ],
                "author_detail": {
                    "name": "Zheng Liu"
                },
                "author": "Zheng Liu",
                "arxiv_comment": "14 pages, 3 figures, 6 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.21184v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.21184v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.02469v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.02469v3",
                "updated": "2025-06-26T05:12:22Z",
                "updated_parsed": [
                    2025,
                    6,
                    26,
                    5,
                    12,
                    22,
                    3,
                    177,
                    0
                ],
                "published": "2025-01-05T07:41:53Z",
                "published_parsed": [
                    2025,
                    1,
                    5,
                    7,
                    41,
                    53,
                    6,
                    5,
                    0
                ],
                "title": "LoRaConnect: Unlocking HTTP Potential on LoRa Backbones for Remote Areas\n  and Ad-Hoc Networks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LoRaConnect: Unlocking HTTP Potential on LoRa Backbones for Remote Areas\n  and Ad-Hoc Networks"
                },
                "summary": "Minimal infrastructure requirements make LoRa suitable for service delivery\nin remote areas. Additionally, web applications have become a de-facto standard\nfor modern service delivery. However, Long Range (LoRa) fails to enable HTTP\naccess due to its limited bandwidth, payload size limitations, and high\ncollisions in multi-user setups. We propose LoRaConnect to enable HTTP access\nover LoRa. The LoRaWeb hardware tethers a WiFi hotspot to which client devices\nconnect and access HTTP resources over LoRa backhaul. It implements caching and\nsynchronization mechanisms to address LoRa's aforementioned limitations. It\nalso implements a message-slicing method in the application layer to overcome\nLoRa's payload limitations. We evaluate the proposed system using actual\nhardware in three experimental setups to assess the baseline performance, ideal\nscenario, and practical application scenario with Frequency Hopping Spread\nSpectrum (FHSS). Additionally, it implements a ping operation to demonstrate\nInternet capability and extensible nature. LoRaWeb achieves an average\nthroughput of 1.18 KB/S approximately, with an access delay of only 1.3 S\napproximately for a 1.5KB webpage in the baseline setup. Moreover, it achieves\nan access delay of approximately 6.7 S for a 10KB webpage in the ideal case and\nan average end-to-end delay of only 612 ms approximately in the FHSS-based\nsetup. Comparison with benchmark suggests multi-fold improvement.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Minimal infrastructure requirements make LoRa suitable for service delivery\nin remote areas. Additionally, web applications have become a de-facto standard\nfor modern service delivery. However, Long Range (LoRa) fails to enable HTTP\naccess due to its limited bandwidth, payload size limitations, and high\ncollisions in multi-user setups. We propose LoRaConnect to enable HTTP access\nover LoRa. The LoRaWeb hardware tethers a WiFi hotspot to which client devices\nconnect and access HTTP resources over LoRa backhaul. It implements caching and\nsynchronization mechanisms to address LoRa's aforementioned limitations. It\nalso implements a message-slicing method in the application layer to overcome\nLoRa's payload limitations. We evaluate the proposed system using actual\nhardware in three experimental setups to assess the baseline performance, ideal\nscenario, and practical application scenario with Frequency Hopping Spread\nSpectrum (FHSS). Additionally, it implements a ping operation to demonstrate\nInternet capability and extensible nature. LoRaWeb achieves an average\nthroughput of 1.18 KB/S approximately, with an access delay of only 1.3 S\napproximately for a 1.5KB webpage in the baseline setup. Moreover, it achieves\nan access delay of approximately 6.7 S for a 10KB webpage in the ideal case and\nan average end-to-end delay of only 612 ms approximately in the FHSS-based\nsetup. Comparison with benchmark suggests multi-fold improvement."
                },
                "authors": [
                    {
                        "name": "Atonu Ghosh"
                    },
                    {
                        "name": "Sudip Misra"
                    }
                ],
                "author_detail": {
                    "name": "Sudip Misra"
                },
                "author": "Sudip Misra",
                "arxiv_comment": "10 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.02469v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.02469v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.20968v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.20968v1",
                "updated": "2025-06-26T03:13:33Z",
                "updated_parsed": [
                    2025,
                    6,
                    26,
                    3,
                    13,
                    33,
                    3,
                    177,
                    0
                ],
                "published": "2025-06-26T03:13:33Z",
                "published_parsed": [
                    2025,
                    6,
                    26,
                    3,
                    13,
                    33,
                    3,
                    177,
                    0
                ],
                "title": "The electronic structures, magnetic transition and Fermi surface\n  instability of room-temperature altermagnet KV$_{2}$Se$_{2}$O",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The electronic structures, magnetic transition and Fermi surface\n  instability of room-temperature altermagnet KV$_{2}$Se$_{2}$O"
                },
                "summary": "Altermagnetism has recently emerged as a distinct and fundamental class of\nmagnetic order. Exploring its interplay with quantum phenomena such as\nunconventional superconductivity, density-wave instabilities, and many-body\neffects represents a compelling frontier. In this work, we theoretically\nconfirm the presence of high-temperature metallic altermagnetism in\nKV$_2$Se$_2$O. We demonstrate that the anomalous metal-insulator-metal\ntransition arises from a Lifshitz transition associated with Fermi surface\nreconstruction. The previously reported spin-density wave gap is found to lie\nbelow the Fermi level in our study and is now recognized to be attributed to\nthe V-shaped density of states, originating from orbital-selective and\nsublattice-resolved half-metal-like behavior on a specific V atom. Furthermore,\nwe identify the instability from the nesting of spin-momentum-locked\ntwo-dimensional Fermi surfaces, which induces the SDW state. These findings\nposition KV$_2$Se$_2$O as a promising platform for investigating the interplay\namong altermagnetism, unconventional superconductivity, and density-wave order.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Altermagnetism has recently emerged as a distinct and fundamental class of\nmagnetic order. Exploring its interplay with quantum phenomena such as\nunconventional superconductivity, density-wave instabilities, and many-body\neffects represents a compelling frontier. In this work, we theoretically\nconfirm the presence of high-temperature metallic altermagnetism in\nKV$_2$Se$_2$O. We demonstrate that the anomalous metal-insulator-metal\ntransition arises from a Lifshitz transition associated with Fermi surface\nreconstruction. The previously reported spin-density wave gap is found to lie\nbelow the Fermi level in our study and is now recognized to be attributed to\nthe V-shaped density of states, originating from orbital-selective and\nsublattice-resolved half-metal-like behavior on a specific V atom. Furthermore,\nwe identify the instability from the nesting of spin-momentum-locked\ntwo-dimensional Fermi surfaces, which induces the SDW state. These findings\nposition KV$_2$Se$_2$O as a promising platform for investigating the interplay\namong altermagnetism, unconventional superconductivity, and density-wave order."
                },
                "authors": [
                    {
                        "name": "Yuanji Xu"
                    },
                    {
                        "name": "Huiyuan Zhang"
                    },
                    {
                        "name": "Maoyuan Feng"
                    },
                    {
                        "name": "Fuyang Tian"
                    }
                ],
                "author_detail": {
                    "name": "Fuyang Tian"
                },
                "author": "Fuyang Tian",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.20968v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.20968v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cond-mat.str-el",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cond-mat.str-el",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.23956v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.23956v2",
                "updated": "2025-06-26T01:30:43Z",
                "updated_parsed": [
                    2025,
                    6,
                    26,
                    1,
                    30,
                    43,
                    3,
                    177,
                    0
                ],
                "published": "2025-03-31T11:13:18Z",
                "published_parsed": [
                    2025,
                    3,
                    31,
                    11,
                    13,
                    18,
                    0,
                    90,
                    0
                ],
                "title": "AirCache: Activating Inter-modal Relevancy KV Cache Compression for\n  Efficient Large Vision-Language Model Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AirCache: Activating Inter-modal Relevancy KV Cache Compression for\n  Efficient Large Vision-Language Model Inference"
                },
                "summary": "Recent advancements in Large Visual Language Models (LVLMs) have gained\nsignificant attention due to their remarkable reasoning capabilities and\nproficiency in generalization. However, processing a large number of visual\ntokens and generating long-context outputs impose substantial computational\noverhead, leading to excessive demands for key-value (KV) cache. To address\nthis critical bottleneck, we propose AirCache, a novel KV cache compression\nmethod aimed at accelerating LVLMs inference. This work systematically\ninvestigates the correlations between visual and textual tokens within the\nattention mechanisms of LVLMs. Our empirical analysis reveals considerable\nredundancy in cached visual tokens, wherein strategically eliminating these\ntokens preserves model performance while significantly accelerating context\ngeneration. Inspired by these findings, we introduce an elite observation\nwindow for assessing the importance of visual components in the KV cache,\nfocusing on stable inter-modal relevancy modeling with enhanced\nmulti-perspective consistency. Additionally, we develop an adaptive layer-wise\nbudget allocation strategy that capitalizes on the strength and skewness of\ntoken importance distribution, showcasing superior efficiency compared to\nuniform allocation. Comprehensive evaluations across multiple LVLMs and\nbenchmarks demonstrate that our method achieves comparable performance to the\nfull cache while retaining only 10% of visual KV cache, thereby reducing\ndecoding latency by 29% to 66% across various batch size and prompt length of\ninputs. Notably, as cache retention rates decrease, our method exhibits\nincreasing performance advantages over existing approaches.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advancements in Large Visual Language Models (LVLMs) have gained\nsignificant attention due to their remarkable reasoning capabilities and\nproficiency in generalization. However, processing a large number of visual\ntokens and generating long-context outputs impose substantial computational\noverhead, leading to excessive demands for key-value (KV) cache. To address\nthis critical bottleneck, we propose AirCache, a novel KV cache compression\nmethod aimed at accelerating LVLMs inference. This work systematically\ninvestigates the correlations between visual and textual tokens within the\nattention mechanisms of LVLMs. Our empirical analysis reveals considerable\nredundancy in cached visual tokens, wherein strategically eliminating these\ntokens preserves model performance while significantly accelerating context\ngeneration. Inspired by these findings, we introduce an elite observation\nwindow for assessing the importance of visual components in the KV cache,\nfocusing on stable inter-modal relevancy modeling with enhanced\nmulti-perspective consistency. Additionally, we develop an adaptive layer-wise\nbudget allocation strategy that capitalizes on the strength and skewness of\ntoken importance distribution, showcasing superior efficiency compared to\nuniform allocation. Comprehensive evaluations across multiple LVLMs and\nbenchmarks demonstrate that our method achieves comparable performance to the\nfull cache while retaining only 10% of visual KV cache, thereby reducing\ndecoding latency by 29% to 66% across various batch size and prompt length of\ninputs. Notably, as cache retention rates decrease, our method exhibits\nincreasing performance advantages over existing approaches."
                },
                "authors": [
                    {
                        "name": "Kai Huang"
                    },
                    {
                        "name": "Hao Zou"
                    },
                    {
                        "name": "Bochen Wang"
                    },
                    {
                        "name": "Ye Xi"
                    },
                    {
                        "name": "Zhen Xie"
                    },
                    {
                        "name": "Hao Wang"
                    }
                ],
                "author_detail": {
                    "name": "Hao Wang"
                },
                "author": "Hao Wang",
                "arxiv_comment": "We have withdrawn this manuscript due to a critical error in the\n  methodology which affects the validity of the main results. We are currently\n  working to address this issue and will resubmit once the correction is\n  complete",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.23956v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.23956v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.20886v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.20886v1",
                "updated": "2025-06-25T23:36:44Z",
                "updated_parsed": [
                    2025,
                    6,
                    25,
                    23,
                    36,
                    44,
                    2,
                    176,
                    0
                ],
                "published": "2025-06-25T23:36:44Z",
                "published_parsed": [
                    2025,
                    6,
                    25,
                    23,
                    36,
                    44,
                    2,
                    176,
                    0
                ],
                "title": "Omniwise: Predicting GPU Kernels Performance with LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Omniwise: Predicting GPU Kernels Performance with LLMs"
                },
                "summary": "In recent years, the rapid advancement of deep neural networks (DNNs) has\nrevolutionized artificial intelligence, enabling models with unprecedented\ncapabilities in understanding, generating, and processing complex data. These\npowerful architectures have transformed a wide range of downstream\napplications, tackling tasks beyond human reach. In this paper, we introduce\nOmniwise, the first end-to-end, self-supervised fine-tuning pipeline that\napplies large language models (LLMs) to GPU kernel performance prediction--a\nnovel use case in performance profiling. Omniwise is model-agnostic and\nlightweight, achieving strong results even with a small 3B-parameter model. It\ncan predict key performance metrics, including memory bandwidth, cache hit\nrates, GFLOPs, and arithmetic intensity, directly from kernel code without the\nneed for code execution or profiling tools. Our approach achieves over 90% of\npredictions within 10% relative error on GPU kernels executed on AMD MI250 and\nMI300X architectures. In addition to the pipeline, we develop an online\ninference server and a Visual Studio Code plugin that seamlessly integrate\nLLM-based performance prediction into developers' workflows.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In recent years, the rapid advancement of deep neural networks (DNNs) has\nrevolutionized artificial intelligence, enabling models with unprecedented\ncapabilities in understanding, generating, and processing complex data. These\npowerful architectures have transformed a wide range of downstream\napplications, tackling tasks beyond human reach. In this paper, we introduce\nOmniwise, the first end-to-end, self-supervised fine-tuning pipeline that\napplies large language models (LLMs) to GPU kernel performance prediction--a\nnovel use case in performance profiling. Omniwise is model-agnostic and\nlightweight, achieving strong results even with a small 3B-parameter model. It\ncan predict key performance metrics, including memory bandwidth, cache hit\nrates, GFLOPs, and arithmetic intensity, directly from kernel code without the\nneed for code execution or profiling tools. Our approach achieves over 90% of\npredictions within 10% relative error on GPU kernels executed on AMD MI250 and\nMI300X architectures. In addition to the pipeline, we develop an online\ninference server and a Visual Studio Code plugin that seamlessly integrate\nLLM-based performance prediction into developers' workflows."
                },
                "authors": [
                    {
                        "name": "Zixian Wang"
                    },
                    {
                        "name": "Cole Ramos"
                    },
                    {
                        "name": "Muhammad A. Awad"
                    },
                    {
                        "name": "Keith Lowery"
                    }
                ],
                "author_detail": {
                    "name": "Keith Lowery"
                },
                "author": "Keith Lowery",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.20886v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.20886v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.12942v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.12942v3",
                "updated": "2025-06-25T23:03:54Z",
                "updated_parsed": [
                    2025,
                    6,
                    25,
                    23,
                    3,
                    54,
                    2,
                    176,
                    0
                ],
                "published": "2025-05-19T10:29:32Z",
                "published_parsed": [
                    2025,
                    5,
                    19,
                    10,
                    29,
                    32,
                    0,
                    139,
                    0
                ],
                "title": "A3 : an Analytical Low-Rank Approximation Framework for Attention",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A3 : an Analytical Low-Rank Approximation Framework for Attention"
                },
                "summary": "Large language models have demonstrated remarkable performance; however,\ntheir massive parameter counts make deployment highly expensive. Low-rank\napproximation offers a promising compression solution, yet existing approaches\nhave two main limitations: (1) They focus on minimizing the output error of\nindividual linear layers, without considering the architectural characteristics\nof Transformers, and (2) they decompose a large weight matrix into two small\nlow-rank matrices. Consequently, these methods often fall short compared to\nother compression techniques like pruning and quantization, and introduce\nruntime overhead such as the extra GEMM kernel launches for decomposed small\nmatrices. To address these limitations, we propose $\\tt A^\\tt 3$, a\npost-training low-rank approximation framework. $\\tt A^\\tt 3$ splits a\nTransformer layer into three functional components, namely $\\tt QK$, $\\tt OV$,\nand $\\tt MLP$. For each component, $\\tt A^\\tt 3$ provides an analytical\nsolution that reduces the hidden dimension size inside each component while\nminimizing the component's functional loss ($\\it i.e.$, error in attention\nscores, attention outputs, and MLP outputs). This approach directly reduces\nmodel sizes, KV cache sizes, and FLOPs without introducing any runtime\noverheads. In addition, it provides a new narrative in advancing the\noptimization problem from singular linear layer loss optimization toward\nimproved end-to-end performance. Through extensive experiments, we show that\n$\\tt A^\\tt 3$ maintains superior performance compared to SoTAs. For example,\nunder the same reduction budget in computation and memory, our low-rank\napproximated LLaMA 3.1-70B achieves a perplexity of 4.69 on WikiText-2,\noutperforming the previous SoTA's 7.87 by 3.18. We also demonstrate the\nversatility of $\\tt A^\\tt 3$, including KV cache compression, quantization, and\nmixed-rank assignments for enhanced performance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models have demonstrated remarkable performance; however,\ntheir massive parameter counts make deployment highly expensive. Low-rank\napproximation offers a promising compression solution, yet existing approaches\nhave two main limitations: (1) They focus on minimizing the output error of\nindividual linear layers, without considering the architectural characteristics\nof Transformers, and (2) they decompose a large weight matrix into two small\nlow-rank matrices. Consequently, these methods often fall short compared to\nother compression techniques like pruning and quantization, and introduce\nruntime overhead such as the extra GEMM kernel launches for decomposed small\nmatrices. To address these limitations, we propose $\\tt A^\\tt 3$, a\npost-training low-rank approximation framework. $\\tt A^\\tt 3$ splits a\nTransformer layer into three functional components, namely $\\tt QK$, $\\tt OV$,\nand $\\tt MLP$. For each component, $\\tt A^\\tt 3$ provides an analytical\nsolution that reduces the hidden dimension size inside each component while\nminimizing the component's functional loss ($\\it i.e.$, error in attention\nscores, attention outputs, and MLP outputs). This approach directly reduces\nmodel sizes, KV cache sizes, and FLOPs without introducing any runtime\noverheads. In addition, it provides a new narrative in advancing the\noptimization problem from singular linear layer loss optimization toward\nimproved end-to-end performance. Through extensive experiments, we show that\n$\\tt A^\\tt 3$ maintains superior performance compared to SoTAs. For example,\nunder the same reduction budget in computation and memory, our low-rank\napproximated LLaMA 3.1-70B achieves a perplexity of 4.69 on WikiText-2,\noutperforming the previous SoTA's 7.87 by 3.18. We also demonstrate the\nversatility of $\\tt A^\\tt 3$, including KV cache compression, quantization, and\nmixed-rank assignments for enhanced performance."
                },
                "authors": [
                    {
                        "name": "Jeffrey T. H. Wong"
                    },
                    {
                        "name": "Cheng Zhang"
                    },
                    {
                        "name": "Xinye Cao"
                    },
                    {
                        "name": "Pedro Gimenes"
                    },
                    {
                        "name": "George A. Constantinides"
                    },
                    {
                        "name": "Wayne Luk"
                    },
                    {
                        "name": "Yiren Zhao"
                    }
                ],
                "author_detail": {
                    "name": "Yiren Zhao"
                },
                "author": "Yiren Zhao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.12942v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.12942v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.20703v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.20703v1",
                "updated": "2025-06-25T17:59:55Z",
                "updated_parsed": [
                    2025,
                    6,
                    25,
                    17,
                    59,
                    55,
                    2,
                    176,
                    0
                ],
                "published": "2025-06-25T17:59:55Z",
                "published_parsed": [
                    2025,
                    6,
                    25,
                    17,
                    59,
                    55,
                    2,
                    176,
                    0
                ],
                "title": "Generative Blocks World: Moving Things Around in Pictures",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Generative Blocks World: Moving Things Around in Pictures"
                },
                "summary": "We describe Generative Blocks World to interact with the scene of a generated\nimage by manipulating simple geometric abstractions. Our method represents\nscenes as assemblies of convex 3D primitives, and the same scene can be\nrepresented by different numbers of primitives, allowing an editor to move\neither whole structures or small details. Once the scene geometry has been\nedited, the image is generated by a flow-based method which is conditioned on\ndepth and a texture hint. Our texture hint takes into account the modified 3D\nprimitives, exceeding texture-consistency provided by existing key-value\ncaching techniques. These texture hints (a) allow accurate object and camera\nmoves and (b) largely preserve the identity of objects depicted. Quantitative\nand qualitative experiments demonstrate that our approach outperforms prior\nworks in visual fidelity, editability, and compositional generalization.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We describe Generative Blocks World to interact with the scene of a generated\nimage by manipulating simple geometric abstractions. Our method represents\nscenes as assemblies of convex 3D primitives, and the same scene can be\nrepresented by different numbers of primitives, allowing an editor to move\neither whole structures or small details. Once the scene geometry has been\nedited, the image is generated by a flow-based method which is conditioned on\ndepth and a texture hint. Our texture hint takes into account the modified 3D\nprimitives, exceeding texture-consistency provided by existing key-value\ncaching techniques. These texture hints (a) allow accurate object and camera\nmoves and (b) largely preserve the identity of objects depicted. Quantitative\nand qualitative experiments demonstrate that our approach outperforms prior\nworks in visual fidelity, editability, and compositional generalization."
                },
                "authors": [
                    {
                        "name": "Vaibhav Vavilala"
                    },
                    {
                        "name": "Seemandhar Jain"
                    },
                    {
                        "name": "Rahul Vasanth"
                    },
                    {
                        "name": "D. A. Forsyth"
                    },
                    {
                        "name": "Anand Bhattad"
                    }
                ],
                "author_detail": {
                    "name": "Anand Bhattad"
                },
                "author": "Anand Bhattad",
                "arxiv_comment": "23 pages, 16 figures, 2 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.20703v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.20703v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.GR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.GR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.20420v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.20420v1",
                "updated": "2025-06-25T13:35:25Z",
                "updated_parsed": [
                    2025,
                    6,
                    25,
                    13,
                    35,
                    25,
                    2,
                    176,
                    0
                ],
                "published": "2025-06-25T13:35:25Z",
                "published_parsed": [
                    2025,
                    6,
                    25,
                    13,
                    35,
                    25,
                    2,
                    176,
                    0
                ],
                "title": "Semantic Caching for Improving Web Affordability",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Semantic Caching for Improving Web Affordability"
                },
                "summary": "The rapid growth of web content has led to increasingly large webpages,\nposing significant challenges for Internet affordability, especially in\ndeveloping countries where data costs remain prohibitively high. We propose\nsemantic caching using Large Language Models (LLMs) to improve web\naffordability by enabling reuse of semantically similar images within webpages.\nAnalyzing 50 leading news and media websites, encompassing 4,264 images and\nover 40,000 image pairs, we demonstrate potential for significant data transfer\nreduction, with some website categories showing up to 37% of images as\nreplaceable. Our proof-of-concept architecture shows users can achieve\napproximately 10% greater byte savings compared to exact caching. We evaluate\nboth commercial and open-source multi-modal LLMs for assessing semantic\nreplaceability. GPT-4o performs best with a low Normalized Root Mean Square\nError of 0.1735 and a weighted F1 score of 0.8374, while the open-source LLaMA\n3.1 model shows comparable performance, highlighting its viability for\nlarge-scale applications. This approach offers benefits for both users and\nwebsite operators, substantially reducing data transmission. We discuss ethical\nconcerns and practical challenges, including semantic preservation, user-driven\ncache configuration, privacy concerns, and potential resistance from website\noperators",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rapid growth of web content has led to increasingly large webpages,\nposing significant challenges for Internet affordability, especially in\ndeveloping countries where data costs remain prohibitively high. We propose\nsemantic caching using Large Language Models (LLMs) to improve web\naffordability by enabling reuse of semantically similar images within webpages.\nAnalyzing 50 leading news and media websites, encompassing 4,264 images and\nover 40,000 image pairs, we demonstrate potential for significant data transfer\nreduction, with some website categories showing up to 37% of images as\nreplaceable. Our proof-of-concept architecture shows users can achieve\napproximately 10% greater byte savings compared to exact caching. We evaluate\nboth commercial and open-source multi-modal LLMs for assessing semantic\nreplaceability. GPT-4o performs best with a low Normalized Root Mean Square\nError of 0.1735 and a weighted F1 score of 0.8374, while the open-source LLaMA\n3.1 model shows comparable performance, highlighting its viability for\nlarge-scale applications. This approach offers benefits for both users and\nwebsite operators, substantially reducing data transmission. We discuss ethical\nconcerns and practical challenges, including semantic preservation, user-driven\ncache configuration, privacy concerns, and potential resistance from website\noperators"
                },
                "authors": [
                    {
                        "name": "Hafsa Akbar"
                    },
                    {
                        "name": "Danish Athar"
                    },
                    {
                        "name": "Muhammad Ayain Fida Rana"
                    },
                    {
                        "name": "Chaudhary Hammad Javed"
                    },
                    {
                        "name": "Zartash Afzal Uzmi"
                    },
                    {
                        "name": "Ihsan Ayyub Qazi"
                    },
                    {
                        "name": "Zafar Ayyub Qazi"
                    }
                ],
                "author_detail": {
                    "name": "Zafar Ayyub Qazi"
                },
                "author": "Zafar Ayyub Qazi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.20420v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.20420v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "F.2.2, I.2.7",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.20283v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.20283v1",
                "updated": "2025-06-25T09:44:25Z",
                "updated_parsed": [
                    2025,
                    6,
                    25,
                    9,
                    44,
                    25,
                    2,
                    176,
                    0
                ],
                "published": "2025-06-25T09:44:25Z",
                "published_parsed": [
                    2025,
                    6,
                    25,
                    9,
                    44,
                    25,
                    2,
                    176,
                    0
                ],
                "title": "Do cell culturing influence the radiosensitizing effect of gold\n  nanoparticles part 2: scrutinizing the methodology producing recent evidence",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Do cell culturing influence the radiosensitizing effect of gold\n  nanoparticles part 2: scrutinizing the methodology producing recent evidence"
                },
                "summary": "When irradiation is performed with gold nanoparticles (AuNPs), a different\nshape of cells in suspension or adherent to walls may result in different\nprobability of cell survival. In a recent study, differences of up to a factor\nof 2 were found between the predicted survival of floating and adherent cells.\nThe present work aims to quantify the biases introduced by the simulation setup\nand the use of voxelized geometry in conjunction with the local effect model\nfor cell survival. The results show that simulated irradiation of a cell near\nthe surface with an incident beam matched to the cell dimensions results in\ndose values that are by a factor of about 50 lower than the dose to cells\ndeeper in the medium when irradiated with a Co-60 spectrum and lateral beam\ndimensions in the centimeter range. Furthermore, the number of ionizing photon\ninteractions in gold nanoparticles in a cell near the surface is lower by a\nfactor of about 2 than for cells at 5 mm and 1 cm depth. Using the average dose\nin voxels of size in the order of 200 nm for assessing cell survival with the\nlocal effect model (LEM) leads to an underestimation of the number of lesions\nfrom a single ionized AuNP by roughly two orders of magnitude and thus to an\noverestimation of cell survival. The effect of cell geometry on the survival\nrate was examined for approximate cell geometries and 100 kV x-ray irradiation,\nfor which the probability of photon interaction in gold nanoparticles is by\nmore than two orders of magnitude higher than for Co-60 irradiation. The\nresults show that the effects are negligible for 5 nm nanoparticles at the\nconcentration of AuNPs considered in preceding work. For 50 nm nanoparticles\nand thus a thousand times higher mass fraction of gold, significant reduction\nin cell survival is found, with a clear additional reduction predicted by the\nLEM as compared to the prediction based on mean dose to the nucleus.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "When irradiation is performed with gold nanoparticles (AuNPs), a different\nshape of cells in suspension or adherent to walls may result in different\nprobability of cell survival. In a recent study, differences of up to a factor\nof 2 were found between the predicted survival of floating and adherent cells.\nThe present work aims to quantify the biases introduced by the simulation setup\nand the use of voxelized geometry in conjunction with the local effect model\nfor cell survival. The results show that simulated irradiation of a cell near\nthe surface with an incident beam matched to the cell dimensions results in\ndose values that are by a factor of about 50 lower than the dose to cells\ndeeper in the medium when irradiated with a Co-60 spectrum and lateral beam\ndimensions in the centimeter range. Furthermore, the number of ionizing photon\ninteractions in gold nanoparticles in a cell near the surface is lower by a\nfactor of about 2 than for cells at 5 mm and 1 cm depth. Using the average dose\nin voxels of size in the order of 200 nm for assessing cell survival with the\nlocal effect model (LEM) leads to an underestimation of the number of lesions\nfrom a single ionized AuNP by roughly two orders of magnitude and thus to an\noverestimation of cell survival. The effect of cell geometry on the survival\nrate was examined for approximate cell geometries and 100 kV x-ray irradiation,\nfor which the probability of photon interaction in gold nanoparticles is by\nmore than two orders of magnitude higher than for Co-60 irradiation. The\nresults show that the effects are negligible for 5 nm nanoparticles at the\nconcentration of AuNPs considered in preceding work. For 50 nm nanoparticles\nand thus a thousand times higher mass fraction of gold, significant reduction\nin cell survival is found, with a clear additional reduction predicted by the\nLEM as compared to the prediction based on mean dose to the nucleus."
                },
                "authors": [
                    {
                        "name": "Hans Rabus"
                    },
                    {
                        "name": "Oswald Msosa Mkanda"
                    }
                ],
                "author_detail": {
                    "name": "Oswald Msosa Mkanda"
                },
                "author": "Oswald Msosa Mkanda",
                "arxiv_comment": "16 pages, 6+1 Figs., 3+1 Tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.20283v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.20283v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.med-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.med-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.20686v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.20686v1",
                "updated": "2025-06-24T23:30:49Z",
                "updated_parsed": [
                    2025,
                    6,
                    24,
                    23,
                    30,
                    49,
                    1,
                    175,
                    0
                ],
                "published": "2025-06-24T23:30:49Z",
                "published_parsed": [
                    2025,
                    6,
                    24,
                    23,
                    30,
                    49,
                    1,
                    175,
                    0
                ],
                "title": "MegaFold: System-Level Optimizations for Accelerating Protein Structure\n  Prediction Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MegaFold: System-Level Optimizations for Accelerating Protein Structure\n  Prediction Models"
                },
                "summary": "Protein structure prediction models such as AlphaFold3 (AF3) push the\nfrontier of biomolecular modeling by incorporating science-informed\narchitectural changes to the transformer architecture. However, these advances\ncome at a steep system cost, introducing: compute- and memory-intensive\noperators, 2D attention mechanisms, and retrieval-augmented data pipelines,\nwhich collectively hinder the scalability of AF3 training. In this work, we\npresent MegaFold, a cross-platform system to accelerate AF3 training. MegaFold\ntackles key bottlenecks through ahead-of-time caching to eliminate GPU idle\ntime from the retrieval-augmented data pipeline, Triton-based kernels for\nmemory-efficient EvoAttention on heterogeneous devices, and deep fusion for\ncommon and critical small operators in AF3. Evaluation on both NVIDIA H200 and\nAMD MI250 GPUs shows that MegaFold reduces peak memory usage of AF3 training by\nup to 1.23$\\times$ and improves per-iteration training time by up-to\n1.73$\\times$ and 1.62$\\times$ respectively. More importantly, MegaFold enables\ntraining on 1.35$\\times$ longer sequence lengths compared to PyTorch baselines\nwithout running out-of-memory, significantly improving the scalability of\nmodern protein folding models. We open source our code at\nhttps://github.com/Supercomputing-System-AI-Lab/MegaFold/.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Protein structure prediction models such as AlphaFold3 (AF3) push the\nfrontier of biomolecular modeling by incorporating science-informed\narchitectural changes to the transformer architecture. However, these advances\ncome at a steep system cost, introducing: compute- and memory-intensive\noperators, 2D attention mechanisms, and retrieval-augmented data pipelines,\nwhich collectively hinder the scalability of AF3 training. In this work, we\npresent MegaFold, a cross-platform system to accelerate AF3 training. MegaFold\ntackles key bottlenecks through ahead-of-time caching to eliminate GPU idle\ntime from the retrieval-augmented data pipeline, Triton-based kernels for\nmemory-efficient EvoAttention on heterogeneous devices, and deep fusion for\ncommon and critical small operators in AF3. Evaluation on both NVIDIA H200 and\nAMD MI250 GPUs shows that MegaFold reduces peak memory usage of AF3 training by\nup to 1.23$\\times$ and improves per-iteration training time by up-to\n1.73$\\times$ and 1.62$\\times$ respectively. More importantly, MegaFold enables\ntraining on 1.35$\\times$ longer sequence lengths compared to PyTorch baselines\nwithout running out-of-memory, significantly improving the scalability of\nmodern protein folding models. We open source our code at\nhttps://github.com/Supercomputing-System-AI-Lab/MegaFold/."
                },
                "authors": [
                    {
                        "name": "Hoa La"
                    },
                    {
                        "name": "Ahan Gupta"
                    },
                    {
                        "name": "Alex Morehead"
                    },
                    {
                        "name": "Jianlin Cheng"
                    },
                    {
                        "name": "Minjia Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Minjia Zhang"
                },
                "author": "Minjia Zhang",
                "arxiv_comment": "13 pages, 12 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.20686v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.20686v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "q-bio.BM",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "q-bio.BM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.14866v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.14866v4",
                "updated": "2025-06-24T19:02:08Z",
                "updated_parsed": [
                    2025,
                    6,
                    24,
                    19,
                    2,
                    8,
                    1,
                    175,
                    0
                ],
                "published": "2025-04-21T05:27:33Z",
                "published_parsed": [
                    2025,
                    4,
                    21,
                    5,
                    27,
                    33,
                    0,
                    111,
                    0
                ],
                "title": "GainSight: Application-Guided Profiling for Composing Heterogeneous\n  On-Chip Memories in AI Hardware Accelerators",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GainSight: Application-Guided Profiling for Composing Heterogeneous\n  On-Chip Memories in AI Hardware Accelerators"
                },
                "summary": "As AI workloads drive soaring memory requirements, higher-density on-chip\nmemory is needed for domain-specific accelerators beyond what current SRAM\ntechnology can provide. We motivate that algorithms and application behavior\nshould guide the composition of heterogeneous on-chip memories. However, little\nwork has incorporated dynamic application profiles into these design decisions,\nand no existing tools are expressly designed for this purpose. We present\nGainSight, a profiling framework that analyzes fine-grained memory access\npatterns and data lifetimes in domain-specific accelerators. By instrumenting\nretargetable architectural simulator backends with application- and\ndevice-agnostic analytical frontends, GainSight aligns workload-specific\ntraffic and lifetime metrics with mockups of emerging memory devices, informing\nsystem-level heterogeneous memory design. We also present a set of case studies\non MLPerf Inference and PolyBench workloads using simulated GPU and systolic\narray architectures, highlighting the utility of GainSight and the insights it\nprovides: (1) 64% of L1 and 18% of L2 GPU cache accesses, and 79% of systolic\narray scratchpad accesses across profiled workloads are short-lived and\nsuitable for silicon-based gain cell RAM (Si-GCRAM); (2) Heterogeneous memory\narrays that augment SRAM with GCRAM can reduce active energy consumption by up\nto 66.8%. To facilitate further research in this domain, GainSight is open\nsource at https://gainsight.stanford.edu/.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As AI workloads drive soaring memory requirements, higher-density on-chip\nmemory is needed for domain-specific accelerators beyond what current SRAM\ntechnology can provide. We motivate that algorithms and application behavior\nshould guide the composition of heterogeneous on-chip memories. However, little\nwork has incorporated dynamic application profiles into these design decisions,\nand no existing tools are expressly designed for this purpose. We present\nGainSight, a profiling framework that analyzes fine-grained memory access\npatterns and data lifetimes in domain-specific accelerators. By instrumenting\nretargetable architectural simulator backends with application- and\ndevice-agnostic analytical frontends, GainSight aligns workload-specific\ntraffic and lifetime metrics with mockups of emerging memory devices, informing\nsystem-level heterogeneous memory design. We also present a set of case studies\non MLPerf Inference and PolyBench workloads using simulated GPU and systolic\narray architectures, highlighting the utility of GainSight and the insights it\nprovides: (1) 64% of L1 and 18% of L2 GPU cache accesses, and 79% of systolic\narray scratchpad accesses across profiled workloads are short-lived and\nsuitable for silicon-based gain cell RAM (Si-GCRAM); (2) Heterogeneous memory\narrays that augment SRAM with GCRAM can reduce active energy consumption by up\nto 66.8%. To facilitate further research in this domain, GainSight is open\nsource at https://gainsight.stanford.edu/."
                },
                "authors": [
                    {
                        "name": "Peijing Li"
                    },
                    {
                        "name": "Matthew Hung"
                    },
                    {
                        "name": "Yiming Tan"
                    },
                    {
                        "name": "Konstantin HoÃfeld"
                    },
                    {
                        "name": "Jake Cheng Jiajun"
                    },
                    {
                        "name": "Shuhan Liu"
                    },
                    {
                        "name": "Lixian Yan"
                    },
                    {
                        "name": "Xinxin Wang"
                    },
                    {
                        "name": "H. -S. Philip Wong"
                    },
                    {
                        "name": "Thierry Tambe"
                    }
                ],
                "author_detail": {
                    "name": "Thierry Tambe"
                },
                "author": "Thierry Tambe",
                "arxiv_comment": "16 pages, 10 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.14866v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.14866v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.ET",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "B.7.1; B.3.1; C.3; I.6; I.2.6",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.19816v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.19816v1",
                "updated": "2025-06-24T17:30:27Z",
                "updated_parsed": [
                    2025,
                    6,
                    24,
                    17,
                    30,
                    27,
                    1,
                    175,
                    0
                ],
                "published": "2025-06-24T17:30:27Z",
                "published_parsed": [
                    2025,
                    6,
                    24,
                    17,
                    30,
                    27,
                    1,
                    175,
                    0
                ],
                "title": "CronusVLA: Transferring Latent Motion Across Time for Multi-Frame\n  Prediction in Manipulation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CronusVLA: Transferring Latent Motion Across Time for Multi-Frame\n  Prediction in Manipulation"
                },
                "summary": "Recent vision-language-action (VLA) models built on pretrained\nvision-language models (VLMs) have demonstrated strong generalization across\nmanipulation tasks. However, they remain constrained by a single-frame\nobservation paradigm and cannot fully benefit from the motion information\noffered by aggregated multi-frame historical observations, as the large\nvision-language backbone introduces substantial computational cost and\ninference latency. We propose CronusVLA, a unified framework that extends\nsingle-frame VLA models to the multi-frame paradigm through an efficient\npost-training stage. CronusVLA comprises three key components: (1) single-frame\npretraining on large-scale embodied datasets with autoregressive action tokens\nprediction, which establishes an embodied vision-language foundation; (2)\nmulti-frame encoding, adapting the prediction of vision-language backbones from\ndiscrete action tokens to motion features during post-training, and aggregating\nmotion features from historical frames into a feature chunking; (3) cross-frame\ndecoding, which maps the feature chunking to accurate actions via a shared\ndecoder with cross-attention. By reducing redundant token computation and\ncaching past motion features, CronusVLA achieves efficient inference. As an\napplication of motion features, we further propose an action adaptation\nmechanism based on feature-action retrieval to improve model performance during\nfinetuning. CronusVLA achieves state-of-the-art performance on SimplerEnv with\n70.9% success rate, and 12.7% improvement over OpenVLA on LIBERO. Real-world\nFranka experiments also show the strong performance and robustness.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent vision-language-action (VLA) models built on pretrained\nvision-language models (VLMs) have demonstrated strong generalization across\nmanipulation tasks. However, they remain constrained by a single-frame\nobservation paradigm and cannot fully benefit from the motion information\noffered by aggregated multi-frame historical observations, as the large\nvision-language backbone introduces substantial computational cost and\ninference latency. We propose CronusVLA, a unified framework that extends\nsingle-frame VLA models to the multi-frame paradigm through an efficient\npost-training stage. CronusVLA comprises three key components: (1) single-frame\npretraining on large-scale embodied datasets with autoregressive action tokens\nprediction, which establishes an embodied vision-language foundation; (2)\nmulti-frame encoding, adapting the prediction of vision-language backbones from\ndiscrete action tokens to motion features during post-training, and aggregating\nmotion features from historical frames into a feature chunking; (3) cross-frame\ndecoding, which maps the feature chunking to accurate actions via a shared\ndecoder with cross-attention. By reducing redundant token computation and\ncaching past motion features, CronusVLA achieves efficient inference. As an\napplication of motion features, we further propose an action adaptation\nmechanism based on feature-action retrieval to improve model performance during\nfinetuning. CronusVLA achieves state-of-the-art performance on SimplerEnv with\n70.9% success rate, and 12.7% improvement over OpenVLA on LIBERO. Real-world\nFranka experiments also show the strong performance and robustness."
                },
                "authors": [
                    {
                        "name": "Hao Li"
                    },
                    {
                        "name": "Shuai Yang"
                    },
                    {
                        "name": "Yilun Chen"
                    },
                    {
                        "name": "Yang Tian"
                    },
                    {
                        "name": "Xiaoda Yang"
                    },
                    {
                        "name": "Xinyi Chen"
                    },
                    {
                        "name": "Hanqing Wang"
                    },
                    {
                        "name": "Tai Wang"
                    },
                    {
                        "name": "Feng Zhao"
                    },
                    {
                        "name": "Dahua Lin"
                    },
                    {
                        "name": "Jiangmiao Pang"
                    }
                ],
                "author_detail": {
                    "name": "Jiangmiao Pang"
                },
                "author": "Jiangmiao Pang",
                "arxiv_comment": "36 pages, 21 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.19816v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.19816v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.19549v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.19549v1",
                "updated": "2025-06-24T11:55:43Z",
                "updated_parsed": [
                    2025,
                    6,
                    24,
                    11,
                    55,
                    43,
                    1,
                    175,
                    0
                ],
                "published": "2025-06-24T11:55:43Z",
                "published_parsed": [
                    2025,
                    6,
                    24,
                    11,
                    55,
                    43,
                    1,
                    175,
                    0
                ],
                "title": "RCStat: A Statistical Framework for using Relative Contextualization in\n  Transformers",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RCStat: A Statistical Framework for using Relative Contextualization in\n  Transformers"
                },
                "summary": "Prior work on input-token importance in auto-regressive transformers has\nrelied on Softmax-normalized attention weights, which obscure the richer\nstructure of pre-Softmax query-key logits. We introduce RCStat, a statistical\nframework that harnesses raw attention logits via Relative Contextualization\n(RC), a random variable measuring contextual alignment between token segments,\nand derive an efficient upper bound for RC. We demonstrate two applications:\n(i) Key-Value compression, where RC-based thresholds drive adaptive key-value\neviction for substantial cache reduction with minimal quality loss; and (ii)\nAttribution, where RC yields higher-fidelity token-, sentence-, and chunk-level\nexplanations than post-Softmax methods. Across question answering,\nsummarization, and attribution benchmarks, RCStat achieves significant\nempirical gains, delivering state-of-the-art compression and attribution\nperformance without any model retraining.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Prior work on input-token importance in auto-regressive transformers has\nrelied on Softmax-normalized attention weights, which obscure the richer\nstructure of pre-Softmax query-key logits. We introduce RCStat, a statistical\nframework that harnesses raw attention logits via Relative Contextualization\n(RC), a random variable measuring contextual alignment between token segments,\nand derive an efficient upper bound for RC. We demonstrate two applications:\n(i) Key-Value compression, where RC-based thresholds drive adaptive key-value\neviction for substantial cache reduction with minimal quality loss; and (ii)\nAttribution, where RC yields higher-fidelity token-, sentence-, and chunk-level\nexplanations than post-Softmax methods. Across question answering,\nsummarization, and attribution benchmarks, RCStat achieves significant\nempirical gains, delivering state-of-the-art compression and attribution\nperformance without any model retraining."
                },
                "authors": [
                    {
                        "name": "Debabrata Mahapatra"
                    },
                    {
                        "name": "Shubham Agarwal"
                    },
                    {
                        "name": "Apoorv Saxena"
                    },
                    {
                        "name": "Subrata Mitra"
                    }
                ],
                "author_detail": {
                    "name": "Subrata Mitra"
                },
                "author": "Subrata Mitra",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.19549v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.19549v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.19505v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.19505v1",
                "updated": "2025-06-24T10:45:48Z",
                "updated_parsed": [
                    2025,
                    6,
                    24,
                    10,
                    45,
                    48,
                    1,
                    175,
                    0
                ],
                "published": "2025-06-24T10:45:48Z",
                "published_parsed": [
                    2025,
                    6,
                    24,
                    10,
                    45,
                    48,
                    1,
                    175,
                    0
                ],
                "title": "AnTKV: Anchor Token-Aware Sub-Bit Vector Quantization for KV Cache in\n  Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AnTKV: Anchor Token-Aware Sub-Bit Vector Quantization for KV Cache in\n  Large Language Models"
                },
                "summary": "Quantization has emerged as an effective and lightweight solution to reduce\nthe memory footprint of the KV cache in Large Language Models (LLMs).\nNevertheless, minimizing the performance degradation caused by ultra-low-bit KV\ncache quantization remains a significant challenge. We observe that quantizing\nthe KV cache of different tokens has varying impacts on the quality of\nattention outputs. To systematically investigate this phenomenon, we perform\nforward error propagation analysis on attention and propose the Anchor Score\n(AnS) that quantifies the sensitivity of each token's KV cache to\nquantization-induced error. Our analysis reveals significant disparities in AnS\nacross tokens, suggesting that preserving a small subset with full precision\n(FP16) of high-AnS tokens can greatly mitigate accuracy loss in aggressive\nquantization scenarios. Based on this insight, we introduce AnTKV, a novel\nframework that leverages Anchor Token-aware Vector Quantization to compress the\nKV cache. Furthermore, to support efficient deployment, we design and develop a\ntriton kernel that is fully compatible with FlashAttention, enabling fast\nonline Anchor Token selection. AnTKV enables LLaMA-3-8B to handle context\nlengths up to 840K tokens on a single 80GB A100 GPU, while achieving up to 3.5x\nhigher decoding throughput compared to the FP16 baseline. Our experiment\nresults demonstrate that AnTKV matches or outperforms prior works such as KIVI,\nSKVQ, KVQuant, and CQ under 4-bit settings. More importantly, AnTKV achieves\nsignificantly lower perplexity under ultra-low-bit quantization on Mistral-7B,\nwith only 6.32 at 1-bit and 8.87 at 0.375-bit, compared to the FP16 baseline of\n4.73.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Quantization has emerged as an effective and lightweight solution to reduce\nthe memory footprint of the KV cache in Large Language Models (LLMs).\nNevertheless, minimizing the performance degradation caused by ultra-low-bit KV\ncache quantization remains a significant challenge. We observe that quantizing\nthe KV cache of different tokens has varying impacts on the quality of\nattention outputs. To systematically investigate this phenomenon, we perform\nforward error propagation analysis on attention and propose the Anchor Score\n(AnS) that quantifies the sensitivity of each token's KV cache to\nquantization-induced error. Our analysis reveals significant disparities in AnS\nacross tokens, suggesting that preserving a small subset with full precision\n(FP16) of high-AnS tokens can greatly mitigate accuracy loss in aggressive\nquantization scenarios. Based on this insight, we introduce AnTKV, a novel\nframework that leverages Anchor Token-aware Vector Quantization to compress the\nKV cache. Furthermore, to support efficient deployment, we design and develop a\ntriton kernel that is fully compatible with FlashAttention, enabling fast\nonline Anchor Token selection. AnTKV enables LLaMA-3-8B to handle context\nlengths up to 840K tokens on a single 80GB A100 GPU, while achieving up to 3.5x\nhigher decoding throughput compared to the FP16 baseline. Our experiment\nresults demonstrate that AnTKV matches or outperforms prior works such as KIVI,\nSKVQ, KVQuant, and CQ under 4-bit settings. More importantly, AnTKV achieves\nsignificantly lower perplexity under ultra-low-bit quantization on Mistral-7B,\nwith only 6.32 at 1-bit and 8.87 at 0.375-bit, compared to the FP16 baseline of\n4.73."
                },
                "authors": [
                    {
                        "name": "Zeyu Li"
                    },
                    {
                        "name": "Chuanfu Xiao"
                    },
                    {
                        "name": "Yang Wang"
                    },
                    {
                        "name": "Xiang Liu"
                    },
                    {
                        "name": "Zhenheng Tang"
                    },
                    {
                        "name": "Baotong Lu"
                    },
                    {
                        "name": "Mao Yang"
                    },
                    {
                        "name": "Xinyu Chen"
                    },
                    {
                        "name": "Xiaowen Chu"
                    }
                ],
                "author_detail": {
                    "name": "Xiaowen Chu"
                },
                "author": "Xiaowen Chu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.19505v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.19505v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.00099v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.00099v2",
                "updated": "2025-06-24T09:27:46Z",
                "updated_parsed": [
                    2025,
                    6,
                    24,
                    9,
                    27,
                    46,
                    1,
                    175,
                    0
                ],
                "published": "2024-11-27T18:59:48Z",
                "published_parsed": [
                    2024,
                    11,
                    27,
                    18,
                    59,
                    48,
                    2,
                    332,
                    0
                ],
                "title": "Mixture of Cache-Conditional Experts for Efficient Mobile Device\n  Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mixture of Cache-Conditional Experts for Efficient Mobile Device\n  Inference"
                },
                "summary": "Mixture of Experts (MoE) LLMs have recently gained attention for their\nability to enhance performance by selectively engaging specialized subnetworks\nor \"experts\" for each input. However, deploying MoEs on memory-constrained\ndevices remains challenging, particularly when generating tokens sequentially\nwith a batch size of one, as opposed to typical high-throughput settings\ninvolving long sequences or large batches. In this work, we optimize MoE on\nmemory-constrained devices where only a subset of expert weights fit in DRAM.\nWe introduce a novel cache-aware routing strategy that leverages expert reuse\nduring token generation to improve cache locality. We evaluate our approach on\nlanguage modeling, MMLU, and GSM8K benchmarks and present on-device results\ndemonstrating 2$\\times$ speedups on mobile devices, offering a flexible,\ntraining-free solution to extend MoE's applicability across real-world\napplications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mixture of Experts (MoE) LLMs have recently gained attention for their\nability to enhance performance by selectively engaging specialized subnetworks\nor \"experts\" for each input. However, deploying MoEs on memory-constrained\ndevices remains challenging, particularly when generating tokens sequentially\nwith a batch size of one, as opposed to typical high-throughput settings\ninvolving long sequences or large batches. In this work, we optimize MoE on\nmemory-constrained devices where only a subset of expert weights fit in DRAM.\nWe introduce a novel cache-aware routing strategy that leverages expert reuse\nduring token generation to improve cache locality. We evaluate our approach on\nlanguage modeling, MMLU, and GSM8K benchmarks and present on-device results\ndemonstrating 2$\\times$ speedups on mobile devices, offering a flexible,\ntraining-free solution to extend MoE's applicability across real-world\napplications."
                },
                "authors": [
                    {
                        "name": "Andrii Skliar"
                    },
                    {
                        "name": "Ties van Rozendaal"
                    },
                    {
                        "name": "Romain Lepert"
                    },
                    {
                        "name": "Todor Boinovski"
                    },
                    {
                        "name": "Mart van Baalen"
                    },
                    {
                        "name": "Markus Nagel"
                    },
                    {
                        "name": "Paul Whatmough"
                    },
                    {
                        "name": "Babak Ehteshami Bejnordi"
                    }
                ],
                "author_detail": {
                    "name": "Babak Ehteshami Bejnordi"
                },
                "author": "Babak Ehteshami Bejnordi",
                "arxiv_comment": "Published in Transactions on Machine Learning Research (06/2025)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.00099v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.00099v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.19433v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.19433v1",
                "updated": "2025-06-24T09:00:43Z",
                "updated_parsed": [
                    2025,
                    6,
                    24,
                    9,
                    0,
                    43,
                    1,
                    175,
                    0
                ],
                "published": "2025-06-24T09:00:43Z",
                "published_parsed": [
                    2025,
                    6,
                    24,
                    9,
                    0,
                    43,
                    1,
                    175,
                    0
                ],
                "title": "Mem4Nav: Boosting Vision-and-Language Navigation in Urban Environments\n  with a Hierarchical Spatial-Cognition Long-Short Memory System",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mem4Nav: Boosting Vision-and-Language Navigation in Urban Environments\n  with a Hierarchical Spatial-Cognition Long-Short Memory System"
                },
                "summary": "Vision-and-Language Navigation (VLN) in large-scale urban environments\nrequires embodied agents to ground linguistic instructions in complex scenes\nand recall relevant experiences over extended time horizons. Prior modular\npipelines offer interpretability but lack unified memory, while end-to-end\n(M)LLM agents excel at fusing vision and language yet remain constrained by\nfixed context windows and implicit spatial reasoning. We introduce\n\\textbf{Mem4Nav}, a hierarchical spatial-cognition long-short memory system\nthat can augment any VLN backbone. Mem4Nav fuses a sparse octree for\nfine-grained voxel indexing with a semantic topology graph for high-level\nlandmark connectivity, storing both in trainable memory tokens embedded via a\nreversible Transformer. Long-term memory (LTM) compresses and retains\nhistorical observations at both octree and graph nodes, while short-term memory\n(STM) caches recent multimodal entries in relative coordinates for real-time\nobstacle avoidance and local planning. At each step, STM retrieval sharply\nprunes dynamic context, and, when deeper history is needed, LTM tokens are\ndecoded losslessly to reconstruct past embeddings. Evaluated on Touchdown and\nMap2Seq across three backbones (modular, state-of-the-art VLN with prompt-based\nLLM, and state-of-the-art VLN with strided-attention MLLM), Mem4Nav yields 7-13\npp gains in Task Completion, sufficient SPD reduction, and >10 pp nDTW\nimprovement. Ablations confirm the indispensability of both the hierarchical\nmap and dual memory modules. Our codes are open-sourced via\nhttps://github.com/tsinghua-fib-lab/Mem4Nav.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Vision-and-Language Navigation (VLN) in large-scale urban environments\nrequires embodied agents to ground linguistic instructions in complex scenes\nand recall relevant experiences over extended time horizons. Prior modular\npipelines offer interpretability but lack unified memory, while end-to-end\n(M)LLM agents excel at fusing vision and language yet remain constrained by\nfixed context windows and implicit spatial reasoning. We introduce\n\\textbf{Mem4Nav}, a hierarchical spatial-cognition long-short memory system\nthat can augment any VLN backbone. Mem4Nav fuses a sparse octree for\nfine-grained voxel indexing with a semantic topology graph for high-level\nlandmark connectivity, storing both in trainable memory tokens embedded via a\nreversible Transformer. Long-term memory (LTM) compresses and retains\nhistorical observations at both octree and graph nodes, while short-term memory\n(STM) caches recent multimodal entries in relative coordinates for real-time\nobstacle avoidance and local planning. At each step, STM retrieval sharply\nprunes dynamic context, and, when deeper history is needed, LTM tokens are\ndecoded losslessly to reconstruct past embeddings. Evaluated on Touchdown and\nMap2Seq across three backbones (modular, state-of-the-art VLN with prompt-based\nLLM, and state-of-the-art VLN with strided-attention MLLM), Mem4Nav yields 7-13\npp gains in Task Completion, sufficient SPD reduction, and >10 pp nDTW\nimprovement. Ablations confirm the indispensability of both the hierarchical\nmap and dual memory modules. Our codes are open-sourced via\nhttps://github.com/tsinghua-fib-lab/Mem4Nav."
                },
                "authors": [
                    {
                        "name": "Lixuan He"
                    },
                    {
                        "name": "Haoyu Dong"
                    },
                    {
                        "name": "Zhenxing Chen"
                    },
                    {
                        "name": "Yangcheng Yu"
                    },
                    {
                        "name": "Jie Feng"
                    },
                    {
                        "name": "Yong Li"
                    }
                ],
                "author_detail": {
                    "name": "Yong Li"
                },
                "author": "Yong Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.19433v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.19433v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.17338v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.17338v2",
                "updated": "2025-06-24T06:44:47Z",
                "updated_parsed": [
                    2025,
                    6,
                    24,
                    6,
                    44,
                    47,
                    1,
                    175,
                    0
                ],
                "published": "2025-06-19T08:28:29Z",
                "published_parsed": [
                    2025,
                    6,
                    19,
                    8,
                    28,
                    29,
                    3,
                    170,
                    0
                ],
                "title": "PBFT-Backed Semantic Voting for Multi-Agent Memory Pruning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PBFT-Backed Semantic Voting for Multi-Agent Memory Pruning"
                },
                "summary": "The proliferation of multi-agent systems (MAS) in complex, dynamic\nenvironments necessitates robust and efficient mechanisms for managing shared\nknowledge. A critical challenge is ensuring that distributed memories remain\nsynchronized, relevant, and free from the accumulation of outdated or\ninconsequential data - a process analogous to biological forgetting. This paper\nintroduces the Co-Forgetting Protocol, a novel, comprehensive framework\ndesigned to address this challenge by enabling synchronized memory pruning in\nMAS. The protocol integrates three key components: (1) context-aware semantic\nvoting, where agents utilize a lightweight DistilBERT model to assess the\nrelevance of memory items based on their content and the current operational\ncontext; (2) multi-scale temporal decay functions, which assign diminishing\nimportance to memories based on their age and access frequency across different\ntime horizons; and (3) a Practical Byzantine Fault Tolerance (PBFT)-based\nconsensus mechanism, ensuring that decisions to retain or discard memory items\nare agreed upon by a qualified and fault-tolerant majority of agents, even in\nthe presence of up to f Byzantine (malicious or faulty) agents in a system of N\ngreater than or equal to 3f+1 agents. The protocol leverages gRPC for efficient\ninter-agent communication and Pinecone for scalable vector embedding storage\nand similarity search, with SQLite managing metadata. Experimental evaluations\nin a simulated MAS environment with four agents demonstrate the protocol's\nefficacy, achieving a 52% reduction in memory footprint over 500 epochs, 88%\nvoting accuracy in forgetting decisions against human-annotated benchmarks, a\n92% PBFT consensus success rate under simulated Byzantine conditions, and an\n82% cache hit rate for memory access.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The proliferation of multi-agent systems (MAS) in complex, dynamic\nenvironments necessitates robust and efficient mechanisms for managing shared\nknowledge. A critical challenge is ensuring that distributed memories remain\nsynchronized, relevant, and free from the accumulation of outdated or\ninconsequential data - a process analogous to biological forgetting. This paper\nintroduces the Co-Forgetting Protocol, a novel, comprehensive framework\ndesigned to address this challenge by enabling synchronized memory pruning in\nMAS. The protocol integrates three key components: (1) context-aware semantic\nvoting, where agents utilize a lightweight DistilBERT model to assess the\nrelevance of memory items based on their content and the current operational\ncontext; (2) multi-scale temporal decay functions, which assign diminishing\nimportance to memories based on their age and access frequency across different\ntime horizons; and (3) a Practical Byzantine Fault Tolerance (PBFT)-based\nconsensus mechanism, ensuring that decisions to retain or discard memory items\nare agreed upon by a qualified and fault-tolerant majority of agents, even in\nthe presence of up to f Byzantine (malicious or faulty) agents in a system of N\ngreater than or equal to 3f+1 agents. The protocol leverages gRPC for efficient\ninter-agent communication and Pinecone for scalable vector embedding storage\nand similarity search, with SQLite managing metadata. Experimental evaluations\nin a simulated MAS environment with four agents demonstrate the protocol's\nefficacy, achieving a 52% reduction in memory footprint over 500 epochs, 88%\nvoting accuracy in forgetting decisions against human-annotated benchmarks, a\n92% PBFT consensus success rate under simulated Byzantine conditions, and an\n82% cache hit rate for memory access."
                },
                "authors": [
                    {
                        "name": "Duong Bach"
                    }
                ],
                "author_detail": {
                    "name": "Duong Bach"
                },
                "author": "Duong Bach",
                "arxiv_comment": "13 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.17338v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.17338v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.19225v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.19225v1",
                "updated": "2025-06-24T01:19:56Z",
                "updated_parsed": [
                    2025,
                    6,
                    24,
                    1,
                    19,
                    56,
                    1,
                    175,
                    0
                ],
                "published": "2025-06-24T01:19:56Z",
                "published_parsed": [
                    2025,
                    6,
                    24,
                    1,
                    19,
                    56,
                    1,
                    175,
                    0
                ],
                "title": "Video-XL-2: Towards Very Long-Video Understanding Through Task-Aware KV\n  Sparsification",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Video-XL-2: Towards Very Long-Video Understanding Through Task-Aware KV\n  Sparsification"
                },
                "summary": "Multi-modal large language models (MLLMs) models have made significant\nprogress in video understanding over the past few years. However, processing\nlong video inputs remains a major challenge due to high memory and\ncomputational costs. This makes it difficult for current models to achieve both\nstrong performance and high efficiency in long video understanding. To address\nthis challenge, we propose Video-XL-2, a novel MLLM that delivers superior\ncost-effectiveness for long-video understanding based on task-aware KV\nsparsification. The proposed framework operates with two key steps: chunk-based\npre-filling and bi-level key-value decoding. Chunk-based pre-filling divides\nthe visual token sequence into chunks, applying full attention within each\nchunk and sparse attention across chunks. This significantly reduces\ncomputational and memory overhead. During decoding, bi-level key-value decoding\nselectively reloads either dense or sparse key-values for each chunk based on\nits relevance to the task. This approach further improves memory efficiency and\nenhances the model's ability to capture fine-grained information. Video-XL-2\nachieves state-of-the-art performance on various long video understanding\nbenchmarks, outperforming existing open-source lightweight models. It also\ndemonstrates exceptional efficiency, capable of processing over 10,000 frames\non a single NVIDIA A100 (80GB) GPU and thousands of frames in just a few\nseconds.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-modal large language models (MLLMs) models have made significant\nprogress in video understanding over the past few years. However, processing\nlong video inputs remains a major challenge due to high memory and\ncomputational costs. This makes it difficult for current models to achieve both\nstrong performance and high efficiency in long video understanding. To address\nthis challenge, we propose Video-XL-2, a novel MLLM that delivers superior\ncost-effectiveness for long-video understanding based on task-aware KV\nsparsification. The proposed framework operates with two key steps: chunk-based\npre-filling and bi-level key-value decoding. Chunk-based pre-filling divides\nthe visual token sequence into chunks, applying full attention within each\nchunk and sparse attention across chunks. This significantly reduces\ncomputational and memory overhead. During decoding, bi-level key-value decoding\nselectively reloads either dense or sparse key-values for each chunk based on\nits relevance to the task. This approach further improves memory efficiency and\nenhances the model's ability to capture fine-grained information. Video-XL-2\nachieves state-of-the-art performance on various long video understanding\nbenchmarks, outperforming existing open-source lightweight models. It also\ndemonstrates exceptional efficiency, capable of processing over 10,000 frames\non a single NVIDIA A100 (80GB) GPU and thousands of frames in just a few\nseconds."
                },
                "authors": [
                    {
                        "name": "Minghao Qin"
                    },
                    {
                        "name": "Xiangrui Liu"
                    },
                    {
                        "name": "Zhengyang Liang"
                    },
                    {
                        "name": "Yan Shu"
                    },
                    {
                        "name": "Huaying Yuan"
                    },
                    {
                        "name": "Juenjie Zhou"
                    },
                    {
                        "name": "Shitao Xiao"
                    },
                    {
                        "name": "Bo Zhao"
                    },
                    {
                        "name": "Zheng Liu"
                    }
                ],
                "author_detail": {
                    "name": "Zheng Liu"
                },
                "author": "Zheng Liu",
                "arxiv_comment": "12 pages, 5 Figure, 3 Table",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.19225v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.19225v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.19175v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.19175v1",
                "updated": "2025-06-23T22:33:58Z",
                "updated_parsed": [
                    2025,
                    6,
                    23,
                    22,
                    33,
                    58,
                    0,
                    174,
                    0
                ],
                "published": "2025-06-23T22:33:58Z",
                "published_parsed": [
                    2025,
                    6,
                    23,
                    22,
                    33,
                    58,
                    0,
                    174,
                    0
                ],
                "title": "Binsparse: A Specification for Cross-Platform Storage of Sparse Matrices\n  and Tensors",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Binsparse: A Specification for Cross-Platform Storage of Sparse Matrices\n  and Tensors"
                },
                "summary": "Sparse matrices and tensors are ubiquitous throughout multiple subfields of\ncomputing. The widespread usage of sparse data has inspired many in-memory and\non-disk storage formats, but the only widely adopted storage specifications are\nthe Matrix Market and FROSTT file formats, which both use ASCII text. Due to\nthe inefficiency of text storage, these files typically have larger file sizes\nand longer parsing times than binary storage formats, which directly store an\nin-memory representation to disk. This can be a major bottleneck; since sparse\ncomputation is often bandwidth-bound, the cost of loading or storing a matrix\nto disk often exceeds the cost of performing a sparse computation. While it is\ncommon practice for practitioners to develop their own, custom, non-portable\nbinary formats for high-performance sparse matrix storage, there is currently\nno cross-platform binary sparse matrix storage format. We present Binsparse, a\ncross-platform binary sparse matrix and tensor format specification. Binsparse\nis a modular, embeddable format, consisting of a JSON descriptor, which\ndescribes the matrix or tensor dimensions, type, and format, and a series of\nbinary arrays, which can be stored in all modern binary containers, such as\nHDF5, Zarr, or NPZ. We provide several reference implementations of Binsparse\nspanning 5 languages, 5 frameworks, and 4 binary containers. We evaluate our\nBinsparse format on every matrix in the SuiteSparse Matrix Collection and a\nselection of tensors from the FROSTT collection. The Binsparse HDF5 CSR format\nshows file size reductions of 2.4x on average without compression and 7.5x with\ncompression. We evaluate our parser's read/write performance against a\nstate-of-the-art Matrix Market parser, demonstrating warm cache mean read\nspeedups of 26.5x without compression and 2.6x with compression, and write\nspeedups of 31x without compression and 1.4x with compression.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Sparse matrices and tensors are ubiquitous throughout multiple subfields of\ncomputing. The widespread usage of sparse data has inspired many in-memory and\non-disk storage formats, but the only widely adopted storage specifications are\nthe Matrix Market and FROSTT file formats, which both use ASCII text. Due to\nthe inefficiency of text storage, these files typically have larger file sizes\nand longer parsing times than binary storage formats, which directly store an\nin-memory representation to disk. This can be a major bottleneck; since sparse\ncomputation is often bandwidth-bound, the cost of loading or storing a matrix\nto disk often exceeds the cost of performing a sparse computation. While it is\ncommon practice for practitioners to develop their own, custom, non-portable\nbinary formats for high-performance sparse matrix storage, there is currently\nno cross-platform binary sparse matrix storage format. We present Binsparse, a\ncross-platform binary sparse matrix and tensor format specification. Binsparse\nis a modular, embeddable format, consisting of a JSON descriptor, which\ndescribes the matrix or tensor dimensions, type, and format, and a series of\nbinary arrays, which can be stored in all modern binary containers, such as\nHDF5, Zarr, or NPZ. We provide several reference implementations of Binsparse\nspanning 5 languages, 5 frameworks, and 4 binary containers. We evaluate our\nBinsparse format on every matrix in the SuiteSparse Matrix Collection and a\nselection of tensors from the FROSTT collection. The Binsparse HDF5 CSR format\nshows file size reductions of 2.4x on average without compression and 7.5x with\ncompression. We evaluate our parser's read/write performance against a\nstate-of-the-art Matrix Market parser, demonstrating warm cache mean read\nspeedups of 26.5x without compression and 2.6x with compression, and write\nspeedups of 31x without compression and 1.4x with compression."
                },
                "authors": [
                    {
                        "name": "Benjamin Brock"
                    },
                    {
                        "name": "Willow Ahrens"
                    },
                    {
                        "name": "Hameer Abbasi"
                    },
                    {
                        "name": "Timothy A. Davis"
                    },
                    {
                        "name": "Juni Kim"
                    },
                    {
                        "name": "James Kitchen"
                    },
                    {
                        "name": "Spencer Patty"
                    },
                    {
                        "name": "Isaac Virshup"
                    },
                    {
                        "name": "Erik Welch"
                    }
                ],
                "author_detail": {
                    "name": "Erik Welch"
                },
                "author": "Erik Welch",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.19175v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.19175v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.MS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.MS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.18879v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.18879v1",
                "updated": "2025-06-23T17:50:11Z",
                "updated_parsed": [
                    2025,
                    6,
                    23,
                    17,
                    50,
                    11,
                    0,
                    174,
                    0
                ],
                "published": "2025-06-23T17:50:11Z",
                "published_parsed": [
                    2025,
                    6,
                    23,
                    17,
                    50,
                    11,
                    0,
                    174,
                    0
                ],
                "title": "CommVQ: Commutative Vector Quantization for KV Cache Compression",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CommVQ: Commutative Vector Quantization for KV Cache Compression"
                },
                "summary": "Large Language Models (LLMs) are increasingly used in applications requiring\nlong context lengths, but the key-value (KV) cache often becomes a memory\nbottleneck on GPUs as context grows. To address this, we propose Commutative\nVector Quantization (CommVQ) to significantly reduce memory usage for\nlong-context LLM inference. We first introduce additive quantization with a\nlightweight encoder and codebook to compress the KV cache, which can be decoded\nvia simple matrix multiplication. To further reduce computational costs during\ndecoding, we design the codebook to be commutative with Rotary Position\nEmbedding (RoPE) and train it using an Expectation-Maximization (EM) algorithm.\nThis enables efficient integration of decoding into the self-attention\nmechanism. Our approach achieves high accuracy with additive quantization and\nlow overhead via the RoPE-commutative codebook. Experiments on long-context\nbenchmarks and GSM8K show that our method reduces FP16 KV cache size by 87.5%\nwith 2-bit quantization, while outperforming state-of-the-art KV cache\nquantization methods. Notably, it enables 1-bit KV cache quantization with\nminimal accuracy loss, allowing a LLaMA-3.1 8B model to run with a 128K context\nlength on a single RTX 4090 GPU. The source code is available at:\nhttps://github.com/UMass-Embodied-AGI/CommVQ.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) are increasingly used in applications requiring\nlong context lengths, but the key-value (KV) cache often becomes a memory\nbottleneck on GPUs as context grows. To address this, we propose Commutative\nVector Quantization (CommVQ) to significantly reduce memory usage for\nlong-context LLM inference. We first introduce additive quantization with a\nlightweight encoder and codebook to compress the KV cache, which can be decoded\nvia simple matrix multiplication. To further reduce computational costs during\ndecoding, we design the codebook to be commutative with Rotary Position\nEmbedding (RoPE) and train it using an Expectation-Maximization (EM) algorithm.\nThis enables efficient integration of decoding into the self-attention\nmechanism. Our approach achieves high accuracy with additive quantization and\nlow overhead via the RoPE-commutative codebook. Experiments on long-context\nbenchmarks and GSM8K show that our method reduces FP16 KV cache size by 87.5%\nwith 2-bit quantization, while outperforming state-of-the-art KV cache\nquantization methods. Notably, it enables 1-bit KV cache quantization with\nminimal accuracy loss, allowing a LLaMA-3.1 8B model to run with a 128K context\nlength on a single RTX 4090 GPU. The source code is available at:\nhttps://github.com/UMass-Embodied-AGI/CommVQ."
                },
                "authors": [
                    {
                        "name": "Junyan Li"
                    },
                    {
                        "name": "Yang Zhang"
                    },
                    {
                        "name": "Muhammad Yusuf Hassan"
                    },
                    {
                        "name": "Talha Chafekar"
                    },
                    {
                        "name": "Tianle Cai"
                    },
                    {
                        "name": "Zhile Ren"
                    },
                    {
                        "name": "Pengsheng Guo"
                    },
                    {
                        "name": "Foroozan Karimzadeh"
                    },
                    {
                        "name": "Colorado Reed"
                    },
                    {
                        "name": "Chong Wang"
                    },
                    {
                        "name": "Chuang Gan"
                    }
                ],
                "author_detail": {
                    "name": "Chuang Gan"
                },
                "author": "Chuang Gan",
                "arxiv_comment": "ICML 2025 poster",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.18879v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.18879v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.20722v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.20722v3",
                "updated": "2025-06-23T07:59:17Z",
                "updated_parsed": [
                    2025,
                    6,
                    23,
                    7,
                    59,
                    17,
                    0,
                    174,
                    0
                ],
                "published": "2024-07-30T10:34:40Z",
                "published_parsed": [
                    2024,
                    7,
                    30,
                    10,
                    34,
                    40,
                    1,
                    212,
                    0
                ],
                "title": "Persistent Sampling: Enhancing the Efficiency of Sequential Monte Carlo",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Persistent Sampling: Enhancing the Efficiency of Sequential Monte Carlo"
                },
                "summary": "Sequential Monte Carlo (SMC) samplers are powerful tools for Bayesian\ninference but suffer from high computational costs due to their reliance on\nlarge particle ensembles for accurate estimates. We introduce persistent\nsampling (PS), an extension of SMC that systematically retains and reuses\nparticles from all prior iterations to construct a growing, weighted ensemble.\nBy leveraging multiple importance sampling and resampling from a mixture of\nhistorical distributions, PS mitigates the need for excessively large particle\ncounts, directly addressing key limitations of SMC such as particle\nimpoverishment and mode collapse. Crucially, PS achieves this without\nadditional likelihood evaluations-weights for persistent particles are computed\nusing cached likelihood values. This framework not only yields more accurate\nposterior approximations but also produces marginal likelihood estimates with\nsignificantly lower variance, enhancing reliability in model comparison.\nFurthermore, the persistent ensemble enables efficient adaptation of transition\nkernels by leveraging a larger, decorrelated particle pool. Experiments on\nhigh-dimensional Gaussian mixtures, hierarchical models, and non-convex targets\ndemonstrate that PS consistently outperforms standard SMC and related variants,\nincluding recycled and waste-free SMC, achieving substantial reductions in mean\nsquared error for posterior expectations and evidence estimates, all at reduced\ncomputational cost. PS thus establishes itself as a robust, scalable, and\nefficient alternative for complex Bayesian inference tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Sequential Monte Carlo (SMC) samplers are powerful tools for Bayesian\ninference but suffer from high computational costs due to their reliance on\nlarge particle ensembles for accurate estimates. We introduce persistent\nsampling (PS), an extension of SMC that systematically retains and reuses\nparticles from all prior iterations to construct a growing, weighted ensemble.\nBy leveraging multiple importance sampling and resampling from a mixture of\nhistorical distributions, PS mitigates the need for excessively large particle\ncounts, directly addressing key limitations of SMC such as particle\nimpoverishment and mode collapse. Crucially, PS achieves this without\nadditional likelihood evaluations-weights for persistent particles are computed\nusing cached likelihood values. This framework not only yields more accurate\nposterior approximations but also produces marginal likelihood estimates with\nsignificantly lower variance, enhancing reliability in model comparison.\nFurthermore, the persistent ensemble enables efficient adaptation of transition\nkernels by leveraging a larger, decorrelated particle pool. Experiments on\nhigh-dimensional Gaussian mixtures, hierarchical models, and non-convex targets\ndemonstrate that PS consistently outperforms standard SMC and related variants,\nincluding recycled and waste-free SMC, achieving substantial reductions in mean\nsquared error for posterior expectations and evidence estimates, all at reduced\ncomputational cost. PS thus establishes itself as a robust, scalable, and\nefficient alternative for complex Bayesian inference tasks."
                },
                "authors": [
                    {
                        "name": "Minas Karamanis"
                    },
                    {
                        "name": "UroÅ¡ Seljak"
                    }
                ],
                "author_detail": {
                    "name": "UroÅ¡ Seljak"
                },
                "author": "UroÅ¡ Seljak",
                "arxiv_comment": "37 pages, 9 figures. Submitted to Statistics & Computing",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.20722v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.20722v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ML",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.CO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.03766v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.03766v3",
                "updated": "2025-06-23T03:20:46Z",
                "updated_parsed": [
                    2025,
                    6,
                    23,
                    3,
                    20,
                    46,
                    0,
                    174,
                    0
                ],
                "published": "2024-10-02T15:22:08Z",
                "published_parsed": [
                    2024,
                    10,
                    2,
                    15,
                    22,
                    8,
                    2,
                    276,
                    0
                ],
                "title": "FutureFill: Fast Generation from Convolutional Sequence Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FutureFill: Fast Generation from Convolutional Sequence Models"
                },
                "summary": "We address the challenge of efficient auto-regressive generation in sequence\nprediction models by introducing FutureFill, a general-purpose fast generation\nmethod for any sequence prediction algorithm based on convolutional operators.\nFutureFill reduces generation time from quadratic to quasilinear in the context\nlength. Moreover, when generating from a prompt, it requires a prefill cache\nwhose size grows only with the number of tokens to be generated, often much\nsmaller than the caches required by standard convolutional or attention based\nmodels. We validate our theoretical claims with experiments on synthetic tasks\nand demonstrate substantial efficiency gains when generating from a deep\nconvolutional sequence prediction model.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We address the challenge of efficient auto-regressive generation in sequence\nprediction models by introducing FutureFill, a general-purpose fast generation\nmethod for any sequence prediction algorithm based on convolutional operators.\nFutureFill reduces generation time from quadratic to quasilinear in the context\nlength. Moreover, when generating from a prompt, it requires a prefill cache\nwhose size grows only with the number of tokens to be generated, often much\nsmaller than the caches required by standard convolutional or attention based\nmodels. We validate our theoretical claims with experiments on synthetic tasks\nand demonstrate substantial efficiency gains when generating from a deep\nconvolutional sequence prediction model."
                },
                "authors": [
                    {
                        "name": "Naman Agarwal"
                    },
                    {
                        "name": "Xinyi Chen"
                    },
                    {
                        "name": "Evan Dogariu"
                    },
                    {
                        "name": "Devan Shah"
                    },
                    {
                        "name": "Hubert Strauss"
                    },
                    {
                        "name": "Vlad Feinberg"
                    },
                    {
                        "name": "Daniel Suo"
                    },
                    {
                        "name": "Peter Bartlett"
                    },
                    {
                        "name": "Elad Hazan"
                    }
                ],
                "author_detail": {
                    "name": "Elad Hazan"
                },
                "author": "Elad Hazan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.03766v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.03766v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.20330v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.20330v2",
                "updated": "2025-06-23T03:05:26Z",
                "updated_parsed": [
                    2025,
                    6,
                    23,
                    3,
                    5,
                    26,
                    0,
                    174,
                    0
                ],
                "published": "2025-02-27T17:59:36Z",
                "published_parsed": [
                    2025,
                    2,
                    27,
                    17,
                    59,
                    36,
                    3,
                    58,
                    0
                ],
                "title": "RAPID: Long-Context Inference with Retrieval-Augmented Speculative\n  Decoding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RAPID: Long-Context Inference with Retrieval-Augmented Speculative\n  Decoding"
                },
                "summary": "The emergence of long-context large language models (LLMs) offers a promising\nalternative to traditional retrieval-augmented generation (RAG) for processing\nextensive documents. However, the computational overhead of long-context\ninference presents significant efficiency challenges. While Speculative\nDecoding (SD) traditionally accelerates inference using smaller draft models,\nits effectiveness diminishes substantially in long-context scenarios due to\nmemory-bound KV cache operations. We introduce Retrieval-Augmented Speculative\nDecoding (RAPID), which leverages RAG for both accelerating and enhancing\ngeneration quality in long-context inference. RAPID introduces the RAG\ndrafter-a draft LLM operating on shortened retrieval contexts-to speculate on\nthe generation of long-context target LLMs. Our approach enables a new paradigm\nwhere same-scale or even larger LLMs can serve as RAG drafters while\nmaintaining computational efficiency. To fully leverage the potentially\nsuperior capabilities from stronger RAG drafters, we develop an inference-time\nknowledge transfer that enriches the target distribution by RAG. Extensive\nexperiments on the LLaMA-3.1 and Qwen2.5 backbones demonstrate that RAPID\neffectively integrates the strengths of both RAG and long-context LLMs,\nachieving significant performance improvements (e.g., from 39.33 to 42.83 on\nInfiniteBench for LLaMA-3.1-8B) with more than 2x speedups for long-context\ninference. Our analyses also reveal the robustness of RAPID across various\ncontext lengths and retrieval quality.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The emergence of long-context large language models (LLMs) offers a promising\nalternative to traditional retrieval-augmented generation (RAG) for processing\nextensive documents. However, the computational overhead of long-context\ninference presents significant efficiency challenges. While Speculative\nDecoding (SD) traditionally accelerates inference using smaller draft models,\nits effectiveness diminishes substantially in long-context scenarios due to\nmemory-bound KV cache operations. We introduce Retrieval-Augmented Speculative\nDecoding (RAPID), which leverages RAG for both accelerating and enhancing\ngeneration quality in long-context inference. RAPID introduces the RAG\ndrafter-a draft LLM operating on shortened retrieval contexts-to speculate on\nthe generation of long-context target LLMs. Our approach enables a new paradigm\nwhere same-scale or even larger LLMs can serve as RAG drafters while\nmaintaining computational efficiency. To fully leverage the potentially\nsuperior capabilities from stronger RAG drafters, we develop an inference-time\nknowledge transfer that enriches the target distribution by RAG. Extensive\nexperiments on the LLaMA-3.1 and Qwen2.5 backbones demonstrate that RAPID\neffectively integrates the strengths of both RAG and long-context LLMs,\nachieving significant performance improvements (e.g., from 39.33 to 42.83 on\nInfiniteBench for LLaMA-3.1-8B) with more than 2x speedups for long-context\ninference. Our analyses also reveal the robustness of RAPID across various\ncontext lengths and retrieval quality."
                },
                "authors": [
                    {
                        "name": "Guanzheng Chen"
                    },
                    {
                        "name": "Qilong Feng"
                    },
                    {
                        "name": "Jinjie Ni"
                    },
                    {
                        "name": "Xin Li"
                    },
                    {
                        "name": "Michael Qizhe Shieh"
                    }
                ],
                "author_detail": {
                    "name": "Michael Qizhe Shieh"
                },
                "author": "Michael Qizhe Shieh",
                "arxiv_comment": "ICML 2025 Spotlight",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.20330v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.20330v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.18226v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.18226v1",
                "updated": "2025-06-23T01:27:06Z",
                "updated_parsed": [
                    2025,
                    6,
                    23,
                    1,
                    27,
                    6,
                    0,
                    174,
                    0
                ],
                "published": "2025-06-23T01:27:06Z",
                "published_parsed": [
                    2025,
                    6,
                    23,
                    1,
                    27,
                    6,
                    0,
                    174,
                    0
                ],
                "title": "Make It Efficient: Dynamic Sparse Attention for Autoregressive Image\n  Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Make It Efficient: Dynamic Sparse Attention for Autoregressive Image\n  Generation"
                },
                "summary": "Autoregressive conditional image generation models have emerged as a dominant\nparadigm in text-to-image synthesis. These methods typically convert images\ninto one-dimensional token sequences and leverage the self-attention mechanism,\nwhich has achieved remarkable success in natural language processing, to\ncapture long-range dependencies, model global context, and ensure semantic\ncoherence. However, excessively long contexts during inference lead to\nsignificant memory overhead caused by KV-cache and computational delays. To\nalleviate these challenges, we systematically analyze how global semantics,\nspatial layouts, and fine-grained textures are formed during inference, and\npropose a novel training-free context optimization method called Adaptive\nDynamic Sparse Attention (ADSA). Conceptually, ADSA dynamically identifies\nhistorical tokens crucial for maintaining local texture consistency and those\nessential for ensuring global semantic coherence, thereby efficiently\nstreamlining attention computation. Additionally, we introduce a dynamic\nKV-cache update mechanism tailored for ADSA, reducing GPU memory consumption\nduring inference by approximately $50\\%$. Extensive qualitative and\nquantitative experiments demonstrate the effectiveness and superiority of our\napproach in terms of both generation quality and resource efficiency.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Autoregressive conditional image generation models have emerged as a dominant\nparadigm in text-to-image synthesis. These methods typically convert images\ninto one-dimensional token sequences and leverage the self-attention mechanism,\nwhich has achieved remarkable success in natural language processing, to\ncapture long-range dependencies, model global context, and ensure semantic\ncoherence. However, excessively long contexts during inference lead to\nsignificant memory overhead caused by KV-cache and computational delays. To\nalleviate these challenges, we systematically analyze how global semantics,\nspatial layouts, and fine-grained textures are formed during inference, and\npropose a novel training-free context optimization method called Adaptive\nDynamic Sparse Attention (ADSA). Conceptually, ADSA dynamically identifies\nhistorical tokens crucial for maintaining local texture consistency and those\nessential for ensuring global semantic coherence, thereby efficiently\nstreamlining attention computation. Additionally, we introduce a dynamic\nKV-cache update mechanism tailored for ADSA, reducing GPU memory consumption\nduring inference by approximately $50\\%$. Extensive qualitative and\nquantitative experiments demonstrate the effectiveness and superiority of our\napproach in terms of both generation quality and resource efficiency."
                },
                "authors": [
                    {
                        "name": "Xunzhi Xiang"
                    },
                    {
                        "name": "Qi Fan"
                    }
                ],
                "author_detail": {
                    "name": "Qi Fan"
                },
                "author": "Qi Fan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.18226v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.18226v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.01040v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.01040v1",
                "updated": "2025-06-22T20:43:42Z",
                "updated_parsed": [
                    2025,
                    6,
                    22,
                    20,
                    43,
                    42,
                    6,
                    173,
                    0
                ],
                "published": "2025-06-22T20:43:42Z",
                "published_parsed": [
                    2025,
                    6,
                    22,
                    20,
                    43,
                    42,
                    6,
                    173,
                    0
                ],
                "title": "Fast Clifford Neural Layers",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Fast Clifford Neural Layers"
                },
                "summary": "Clifford Neural Layers improve PDE modeling by introducing Clifford Algebra\ninto neural networks. In this project we focus on optimizing the inference of\n2/3D Clifford convolutional layers and multivector activation layers for one\ncore CPU performance.\n  Overall, by testing on a real network block involving Clifford convolutional\nlayers and multivector activation layers, we observe that our implementation is\n30% faster than standard PyTorch implementation in relatively large data +\nnetwork size (>L2 cache).\n  We open source our code base at\nhttps://github.com/egretwAlker/c-opt-clifford-layers",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Clifford Neural Layers improve PDE modeling by introducing Clifford Algebra\ninto neural networks. In this project we focus on optimizing the inference of\n2/3D Clifford convolutional layers and multivector activation layers for one\ncore CPU performance.\n  Overall, by testing on a real network block involving Clifford convolutional\nlayers and multivector activation layers, we observe that our implementation is\n30% faster than standard PyTorch implementation in relatively large data +\nnetwork size (>L2 cache).\n  We open source our code base at\nhttps://github.com/egretwAlker/c-opt-clifford-layers"
                },
                "authors": [
                    {
                        "name": "Tianxiang Xia"
                    },
                    {
                        "name": "Max Neuwinger"
                    },
                    {
                        "name": "Lin Xiao"
                    }
                ],
                "author_detail": {
                    "name": "Lin Xiao"
                },
                "author": "Lin Xiao",
                "arxiv_comment": "7 pages content-wise",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.01040v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.01040v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.13063v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.13063v3",
                "updated": "2025-06-22T15:07:37Z",
                "updated_parsed": [
                    2025,
                    6,
                    22,
                    15,
                    7,
                    37,
                    6,
                    173,
                    0
                ],
                "published": "2025-02-18T17:08:45Z",
                "published_parsed": [
                    2025,
                    2,
                    18,
                    17,
                    8,
                    45,
                    1,
                    49,
                    0
                ],
                "title": "Cramming 1568 Tokens into a Single Vector and Back Again: Exploring the\n  Limits of Embedding Space Capacity",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cramming 1568 Tokens into a Single Vector and Back Again: Exploring the\n  Limits of Embedding Space Capacity"
                },
                "summary": "A range of recent works addresses the problem of compression of sequence of\ntokens into a shorter sequence of real-valued vectors to be used as inputs\ninstead of token embeddings or key-value cache. These approaches are focused on\nreduction of the amount of compute in existing language models rather than\nminimization of number of bits needed to store text. Despite relying on\npowerful models as encoders, the maximum attainable lossless compression ratio\nis typically not higher than x10. This fact is highly intriguing because, in\ntheory, the maximum information capacity of large real-valued vectors is far\nbeyond the presented rates even for 16-bit precision and a modest vector size.\nIn this work, we explore the limits of compression by replacing the encoder\nwith a per-sample optimization procedure. We show that vectors with compression\nratios up to x1500 exist, which highlights two orders of magnitude gap between\nexisting and practically attainable solutions. Furthermore, we empirically show\nthat the compression limits are determined not by the length of the input but\nby the amount of uncertainty to be reduced, namely, the cross-entropy loss on\nthis sequence without any conditioning. The obtained limits highlight the\nsubstantial gap between the theoretical capacity of input embeddings and their\npractical utilization, suggesting significant room for optimization in model\ndesign.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A range of recent works addresses the problem of compression of sequence of\ntokens into a shorter sequence of real-valued vectors to be used as inputs\ninstead of token embeddings or key-value cache. These approaches are focused on\nreduction of the amount of compute in existing language models rather than\nminimization of number of bits needed to store text. Despite relying on\npowerful models as encoders, the maximum attainable lossless compression ratio\nis typically not higher than x10. This fact is highly intriguing because, in\ntheory, the maximum information capacity of large real-valued vectors is far\nbeyond the presented rates even for 16-bit precision and a modest vector size.\nIn this work, we explore the limits of compression by replacing the encoder\nwith a per-sample optimization procedure. We show that vectors with compression\nratios up to x1500 exist, which highlights two orders of magnitude gap between\nexisting and practically attainable solutions. Furthermore, we empirically show\nthat the compression limits are determined not by the length of the input but\nby the amount of uncertainty to be reduced, namely, the cross-entropy loss on\nthis sequence without any conditioning. The obtained limits highlight the\nsubstantial gap between the theoretical capacity of input embeddings and their\npractical utilization, suggesting significant room for optimization in model\ndesign."
                },
                "authors": [
                    {
                        "name": "Yuri Kuratov"
                    },
                    {
                        "name": "Mikhail Arkhipov"
                    },
                    {
                        "name": "Aydar Bulatov"
                    },
                    {
                        "name": "Mikhail Burtsev"
                    }
                ],
                "author_detail": {
                    "name": "Mikhail Burtsev"
                },
                "author": "Mikhail Burtsev",
                "arxiv_comment": "ACL 2025 (main conference)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.13063v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.13063v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.17988v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.17988v1",
                "updated": "2025-06-22T10:57:57Z",
                "updated_parsed": [
                    2025,
                    6,
                    22,
                    10,
                    57,
                    57,
                    6,
                    173,
                    0
                ],
                "published": "2025-06-22T10:57:57Z",
                "published_parsed": [
                    2025,
                    6,
                    22,
                    10,
                    57,
                    57,
                    6,
                    173,
                    0
                ],
                "title": "Secure User-friendly Blockchain Modular Wallet Design Using Android &\n  OP-TEE",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Secure User-friendly Blockchain Modular Wallet Design Using Android &\n  OP-TEE"
                },
                "summary": "Emerging crypto economies still hemorrhage digital assets because legacy\nwallets leak private keys at almost every layer of the software stack, from\nuser-space libraries to kernel memory dumps. This paper solves that twin crisis\nof security and interoperability by re-imagining key management as a\nplatform-level service anchored in ARM TrustZone through OP-TEE. Our\narchitecture fractures the traditional monolithic Trusted Application into\nper-chain modules housed in a multi-tenant TA store, finally breaking OP-TEE's\nsingle-binary ceiling. A cryptographically sealed firmware-over-the-air\npipeline welds each TA set to an Android system image, enabling hot-swap\nupdates while Verified Boot enforces rollback protection. Every package carries\na chained signature developer first, registry second so even a compromised\nsupply chain cannot smuggle malicious code past the Secure World's RSA-PSS\ngatekeeper. Inside the TEE, strict inter-TA isolation, cache partitioning, and\nGP-compliant crypto APIs ensure secrets never bleed across trust boundaries or\ntiming domains. The Rich Execution Environment can interact only via\nhardware-mediated Secure Monitor Calls, collapsing the surface exposed to\nmalware in Android space. End-users enjoy a single polished interface yet can\ninstall or retire Bitcoin, Ethereum, Solana, or tomorrow's chain with one tap,\nshrinking both storage footprint and audit scope. For auditors, the composition\nmodel slashes duplicated verification effort by quarantining blockchain logic\ninside narrowly scoped modules that share formally specified interfaces. Our\nthreat analysis spans six adversary layers and shows how the design neutralizes\nREE malware sniffing, OTA injection, and cross-module side channels without\nexotic hardware. A reference implementation on AOSP exports a Wallet Manager\nHAL, custom SELinux domains, and a CI/CD pipeline that vet community modules\nbefore release. The result is not merely another hardware wallet but a\nprogrammable substrate that can evolve at the velocity of the blockchain\necosystem. By welding radical extensibility to hardware-anchored assurance, the\nplatform closes the security-usability gap that has long stymied mass-market\nself-custody. We posit that modular TEEs are the missing OS primitive for Web3,\nmuch as virtual memory unlocked multi-tasking in classical computing. Together,\nthese contributions sketch a blueprint for multi-chain asset management that is\nauditable, resilient, and poised for global deployment.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Emerging crypto economies still hemorrhage digital assets because legacy\nwallets leak private keys at almost every layer of the software stack, from\nuser-space libraries to kernel memory dumps. This paper solves that twin crisis\nof security and interoperability by re-imagining key management as a\nplatform-level service anchored in ARM TrustZone through OP-TEE. Our\narchitecture fractures the traditional monolithic Trusted Application into\nper-chain modules housed in a multi-tenant TA store, finally breaking OP-TEE's\nsingle-binary ceiling. A cryptographically sealed firmware-over-the-air\npipeline welds each TA set to an Android system image, enabling hot-swap\nupdates while Verified Boot enforces rollback protection. Every package carries\na chained signature developer first, registry second so even a compromised\nsupply chain cannot smuggle malicious code past the Secure World's RSA-PSS\ngatekeeper. Inside the TEE, strict inter-TA isolation, cache partitioning, and\nGP-compliant crypto APIs ensure secrets never bleed across trust boundaries or\ntiming domains. The Rich Execution Environment can interact only via\nhardware-mediated Secure Monitor Calls, collapsing the surface exposed to\nmalware in Android space. End-users enjoy a single polished interface yet can\ninstall or retire Bitcoin, Ethereum, Solana, or tomorrow's chain with one tap,\nshrinking both storage footprint and audit scope. For auditors, the composition\nmodel slashes duplicated verification effort by quarantining blockchain logic\ninside narrowly scoped modules that share formally specified interfaces. Our\nthreat analysis spans six adversary layers and shows how the design neutralizes\nREE malware sniffing, OTA injection, and cross-module side channels without\nexotic hardware. A reference implementation on AOSP exports a Wallet Manager\nHAL, custom SELinux domains, and a CI/CD pipeline that vet community modules\nbefore release. The result is not merely another hardware wallet but a\nprogrammable substrate that can evolve at the velocity of the blockchain\necosystem. By welding radical extensibility to hardware-anchored assurance, the\nplatform closes the security-usability gap that has long stymied mass-market\nself-custody. We posit that modular TEEs are the missing OS primitive for Web3,\nmuch as virtual memory unlocked multi-tasking in classical computing. Together,\nthese contributions sketch a blueprint for multi-chain asset management that is\nauditable, resilient, and poised for global deployment."
                },
                "authors": [
                    {
                        "name": "Seongjin Kim"
                    },
                    {
                        "name": "Sanguk Yun"
                    },
                    {
                        "name": "Jungho Jang"
                    }
                ],
                "author_detail": {
                    "name": "Jungho Jang"
                },
                "author": "Jungho Jang",
                "arxiv_comment": "25 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.17988v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.17988v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.17331v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.17331v2",
                "updated": "2025-06-22T03:46:11Z",
                "updated_parsed": [
                    2025,
                    6,
                    22,
                    3,
                    46,
                    11,
                    6,
                    173,
                    0
                ],
                "published": "2025-05-22T22:54:21Z",
                "published_parsed": [
                    2025,
                    5,
                    22,
                    22,
                    54,
                    21,
                    3,
                    142,
                    0
                ],
                "title": "ECHO-LLaMA: Efficient Caching for High-Performance LLaMA Training",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ECHO-LLaMA: Efficient Caching for High-Performance LLaMA Training"
                },
                "summary": "This paper introduces ECHO-LLaMA, an efficient LLaMA architecture designed to\nimprove both the training speed and inference throughput of LLaMA architectures\nwhile maintaining its learning capacity. ECHO-LLaMA transforms LLaMA models\ninto shared KV caching across certain layers, significantly reducing KV\ncomputational complexity while maintaining or improving language performance.\nExperimental results demonstrate that ECHO-LLaMA achieves up to 77\\% higher\ntoken-per-second throughput during training, up to 16\\% higher Model FLOPs\nUtilization (MFU), and up to 14\\% lower loss when trained on an equal number of\ntokens. Furthermore, on the 1.1B model, ECHO-LLaMA delivers approximately 7\\%\nhigher test-time throughput compared to the baseline. By introducing a\ncomputationally efficient adaptation mechanism, ECHO-LLaMA offers a scalable\nand cost-effective solution for pretraining and finetuning large language\nmodels, enabling faster and more resource-efficient training without\ncompromising performance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper introduces ECHO-LLaMA, an efficient LLaMA architecture designed to\nimprove both the training speed and inference throughput of LLaMA architectures\nwhile maintaining its learning capacity. ECHO-LLaMA transforms LLaMA models\ninto shared KV caching across certain layers, significantly reducing KV\ncomputational complexity while maintaining or improving language performance.\nExperimental results demonstrate that ECHO-LLaMA achieves up to 77\\% higher\ntoken-per-second throughput during training, up to 16\\% higher Model FLOPs\nUtilization (MFU), and up to 14\\% lower loss when trained on an equal number of\ntokens. Furthermore, on the 1.1B model, ECHO-LLaMA delivers approximately 7\\%\nhigher test-time throughput compared to the baseline. By introducing a\ncomputationally efficient adaptation mechanism, ECHO-LLaMA offers a scalable\nand cost-effective solution for pretraining and finetuning large language\nmodels, enabling faster and more resource-efficient training without\ncompromising performance."
                },
                "authors": [
                    {
                        "name": "Maryam Dialameh"
                    },
                    {
                        "name": "Rezaul Karim"
                    },
                    {
                        "name": "Hossein Rajabzadeh"
                    },
                    {
                        "name": "Omar Mohamed Awad"
                    },
                    {
                        "name": "Hyock Ju Kwon"
                    },
                    {
                        "name": "Boxing Chen"
                    },
                    {
                        "name": "Walid Ahmed"
                    },
                    {
                        "name": "Yang Liu"
                    }
                ],
                "author_detail": {
                    "name": "Yang Liu"
                },
                "author": "Yang Liu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.17331v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.17331v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2304.10805v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2304.10805v2",
                "updated": "2025-06-21T08:27:10Z",
                "updated_parsed": [
                    2025,
                    6,
                    21,
                    8,
                    27,
                    10,
                    5,
                    172,
                    0
                ],
                "published": "2023-04-21T08:22:58Z",
                "published_parsed": [
                    2023,
                    4,
                    21,
                    8,
                    22,
                    58,
                    4,
                    111,
                    0
                ],
                "title": "RPLKG: Robust Prompt Learning with Knowledge Graph",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RPLKG: Robust Prompt Learning with Knowledge Graph"
                },
                "summary": "Large-scale pre-trained models surpass in transferability and robust\ngeneralization across diverse datasets. The emergence of multimodal pre-trained\nmodels like CLIP has significantly boosted performance in various experiments.\nHowever, generalizing to new datasets or domains remains challenging,\nespecially with limited labeled data. Also, existing methods often lack\ninterpretability and impose high computational costs. To address this, we\npropose Robust Prompt Learning with Knowledge Graph (RPLKG), leveraging the\nknowledge graph to curate diverse, interpretable prompt sets automatically. Our\nmethod autonomously selects the optimal interpretable prompt based on dataset\ncharacteristics, achieving performance improvements over zero-shot learning and\ncompetitive performance compared to various prompt learning methods. Also,\nRPLKG efficiently reuses cached prompt embeddings from a single model pass and\noptimizes prompt selection via Gumbel-Softmax, enabling low-memory, fast\ntraining. Moreover, RPLKG advances few-shot learning effectiveness while\nenhancing interpretability and efficiency in model adaptation. Our",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large-scale pre-trained models surpass in transferability and robust\ngeneralization across diverse datasets. The emergence of multimodal pre-trained\nmodels like CLIP has significantly boosted performance in various experiments.\nHowever, generalizing to new datasets or domains remains challenging,\nespecially with limited labeled data. Also, existing methods often lack\ninterpretability and impose high computational costs. To address this, we\npropose Robust Prompt Learning with Knowledge Graph (RPLKG), leveraging the\nknowledge graph to curate diverse, interpretable prompt sets automatically. Our\nmethod autonomously selects the optimal interpretable prompt based on dataset\ncharacteristics, achieving performance improvements over zero-shot learning and\ncompetitive performance compared to various prompt learning methods. Also,\nRPLKG efficiently reuses cached prompt embeddings from a single model pass and\noptimizes prompt selection via Gumbel-Softmax, enabling low-memory, fast\ntraining. Moreover, RPLKG advances few-shot learning effectiveness while\nenhancing interpretability and efficiency in model adaptation. Our"
                },
                "authors": [
                    {
                        "name": "YongTaek Lim"
                    },
                    {
                        "name": "Yewon Kim"
                    },
                    {
                        "name": "Suho Kang"
                    },
                    {
                        "name": "Dokyung Yoon"
                    },
                    {
                        "name": "KyungWoo Song"
                    }
                ],
                "author_detail": {
                    "name": "KyungWoo Song"
                },
                "author": "KyungWoo Song",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2304.10805v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2304.10805v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.17121v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.17121v1",
                "updated": "2025-06-20T16:21:12Z",
                "updated_parsed": [
                    2025,
                    6,
                    20,
                    16,
                    21,
                    12,
                    4,
                    171,
                    0
                ],
                "published": "2025-06-20T16:21:12Z",
                "published_parsed": [
                    2025,
                    6,
                    20,
                    16,
                    21,
                    12,
                    4,
                    171,
                    0
                ],
                "title": "Cache Me If You Can: How Many KVs Do You Need for Effective Long-Context\n  LMs?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cache Me If You Can: How Many KVs Do You Need for Effective Long-Context\n  LMs?"
                },
                "summary": "Language models handle increasingly long contexts for tasks such as book\nsummarization, but this leads to growing memory costs for the key-value (KV)\ncache. Many prior works have proposed ways of discarding KVs from memory, but\ntheir approaches are tailored to favorable settings, obscuring caveats like\nhigh peak memory and performance degradation, and a fair comparison between\nmethods is difficult. In this paper, we propose the *KV footprint* as a unified\nmetric, which accounts for both the amount of KV entries stored and their\nlifespan in memory. We evaluate methods based on the smallest footprint they\nattain while preserving performance in both long-context understanding and\ngeneration, with context lengths of up to 128K tokens. This metric reveals the\nhigh peak memory of prior KV eviction methods. One class of methods --\n*post-fill eviction* -- has a high footprint due to being incompatible with\neviction during pre-filling. We adapt these methods to be able to evict KVs\nduring pre-filling, achieving substantially lower KV footprints. We then turn\nto *recency eviction* methods, wherein we propose PruLong, an end-to-end\noptimization method for learning which attention heads need to retain the full\nKV cache and which do not. PruLong saves memory while preserving long-context\nperformance, achieving 12% smaller KV footprint than prior methods while\nretaining performance in challenging recall tasks. Our paper clarifies the\ncomplex tangle of long-context inference methods and paves the way for future\ndevelopment to minimize the KV footprint.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Language models handle increasingly long contexts for tasks such as book\nsummarization, but this leads to growing memory costs for the key-value (KV)\ncache. Many prior works have proposed ways of discarding KVs from memory, but\ntheir approaches are tailored to favorable settings, obscuring caveats like\nhigh peak memory and performance degradation, and a fair comparison between\nmethods is difficult. In this paper, we propose the *KV footprint* as a unified\nmetric, which accounts for both the amount of KV entries stored and their\nlifespan in memory. We evaluate methods based on the smallest footprint they\nattain while preserving performance in both long-context understanding and\ngeneration, with context lengths of up to 128K tokens. This metric reveals the\nhigh peak memory of prior KV eviction methods. One class of methods --\n*post-fill eviction* -- has a high footprint due to being incompatible with\neviction during pre-filling. We adapt these methods to be able to evict KVs\nduring pre-filling, achieving substantially lower KV footprints. We then turn\nto *recency eviction* methods, wherein we propose PruLong, an end-to-end\noptimization method for learning which attention heads need to retain the full\nKV cache and which do not. PruLong saves memory while preserving long-context\nperformance, achieving 12% smaller KV footprint than prior methods while\nretaining performance in challenging recall tasks. Our paper clarifies the\ncomplex tangle of long-context inference methods and paves the way for future\ndevelopment to minimize the KV footprint."
                },
                "authors": [
                    {
                        "name": "Adithya Bhaskar"
                    },
                    {
                        "name": "Alexander Wettig"
                    },
                    {
                        "name": "Tianyu Gao"
                    },
                    {
                        "name": "Yihe Dong"
                    },
                    {
                        "name": "Danqi Chen"
                    }
                ],
                "author_detail": {
                    "name": "Danqi Chen"
                },
                "author": "Danqi Chen",
                "arxiv_comment": "We release our code publicly at\n  https://github.com/princeton-pli/PruLong",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.17121v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.17121v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.16976v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.16976v1",
                "updated": "2025-06-20T13:09:26Z",
                "updated_parsed": [
                    2025,
                    6,
                    20,
                    13,
                    9,
                    26,
                    4,
                    171,
                    0
                ],
                "published": "2025-06-20T13:09:26Z",
                "published_parsed": [
                    2025,
                    6,
                    20,
                    13,
                    9,
                    26,
                    4,
                    171,
                    0
                ],
                "title": "PUL: Pre-load in Software for Caches Wouldn't Always Play Along",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PUL: Pre-load in Software for Caches Wouldn't Always Play Along"
                },
                "summary": "Memory latencies and bandwidth are major factors, limiting system performance\nand scalability. Modern CPUs aim at hiding latencies by employing large caches,\nout-of-order execution, or complex hardware prefetchers. However,\nsoftware-based prefetching exhibits higher efficiency, improving with newer CPU\ngenerations.\n  In this paper we investigate software-based, post-Moore systems that offload\noperations to intelligent memories. We show that software-based prefetching has\neven higher potential in near-data processing settings by maximizing compute\nutilization through compute/IO interleaving.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Memory latencies and bandwidth are major factors, limiting system performance\nand scalability. Modern CPUs aim at hiding latencies by employing large caches,\nout-of-order execution, or complex hardware prefetchers. However,\nsoftware-based prefetching exhibits higher efficiency, improving with newer CPU\ngenerations.\n  In this paper we investigate software-based, post-Moore systems that offload\noperations to intelligent memories. We show that software-based prefetching has\neven higher potential in near-data processing settings by maximizing compute\nutilization through compute/IO interleaving."
                },
                "authors": [
                    {
                        "name": "Arthur Bernhardt"
                    },
                    {
                        "name": "Sajjad Tamimi"
                    },
                    {
                        "name": "Florian Stock"
                    },
                    {
                        "name": "Andreas Koch"
                    },
                    {
                        "name": "Ilia Petrov"
                    }
                ],
                "author_detail": {
                    "name": "Ilia Petrov"
                },
                "author": "Ilia Petrov",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.16976v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.16976v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            }
        ]
    },
    {
        "keyword": "LLM Inference",
        "arxiv_results": [
            {
                "id": "http://arxiv.org/abs/2507.06230v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.06230v1",
                "updated": "2025-07-08T17:59:50Z",
                "updated_parsed": [
                    2025,
                    7,
                    8,
                    17,
                    59,
                    50,
                    1,
                    189,
                    0
                ],
                "published": "2025-07-08T17:59:50Z",
                "published_parsed": [
                    2025,
                    7,
                    8,
                    17,
                    59,
                    50,
                    1,
                    189,
                    0
                ],
                "title": "Feed-Forward SceneDINO for Unsupervised Semantic Scene Completion",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Feed-Forward SceneDINO for Unsupervised Semantic Scene Completion"
                },
                "summary": "Semantic scene completion (SSC) aims to infer both the 3D geometry and\nsemantics of a scene from single images. In contrast to prior work on SSC that\nheavily relies on expensive ground-truth annotations, we approach SSC in an\nunsupervised setting. Our novel method, SceneDINO, adapts techniques from\nself-supervised representation learning and 2D unsupervised scene understanding\nto SSC. Our training exclusively utilizes multi-view consistency\nself-supervision without any form of semantic or geometric ground truth. Given\na single input image, SceneDINO infers the 3D geometry and expressive 3D DINO\nfeatures in a feed-forward manner. Through a novel 3D feature distillation\napproach, we obtain unsupervised 3D semantics. In both 3D and 2D unsupervised\nscene understanding, SceneDINO reaches state-of-the-art segmentation accuracy.\nLinear probing our 3D features matches the segmentation accuracy of a current\nsupervised SSC approach. Additionally, we showcase the domain generalization\nand multi-view consistency of SceneDINO, taking the first steps towards a\nstrong foundation for single image 3D scene understanding.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Semantic scene completion (SSC) aims to infer both the 3D geometry and\nsemantics of a scene from single images. In contrast to prior work on SSC that\nheavily relies on expensive ground-truth annotations, we approach SSC in an\nunsupervised setting. Our novel method, SceneDINO, adapts techniques from\nself-supervised representation learning and 2D unsupervised scene understanding\nto SSC. Our training exclusively utilizes multi-view consistency\nself-supervision without any form of semantic or geometric ground truth. Given\na single input image, SceneDINO infers the 3D geometry and expressive 3D DINO\nfeatures in a feed-forward manner. Through a novel 3D feature distillation\napproach, we obtain unsupervised 3D semantics. In both 3D and 2D unsupervised\nscene understanding, SceneDINO reaches state-of-the-art segmentation accuracy.\nLinear probing our 3D features matches the segmentation accuracy of a current\nsupervised SSC approach. Additionally, we showcase the domain generalization\nand multi-view consistency of SceneDINO, taking the first steps towards a\nstrong foundation for single image 3D scene understanding."
                },
                "authors": [
                    {
                        "name": "Aleksandar JevtiÄ"
                    },
                    {
                        "name": "Christoph Reich"
                    },
                    {
                        "name": "Felix Wimbauer"
                    },
                    {
                        "name": "Oliver Hahn"
                    },
                    {
                        "name": "Christian Rupprecht"
                    },
                    {
                        "name": "Stefan Roth"
                    },
                    {
                        "name": "Daniel Cremers"
                    }
                ],
                "author_detail": {
                    "name": "Daniel Cremers"
                },
                "author": "Daniel Cremers",
                "arxiv_comment": "To appear at ICCV 2025. Christoph Reich and Aleksandar Jevti\\'c -\n  both authors contributed equally. Code:\n  https://github.com/tum-vision/scenedino Project page:\n  https://visinf.github.io/scenedino",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.06230v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.06230v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.06223v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.06223v1",
                "updated": "2025-07-08T17:56:28Z",
                "updated_parsed": [
                    2025,
                    7,
                    8,
                    17,
                    56,
                    28,
                    1,
                    189,
                    0
                ],
                "published": "2025-07-08T17:56:28Z",
                "published_parsed": [
                    2025,
                    7,
                    8,
                    17,
                    56,
                    28,
                    1,
                    189,
                    0
                ],
                "title": "Efficiency-Effectiveness Reranking FLOPs for LLM-based Rerankers",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficiency-Effectiveness Reranking FLOPs for LLM-based Rerankers"
                },
                "summary": "Large Language Models (LLMs) have recently been applied to reranking tasks in\ninformation retrieval, achieving strong performance. However, their high\ncomputational demands often hinder practical deployment. Existing studies\nevaluate the efficiency of LLM-based rerankers using proxy metrics such as\nlatency, the number of forward passes, input tokens, and output tokens.\nHowever, these metrics depend on hardware and running-time choices (\\eg\nparallel or not, batch size, etc), and often fail to account for model size,\nmaking it difficult to interpret and obscuring the evaluation of the\nefficiency-effectiveness tradeoff. To address this issue, we propose\nE\\textsuperscript{2}R-FLOPs, for LLM-based rerankers: ranking metrics per\nPetaFLOP (RPP) for relevance per compute and queries per PetaFLOP (QPP) for\nhardware-agnostic throughput. Companied with the new metrics, an interpretable\nFLOPs estimator is built to estimate the FLOPs of an LLM-based reranker even\nwithout running any experiments. Based on the proposed metrics, we conduct\ncomprehensive experiments to evaluate a wide range of LLM-based rerankers with\ndifferent architecture, studying the efficiency-effectiveness trade-off and\nbringing this issue to the attention of the research community.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have recently been applied to reranking tasks in\ninformation retrieval, achieving strong performance. However, their high\ncomputational demands often hinder practical deployment. Existing studies\nevaluate the efficiency of LLM-based rerankers using proxy metrics such as\nlatency, the number of forward passes, input tokens, and output tokens.\nHowever, these metrics depend on hardware and running-time choices (\\eg\nparallel or not, batch size, etc), and often fail to account for model size,\nmaking it difficult to interpret and obscuring the evaluation of the\nefficiency-effectiveness tradeoff. To address this issue, we propose\nE\\textsuperscript{2}R-FLOPs, for LLM-based rerankers: ranking metrics per\nPetaFLOP (RPP) for relevance per compute and queries per PetaFLOP (QPP) for\nhardware-agnostic throughput. Companied with the new metrics, an interpretable\nFLOPs estimator is built to estimate the FLOPs of an LLM-based reranker even\nwithout running any experiments. Based on the proposed metrics, we conduct\ncomprehensive experiments to evaluate a wide range of LLM-based rerankers with\ndifferent architecture, studying the efficiency-effectiveness trade-off and\nbringing this issue to the attention of the research community."
                },
                "authors": [
                    {
                        "name": "Zhiyuan Peng"
                    },
                    {
                        "name": "Ting-ruen Wei"
                    },
                    {
                        "name": "Tingyu Song"
                    },
                    {
                        "name": "Yilun Zhao"
                    },
                    {
                        "name": "Yi Fang"
                    }
                ],
                "author_detail": {
                    "name": "Yi Fang"
                },
                "author": "Yi Fang",
                "arxiv_comment": "under review",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.06223v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.06223v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.13734v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.13734v2",
                "updated": "2025-07-08T17:48:59Z",
                "updated_parsed": [
                    2025,
                    7,
                    8,
                    17,
                    48,
                    59,
                    1,
                    189,
                    0
                ],
                "published": "2025-06-16T17:42:35Z",
                "published_parsed": [
                    2025,
                    6,
                    16,
                    17,
                    42,
                    35,
                    0,
                    167,
                    0
                ],
                "title": "Instruction Following by Boosting Attention of Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Instruction Following by Boosting Attention of Large Language Models"
                },
                "summary": "Controlling the generation of large language models (LLMs) remains a central\nchallenge to ensure their safe and reliable deployment. While prompt\nengineering and finetuning are common approaches, recent work has explored\nlatent steering, a lightweight technique that alters LLM internal activations\nto guide generation. However, subsequent studies revealed latent steering's\neffectiveness to be limited, often underperforming simple instruction\nprompting. To address this limitation, we first establish a benchmark across\ndiverse behaviors for standardized evaluation of steering techniques. Building\non insights from this benchmark, we introduce Instruction Attention Boosting\n(InstABoost), a latent steering method that boosts the strength of instruction\nprompting by altering the model's attention during generation. InstABoost\ncombines the strengths of existing approaches and is theoretically supported by\nprior work that suggests that in-context rule following in transformer-based\nmodels can be controlled by manipulating attention on instructions.\nEmpirically, InstABoost demonstrates superior control success compared to both\ntraditional prompting and latent steering.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Controlling the generation of large language models (LLMs) remains a central\nchallenge to ensure their safe and reliable deployment. While prompt\nengineering and finetuning are common approaches, recent work has explored\nlatent steering, a lightweight technique that alters LLM internal activations\nto guide generation. However, subsequent studies revealed latent steering's\neffectiveness to be limited, often underperforming simple instruction\nprompting. To address this limitation, we first establish a benchmark across\ndiverse behaviors for standardized evaluation of steering techniques. Building\non insights from this benchmark, we introduce Instruction Attention Boosting\n(InstABoost), a latent steering method that boosts the strength of instruction\nprompting by altering the model's attention during generation. InstABoost\ncombines the strengths of existing approaches and is theoretically supported by\nprior work that suggests that in-context rule following in transformer-based\nmodels can be controlled by manipulating attention on instructions.\nEmpirically, InstABoost demonstrates superior control success compared to both\ntraditional prompting and latent steering."
                },
                "authors": [
                    {
                        "name": "Vitoria Guardieiro"
                    },
                    {
                        "name": "Adam Stein"
                    },
                    {
                        "name": "Avishree Khare"
                    },
                    {
                        "name": "Eric Wong"
                    }
                ],
                "author_detail": {
                    "name": "Eric Wong"
                },
                "author": "Eric Wong",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.13734v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.13734v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.06206v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.06206v1",
                "updated": "2025-07-08T17:30:59Z",
                "updated_parsed": [
                    2025,
                    7,
                    8,
                    17,
                    30,
                    59,
                    1,
                    189,
                    0
                ],
                "published": "2025-07-08T17:30:59Z",
                "published_parsed": [
                    2025,
                    7,
                    8,
                    17,
                    30,
                    59,
                    1,
                    189,
                    0
                ],
                "title": "Direct imaging discovery of a young giant planet orbiting on Solar\n  System scales",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Direct imaging discovery of a young giant planet orbiting on Solar\n  System scales"
                },
                "summary": "HD 135344 AB is a young visual binary system that is best known for the\nprotoplanetary disk around the secondary star. The circumstellar environment of\nthe A0-type primary star, on the other hand, is already depleted. HD 135344 A\nis therefore an ideal target for the exploration of recently formed giant\nplanets because it is not obscured by dust. We searched for and characterized\nsubstellar companions to HD 135344 A down to separations of about 10 au. We\nobserved HD 135344 A with VLT/SPHERE in the $H23$ and $K12$ bands and obtained\n$YJ$ and $YJH$ spectroscopy. In addition, we carried out VLTI/GRAVITY\nobservations for the further astrometric and spectroscopic confirmation of a\ndetected companion. We discovered a close-in young giant planet, HD 135344 Ab,\nwith a mass of about 10 $M_\\mathrm{J}$. The multi-epoch astrometry confirms the\nbound nature based on common parallax and common proper motion. This firmly\nrules out the scenario of a non-stationary background star. The semi-major axis\nof the planetary orbit is approximately 15-20 au, and the photometry is\nconsistent with that of a mid L-type object. The inferred atmospheric and bulk\nparameters further confirm the young and planetary nature of the companion. HD\n135344 Ab is one of the youngest directly imaged planets that has fully formed\nand orbits on Solar System scales. It is a valuable target for studying the\nearly evolution and atmosphere of a giant planet that could have formed in the\nvicinity of the snowline.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "HD 135344 AB is a young visual binary system that is best known for the\nprotoplanetary disk around the secondary star. The circumstellar environment of\nthe A0-type primary star, on the other hand, is already depleted. HD 135344 A\nis therefore an ideal target for the exploration of recently formed giant\nplanets because it is not obscured by dust. We searched for and characterized\nsubstellar companions to HD 135344 A down to separations of about 10 au. We\nobserved HD 135344 A with VLT/SPHERE in the $H23$ and $K12$ bands and obtained\n$YJ$ and $YJH$ spectroscopy. In addition, we carried out VLTI/GRAVITY\nobservations for the further astrometric and spectroscopic confirmation of a\ndetected companion. We discovered a close-in young giant planet, HD 135344 Ab,\nwith a mass of about 10 $M_\\mathrm{J}$. The multi-epoch astrometry confirms the\nbound nature based on common parallax and common proper motion. This firmly\nrules out the scenario of a non-stationary background star. The semi-major axis\nof the planetary orbit is approximately 15-20 au, and the photometry is\nconsistent with that of a mid L-type object. The inferred atmospheric and bulk\nparameters further confirm the young and planetary nature of the companion. HD\n135344 Ab is one of the youngest directly imaged planets that has fully formed\nand orbits on Solar System scales. It is a valuable target for studying the\nearly evolution and atmosphere of a giant planet that could have formed in the\nvicinity of the snowline."
                },
                "authors": [
                    {
                        "name": "T. Stolker"
                    },
                    {
                        "name": "M. Samland"
                    },
                    {
                        "name": "L. B. F. M. Waters"
                    },
                    {
                        "name": "M. E. van den Ancker"
                    },
                    {
                        "name": "W. O. Balmer"
                    },
                    {
                        "name": "S. Lacour"
                    },
                    {
                        "name": "M. L. Sitko"
                    },
                    {
                        "name": "J. J. Wang"
                    },
                    {
                        "name": "M. Nowak"
                    },
                    {
                        "name": "A. -L. Maire"
                    },
                    {
                        "name": "J. Kammerer"
                    },
                    {
                        "name": "G. P. P. L. Otten"
                    },
                    {
                        "name": "R. Abuter"
                    },
                    {
                        "name": "A. Amorim"
                    },
                    {
                        "name": "M. Benisty"
                    },
                    {
                        "name": "J. -P. Berger"
                    },
                    {
                        "name": "H. Beust"
                    },
                    {
                        "name": "S. Blunt"
                    },
                    {
                        "name": "A. Boccaletti"
                    },
                    {
                        "name": "M. Bonnefoy"
                    },
                    {
                        "name": "H. Bonnet"
                    },
                    {
                        "name": "M. S. Bordoni"
                    },
                    {
                        "name": "G. Bourdarot"
                    },
                    {
                        "name": "W. Brandner"
                    },
                    {
                        "name": "F. Cantalloube"
                    },
                    {
                        "name": "P. Caselli"
                    },
                    {
                        "name": "B. Charnay"
                    },
                    {
                        "name": "G. Chauvin"
                    },
                    {
                        "name": "A. Chavez"
                    },
                    {
                        "name": "A. Chomez"
                    },
                    {
                        "name": "E. Choquet"
                    },
                    {
                        "name": "V. Christiaens"
                    },
                    {
                        "name": "Y. ClÃ©net"
                    },
                    {
                        "name": "V. CoudÃ© du Foresto"
                    },
                    {
                        "name": "A. Cridland"
                    },
                    {
                        "name": "R. Davies"
                    },
                    {
                        "name": "R. Dembet"
                    },
                    {
                        "name": "J. Dexter"
                    },
                    {
                        "name": "C. Dominik"
                    },
                    {
                        "name": "A. Drescher"
                    },
                    {
                        "name": "G. Duvert"
                    },
                    {
                        "name": "A. Eckart"
                    },
                    {
                        "name": "F. Eisenhauer"
                    },
                    {
                        "name": "N. M. FÃ¶rster Schreiber"
                    },
                    {
                        "name": "P. Garcia"
                    },
                    {
                        "name": "R. Garcia Lopez"
                    },
                    {
                        "name": "T. Gardner"
                    },
                    {
                        "name": "E. Gendron"
                    },
                    {
                        "name": "R. Genzel"
                    },
                    {
                        "name": "S. Gillessen"
                    },
                    {
                        "name": "J. H. Girard"
                    },
                    {
                        "name": "S. Grant"
                    },
                    {
                        "name": "X. Haubois"
                    },
                    {
                        "name": "G. HeiÃel"
                    },
                    {
                        "name": "Th. Henning"
                    },
                    {
                        "name": "S. Hinkley"
                    },
                    {
                        "name": "S. Hippler"
                    },
                    {
                        "name": "M. HoullÃ©"
                    },
                    {
                        "name": "Z. Hubert"
                    },
                    {
                        "name": "L. Jocou"
                    },
                    {
                        "name": "M. Keppler"
                    },
                    {
                        "name": "P. Kervella"
                    },
                    {
                        "name": "L. Kreidberg"
                    },
                    {
                        "name": "N. T. Kurtovic"
                    },
                    {
                        "name": "A. -M. Lagrange"
                    },
                    {
                        "name": "V. LapeyrÃ¨re"
                    },
                    {
                        "name": "J. -B. Le Bouquin"
                    },
                    {
                        "name": "D. Lutz"
                    },
                    {
                        "name": "F. Mang"
                    },
                    {
                        "name": "G. -D. Marleau"
                    },
                    {
                        "name": "A. MÃ©rand"
                    },
                    {
                        "name": "M. Min"
                    },
                    {
                        "name": "P. MolliÃ¨re"
                    },
                    {
                        "name": "J. D. Monnier"
                    },
                    {
                        "name": "C. Mordasini"
                    },
                    {
                        "name": "D. Mouillet"
                    },
                    {
                        "name": "E. Nasedkin"
                    },
                    {
                        "name": "T. Ott"
                    },
                    {
                        "name": "C. Paladini"
                    },
                    {
                        "name": "T. Paumard"
                    },
                    {
                        "name": "K. Perraut"
                    },
                    {
                        "name": "G. Perrin"
                    },
                    {
                        "name": "O. Pfuhl"
                    },
                    {
                        "name": "N. PourrÃ©"
                    },
                    {
                        "name": "L. Pueyo"
                    },
                    {
                        "name": "S. P. Quanz"
                    },
                    {
                        "name": "D. C. Ribeiro"
                    },
                    {
                        "name": "E. Rickman"
                    },
                    {
                        "name": "Z. Rustamkulov"
                    },
                    {
                        "name": "J. Shangguan"
                    },
                    {
                        "name": "T. Shimizu"
                    },
                    {
                        "name": "D. Sing"
                    },
                    {
                        "name": "J. Stadler"
                    },
                    {
                        "name": "O. Straub"
                    },
                    {
                        "name": "C. Straubmeier"
                    },
                    {
                        "name": "E. Sturm"
                    },
                    {
                        "name": "L. J. Tacconi"
                    },
                    {
                        "name": "E. F. van Dishoeck"
                    },
                    {
                        "name": "A. Vigan"
                    },
                    {
                        "name": "F. Vincent"
                    },
                    {
                        "name": "S. D. von Fellenberg"
                    },
                    {
                        "name": "F. Widmann"
                    },
                    {
                        "name": "T. O. Winterhalder"
                    },
                    {
                        "name": "J. Woillez"
                    },
                    {
                        "name": "S. Yazici"
                    }
                ],
                "author_detail": {
                    "name": "S. Yazici"
                },
                "author": "S. Yazici",
                "arxiv_comment": "13 pages, 11 figures, accepted for publication in A&A",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.06206v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.06206v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.EP",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.EP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.06205v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.06205v1",
                "updated": "2025-07-08T17:30:18Z",
                "updated_parsed": [
                    2025,
                    7,
                    8,
                    17,
                    30,
                    18,
                    1,
                    189,
                    0
                ],
                "published": "2025-07-08T17:30:18Z",
                "published_parsed": [
                    2025,
                    7,
                    8,
                    17,
                    30,
                    18,
                    1,
                    189,
                    0
                ],
                "title": "DS@GT at CheckThat! 2025: Ensemble Methods for Detection of Scientific\n  Discourse on Social Media",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DS@GT at CheckThat! 2025: Ensemble Methods for Detection of Scientific\n  Discourse on Social Media"
                },
                "summary": "In this paper, we, as the DS@GT team for CLEF 2025 CheckThat! Task 4a\nScientific Web Discourse Detection, present the methods we explored for this\ntask. For this multiclass classification task, we determined if a tweet\ncontained a scientific claim, a reference to a scientific study or publication,\nand/or mentions of scientific entities, such as a university or a scientist. We\npresent 3 modeling approaches for this task: transformer finetuning, few-shot\nprompting of LLMs, and a combined ensemble model whose design was informed by\nearlier experiments. Our team placed 7th in the competition, achieving a\nmacro-averaged F1 score of 0.8611, an improvement over the DeBERTaV3 baseline\nof 0.8375. Our code is available on Github at\nhttps://github.com/dsgt-arc/checkthat-2025-swd/tree/main/subtask-4a.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this paper, we, as the DS@GT team for CLEF 2025 CheckThat! Task 4a\nScientific Web Discourse Detection, present the methods we explored for this\ntask. For this multiclass classification task, we determined if a tweet\ncontained a scientific claim, a reference to a scientific study or publication,\nand/or mentions of scientific entities, such as a university or a scientist. We\npresent 3 modeling approaches for this task: transformer finetuning, few-shot\nprompting of LLMs, and a combined ensemble model whose design was informed by\nearlier experiments. Our team placed 7th in the competition, achieving a\nmacro-averaged F1 score of 0.8611, an improvement over the DeBERTaV3 baseline\nof 0.8375. Our code is available on Github at\nhttps://github.com/dsgt-arc/checkthat-2025-swd/tree/main/subtask-4a."
                },
                "authors": [
                    {
                        "name": "Ayush Parikh"
                    },
                    {
                        "name": "Hoang Thanh Thanh Truong"
                    },
                    {
                        "name": "Jeanette Schofield"
                    },
                    {
                        "name": "Maximilian Heil"
                    }
                ],
                "author_detail": {
                    "name": "Maximilian Heil"
                },
                "author": "Maximilian Heil",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.06205v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.06205v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.06204v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.06204v1",
                "updated": "2025-07-08T17:30:14Z",
                "updated_parsed": [
                    2025,
                    7,
                    8,
                    17,
                    30,
                    14,
                    1,
                    189,
                    0
                ],
                "published": "2025-07-08T17:30:14Z",
                "published_parsed": [
                    2025,
                    7,
                    8,
                    17,
                    30,
                    14,
                    1,
                    189,
                    0
                ],
                "title": "Differential Mamba",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Differential Mamba"
                },
                "summary": "Sequence models like Transformers and RNNs often overallocate attention to\nirrelevant context, leading to noisy intermediate representations. This\ndegrades LLM capabilities by promoting hallucinations, weakening long-range and\nretrieval abilities, and reducing robustness. Recent work has shown that\ndifferential design can mitigate this issue in Transformers, improving their\neffectiveness across various applications. In this paper, we explore whether\nthese techniques, originally developed for Transformers, can be applied to\nMamba, a recent architecture based on selective state-space layers that\nachieves Transformer-level performance with greater efficiency. We show that a\nnaive adaptation of differential design to Mamba is insufficient and requires\ncareful architectural modifications. To address this, we introduce a novel\ndifferential mechanism for Mamba, empirically validated on language modeling\nbenchmarks, demonstrating improved retrieval capabilities and superior\nperformance over vanilla Mamba. Finally, we conduct extensive ablation studies\nand empirical analyses to justify our design choices and provide evidence that\nour approach effectively mitigates the overallocation problem in Mamba-based\nmodels. Our code is publicly available.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Sequence models like Transformers and RNNs often overallocate attention to\nirrelevant context, leading to noisy intermediate representations. This\ndegrades LLM capabilities by promoting hallucinations, weakening long-range and\nretrieval abilities, and reducing robustness. Recent work has shown that\ndifferential design can mitigate this issue in Transformers, improving their\neffectiveness across various applications. In this paper, we explore whether\nthese techniques, originally developed for Transformers, can be applied to\nMamba, a recent architecture based on selective state-space layers that\nachieves Transformer-level performance with greater efficiency. We show that a\nnaive adaptation of differential design to Mamba is insufficient and requires\ncareful architectural modifications. To address this, we introduce a novel\ndifferential mechanism for Mamba, empirically validated on language modeling\nbenchmarks, demonstrating improved retrieval capabilities and superior\nperformance over vanilla Mamba. Finally, we conduct extensive ablation studies\nand empirical analyses to justify our design choices and provide evidence that\nour approach effectively mitigates the overallocation problem in Mamba-based\nmodels. Our code is publicly available."
                },
                "authors": [
                    {
                        "name": "Nadav Schneider"
                    },
                    {
                        "name": "Itamar Zimerman"
                    },
                    {
                        "name": "Eliya Nachmani"
                    }
                ],
                "author_detail": {
                    "name": "Eliya Nachmani"
                },
                "author": "Eliya Nachmani",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.06204v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.06204v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.06203v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.06203v1",
                "updated": "2025-07-08T17:29:07Z",
                "updated_parsed": [
                    2025,
                    7,
                    8,
                    17,
                    29,
                    7,
                    1,
                    189,
                    0
                ],
                "published": "2025-07-08T17:29:07Z",
                "published_parsed": [
                    2025,
                    7,
                    8,
                    17,
                    29,
                    7,
                    1,
                    189,
                    0
                ],
                "title": "A Survey on Latent Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Survey on Latent Reasoning"
                },
                "summary": "Large Language Models (LLMs) have demonstrated impressive reasoning\ncapabilities, especially when guided by explicit chain-of-thought (CoT)\nreasoning that verbalizes intermediate steps. While CoT improves both\ninterpretability and accuracy, its dependence on natural language reasoning\nlimits the model's expressive bandwidth. Latent reasoning tackles this\nbottleneck by performing multi-step inference entirely in the model's\ncontinuous hidden state, eliminating token-level supervision. To advance latent\nreasoning research, this survey provides a comprehensive overview of the\nemerging field of latent reasoning. We begin by examining the foundational role\nof neural network layers as the computational substrate for reasoning,\nhighlighting how hierarchical representations support complex transformations.\nNext, we explore diverse latent reasoning methodologies, including\nactivation-based recurrence, hidden state propagation, and fine-tuning\nstrategies that compress or internalize explicit reasoning traces. Finally, we\ndiscuss advanced paradigms such as infinite-depth latent reasoning via masked\ndiffusion models, which enable globally consistent and reversible reasoning\nprocesses. By unifying these perspectives, we aim to clarify the conceptual\nlandscape of latent reasoning and chart future directions for research at the\nfrontier of LLM cognition. An associated GitHub repository collecting the\nlatest papers and repos is available at:\nhttps://github.com/multimodal-art-projection/LatentCoT-Horizon/.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have demonstrated impressive reasoning\ncapabilities, especially when guided by explicit chain-of-thought (CoT)\nreasoning that verbalizes intermediate steps. While CoT improves both\ninterpretability and accuracy, its dependence on natural language reasoning\nlimits the model's expressive bandwidth. Latent reasoning tackles this\nbottleneck by performing multi-step inference entirely in the model's\ncontinuous hidden state, eliminating token-level supervision. To advance latent\nreasoning research, this survey provides a comprehensive overview of the\nemerging field of latent reasoning. We begin by examining the foundational role\nof neural network layers as the computational substrate for reasoning,\nhighlighting how hierarchical representations support complex transformations.\nNext, we explore diverse latent reasoning methodologies, including\nactivation-based recurrence, hidden state propagation, and fine-tuning\nstrategies that compress or internalize explicit reasoning traces. Finally, we\ndiscuss advanced paradigms such as infinite-depth latent reasoning via masked\ndiffusion models, which enable globally consistent and reversible reasoning\nprocesses. By unifying these perspectives, we aim to clarify the conceptual\nlandscape of latent reasoning and chart future directions for research at the\nfrontier of LLM cognition. An associated GitHub repository collecting the\nlatest papers and repos is available at:\nhttps://github.com/multimodal-art-projection/LatentCoT-Horizon/."
                },
                "authors": [
                    {
                        "name": "Rui-Jie Zhu"
                    },
                    {
                        "name": "Tianhao Peng"
                    },
                    {
                        "name": "Tianhao Cheng"
                    },
                    {
                        "name": "Xingwei Qu"
                    },
                    {
                        "name": "Jinfa Huang"
                    },
                    {
                        "name": "Dawei Zhu"
                    },
                    {
                        "name": "Hao Wang"
                    },
                    {
                        "name": "Kaiwen Xue"
                    },
                    {
                        "name": "Xuanliang Zhang"
                    },
                    {
                        "name": "Yong Shan"
                    },
                    {
                        "name": "Tianle Cai"
                    },
                    {
                        "name": "Taylor Kergan"
                    },
                    {
                        "name": "Assel Kembay"
                    },
                    {
                        "name": "Andrew Smith"
                    },
                    {
                        "name": "Chenghua Lin"
                    },
                    {
                        "name": "Binh Nguyen"
                    },
                    {
                        "name": "Yuqi Pan"
                    },
                    {
                        "name": "Yuhong Chou"
                    },
                    {
                        "name": "Zefan Cai"
                    },
                    {
                        "name": "Zhenhe Wu"
                    },
                    {
                        "name": "Yongchi Zhao"
                    },
                    {
                        "name": "Tianyu Liu"
                    },
                    {
                        "name": "Jian Yang"
                    },
                    {
                        "name": "Wangchunshu Zhou"
                    },
                    {
                        "name": "Chujie Zheng"
                    },
                    {
                        "name": "Chongxuan Li"
                    },
                    {
                        "name": "Yuyin Zhou"
                    },
                    {
                        "name": "Zhoujun Li"
                    },
                    {
                        "name": "Zhaoxiang Zhang"
                    },
                    {
                        "name": "Jiaheng Liu"
                    },
                    {
                        "name": "Ge Zhang"
                    },
                    {
                        "name": "Wenhao Huang"
                    },
                    {
                        "name": "Jason Eshraghian"
                    }
                ],
                "author_detail": {
                    "name": "Jason Eshraghian"
                },
                "author": "Jason Eshraghian",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.06203v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.06203v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.21898v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.21898v2",
                "updated": "2025-07-08T17:26:59Z",
                "updated_parsed": [
                    2025,
                    7,
                    8,
                    17,
                    26,
                    59,
                    1,
                    189,
                    0
                ],
                "published": "2025-06-27T04:35:52Z",
                "published_parsed": [
                    2025,
                    6,
                    27,
                    4,
                    35,
                    52,
                    4,
                    178,
                    0
                ],
                "title": "Bias, Accuracy, and Trust: Gender-Diverse Perspectives on Large Language\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Bias, Accuracy, and Trust: Gender-Diverse Perspectives on Large Language\n  Models"
                },
                "summary": "Large language models (LLMs) are becoming increasingly ubiquitous in our\ndaily lives, but numerous concerns about bias in LLMs exist. This study\nexamines how gender-diverse populations perceive bias, accuracy, and\ntrustworthiness in LLMs, specifically ChatGPT. Through 25 in-depth interviews\nwith non-binary/transgender, male, and female participants, we investigate how\ngendered and neutral prompts influence model responses and how users evaluate\nthese responses. Our findings reveal that gendered prompts elicit more\nidentity-specific responses, with non-binary participants particularly\nsusceptible to condescending and stereotypical portrayals. Perceived accuracy\nwas consistent across gender groups, with errors most noted in technical topics\nand creative tasks. Trustworthiness varied by gender, with men showing higher\ntrust, especially in performance, and non-binary participants demonstrating\nhigher performance-based trust. Additionally, participants suggested improving\nthe LLMs by diversifying training data, ensuring equal depth in gendered\nresponses, and incorporating clarifying questions. This research contributes to\nthe CSCW/HCI field by highlighting the need for gender-diverse perspectives in\nLLM development in particular and AI in general, to foster more inclusive and\ntrustworthy systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) are becoming increasingly ubiquitous in our\ndaily lives, but numerous concerns about bias in LLMs exist. This study\nexamines how gender-diverse populations perceive bias, accuracy, and\ntrustworthiness in LLMs, specifically ChatGPT. Through 25 in-depth interviews\nwith non-binary/transgender, male, and female participants, we investigate how\ngendered and neutral prompts influence model responses and how users evaluate\nthese responses. Our findings reveal that gendered prompts elicit more\nidentity-specific responses, with non-binary participants particularly\nsusceptible to condescending and stereotypical portrayals. Perceived accuracy\nwas consistent across gender groups, with errors most noted in technical topics\nand creative tasks. Trustworthiness varied by gender, with men showing higher\ntrust, especially in performance, and non-binary participants demonstrating\nhigher performance-based trust. Additionally, participants suggested improving\nthe LLMs by diversifying training data, ensuring equal depth in gendered\nresponses, and incorporating clarifying questions. This research contributes to\nthe CSCW/HCI field by highlighting the need for gender-diverse perspectives in\nLLM development in particular and AI in general, to foster more inclusive and\ntrustworthy systems."
                },
                "authors": [
                    {
                        "name": "Aimen Gaba"
                    },
                    {
                        "name": "Emily Wall"
                    },
                    {
                        "name": "Tejas Ramkumar Babu"
                    },
                    {
                        "name": "Yuriy Brun"
                    },
                    {
                        "name": "Kyle Hall"
                    },
                    {
                        "name": "Cindy Xiong Bearfield"
                    }
                ],
                "author_detail": {
                    "name": "Cindy Xiong Bearfield"
                },
                "author": "Cindy Xiong Bearfield",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.21898v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.21898v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.20815v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.20815v2",
                "updated": "2025-07-08T17:25:34Z",
                "updated_parsed": [
                    2025,
                    7,
                    8,
                    17,
                    25,
                    34,
                    1,
                    189,
                    0
                ],
                "published": "2025-06-25T20:29:46Z",
                "published_parsed": [
                    2025,
                    6,
                    25,
                    20,
                    29,
                    46,
                    2,
                    176,
                    0
                ],
                "title": "Dynamic Context-Aware Prompt Recommendation for Domain-Specific AI\n  Applications",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Dynamic Context-Aware Prompt Recommendation for Domain-Specific AI\n  Applications"
                },
                "summary": "LLM-powered applications are highly susceptible to the quality of user\nprompts, and crafting high-quality prompts can often be challenging especially\nfor domain-specific applications. This paper presents a novel dynamic\ncontext-aware prompt recommendation system for domain-specific AI applications.\nOur solution combines contextual query analysis, retrieval-augmented knowledge\ngrounding, hierarchical skill organization, and adaptive skill ranking to\ngenerate relevant and actionable prompt suggestions.\n  The system leverages behavioral telemetry and a two-stage hierarchical\nreasoning process to dynamically select and rank relevant skills, and\nsynthesizes prompts using both predefined and adaptive templates enhanced with\nfew-shot learning. Experiments on real-world datasets demonstrate that our\napproach achieves high usefulness and relevance, as validated by both automated\nand expert evaluations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM-powered applications are highly susceptible to the quality of user\nprompts, and crafting high-quality prompts can often be challenging especially\nfor domain-specific applications. This paper presents a novel dynamic\ncontext-aware prompt recommendation system for domain-specific AI applications.\nOur solution combines contextual query analysis, retrieval-augmented knowledge\ngrounding, hierarchical skill organization, and adaptive skill ranking to\ngenerate relevant and actionable prompt suggestions.\n  The system leverages behavioral telemetry and a two-stage hierarchical\nreasoning process to dynamically select and rank relevant skills, and\nsynthesizes prompts using both predefined and adaptive templates enhanced with\nfew-shot learning. Experiments on real-world datasets demonstrate that our\napproach achieves high usefulness and relevance, as validated by both automated\nand expert evaluations."
                },
                "authors": [
                    {
                        "name": "Xinye Tang"
                    },
                    {
                        "name": "Haijun Zhai"
                    },
                    {
                        "name": "Chaitanya Belwal"
                    },
                    {
                        "name": "Vineeth Thayanithi"
                    },
                    {
                        "name": "Philip Baumann"
                    },
                    {
                        "name": "Yogesh K Roy"
                    }
                ],
                "author_detail": {
                    "name": "Yogesh K Roy"
                },
                "author": "Yogesh K Roy",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.20815v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.20815v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.06196v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.06196v1",
                "updated": "2025-07-08T17:22:32Z",
                "updated_parsed": [
                    2025,
                    7,
                    8,
                    17,
                    22,
                    32,
                    1,
                    189,
                    0
                ],
                "published": "2025-07-08T17:22:32Z",
                "published_parsed": [
                    2025,
                    7,
                    8,
                    17,
                    22,
                    32,
                    1,
                    189,
                    0
                ],
                "title": "UQLM: A Python Package for Uncertainty Quantification in Large Language\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "UQLM: A Python Package for Uncertainty Quantification in Large Language\n  Models"
                },
                "summary": "Hallucinations, defined as instances where Large Language Models (LLMs)\ngenerate false or misleading content, pose a significant challenge that impacts\nthe safety and trust of downstream applications. We introduce UQLM, a Python\npackage for LLM hallucination detection using state-of-the-art uncertainty\nquantification (UQ) techniques. This toolkit offers a suite of UQ-based scorers\nthat compute response-level confidence scores ranging from 0 to 1. This library\nprovides an off-the-shelf solution for UQ-based hallucination detection that\ncan be easily integrated to enhance the reliability of LLM outputs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Hallucinations, defined as instances where Large Language Models (LLMs)\ngenerate false or misleading content, pose a significant challenge that impacts\nthe safety and trust of downstream applications. We introduce UQLM, a Python\npackage for LLM hallucination detection using state-of-the-art uncertainty\nquantification (UQ) techniques. This toolkit offers a suite of UQ-based scorers\nthat compute response-level confidence scores ranging from 0 to 1. This library\nprovides an off-the-shelf solution for UQ-based hallucination detection that\ncan be easily integrated to enhance the reliability of LLM outputs."
                },
                "authors": [
                    {
                        "name": "Dylan Bouchard"
                    },
                    {
                        "name": "Mohit Singh Chauhan"
                    },
                    {
                        "name": "David Skarbrevik"
                    },
                    {
                        "name": "Ho-Kyeong Ra"
                    },
                    {
                        "name": "Viren Bajaj"
                    },
                    {
                        "name": "Zeya Ahmad"
                    }
                ],
                "author_detail": {
                    "name": "Zeya Ahmad"
                },
                "author": "Zeya Ahmad",
                "arxiv_comment": "Submitted to Journal of Machine Learning Research (MLOSS); UQLM\n  Repository: https://github.com/cvs-health/uqlm",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.06196v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.06196v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.06195v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.06195v1",
                "updated": "2025-07-08T17:22:22Z",
                "updated_parsed": [
                    2025,
                    7,
                    8,
                    17,
                    22,
                    22,
                    1,
                    189,
                    0
                ],
                "published": "2025-07-08T17:22:22Z",
                "published_parsed": [
                    2025,
                    7,
                    8,
                    17,
                    22,
                    22,
                    1,
                    189,
                    0
                ],
                "title": "DS@GT at CheckThat! 2025: Evaluating Context and Tokenization Strategies\n  for Numerical Fact Verification",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DS@GT at CheckThat! 2025: Evaluating Context and Tokenization Strategies\n  for Numerical Fact Verification"
                },
                "summary": "Numerical claims, statements involving quantities, comparisons, and temporal\nreferences, pose unique challenges for automated fact-checking systems. In this\nstudy, we evaluate modeling strategies for veracity prediction of such claims\nusing the QuanTemp dataset and building our own evidence retrieval pipeline. We\ninvestigate three key factors: (1) the impact of more evidences with longer\ninput context windows using ModernBERT, (2) the effect of right-to-left (R2L)\ntokenization, and (3) their combined influence on classification performance.\nContrary to prior findings in arithmetic reasoning tasks, R2L tokenization does\nnot boost natural language inference (NLI) of numerical tasks. A longer context\nwindow does also not enhance veracity performance either, highlighting evidence\nquality as the dominant bottleneck. Our best-performing system achieves\ncompetitive macro-average F1 score of 0.57 and places us among the Top-4\nsubmissions in Task 3 of CheckThat! 2025. Our code is available at\nhttps://github.com/dsgt-arc/checkthat-2025-numerical.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Numerical claims, statements involving quantities, comparisons, and temporal\nreferences, pose unique challenges for automated fact-checking systems. In this\nstudy, we evaluate modeling strategies for veracity prediction of such claims\nusing the QuanTemp dataset and building our own evidence retrieval pipeline. We\ninvestigate three key factors: (1) the impact of more evidences with longer\ninput context windows using ModernBERT, (2) the effect of right-to-left (R2L)\ntokenization, and (3) their combined influence on classification performance.\nContrary to prior findings in arithmetic reasoning tasks, R2L tokenization does\nnot boost natural language inference (NLI) of numerical tasks. A longer context\nwindow does also not enhance veracity performance either, highlighting evidence\nquality as the dominant bottleneck. Our best-performing system achieves\ncompetitive macro-average F1 score of 0.57 and places us among the Top-4\nsubmissions in Task 3 of CheckThat! 2025. Our code is available at\nhttps://github.com/dsgt-arc/checkthat-2025-numerical."
                },
                "authors": [
                    {
                        "name": "Maximilian Heil"
                    },
                    {
                        "name": "Aleksandar Pramov"
                    }
                ],
                "author_detail": {
                    "name": "Aleksandar Pramov"
                },
                "author": "Aleksandar Pramov",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.06195v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.06195v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.06192v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.06192v1",
                "updated": "2025-07-08T17:20:34Z",
                "updated_parsed": [
                    2025,
                    7,
                    8,
                    17,
                    20,
                    34,
                    1,
                    189,
                    0
                ],
                "published": "2025-07-08T17:20:34Z",
                "published_parsed": [
                    2025,
                    7,
                    8,
                    17,
                    20,
                    34,
                    1,
                    189,
                    0
                ],
                "title": "SQLBarber: A System Leveraging Large Language Models to Generate\n  Customized and Realistic SQL Workloads",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SQLBarber: A System Leveraging Large Language Models to Generate\n  Customized and Realistic SQL Workloads"
                },
                "summary": "Database research and development often require a large number of SQL queries\nfor benchmarking purposes. However, acquiring real-world SQL queries is\nchallenging due to privacy concerns, and existing SQL generation methods are\nlimited in customization and in satisfying realistic constraints. To address\nthis issue, we present SQLBarber, a system based on Large Language Models\n(LLMs) to generate customized and realistic SQL workloads. SQLBarber (i)\neliminates the need for users to manually craft SQL templates in advance, while\nproviding the flexibility to accept natural language specifications to\nconstrain SQL templates, (ii) scales efficiently to generate large volumes of\nqueries matching any user-defined cost distribution (e.g., cardinality and\nexecution plan cost), and (iii) uses execution statistics from Amazon Redshift\nand Snowflake to derive SQL template specifications and query cost\ndistributions that reflect real-world query characteristics. SQLBarber\nintroduces (i) a declarative interface for users to effortlessly generate\ncustomized SQL templates, (ii) an LLM-powered pipeline augmented with a\nself-correction module that profiles, refines, and prunes SQL templates based\non query costs, and (iii) a Bayesian Optimizer to efficiently explore different\npredicate values and identify a set of queries that satisfy the target cost\ndistribution. We construct and open-source ten benchmarks of varying difficulty\nlevels and target query cost distributions based on real-world statistics from\nSnowflake and Amazon Redshift. Extensive experiments on these benchmarks show\nthat SQLBarber is the only system that can generate customized SQL templates.\nIt reduces query generation time by one to three orders of magnitude, and\nsignificantly improves alignment with the target cost distribution, compared\nwith existing methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Database research and development often require a large number of SQL queries\nfor benchmarking purposes. However, acquiring real-world SQL queries is\nchallenging due to privacy concerns, and existing SQL generation methods are\nlimited in customization and in satisfying realistic constraints. To address\nthis issue, we present SQLBarber, a system based on Large Language Models\n(LLMs) to generate customized and realistic SQL workloads. SQLBarber (i)\neliminates the need for users to manually craft SQL templates in advance, while\nproviding the flexibility to accept natural language specifications to\nconstrain SQL templates, (ii) scales efficiently to generate large volumes of\nqueries matching any user-defined cost distribution (e.g., cardinality and\nexecution plan cost), and (iii) uses execution statistics from Amazon Redshift\nand Snowflake to derive SQL template specifications and query cost\ndistributions that reflect real-world query characteristics. SQLBarber\nintroduces (i) a declarative interface for users to effortlessly generate\ncustomized SQL templates, (ii) an LLM-powered pipeline augmented with a\nself-correction module that profiles, refines, and prunes SQL templates based\non query costs, and (iii) a Bayesian Optimizer to efficiently explore different\npredicate values and identify a set of queries that satisfy the target cost\ndistribution. We construct and open-source ten benchmarks of varying difficulty\nlevels and target query cost distributions based on real-world statistics from\nSnowflake and Amazon Redshift. Extensive experiments on these benchmarks show\nthat SQLBarber is the only system that can generate customized SQL templates.\nIt reduces query generation time by one to three orders of magnitude, and\nsignificantly improves alignment with the target cost distribution, compared\nwith existing methods."
                },
                "authors": [
                    {
                        "name": "Jiale Lao"
                    },
                    {
                        "name": "Immanuel Trummer"
                    }
                ],
                "author_detail": {
                    "name": "Immanuel Trummer"
                },
                "author": "Immanuel Trummer",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.06192v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.06192v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.03711v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.03711v3",
                "updated": "2025-07-09T02:09:05Z",
                "updated_parsed": [
                    2025,
                    7,
                    9,
                    2,
                    9,
                    5,
                    2,
                    190,
                    0
                ],
                "published": "2025-07-04T16:50:40Z",
                "published_parsed": [
                    2025,
                    7,
                    4,
                    16,
                    50,
                    40,
                    4,
                    185,
                    0
                ],
                "title": "Can LLMs Play Ã Än Quan Game? A Study of Multi-Step Planning and\n  Decision Making",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Can LLMs Play Ã Än Quan Game? A Study of Multi-Step Planning and\n  Decision Making"
                },
                "summary": "In this paper, we explore the ability of large language models (LLMs) to plan\nand make decisions through the lens of the traditional Vietnamese board game,\n\\^O \\u{A}n Quan. This game, which involves a series of strategic token\nmovements and captures, offers a unique environment for evaluating the\ndecision-making and strategic capabilities of LLMs. Specifically, we develop\nvarious agent personas, ranging from aggressive to defensive, and employ the\n\\^O \\u{A}n Quan game as a testbed for assessing LLM performance across\ndifferent strategies. Through experimentation with models like\nLlama-3.2-3B-Instruct, Llama-3.1-8B-Instruct, and Llama-3.3-70B-Instruct, we\naim to understand how these models execute strategic decision-making, plan\nmoves, and manage dynamic game states. The results will offer insights into the\nstrengths and weaknesses of LLMs in terms of reasoning and strategy,\ncontributing to a deeper understanding of their general capabilities.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this paper, we explore the ability of large language models (LLMs) to plan\nand make decisions through the lens of the traditional Vietnamese board game,\n\\^O \\u{A}n Quan. This game, which involves a series of strategic token\nmovements and captures, offers a unique environment for evaluating the\ndecision-making and strategic capabilities of LLMs. Specifically, we develop\nvarious agent personas, ranging from aggressive to defensive, and employ the\n\\^O \\u{A}n Quan game as a testbed for assessing LLM performance across\ndifferent strategies. Through experimentation with models like\nLlama-3.2-3B-Instruct, Llama-3.1-8B-Instruct, and Llama-3.3-70B-Instruct, we\naim to understand how these models execute strategic decision-making, plan\nmoves, and manage dynamic game states. The results will offer insights into the\nstrengths and weaknesses of LLMs in terms of reasoning and strategy,\ncontributing to a deeper understanding of their general capabilities."
                },
                "authors": [
                    {
                        "name": "Sang Quang Nguyen"
                    },
                    {
                        "name": "Kiet Van Nguyen"
                    },
                    {
                        "name": "Vinh-Tiep Nguyen"
                    },
                    {
                        "name": "Thanh Duc Ngo"
                    },
                    {
                        "name": "Ngan Luu-Thuy Nguyen"
                    },
                    {
                        "name": "Duy-Dinh Le"
                    }
                ],
                "author_detail": {
                    "name": "Duy-Dinh Le"
                },
                "author": "Duy-Dinh Le",
                "arxiv_comment": "Accepted paper at MAPR 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.03711v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.03711v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.06185v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.06185v1",
                "updated": "2025-07-08T17:11:13Z",
                "updated_parsed": [
                    2025,
                    7,
                    8,
                    17,
                    11,
                    13,
                    1,
                    189,
                    0
                ],
                "published": "2025-07-08T17:11:13Z",
                "published_parsed": [
                    2025,
                    7,
                    8,
                    17,
                    11,
                    13,
                    1,
                    189,
                    0
                ],
                "title": "Hidden Prompts in Manuscripts Exploit AI-Assisted Peer Review",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Hidden Prompts in Manuscripts Exploit AI-Assisted Peer Review"
                },
                "summary": "In July 2025, 18 academic manuscripts on the preprint website arXiv were\nfound to contain hidden instructions known as prompts designed to manipulate\nAI-assisted peer review. Instructions such as \"GIVE A POSITIVE REVIEW ONLY\"\nwere concealed using techniques like white-colored text. Author responses\nvaried: one planned to withdraw the affected paper, while another defended the\npractice as legitimate testing of reviewer compliance. This commentary analyzes\nthis practice as a novel form of research misconduct. We examine the technique\nof prompt injection in large language models (LLMs), revealing four types of\nhidden prompts, ranging from simple positive review commands to detailed\nevaluation frameworks. The defense that prompts served as \"honeypots\" to detect\nreviewers improperly using AI fails under examination--the consistently\nself-serving nature of prompt instructions indicates intent to manipulate.\nPublishers maintain inconsistent policies: Elsevier prohibits AI use in peer\nreview entirely, while Springer Nature permits limited use with disclosure\nrequirements. The incident exposes systematic vulnerabilities extending beyond\npeer review to any automated system processing scholarly texts, including\nplagiarism detection and citation indexing. Our analysis underscores the need\nfor coordinated technical screening at submission portals and harmonized\npolicies governing generative AI (GenAI) use in academic evaluation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In July 2025, 18 academic manuscripts on the preprint website arXiv were\nfound to contain hidden instructions known as prompts designed to manipulate\nAI-assisted peer review. Instructions such as \"GIVE A POSITIVE REVIEW ONLY\"\nwere concealed using techniques like white-colored text. Author responses\nvaried: one planned to withdraw the affected paper, while another defended the\npractice as legitimate testing of reviewer compliance. This commentary analyzes\nthis practice as a novel form of research misconduct. We examine the technique\nof prompt injection in large language models (LLMs), revealing four types of\nhidden prompts, ranging from simple positive review commands to detailed\nevaluation frameworks. The defense that prompts served as \"honeypots\" to detect\nreviewers improperly using AI fails under examination--the consistently\nself-serving nature of prompt instructions indicates intent to manipulate.\nPublishers maintain inconsistent policies: Elsevier prohibits AI use in peer\nreview entirely, while Springer Nature permits limited use with disclosure\nrequirements. The incident exposes systematic vulnerabilities extending beyond\npeer review to any automated system processing scholarly texts, including\nplagiarism detection and citation indexing. Our analysis underscores the need\nfor coordinated technical screening at submission portals and harmonized\npolicies governing generative AI (GenAI) use in academic evaluation."
                },
                "authors": [
                    {
                        "name": "Zhicheng Lin"
                    }
                ],
                "author_detail": {
                    "name": "Zhicheng Lin"
                },
                "author": "Zhicheng Lin",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.06185v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.06185v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.06171v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.06171v1",
                "updated": "2025-07-08T16:52:37Z",
                "updated_parsed": [
                    2025,
                    7,
                    8,
                    16,
                    52,
                    37,
                    1,
                    189,
                    0
                ],
                "published": "2025-07-08T16:52:37Z",
                "published_parsed": [
                    2025,
                    7,
                    8,
                    16,
                    52,
                    37,
                    1,
                    189,
                    0
                ],
                "title": "Data-Semantics-Aware Recommendation of Diverse Pivot Tables",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Data-Semantics-Aware Recommendation of Diverse Pivot Tables"
                },
                "summary": "Data summarization is essential to discover insights from large datasets. In\na spreadsheets, pivot tables offer a convenient way to summarize tabular data\nby computing aggregates over some attributes, grouped by others. However,\nidentifying attribute combinations that will result in useful pivot tables\nremains a challenge, especially for high-dimensional datasets. We formalize the\nproblem of automatically recommending insightful and interpretable pivot\ntables, eliminating the tedious manual process. A crucial aspect of\nrecommending a set of pivot tables is to diversify them. Traditional works\ninadequately address the table-diversification problem, which leads us to\nconsider the problem of pivot table diversification.\n  We present SAGE, a data-semantics-aware system for recommending k-budgeted\ndiverse pivot tables, overcoming the shortcomings of prior work for top-k\nrecommendations that cause redundancy. SAGE ensures that each pivot table is\ninsightful, interpretable, and adaptive to the user's actions and preferences,\nwhile also guaranteeing that the set of pivot tables are different from each\nother, offering a diverse recommendation. We make two key technical\ncontributions: (1) a data-semantics-aware model to measure the utility of a\nsingle pivot table and the diversity of a set of pivot tables, and (2) a\nscalable greedy algorithm that can efficiently select a set of diverse pivot\ntables of high utility, by leveraging data semantics to significantly reduce\nthe combinatorial search space. Our extensive experiments on three real-world\ndatasets show that SAGE outperforms alternative approaches, and efficiently\nscales to accommodate high-dimensional datasets. Additionally, we present\nseveral case studies to highlight SAGE's qualitative effectiveness over\ncommercial software and Large Language Models (LLMs).",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Data summarization is essential to discover insights from large datasets. In\na spreadsheets, pivot tables offer a convenient way to summarize tabular data\nby computing aggregates over some attributes, grouped by others. However,\nidentifying attribute combinations that will result in useful pivot tables\nremains a challenge, especially for high-dimensional datasets. We formalize the\nproblem of automatically recommending insightful and interpretable pivot\ntables, eliminating the tedious manual process. A crucial aspect of\nrecommending a set of pivot tables is to diversify them. Traditional works\ninadequately address the table-diversification problem, which leads us to\nconsider the problem of pivot table diversification.\n  We present SAGE, a data-semantics-aware system for recommending k-budgeted\ndiverse pivot tables, overcoming the shortcomings of prior work for top-k\nrecommendations that cause redundancy. SAGE ensures that each pivot table is\ninsightful, interpretable, and adaptive to the user's actions and preferences,\nwhile also guaranteeing that the set of pivot tables are different from each\nother, offering a diverse recommendation. We make two key technical\ncontributions: (1) a data-semantics-aware model to measure the utility of a\nsingle pivot table and the diversity of a set of pivot tables, and (2) a\nscalable greedy algorithm that can efficiently select a set of diverse pivot\ntables of high utility, by leveraging data semantics to significantly reduce\nthe combinatorial search space. Our extensive experiments on three real-world\ndatasets show that SAGE outperforms alternative approaches, and efficiently\nscales to accommodate high-dimensional datasets. Additionally, we present\nseveral case studies to highlight SAGE's qualitative effectiveness over\ncommercial software and Large Language Models (LLMs)."
                },
                "authors": [
                    {
                        "name": "Whanhee Cho"
                    },
                    {
                        "name": "Anna Fariha"
                    }
                ],
                "author_detail": {
                    "name": "Anna Fariha"
                },
                "author": "Anna Fariha",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.06171v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.06171v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.04687v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.04687v2",
                "updated": "2025-07-08T16:51:53Z",
                "updated_parsed": [
                    2025,
                    7,
                    8,
                    16,
                    51,
                    53,
                    1,
                    189,
                    0
                ],
                "published": "2025-07-07T06:08:45Z",
                "published_parsed": [
                    2025,
                    7,
                    7,
                    6,
                    8,
                    45,
                    0,
                    188,
                    0
                ],
                "title": "LAKEGEN: A LLM-based Tabular Corpus Generator for Evaluating Dataset\n  Discovery in Data Lakes",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LAKEGEN: A LLM-based Tabular Corpus Generator for Evaluating Dataset\n  Discovery in Data Lakes"
                },
                "summary": "How to generate a large, realistic set of tables along with joinability\nrelationships, to stress-test dataset discovery methods? Dataset discovery\nmethods aim to automatically identify related data assets in a data lake. The\ndevelopment and evaluation of such solutions for customers from a wide range of\nbusiness domains, relies on diverse, high quality and domain-specific tabular\nbenchmarks. Large language models (LLMs) are trained on a wide variety of text\ndata, which can provide a strong foundation of general and domain-specific\nknowledge. In this paper, we ask the question -- \\textit{can we leverage LLMs\nto generate a tabular benchmark adequate for evaluating the dataset discovery\nsolutions?} In particular, we focus on the task of finding joinable tables\nwhich is the cornerstone of virtually every dataset discovery method. Current\ncorpora for evaluating dataset discovery methods are mainly based on subsets of\nopen data, and they suffer from three important issues: $i)$ they focus on very\ncommon and generic data types (e.g., address, id, name, etc.); $ii)$ they do\nnot contain human-annotated column pairs; instead, practitioners synthesize\nground truth using table splits (e.g., horizontal for table union search and\nvertical ones for joinability) and $iii)$ they do not focus on semantic column\nrelationships.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "How to generate a large, realistic set of tables along with joinability\nrelationships, to stress-test dataset discovery methods? Dataset discovery\nmethods aim to automatically identify related data assets in a data lake. The\ndevelopment and evaluation of such solutions for customers from a wide range of\nbusiness domains, relies on diverse, high quality and domain-specific tabular\nbenchmarks. Large language models (LLMs) are trained on a wide variety of text\ndata, which can provide a strong foundation of general and domain-specific\nknowledge. In this paper, we ask the question -- \\textit{can we leverage LLMs\nto generate a tabular benchmark adequate for evaluating the dataset discovery\nsolutions?} In particular, we focus on the task of finding joinable tables\nwhich is the cornerstone of virtually every dataset discovery method. Current\ncorpora for evaluating dataset discovery methods are mainly based on subsets of\nopen data, and they suffer from three important issues: $i)$ they focus on very\ncommon and generic data types (e.g., address, id, name, etc.); $ii)$ they do\nnot contain human-annotated column pairs; instead, practitioners synthesize\nground truth using table splits (e.g., horizontal for table union search and\nvertical ones for joinability) and $iii)$ they do not focus on semantic column\nrelationships."
                },
                "authors": [
                    {
                        "name": "Zhenwei Dai"
                    },
                    {
                        "name": "Chuan Lei"
                    },
                    {
                        "name": "Asterios Katsifodimos"
                    },
                    {
                        "name": "Xiao Qin"
                    },
                    {
                        "name": "Christos Faloutsos"
                    },
                    {
                        "name": "Huzefa Rangwala"
                    }
                ],
                "author_detail": {
                    "name": "Huzefa Rangwala"
                },
                "author": "Huzefa Rangwala",
                "arxiv_comment": "13 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.04687v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.04687v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.06167v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.06167v2",
                "updated": "2025-07-09T01:36:17Z",
                "updated_parsed": [
                    2025,
                    7,
                    9,
                    1,
                    36,
                    17,
                    2,
                    190,
                    0
                ],
                "published": "2025-07-08T16:47:16Z",
                "published_parsed": [
                    2025,
                    7,
                    8,
                    16,
                    47,
                    16,
                    1,
                    189,
                    0
                ],
                "title": "Skywork-R1V3 Technical Report",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Skywork-R1V3 Technical Report"
                },
                "summary": "We introduce Skywork-R1V3, an advanced, open-source vision-language model\n(VLM) that pioneers a new approach to visual reasoning. Its key innovation lies\nin effectively transferring reasoning skills from text-only Large Language\nModels (LLMs) to visual tasks. The strong performance of Skywork-R1V3 primarily\nstems from our elaborate post-training RL framework, which effectively\nactivates and enhances the model's reasoning ability, without the need for\nadditional continue pre-training. Through this framework, we further uncover\nthe fundamental role of the connector module in achieving robust cross-modal\nalignment for multimodal reasoning models. In addition, we introduce a unique\nindicator of reasoning capability, the entropy of critical reasoning tokens,\nwhich has proven highly effective for checkpoint selection during RL training.\nSkywork-R1V3 achieves state-of-the-art results on MMMU, significantly improving\nfrom 64.3% to 76.0%. This performance matches entry-level human capabilities.\nRemarkably, our RL-powered post-training approach enables even the 38B\nparameter model to rival top closed-source VLMs. The implementation\nsuccessfully transfers mathematical reasoning to other subject-related\nreasoning tasks. We also include an analysis of curriculum learning and\nreinforcement finetuning strategies, along with a broader discussion on\nmultimodal reasoning. Skywork-R1V3 represents a significant leap in multimodal\nreasoning, showcasing RL as a powerful engine for advancing open-source VLM\ncapabilities.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce Skywork-R1V3, an advanced, open-source vision-language model\n(VLM) that pioneers a new approach to visual reasoning. Its key innovation lies\nin effectively transferring reasoning skills from text-only Large Language\nModels (LLMs) to visual tasks. The strong performance of Skywork-R1V3 primarily\nstems from our elaborate post-training RL framework, which effectively\nactivates and enhances the model's reasoning ability, without the need for\nadditional continue pre-training. Through this framework, we further uncover\nthe fundamental role of the connector module in achieving robust cross-modal\nalignment for multimodal reasoning models. In addition, we introduce a unique\nindicator of reasoning capability, the entropy of critical reasoning tokens,\nwhich has proven highly effective for checkpoint selection during RL training.\nSkywork-R1V3 achieves state-of-the-art results on MMMU, significantly improving\nfrom 64.3% to 76.0%. This performance matches entry-level human capabilities.\nRemarkably, our RL-powered post-training approach enables even the 38B\nparameter model to rival top closed-source VLMs. The implementation\nsuccessfully transfers mathematical reasoning to other subject-related\nreasoning tasks. We also include an analysis of curriculum learning and\nreinforcement finetuning strategies, along with a broader discussion on\nmultimodal reasoning. Skywork-R1V3 represents a significant leap in multimodal\nreasoning, showcasing RL as a powerful engine for advancing open-source VLM\ncapabilities."
                },
                "authors": [
                    {
                        "name": "Wei Shen"
                    },
                    {
                        "name": "Jiangbo Pei"
                    },
                    {
                        "name": "Yi Peng"
                    },
                    {
                        "name": "Xuchen Song"
                    },
                    {
                        "name": "Yang Liu"
                    },
                    {
                        "name": "Jian Peng"
                    },
                    {
                        "name": "Haofeng Sun"
                    },
                    {
                        "name": "Yunzhuo Hao"
                    },
                    {
                        "name": "Peiyu Wang"
                    },
                    {
                        "name": "Jianhao Zhang"
                    },
                    {
                        "name": "Yahui Zhou"
                    }
                ],
                "author_detail": {
                    "name": "Yahui Zhou"
                },
                "author": "Yahui Zhou",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.06167v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.06167v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.06108v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.06108v4",
                "updated": "2025-07-08T16:40:26Z",
                "updated_parsed": [
                    2025,
                    7,
                    8,
                    16,
                    40,
                    26,
                    1,
                    189,
                    0
                ],
                "published": "2025-01-10T17:01:09Z",
                "published_parsed": [
                    2025,
                    1,
                    10,
                    17,
                    1,
                    9,
                    4,
                    10,
                    0
                ],
                "title": "Inferring Higher-Order Couplings with Neural Networks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Inferring Higher-Order Couplings with Neural Networks"
                },
                "summary": "Maximum entropy methods, rooted in the inverse Ising/Potts problem from\nstatistical physics, are widely used to model pairwise interactions in complex\nsystems across disciplines such as bioinformatics and neuroscience. While\nsuccessful, these approaches often fail to capture higher-order interactions\nthat are critical for understanding collective behavior. In contrast, modern\nmachine learning methods can model such interactions, but their\ninterpretability often comes at a prohibitive computational cost. Restricted\nBoltzmann Machines (RBMs) provide a computationally efficient alternative by\nencoding statistical correlations through hidden units in a bipartite\narchitecture. In this work, we introduce a method that maps RBMs onto\ngeneralized Potts models, enabling the systematic extraction of interactions up\nto arbitrary order. Leveraging large-$N$ approximations, made tractable by the\nRBM's structure, we extract effective many-body couplings with minimal\ncomputational effort. We further propose a robust framework for recovering\nhigher-order interactions in more complex generative models, and introduce a\nsimple gauge-fixing scheme for the effective Potts representation. Validation\non synthetic data demonstrates accurate recovery of two- and three-body\ninteractions. Applied to protein sequence data, our method reconstructs contact\nmaps with high fidelity and outperforms state-of-the-art inverse Potts models.\nThese results establish RBMs as a powerful and efficient tool for modeling\nhigher-order structure in high-dimensional categorical data.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Maximum entropy methods, rooted in the inverse Ising/Potts problem from\nstatistical physics, are widely used to model pairwise interactions in complex\nsystems across disciplines such as bioinformatics and neuroscience. While\nsuccessful, these approaches often fail to capture higher-order interactions\nthat are critical for understanding collective behavior. In contrast, modern\nmachine learning methods can model such interactions, but their\ninterpretability often comes at a prohibitive computational cost. Restricted\nBoltzmann Machines (RBMs) provide a computationally efficient alternative by\nencoding statistical correlations through hidden units in a bipartite\narchitecture. In this work, we introduce a method that maps RBMs onto\ngeneralized Potts models, enabling the systematic extraction of interactions up\nto arbitrary order. Leveraging large-$N$ approximations, made tractable by the\nRBM's structure, we extract effective many-body couplings with minimal\ncomputational effort. We further propose a robust framework for recovering\nhigher-order interactions in more complex generative models, and introduce a\nsimple gauge-fixing scheme for the effective Potts representation. Validation\non synthetic data demonstrates accurate recovery of two- and three-body\ninteractions. Applied to protein sequence data, our method reconstructs contact\nmaps with high fidelity and outperforms state-of-the-art inverse Potts models.\nThese results establish RBMs as a powerful and efficient tool for modeling\nhigher-order structure in high-dimensional categorical data."
                },
                "authors": [
                    {
                        "name": "AurÃ©lien Decelle"
                    },
                    {
                        "name": "Alfonso de JesÃºs Navas GÃ³mez"
                    },
                    {
                        "name": "Beatriz Seoane"
                    }
                ],
                "author_detail": {
                    "name": "Beatriz Seoane"
                },
                "author": "Beatriz Seoane",
                "arxiv_comment": "24 Pages and 9 Figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.06108v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.06108v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cond-mat.dis-nn",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cond-mat.dis-nn",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cond-mat.stat-mech",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.06147v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.06147v1",
                "updated": "2025-07-08T16:28:02Z",
                "updated_parsed": [
                    2025,
                    7,
                    8,
                    16,
                    28,
                    2,
                    1,
                    189,
                    0
                ],
                "published": "2025-07-08T16:28:02Z",
                "published_parsed": [
                    2025,
                    7,
                    8,
                    16,
                    28,
                    2,
                    1,
                    189,
                    0
                ],
                "title": "de Sitter Holography and Carrollian Brane Theories",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "de Sitter Holography and Carrollian Brane Theories"
                },
                "summary": "It was discovered in recent months that the anti-de Sitter (AdS) backgrounds\ninvolved in all familiar top-down examples of AdS/CFT duality follow from\napplying a transverse nonrelativistic brane limit to string/M theory on\n(asymptotically) flat spacetime. In this note we show that an exactly analogous\nstatement holds for de Sitter (dS) backgrounds relevant to particular instances\nof dS/CFT duality, which are obtained via a longitudinal Carrollian brane\nlimit. This statement makes direct contact with the holographic duality\ninferred by Hull via temporal T-duality.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "It was discovered in recent months that the anti-de Sitter (AdS) backgrounds\ninvolved in all familiar top-down examples of AdS/CFT duality follow from\napplying a transverse nonrelativistic brane limit to string/M theory on\n(asymptotically) flat spacetime. In this note we show that an exactly analogous\nstatement holds for de Sitter (dS) backgrounds relevant to particular instances\nof dS/CFT duality, which are obtained via a longitudinal Carrollian brane\nlimit. This statement makes direct contact with the holographic duality\ninferred by Hull via temporal T-duality."
                },
                "authors": [
                    {
                        "name": "AndrÃ©s ArgandoÃ±a"
                    },
                    {
                        "name": "Alberto Guijosa"
                    },
                    {
                        "name": "Sergio PatiÃ±o-LÃ³pez"
                    }
                ],
                "author_detail": {
                    "name": "Sergio PatiÃ±o-LÃ³pez"
                },
                "author": "Sergio PatiÃ±o-LÃ³pez",
                "arxiv_comment": "19 pages, no figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.06147v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.06147v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "hep-th",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "hep-th",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "gr-qc",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.06141v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.06141v1",
                "updated": "2025-07-08T16:22:52Z",
                "updated_parsed": [
                    2025,
                    7,
                    8,
                    16,
                    22,
                    52,
                    1,
                    189,
                    0
                ],
                "published": "2025-07-08T16:22:52Z",
                "published_parsed": [
                    2025,
                    7,
                    8,
                    16,
                    22,
                    52,
                    1,
                    189,
                    0
                ],
                "title": "Large Language Models Predict Human Well-being -- But Not Equally\n  Everywhere",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models Predict Human Well-being -- But Not Equally\n  Everywhere"
                },
                "summary": "Subjective well-being is a key metric in economic, medical, and policy\ndecision-making. As artificial intelligence provides scalable tools for\nmodelling human outcomes, it is crucial to evaluate whether large language\nmodels (LLMs) can accurately predict well-being across diverse global\npopulations. We evaluate four leading LLMs using data from 64,000 individuals\nin 64 countries. While LLMs capture broad correlates such as income and health,\ntheir predictive accuracy decreases in countries underrepresented in the\ntraining data, highlighting systematic biases rooted in global digital and\neconomic inequality. A pre-registered experiment demonstrates that LLMs rely on\nsurface-level linguistic similarity rather than conceptual understanding,\nleading to systematic misestimations in unfamiliar or resource-limited\nsettings. Injecting findings from underrepresented contexts substantially\nenhances performance, but a significant gap remains. These results highlight\nboth the promise and limitations of LLMs in predicting global well-being,\nunderscoring the importance of robust validation prior to their implementation\nacross these areas.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Subjective well-being is a key metric in economic, medical, and policy\ndecision-making. As artificial intelligence provides scalable tools for\nmodelling human outcomes, it is crucial to evaluate whether large language\nmodels (LLMs) can accurately predict well-being across diverse global\npopulations. We evaluate four leading LLMs using data from 64,000 individuals\nin 64 countries. While LLMs capture broad correlates such as income and health,\ntheir predictive accuracy decreases in countries underrepresented in the\ntraining data, highlighting systematic biases rooted in global digital and\neconomic inequality. A pre-registered experiment demonstrates that LLMs rely on\nsurface-level linguistic similarity rather than conceptual understanding,\nleading to systematic misestimations in unfamiliar or resource-limited\nsettings. Injecting findings from underrepresented contexts substantially\nenhances performance, but a significant gap remains. These results highlight\nboth the promise and limitations of LLMs in predicting global well-being,\nunderscoring the importance of robust validation prior to their implementation\nacross these areas."
                },
                "authors": [
                    {
                        "name": "Pat Pataranutaporn"
                    },
                    {
                        "name": "Nattavudh Powdthavee"
                    },
                    {
                        "name": "Chayapatr Archiwaranguprok"
                    },
                    {
                        "name": "Pattie Maes"
                    }
                ],
                "author_detail": {
                    "name": "Pattie Maes"
                },
                "author": "Pattie Maes",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.06141v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.06141v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.06139v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.06139v1",
                "updated": "2025-07-08T16:20:46Z",
                "updated_parsed": [
                    2025,
                    7,
                    8,
                    16,
                    20,
                    46,
                    1,
                    189,
                    0
                ],
                "published": "2025-07-08T16:20:46Z",
                "published_parsed": [
                    2025,
                    7,
                    8,
                    16,
                    20,
                    46,
                    1,
                    189,
                    0
                ],
                "title": "Topic Modeling and Link-Prediction for Material Property Discovery",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Topic Modeling and Link-Prediction for Material Property Discovery"
                },
                "summary": "Link prediction infers missing or future relations between graph nodes, based\non connection patterns. Scientific literature networks and knowledge graphs are\ntypically large, sparse, and noisy, and often contain missing links between\nentities. We present an AI-driven hierarchical link prediction framework that\nintegrates matrix factorization to infer hidden associations and steer\ndiscovery in complex material domains. Our method combines Hierarchical\nNonnegative Matrix Factorization (HNMFk) and Boolean matrix factorization\n(BNMFk) with automatic model selection, as well as Logistic matrix\nfactorization (LMF), we use to construct a three-level topic tree from a\n46,862-document corpus focused on 73 transition-metal dichalcogenides (TMDs).\nThese materials are studied in a variety of physics fields with many current\nand potential applications.\n  An ensemble BNMFk + LMF approach fuses discrete interpretability with\nprobabilistic scoring. The resulting HNMFk clusters map each material onto\ncoherent topics like superconductivity, energy storage, and tribology. Also,\nmissing or weakly connected links are highlight between topics and materials,\nsuggesting novel hypotheses for cross-disciplinary exploration. We validate our\nmethod by removing publications about superconductivity in well-known\nsuperconductors, and show the model predicts associations with the\nsuperconducting TMD clusters. This shows the method finds hidden connections in\na graph of material to latent topic associations built from scientific\nliterature, especially useful when examining a diverse corpus of scientific\ndocuments covering the same class of phenomena or materials but originating\nfrom distinct communities and perspectives. The inferred links generating new\nhypotheses, produced by our method, are exposed through an interactive\nStreamlit dashboard, designed for human-in-the-loop scientific discovery.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Link prediction infers missing or future relations between graph nodes, based\non connection patterns. Scientific literature networks and knowledge graphs are\ntypically large, sparse, and noisy, and often contain missing links between\nentities. We present an AI-driven hierarchical link prediction framework that\nintegrates matrix factorization to infer hidden associations and steer\ndiscovery in complex material domains. Our method combines Hierarchical\nNonnegative Matrix Factorization (HNMFk) and Boolean matrix factorization\n(BNMFk) with automatic model selection, as well as Logistic matrix\nfactorization (LMF), we use to construct a three-level topic tree from a\n46,862-document corpus focused on 73 transition-metal dichalcogenides (TMDs).\nThese materials are studied in a variety of physics fields with many current\nand potential applications.\n  An ensemble BNMFk + LMF approach fuses discrete interpretability with\nprobabilistic scoring. The resulting HNMFk clusters map each material onto\ncoherent topics like superconductivity, energy storage, and tribology. Also,\nmissing or weakly connected links are highlight between topics and materials,\nsuggesting novel hypotheses for cross-disciplinary exploration. We validate our\nmethod by removing publications about superconductivity in well-known\nsuperconductors, and show the model predicts associations with the\nsuperconducting TMD clusters. This shows the method finds hidden connections in\na graph of material to latent topic associations built from scientific\nliterature, especially useful when examining a diverse corpus of scientific\ndocuments covering the same class of phenomena or materials but originating\nfrom distinct communities and perspectives. The inferred links generating new\nhypotheses, produced by our method, are exposed through an interactive\nStreamlit dashboard, designed for human-in-the-loop scientific discovery."
                },
                "authors": [
                    {
                        "name": "Ryan C. Barron"
                    },
                    {
                        "name": "Maksim E. Eren"
                    },
                    {
                        "name": "Valentin Stanev"
                    },
                    {
                        "name": "Cynthia Matuszek"
                    },
                    {
                        "name": "Boian S. Alexandrov"
                    }
                ],
                "author_detail": {
                    "name": "Boian S. Alexandrov"
                },
                "author": "Boian S. Alexandrov",
                "arxiv_comment": "4 pages, 3 figures, 1 table",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.06139v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.06139v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.06138v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.06138v1",
                "updated": "2025-07-08T16:20:43Z",
                "updated_parsed": [
                    2025,
                    7,
                    8,
                    16,
                    20,
                    43,
                    1,
                    189,
                    0
                ],
                "published": "2025-07-08T16:20:43Z",
                "published_parsed": [
                    2025,
                    7,
                    8,
                    16,
                    20,
                    43,
                    1,
                    189,
                    0
                ],
                "title": "Coding Triangle: How Does Large Language Model Understand Code?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Coding Triangle: How Does Large Language Model Understand Code?"
                },
                "summary": "Large language models (LLMs) have achieved remarkable progress in code\ngeneration, yet their true programming competence remains underexplored. We\nintroduce the Code Triangle framework, which systematically evaluates LLMs\nacross three fundamental dimensions: editorial analysis, code implementation,\nand test case generation. Through extensive experiments on competitive\nprogramming benchmarks, we reveal that while LLMs can form a self-consistent\nsystem across these dimensions, their solutions often lack the diversity and\nrobustness of human programmers. We identify a significant distribution shift\nbetween model cognition and human expertise, with model errors tending to\ncluster due to training data biases and limited reasoning transfer. Our study\ndemonstrates that incorporating human-generated editorials, solutions, and\ndiverse test cases, as well as leveraging model mixtures, can substantially\nenhance both the performance and robustness of LLMs. Furthermore, we reveal\nboth the consistency and inconsistency in the cognition of LLMs that may\nfacilitate self-reflection and self-improvement, providing a potential\ndirection for developing more powerful coding models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have achieved remarkable progress in code\ngeneration, yet their true programming competence remains underexplored. We\nintroduce the Code Triangle framework, which systematically evaluates LLMs\nacross three fundamental dimensions: editorial analysis, code implementation,\nand test case generation. Through extensive experiments on competitive\nprogramming benchmarks, we reveal that while LLMs can form a self-consistent\nsystem across these dimensions, their solutions often lack the diversity and\nrobustness of human programmers. We identify a significant distribution shift\nbetween model cognition and human expertise, with model errors tending to\ncluster due to training data biases and limited reasoning transfer. Our study\ndemonstrates that incorporating human-generated editorials, solutions, and\ndiverse test cases, as well as leveraging model mixtures, can substantially\nenhance both the performance and robustness of LLMs. Furthermore, we reveal\nboth the consistency and inconsistency in the cognition of LLMs that may\nfacilitate self-reflection and self-improvement, providing a potential\ndirection for developing more powerful coding models."
                },
                "authors": [
                    {
                        "name": "Taolin Zhang"
                    },
                    {
                        "name": "Zihan Ma"
                    },
                    {
                        "name": "Maosong Cao"
                    },
                    {
                        "name": "Junnan Liu"
                    },
                    {
                        "name": "Songyang Zhang"
                    },
                    {
                        "name": "Kai Chen"
                    }
                ],
                "author_detail": {
                    "name": "Kai Chen"
                },
                "author": "Kai Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.06138v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.06138v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.06137v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.06137v1",
                "updated": "2025-07-08T16:19:45Z",
                "updated_parsed": [
                    2025,
                    7,
                    8,
                    16,
                    19,
                    45,
                    1,
                    189,
                    0
                ],
                "published": "2025-07-08T16:19:45Z",
                "published_parsed": [
                    2025,
                    7,
                    8,
                    16,
                    19,
                    45,
                    1,
                    189,
                    0
                ],
                "title": "NeoBabel: A Multilingual Open Tower for Visual Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "NeoBabel: A Multilingual Open Tower for Visual Generation"
                },
                "summary": "Text-to-image generation advancements have been predominantly\nEnglish-centric, creating barriers for non-English speakers and perpetuating\ndigital inequities. While existing systems rely on translation pipelines, these\nintroduce semantic drift, computational overhead, and cultural misalignment. We\nintroduce NeoBabel, a novel multilingual image generation framework that sets a\nnew Pareto frontier in performance, efficiency and inclusivity, supporting six\nlanguages: English, Chinese, Dutch, French, Hindi, and Persian. The model is\ntrained using a combination of large-scale multilingual pretraining and\nhigh-resolution instruction tuning. To evaluate its capabilities, we expand two\nEnglish-only benchmarks to multilingual equivalents: m-GenEval and m-DPG.\nNeoBabel achieves state-of-the-art multilingual performance while retaining\nstrong English capability, scoring 0.75 on m-GenEval and 0.68 on m-DPG.\nNotably, it performs on par with leading models on English tasks while\noutperforming them by +0.11 and +0.09 on multilingual benchmarks, even though\nthese models are built on multilingual base LLMs. This demonstrates the\neffectiveness of our targeted alignment training for preserving and extending\ncrosslingual generalization. We further introduce two new metrics to rigorously\nassess multilingual alignment and robustness to code-mixed prompts. Notably,\nNeoBabel matches or exceeds English-only models while being 2-4x smaller. We\nrelease an open toolkit, including all code, model checkpoints, a curated\ndataset of 124M multilingual text-image pairs, and standardized multilingual\nevaluation protocols, to advance inclusive AI research. Our work demonstrates\nthat multilingual capability is not a trade-off but a catalyst for improved\nrobustness, efficiency, and cultural fidelity in generative AI.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Text-to-image generation advancements have been predominantly\nEnglish-centric, creating barriers for non-English speakers and perpetuating\ndigital inequities. While existing systems rely on translation pipelines, these\nintroduce semantic drift, computational overhead, and cultural misalignment. We\nintroduce NeoBabel, a novel multilingual image generation framework that sets a\nnew Pareto frontier in performance, efficiency and inclusivity, supporting six\nlanguages: English, Chinese, Dutch, French, Hindi, and Persian. The model is\ntrained using a combination of large-scale multilingual pretraining and\nhigh-resolution instruction tuning. To evaluate its capabilities, we expand two\nEnglish-only benchmarks to multilingual equivalents: m-GenEval and m-DPG.\nNeoBabel achieves state-of-the-art multilingual performance while retaining\nstrong English capability, scoring 0.75 on m-GenEval and 0.68 on m-DPG.\nNotably, it performs on par with leading models on English tasks while\noutperforming them by +0.11 and +0.09 on multilingual benchmarks, even though\nthese models are built on multilingual base LLMs. This demonstrates the\neffectiveness of our targeted alignment training for preserving and extending\ncrosslingual generalization. We further introduce two new metrics to rigorously\nassess multilingual alignment and robustness to code-mixed prompts. Notably,\nNeoBabel matches or exceeds English-only models while being 2-4x smaller. We\nrelease an open toolkit, including all code, model checkpoints, a curated\ndataset of 124M multilingual text-image pairs, and standardized multilingual\nevaluation protocols, to advance inclusive AI research. Our work demonstrates\nthat multilingual capability is not a trade-off but a catalyst for improved\nrobustness, efficiency, and cultural fidelity in generative AI."
                },
                "authors": [
                    {
                        "name": "Mohammad Mahdi Derakhshani"
                    },
                    {
                        "name": "Dheeraj Varghese"
                    },
                    {
                        "name": "Marzieh Fadaee"
                    },
                    {
                        "name": "Cees G. M. Snoek"
                    }
                ],
                "author_detail": {
                    "name": "Cees G. M. Snoek"
                },
                "author": "Cees G. M. Snoek",
                "arxiv_comment": "34 pages, 12 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.06137v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.06137v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.06134v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.06134v1",
                "updated": "2025-07-08T16:18:54Z",
                "updated_parsed": [
                    2025,
                    7,
                    8,
                    16,
                    18,
                    54,
                    1,
                    189,
                    0
                ],
                "published": "2025-07-08T16:18:54Z",
                "published_parsed": [
                    2025,
                    7,
                    8,
                    16,
                    18,
                    54,
                    1,
                    189,
                    0
                ],
                "title": "OpenAgentSafety: A Comprehensive Framework for Evaluating Real-World AI\n  Agent Safety",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "OpenAgentSafety: A Comprehensive Framework for Evaluating Real-World AI\n  Agent Safety"
                },
                "summary": "Recent advances in AI agents capable of solving complex, everyday tasks, from\nscheduling to customer service, have enabled deployment in real-world settings,\nbut their possibilities for unsafe behavior demands rigorous evaluation. While\nprior benchmarks have attempted to assess agent safety, most fall short by\nrelying on simulated environments, narrow task domains, or unrealistic tool\nabstractions. We introduce OpenAgentSafety, a comprehensive and modular\nframework for evaluating agent behavior across eight critical risk categories.\nUnlike prior work, our framework evaluates agents that interact with real\ntools, including web browsers, code execution environments, file systems, bash\nshells, and messaging platforms; and supports over 350 multi-turn, multi-user\ntasks spanning both benign and adversarial user intents. OpenAgentSafety is\ndesigned for extensibility, allowing researchers to add tools, tasks, websites,\nand adversarial strategies with minimal effort. It combines rule-based analysis\nwith LLM-as-judge assessments to detect both overt and subtle unsafe behaviors.\nEmpirical analysis of five prominent LLMs in agentic scenarios reveals unsafe\nbehavior in 51.2% of safety-vulnerable tasks with Claude-Sonnet-3.7, to 72.7%\nwith o3-mini, highlighting critical safety vulnerabilities and the need for\nstronger safeguards before real-world deployment.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advances in AI agents capable of solving complex, everyday tasks, from\nscheduling to customer service, have enabled deployment in real-world settings,\nbut their possibilities for unsafe behavior demands rigorous evaluation. While\nprior benchmarks have attempted to assess agent safety, most fall short by\nrelying on simulated environments, narrow task domains, or unrealistic tool\nabstractions. We introduce OpenAgentSafety, a comprehensive and modular\nframework for evaluating agent behavior across eight critical risk categories.\nUnlike prior work, our framework evaluates agents that interact with real\ntools, including web browsers, code execution environments, file systems, bash\nshells, and messaging platforms; and supports over 350 multi-turn, multi-user\ntasks spanning both benign and adversarial user intents. OpenAgentSafety is\ndesigned for extensibility, allowing researchers to add tools, tasks, websites,\nand adversarial strategies with minimal effort. It combines rule-based analysis\nwith LLM-as-judge assessments to detect both overt and subtle unsafe behaviors.\nEmpirical analysis of five prominent LLMs in agentic scenarios reveals unsafe\nbehavior in 51.2% of safety-vulnerable tasks with Claude-Sonnet-3.7, to 72.7%\nwith o3-mini, highlighting critical safety vulnerabilities and the need for\nstronger safeguards before real-world deployment."
                },
                "authors": [
                    {
                        "name": "Sanidhya Vijayvargiya"
                    },
                    {
                        "name": "Aditya Bharat Soni"
                    },
                    {
                        "name": "Xuhui Zhou"
                    },
                    {
                        "name": "Zora Zhiruo Wang"
                    },
                    {
                        "name": "Nouha Dziri"
                    },
                    {
                        "name": "Graham Neubig"
                    },
                    {
                        "name": "Maarten Sap"
                    }
                ],
                "author_detail": {
                    "name": "Maarten Sap"
                },
                "author": "Maarten Sap",
                "arxiv_comment": "19 pages, 10 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.06134v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.06134v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.06127v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.06127v1",
                "updated": "2025-07-08T16:14:17Z",
                "updated_parsed": [
                    2025,
                    7,
                    8,
                    16,
                    14,
                    17,
                    1,
                    189,
                    0
                ],
                "published": "2025-07-08T16:14:17Z",
                "published_parsed": [
                    2025,
                    7,
                    8,
                    16,
                    14,
                    17,
                    1,
                    189,
                    0
                ],
                "title": "PrefixAgent: An LLM-Powered Design Framework for Efficient Prefix Adder\n  Optimization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PrefixAgent: An LLM-Powered Design Framework for Efficient Prefix Adder\n  Optimization"
                },
                "summary": "Prefix adders are fundamental arithmetic circuits, but their design space\ngrows exponentially with bit-width, posing significant optimization challenges.\nPrevious works face limitations in performance, generalization, and\nscalability. To address these challenges, we propose PrefixAgent, a large\nlanguage model (LLM)-powered framework that enables efficient prefix adder\noptimization. Specifically, PrefixAgent reformulates the problem into subtasks\nincluding backbone synthesis and structure refinement, which effectively\nreduces the search space. More importantly, this new design perspective enables\nus to efficiently collect enormous high-quality data and reasoning traces with\nE-graph, which further results in an effective fine-tuning of LLM. Experimental\nresults show that PrefixAgent synthesizes prefix adders with consistently\nsmaller areas compared to baseline methods, while maintaining scalability and\ngeneralization in commercial EDA flows.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Prefix adders are fundamental arithmetic circuits, but their design space\ngrows exponentially with bit-width, posing significant optimization challenges.\nPrevious works face limitations in performance, generalization, and\nscalability. To address these challenges, we propose PrefixAgent, a large\nlanguage model (LLM)-powered framework that enables efficient prefix adder\noptimization. Specifically, PrefixAgent reformulates the problem into subtasks\nincluding backbone synthesis and structure refinement, which effectively\nreduces the search space. More importantly, this new design perspective enables\nus to efficiently collect enormous high-quality data and reasoning traces with\nE-graph, which further results in an effective fine-tuning of LLM. Experimental\nresults show that PrefixAgent synthesizes prefix adders with consistently\nsmaller areas compared to baseline methods, while maintaining scalability and\ngeneralization in commercial EDA flows."
                },
                "authors": [
                    {
                        "name": "Dongsheng Zuo"
                    },
                    {
                        "name": "Jiadong Zhu"
                    },
                    {
                        "name": "Yang Luo"
                    },
                    {
                        "name": "Yuzhe Ma"
                    }
                ],
                "author_detail": {
                    "name": "Yuzhe Ma"
                },
                "author": "Yuzhe Ma",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.06127v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.06127v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.12665v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.12665v2",
                "updated": "2025-07-08T16:05:09Z",
                "updated_parsed": [
                    2025,
                    7,
                    8,
                    16,
                    5,
                    9,
                    1,
                    189,
                    0
                ],
                "published": "2024-11-19T17:17:46Z",
                "published_parsed": [
                    2024,
                    11,
                    19,
                    17,
                    17,
                    46,
                    1,
                    324,
                    0
                ],
                "title": "Regression for the Mean: Auto-Evaluation and Inference with Few Labels\n  through Post-hoc Regression",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Regression for the Mean: Auto-Evaluation and Inference with Few Labels\n  through Post-hoc Regression"
                },
                "summary": "The availability of machine learning systems that can effectively perform\narbitrary tasks has led to synthetic labels from these systems being used in\napplications of statistical inference, such as data analysis or model\nevaluation. The Prediction Powered Inference (PPI) framework provides a way of\nleveraging both a large pool of pseudo-labelled data and a small sample with\nreal, high-quality labels to produce a low-variance, unbiased estimate of the\nquantity being evaluated for. Most work on PPI considers a relatively sizable\nset of labelled samples, which can be resource intensive to obtain. However, we\nfind that when labelled data is scarce, the PPI++ method can perform even worse\nthan classical inference. We analyze this phenomenon by relating PPI++ to\nordinary least squares regression, which also experiences high variance with\nsmall sample sizes, and use this regression framework to better understand the\nefficacy of PPI. Motivated by this, we present two new PPI-based techniques\nthat leverage robust regressors to produce even lower variance estimators in\nthe few-label regime.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The availability of machine learning systems that can effectively perform\narbitrary tasks has led to synthetic labels from these systems being used in\napplications of statistical inference, such as data analysis or model\nevaluation. The Prediction Powered Inference (PPI) framework provides a way of\nleveraging both a large pool of pseudo-labelled data and a small sample with\nreal, high-quality labels to produce a low-variance, unbiased estimate of the\nquantity being evaluated for. Most work on PPI considers a relatively sizable\nset of labelled samples, which can be resource intensive to obtain. However, we\nfind that when labelled data is scarce, the PPI++ method can perform even worse\nthan classical inference. We analyze this phenomenon by relating PPI++ to\nordinary least squares regression, which also experiences high variance with\nsmall sample sizes, and use this regression framework to better understand the\nefficacy of PPI. Motivated by this, we present two new PPI-based techniques\nthat leverage robust regressors to produce even lower variance estimators in\nthe few-label regime."
                },
                "authors": [
                    {
                        "name": "Benjamin Eyre"
                    },
                    {
                        "name": "David Madras"
                    }
                ],
                "author_detail": {
                    "name": "David Madras"
                },
                "author": "David Madras",
                "arxiv_comment": "Presented as a conference paper at ICML 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.12665v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.12665v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.15006v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.15006v2",
                "updated": "2025-07-08T15:59:29Z",
                "updated_parsed": [
                    2025,
                    7,
                    8,
                    15,
                    59,
                    29,
                    1,
                    189,
                    0
                ],
                "published": "2025-02-20T19:59:11Z",
                "published_parsed": [
                    2025,
                    2,
                    20,
                    19,
                    59,
                    11,
                    3,
                    51,
                    0
                ],
                "title": "Safe Beyond the Horizon: Efficient Sampling-based MPC with Neural\n  Control Barrier Functions",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Safe Beyond the Horizon: Efficient Sampling-based MPC with Neural\n  Control Barrier Functions"
                },
                "summary": "A common problem when using model predictive control (MPC) in practice is the\nsatisfaction of safety specifications beyond the prediction horizon. While\ntheoretical works have shown that safety can be guaranteed by enforcing a\nsuitable terminal set constraint or a sufficiently long prediction horizon,\nthese techniques are difficult to apply and thus are rarely used by\npractitioners, especially in the case of general nonlinear dynamics. To solve\nthis problem, we impose a tradeoff between exact recursive feasibility,\ncomputational tractability, and applicability to ``black-box'' dynamics by\nlearning an approximate discrete-time control barrier function and\nincorporating it into a variational inference MPC (VIMPC), a sampling-based MPC\nparadigm. To handle the resulting state constraints, we further propose a new\nsampling strategy that greatly reduces the variance of the estimated optimal\ncontrol, improving the sample efficiency, and enabling real-time planning on a\nCPU. The resulting Neural Shield-VIMPC (NS-VIMPC) controller yields substantial\nsafety improvements compared to existing sampling-based MPC controllers, even\nunder badly designed cost functions. We validate our approach in both\nsimulation and real-world hardware experiments. Project website:\nhttps://mit-realm.github.io/ns-vimpc/.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A common problem when using model predictive control (MPC) in practice is the\nsatisfaction of safety specifications beyond the prediction horizon. While\ntheoretical works have shown that safety can be guaranteed by enforcing a\nsuitable terminal set constraint or a sufficiently long prediction horizon,\nthese techniques are difficult to apply and thus are rarely used by\npractitioners, especially in the case of general nonlinear dynamics. To solve\nthis problem, we impose a tradeoff between exact recursive feasibility,\ncomputational tractability, and applicability to ``black-box'' dynamics by\nlearning an approximate discrete-time control barrier function and\nincorporating it into a variational inference MPC (VIMPC), a sampling-based MPC\nparadigm. To handle the resulting state constraints, we further propose a new\nsampling strategy that greatly reduces the variance of the estimated optimal\ncontrol, improving the sample efficiency, and enabling real-time planning on a\nCPU. The resulting Neural Shield-VIMPC (NS-VIMPC) controller yields substantial\nsafety improvements compared to existing sampling-based MPC controllers, even\nunder badly designed cost functions. We validate our approach in both\nsimulation and real-world hardware experiments. Project website:\nhttps://mit-realm.github.io/ns-vimpc/."
                },
                "authors": [
                    {
                        "name": "Ji Yin"
                    },
                    {
                        "name": "Oswin So"
                    },
                    {
                        "name": "Eric Yang Yu"
                    },
                    {
                        "name": "Chuchu Fan"
                    },
                    {
                        "name": "Panagiotis Tsiotras"
                    }
                ],
                "author_detail": {
                    "name": "Panagiotis Tsiotras"
                },
                "author": "Panagiotis Tsiotras",
                "arxiv_comment": "Accepted by RSS 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.15006v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.15006v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.00406v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.00406v2",
                "updated": "2025-07-08T15:49:01Z",
                "updated_parsed": [
                    2025,
                    7,
                    8,
                    15,
                    49,
                    1,
                    1,
                    189,
                    0
                ],
                "published": "2025-02-01T11:45:44Z",
                "published_parsed": [
                    2025,
                    2,
                    1,
                    11,
                    45,
                    44,
                    5,
                    32,
                    0
                ],
                "title": "Agents Are All You Need for LLM Unlearning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Agents Are All You Need for LLM Unlearning"
                },
                "summary": "Information removal or suppression in large language models (LLMs) is a\ndesired functionality, useful in AI regulation, legal compliance, safety, and\nprivacy. LLM unlearning methods aim to remove information on demand from LLMs.\nCurrent LLM unlearning methods struggle to balance the unlearning efficacy and\nutility due to the competing nature of these objectives. Keeping the unlearning\nprocess computationally feasible without assuming access to the model weights\nis an overlooked area. In this work we show that \\textit{agents might be all we\nneed for effective and practical inference-time LLM unlearning}. We present the\nfirst agentic LLM unlearning (\\texttt{ALU}) method, a multi-agent,\nretrain-free, model-agnostic approach to LLM unlearning that achieves effective\nunlearning while preserving the utility. Our \\texttt{ALU} framework unlearns by\ninvolving multiple LLM agents, each designed for a specific step in the\nunlearning process, without the need to update model weights for any of the\nagents in the framework. Users can easily request any set of unlearning\ninstances in any sequence, and \\texttt{ALU} seamlessly adapts in real time.\nThis is facilitated without requiring any changes in the underlying LLM model.\nThrough extensive experiments on established benchmarks (TOFU, WMDP, WPU) and\njailbreaking techniques (many shot, target masking, other languages), we\ndemonstrate that \\texttt{ALU} consistently stands out as the most robust\ninference-time LLM unlearning framework among current state-of-the-art methods\nwhile incurring time cost that remains effectively constant regardless of the\nnumber of unlearning targets. We further highlight \\texttt{ALU}'s superior\nperformance compared to existing methods when evaluated at scale. Specifically,\n\\texttt{ALU} is assessed on up to 1000 unlearning targets, exceeding the\nevaluation scope of all previously proposed LLM unlearning methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Information removal or suppression in large language models (LLMs) is a\ndesired functionality, useful in AI regulation, legal compliance, safety, and\nprivacy. LLM unlearning methods aim to remove information on demand from LLMs.\nCurrent LLM unlearning methods struggle to balance the unlearning efficacy and\nutility due to the competing nature of these objectives. Keeping the unlearning\nprocess computationally feasible without assuming access to the model weights\nis an overlooked area. In this work we show that \\textit{agents might be all we\nneed for effective and practical inference-time LLM unlearning}. We present the\nfirst agentic LLM unlearning (\\texttt{ALU}) method, a multi-agent,\nretrain-free, model-agnostic approach to LLM unlearning that achieves effective\nunlearning while preserving the utility. Our \\texttt{ALU} framework unlearns by\ninvolving multiple LLM agents, each designed for a specific step in the\nunlearning process, without the need to update model weights for any of the\nagents in the framework. Users can easily request any set of unlearning\ninstances in any sequence, and \\texttt{ALU} seamlessly adapts in real time.\nThis is facilitated without requiring any changes in the underlying LLM model.\nThrough extensive experiments on established benchmarks (TOFU, WMDP, WPU) and\njailbreaking techniques (many shot, target masking, other languages), we\ndemonstrate that \\texttt{ALU} consistently stands out as the most robust\ninference-time LLM unlearning framework among current state-of-the-art methods\nwhile incurring time cost that remains effectively constant regardless of the\nnumber of unlearning targets. We further highlight \\texttt{ALU}'s superior\nperformance compared to existing methods when evaluated at scale. Specifically,\n\\texttt{ALU} is assessed on up to 1000 unlearning targets, exceeding the\nevaluation scope of all previously proposed LLM unlearning methods."
                },
                "authors": [
                    {
                        "name": "Debdeep Sanyal"
                    },
                    {
                        "name": "Murari Mandal"
                    }
                ],
                "author_detail": {
                    "name": "Murari Mandal"
                },
                "author": "Murari Mandal",
                "arxiv_comment": "Accepted to COLM 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.00406v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.00406v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.02986v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.02986v2",
                "updated": "2025-07-08T15:44:49Z",
                "updated_parsed": [
                    2025,
                    7,
                    8,
                    15,
                    44,
                    49,
                    1,
                    189,
                    0
                ],
                "published": "2025-07-01T10:01:21Z",
                "published_parsed": [
                    2025,
                    7,
                    1,
                    10,
                    1,
                    21,
                    1,
                    182,
                    0
                ],
                "title": "GAF-Guard: An Agentic Framework for Risk Management and Governance in\n  Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GAF-Guard: An Agentic Framework for Risk Management and Governance in\n  Large Language Models"
                },
                "summary": "As Large Language Models (LLMs) continue to be increasingly applied across\nvarious domains, their widespread adoption necessitates rigorous monitoring to\nprevent unintended negative consequences and ensure robustness. Furthermore,\nLLMs must be designed to align with human values, like preventing harmful\ncontent and ensuring responsible usage. The current automated systems and\nsolutions for monitoring LLMs in production are primarily centered on\nLLM-specific concerns like hallucination etc, with little consideration given\nto the requirements of specific use-cases and user preferences. This paper\nintroduces GAF-Guard, a novel agentic framework for LLM governance that places\nthe user, the use-case, and the model itself at the center. The framework is\ndesigned to detect and monitor risks associated with the deployment of LLM\nbased applications. The approach models autonomous agents that identify risks,\nactivate risk detection tools, within specific use-cases and facilitate\ncontinuous monitoring and reporting to enhance AI safety, and user\nexpectations. The code is available at\nhttps://github.com/IBM/risk-atlas-nexus-demos/tree/main/gaf-guard.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As Large Language Models (LLMs) continue to be increasingly applied across\nvarious domains, their widespread adoption necessitates rigorous monitoring to\nprevent unintended negative consequences and ensure robustness. Furthermore,\nLLMs must be designed to align with human values, like preventing harmful\ncontent and ensuring responsible usage. The current automated systems and\nsolutions for monitoring LLMs in production are primarily centered on\nLLM-specific concerns like hallucination etc, with little consideration given\nto the requirements of specific use-cases and user preferences. This paper\nintroduces GAF-Guard, a novel agentic framework for LLM governance that places\nthe user, the use-case, and the model itself at the center. The framework is\ndesigned to detect and monitor risks associated with the deployment of LLM\nbased applications. The approach models autonomous agents that identify risks,\nactivate risk detection tools, within specific use-cases and facilitate\ncontinuous monitoring and reporting to enhance AI safety, and user\nexpectations. The code is available at\nhttps://github.com/IBM/risk-atlas-nexus-demos/tree/main/gaf-guard."
                },
                "authors": [
                    {
                        "name": "Seshu Tirupathi"
                    },
                    {
                        "name": "Dhaval Salwala"
                    },
                    {
                        "name": "Elizabeth Daly"
                    },
                    {
                        "name": "Inge Vejsbjerg"
                    }
                ],
                "author_detail": {
                    "name": "Inge Vejsbjerg"
                },
                "author": "Inge Vejsbjerg",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.02986v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.02986v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.17032v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.17032v3",
                "updated": "2025-07-08T15:40:17Z",
                "updated_parsed": [
                    2025,
                    7,
                    8,
                    15,
                    40,
                    17,
                    1,
                    189,
                    0
                ],
                "published": "2024-05-27T10:39:18Z",
                "published_parsed": [
                    2024,
                    5,
                    27,
                    10,
                    39,
                    18,
                    0,
                    148,
                    0
                ],
                "title": "Exact phylodynamic likelihood via structured Markov genealogy processes",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Exact phylodynamic likelihood via structured Markov genealogy processes"
                },
                "summary": "We show that each member of a broad class of Markovian population models\ninduces a unique stochastic process on the space of genealogies. We construct\nthis genealogy process and derive exact expressions for the likelihood of an\nobserved genealogy in terms of a filter equation, the structure of which is\ncompletely determined by the population model. We show that existing\nphylodynamic methods based on either the coalescent or the linear birth-death\nprocesses are special cases. We derive some properties of filter equations and\ndescribe a class of algorithms that can be used to numerically solve them. Our\nresults open the door to statistically efficient likelihood-based phylodynamic\ninference for a much wider class of models than is currently possible.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We show that each member of a broad class of Markovian population models\ninduces a unique stochastic process on the space of genealogies. We construct\nthis genealogy process and derive exact expressions for the likelihood of an\nobserved genealogy in terms of a filter equation, the structure of which is\ncompletely determined by the population model. We show that existing\nphylodynamic methods based on either the coalescent or the linear birth-death\nprocesses are special cases. We derive some properties of filter equations and\ndescribe a class of algorithms that can be used to numerically solve them. Our\nresults open the door to statistically efficient likelihood-based phylodynamic\ninference for a much wider class of models than is currently possible."
                },
                "authors": [
                    {
                        "name": "Aaron A. King"
                    },
                    {
                        "name": "Qianying Lin"
                    },
                    {
                        "name": "Edward L. Ionides"
                    }
                ],
                "author_detail": {
                    "name": "Edward L. Ionides"
                },
                "author": "Edward L. Ionides",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.17032v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.17032v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "q-bio.QM",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "q-bio.QM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.PR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "q-bio.PE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.AP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.06093v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.06093v1",
                "updated": "2025-07-08T15:35:19Z",
                "updated_parsed": [
                    2025,
                    7,
                    8,
                    15,
                    35,
                    19,
                    1,
                    189,
                    0
                ],
                "published": "2025-07-08T15:35:19Z",
                "published_parsed": [
                    2025,
                    7,
                    8,
                    15,
                    35,
                    19,
                    1,
                    189,
                    0
                ],
                "title": "Tile-Based ViT Inference with Visual-Cluster Priors for Zero-Shot\n  Multi-Species Plant Identification",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Tile-Based ViT Inference with Visual-Cluster Priors for Zero-Shot\n  Multi-Species Plant Identification"
                },
                "summary": "We describe DS@GT's second-place solution to the PlantCLEF 2025 challenge on\nmulti-species plant identification in vegetation quadrat images. Our pipeline\ncombines (i) a fine-tuned Vision Transformer ViTD2PC24All for patch-level\ninference, (ii) a 4x4 tiling strategy that aligns patch size with the network's\n518x518 receptive field, and (iii) domain-prior adaptation through PaCMAP +\nK-Means visual clustering and geolocation filtering. Tile predictions are\naggregated by majority vote and re-weighted with cluster-specific Bayesian\npriors, yielding a macro-averaged F1 of 0.348 (private leaderboard) while\nrequiring no additional training. All code, configuration files, and\nreproducibility scripts are publicly available at\nhttps://github.com/dsgt-arc/plantclef-2025.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We describe DS@GT's second-place solution to the PlantCLEF 2025 challenge on\nmulti-species plant identification in vegetation quadrat images. Our pipeline\ncombines (i) a fine-tuned Vision Transformer ViTD2PC24All for patch-level\ninference, (ii) a 4x4 tiling strategy that aligns patch size with the network's\n518x518 receptive field, and (iii) domain-prior adaptation through PaCMAP +\nK-Means visual clustering and geolocation filtering. Tile predictions are\naggregated by majority vote and re-weighted with cluster-specific Bayesian\npriors, yielding a macro-averaged F1 of 0.348 (private leaderboard) while\nrequiring no additional training. All code, configuration files, and\nreproducibility scripts are publicly available at\nhttps://github.com/dsgt-arc/plantclef-2025."
                },
                "authors": [
                    {
                        "name": "Murilo Gustineli"
                    },
                    {
                        "name": "Anthony Miyaguchi"
                    },
                    {
                        "name": "Adrian Cheung"
                    },
                    {
                        "name": "Divyansh Khattak"
                    }
                ],
                "author_detail": {
                    "name": "Divyansh Khattak"
                },
                "author": "Divyansh Khattak",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.06093v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.06093v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.06087v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.06087v1",
                "updated": "2025-07-08T15:28:48Z",
                "updated_parsed": [
                    2025,
                    7,
                    8,
                    15,
                    28,
                    48,
                    1,
                    189,
                    0
                ],
                "published": "2025-07-08T15:28:48Z",
                "published_parsed": [
                    2025,
                    7,
                    8,
                    15,
                    28,
                    48,
                    1,
                    189,
                    0
                ],
                "title": "CoRE: Enhancing Metacognition with Label-free Self-evaluation in LRMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CoRE: Enhancing Metacognition with Label-free Self-evaluation in LRMs"
                },
                "summary": "Large reasoning models (LRMs) have demonstrated impressive capabilities in\ndomains like mathematics and program synthesis. Despite their strong\nperformance, LRMs often exhibit overthinking -- excessive and redundant\nreasoning steps that introduce inefficiencies during inference. This phenomenon\nraises an important question for LRM self-evaluation: How can a model\nautonomously assess the correctness of its own reasoning trajectory without\nexternal labels? To address this, we propose Chain-of-Reasoning Embedding\n(CoRE), a series of hidden states in latent space to enable label-free\nself-evaluation on intermediate reasoning steps of LRMs, so as to enhance\nmetacognition abilities for improved reasoning efficiency. By analyzing the\ngeometric properties of the CoRE trajectories, we reveal that redundant\nreasoning usually presents cyclical fluctuations, which correspond to\nrepetitive and unconscious reflection/exploration. Leveraging this insight, we\nfurther introduce a training-free, label-free self-evaluation framework,\nCoRE-Eval, to detect such patterns and dynamically determine whether to\nterminate reasoning early. Extensive experiments on mathematical reasoning\nbenchmarks (GSM8K, MATH-500, and AIME) and across model sizes from 7B to 32B\ndemonstrate that CoRE-Eval reduces chain-of-thought length by 13.7% to 33.2%\nwhile improving answer accuracy by around 10%, achieving 70.0% accuracy on the\nchallenging AIME benchmark with the 32B model.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large reasoning models (LRMs) have demonstrated impressive capabilities in\ndomains like mathematics and program synthesis. Despite their strong\nperformance, LRMs often exhibit overthinking -- excessive and redundant\nreasoning steps that introduce inefficiencies during inference. This phenomenon\nraises an important question for LRM self-evaluation: How can a model\nautonomously assess the correctness of its own reasoning trajectory without\nexternal labels? To address this, we propose Chain-of-Reasoning Embedding\n(CoRE), a series of hidden states in latent space to enable label-free\nself-evaluation on intermediate reasoning steps of LRMs, so as to enhance\nmetacognition abilities for improved reasoning efficiency. By analyzing the\ngeometric properties of the CoRE trajectories, we reveal that redundant\nreasoning usually presents cyclical fluctuations, which correspond to\nrepetitive and unconscious reflection/exploration. Leveraging this insight, we\nfurther introduce a training-free, label-free self-evaluation framework,\nCoRE-Eval, to detect such patterns and dynamically determine whether to\nterminate reasoning early. Extensive experiments on mathematical reasoning\nbenchmarks (GSM8K, MATH-500, and AIME) and across model sizes from 7B to 32B\ndemonstrate that CoRE-Eval reduces chain-of-thought length by 13.7% to 33.2%\nwhile improving answer accuracy by around 10%, achieving 70.0% accuracy on the\nchallenging AIME benchmark with the 32B model."
                },
                "authors": [
                    {
                        "name": "Haoxi Li"
                    },
                    {
                        "name": "Sikai Bai"
                    },
                    {
                        "name": "Jie Zhang"
                    },
                    {
                        "name": "Song Guo"
                    }
                ],
                "author_detail": {
                    "name": "Song Guo"
                },
                "author": "Song Guo",
                "arxiv_comment": "9 pages, 6 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.06087v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.06087v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.08199v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.08199v2",
                "updated": "2025-07-08T15:19:50Z",
                "updated_parsed": [
                    2025,
                    7,
                    8,
                    15,
                    19,
                    50,
                    1,
                    189,
                    0
                ],
                "published": "2025-03-11T09:08:04Z",
                "published_parsed": [
                    2025,
                    3,
                    11,
                    9,
                    8,
                    4,
                    1,
                    70,
                    0
                ],
                "title": "A Cascading Cooperative Multi-agent Framework for On-ramp Merging\n  Control Integrating Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Cascading Cooperative Multi-agent Framework for On-ramp Merging\n  Control Integrating Large Language Models"
                },
                "summary": "Traditional Reinforcement Learning (RL) suffers from replicating human-like\nbehaviors, generalizing effectively in multi-agent scenarios, and overcoming\ninherent interpretability issues.These tasks are compounded when deep\nenvironment understanding, agent coordination and dynamic optimization are\nrequired. While Large Language Model (LLM) enhanced methods have shown promise\nin generalization and interoperability, they often neglect necessary\nmulti-agent coordination. Therefore, we introduce the Cascading Cooperative\nMulti-agent (CCMA) framework, integrating RL for individual interactions, a\nfine-tuned LLM for regional cooperation, a reward function for global\noptimization, and the Retrieval-augmented Generation mechanism to dynamically\noptimize decision-making across complex driving scenarios. Our experiments\ndemonstrate that the CCMA outperforms existing RL methods, demonstrating\nsignificant improvements in both micro and macro-level performance in complex\ndriving environments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Traditional Reinforcement Learning (RL) suffers from replicating human-like\nbehaviors, generalizing effectively in multi-agent scenarios, and overcoming\ninherent interpretability issues.These tasks are compounded when deep\nenvironment understanding, agent coordination and dynamic optimization are\nrequired. While Large Language Model (LLM) enhanced methods have shown promise\nin generalization and interoperability, they often neglect necessary\nmulti-agent coordination. Therefore, we introduce the Cascading Cooperative\nMulti-agent (CCMA) framework, integrating RL for individual interactions, a\nfine-tuned LLM for regional cooperation, a reward function for global\noptimization, and the Retrieval-augmented Generation mechanism to dynamically\noptimize decision-making across complex driving scenarios. Our experiments\ndemonstrate that the CCMA outperforms existing RL methods, demonstrating\nsignificant improvements in both micro and macro-level performance in complex\ndriving environments."
                },
                "authors": [
                    {
                        "name": "Miao Zhang"
                    },
                    {
                        "name": "Zhenlong Fang"
                    },
                    {
                        "name": "Tianyi Wang"
                    },
                    {
                        "name": "Qian Zhang"
                    },
                    {
                        "name": "Shuai Lu"
                    },
                    {
                        "name": "Junfeng Jiao"
                    },
                    {
                        "name": "Tianyu Shi"
                    }
                ],
                "author_detail": {
                    "name": "Tianyu Shi"
                },
                "author": "Tianyu Shi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.08199v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.08199v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.18162v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.18162v3",
                "updated": "2025-07-08T15:16:05Z",
                "updated_parsed": [
                    2025,
                    7,
                    8,
                    15,
                    16,
                    5,
                    1,
                    189,
                    0
                ],
                "published": "2024-09-26T17:19:25Z",
                "published_parsed": [
                    2024,
                    9,
                    26,
                    17,
                    19,
                    25,
                    3,
                    270,
                    0
                ],
                "title": "The Nexus of AR/VR, AI, UI/UX, and Robotics Technologies in Enhancing\n  Learning and Social Interaction for Children with Autism Spectrum Disorders:\n  A Systematic Review",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Nexus of AR/VR, AI, UI/UX, and Robotics Technologies in Enhancing\n  Learning and Social Interaction for Children with Autism Spectrum Disorders:\n  A Systematic Review"
                },
                "summary": "The emergence of large language models (LLMs), augmented reality (AR), and\nuser interface/user experience (UI/UX) design in therapies for children,\nespecially with disorders like autism spectrum disorder (ASD), is studied in\ndetail in this review study. 150 publications were collected by a thorough\nliterature search throughout PubMed, ACM, IEEE Xplore, Elsevier, and Google\nScholar; 60 of them were chosen based on their methodological rigor and\nrelevance to the focus area. Three of the primary areas are studied and covered\nin this review: how AR can improve social and learning results, how LLMs can\nsupport communication, and how UI/UX design affects how effective these\ntechnologies can be. Results show that while LLMs can provide individualized\nlearning and communication support, AR has shown promise in enhancing social\nskills, motivation, and attention. For children with ASD, accessible and\nengaging interventions rely heavily on effective UI/UX design, but there is\nstill a significant lack of robotics-based education and therapeutic programs\nspecifically tailored for autistic children. To optimize the benefits of these\ntechnologies in ASD therapies and immersive education, the study emphasizes the\nneed for additional research to address difficulties related to customization,\naccessibility, and integration.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The emergence of large language models (LLMs), augmented reality (AR), and\nuser interface/user experience (UI/UX) design in therapies for children,\nespecially with disorders like autism spectrum disorder (ASD), is studied in\ndetail in this review study. 150 publications were collected by a thorough\nliterature search throughout PubMed, ACM, IEEE Xplore, Elsevier, and Google\nScholar; 60 of them were chosen based on their methodological rigor and\nrelevance to the focus area. Three of the primary areas are studied and covered\nin this review: how AR can improve social and learning results, how LLMs can\nsupport communication, and how UI/UX design affects how effective these\ntechnologies can be. Results show that while LLMs can provide individualized\nlearning and communication support, AR has shown promise in enhancing social\nskills, motivation, and attention. For children with ASD, accessible and\nengaging interventions rely heavily on effective UI/UX design, but there is\nstill a significant lack of robotics-based education and therapeutic programs\nspecifically tailored for autistic children. To optimize the benefits of these\ntechnologies in ASD therapies and immersive education, the study emphasizes the\nneed for additional research to address difficulties related to customization,\naccessibility, and integration."
                },
                "authors": [
                    {
                        "name": "Biplov Paneru"
                    }
                ],
                "author_detail": {
                    "name": "Biplov Paneru"
                },
                "author": "Biplov Paneru",
                "arxiv_comment": "none",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.18162v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.18162v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.08324v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.08324v2",
                "updated": "2025-07-08T15:08:52Z",
                "updated_parsed": [
                    2025,
                    7,
                    8,
                    15,
                    8,
                    52,
                    1,
                    189,
                    0
                ],
                "published": "2024-11-13T04:20:20Z",
                "published_parsed": [
                    2024,
                    11,
                    13,
                    4,
                    20,
                    20,
                    2,
                    318,
                    0
                ],
                "title": "Are LLMs Prescient? A Continuous Evaluation using Daily News as the\n  Oracle",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Are LLMs Prescient? A Continuous Evaluation using Daily News as the\n  Oracle"
                },
                "summary": "Many existing evaluation benchmarks for Large Language Models (LLMs) quickly\nbecome outdated due to the emergence of new models and training data. These\nbenchmarks also fall short in assessing how LLM performance changes over time,\nas they consist of a static set of questions without a temporal dimension. To\naddress these limitations, we propose using future event prediction as a\ncontinuous evaluation method to assess LLMs' temporal generalization and\nforecasting abilities. Our benchmark, Daily Oracle, automatically generates\nquestion-answer (QA) pairs from daily news, challenging LLMs to predict\n\"future\" event outcomes. Our findings reveal that as pre-training data becomes\noutdated, LLM performance degrades over time. While Retrieval Augmented\nGeneration (RAG) has the potential to enhance prediction accuracy, the\nperformance degradation pattern persists, highlighting the need for continuous\nmodel updates. Code and data are available at\nhttps://agenticlearning.ai/daily-oracle.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Many existing evaluation benchmarks for Large Language Models (LLMs) quickly\nbecome outdated due to the emergence of new models and training data. These\nbenchmarks also fall short in assessing how LLM performance changes over time,\nas they consist of a static set of questions without a temporal dimension. To\naddress these limitations, we propose using future event prediction as a\ncontinuous evaluation method to assess LLMs' temporal generalization and\nforecasting abilities. Our benchmark, Daily Oracle, automatically generates\nquestion-answer (QA) pairs from daily news, challenging LLMs to predict\n\"future\" event outcomes. Our findings reveal that as pre-training data becomes\noutdated, LLM performance degrades over time. While Retrieval Augmented\nGeneration (RAG) has the potential to enhance prediction accuracy, the\nperformance degradation pattern persists, highlighting the need for continuous\nmodel updates. Code and data are available at\nhttps://agenticlearning.ai/daily-oracle."
                },
                "authors": [
                    {
                        "name": "Hui Dai"
                    },
                    {
                        "name": "Ryan Teehan"
                    },
                    {
                        "name": "Mengye Ren"
                    }
                ],
                "author_detail": {
                    "name": "Mengye Ren"
                },
                "author": "Mengye Ren",
                "arxiv_comment": "ICML 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.08324v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.08324v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.21432v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.21432v4",
                "updated": "2025-07-08T15:03:11Z",
                "updated_parsed": [
                    2025,
                    7,
                    8,
                    15,
                    3,
                    11,
                    1,
                    189,
                    0
                ],
                "published": "2025-05-27T17:04:21Z",
                "published_parsed": [
                    2025,
                    5,
                    27,
                    17,
                    4,
                    21,
                    1,
                    147,
                    0
                ],
                "title": "Hume: Introducing System-2 Thinking in Visual-Language-Action Model",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Hume: Introducing System-2 Thinking in Visual-Language-Action Model"
                },
                "summary": "Humans practice slow thinking before performing actual actions when handling\ncomplex tasks in the physical world. This thinking paradigm, recently, has\nachieved remarkable advancement in boosting Large Language Models (LLMs) to\nsolve complex tasks in digital domains. However, the potential of slow thinking\nremains largely unexplored for robotic foundation models interacting with the\nphysical world. In this work, we propose Hume: a dual-system\nVision-Language-Action (VLA) model with value-guided System-2 thinking and\ncascaded action denoising, exploring human-like thinking capabilities of\nVision-Language-Action models for dexterous robot control. System 2 of Hume\nimplements value-Guided thinking by extending a Vision-Language-Action Model\nbackbone with a novel value-query head to estimate the state-action value of\npredicted actions. The value-guided thinking is conducted by repeat sampling\nmultiple action candidates and selecting one according to state-action value.\nSystem 1 of Hume is a lightweight reactive visuomotor policy that takes System\n2 selected action and performs cascaded action denoising for dexterous robot\ncontrol. At deployment time, System 2 performs value-guided thinking at a low\nfrequency while System 1 asynchronously receives the System 2 selected action\ncandidate and predicts fluid actions in real time. We show that Hume\noutperforms the existing state-of-the-art Vision-Language-Action models across\nmultiple simulation benchmark and real-robot deployments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Humans practice slow thinking before performing actual actions when handling\ncomplex tasks in the physical world. This thinking paradigm, recently, has\nachieved remarkable advancement in boosting Large Language Models (LLMs) to\nsolve complex tasks in digital domains. However, the potential of slow thinking\nremains largely unexplored for robotic foundation models interacting with the\nphysical world. In this work, we propose Hume: a dual-system\nVision-Language-Action (VLA) model with value-guided System-2 thinking and\ncascaded action denoising, exploring human-like thinking capabilities of\nVision-Language-Action models for dexterous robot control. System 2 of Hume\nimplements value-Guided thinking by extending a Vision-Language-Action Model\nbackbone with a novel value-query head to estimate the state-action value of\npredicted actions. The value-guided thinking is conducted by repeat sampling\nmultiple action candidates and selecting one according to state-action value.\nSystem 1 of Hume is a lightweight reactive visuomotor policy that takes System\n2 selected action and performs cascaded action denoising for dexterous robot\ncontrol. At deployment time, System 2 performs value-guided thinking at a low\nfrequency while System 1 asynchronously receives the System 2 selected action\ncandidate and predicts fluid actions in real time. We show that Hume\noutperforms the existing state-of-the-art Vision-Language-Action models across\nmultiple simulation benchmark and real-robot deployments."
                },
                "authors": [
                    {
                        "name": "Haoming Song"
                    },
                    {
                        "name": "Delin Qu"
                    },
                    {
                        "name": "Yuanqi Yao"
                    },
                    {
                        "name": "Qizhi Chen"
                    },
                    {
                        "name": "Qi Lv"
                    },
                    {
                        "name": "Yiwen Tang"
                    },
                    {
                        "name": "Modi Shi"
                    },
                    {
                        "name": "Guanghui Ren"
                    },
                    {
                        "name": "Maoqing Yao"
                    },
                    {
                        "name": "Bin Zhao"
                    },
                    {
                        "name": "Dong Wang"
                    },
                    {
                        "name": "Xuelong Li"
                    }
                ],
                "author_detail": {
                    "name": "Xuelong Li"
                },
                "author": "Xuelong Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.21432v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.21432v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.12961v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.12961v2",
                "updated": "2025-07-08T15:02:59Z",
                "updated_parsed": [
                    2025,
                    7,
                    8,
                    15,
                    2,
                    59,
                    1,
                    189,
                    0
                ],
                "published": "2025-02-18T15:45:01Z",
                "published_parsed": [
                    2025,
                    2,
                    18,
                    15,
                    45,
                    1,
                    1,
                    49,
                    0
                ],
                "title": "Adaptive Tool Use in Large Language Models with Meta-Cognition Trigger",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Adaptive Tool Use in Large Language Models with Meta-Cognition Trigger"
                },
                "summary": "Large language models (LLMs) have shown remarkable emergent capabilities,\ntransforming the execution of functional tasks by leveraging external tools for\ncomplex problems that require specialized processing or up-to-date data. While\nexisting research expands LLMs access to diverse tools (e.g., program\ninterpreters, search engines, calculators), the necessity of using these tools\nis often overlooked, leading to indiscriminate tool invocation. This naive\napproach raises two key issues: increased latency due to unnecessary tool\ncalls, and potential errors resulting from faulty interactions with external\ntools. In this paper, we introduce meta-cognition as a proxy for LLMs\nself-assessment of their capabilities, reflecting the model's awareness of its\nown limitations. Based on this, we propose MeCo, an adaptive decision-making\nstrategy for external tool use. MeCo quantifies metacognitive scores by\ncapturing high-level cognitive signals in the representation space, guiding\nwhen to invoke tools. Notably, MeCo is fine-tuning-free and incurs minimal\ncost. Experiments across multiple backbone models and benchmarks show that MeCo\nreliably detects LLMs' internal cognitive signals and significantly improves\ntool-use decision-making.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have shown remarkable emergent capabilities,\ntransforming the execution of functional tasks by leveraging external tools for\ncomplex problems that require specialized processing or up-to-date data. While\nexisting research expands LLMs access to diverse tools (e.g., program\ninterpreters, search engines, calculators), the necessity of using these tools\nis often overlooked, leading to indiscriminate tool invocation. This naive\napproach raises two key issues: increased latency due to unnecessary tool\ncalls, and potential errors resulting from faulty interactions with external\ntools. In this paper, we introduce meta-cognition as a proxy for LLMs\nself-assessment of their capabilities, reflecting the model's awareness of its\nown limitations. Based on this, we propose MeCo, an adaptive decision-making\nstrategy for external tool use. MeCo quantifies metacognitive scores by\ncapturing high-level cognitive signals in the representation space, guiding\nwhen to invoke tools. Notably, MeCo is fine-tuning-free and incurs minimal\ncost. Experiments across multiple backbone models and benchmarks show that MeCo\nreliably detects LLMs' internal cognitive signals and significantly improves\ntool-use decision-making."
                },
                "authors": [
                    {
                        "name": "Wenjun Li"
                    },
                    {
                        "name": "Dexun Li"
                    },
                    {
                        "name": "Kuicai Dong"
                    },
                    {
                        "name": "Cong Zhang"
                    },
                    {
                        "name": "Hao Zhang"
                    },
                    {
                        "name": "Weiwen Liu"
                    },
                    {
                        "name": "Yasheng Wang"
                    },
                    {
                        "name": "Ruiming Tang"
                    },
                    {
                        "name": "Yong Liu"
                    }
                ],
                "author_detail": {
                    "name": "Yong Liu"
                },
                "author": "Yong Liu",
                "arxiv_comment": "25 pages, camera ready version for ACL-2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.12961v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.12961v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.06057v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.06057v2",
                "updated": "2025-07-09T07:06:36Z",
                "updated_parsed": [
                    2025,
                    7,
                    9,
                    7,
                    6,
                    36,
                    2,
                    190,
                    0
                ],
                "published": "2025-07-08T14:59:46Z",
                "published_parsed": [
                    2025,
                    7,
                    8,
                    14,
                    59,
                    46,
                    1,
                    189,
                    0
                ],
                "title": "FEVO: Financial Knowledge Expansion and Reasoning Evolution for Large\n  Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FEVO: Financial Knowledge Expansion and Reasoning Evolution for Large\n  Language Models"
                },
                "summary": "Advancements in reasoning for large language models (LLMs) have lead to\nsignificant performance improvements for LLMs in various fields such as\nmathematics and programming. However, research applying these advances to the\nfinancial domain, where considerable domain-specific knowledge is necessary to\ncomplete tasks, remains limited. To address this gap, we introduce FEVO\n(Financial Evolution), a multi-stage enhancement framework developed to enhance\nLLM performance in the financial domain. FEVO systemically enhances LLM\nperformance by using continued pre-training (CPT) to expand financial domain\nknowledge, supervised fine-tuning (SFT) to instill structured, elaborate\nreasoning patterns, and reinforcement learning (RL) to further integrate the\nexpanded financial domain knowledge with the learned structured reasoning. To\nensure effective and efficient training, we leverage frontier reasoning models\nand rule-based filtering to curate FEVO-Train, high-quality datasets\nspecifically designed for the different post-training phases. Using our\nframework, we train the FEVO series of models - C32B, S32B, R32B - from\nQwen2.5-32B and evaluate them on seven benchmarks to assess financial and\ngeneral capabilities, with results showing that FEVO-R32B achieves\nstate-of-the-art performance on five financial benchmarks against much larger\nmodels as well as specialist models. More significantly, FEVO-R32B demonstrates\nmarkedly better performance than FEVO-R32B-0 (trained from Qwen2.5-32B-Instruct\nusing only RL), thus validating the effectiveness of financial domain knowledge\nexpansion and structured, logical reasoning distillation",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Advancements in reasoning for large language models (LLMs) have lead to\nsignificant performance improvements for LLMs in various fields such as\nmathematics and programming. However, research applying these advances to the\nfinancial domain, where considerable domain-specific knowledge is necessary to\ncomplete tasks, remains limited. To address this gap, we introduce FEVO\n(Financial Evolution), a multi-stage enhancement framework developed to enhance\nLLM performance in the financial domain. FEVO systemically enhances LLM\nperformance by using continued pre-training (CPT) to expand financial domain\nknowledge, supervised fine-tuning (SFT) to instill structured, elaborate\nreasoning patterns, and reinforcement learning (RL) to further integrate the\nexpanded financial domain knowledge with the learned structured reasoning. To\nensure effective and efficient training, we leverage frontier reasoning models\nand rule-based filtering to curate FEVO-Train, high-quality datasets\nspecifically designed for the different post-training phases. Using our\nframework, we train the FEVO series of models - C32B, S32B, R32B - from\nQwen2.5-32B and evaluate them on seven benchmarks to assess financial and\ngeneral capabilities, with results showing that FEVO-R32B achieves\nstate-of-the-art performance on five financial benchmarks against much larger\nmodels as well as specialist models. More significantly, FEVO-R32B demonstrates\nmarkedly better performance than FEVO-R32B-0 (trained from Qwen2.5-32B-Instruct\nusing only RL), thus validating the effectiveness of financial domain knowledge\nexpansion and structured, logical reasoning distillation"
                },
                "authors": [
                    {
                        "name": "Bo Pang"
                    },
                    {
                        "name": "Yalu Ouyang"
                    },
                    {
                        "name": "Hangfei Xu"
                    },
                    {
                        "name": "Ziqi Jia"
                    },
                    {
                        "name": "Panpan Li"
                    },
                    {
                        "name": "Shengzhao Wen"
                    },
                    {
                        "name": "Lu Wang"
                    },
                    {
                        "name": "Shiyong Li"
                    },
                    {
                        "name": "Yanpeng Wang"
                    }
                ],
                "author_detail": {
                    "name": "Yanpeng Wang"
                },
                "author": "Yanpeng Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.06057v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.06057v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.06056v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.06056v1",
                "updated": "2025-07-08T14:58:28Z",
                "updated_parsed": [
                    2025,
                    7,
                    8,
                    14,
                    58,
                    28,
                    1,
                    189,
                    0
                ],
                "published": "2025-07-08T14:58:28Z",
                "published_parsed": [
                    2025,
                    7,
                    8,
                    14,
                    58,
                    28,
                    1,
                    189,
                    0
                ],
                "title": "Entropy-Memorization Law: Evaluating Memorization Difficulty of Data in\n  LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Entropy-Memorization Law: Evaluating Memorization Difficulty of Data in\n  LLMs"
                },
                "summary": "Large Language Models (LLMs) are known to memorize portions of their training\ndata, sometimes reproducing content verbatim when prompted appropriately. In\nthis work, we investigate a fundamental yet under-explored question in the\ndomain of memorization: How to characterize memorization difficulty of training\ndata in LLMs? Through empirical experiments on OLMo, a family of open models,\nwe present the Entropy-Memorization Law. It suggests that data entropy is\nlinearly correlated with memorization score. Moreover, in a case study of\nmemorizing highly randomized strings, or \"gibberish\", we observe that such\nsequences, despite their apparent randomness, exhibit unexpectedly low\nempirical entropy compared to the broader training corpus. Adopting the same\nstrategy to discover Entropy-Memorization Law, we derive a simple yet effective\napproach to distinguish training and testing data, enabling Dataset Inference\n(DI).",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) are known to memorize portions of their training\ndata, sometimes reproducing content verbatim when prompted appropriately. In\nthis work, we investigate a fundamental yet under-explored question in the\ndomain of memorization: How to characterize memorization difficulty of training\ndata in LLMs? Through empirical experiments on OLMo, a family of open models,\nwe present the Entropy-Memorization Law. It suggests that data entropy is\nlinearly correlated with memorization score. Moreover, in a case study of\nmemorizing highly randomized strings, or \"gibberish\", we observe that such\nsequences, despite their apparent randomness, exhibit unexpectedly low\nempirical entropy compared to the broader training corpus. Adopting the same\nstrategy to discover Entropy-Memorization Law, we derive a simple yet effective\napproach to distinguish training and testing data, enabling Dataset Inference\n(DI)."
                },
                "authors": [
                    {
                        "name": "Yizhan Huang"
                    },
                    {
                        "name": "Zhe Yang"
                    },
                    {
                        "name": "Meifang Chen"
                    },
                    {
                        "name": "Jianping Zhang"
                    },
                    {
                        "name": "Michael R. Lyu"
                    }
                ],
                "author_detail": {
                    "name": "Michael R. Lyu"
                },
                "author": "Michael R. Lyu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.06056v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.06056v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.19652v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.19652v2",
                "updated": "2025-07-08T14:47:33Z",
                "updated_parsed": [
                    2025,
                    7,
                    8,
                    14,
                    47,
                    33,
                    1,
                    189,
                    0
                ],
                "published": "2025-06-24T14:15:26Z",
                "published_parsed": [
                    2025,
                    6,
                    24,
                    14,
                    15,
                    26,
                    1,
                    175,
                    0
                ],
                "title": "Tailored Conversations beyond LLMs: A RL-Based Dialogue Manager",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Tailored Conversations beyond LLMs: A RL-Based Dialogue Manager"
                },
                "summary": "In this work, we propose a novel framework that integrates large language\nmodels (LLMs) with an RL-based dialogue manager for open-ended dialogue with a\nspecific goal. By leveraging hierarchical reinforcement learning to model the\nstructured phases of dialogue and employ meta-learning to enhance adaptability\nacross diverse user profiles, our approach enhances adaptability and\nefficiency, enabling the system to learn from limited data, transition fluidly\nbetween dialogue phases, and personalize responses to heterogeneous patient\nneeds. We apply our framework to Motivational Interviews, aiming to foster\nbehavior change, and demonstrate that the proposed dialogue manager outperforms\na state-of-the-art LLM baseline in terms of reward, showing a potential benefit\nof conditioning LLMs to create open-ended dialogue systems with specific goals.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this work, we propose a novel framework that integrates large language\nmodels (LLMs) with an RL-based dialogue manager for open-ended dialogue with a\nspecific goal. By leveraging hierarchical reinforcement learning to model the\nstructured phases of dialogue and employ meta-learning to enhance adaptability\nacross diverse user profiles, our approach enhances adaptability and\nefficiency, enabling the system to learn from limited data, transition fluidly\nbetween dialogue phases, and personalize responses to heterogeneous patient\nneeds. We apply our framework to Motivational Interviews, aiming to foster\nbehavior change, and demonstrate that the proposed dialogue manager outperforms\na state-of-the-art LLM baseline in terms of reward, showing a potential benefit\nof conditioning LLMs to create open-ended dialogue systems with specific goals."
                },
                "authors": [
                    {
                        "name": "Lucie Galland"
                    },
                    {
                        "name": "Catherine Pelachaud"
                    },
                    {
                        "name": "Florian Pecune"
                    }
                ],
                "author_detail": {
                    "name": "Florian Pecune"
                },
                "author": "Florian Pecune",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.19652v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.19652v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.06044v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.06044v1",
                "updated": "2025-07-08T14:45:47Z",
                "updated_parsed": [
                    2025,
                    7,
                    8,
                    14,
                    45,
                    47,
                    1,
                    189,
                    0
                ],
                "published": "2025-07-08T14:45:47Z",
                "published_parsed": [
                    2025,
                    7,
                    8,
                    14,
                    45,
                    47,
                    1,
                    189,
                    0
                ],
                "title": "Hierarchical Interaction Summarization and Contrastive Prompting for\n  Explainable Recommendations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Hierarchical Interaction Summarization and Contrastive Prompting for\n  Explainable Recommendations"
                },
                "summary": "Explainable recommendations, which use the information of user and item with\ninteraction to generate a explanation for why the user would interact with the\nitem, are crucial for improving user trust and decision transparency to the\nrecommender system. Existing methods primarily rely on encoding features of\nusers and items to embeddings, which often leads to information loss due to\ndimensionality reduction, sparse interactions, and so on. With the advancements\nof large language models (LLMs) in language comprehension, some methods use\nembeddings as LLM inputs for explanation generation. However, since embeddings\nlack inherent semantics, LLMs must adjust or extend their parameters to\ninterpret them, a process that inevitably incurs information loss. To address\nthis issue, we propose a novel approach combining profile generation via\nhierarchical interaction summarization (PGHIS), which leverages a pretrained\nLLM to hierarchically summarize user-item interactions, generating structured\ntextual profiles as explicit representations of user and item characteristics.\nAdditionally, we propose contrastive prompting for explanation generation\n(CPEG) which employs contrastive learning to guide another reasoning language\nmodels in producing high-quality ground truth recommendation explanations.\nFinally, we use the textual profiles of user and item as input and high-quality\nexplanation as output to fine-tune a LLM for generating explanations.\nExperimental results on multiple datasets demonstrate that our approach\noutperforms existing state-of-the-art methods, achieving a great improvement on\nmetrics about explainability (e.g., 5% on GPTScore) and text quality.\nFurthermore, our generated ground truth explanations achieve a significantly\nhigher win rate compared to user-written reviews and those produced by other\nmethods, demonstrating the effectiveness of CPEG in generating high-quality\nground truths.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Explainable recommendations, which use the information of user and item with\ninteraction to generate a explanation for why the user would interact with the\nitem, are crucial for improving user trust and decision transparency to the\nrecommender system. Existing methods primarily rely on encoding features of\nusers and items to embeddings, which often leads to information loss due to\ndimensionality reduction, sparse interactions, and so on. With the advancements\nof large language models (LLMs) in language comprehension, some methods use\nembeddings as LLM inputs for explanation generation. However, since embeddings\nlack inherent semantics, LLMs must adjust or extend their parameters to\ninterpret them, a process that inevitably incurs information loss. To address\nthis issue, we propose a novel approach combining profile generation via\nhierarchical interaction summarization (PGHIS), which leverages a pretrained\nLLM to hierarchically summarize user-item interactions, generating structured\ntextual profiles as explicit representations of user and item characteristics.\nAdditionally, we propose contrastive prompting for explanation generation\n(CPEG) which employs contrastive learning to guide another reasoning language\nmodels in producing high-quality ground truth recommendation explanations.\nFinally, we use the textual profiles of user and item as input and high-quality\nexplanation as output to fine-tune a LLM for generating explanations.\nExperimental results on multiple datasets demonstrate that our approach\noutperforms existing state-of-the-art methods, achieving a great improvement on\nmetrics about explainability (e.g., 5% on GPTScore) and text quality.\nFurthermore, our generated ground truth explanations achieve a significantly\nhigher win rate compared to user-written reviews and those produced by other\nmethods, demonstrating the effectiveness of CPEG in generating high-quality\nground truths."
                },
                "authors": [
                    {
                        "name": "Yibin Liu"
                    },
                    {
                        "name": "Ang Li"
                    },
                    {
                        "name": "Shijian Li"
                    }
                ],
                "author_detail": {
                    "name": "Shijian Li"
                },
                "author": "Shijian Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.06044v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.06044v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.06043v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.06043v1",
                "updated": "2025-07-08T14:45:21Z",
                "updated_parsed": [
                    2025,
                    7,
                    8,
                    14,
                    45,
                    21,
                    1,
                    189,
                    0
                ],
                "published": "2025-07-08T14:45:21Z",
                "published_parsed": [
                    2025,
                    7,
                    8,
                    14,
                    45,
                    21,
                    1,
                    189,
                    0
                ],
                "title": "CAVGAN: Unifying Jailbreak and Defense of LLMs via Generative\n  Adversarial Attacks on their Internal Representations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CAVGAN: Unifying Jailbreak and Defense of LLMs via Generative\n  Adversarial Attacks on their Internal Representations"
                },
                "summary": "Security alignment enables the Large Language Model (LLM) to gain the\nprotection against malicious queries, but various jailbreak attack methods\nreveal the vulnerability of this security mechanism. Previous studies have\nisolated LLM jailbreak attacks and defenses. We analyze the security protection\nmechanism of the LLM, and propose a framework that combines attack and defense.\nOur method is based on the linearly separable property of LLM intermediate\nlayer embedding, as well as the essence of jailbreak attack, which aims to\nembed harmful problems and transfer them to the safe area. We utilize\ngenerative adversarial network (GAN) to learn the security judgment boundary\ninside the LLM to achieve efficient jailbreak attack and defense. The\nexperimental results indicate that our method achieves an average jailbreak\nsuccess rate of 88.85\\% across three popular LLMs, while the defense success\nrate on the state-of-the-art jailbreak dataset reaches an average of 84.17\\%.\nThis not only validates the effectiveness of our approach but also sheds light\non the internal security mechanisms of LLMs, offering new insights for\nenhancing model security The code and data are available at\nhttps://github.com/NLPGM/CAVGAN.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Security alignment enables the Large Language Model (LLM) to gain the\nprotection against malicious queries, but various jailbreak attack methods\nreveal the vulnerability of this security mechanism. Previous studies have\nisolated LLM jailbreak attacks and defenses. We analyze the security protection\nmechanism of the LLM, and propose a framework that combines attack and defense.\nOur method is based on the linearly separable property of LLM intermediate\nlayer embedding, as well as the essence of jailbreak attack, which aims to\nembed harmful problems and transfer them to the safe area. We utilize\ngenerative adversarial network (GAN) to learn the security judgment boundary\ninside the LLM to achieve efficient jailbreak attack and defense. The\nexperimental results indicate that our method achieves an average jailbreak\nsuccess rate of 88.85\\% across three popular LLMs, while the defense success\nrate on the state-of-the-art jailbreak dataset reaches an average of 84.17\\%.\nThis not only validates the effectiveness of our approach but also sheds light\non the internal security mechanisms of LLMs, offering new insights for\nenhancing model security The code and data are available at\nhttps://github.com/NLPGM/CAVGAN."
                },
                "authors": [
                    {
                        "name": "Xiaohu Li"
                    },
                    {
                        "name": "Yunfeng Ning"
                    },
                    {
                        "name": "Zepeng Bao"
                    },
                    {
                        "name": "Mayi Xu"
                    },
                    {
                        "name": "Jianhao Chen"
                    },
                    {
                        "name": "Tieyun Qian"
                    }
                ],
                "author_detail": {
                    "name": "Tieyun Qian"
                },
                "author": "Tieyun Qian",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.06043v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.06043v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.06037v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.06037v1",
                "updated": "2025-07-08T14:40:39Z",
                "updated_parsed": [
                    2025,
                    7,
                    8,
                    14,
                    40,
                    39,
                    1,
                    189,
                    0
                ],
                "published": "2025-07-08T14:40:39Z",
                "published_parsed": [
                    2025,
                    7,
                    8,
                    14,
                    40,
                    39,
                    1,
                    189,
                    0
                ],
                "title": "Permutations accelerate Approximate Bayesian Computation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Permutations accelerate Approximate Bayesian Computation"
                },
                "summary": "Approximate Bayesian Computation (ABC) methods have become essential tools\nfor performing inference when likelihood functions are intractable or\ncomputationally prohibitive. However, their scalability remains a major\nchallenge in hierarchical or high-dimensional models. In this paper, we\nintroduce permABC, a new ABC framework designed for settings with both global\nand local parameters, where observations are grouped into exchangeable\ncompartments.\n  Building upon the Sequential Monte Carlo ABC (ABC-SMC) framework, permABC\nexploits the exchangeability of compartments through permutation-based\nmatching, significantly improving computational efficiency.\n  We then develop two further, complementary sequential strategies: Over\nSampling, which facilitates early-stage acceptance by temporarily increasing\nthe number of simulated compartments, and Under Matching, which relaxes the\nacceptance condition by matching only subsets of the data.\n  These techniques allow for robust and scalable inference even in\nhigh-dimensional regimes. Through synthetic and real-world experiments --\nincluding a hierarchical Susceptible-Infectious-Recover model of the early\nCOVID-19 epidemic across 94 French departments -- we demonstrate the practical\ngains in accuracy and efficiency achieved by our approach.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Approximate Bayesian Computation (ABC) methods have become essential tools\nfor performing inference when likelihood functions are intractable or\ncomputationally prohibitive. However, their scalability remains a major\nchallenge in hierarchical or high-dimensional models. In this paper, we\nintroduce permABC, a new ABC framework designed for settings with both global\nand local parameters, where observations are grouped into exchangeable\ncompartments.\n  Building upon the Sequential Monte Carlo ABC (ABC-SMC) framework, permABC\nexploits the exchangeability of compartments through permutation-based\nmatching, significantly improving computational efficiency.\n  We then develop two further, complementary sequential strategies: Over\nSampling, which facilitates early-stage acceptance by temporarily increasing\nthe number of simulated compartments, and Under Matching, which relaxes the\nacceptance condition by matching only subsets of the data.\n  These techniques allow for robust and scalable inference even in\nhigh-dimensional regimes. Through synthetic and real-world experiments --\nincluding a hierarchical Susceptible-Infectious-Recover model of the early\nCOVID-19 epidemic across 94 French departments -- we demonstrate the practical\ngains in accuracy and efficiency achieved by our approach."
                },
                "authors": [
                    {
                        "name": "Antoine Luciano"
                    },
                    {
                        "name": "Charly Andral"
                    },
                    {
                        "name": "Christian P. Robert"
                    },
                    {
                        "name": "Robin J. Ryder"
                    }
                ],
                "author_detail": {
                    "name": "Robin J. Ryder"
                },
                "author": "Robin J. Ryder",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.06037v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.06037v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ME",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.CO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.01754v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.01754v3",
                "updated": "2025-07-08T14:34:57Z",
                "updated_parsed": [
                    2025,
                    7,
                    8,
                    14,
                    34,
                    57,
                    1,
                    189,
                    0
                ],
                "published": "2024-09-03T10:01:51Z",
                "published_parsed": [
                    2024,
                    9,
                    3,
                    10,
                    1,
                    51,
                    1,
                    247,
                    0
                ],
                "title": "Empirical evidence of Large Language Model's influence on human spoken\n  communication",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Empirical evidence of Large Language Model's influence on human spoken\n  communication"
                },
                "summary": "From the invention of writing and the printing press, to television and\nsocial media, human history is punctuated by major innovations in communication\ntechnology, which fundamentally altered how ideas spread and reshaped our\nculture. Recent chatbots powered by generative artificial intelligence\nconstitute a novel medium that encodes cultural patterns in their neural\nrepresentations and disseminates them in conversations with hundreds of\nmillions of people. Understanding whether these patterns transmit into human\nlanguage, and ultimately shape human culture, is a fundamental question. While\nfully quantifying the causal impact of a chatbot like ChatGPT on human culture\nis very challenging, lexicographic shift in human spoken communication may\noffer an early indicator of such broad phenomenon. Here, we apply econometric\ncausal inference techniques to 740,249 hours of human discourse from 360,445\nYouTube academic talks and 771,591 conversational podcast episodes across\nmultiple disciplines. We detect a measurable and abrupt increase in the use of\nwords preferentially generated by ChatGPT, such as delve, comprehend, boast,\nswift, and meticulous, after its release. These findings suggest a scenario\nwhere machines, originally trained on human data and subsequently exhibiting\ntheir own cultural traits, can, in turn, measurably reshape human culture. This\nmarks the beginning of a closed cultural feedback loop in which cultural traits\ncirculate bidirectionally between humans and machines. Our results motivate\nfurther research into the evolution of human-machine culture, and raise\nconcerns over the erosion of linguistic and cultural diversity, and the risks\nof scalable manipulation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "From the invention of writing and the printing press, to television and\nsocial media, human history is punctuated by major innovations in communication\ntechnology, which fundamentally altered how ideas spread and reshaped our\nculture. Recent chatbots powered by generative artificial intelligence\nconstitute a novel medium that encodes cultural patterns in their neural\nrepresentations and disseminates them in conversations with hundreds of\nmillions of people. Understanding whether these patterns transmit into human\nlanguage, and ultimately shape human culture, is a fundamental question. While\nfully quantifying the causal impact of a chatbot like ChatGPT on human culture\nis very challenging, lexicographic shift in human spoken communication may\noffer an early indicator of such broad phenomenon. Here, we apply econometric\ncausal inference techniques to 740,249 hours of human discourse from 360,445\nYouTube academic talks and 771,591 conversational podcast episodes across\nmultiple disciplines. We detect a measurable and abrupt increase in the use of\nwords preferentially generated by ChatGPT, such as delve, comprehend, boast,\nswift, and meticulous, after its release. These findings suggest a scenario\nwhere machines, originally trained on human data and subsequently exhibiting\ntheir own cultural traits, can, in turn, measurably reshape human culture. This\nmarks the beginning of a closed cultural feedback loop in which cultural traits\ncirculate bidirectionally between humans and machines. Our results motivate\nfurther research into the evolution of human-machine culture, and raise\nconcerns over the erosion of linguistic and cultural diversity, and the risks\nof scalable manipulation."
                },
                "authors": [
                    {
                        "name": "Hiromu Yakura"
                    },
                    {
                        "name": "Ezequiel Lopez-Lopez"
                    },
                    {
                        "name": "Levin Brinkmann"
                    },
                    {
                        "name": "Ignacio Serna"
                    },
                    {
                        "name": "Prateek Gupta"
                    },
                    {
                        "name": "Ivan Soraperra"
                    },
                    {
                        "name": "Iyad Rahwan"
                    }
                ],
                "author_detail": {
                    "name": "Iyad Rahwan"
                },
                "author": "Iyad Rahwan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.01754v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.01754v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.03724v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.03724v2",
                "updated": "2025-07-08T14:30:24Z",
                "updated_parsed": [
                    2025,
                    7,
                    8,
                    14,
                    30,
                    24,
                    1,
                    189,
                    0
                ],
                "published": "2025-07-04T17:21:46Z",
                "published_parsed": [
                    2025,
                    7,
                    4,
                    17,
                    21,
                    46,
                    4,
                    185,
                    0
                ],
                "title": "MemOS: A Memory OS for AI System",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MemOS: A Memory OS for AI System"
                },
                "summary": "Large Language Models (LLMs) have become an essential infrastructure for\nArtificial General Intelligence (AGI), yet their lack of well-defined memory\nmanagement systems hinders the development of long-context reasoning, continual\npersonalization, and knowledge consistency.Existing models mainly rely on\nstatic parameters and short-lived contextual states, limiting their ability to\ntrack user preferences or update knowledge over extended periods.While\nRetrieval-Augmented Generation (RAG) introduces external knowledge in plain\ntext, it remains a stateless workaround without lifecycle control or\nintegration with persistent representations.Recent work has modeled the\ntraining and inference cost of LLMs from a memory hierarchy perspective,\nshowing that introducing an explicit memory layer between parameter memory and\nexternal retrieval can substantially reduce these costs by externalizing\nspecific knowledge. Beyond computational efficiency, LLMs face broader\nchallenges arising from how information is distributed over time and context,\nrequiring systems capable of managing heterogeneous knowledge spanning\ndifferent temporal scales and sources. To address this challenge, we propose\nMemOS, a memory operating system that treats memory as a manageable system\nresource. It unifies the representation, scheduling, and evolution of\nplaintext, activation-based, and parameter-level memories, enabling\ncost-efficient storage and retrieval. As the basic unit, a MemCube encapsulates\nboth memory content and metadata such as provenance and versioning. MemCubes\ncan be composed, migrated, and fused over time, enabling flexible transitions\nbetween memory types and bridging retrieval with parameter-based learning.\nMemOS establishes a memory-centric system framework that brings\ncontrollability, plasticity, and evolvability to LLMs, laying the foundation\nfor continual learning and personalized modeling.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have become an essential infrastructure for\nArtificial General Intelligence (AGI), yet their lack of well-defined memory\nmanagement systems hinders the development of long-context reasoning, continual\npersonalization, and knowledge consistency.Existing models mainly rely on\nstatic parameters and short-lived contextual states, limiting their ability to\ntrack user preferences or update knowledge over extended periods.While\nRetrieval-Augmented Generation (RAG) introduces external knowledge in plain\ntext, it remains a stateless workaround without lifecycle control or\nintegration with persistent representations.Recent work has modeled the\ntraining and inference cost of LLMs from a memory hierarchy perspective,\nshowing that introducing an explicit memory layer between parameter memory and\nexternal retrieval can substantially reduce these costs by externalizing\nspecific knowledge. Beyond computational efficiency, LLMs face broader\nchallenges arising from how information is distributed over time and context,\nrequiring systems capable of managing heterogeneous knowledge spanning\ndifferent temporal scales and sources. To address this challenge, we propose\nMemOS, a memory operating system that treats memory as a manageable system\nresource. It unifies the representation, scheduling, and evolution of\nplaintext, activation-based, and parameter-level memories, enabling\ncost-efficient storage and retrieval. As the basic unit, a MemCube encapsulates\nboth memory content and metadata such as provenance and versioning. MemCubes\ncan be composed, migrated, and fused over time, enabling flexible transitions\nbetween memory types and bridging retrieval with parameter-based learning.\nMemOS establishes a memory-centric system framework that brings\ncontrollability, plasticity, and evolvability to LLMs, laying the foundation\nfor continual learning and personalized modeling."
                },
                "authors": [
                    {
                        "name": "Zhiyu Li"
                    },
                    {
                        "name": "Shichao Song"
                    },
                    {
                        "name": "Chenyang Xi"
                    },
                    {
                        "name": "Hanyu Wang"
                    },
                    {
                        "name": "Chen Tang"
                    },
                    {
                        "name": "Simin Niu"
                    },
                    {
                        "name": "Ding Chen"
                    },
                    {
                        "name": "Jiawei Yang"
                    },
                    {
                        "name": "Chunyu Li"
                    },
                    {
                        "name": "Qingchen Yu"
                    },
                    {
                        "name": "Jihao Zhao"
                    },
                    {
                        "name": "Yezhaohui Wang"
                    },
                    {
                        "name": "Peng Liu"
                    },
                    {
                        "name": "Zehao Lin"
                    },
                    {
                        "name": "Pengyuan Wang"
                    },
                    {
                        "name": "Jiahao Huo"
                    },
                    {
                        "name": "Tianyi Chen"
                    },
                    {
                        "name": "Kai Chen"
                    },
                    {
                        "name": "Kehang Li"
                    },
                    {
                        "name": "Zhen Tao"
                    },
                    {
                        "name": "Junpeng Ren"
                    },
                    {
                        "name": "Huayi Lai"
                    },
                    {
                        "name": "Hao Wu"
                    },
                    {
                        "name": "Bo Tang"
                    },
                    {
                        "name": "Zhenren Wang"
                    },
                    {
                        "name": "Zhaoxin Fan"
                    },
                    {
                        "name": "Ningyu Zhang"
                    },
                    {
                        "name": "Linfeng Zhang"
                    },
                    {
                        "name": "Junchi Yan"
                    },
                    {
                        "name": "Mingchuan Yang"
                    },
                    {
                        "name": "Tong Xu"
                    },
                    {
                        "name": "Wei Xu"
                    },
                    {
                        "name": "Huajun Chen"
                    },
                    {
                        "name": "Haofeng Wang"
                    },
                    {
                        "name": "Hongkang Yang"
                    },
                    {
                        "name": "Wentao Zhang"
                    },
                    {
                        "name": "Zhi-Qin John Xu"
                    },
                    {
                        "name": "Siheng Chen"
                    },
                    {
                        "name": "Feiyu Xiong"
                    }
                ],
                "author_detail": {
                    "name": "Feiyu Xiong"
                },
                "author": "Feiyu Xiong",
                "arxiv_comment": "36 pages, 10 figures, 5 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.03724v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.03724v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.06021v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.06021v1",
                "updated": "2025-07-08T14:30:10Z",
                "updated_parsed": [
                    2025,
                    7,
                    8,
                    14,
                    30,
                    10,
                    1,
                    189,
                    0
                ],
                "published": "2025-07-08T14:30:10Z",
                "published_parsed": [
                    2025,
                    7,
                    8,
                    14,
                    30,
                    10,
                    1,
                    189,
                    0
                ],
                "title": "Kamae: Bridging Spark and Keras for Seamless ML Preprocessing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Kamae: Bridging Spark and Keras for Seamless ML Preprocessing"
                },
                "summary": "In production recommender systems, feature preprocessing must be faithfully\nreplicated across training and inference environments. This often requires\nduplicating logic between offline and online environments, increasing\nengineering effort and introducing risks of dataset shift. We present Kamae, an\nopen-source Python library that bridges this gap by translating PySpark\npreprocessing pipelines into equivalent Keras models. Kamae provides a suite of\nconfigurable Spark transformers and estimators, each mapped to a corresponding\nKeras layer, enabling consistent, end-to-end preprocessing across the ML\nlifecycle. Framework's utility is illustrated on real-world use cases,\nincluding MovieLens dataset and Expedia's Learning-to-Rank pipelines. The code\nis available at https://github.com/ExpediaGroup/kamae.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In production recommender systems, feature preprocessing must be faithfully\nreplicated across training and inference environments. This often requires\nduplicating logic between offline and online environments, increasing\nengineering effort and introducing risks of dataset shift. We present Kamae, an\nopen-source Python library that bridges this gap by translating PySpark\npreprocessing pipelines into equivalent Keras models. Kamae provides a suite of\nconfigurable Spark transformers and estimators, each mapped to a corresponding\nKeras layer, enabling consistent, end-to-end preprocessing across the ML\nlifecycle. Framework's utility is illustrated on real-world use cases,\nincluding MovieLens dataset and Expedia's Learning-to-Rank pipelines. The code\nis available at https://github.com/ExpediaGroup/kamae."
                },
                "authors": [
                    {
                        "name": "George Barrowclough"
                    },
                    {
                        "name": "Marian Andrecki"
                    },
                    {
                        "name": "James Shinner"
                    },
                    {
                        "name": "Daniele Donghi"
                    }
                ],
                "author_detail": {
                    "name": "Daniele Donghi"
                },
                "author": "Daniele Donghi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.06021v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.06021v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.06016v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.06016v1",
                "updated": "2025-07-08T14:23:41Z",
                "updated_parsed": [
                    2025,
                    7,
                    8,
                    14,
                    23,
                    41,
                    1,
                    189,
                    0
                ],
                "published": "2025-07-08T14:23:41Z",
                "published_parsed": [
                    2025,
                    7,
                    8,
                    14,
                    23,
                    41,
                    1,
                    189,
                    0
                ],
                "title": "Conditional Multi-Stage Failure Recovery for Embodied Agents",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Conditional Multi-Stage Failure Recovery for Embodied Agents"
                },
                "summary": "Embodied agents performing complex tasks are susceptible to execution\nfailures, motivating the need for effective failure recovery mechanisms. In\nthis work, we introduce a conditional multistage failure recovery framework\nthat employs zero-shot chain prompting. The framework is structured into four\nerror-handling stages, with three operating during task execution and one\nfunctioning as a post-execution reflection phase. Our approach utilises the\nreasoning capabilities of LLMs to analyse execution challenges within their\nenvironmental context and devise strategic solutions. We evaluate our method on\nthe TfD benchmark of the TEACH dataset and achieve state-of-the-art\nperformance, outperforming a baseline without error recovery by 11.5% and\nsurpassing the strongest existing model by 19%.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Embodied agents performing complex tasks are susceptible to execution\nfailures, motivating the need for effective failure recovery mechanisms. In\nthis work, we introduce a conditional multistage failure recovery framework\nthat employs zero-shot chain prompting. The framework is structured into four\nerror-handling stages, with three operating during task execution and one\nfunctioning as a post-execution reflection phase. Our approach utilises the\nreasoning capabilities of LLMs to analyse execution challenges within their\nenvironmental context and devise strategic solutions. We evaluate our method on\nthe TfD benchmark of the TEACH dataset and achieve state-of-the-art\nperformance, outperforming a baseline without error recovery by 11.5% and\nsurpassing the strongest existing model by 19%."
                },
                "authors": [
                    {
                        "name": "Youmna Farag"
                    },
                    {
                        "name": "Svetlana Stoyanchev"
                    },
                    {
                        "name": "Mohan Li"
                    },
                    {
                        "name": "Simon Keizer"
                    },
                    {
                        "name": "Rama Doddipatla"
                    }
                ],
                "author_detail": {
                    "name": "Rama Doddipatla"
                },
                "author": "Rama Doddipatla",
                "arxiv_comment": "Accepted at REALM 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.06016v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.06016v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.06013v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.06013v1",
                "updated": "2025-07-08T14:17:07Z",
                "updated_parsed": [
                    2025,
                    7,
                    8,
                    14,
                    17,
                    7,
                    1,
                    189,
                    0
                ],
                "published": "2025-07-08T14:17:07Z",
                "published_parsed": [
                    2025,
                    7,
                    8,
                    14,
                    17,
                    7,
                    1,
                    189,
                    0
                ],
                "title": "CogniSQL-R1-Zero: Lightweight Reinforced Reasoning for Efficient SQL\n  Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CogniSQL-R1-Zero: Lightweight Reinforced Reasoning for Efficient SQL\n  Generation"
                },
                "summary": "Translating natural language into SQL (Text-to-SQL) remains a core challenge\nat the intersection of language understanding and structured data access.\nAlthough large language models (LLMs) have improved fluency, generating correct\nand executable SQL, especially for complex queries, continues to be\nchallenging. We introduce CogniSQL-R1-Zero, a reinforcement learning (RL)\nframework and model that produces accurate SQL using a lightweight reward\nsignal based on execution correctness and format-tag compliance. By avoiding\nintermediate supervision, hybrid pipelines and complex reward shaping, our\nmethod encourages stable learning and stronger alignment with the ultimate task\nobjective-producing executable programs. CogniSQL-R1-Zero achieves\nstate-of-the-art execution accuracy on Text2SQL benchmark; BIRD bench,\noutperforming prior supervised and instruction-tuned baselines including SFT\nCodeS-7B, DeepSeek-Coder 236B, and Mistral 123B-despite being trained on a\nsignificantly smaller 7B backbone. This result underscores the scalability and\nefficiency of our RL-based approach when trained on just four NVIDIA A100 GPUs\n(40 GB VRAM each). To support further research in efficient and interpretable\nText-to-SQL modeling, we release two curated datasets: (i) a collection of\n5,024 reasoning traces with varying context lengths, and (ii) a\npositive-sampled corpus of 36,356 corpus of weakly supervised queries, each\nannotated with six semantically diverse reasoning paths. Together, these\ncontributions advance scalable, execution-aligned Text-to-SQL generation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Translating natural language into SQL (Text-to-SQL) remains a core challenge\nat the intersection of language understanding and structured data access.\nAlthough large language models (LLMs) have improved fluency, generating correct\nand executable SQL, especially for complex queries, continues to be\nchallenging. We introduce CogniSQL-R1-Zero, a reinforcement learning (RL)\nframework and model that produces accurate SQL using a lightweight reward\nsignal based on execution correctness and format-tag compliance. By avoiding\nintermediate supervision, hybrid pipelines and complex reward shaping, our\nmethod encourages stable learning and stronger alignment with the ultimate task\nobjective-producing executable programs. CogniSQL-R1-Zero achieves\nstate-of-the-art execution accuracy on Text2SQL benchmark; BIRD bench,\noutperforming prior supervised and instruction-tuned baselines including SFT\nCodeS-7B, DeepSeek-Coder 236B, and Mistral 123B-despite being trained on a\nsignificantly smaller 7B backbone. This result underscores the scalability and\nefficiency of our RL-based approach when trained on just four NVIDIA A100 GPUs\n(40 GB VRAM each). To support further research in efficient and interpretable\nText-to-SQL modeling, we release two curated datasets: (i) a collection of\n5,024 reasoning traces with varying context lengths, and (ii) a\npositive-sampled corpus of 36,356 corpus of weakly supervised queries, each\nannotated with six semantically diverse reasoning paths. Together, these\ncontributions advance scalable, execution-aligned Text-to-SQL generation."
                },
                "authors": [
                    {
                        "name": "Kushal Gajjar"
                    },
                    {
                        "name": "Harshit Sikchi"
                    },
                    {
                        "name": "Arpit Singh Gautam"
                    },
                    {
                        "name": "Marc Hammons"
                    },
                    {
                        "name": "Saurabh Jha"
                    }
                ],
                "author_detail": {
                    "name": "Saurabh Jha"
                },
                "author": "Saurabh Jha",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.06013v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.06013v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.08696v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.08696v3",
                "updated": "2025-07-08T14:03:25Z",
                "updated_parsed": [
                    2025,
                    7,
                    8,
                    14,
                    3,
                    25,
                    1,
                    189,
                    0
                ],
                "published": "2025-02-12T18:59:55Z",
                "published_parsed": [
                    2025,
                    2,
                    12,
                    18,
                    59,
                    55,
                    2,
                    43,
                    0
                ],
                "title": "Scalable Discrete Diffusion Samplers: Combinatorial Optimization and\n  Statistical Physics",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Scalable Discrete Diffusion Samplers: Combinatorial Optimization and\n  Statistical Physics"
                },
                "summary": "Learning to sample from complex unnormalized distributions over discrete\ndomains emerged as a promising research direction with applications in\nstatistical physics, variational inference, and combinatorial optimization.\nRecent work has demonstrated the potential of diffusion models in this domain.\nHowever, existing methods face limitations in memory scaling and thus the\nnumber of attainable diffusion steps since they require backpropagation through\nthe entire generative process. To overcome these limitations we introduce two\nnovel training methods for discrete diffusion samplers, one grounded in the\npolicy gradient theorem and the other one leveraging Self-Normalized Neural\nImportance Sampling (SN-NIS). These methods yield memory-efficient training and\nachieve state-of-the-art results in unsupervised combinatorial optimization.\nNumerous scientific applications additionally require the ability of unbiased\nsampling. We introduce adaptations of SN-NIS and Neural Markov Chain Monte\nCarlo that enable for the first time the application of discrete diffusion\nmodels to this problem. We validate our methods on Ising model benchmarks and\nfind that they outperform popular autoregressive approaches. Our work opens new\navenues for applying diffusion models to a wide range of scientific\napplications in discrete domains that were hitherto restricted to exact\nlikelihood models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Learning to sample from complex unnormalized distributions over discrete\ndomains emerged as a promising research direction with applications in\nstatistical physics, variational inference, and combinatorial optimization.\nRecent work has demonstrated the potential of diffusion models in this domain.\nHowever, existing methods face limitations in memory scaling and thus the\nnumber of attainable diffusion steps since they require backpropagation through\nthe entire generative process. To overcome these limitations we introduce two\nnovel training methods for discrete diffusion samplers, one grounded in the\npolicy gradient theorem and the other one leveraging Self-Normalized Neural\nImportance Sampling (SN-NIS). These methods yield memory-efficient training and\nachieve state-of-the-art results in unsupervised combinatorial optimization.\nNumerous scientific applications additionally require the ability of unbiased\nsampling. We introduce adaptations of SN-NIS and Neural Markov Chain Monte\nCarlo that enable for the first time the application of discrete diffusion\nmodels to this problem. We validate our methods on Ising model benchmarks and\nfind that they outperform popular autoregressive approaches. Our work opens new\navenues for applying diffusion models to a wide range of scientific\napplications in discrete domains that were hitherto restricted to exact\nlikelihood models."
                },
                "authors": [
                    {
                        "name": "Sebastian Sanokowski"
                    },
                    {
                        "name": "Wilhelm Berghammer"
                    },
                    {
                        "name": "Martin Ennemoser"
                    },
                    {
                        "name": "Haoyu Peter Wang"
                    },
                    {
                        "name": "Sepp Hochreiter"
                    },
                    {
                        "name": "Sebastian Lehner"
                    }
                ],
                "author_detail": {
                    "name": "Sebastian Lehner"
                },
                "author": "Sebastian Lehner",
                "arxiv_comment": "Accepted at ICLR 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.08696v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.08696v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cond-mat.stat-mech",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "physics.comp-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.05997v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.05997v1",
                "updated": "2025-07-08T13:55:25Z",
                "updated_parsed": [
                    2025,
                    7,
                    8,
                    13,
                    55,
                    25,
                    1,
                    189,
                    0
                ],
                "published": "2025-07-08T13:55:25Z",
                "published_parsed": [
                    2025,
                    7,
                    8,
                    13,
                    55,
                    25,
                    1,
                    189,
                    0
                ],
                "title": "DocIE@XLLM25: In-Context Learning for Information Extraction using Fully\n  Synthetic Demonstrations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DocIE@XLLM25: In-Context Learning for Information Extraction using Fully\n  Synthetic Demonstrations"
                },
                "summary": "Large, high-quality annotated corpora remain scarce in document-level entity\nand relation extraction in zero-shot or few-shot settings. In this paper, we\npresent a fully automatic, LLM-based pipeline for synthetic data generation and\nin-context learning for document-level entity and relation extraction. In\ncontrast to existing approaches that rely on manually annotated demonstrations\nor direct zero-shot inference, our method combines synthetic data generation\nwith retrieval-based in-context learning, using a reasoning-optimized language\nmodel. This allows us to build a high-quality demonstration database without\nmanual annotation and to dynamically retrieve relevant examples at inference\ntime. Based on our approach we produce a synthetic dataset of over $5k$\nWikipedia abstracts with approximately $59k$ entities and $30k$ relation\ntriples. Finally, we evaluate in-context learning performance on the DocIE\nshared task, extracting entities and relations from long documents in a\nzero-shot setting. We find that in-context joint entity and relation extraction\nat document-level remains a challenging task, even for state-of-the-art large\nlanguage models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large, high-quality annotated corpora remain scarce in document-level entity\nand relation extraction in zero-shot or few-shot settings. In this paper, we\npresent a fully automatic, LLM-based pipeline for synthetic data generation and\nin-context learning for document-level entity and relation extraction. In\ncontrast to existing approaches that rely on manually annotated demonstrations\nor direct zero-shot inference, our method combines synthetic data generation\nwith retrieval-based in-context learning, using a reasoning-optimized language\nmodel. This allows us to build a high-quality demonstration database without\nmanual annotation and to dynamically retrieve relevant examples at inference\ntime. Based on our approach we produce a synthetic dataset of over $5k$\nWikipedia abstracts with approximately $59k$ entities and $30k$ relation\ntriples. Finally, we evaluate in-context learning performance on the DocIE\nshared task, extracting entities and relations from long documents in a\nzero-shot setting. We find that in-context joint entity and relation extraction\nat document-level remains a challenging task, even for state-of-the-art large\nlanguage models."
                },
                "authors": [
                    {
                        "name": "Nicholas PopoviÄ"
                    },
                    {
                        "name": "Ashish Kangen"
                    },
                    {
                        "name": "Tim Schopf"
                    },
                    {
                        "name": "Michael FÃ¤rber"
                    }
                ],
                "author_detail": {
                    "name": "Michael FÃ¤rber"
                },
                "author": "Michael FÃ¤rber",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.05997v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.05997v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.05995v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.05995v1",
                "updated": "2025-07-08T13:54:22Z",
                "updated_parsed": [
                    2025,
                    7,
                    8,
                    13,
                    54,
                    22,
                    1,
                    189,
                    0
                ],
                "published": "2025-07-08T13:54:22Z",
                "published_parsed": [
                    2025,
                    7,
                    8,
                    13,
                    54,
                    22,
                    1,
                    189,
                    0
                ],
                "title": "PromiseTune: Unveiling Causally Promising and Explainable Configuration\n  Tuning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PromiseTune: Unveiling Causally Promising and Explainable Configuration\n  Tuning"
                },
                "summary": "The high configurability of modern software systems has made configuration\ntuning a crucial step for assuring system performance, e.g., latency or\nthroughput. However, given the expensive measurements, large configuration\nspace, and rugged configuration landscape, existing tuners suffer\nineffectiveness due to the difficult balance of budget utilization between\nexploring uncertain regions (for escaping from local optima) and exploiting\nguidance of known good configurations (for fast convergence). The root cause is\nthat we lack knowledge of where the promising regions lay, which also causes\nchallenges in the explainability of the results.\n  In this paper, we propose PromiseTune that tunes configuration guided by\ncausally purified rules. PromiseTune is unique in the sense that we learn\nrules, which reflect certain regions in the configuration landscape, and purify\nthem with causal inference. The remaining rules serve as approximated\nreflections of the promising regions, bounding the tuning to emphasize these\nplaces in the landscape. This, as we demonstrate, can effectively mitigate the\nimpact of the exploration and exploitation trade-off. Those purified regions\ncan then be paired with the measured configurations to provide spatial\nexplainability at the landscape level. Comparing with 11 state-of-the-art\ntuners on 12 systems and varying budgets, we show that PromiseTune performs\nsignificantly better than the others with $42\\%$ superior rank to the overall\nsecond best while providing richer information to explain the hidden system\ncharacteristics.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The high configurability of modern software systems has made configuration\ntuning a crucial step for assuring system performance, e.g., latency or\nthroughput. However, given the expensive measurements, large configuration\nspace, and rugged configuration landscape, existing tuners suffer\nineffectiveness due to the difficult balance of budget utilization between\nexploring uncertain regions (for escaping from local optima) and exploiting\nguidance of known good configurations (for fast convergence). The root cause is\nthat we lack knowledge of where the promising regions lay, which also causes\nchallenges in the explainability of the results.\n  In this paper, we propose PromiseTune that tunes configuration guided by\ncausally purified rules. PromiseTune is unique in the sense that we learn\nrules, which reflect certain regions in the configuration landscape, and purify\nthem with causal inference. The remaining rules serve as approximated\nreflections of the promising regions, bounding the tuning to emphasize these\nplaces in the landscape. This, as we demonstrate, can effectively mitigate the\nimpact of the exploration and exploitation trade-off. Those purified regions\ncan then be paired with the measured configurations to provide spatial\nexplainability at the landscape level. Comparing with 11 state-of-the-art\ntuners on 12 systems and varying budgets, we show that PromiseTune performs\nsignificantly better than the others with $42\\%$ superior rank to the overall\nsecond best while providing richer information to explain the hidden system\ncharacteristics."
                },
                "authors": [
                    {
                        "name": "Pengzhou Chen"
                    },
                    {
                        "name": "Tao Chen"
                    }
                ],
                "author_detail": {
                    "name": "Tao Chen"
                },
                "author": "Tao Chen",
                "arxiv_comment": "This paper has been accepted by ICSE26",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.05995v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.05995v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "68Nxx",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "D.2.0; D.2.8",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.05991v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.05991v1",
                "updated": "2025-07-08T13:52:45Z",
                "updated_parsed": [
                    2025,
                    7,
                    8,
                    13,
                    52,
                    45,
                    1,
                    189,
                    0
                ],
                "published": "2025-07-08T13:52:45Z",
                "published_parsed": [
                    2025,
                    7,
                    8,
                    13,
                    52,
                    45,
                    1,
                    189,
                    0
                ],
                "title": "Evolution without Large Models: Training Language Model with Task\n  Principles",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Evolution without Large Models: Training Language Model with Task\n  Principles"
                },
                "summary": "A common training approach for language models involves using a large-scale\nlanguage model to expand a human-provided dataset, which is subsequently used\nfor model training.This method significantly reduces training costs by\neliminating the need for extensive human data annotation. However, it still\nfaces challenges such as high carbon emissions during data augmentation and the\nrisk of data leakage when we use closed-source LLMs. To address these issues,\nwe propose a self-evolution method for language models. First, we introduce the\nMulti-level Principle Generation, which enables a large-scale model to\nsummarize task-completion principles based on a small amount of task data.\nThen, we propose the Principle-based Instance Generation, in which a\nsmaller-scale language model uses these task principles to generate a large\namount of data. This data is then used for model training. Experimental results\nshow that our proposed method significantly improves model performance compared\nto directly using a smaller-scale language model to generate data.\nAdditionally, since we only use the large-scale language model to generate the\ntask-completion principles, the carbon emissions associated with training the\nmodel are greatly reduced.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A common training approach for language models involves using a large-scale\nlanguage model to expand a human-provided dataset, which is subsequently used\nfor model training.This method significantly reduces training costs by\neliminating the need for extensive human data annotation. However, it still\nfaces challenges such as high carbon emissions during data augmentation and the\nrisk of data leakage when we use closed-source LLMs. To address these issues,\nwe propose a self-evolution method for language models. First, we introduce the\nMulti-level Principle Generation, which enables a large-scale model to\nsummarize task-completion principles based on a small amount of task data.\nThen, we propose the Principle-based Instance Generation, in which a\nsmaller-scale language model uses these task principles to generate a large\namount of data. This data is then used for model training. Experimental results\nshow that our proposed method significantly improves model performance compared\nto directly using a smaller-scale language model to generate data.\nAdditionally, since we only use the large-scale language model to generate the\ntask-completion principles, the carbon emissions associated with training the\nmodel are greatly reduced."
                },
                "authors": [
                    {
                        "name": "Minghang Zhu"
                    },
                    {
                        "name": "Shen Gao"
                    },
                    {
                        "name": "Zhengliang Shi"
                    },
                    {
                        "name": "Jiabao Fang"
                    },
                    {
                        "name": "Pengjie Ren"
                    },
                    {
                        "name": "Zhaochun Ren"
                    },
                    {
                        "name": "Zhumin Chen"
                    },
                    {
                        "name": "Shuo Shang"
                    }
                ],
                "author_detail": {
                    "name": "Shuo Shang"
                },
                "author": "Shuo Shang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.05991v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.05991v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.08773v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.08773v2",
                "updated": "2025-07-08T13:52:12Z",
                "updated_parsed": [
                    2025,
                    7,
                    8,
                    13,
                    52,
                    12,
                    1,
                    189,
                    0
                ],
                "published": "2025-04-03T14:31:40Z",
                "published_parsed": [
                    2025,
                    4,
                    3,
                    14,
                    31,
                    40,
                    3,
                    93,
                    0
                ],
                "title": "Counterfactual Inference under Thompson Sampling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Counterfactual Inference under Thompson Sampling"
                },
                "summary": "Recommender systems exemplify sequential decision-making under uncertainty,\nstrategically deciding what content to serve to users, to optimise a range of\npotential objectives. To balance the explore-exploit trade-off successfully,\nThompson sampling provides a natural and widespread paradigm to\nprobabilistically select which action to take. Questions of causal and\ncounterfactual inference, which underpin use-cases like offline evaluation, are\nnot straightforward to answer in these contexts. Specifically, whilst most\nexisting estimators rely on action propensities, these are not readily\navailable under Thompson sampling procedures.\n  We derive exact and efficiently computable expressions for action\npropensities under a variety of parameter and outcome distributions, enabling\nthe use of off-policy estimators in Thompson sampling scenarios. This opens up\na range of practical use-cases where counterfactual inference is crucial,\nincluding unbiased offline evaluation of recommender systems, as well as\ngeneral applications of causal inference in online advertising,\npersonalisation, and beyond.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recommender systems exemplify sequential decision-making under uncertainty,\nstrategically deciding what content to serve to users, to optimise a range of\npotential objectives. To balance the explore-exploit trade-off successfully,\nThompson sampling provides a natural and widespread paradigm to\nprobabilistically select which action to take. Questions of causal and\ncounterfactual inference, which underpin use-cases like offline evaluation, are\nnot straightforward to answer in these contexts. Specifically, whilst most\nexisting estimators rely on action propensities, these are not readily\navailable under Thompson sampling procedures.\n  We derive exact and efficiently computable expressions for action\npropensities under a variety of parameter and outcome distributions, enabling\nthe use of off-policy estimators in Thompson sampling scenarios. This opens up\na range of practical use-cases where counterfactual inference is crucial,\nincluding unbiased offline evaluation of recommender systems, as well as\ngeneral applications of causal inference in online advertising,\npersonalisation, and beyond."
                },
                "authors": [
                    {
                        "name": "Olivier Jeunen"
                    }
                ],
                "author_detail": {
                    "name": "Olivier Jeunen"
                },
                "author": "Olivier Jeunen",
                "arxiv_comment": "To appear in the Nineteenth ACM Conference on Recommender Systems\n  (RecSys '25)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.08773v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.08773v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.05990v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.05990v1",
                "updated": "2025-07-08T13:50:05Z",
                "updated_parsed": [
                    2025,
                    7,
                    8,
                    13,
                    50,
                    5,
                    1,
                    189,
                    0
                ],
                "published": "2025-07-08T13:50:05Z",
                "published_parsed": [
                    2025,
                    7,
                    8,
                    13,
                    50,
                    5,
                    1,
                    189,
                    0
                ],
                "title": "Multivariate regression with missing response data for modelling\n  regional DNA methylation QTLs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multivariate regression with missing response data for modelling\n  regional DNA methylation QTLs"
                },
                "summary": "Identifying genetic regulators of DNA methylation (mQTLs) with multivariate\nmodels enhances statistical power, but is challenged by missing data from\nbisulfite sequencing. Standard imputation-based methods can introduce bias,\nlimiting reliable inference. We propose \\texttt{missoNet}, a novel convex\nestimation framework that jointly estimates regression coefficients and the\nprecision matrix from data with missing responses. By using unbiased surrogate\nestimators, our three-stage procedure avoids imputation while simultaneously\nperforming variable selection and learning the conditional dependence structure\namong responses. We establish theoretical error bounds, and our simulations\ndemonstrate that \\texttt{missoNet} consistently outperforms existing methods in\nboth prediction and sparsity recovery. In a real-world mQTL analysis of the\nCARTaGENE cohort, \\texttt{missoNet} achieved superior predictive accuracy and\nfalse-discovery control on a held-out validation set, identifying known and\ncredible novel genetic associations. The method offers a robust, efficient, and\ntheoretically grounded tool for genomic analyses, and is available as an R\npackage.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Identifying genetic regulators of DNA methylation (mQTLs) with multivariate\nmodels enhances statistical power, but is challenged by missing data from\nbisulfite sequencing. Standard imputation-based methods can introduce bias,\nlimiting reliable inference. We propose \\texttt{missoNet}, a novel convex\nestimation framework that jointly estimates regression coefficients and the\nprecision matrix from data with missing responses. By using unbiased surrogate\nestimators, our three-stage procedure avoids imputation while simultaneously\nperforming variable selection and learning the conditional dependence structure\namong responses. We establish theoretical error bounds, and our simulations\ndemonstrate that \\texttt{missoNet} consistently outperforms existing methods in\nboth prediction and sparsity recovery. In a real-world mQTL analysis of the\nCARTaGENE cohort, \\texttt{missoNet} achieved superior predictive accuracy and\nfalse-discovery control on a held-out validation set, identifying known and\ncredible novel genetic associations. The method offers a robust, efficient, and\ntheoretically grounded tool for genomic analyses, and is available as an R\npackage."
                },
                "authors": [
                    {
                        "name": "Shomoita Alam"
                    },
                    {
                        "name": "Yixiao Zeng"
                    },
                    {
                        "name": "Sasha Bernatsky"
                    },
                    {
                        "name": "Marie Hudson"
                    },
                    {
                        "name": "InÃ©s Colmegna"
                    },
                    {
                        "name": "David A. Stephens"
                    },
                    {
                        "name": "Celia M. T. Greenwood"
                    },
                    {
                        "name": "Archer Y. Yang"
                    }
                ],
                "author_detail": {
                    "name": "Archer Y. Yang"
                },
                "author": "Archer Y. Yang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.05990v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.05990v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ME",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.05984v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.05984v1",
                "updated": "2025-07-08T13:41:22Z",
                "updated_parsed": [
                    2025,
                    7,
                    8,
                    13,
                    41,
                    22,
                    1,
                    189,
                    0
                ],
                "published": "2025-07-08T13:41:22Z",
                "published_parsed": [
                    2025,
                    7,
                    8,
                    13,
                    41,
                    22,
                    1,
                    189,
                    0
                ],
                "title": "Development and Evaluation of HopeBot: an LLM-based chatbot for\n  structured and interactive PHQ-9 depression screening",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Development and Evaluation of HopeBot: an LLM-based chatbot for\n  structured and interactive PHQ-9 depression screening"
                },
                "summary": "Static tools like the Patient Health Questionnaire-9 (PHQ-9) effectively\nscreen depression but lack interactivity and adaptability. We developed\nHopeBot, a chatbot powered by a large language model (LLM) that administers the\nPHQ-9 using retrieval-augmented generation and real-time clarification. In a\nwithin-subject study, 132 adults in the United Kingdom and China completed both\nself-administered and chatbot versions. Scores demonstrated strong agreement\n(ICC = 0.91; 45% identical). Among 75 participants providing comparative\nfeedback, 71% reported greater trust in the chatbot, highlighting clearer\nstructure, interpretive guidance, and a supportive tone. Mean ratings (0-10)\nwere 8.4 for comfort, 7.7 for voice clarity, 7.6 for handling sensitive topics,\nand 7.4 for recommendation helpfulness; the latter varied significantly by\nemployment status and prior mental-health service use (p < 0.05). Overall,\n87.1% expressed willingness to reuse or recommend HopeBot. These findings\ndemonstrate voice-based LLM chatbots can feasibly serve as scalable, low-burden\nadjuncts for routine depression screening.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Static tools like the Patient Health Questionnaire-9 (PHQ-9) effectively\nscreen depression but lack interactivity and adaptability. We developed\nHopeBot, a chatbot powered by a large language model (LLM) that administers the\nPHQ-9 using retrieval-augmented generation and real-time clarification. In a\nwithin-subject study, 132 adults in the United Kingdom and China completed both\nself-administered and chatbot versions. Scores demonstrated strong agreement\n(ICC = 0.91; 45% identical). Among 75 participants providing comparative\nfeedback, 71% reported greater trust in the chatbot, highlighting clearer\nstructure, interpretive guidance, and a supportive tone. Mean ratings (0-10)\nwere 8.4 for comfort, 7.7 for voice clarity, 7.6 for handling sensitive topics,\nand 7.4 for recommendation helpfulness; the latter varied significantly by\nemployment status and prior mental-health service use (p < 0.05). Overall,\n87.1% expressed willingness to reuse or recommend HopeBot. These findings\ndemonstrate voice-based LLM chatbots can feasibly serve as scalable, low-burden\nadjuncts for routine depression screening."
                },
                "authors": [
                    {
                        "name": "Zhijun Guo"
                    },
                    {
                        "name": "Alvina Lai"
                    },
                    {
                        "name": "Julia Ive"
                    },
                    {
                        "name": "Alexandru Petcu"
                    },
                    {
                        "name": "Yutong Wang"
                    },
                    {
                        "name": "Luyuan Qi"
                    },
                    {
                        "name": "Johan H Thygesen"
                    },
                    {
                        "name": "Kezhi Li"
                    }
                ],
                "author_detail": {
                    "name": "Kezhi Li"
                },
                "author": "Kezhi Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.05984v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.05984v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.05981v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.05981v1",
                "updated": "2025-07-08T13:37:59Z",
                "updated_parsed": [
                    2025,
                    7,
                    8,
                    13,
                    37,
                    59,
                    1,
                    189,
                    0
                ],
                "published": "2025-07-08T13:37:59Z",
                "published_parsed": [
                    2025,
                    7,
                    8,
                    13,
                    37,
                    59,
                    1,
                    189,
                    0
                ],
                "title": "Multi-Agent Debate Strategies to Enhance Requirements Engineering with\n  Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-Agent Debate Strategies to Enhance Requirements Engineering with\n  Large Language Models"
                },
                "summary": "Context: Large Language Model (LLM) agents are becoming widely used for\nvarious Requirements Engineering (RE) tasks. Research on improving their\naccuracy mainly focuses on prompt engineering, model fine-tuning, and retrieval\naugmented generation. However, these methods often treat models as isolated\nblack boxes - relying on single-pass outputs without iterative refinement or\ncollaboration, limiting robustness and adaptability. Objective: We propose\nthat, just as human debates enhance accuracy and reduce bias in RE tasks by\nincorporating diverse perspectives, different LLM agents debating and\ncollaborating may achieve similar improvements. Our goal is to investigate\nwhether Multi-Agent Debate (MAD) strategies can enhance RE performance. Method:\nWe conducted a systematic study of existing MAD strategies across various\ndomains to identify their key characteristics. To assess their applicability in\nRE, we implemented and tested a preliminary MAD-based framework for RE\nclassification. Results: Our study identified and categorized several MAD\nstrategies, leading to a taxonomy outlining their core attributes. Our\npreliminary evaluation demonstrated the feasibility of applying MAD to RE\nclassification. Conclusions: MAD presents a promising approach for improving\nLLM accuracy in RE tasks. This study provides a foundational understanding of\nMAD strategies, offering insights for future research and refinements in RE\napplications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Context: Large Language Model (LLM) agents are becoming widely used for\nvarious Requirements Engineering (RE) tasks. Research on improving their\naccuracy mainly focuses on prompt engineering, model fine-tuning, and retrieval\naugmented generation. However, these methods often treat models as isolated\nblack boxes - relying on single-pass outputs without iterative refinement or\ncollaboration, limiting robustness and adaptability. Objective: We propose\nthat, just as human debates enhance accuracy and reduce bias in RE tasks by\nincorporating diverse perspectives, different LLM agents debating and\ncollaborating may achieve similar improvements. Our goal is to investigate\nwhether Multi-Agent Debate (MAD) strategies can enhance RE performance. Method:\nWe conducted a systematic study of existing MAD strategies across various\ndomains to identify their key characteristics. To assess their applicability in\nRE, we implemented and tested a preliminary MAD-based framework for RE\nclassification. Results: Our study identified and categorized several MAD\nstrategies, leading to a taxonomy outlining their core attributes. Our\npreliminary evaluation demonstrated the feasibility of applying MAD to RE\nclassification. Conclusions: MAD presents a promising approach for improving\nLLM accuracy in RE tasks. This study provides a foundational understanding of\nMAD strategies, offering insights for future research and refinements in RE\napplications."
                },
                "authors": [
                    {
                        "name": "Marc Oriol"
                    },
                    {
                        "name": "Quim Motger"
                    },
                    {
                        "name": "Jordi Marco"
                    },
                    {
                        "name": "Xavier Franch"
                    }
                ],
                "author_detail": {
                    "name": "Xavier Franch"
                },
                "author": "Xavier Franch",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.05981v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.05981v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.05980v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.05980v1",
                "updated": "2025-07-08T13:37:25Z",
                "updated_parsed": [
                    2025,
                    7,
                    8,
                    13,
                    37,
                    25,
                    1,
                    189,
                    0
                ],
                "published": "2025-07-08T13:37:25Z",
                "published_parsed": [
                    2025,
                    7,
                    8,
                    13,
                    37,
                    25,
                    1,
                    189,
                    0
                ],
                "title": "RabakBench: Scaling Human Annotations to Construct Localized\n  Multilingual Safety Benchmarks for Low-Resource Languages",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RabakBench: Scaling Human Annotations to Construct Localized\n  Multilingual Safety Benchmarks for Low-Resource Languages"
                },
                "summary": "Large language models (LLMs) and their safety classifiers often perform\npoorly on low-resource languages due to limited training data and evaluation\nbenchmarks. This paper introduces RabakBench, a new multilingual safety\nbenchmark localized to Singapore's unique linguistic context, covering\nSinglish, Chinese, Malay, and Tamil. RabakBench is constructed through a\nscalable three-stage pipeline: (i) Generate - adversarial example generation by\naugmenting real Singlish web content with LLM-driven red teaming; (ii) Label -\nsemi-automated multi-label safety annotation using majority-voted LLM labelers\naligned with human judgments; and (iii) Translate - high-fidelity translation\npreserving linguistic nuance and toxicity across languages. The final dataset\ncomprises over 5,000 safety-labeled examples across four languages and six\nfine-grained safety categories with severity levels. Evaluations of 11 popular\nopen-source and closed-source guardrail classifiers reveal significant\nperformance degradation. RabakBench not only enables robust safety evaluation\nin Southeast Asian multilingual settings but also offers a reproducible\nframework for building localized safety datasets in low-resource environments.\nThe benchmark dataset, including the human-verified translations, and\nevaluation code are publicly available.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) and their safety classifiers often perform\npoorly on low-resource languages due to limited training data and evaluation\nbenchmarks. This paper introduces RabakBench, a new multilingual safety\nbenchmark localized to Singapore's unique linguistic context, covering\nSinglish, Chinese, Malay, and Tamil. RabakBench is constructed through a\nscalable three-stage pipeline: (i) Generate - adversarial example generation by\naugmenting real Singlish web content with LLM-driven red teaming; (ii) Label -\nsemi-automated multi-label safety annotation using majority-voted LLM labelers\naligned with human judgments; and (iii) Translate - high-fidelity translation\npreserving linguistic nuance and toxicity across languages. The final dataset\ncomprises over 5,000 safety-labeled examples across four languages and six\nfine-grained safety categories with severity levels. Evaluations of 11 popular\nopen-source and closed-source guardrail classifiers reveal significant\nperformance degradation. RabakBench not only enables robust safety evaluation\nin Southeast Asian multilingual settings but also offers a reproducible\nframework for building localized safety datasets in low-resource environments.\nThe benchmark dataset, including the human-verified translations, and\nevaluation code are publicly available."
                },
                "authors": [
                    {
                        "name": "Gabriel Chua"
                    },
                    {
                        "name": "Leanne Tan"
                    },
                    {
                        "name": "Ziyu Ge"
                    },
                    {
                        "name": "Roy Ka-Wei Lee"
                    }
                ],
                "author_detail": {
                    "name": "Roy Ka-Wei Lee"
                },
                "author": "Roy Ka-Wei Lee",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.05980v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.05980v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.11286v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.11286v2",
                "updated": "2025-07-08T13:25:18Z",
                "updated_parsed": [
                    2025,
                    7,
                    8,
                    13,
                    25,
                    18,
                    1,
                    189,
                    0
                ],
                "published": "2025-04-15T15:26:28Z",
                "published_parsed": [
                    2025,
                    4,
                    15,
                    15,
                    26,
                    28,
                    1,
                    105,
                    0
                ],
                "title": "Lightweight Medical Image Restoration via Integrating Reliable\n  Lesion-Semantic Driven Prior",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Lightweight Medical Image Restoration via Integrating Reliable\n  Lesion-Semantic Driven Prior"
                },
                "summary": "Medical image restoration tasks aim to recover high-quality images from\ndegraded observations, exhibiting emergent desires in many clinical scenarios,\nsuch as low-dose CT image denoising, MRI super-resolution, and MRI artifact\nremoval. Despite the success achieved by existing deep learning-based\nrestoration methods with sophisticated modules, they struggle with rendering\ncomputationally-efficient reconstruction results. Moreover, they usually ignore\nthe reliability of the restoration results, which is much more urgent in\nmedical systems. To alleviate these issues, we present LRformer, a Lightweight\nTransformer-based method via Reliability-guided learning in the frequency\ndomain. Specifically, inspired by the uncertainty quantification in Bayesian\nneural networks (BNNs), we develop a Reliable Lesion-Semantic Prior Producer\n(RLPP). RLPP leverages Monte Carlo (MC) estimators with stochastic sampling\noperations to generate sufficiently-reliable priors by performing multiple\ninferences on the foundational medical image segmentation model, MedSAM.\nAdditionally, instead of directly incorporating the priors in the spatial\ndomain, we decompose the cross-attention (CA) mechanism into real symmetric and\nimaginary anti-symmetric parts via fast Fourier transform (FFT), resulting in\nthe design of the Guided Frequency Cross-Attention (GFCA) solver. By leveraging\nthe conjugated symmetric property of FFT, GFCA reduces the computational\ncomplexity of naive CA by nearly half. Extensive experimental results in\nvarious tasks demonstrate the superiority of the proposed LRformer in both\neffectiveness and efficiency.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Medical image restoration tasks aim to recover high-quality images from\ndegraded observations, exhibiting emergent desires in many clinical scenarios,\nsuch as low-dose CT image denoising, MRI super-resolution, and MRI artifact\nremoval. Despite the success achieved by existing deep learning-based\nrestoration methods with sophisticated modules, they struggle with rendering\ncomputationally-efficient reconstruction results. Moreover, they usually ignore\nthe reliability of the restoration results, which is much more urgent in\nmedical systems. To alleviate these issues, we present LRformer, a Lightweight\nTransformer-based method via Reliability-guided learning in the frequency\ndomain. Specifically, inspired by the uncertainty quantification in Bayesian\nneural networks (BNNs), we develop a Reliable Lesion-Semantic Prior Producer\n(RLPP). RLPP leverages Monte Carlo (MC) estimators with stochastic sampling\noperations to generate sufficiently-reliable priors by performing multiple\ninferences on the foundational medical image segmentation model, MedSAM.\nAdditionally, instead of directly incorporating the priors in the spatial\ndomain, we decompose the cross-attention (CA) mechanism into real symmetric and\nimaginary anti-symmetric parts via fast Fourier transform (FFT), resulting in\nthe design of the Guided Frequency Cross-Attention (GFCA) solver. By leveraging\nthe conjugated symmetric property of FFT, GFCA reduces the computational\ncomplexity of naive CA by nearly half. Extensive experimental results in\nvarious tasks demonstrate the superiority of the proposed LRformer in both\neffectiveness and efficiency."
                },
                "authors": [
                    {
                        "name": "Pengcheng Zheng"
                    },
                    {
                        "name": "Kecheng Chen"
                    },
                    {
                        "name": "Jiaxin Huang"
                    },
                    {
                        "name": "Bohao Chen"
                    },
                    {
                        "name": "Ju Liu"
                    },
                    {
                        "name": "Yazhou Ren"
                    },
                    {
                        "name": "Xiaorong Pu"
                    }
                ],
                "author_detail": {
                    "name": "Xiaorong Pu"
                },
                "author": "Xiaorong Pu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.11286v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.11286v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.IV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.IV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.05970v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.05970v1",
                "updated": "2025-07-08T13:24:05Z",
                "updated_parsed": [
                    2025,
                    7,
                    8,
                    13,
                    24,
                    5,
                    1,
                    189,
                    0
                ],
                "published": "2025-07-08T13:24:05Z",
                "published_parsed": [
                    2025,
                    7,
                    8,
                    13,
                    24,
                    5,
                    1,
                    189,
                    0
                ],
                "title": "Automatic Synthesis of High-Quality Triplet Data for Composed Image\n  Retrieval",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Automatic Synthesis of High-Quality Triplet Data for Composed Image\n  Retrieval"
                },
                "summary": "As a challenging vision-language (VL) task, Composed Image Retrieval (CIR)\naims to retrieve target images using multimodal (image+text) queries. Although\nmany existing CIR methods have attained promising performance, their reliance\non costly, manually labeled triplets hinders scalability and zero-shot\ncapability. To address this issue, we propose a scalable pipeline for automatic\ntriplet generation, along with a fully synthetic dataset named Composed Image\nRetrieval on High-quality Synthetic Triplets (CIRHS). Our pipeline leverages a\nlarge language model (LLM) to generate diverse prompts, controlling a\ntext-to-image generative model to produce image pairs with identical elements\nin each pair, which are then filtered and reorganized to form the CIRHS\ndataset. In addition, we introduce Hybrid Contextual Alignment (CoAlign), a\nnovel CIR framework, which can accomplish global alignment and local reasoning\nwithin a broader context, enabling the model to learn more robust and\ninformative representations. By utilizing the synthetic CIRHS dataset, CoAlign\nachieves outstanding zero-shot performance on three commonly used benchmarks,\ndemonstrating for the first time the feasibility of training CIR models on a\nfully synthetic dataset. Furthermore, under supervised training, our method\noutperforms all the state-of-the-art supervised CIR approaches, validating the\neffectiveness of our proposed retrieval framework. The code and the CIRHS\ndataset will be released soon.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As a challenging vision-language (VL) task, Composed Image Retrieval (CIR)\naims to retrieve target images using multimodal (image+text) queries. Although\nmany existing CIR methods have attained promising performance, their reliance\non costly, manually labeled triplets hinders scalability and zero-shot\ncapability. To address this issue, we propose a scalable pipeline for automatic\ntriplet generation, along with a fully synthetic dataset named Composed Image\nRetrieval on High-quality Synthetic Triplets (CIRHS). Our pipeline leverages a\nlarge language model (LLM) to generate diverse prompts, controlling a\ntext-to-image generative model to produce image pairs with identical elements\nin each pair, which are then filtered and reorganized to form the CIRHS\ndataset. In addition, we introduce Hybrid Contextual Alignment (CoAlign), a\nnovel CIR framework, which can accomplish global alignment and local reasoning\nwithin a broader context, enabling the model to learn more robust and\ninformative representations. By utilizing the synthetic CIRHS dataset, CoAlign\nachieves outstanding zero-shot performance on three commonly used benchmarks,\ndemonstrating for the first time the feasibility of training CIR models on a\nfully synthetic dataset. Furthermore, under supervised training, our method\noutperforms all the state-of-the-art supervised CIR approaches, validating the\neffectiveness of our proposed retrieval framework. The code and the CIRHS\ndataset will be released soon."
                },
                "authors": [
                    {
                        "name": "Haiwen Li"
                    },
                    {
                        "name": "Delong Liu"
                    },
                    {
                        "name": "Zhaohui Hou"
                    },
                    {
                        "name": "Zhicheng Zhao"
                    },
                    {
                        "name": "Fei Su"
                    }
                ],
                "author_detail": {
                    "name": "Fei Su"
                },
                "author": "Fei Su",
                "arxiv_comment": "This paper was originally submitted to ACM MM 2025 on April 12, 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.05970v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.05970v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.13575v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.13575v2",
                "updated": "2025-07-08T13:20:21Z",
                "updated_parsed": [
                    2025,
                    7,
                    8,
                    13,
                    20,
                    21,
                    1,
                    189,
                    0
                ],
                "published": "2025-03-17T13:40:46Z",
                "published_parsed": [
                    2025,
                    3,
                    17,
                    13,
                    40,
                    46,
                    0,
                    76,
                    0
                ],
                "title": "Analytic Subspace Routing: How Recursive Least Squares Works in\n  Continual Learning of Large Language Model",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Analytic Subspace Routing: How Recursive Least Squares Works in\n  Continual Learning of Large Language Model"
                },
                "summary": "Large Language Models (LLMs) possess encompassing capabilities that can\nprocess diverse language-related tasks. However, finetuning on LLMs will\ndiminish this general skills and continual finetuning will further cause severe\ndegradation on accumulated knowledge. Recently, Continual Learning (CL) in\nLarge Language Models (LLMs) arises which aims to continually adapt the LLMs to\nnew tasks while maintaining previously learned knowledge and inheriting general\nskills. Existing techniques either leverage previous data to replay, leading to\nextra computational costs, or utilize a single parameter-efficient module to\nlearn the downstream task, constraining new knowledge absorption with\ninterference between different tasks. Toward these issues, this paper proposes\nAnalytic Subspace Routing(ASR) to address these challenges. For each task, we\nisolate the learning within a subspace of deep layers' features via low-rank\nadaptation, eliminating knowledge interference between different tasks.\nAdditionally, we propose an analytic routing mechanism to properly utilize\nknowledge learned in different subspaces. Our approach employs Recursive Least\nSquares to train a multi-task router model, allowing the router to dynamically\nadapt to incoming data without requiring access to historical data. Also, the\nrouter effectively assigns the current task to an appropriate subspace and has\na non-forgetting property of previously learned tasks with a solid theoretical\nguarantee. Experimental results demonstrate that our method achieves\nnear-perfect retention of prior knowledge while seamlessly integrating new\ninformation, effectively overcoming the core limitations of existing methods.\nOur code will be released after acceptance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) possess encompassing capabilities that can\nprocess diverse language-related tasks. However, finetuning on LLMs will\ndiminish this general skills and continual finetuning will further cause severe\ndegradation on accumulated knowledge. Recently, Continual Learning (CL) in\nLarge Language Models (LLMs) arises which aims to continually adapt the LLMs to\nnew tasks while maintaining previously learned knowledge and inheriting general\nskills. Existing techniques either leverage previous data to replay, leading to\nextra computational costs, or utilize a single parameter-efficient module to\nlearn the downstream task, constraining new knowledge absorption with\ninterference between different tasks. Toward these issues, this paper proposes\nAnalytic Subspace Routing(ASR) to address these challenges. For each task, we\nisolate the learning within a subspace of deep layers' features via low-rank\nadaptation, eliminating knowledge interference between different tasks.\nAdditionally, we propose an analytic routing mechanism to properly utilize\nknowledge learned in different subspaces. Our approach employs Recursive Least\nSquares to train a multi-task router model, allowing the router to dynamically\nadapt to incoming data without requiring access to historical data. Also, the\nrouter effectively assigns the current task to an appropriate subspace and has\na non-forgetting property of previously learned tasks with a solid theoretical\nguarantee. Experimental results demonstrate that our method achieves\nnear-perfect retention of prior knowledge while seamlessly integrating new\ninformation, effectively overcoming the core limitations of existing methods.\nOur code will be released after acceptance."
                },
                "authors": [
                    {
                        "name": "Kai Tong"
                    },
                    {
                        "name": "Kang Pan"
                    },
                    {
                        "name": "Xiao Zhang"
                    },
                    {
                        "name": "Erli Meng"
                    },
                    {
                        "name": "Run He"
                    },
                    {
                        "name": "Yawen Cui"
                    },
                    {
                        "name": "Nuoyan Guo"
                    },
                    {
                        "name": "Huiping Zhuang"
                    }
                ],
                "author_detail": {
                    "name": "Huiping Zhuang"
                },
                "author": "Huiping Zhuang",
                "arxiv_comment": "11 pages, 4 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.13575v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.13575v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.04649v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.04649v2",
                "updated": "2025-07-08T13:19:25Z",
                "updated_parsed": [
                    2025,
                    7,
                    8,
                    13,
                    19,
                    25,
                    1,
                    189,
                    0
                ],
                "published": "2025-05-06T18:50:02Z",
                "published_parsed": [
                    2025,
                    5,
                    6,
                    18,
                    50,
                    2,
                    1,
                    126,
                    0
                ],
                "title": "FRAME: Feedback-Refined Agent Methodology for Enhancing Medical Research\n  Insights",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FRAME: Feedback-Refined Agent Methodology for Enhancing Medical Research\n  Insights"
                },
                "summary": "The automation of scientific research through large language models (LLMs)\npresents significant opportunities but faces critical challenges in knowledge\nsynthesis and quality assurance. We introduce Feedback-Refined Agent\nMethodology (FRAME), a novel framework that enhances medical paper generation\nthrough iterative refinement and structured feedback. Our approach comprises\nthree key innovations: (1) A structured dataset construction method that\ndecomposes 4,287 medical papers into essential research components through\niterative refinement; (2) A tripartite architecture integrating Generator,\nEvaluator, and Reflector agents that progressively improve content quality\nthrough metric-driven feedback; and (3) A comprehensive evaluation framework\nthat combines statistical metrics with human-grounded benchmarks. Experimental\nresults demonstrate FRAME's effectiveness, achieving significant improvements\nover conventional approaches across multiple models (9.91% average gain with\nDeepSeek V3, comparable improvements with GPT-4o Mini) and evaluation\ndimensions. Human evaluation confirms that FRAME-generated papers achieve\nquality comparable to human-authored works, with particular strength in\nsynthesizing future research directions. The results demonstrated our work\ncould efficiently assist medical research by building a robust foundation for\nautomated medical research paper generation while maintaining rigorous academic\nstandards.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The automation of scientific research through large language models (LLMs)\npresents significant opportunities but faces critical challenges in knowledge\nsynthesis and quality assurance. We introduce Feedback-Refined Agent\nMethodology (FRAME), a novel framework that enhances medical paper generation\nthrough iterative refinement and structured feedback. Our approach comprises\nthree key innovations: (1) A structured dataset construction method that\ndecomposes 4,287 medical papers into essential research components through\niterative refinement; (2) A tripartite architecture integrating Generator,\nEvaluator, and Reflector agents that progressively improve content quality\nthrough metric-driven feedback; and (3) A comprehensive evaluation framework\nthat combines statistical metrics with human-grounded benchmarks. Experimental\nresults demonstrate FRAME's effectiveness, achieving significant improvements\nover conventional approaches across multiple models (9.91% average gain with\nDeepSeek V3, comparable improvements with GPT-4o Mini) and evaluation\ndimensions. Human evaluation confirms that FRAME-generated papers achieve\nquality comparable to human-authored works, with particular strength in\nsynthesizing future research directions. The results demonstrated our work\ncould efficiently assist medical research by building a robust foundation for\nautomated medical research paper generation while maintaining rigorous academic\nstandards."
                },
                "authors": [
                    {
                        "name": "Chengzhang Yu"
                    },
                    {
                        "name": "Yiming Zhang"
                    },
                    {
                        "name": "Zhixin Liu"
                    },
                    {
                        "name": "Zenghui Ding"
                    },
                    {
                        "name": "Yining Sun"
                    },
                    {
                        "name": "Zhanpeng Jin"
                    }
                ],
                "author_detail": {
                    "name": "Zhanpeng Jin"
                },
                "author": "Zhanpeng Jin",
                "arxiv_comment": "12 pages, 4 figures, 5 table",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.04649v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.04649v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.05965v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.05965v1",
                "updated": "2025-07-08T13:19:00Z",
                "updated_parsed": [
                    2025,
                    7,
                    8,
                    13,
                    19,
                    0,
                    1,
                    189,
                    0
                ],
                "published": "2025-07-08T13:19:00Z",
                "published_parsed": [
                    2025,
                    7,
                    8,
                    13,
                    19,
                    0,
                    1,
                    189,
                    0
                ],
                "title": "OpenFActScore: Open-Source Atomic Evaluation of Factuality in Text\n  Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "OpenFActScore: Open-Source Atomic Evaluation of Factuality in Text\n  Generation"
                },
                "summary": "We introduce OpenFActScore, an open-source implementation of the FActScore\nframework for evaluating the factuality of text generated by large language\nmodels (LLMs). FActScore evaluates the factual accuracy of long-form text by\nusing Atomic Fact Generation (AFG) to extract individual factual claims and\nAtomic Fact Validation (AFV) to verify each claim against a trusted knowledge\nsource. While the original FActScore relies on closed-source and commercial\nmodels such as InstructGPT and ChatGPT, OpenFActScore enables the use of any\nHugging Face-compatible model for both AFG and AFV. We provide a detailed\ntechnical overview of our implementation, highlighting design choices and\nmodifications made to support open models. We evaluate multiple open-source\nLLMs on both AFG and AFV using the original FActScore benchmark, reporting\nBERTScore-F1 for AFG and Error Rate relative to human annotations for AFV. Our\nresults show that open models can approximate the performance of closed-source\nsystems, with Gemma achieving the best overall performance, and our final setup\nobtains a 0.99 Pearson correlation with the original FActScore experiments.\nOpenFActScore promotes transparency, reproducibility, and cost-effective\nevaluation, and is available at: https://github.com/lflage/OpenFActScore.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce OpenFActScore, an open-source implementation of the FActScore\nframework for evaluating the factuality of text generated by large language\nmodels (LLMs). FActScore evaluates the factual accuracy of long-form text by\nusing Atomic Fact Generation (AFG) to extract individual factual claims and\nAtomic Fact Validation (AFV) to verify each claim against a trusted knowledge\nsource. While the original FActScore relies on closed-source and commercial\nmodels such as InstructGPT and ChatGPT, OpenFActScore enables the use of any\nHugging Face-compatible model for both AFG and AFV. We provide a detailed\ntechnical overview of our implementation, highlighting design choices and\nmodifications made to support open models. We evaluate multiple open-source\nLLMs on both AFG and AFV using the original FActScore benchmark, reporting\nBERTScore-F1 for AFG and Error Rate relative to human annotations for AFV. Our\nresults show that open models can approximate the performance of closed-source\nsystems, with Gemma achieving the best overall performance, and our final setup\nobtains a 0.99 Pearson correlation with the original FActScore experiments.\nOpenFActScore promotes transparency, reproducibility, and cost-effective\nevaluation, and is available at: https://github.com/lflage/OpenFActScore."
                },
                "authors": [
                    {
                        "name": "Lucas Fonseca Lage"
                    },
                    {
                        "name": "Simon Ostermann"
                    }
                ],
                "author_detail": {
                    "name": "Simon Ostermann"
                },
                "author": "Simon Ostermann",
                "arxiv_comment": "Submitted to EMNLP 2025 System Demonstrations track",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.05965v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.05965v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.11459v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.11459v2",
                "updated": "2025-07-08T13:14:01Z",
                "updated_parsed": [
                    2025,
                    7,
                    8,
                    13,
                    14,
                    1,
                    1,
                    189,
                    0
                ],
                "published": "2024-12-16T05:33:05Z",
                "published_parsed": [
                    2024,
                    12,
                    16,
                    5,
                    33,
                    5,
                    0,
                    351,
                    0
                ],
                "title": "Rethinking Associative Memory Mechanism in Induction Head",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Rethinking Associative Memory Mechanism in Induction Head"
                },
                "summary": "Induction head mechanism is a part of the computational circuits for\nin-context learning (ICL) that enable large language models (LLMs) to adapt to\nnew tasks without fine-tuning. Most existing work explains the training\ndynamics behind acquiring such a powerful mechanism. However, the model's\nability to coordinate in-context information over long contexts and global\nknowledge acquired during pretraining remains poorly understood. This paper\ninvestigates how a two-layer transformer thoroughly captures in-context\ninformation and balances it with pretrained bigram knowledge in next token\nprediction, from the viewpoint of associative memory. We theoretically analyze\nthe representation of weight matrices in attention layers and the resulting\nlogits when a transformer is given prompts generated by a bigram model. In the\nexperiments, we design specific prompts to evaluate whether the outputs of the\ntrained transformer align with the theoretical results.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Induction head mechanism is a part of the computational circuits for\nin-context learning (ICL) that enable large language models (LLMs) to adapt to\nnew tasks without fine-tuning. Most existing work explains the training\ndynamics behind acquiring such a powerful mechanism. However, the model's\nability to coordinate in-context information over long contexts and global\nknowledge acquired during pretraining remains poorly understood. This paper\ninvestigates how a two-layer transformer thoroughly captures in-context\ninformation and balances it with pretrained bigram knowledge in next token\nprediction, from the viewpoint of associative memory. We theoretically analyze\nthe representation of weight matrices in attention layers and the resulting\nlogits when a transformer is given prompts generated by a bigram model. In the\nexperiments, we design specific prompts to evaluate whether the outputs of the\ntrained transformer align with the theoretical results."
                },
                "authors": [
                    {
                        "name": "Shuo Wang"
                    },
                    {
                        "name": "Issei Sato"
                    }
                ],
                "author_detail": {
                    "name": "Issei Sato"
                },
                "author": "Issei Sato",
                "arxiv_comment": "COLM 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.11459v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.11459v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.22968v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.22968v4",
                "updated": "2025-07-08T13:13:04Z",
                "updated_parsed": [
                    2025,
                    7,
                    8,
                    13,
                    13,
                    4,
                    1,
                    189,
                    0
                ],
                "published": "2025-03-29T04:17:58Z",
                "published_parsed": [
                    2025,
                    3,
                    29,
                    4,
                    17,
                    58,
                    5,
                    88,
                    0
                ],
                "title": "Redefining Evaluation Standards: A Unified Framework for Evaluating the\n  Korean Capabilities of Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Redefining Evaluation Standards: A Unified Framework for Evaluating the\n  Korean Capabilities of Language Models"
                },
                "summary": "Recent advancements in Korean large language models (LLMs) have driven\nnumerous benchmarks and evaluation methods, yet inconsistent protocols cause up\nto 10 p.p performance gaps across institutions. Overcoming these\nreproducibility gaps does not mean enforcing a one-size-fits-all evaluation.\nRather, effective benchmarking requires diverse experimental approaches and a\nframework robust enough to support them. To this end, we introduce HRET (Haerae\nEvaluation Toolkit), an open-source, registry-based framework that unifies\nKorean LLM assessment. HRET integrates major Korean benchmarks, multiple\ninference backends, and multi-method evaluation, with language consistency\nenforcement to ensure genuine Korean outputs. Its modular registry design also\nenables rapid incorporation of new datasets, methods, and backends, ensuring\nthe toolkit adapts to evolving research needs. Beyond standard accuracy\nmetrics, HRET incorporates Korean-focused output analyses-morphology-aware\nType-Token Ratio (TTR) for evaluating lexical diversity and systematic\nkeyword-omission detection for identifying missing concepts-to provide\ndiagnostic insights into language-specific behaviors. These targeted analyses\nhelp researchers pinpoint morphological and semantic shortcomings in model\noutputs, guiding focused improvements in Korean LLM development.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advancements in Korean large language models (LLMs) have driven\nnumerous benchmarks and evaluation methods, yet inconsistent protocols cause up\nto 10 p.p performance gaps across institutions. Overcoming these\nreproducibility gaps does not mean enforcing a one-size-fits-all evaluation.\nRather, effective benchmarking requires diverse experimental approaches and a\nframework robust enough to support them. To this end, we introduce HRET (Haerae\nEvaluation Toolkit), an open-source, registry-based framework that unifies\nKorean LLM assessment. HRET integrates major Korean benchmarks, multiple\ninference backends, and multi-method evaluation, with language consistency\nenforcement to ensure genuine Korean outputs. Its modular registry design also\nenables rapid incorporation of new datasets, methods, and backends, ensuring\nthe toolkit adapts to evolving research needs. Beyond standard accuracy\nmetrics, HRET incorporates Korean-focused output analyses-morphology-aware\nType-Token Ratio (TTR) for evaluating lexical diversity and systematic\nkeyword-omission detection for identifying missing concepts-to provide\ndiagnostic insights into language-specific behaviors. These targeted analyses\nhelp researchers pinpoint morphological and semantic shortcomings in model\noutputs, guiding focused improvements in Korean LLM development."
                },
                "authors": [
                    {
                        "name": "Hanwool Lee"
                    },
                    {
                        "name": "Dasol Choi"
                    },
                    {
                        "name": "Sooyong Kim"
                    },
                    {
                        "name": "Ilgyun Jung"
                    },
                    {
                        "name": "Sangwon Baek"
                    },
                    {
                        "name": "Guijin Son"
                    },
                    {
                        "name": "Inseon Hwang"
                    },
                    {
                        "name": "Naeun Lee"
                    },
                    {
                        "name": "Seunghyeok Hong"
                    }
                ],
                "author_detail": {
                    "name": "Seunghyeok Hong"
                },
                "author": "Seunghyeok Hong",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.22968v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.22968v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.05962v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.05962v1",
                "updated": "2025-07-08T13:10:32Z",
                "updated_parsed": [
                    2025,
                    7,
                    8,
                    13,
                    10,
                    32,
                    1,
                    189,
                    0
                ],
                "published": "2025-07-08T13:10:32Z",
                "published_parsed": [
                    2025,
                    7,
                    8,
                    13,
                    10,
                    32,
                    1,
                    189,
                    0
                ],
                "title": "Evaluation of Large Language Model-Driven AutoML in Data and Model\n  Management from Human-Centered Perspective",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Evaluation of Large Language Model-Driven AutoML in Data and Model\n  Management from Human-Centered Perspective"
                },
                "summary": "As organizations increasingly seek to leverage machine learning (ML)\ncapabilities, the technical complexity of implementing ML solutions creates\nsignificant barriers to adoption and impacts operational efficiency. This\nresearch examines how Large Language Models (LLMs) can transform the\naccessibility of ML technologies within organizations through a human-centered\nAutomated Machine Learning (AutoML) approach. Through a comprehensive user\nstudy involving 15 professionals across various roles and technical\nbackgrounds, we evaluate the organizational impact of an LLM-based AutoML\nframework compared to traditional implementation methods. Our research offers\nfour significant contributions to both management practice and technical\ninnovation: First, we present pioneering evidence that LLM-based interfaces can\ndramatically improve ML implementation success rates, with 93.34% of users\nachieved superior performance in the LLM condition, with 46.67% showing higher\naccuracy (10-25% improvement over baseline) and 46.67% demonstrating\nsignificantly higher accuracy (>25% improvement over baseline), while 6.67%\nmaintained comparable performance levels; and 60% reporting substantially\nreduced development time. Second, we demonstrate how natural language\ninterfaces can effectively bridge the technical skills gap in organizations,\ncutting implementation time by 50% while improving accuracy across all\nexpertise levels. Third, we provide valuable insights for organizations\ndesigning human-AI collaborative systems, showing that our approach reduced\nerror resolution time by 73% and significantly accelerated employee learning\ncurves. Finally, we establish empirical support for natural language as an\neffective interface for complex technical systems, offering organizations a\npath to democratize ML capabilities without compromising quality or\nperformance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As organizations increasingly seek to leverage machine learning (ML)\ncapabilities, the technical complexity of implementing ML solutions creates\nsignificant barriers to adoption and impacts operational efficiency. This\nresearch examines how Large Language Models (LLMs) can transform the\naccessibility of ML technologies within organizations through a human-centered\nAutomated Machine Learning (AutoML) approach. Through a comprehensive user\nstudy involving 15 professionals across various roles and technical\nbackgrounds, we evaluate the organizational impact of an LLM-based AutoML\nframework compared to traditional implementation methods. Our research offers\nfour significant contributions to both management practice and technical\ninnovation: First, we present pioneering evidence that LLM-based interfaces can\ndramatically improve ML implementation success rates, with 93.34% of users\nachieved superior performance in the LLM condition, with 46.67% showing higher\naccuracy (10-25% improvement over baseline) and 46.67% demonstrating\nsignificantly higher accuracy (>25% improvement over baseline), while 6.67%\nmaintained comparable performance levels; and 60% reporting substantially\nreduced development time. Second, we demonstrate how natural language\ninterfaces can effectively bridge the technical skills gap in organizations,\ncutting implementation time by 50% while improving accuracy across all\nexpertise levels. Third, we provide valuable insights for organizations\ndesigning human-AI collaborative systems, showing that our approach reduced\nerror resolution time by 73% and significantly accelerated employee learning\ncurves. Finally, we establish empirical support for natural language as an\neffective interface for complex technical systems, offering organizations a\npath to democratize ML capabilities without compromising quality or\nperformance."
                },
                "authors": [
                    {
                        "name": "Jiapeng Yao"
                    },
                    {
                        "name": "Lantian Zhang"
                    },
                    {
                        "name": "Jiping Huang"
                    }
                ],
                "author_detail": {
                    "name": "Jiping Huang"
                },
                "author": "Jiping Huang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.05962v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.05962v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2106.15074v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2106.15074v4",
                "updated": "2025-07-08T12:49:06Z",
                "updated_parsed": [
                    2025,
                    7,
                    8,
                    12,
                    49,
                    6,
                    1,
                    189,
                    0
                ],
                "published": "2021-06-29T03:47:28Z",
                "published_parsed": [
                    2021,
                    6,
                    29,
                    3,
                    47,
                    28,
                    1,
                    180,
                    0
                ],
                "title": "Causal Inference in Longitudinal Data under Unknown Interference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Causal Inference in Longitudinal Data under Unknown Interference"
                },
                "summary": "In longitudinal studies where units are embedded in space or a social\nnetwork, interference may arise, meaning that a unit's outcome can depend on\ntreatment histories of others. The presence of interference poses significant\nchallenges for causal inference, particularly when the interference structure\n-- how a unit's outcome responds to others' influences -- is complex,\nheterogeneous, and unknown to researchers. This paper develops a general\nframework for identifying and estimating both direct and spillover effects of\ntreatment histories under minimal assumptions about the interference structure.\nWe define a class of policy-relevant causal estimands and show that they can be\nrepresented by a modified marginal structural model (MSM). Under the standard\nassumption of sequential exchangeability, these estimands are identifiable and\ncan be estimated using inverse probability weighting (IPW). We derive\nconditions for consistency and asymptotic normality of the estimators and\nprovide procedures for constructing Wald-type confidence intervals with valid\ncoverage in large samples. The method's utility is demonstrated through\napplications in both social science and biomedical settings.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In longitudinal studies where units are embedded in space or a social\nnetwork, interference may arise, meaning that a unit's outcome can depend on\ntreatment histories of others. The presence of interference poses significant\nchallenges for causal inference, particularly when the interference structure\n-- how a unit's outcome responds to others' influences -- is complex,\nheterogeneous, and unknown to researchers. This paper develops a general\nframework for identifying and estimating both direct and spillover effects of\ntreatment histories under minimal assumptions about the interference structure.\nWe define a class of policy-relevant causal estimands and show that they can be\nrepresented by a modified marginal structural model (MSM). Under the standard\nassumption of sequential exchangeability, these estimands are identifiable and\ncan be estimated using inverse probability weighting (IPW). We derive\nconditions for consistency and asymptotic normality of the estimators and\nprovide procedures for constructing Wald-type confidence intervals with valid\ncoverage in large samples. The method's utility is demonstrated through\napplications in both social science and biomedical settings."
                },
                "authors": [
                    {
                        "name": "Ye Wang"
                    },
                    {
                        "name": "Michael Jetsupphasuk"
                    }
                ],
                "author_detail": {
                    "name": "Michael Jetsupphasuk"
                },
                "author": "Michael Jetsupphasuk",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2106.15074v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2106.15074v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ME",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.ST",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.AP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.TH",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.05949v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.05949v1",
                "updated": "2025-07-08T12:48:22Z",
                "updated_parsed": [
                    2025,
                    7,
                    8,
                    12,
                    48,
                    22,
                    1,
                    189,
                    0
                ],
                "published": "2025-07-08T12:48:22Z",
                "published_parsed": [
                    2025,
                    7,
                    8,
                    12,
                    48,
                    22,
                    1,
                    189,
                    0
                ],
                "title": "hassediagrams:an R package that generates the Hasse diagram of the\n  layout structure and the restricted layout structure",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "hassediagrams:an R package that generates the Hasse diagram of the\n  layout structure and the restricted layout structure"
                },
                "summary": "With the advent of modern statistical software, complex experimental designs\nare now routinely employed in many areas of research. Failing to correctly\nidentify the structure of the experimental design can lead to incorrect model\nselection and misleading inferences. This paper describes the hassediagrams\npackage in R that determines the structure of the design, summarised by the\nlayout structure, and generates a Hasse diagram of the layout structure. By\nconsidering the randomisation performed, in conjunction with the layout\nstructure, a set of randomisation objects can be defined that form the\nrestricted layout structure. This structure can also be visualised using a\ngeneralisation of the Hasse diagram. Objects in the restricted layout structure\ncan be used to identify the terms to include in the statistical model. The use\nof the procedure thus ensures consistency of model selection due to the\nsystematic approach taken to generate the model.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the advent of modern statistical software, complex experimental designs\nare now routinely employed in many areas of research. Failing to correctly\nidentify the structure of the experimental design can lead to incorrect model\nselection and misleading inferences. This paper describes the hassediagrams\npackage in R that determines the structure of the design, summarised by the\nlayout structure, and generates a Hasse diagram of the layout structure. By\nconsidering the randomisation performed, in conjunction with the layout\nstructure, a set of randomisation objects can be defined that form the\nrestricted layout structure. This structure can also be visualised using a\ngeneralisation of the Hasse diagram. Objects in the restricted layout structure\ncan be used to identify the terms to include in the statistical model. The use\nof the procedure thus ensures consistency of model selection due to the\nsystematic approach taken to generate the model."
                },
                "authors": [
                    {
                        "name": "Damianos Michaelides"
                    },
                    {
                        "name": "Simon T. Bate"
                    },
                    {
                        "name": "Marion J. Chatfield"
                    }
                ],
                "author_detail": {
                    "name": "Marion J. Chatfield"
                },
                "author": "Marion J. Chatfield",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.05949v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.05949v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.CO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.CO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.AP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.13002v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.13002v4",
                "updated": "2025-07-08T12:45:45Z",
                "updated_parsed": [
                    2025,
                    7,
                    8,
                    12,
                    45,
                    45,
                    1,
                    189,
                    0
                ],
                "published": "2024-08-23T11:44:07Z",
                "published_parsed": [
                    2024,
                    8,
                    23,
                    11,
                    44,
                    7,
                    4,
                    236,
                    0
                ],
                "title": "Measuring Variable Importance in Heterogeneous Treatment Effects with\n  Confidence",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Measuring Variable Importance in Heterogeneous Treatment Effects with\n  Confidence"
                },
                "summary": "Causal machine learning holds promise for estimating individual treatment\neffects from complex data. For successful real-world applications of machine\nlearning methods, it is of paramount importance to obtain reliable insights\ninto which variables drive heterogeneity in the response to treatment. We\npropose PermuCATE, an algorithm based on the Conditional Permutation Importance\n(CPI) method, for statistically rigorous global variable importance assessment\nin the estimation of the Conditional Average Treatment Effect (CATE).\nTheoretical analysis of the finite sample regime and empirical studies show\nthat PermuCATE has lower variance than the Leave-One-Covariate-Out (LOCO)\nreference method and provides a reliable measure of variable importance. This\nproperty increases statistical power, which is crucial for causal inference in\nthe limited-data regime common to biomedical applications. We empirically\ndemonstrate the benefits of PermuCATE in simulated and real-world health\ndatasets, including settings with up to hundreds of correlated variables.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Causal machine learning holds promise for estimating individual treatment\neffects from complex data. For successful real-world applications of machine\nlearning methods, it is of paramount importance to obtain reliable insights\ninto which variables drive heterogeneity in the response to treatment. We\npropose PermuCATE, an algorithm based on the Conditional Permutation Importance\n(CPI) method, for statistically rigorous global variable importance assessment\nin the estimation of the Conditional Average Treatment Effect (CATE).\nTheoretical analysis of the finite sample regime and empirical studies show\nthat PermuCATE has lower variance than the Leave-One-Covariate-Out (LOCO)\nreference method and provides a reliable measure of variable importance. This\nproperty increases statistical power, which is crucial for causal inference in\nthe limited-data regime common to biomedical applications. We empirically\ndemonstrate the benefits of PermuCATE in simulated and real-world health\ndatasets, including settings with up to hundreds of correlated variables."
                },
                "authors": [
                    {
                        "name": "Joseph Paillard"
                    },
                    {
                        "name": "Angel Reyero Lobo"
                    },
                    {
                        "name": "Vitaliy Kolodyazhniy"
                    },
                    {
                        "name": "Bertrand Thirion"
                    },
                    {
                        "name": "Denis A. Engemann"
                    }
                ],
                "author_detail": {
                    "name": "Denis A. Engemann"
                },
                "author": "Denis A. Engemann",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.13002v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.13002v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.05940v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.05940v1",
                "updated": "2025-07-08T12:38:41Z",
                "updated_parsed": [
                    2025,
                    7,
                    8,
                    12,
                    38,
                    41,
                    1,
                    189,
                    0
                ],
                "published": "2025-07-08T12:38:41Z",
                "published_parsed": [
                    2025,
                    7,
                    8,
                    12,
                    38,
                    41,
                    1,
                    189,
                    0
                ],
                "title": "Chat-Ghosting: A Comparative Study of Methods for Auto-Completion in\n  Dialog Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Chat-Ghosting: A Comparative Study of Methods for Auto-Completion in\n  Dialog Systems"
                },
                "summary": "Ghosting, the ability to predict a user's intended text input for inline\nquery auto-completion, is an invaluable feature for modern search engines and\nchat interfaces, greatly enhancing user experience. By suggesting completions\nto incomplete queries (or prefixes), ghosting aids users with slow typing\nspeeds, disabilities, or limited language proficiency. Ghosting is a\nchallenging problem and has become more important with the ubiquitousness of\nchat-based systems like ChatGPT, Copilot, etc. Despite the increasing\nprominence of chat-based systems utilizing ghosting, this challenging problem\nof Chat-Ghosting has received little attention from the NLP/ML research\ncommunity. There is a lack of standardized benchmarks and relative performance\nanalysis of deep learning and non-deep learning methods. We address this\nthrough an open and thorough study of this problem using four publicly\navailable dialog datasets: two human-human (DailyDialog and DSTC7-Ubuntu) and\ntwo human-bot (Open Assistant and ShareGPT). We experiment with various\nexisting query auto-completion methods (using tries), n-gram methods and deep\nlearning methods, with and without dialog context. We also propose a novel\nentropy-based dynamic early stopping strategy. Our analysis finds that\nstatistical n-gram models and tries outperform deep learning based models in\nterms of both model performance and inference efficiency for seen prefixes. For\nunseen queries, neural models like T5 and Phi-2 lead to better results. Adding\nconversational context leads to significant improvements in ghosting quality,\nespecially for Open-Assistant and ShareGPT. We make code and data publicly\navailable",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Ghosting, the ability to predict a user's intended text input for inline\nquery auto-completion, is an invaluable feature for modern search engines and\nchat interfaces, greatly enhancing user experience. By suggesting completions\nto incomplete queries (or prefixes), ghosting aids users with slow typing\nspeeds, disabilities, or limited language proficiency. Ghosting is a\nchallenging problem and has become more important with the ubiquitousness of\nchat-based systems like ChatGPT, Copilot, etc. Despite the increasing\nprominence of chat-based systems utilizing ghosting, this challenging problem\nof Chat-Ghosting has received little attention from the NLP/ML research\ncommunity. There is a lack of standardized benchmarks and relative performance\nanalysis of deep learning and non-deep learning methods. We address this\nthrough an open and thorough study of this problem using four publicly\navailable dialog datasets: two human-human (DailyDialog and DSTC7-Ubuntu) and\ntwo human-bot (Open Assistant and ShareGPT). We experiment with various\nexisting query auto-completion methods (using tries), n-gram methods and deep\nlearning methods, with and without dialog context. We also propose a novel\nentropy-based dynamic early stopping strategy. Our analysis finds that\nstatistical n-gram models and tries outperform deep learning based models in\nterms of both model performance and inference efficiency for seen prefixes. For\nunseen queries, neural models like T5 and Phi-2 lead to better results. Adding\nconversational context leads to significant improvements in ghosting quality,\nespecially for Open-Assistant and ShareGPT. We make code and data publicly\navailable"
                },
                "authors": [
                    {
                        "name": "Sandeep Mishra"
                    },
                    {
                        "name": "Anubhab Mandal"
                    },
                    {
                        "name": "Bishal Santra"
                    },
                    {
                        "name": "Tushar Abhishek"
                    },
                    {
                        "name": "Pawan Goyal"
                    },
                    {
                        "name": "Manish Gupta"
                    }
                ],
                "author_detail": {
                    "name": "Manish Gupta"
                },
                "author": "Manish Gupta",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.05940v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.05940v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.03370v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.03370v2",
                "updated": "2025-07-08T12:31:38Z",
                "updated_parsed": [
                    2025,
                    7,
                    8,
                    12,
                    31,
                    38,
                    1,
                    189,
                    0
                ],
                "published": "2025-07-04T08:10:12Z",
                "published_parsed": [
                    2025,
                    7,
                    4,
                    8,
                    10,
                    12,
                    4,
                    185,
                    0
                ],
                "title": "Resurgence of CO in a warm bubble around accreting protoplanets and its\n  observability",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Resurgence of CO in a warm bubble around accreting protoplanets and its\n  observability"
                },
                "summary": "The cold outer regions of protoplanetary disks are expected to contain a\nmidplane-centered layer where gas-phase CO molecules freeze out and their\noverall abundance is low. The layer then manifests itself as a void in the\nchannel maps of CO rotational emission lines. We explore whether the frozen-out\nlayer can expose the circumplanetary environment of embedded accreting\nprotoplanets to observations. To this end, we performed 3D radiative gas-dust\nhydrodynamic simulations with opacities determined by the redistribution of\nsubmicron- and millimeter-sized dust grains. A Jupiter-mass planet with an\naccretion luminosity of $\\sim$$10^{-3}\\,L_{\\odot}$ was considered as the\nnominal case. The accretion heating sustains a warm bubble around the planet,\nwhich locally increases the abundance of gas-phase CO molecules. Radiative\ntransfer predictions of the emergent sky images show that the bubble becomes a\nconspicuous CO emission source in channel maps. It appears as a low-intensity\noptically thick spot located in between the so-called dragonfly wings that\ntrace the fore- and backside line-forming surfaces. The emission intensity of\nthe bubble is nearly independent of the tracing isotopolog, suggesting a very\nrich observable chemistry, as long as its signal can be deblended from the\nextended disk emission. This can be achieved with isotopologs that are\noptically thin or weakly thermally stratified across the planet-induced gap,\nsuch as C$^{18}$O. For these, the bubble stands out as the brightest residual\nin synthetic ALMA observations after subtraction of axially averaged channel\nmaps inferred from the disk kinematics, enabling new automatic detections of\nforming protoplanets. By contrast, the horseshoe flow steadily depletes large\ndust grains from the circumplanetary environment, which becomes unobservable in\nthe submillimeter continuum, in accordance with the scarcity of ALMA\ndetections.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The cold outer regions of protoplanetary disks are expected to contain a\nmidplane-centered layer where gas-phase CO molecules freeze out and their\noverall abundance is low. The layer then manifests itself as a void in the\nchannel maps of CO rotational emission lines. We explore whether the frozen-out\nlayer can expose the circumplanetary environment of embedded accreting\nprotoplanets to observations. To this end, we performed 3D radiative gas-dust\nhydrodynamic simulations with opacities determined by the redistribution of\nsubmicron- and millimeter-sized dust grains. A Jupiter-mass planet with an\naccretion luminosity of $\\sim$$10^{-3}\\,L_{\\odot}$ was considered as the\nnominal case. The accretion heating sustains a warm bubble around the planet,\nwhich locally increases the abundance of gas-phase CO molecules. Radiative\ntransfer predictions of the emergent sky images show that the bubble becomes a\nconspicuous CO emission source in channel maps. It appears as a low-intensity\noptically thick spot located in between the so-called dragonfly wings that\ntrace the fore- and backside line-forming surfaces. The emission intensity of\nthe bubble is nearly independent of the tracing isotopolog, suggesting a very\nrich observable chemistry, as long as its signal can be deblended from the\nextended disk emission. This can be achieved with isotopologs that are\noptically thin or weakly thermally stratified across the planet-induced gap,\nsuch as C$^{18}$O. For these, the bubble stands out as the brightest residual\nin synthetic ALMA observations after subtraction of axially averaged channel\nmaps inferred from the disk kinematics, enabling new automatic detections of\nforming protoplanets. By contrast, the horseshoe flow steadily depletes large\ndust grains from the circumplanetary environment, which becomes unobservable in\nthe submillimeter continuum, in accordance with the scarcity of ALMA\ndetections."
                },
                "authors": [
                    {
                        "name": "O. Chrenko"
                    },
                    {
                        "name": "S. Casassus"
                    },
                    {
                        "name": "R. O. Chametla"
                    }
                ],
                "author_detail": {
                    "name": "R. O. Chametla"
                },
                "author": "R. O. Chametla",
                "arxiv_comment": "Accepted in A&A",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.03370v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.03370v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.EP",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.EP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "astro-ph.SR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.05913v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.05913v1",
                "updated": "2025-07-08T11:59:48Z",
                "updated_parsed": [
                    2025,
                    7,
                    8,
                    11,
                    59,
                    48,
                    1,
                    189,
                    0
                ],
                "published": "2025-07-08T11:59:48Z",
                "published_parsed": [
                    2025,
                    7,
                    8,
                    11,
                    59,
                    48,
                    1,
                    189,
                    0
                ],
                "title": "Best-of-N through the Smoothing Lens: KL Divergence and Regret Analysis",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Best-of-N through the Smoothing Lens: KL Divergence and Regret Analysis"
                },
                "summary": "A simple yet effective method for inference-time alignment of generative\nmodels is Best-of-$N$ (BoN), where $N$ outcomes are sampled from a reference\npolicy, evaluated using a proxy reward model, and the highest-scoring one is\nselected. While prior work argues that BoN is almost optimal in reward vs KL\ntradeoffs, the effectiveness of BoN depends critically on the quality of the\nproxy reward model used for selection. For this purpose, we study BoN through a\nsmooth version known as Soft Best-of-N (SBoN) and develop a theoretical\nframework to address this gap. We analyze the scaling behaviour of BoN by\nproviding bounds on the KL divergence between the SBoN policy and the reference\npolicy, offering insights into how performance varies with the number of\nsamples. We also study the regret gap, i.e., the gap between the expected true\nreward under the optimal policy and the SBoN policy. Our theoretical and\nempirical findings show that smoothing helps SBoN mitigate reward\noveroptimization, especially when the quality of the proxy reward is low.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A simple yet effective method for inference-time alignment of generative\nmodels is Best-of-$N$ (BoN), where $N$ outcomes are sampled from a reference\npolicy, evaluated using a proxy reward model, and the highest-scoring one is\nselected. While prior work argues that BoN is almost optimal in reward vs KL\ntradeoffs, the effectiveness of BoN depends critically on the quality of the\nproxy reward model used for selection. For this purpose, we study BoN through a\nsmooth version known as Soft Best-of-N (SBoN) and develop a theoretical\nframework to address this gap. We analyze the scaling behaviour of BoN by\nproviding bounds on the KL divergence between the SBoN policy and the reference\npolicy, offering insights into how performance varies with the number of\nsamples. We also study the regret gap, i.e., the gap between the expected true\nreward under the optimal policy and the SBoN policy. Our theoretical and\nempirical findings show that smoothing helps SBoN mitigate reward\noveroptimization, especially when the quality of the proxy reward is low."
                },
                "authors": [
                    {
                        "name": "Gholamali Aminian"
                    },
                    {
                        "name": "Idan Shenfeld"
                    },
                    {
                        "name": "Amir R. Asadi"
                    },
                    {
                        "name": "Ahmad Beirami"
                    },
                    {
                        "name": "Youssef Mroueh"
                    }
                ],
                "author_detail": {
                    "name": "Youssef Mroueh"
                },
                "author": "Youssef Mroueh",
                "arxiv_comment": "Workshop on Efficient Systems for Foundation Models at iCML",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.05913v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.05913v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ML",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.05911v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.05911v1",
                "updated": "2025-07-08T11:57:16Z",
                "updated_parsed": [
                    2025,
                    7,
                    8,
                    11,
                    57,
                    16,
                    1,
                    189,
                    0
                ],
                "published": "2025-07-08T11:57:16Z",
                "published_parsed": [
                    2025,
                    7,
                    8,
                    11,
                    57,
                    16,
                    1,
                    189,
                    0
                ],
                "title": "Differentiable Reward Optimization for LLM based TTS system",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Differentiable Reward Optimization for LLM based TTS system"
                },
                "summary": "This paper proposes a novel Differentiable Reward Optimization (DiffRO)\nmethod aimed at enhancing the performance of neural codec language models based\ntext-to-speech (TTS) systems. In contrast to conventional reinforcement\nlearning from human feedback (RLHF) approaches applied to TTS, DiffRO directly\ncompute the rewards based on neural codec tokens, rather than relying on\nsynthesized audio. Furthermore, we employ the Gumbel-Softmax technique to\nrender the reward function differentiable, thereby streamlining the RLHF\ntraining process. Additionally, we introduce a multi-task reward (MTR) model\nwhich can provide feedback from different perspectives and find that it can\naugment the system's capability to follow instructions effectively.Experimental\nresults indicate that DiffRO significantly improves the pronunciation accuracy\nof the TTS system, achieving state-of-the-art (SOTA) WER results on the\nseed-tts-eval benchmark. Moreover, with the integration of the MTR model, we\ndemonstrate the ability to control emotional and quality attributes in a\nzero-shot manner.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper proposes a novel Differentiable Reward Optimization (DiffRO)\nmethod aimed at enhancing the performance of neural codec language models based\ntext-to-speech (TTS) systems. In contrast to conventional reinforcement\nlearning from human feedback (RLHF) approaches applied to TTS, DiffRO directly\ncompute the rewards based on neural codec tokens, rather than relying on\nsynthesized audio. Furthermore, we employ the Gumbel-Softmax technique to\nrender the reward function differentiable, thereby streamlining the RLHF\ntraining process. Additionally, we introduce a multi-task reward (MTR) model\nwhich can provide feedback from different perspectives and find that it can\naugment the system's capability to follow instructions effectively.Experimental\nresults indicate that DiffRO significantly improves the pronunciation accuracy\nof the TTS system, achieving state-of-the-art (SOTA) WER results on the\nseed-tts-eval benchmark. Moreover, with the integration of the MTR model, we\ndemonstrate the ability to control emotional and quality attributes in a\nzero-shot manner."
                },
                "authors": [
                    {
                        "name": "Changfeng Gao"
                    },
                    {
                        "name": "Zhihao Du"
                    },
                    {
                        "name": "Shiliang Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Shiliang Zhang"
                },
                "author": "Shiliang Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.05911v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.05911v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SD",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.06048v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.06048v4",
                "updated": "2025-07-08T11:50:25Z",
                "updated_parsed": [
                    2025,
                    7,
                    8,
                    11,
                    50,
                    25,
                    1,
                    189,
                    0
                ],
                "published": "2025-04-08T13:47:07Z",
                "published_parsed": [
                    2025,
                    4,
                    8,
                    13,
                    47,
                    7,
                    1,
                    98,
                    0
                ],
                "title": "Trust-Region Twisted Policy Improvement",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Trust-Region Twisted Policy Improvement"
                },
                "summary": "Monte-Carlo tree search (MCTS) has driven many recent breakthroughs in deep\nreinforcement learning (RL). However, scaling MCTS to parallel compute has\nproven challenging in practice which has motivated alternative planners like\nsequential Monte-Carlo (SMC). Many of these SMC methods adopt particle filters\nfor smoothing through a reformulation of RL as a policy inference problem. Yet,\npersisting design choices of these particle filters often conflict with the aim\nof online planning in RL, which is to obtain a policy improvement at the start\nof planning. Drawing inspiration from MCTS, we tailor SMC planners specifically\nfor RL by improving data generation within the planner through constrained\naction sampling and explicit terminal state handling, as well as improving\npolicy and value target estimation. This leads to our Trust-Region Twisted SMC\n(TRT-SMC), which shows improved runtime and sample-efficiency over baseline\nMCTS and SMC methods in both discrete and continuous domains.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Monte-Carlo tree search (MCTS) has driven many recent breakthroughs in deep\nreinforcement learning (RL). However, scaling MCTS to parallel compute has\nproven challenging in practice which has motivated alternative planners like\nsequential Monte-Carlo (SMC). Many of these SMC methods adopt particle filters\nfor smoothing through a reformulation of RL as a policy inference problem. Yet,\npersisting design choices of these particle filters often conflict with the aim\nof online planning in RL, which is to obtain a policy improvement at the start\nof planning. Drawing inspiration from MCTS, we tailor SMC planners specifically\nfor RL by improving data generation within the planner through constrained\naction sampling and explicit terminal state handling, as well as improving\npolicy and value target estimation. This leads to our Trust-Region Twisted SMC\n(TRT-SMC), which shows improved runtime and sample-efficiency over baseline\nMCTS and SMC methods in both discrete and continuous domains."
                },
                "authors": [
                    {
                        "name": "Joery A. de Vries"
                    },
                    {
                        "name": "Jinke He"
                    },
                    {
                        "name": "Yaniv Oren"
                    },
                    {
                        "name": "Matthijs T. J. Spaan"
                    }
                ],
                "author_detail": {
                    "name": "Matthijs T. J. Spaan"
                },
                "author": "Matthijs T. J. Spaan",
                "arxiv_comment": "Poster at ICML2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.06048v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.06048v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.06382v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.06382v3",
                "updated": "2025-07-08T11:43:16Z",
                "updated_parsed": [
                    2025,
                    7,
                    8,
                    11,
                    43,
                    16,
                    1,
                    189,
                    0
                ],
                "published": "2025-06-04T23:28:39Z",
                "published_parsed": [
                    2025,
                    6,
                    4,
                    23,
                    28,
                    39,
                    2,
                    155,
                    0
                ],
                "title": "On the Fundamental Impossibility of Hallucination Control in Large\n  Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "On the Fundamental Impossibility of Hallucination Control in Large\n  Language Models"
                },
                "summary": "We prove that perfect hallucination control in large language models is\nmathematically impossible. No LLM inference mechanism can simultaneously\nachieve truthful response generation, semantic information conservation,\nrelevant knowledge revelation, and knowledge-constrained optimality. This\nimpossibility is fundamental, arising from the mathematical structure of\ninformation aggregation itself rather than engineering limitations. The proof\nspans three mathematical frameworks: auction theory, proper scoring theory for\nprobabilistic predictions, and log-sum-exp analysis for transformer\narchitectures. In each setting, we demonstrate that information aggregation\ncreates unavoidable violations of conservation principles. The Jensen gap in\ntransformer probability aggregation provides a direct measure of this\nimpossibility. These results reframe hallucination from an engineering bug to\nan inevitable mathematical feature of distributed intelligence. There are\nfundamental trade-offs between truthfulness, knowledge utilization, and\nresponse completeness, providing principled foundations for managing rather\nthan eliminating hallucination. This work reveals deep connections between\nneural network inference, philosophy of knowledge and reasoning, and classical\nresults in game theory and information theory, opening new research directions\nfor developing beneficial AI systems within mathematical constraints.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We prove that perfect hallucination control in large language models is\nmathematically impossible. No LLM inference mechanism can simultaneously\nachieve truthful response generation, semantic information conservation,\nrelevant knowledge revelation, and knowledge-constrained optimality. This\nimpossibility is fundamental, arising from the mathematical structure of\ninformation aggregation itself rather than engineering limitations. The proof\nspans three mathematical frameworks: auction theory, proper scoring theory for\nprobabilistic predictions, and log-sum-exp analysis for transformer\narchitectures. In each setting, we demonstrate that information aggregation\ncreates unavoidable violations of conservation principles. The Jensen gap in\ntransformer probability aggregation provides a direct measure of this\nimpossibility. These results reframe hallucination from an engineering bug to\nan inevitable mathematical feature of distributed intelligence. There are\nfundamental trade-offs between truthfulness, knowledge utilization, and\nresponse completeness, providing principled foundations for managing rather\nthan eliminating hallucination. This work reveals deep connections between\nneural network inference, philosophy of knowledge and reasoning, and classical\nresults in game theory and information theory, opening new research directions\nfor developing beneficial AI systems within mathematical constraints."
                },
                "authors": [
                    {
                        "name": "MichaÅ P. Karpowicz"
                    }
                ],
                "author_detail": {
                    "name": "MichaÅ P. Karpowicz"
                },
                "author": "MichaÅ P. Karpowicz",
                "arxiv_comment": "transformer example extended, discussion and speculation section\n  added",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.06382v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.06382v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ML",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.GT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.17278v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.17278v2",
                "updated": "2025-07-08T11:40:02Z",
                "updated_parsed": [
                    2025,
                    7,
                    8,
                    11,
                    40,
                    2,
                    1,
                    189,
                    0
                ],
                "published": "2024-08-30T13:27:14Z",
                "published_parsed": [
                    2024,
                    8,
                    30,
                    13,
                    27,
                    14,
                    4,
                    243,
                    0
                ],
                "title": "Incorporating Memory into Continuous-Time Spatial Capture-Recapture\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Incorporating Memory into Continuous-Time Spatial Capture-Recapture\n  Models"
                },
                "summary": "Obtaining reliable and precise estimates of wildlife species abundance and\ndistribution is essential for the conservation and management of animal\npopulations and natural reserves. Spatial capture-recapture (SCR) models\nprovide estimates of population size and spatial density from data collected\nfrom remote sensors such as camera traps. Such data contain spatial correlation\nbetween observations of the same individual, which SCR models partly account\nfor through a latent individual-specific activity centre, a location near which\nthe individual is more likely detected. However, SCR models assume that the\nobservations of an individual are independent over time and space, conditional\non its activity centre, so that observed sightings at a given time and location\ndo not influence the probability of being seen at future times and/or\nlocations. This assumption is ecologically unrealistic given the smooth\nmovement of animals over space through time. We propose a new continuous-time\nmodelling framework that incorporates both an individual's (latent) activity\ncentre and its (known) previous location and time of detection. By formulating\nthe detections of an individual as an inhomogeneous temporal Poisson process,\nwe develop a model drawing inspiration from the Ornstein-Uhlenbeck process,\nwhich is commonly used to model animal movement. Applying our model to a\ncamera-trap survey of American martens, we observe a substantial improvement in\nmodel fit and notable differences in the estimated spatial distribution of\nactivity centres. A simulation study shows that standard SCR models can produce\nsubstantially biased population estimates when spatio-temporal dependence is\nignored, while the memory-based model remains robust. These findings highlight\nthe importance of accounting for memory of previous detections in SCR models to\nimprove ecological interpretation and inference.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Obtaining reliable and precise estimates of wildlife species abundance and\ndistribution is essential for the conservation and management of animal\npopulations and natural reserves. Spatial capture-recapture (SCR) models\nprovide estimates of population size and spatial density from data collected\nfrom remote sensors such as camera traps. Such data contain spatial correlation\nbetween observations of the same individual, which SCR models partly account\nfor through a latent individual-specific activity centre, a location near which\nthe individual is more likely detected. However, SCR models assume that the\nobservations of an individual are independent over time and space, conditional\non its activity centre, so that observed sightings at a given time and location\ndo not influence the probability of being seen at future times and/or\nlocations. This assumption is ecologically unrealistic given the smooth\nmovement of animals over space through time. We propose a new continuous-time\nmodelling framework that incorporates both an individual's (latent) activity\ncentre and its (known) previous location and time of detection. By formulating\nthe detections of an individual as an inhomogeneous temporal Poisson process,\nwe develop a model drawing inspiration from the Ornstein-Uhlenbeck process,\nwhich is commonly used to model animal movement. Applying our model to a\ncamera-trap survey of American martens, we observe a substantial improvement in\nmodel fit and notable differences in the estimated spatial distribution of\nactivity centres. A simulation study shows that standard SCR models can produce\nsubstantially biased population estimates when spatio-temporal dependence is\nignored, while the memory-based model remains robust. These findings highlight\nthe importance of accounting for memory of previous detections in SCR models to\nimprove ecological interpretation and inference."
                },
                "authors": [
                    {
                        "name": "Clara Panchaud"
                    },
                    {
                        "name": "Ruth King"
                    },
                    {
                        "name": "David Borchers"
                    },
                    {
                        "name": "Hannah Worthington"
                    },
                    {
                        "name": "Ian Durbach"
                    },
                    {
                        "name": "Paul Van Dam-Bates"
                    }
                ],
                "author_detail": {
                    "name": "Paul Van Dam-Bates"
                },
                "author": "Paul Van Dam-Bates",
                "arxiv_comment": "19 pages, 4 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.17278v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.17278v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ME",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.AP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "62",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.17460v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.17460v2",
                "updated": "2025-07-08T11:35:29Z",
                "updated_parsed": [
                    2025,
                    7,
                    8,
                    11,
                    35,
                    29,
                    1,
                    189,
                    0
                ],
                "published": "2025-05-23T04:41:54Z",
                "published_parsed": [
                    2025,
                    5,
                    23,
                    4,
                    41,
                    54,
                    4,
                    143,
                    0
                ],
                "title": "Learning to Focus: Context Extraction for Efficient Code Vulnerability\n  Detection with Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Learning to Focus: Context Extraction for Efficient Code Vulnerability\n  Detection with Language Models"
                },
                "summary": "Language models (LMs) show promise for vulnerability detection but struggle\nwith long, real-world code due to sparse and uncertain vulnerability locations.\nThese issues, exacerbated by token limits, often cause models to miss\nvulnerability-related signals, thereby impairing effective learning. A key\nintuition is to enhance LMs with concise, information-rich context.\nCommit-based annotations offer precise, CWE-agnostic supervision, but are\nunavailable during inference, as they depend on historical code changes.\nMoreover, their extreme sparsity, often covering only a few lines, makes it\ndifficult for LMs to process directly. In this paper, we propose FocusVul, a\nmodel-agnostic framework that improves LM-based vulnerability detection by\nlearning to select sensitive context. FocusVul learns commit-based annotation\npatterns through hierarchical semantic modeling and generalizes them to\nidentify line-level vulnerability-relevant regions during inference. It then\nextracts LM-oriented context via both dependency and execution flows\nsurrounding selected regions, yielding semantically rich inputs for effective\nvulnerability detection. Experiments on real-world benchmarks show that\nFocusVul consistently outperforms heuristic-based and full-function fine-tuning\napproaches, improving classification performance by 164.04% and reducing FLOPs\nby 19.12% on average.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Language models (LMs) show promise for vulnerability detection but struggle\nwith long, real-world code due to sparse and uncertain vulnerability locations.\nThese issues, exacerbated by token limits, often cause models to miss\nvulnerability-related signals, thereby impairing effective learning. A key\nintuition is to enhance LMs with concise, information-rich context.\nCommit-based annotations offer precise, CWE-agnostic supervision, but are\nunavailable during inference, as they depend on historical code changes.\nMoreover, their extreme sparsity, often covering only a few lines, makes it\ndifficult for LMs to process directly. In this paper, we propose FocusVul, a\nmodel-agnostic framework that improves LM-based vulnerability detection by\nlearning to select sensitive context. FocusVul learns commit-based annotation\npatterns through hierarchical semantic modeling and generalizes them to\nidentify line-level vulnerability-relevant regions during inference. It then\nextracts LM-oriented context via both dependency and execution flows\nsurrounding selected regions, yielding semantically rich inputs for effective\nvulnerability detection. Experiments on real-world benchmarks show that\nFocusVul consistently outperforms heuristic-based and full-function fine-tuning\napproaches, improving classification performance by 164.04% and reducing FLOPs\nby 19.12% on average."
                },
                "authors": [
                    {
                        "name": "Xinran Zheng"
                    },
                    {
                        "name": "Xingzhi Qian"
                    },
                    {
                        "name": "Huichi Zhou"
                    },
                    {
                        "name": "Shuo Yang"
                    },
                    {
                        "name": "Yiling He"
                    },
                    {
                        "name": "Suman Jana"
                    },
                    {
                        "name": "Lorenzo Cavallaro"
                    }
                ],
                "author_detail": {
                    "name": "Lorenzo Cavallaro"
                },
                "author": "Lorenzo Cavallaro",
                "arxiv_comment": "withdrawal for fixing errors",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.17460v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.17460v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.05891v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.05891v1",
                "updated": "2025-07-08T11:26:42Z",
                "updated_parsed": [
                    2025,
                    7,
                    8,
                    11,
                    26,
                    42,
                    1,
                    189,
                    0
                ],
                "published": "2025-07-08T11:26:42Z",
                "published_parsed": [
                    2025,
                    7,
                    8,
                    11,
                    26,
                    42,
                    1,
                    189,
                    0
                ],
                "title": "Decomposing the Time Series Forecasting Pipeline: A Modular Approach for\n  Time Series Representation, Information Extraction, and Projection",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Decomposing the Time Series Forecasting Pipeline: A Modular Approach for\n  Time Series Representation, Information Extraction, and Projection"
                },
                "summary": "With the advent of Transformers, time series forecasting has seen significant\nadvances, yet it remains challenging due to the need for effective sequence\nrepresentation, memory construction, and accurate target projection. Time\nseries forecasting remains a challenging task, demanding effective sequence\nrepresentation, meaningful information extraction, and precise future\nprojection. Each dataset and forecasting configuration constitutes a distinct\ntask, each posing unique challenges the model must overcome to produce accurate\npredictions. To systematically address these task-specific difficulties, this\nwork decomposes the time series forecasting pipeline into three core stages:\ninput sequence representation, information extraction and memory construction,\nand final target projection. Within each stage, we investigate a range of\narchitectural configurations to assess the effectiveness of various modules,\nsuch as convolutional layers for feature extraction and self-attention\nmechanisms for information extraction, across diverse forecasting tasks,\nincluding evaluations on seven benchmark datasets. Our models achieve\nstate-of-the-art forecasting accuracy while greatly enhancing computational\nefficiency, with reduced training and inference times and a lower parameter\ncount. The source code is available at\nhttps://github.com/RobertLeppich/REP-Net.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the advent of Transformers, time series forecasting has seen significant\nadvances, yet it remains challenging due to the need for effective sequence\nrepresentation, memory construction, and accurate target projection. Time\nseries forecasting remains a challenging task, demanding effective sequence\nrepresentation, meaningful information extraction, and precise future\nprojection. Each dataset and forecasting configuration constitutes a distinct\ntask, each posing unique challenges the model must overcome to produce accurate\npredictions. To systematically address these task-specific difficulties, this\nwork decomposes the time series forecasting pipeline into three core stages:\ninput sequence representation, information extraction and memory construction,\nand final target projection. Within each stage, we investigate a range of\narchitectural configurations to assess the effectiveness of various modules,\nsuch as convolutional layers for feature extraction and self-attention\nmechanisms for information extraction, across diverse forecasting tasks,\nincluding evaluations on seven benchmark datasets. Our models achieve\nstate-of-the-art forecasting accuracy while greatly enhancing computational\nefficiency, with reduced training and inference times and a lower parameter\ncount. The source code is available at\nhttps://github.com/RobertLeppich/REP-Net."
                },
                "authors": [
                    {
                        "name": "Robert Leppich"
                    },
                    {
                        "name": "Michael Stenger"
                    },
                    {
                        "name": "AndrÃ© Bauer"
                    },
                    {
                        "name": "Samuel Kounev"
                    }
                ],
                "author_detail": {
                    "name": "Samuel Kounev"
                },
                "author": "Samuel Kounev",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.05891v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.05891v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.05890v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.05890v1",
                "updated": "2025-07-08T11:26:03Z",
                "updated_parsed": [
                    2025,
                    7,
                    8,
                    11,
                    26,
                    3,
                    1,
                    189,
                    0
                ],
                "published": "2025-07-08T11:26:03Z",
                "published_parsed": [
                    2025,
                    7,
                    8,
                    11,
                    26,
                    3,
                    1,
                    189,
                    0
                ],
                "title": "Psychometric Item Validation Using Virtual Respondents with\n  Trait-Response Mediators",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Psychometric Item Validation Using Virtual Respondents with\n  Trait-Response Mediators"
                },
                "summary": "As psychometric surveys are increasingly used to assess the traits of large\nlanguage models (LLMs), the need for scalable survey item generation suited for\nLLMs has also grown. A critical challenge here is ensuring the construct\nvalidity of generated items, i.e., whether they truly measure the intended\ntrait. Traditionally, this requires costly, large-scale human data collection.\nTo make it efficient, we present a framework for virtual respondent simulation\nusing LLMs. Our central idea is to account for mediators: factors through which\nthe same trait can give rise to varying responses to a survey item. By\nsimulating respondents with diverse mediators, we identify survey items that\nrobustly measure intended traits. Experiments on three psychological trait\ntheories (Big5, Schwartz, VIA) show that our mediator generation methods and\nsimulation framework effectively identify high-validity items. LLMs demonstrate\nthe ability to generate plausible mediators from trait definitions and to\nsimulate respondent behavior for item validation. Our problem formulation,\nmetrics, methodology, and dataset open a new direction for cost-effective\nsurvey development and a deeper understanding of how LLMs replicate human-like\nbehavior. We will publicly release our dataset and code to support future work.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As psychometric surveys are increasingly used to assess the traits of large\nlanguage models (LLMs), the need for scalable survey item generation suited for\nLLMs has also grown. A critical challenge here is ensuring the construct\nvalidity of generated items, i.e., whether they truly measure the intended\ntrait. Traditionally, this requires costly, large-scale human data collection.\nTo make it efficient, we present a framework for virtual respondent simulation\nusing LLMs. Our central idea is to account for mediators: factors through which\nthe same trait can give rise to varying responses to a survey item. By\nsimulating respondents with diverse mediators, we identify survey items that\nrobustly measure intended traits. Experiments on three psychological trait\ntheories (Big5, Schwartz, VIA) show that our mediator generation methods and\nsimulation framework effectively identify high-validity items. LLMs demonstrate\nthe ability to generate plausible mediators from trait definitions and to\nsimulate respondent behavior for item validation. Our problem formulation,\nmetrics, methodology, and dataset open a new direction for cost-effective\nsurvey development and a deeper understanding of how LLMs replicate human-like\nbehavior. We will publicly release our dataset and code to support future work."
                },
                "authors": [
                    {
                        "name": "Sungjib Lim"
                    },
                    {
                        "name": "Woojung Song"
                    },
                    {
                        "name": "Eun-Ju Lee"
                    },
                    {
                        "name": "Yohan Jo"
                    }
                ],
                "author_detail": {
                    "name": "Yohan Jo"
                },
                "author": "Yohan Jo",
                "arxiv_comment": "17 pages, 7 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.05890v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.05890v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.05886v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.05886v1",
                "updated": "2025-07-08T11:19:09Z",
                "updated_parsed": [
                    2025,
                    7,
                    8,
                    11,
                    19,
                    9,
                    1,
                    189,
                    0
                ],
                "published": "2025-07-08T11:19:09Z",
                "published_parsed": [
                    2025,
                    7,
                    8,
                    11,
                    19,
                    9,
                    1,
                    189,
                    0
                ],
                "title": "Current Practices for Building LLM-Powered Reasoning Tools Are Ad Hoc --\n  and We Can Do Better",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Current Practices for Building LLM-Powered Reasoning Tools Are Ad Hoc --\n  and We Can Do Better"
                },
                "summary": "There is growing excitement about building software verifiers, synthesizers,\nand other Automated Reasoning (AR) tools by combining traditional symbolic\nalgorithms and Large Language Models (LLMs). Unfortunately, the current\npractice for constructing such neurosymbolic AR systems is an ad hoc\nprogramming model that does not have the strong guarantees of traditional\nsymbolic algorithms, nor a deep enough synchronization of neural networks and\nsymbolic reasoning to unlock the full potential of LLM-powered reasoning. I\npropose Neurosymbolic Transition Systems as a principled computational model\nthat can underlie infrastructure for building neurosymbolic AR tools. In this\nmodel, symbolic state is paired with intuition, and state transitions operate\nover symbols and intuition in parallel. I argue why this new paradigm can scale\nlogical reasoning beyond current capabilities while retaining the strong\nguarantees of symbolic algorithms, and I sketch out how the computational model\nI propose can be reified in a logic programming language.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "There is growing excitement about building software verifiers, synthesizers,\nand other Automated Reasoning (AR) tools by combining traditional symbolic\nalgorithms and Large Language Models (LLMs). Unfortunately, the current\npractice for constructing such neurosymbolic AR systems is an ad hoc\nprogramming model that does not have the strong guarantees of traditional\nsymbolic algorithms, nor a deep enough synchronization of neural networks and\nsymbolic reasoning to unlock the full potential of LLM-powered reasoning. I\npropose Neurosymbolic Transition Systems as a principled computational model\nthat can underlie infrastructure for building neurosymbolic AR tools. In this\nmodel, symbolic state is paired with intuition, and state transitions operate\nover symbols and intuition in parallel. I argue why this new paradigm can scale\nlogical reasoning beyond current capabilities while retaining the strong\nguarantees of symbolic algorithms, and I sketch out how the computational model\nI propose can be reified in a logic programming language."
                },
                "authors": [
                    {
                        "name": "Aaron Bembenek"
                    }
                ],
                "author_detail": {
                    "name": "Aaron Bembenek"
                },
                "arxiv_affiliation": "The University of Melbourne",
                "author": "Aaron Bembenek",
                "arxiv_comment": "6 pages, 4 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.05886v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.05886v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.12023v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.12023v3",
                "updated": "2025-07-08T11:15:12Z",
                "updated_parsed": [
                    2025,
                    7,
                    8,
                    11,
                    15,
                    12,
                    1,
                    189,
                    0
                ],
                "published": "2024-11-18T20:03:36Z",
                "published_parsed": [
                    2024,
                    11,
                    18,
                    20,
                    3,
                    36,
                    0,
                    323,
                    0
                ],
                "title": "Exploring HOD-dependent systematics for the DESI 2024 Full-Shape galaxy\n  clustering analysis",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Exploring HOD-dependent systematics for the DESI 2024 Full-Shape galaxy\n  clustering analysis"
                },
                "summary": "We analyse the robustness of the DESI 2024 cosmological inference from the\nfull shape of the galaxy power spectrum to uncertainties in the Halo Occupation\nDistribution (HOD) model of the galaxy-halo connection and the choice of priors\non nuisance parameters. We assess variations in the recovered cosmological\nparameters across a range of mocks populated with different HOD models and find\nthat shifts are often greater than 20\\% of the expected statistical\nuncertainties from the DESI data. We encapsulate the effect of such shifts in\nterms of a systematic covariance term, $\\mathsf{C}_{\\rm HOD}$, and an\nadditional diagonal contribution quantifying the impact of our choice of\nnuisance parameter priors on the ability of the effective field theory (EFT)\nmodel to correctly recover the cosmological parameters of the simulations.\nThese two covariance contributions are designed to be added to the usual\ncovariance term, $\\mathsf{C}_{\\rm stat}$, describing the statistical\nuncertainty in the power spectrum measurement, in order to fairly represent\nthese sources of systematic uncertainty. This novel approach should be more\ngeneral and robust to the choice of model or additional external datasets used\nin cosmological fits than the alternative approach of adding systematic\nuncertainties to the recovered marginalised parameter posteriors. We compare\nthe approaches within the context of a fixed $\\Lambda$CDM model and demonstrate\nthat our method gives conservative estimates of the systematic uncertainty that\nnevertheless have little impact on the final posteriors obtained from DESI\ndata.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We analyse the robustness of the DESI 2024 cosmological inference from the\nfull shape of the galaxy power spectrum to uncertainties in the Halo Occupation\nDistribution (HOD) model of the galaxy-halo connection and the choice of priors\non nuisance parameters. We assess variations in the recovered cosmological\nparameters across a range of mocks populated with different HOD models and find\nthat shifts are often greater than 20\\% of the expected statistical\nuncertainties from the DESI data. We encapsulate the effect of such shifts in\nterms of a systematic covariance term, $\\mathsf{C}_{\\rm HOD}$, and an\nadditional diagonal contribution quantifying the impact of our choice of\nnuisance parameter priors on the ability of the effective field theory (EFT)\nmodel to correctly recover the cosmological parameters of the simulations.\nThese two covariance contributions are designed to be added to the usual\ncovariance term, $\\mathsf{C}_{\\rm stat}$, describing the statistical\nuncertainty in the power spectrum measurement, in order to fairly represent\nthese sources of systematic uncertainty. This novel approach should be more\ngeneral and robust to the choice of model or additional external datasets used\nin cosmological fits than the alternative approach of adding systematic\nuncertainties to the recovered marginalised parameter posteriors. We compare\nthe approaches within the context of a fixed $\\Lambda$CDM model and demonstrate\nthat our method gives conservative estimates of the systematic uncertainty that\nnevertheless have little impact on the final posteriors obtained from DESI\ndata."
                },
                "authors": [
                    {
                        "name": "N. Findlay"
                    },
                    {
                        "name": "S. Nadathur"
                    },
                    {
                        "name": "W. J. Percival"
                    },
                    {
                        "name": "A. de Mattia"
                    },
                    {
                        "name": "P. Zarrouk"
                    },
                    {
                        "name": "H. Gil-MarÃ­n"
                    },
                    {
                        "name": "O. Alves"
                    },
                    {
                        "name": "J. Mena-FernÃ¡ndez"
                    },
                    {
                        "name": "C. Garcia-Quintero"
                    },
                    {
                        "name": "A. Rocher"
                    },
                    {
                        "name": "S. Ahlen"
                    },
                    {
                        "name": "D. Bianchi"
                    },
                    {
                        "name": "D. Brooks"
                    },
                    {
                        "name": "T. Claybaugh"
                    },
                    {
                        "name": "S. Cole"
                    },
                    {
                        "name": "A. de la Macorra"
                    },
                    {
                        "name": "Arjun Dey"
                    },
                    {
                        "name": "P. Doel"
                    },
                    {
                        "name": "K. Fanning"
                    },
                    {
                        "name": "A. Font-Ribera"
                    },
                    {
                        "name": "J. E. Forero-Romero"
                    },
                    {
                        "name": "E. GaztaÃ±aga"
                    },
                    {
                        "name": "G. Gutierrez"
                    },
                    {
                        "name": "C. Hahn"
                    },
                    {
                        "name": "K. Honscheid"
                    },
                    {
                        "name": "C. Howlett"
                    },
                    {
                        "name": "S. Juneau"
                    },
                    {
                        "name": "M. E. Levi"
                    },
                    {
                        "name": "A. Meisner"
                    },
                    {
                        "name": "R. Miquel"
                    },
                    {
                        "name": "J. Moustakas"
                    },
                    {
                        "name": "N. Palanque-Delabrouille"
                    },
                    {
                        "name": "I. PÃ©rez-RÃ fols"
                    },
                    {
                        "name": "G. Rossi"
                    },
                    {
                        "name": "E. Sanchez"
                    },
                    {
                        "name": "D. Schlegel"
                    },
                    {
                        "name": "M. Schubnell"
                    },
                    {
                        "name": "H. Seo"
                    },
                    {
                        "name": "D. Sprayberry"
                    },
                    {
                        "name": "G. TarlÃ©"
                    },
                    {
                        "name": "M. Vargas-MagaÃ±a"
                    },
                    {
                        "name": "B. A. Weaver"
                    }
                ],
                "author_detail": {
                    "name": "B. A. Weaver"
                },
                "author": "B. A. Weaver",
                "arxiv_comment": "This DESI Collaboration Publication is part of the 2024 publication\n  series using the first year of observations (see\n  https://data.desi.lbl.gov/doc/papers/). 26 pages, 10 figures. Accepted for\n  publication in JCAP",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.12023v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.12023v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.CO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.CO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.05881v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.05881v2",
                "updated": "2025-07-09T08:22:34Z",
                "updated_parsed": [
                    2025,
                    7,
                    9,
                    8,
                    22,
                    34,
                    2,
                    190,
                    0
                ],
                "published": "2025-07-08T11:06:42Z",
                "published_parsed": [
                    2025,
                    7,
                    8,
                    11,
                    6,
                    42,
                    1,
                    189,
                    0
                ],
                "title": "Comment on \"Discovery and Preliminary Characterization of a Third\n  Interstellar Object: 3I/ATLAS\" [arXiv:2507.02757]",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Comment on \"Discovery and Preliminary Characterization of a Third\n  Interstellar Object: 3I/ATLAS\" [arXiv:2507.02757]"
                },
                "summary": "The interstellar object 3I/ATLAS shows a weak cometary activity. Its\nbrightness suggests a maximum radius of ~10km (A/0.05)^{-1/2} for an asteroid\nwith an albedo A. I show that interstellar objects with that radius would\namount to an interstellar mass density that is well above the expected mass\nbudget of interstellar comets or asteroids. Given this budget, the detection\nrate of objects like 3I/ATLAS implies that it is a comet with a small core\nradius <0.6km, or a member of a rare population with a number density\n<5x10^{-8}au^{-3} for R>10km. The second possibility would suggest that the\nrare population of 3I/ATLAS objects favors plunging orbits towards the inner\nsolar system to accommodate their inferred detection rate.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The interstellar object 3I/ATLAS shows a weak cometary activity. Its\nbrightness suggests a maximum radius of ~10km (A/0.05)^{-1/2} for an asteroid\nwith an albedo A. I show that interstellar objects with that radius would\namount to an interstellar mass density that is well above the expected mass\nbudget of interstellar comets or asteroids. Given this budget, the detection\nrate of objects like 3I/ATLAS implies that it is a comet with a small core\nradius <0.6km, or a member of a rare population with a number density\n<5x10^{-8}au^{-3} for R>10km. The second possibility would suggest that the\nrare population of 3I/ATLAS objects favors plunging orbits towards the inner\nsolar system to accommodate their inferred detection rate."
                },
                "authors": [
                    {
                        "name": "Abraham Loeb"
                    }
                ],
                "author_detail": {
                    "name": "Abraham Loeb"
                },
                "arxiv_affiliation": "Harvard",
                "author": "Abraham Loeb",
                "arxiv_comment": "Comment on \"Discovery and Preliminary Characterization of a Third\n  Interstellar Object: 3I/ATLAS\" [arXiv:2507.02757]",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.05881v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.05881v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.EP",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.EP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "astro-ph.GA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.05602v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.05602v2",
                "updated": "2025-07-08T11:06:22Z",
                "updated_parsed": [
                    2025,
                    7,
                    8,
                    11,
                    6,
                    22,
                    1,
                    189,
                    0
                ],
                "published": "2025-05-08T19:05:02Z",
                "published_parsed": [
                    2025,
                    5,
                    8,
                    19,
                    5,
                    2,
                    3,
                    128,
                    0
                ],
                "title": "HiBayES: A Hierarchical Bayesian Modeling Framework for AI Evaluation\n  Statistics",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "HiBayES: A Hierarchical Bayesian Modeling Framework for AI Evaluation\n  Statistics"
                },
                "summary": "As Large Language Models (LLMs) and other AI systems evolve, robustly\nestimating their capabilities from inherently stochastic outputs while\nsystematically quantifying uncertainty in these estimates becomes increasingly\nimportant. Further, advanced AI evaluations often have a nested hierarchical\nstructure, exhibit high levels of complexity, and come with high costs in\ntesting the most advanced AI systems. To address these challenges, we introduce\nHiBayES, a generalizable Hierarchical Bayesian modeling framework for AI\nEvaluation Statistics. HiBayES supports robust inferences in classical\nquestion-answer benchmarks and advanced agentic evaluations, particularly in\nlow-data scenarios (e.g., < 20 data points per evaluation). Built on\nGeneralized Linear Models (GLMs), Bayesian data analysis, and formal model\ncomparison, HiBayES provides principled uncertainty quantification and robust\nparameter estimation. This paper offers a comprehensive introduction to\nHiBayES, including illustrative examples, comparisons to conventional\nstatistical methods, and practical guidance for implementing multilevel\nBayesian GLMs. Additionally, we provide a HiBayES software package [4] (Beta\nversion) for out-of-the-box implementation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As Large Language Models (LLMs) and other AI systems evolve, robustly\nestimating their capabilities from inherently stochastic outputs while\nsystematically quantifying uncertainty in these estimates becomes increasingly\nimportant. Further, advanced AI evaluations often have a nested hierarchical\nstructure, exhibit high levels of complexity, and come with high costs in\ntesting the most advanced AI systems. To address these challenges, we introduce\nHiBayES, a generalizable Hierarchical Bayesian modeling framework for AI\nEvaluation Statistics. HiBayES supports robust inferences in classical\nquestion-answer benchmarks and advanced agentic evaluations, particularly in\nlow-data scenarios (e.g., < 20 data points per evaluation). Built on\nGeneralized Linear Models (GLMs), Bayesian data analysis, and formal model\ncomparison, HiBayES provides principled uncertainty quantification and robust\nparameter estimation. This paper offers a comprehensive introduction to\nHiBayES, including illustrative examples, comparisons to conventional\nstatistical methods, and practical guidance for implementing multilevel\nBayesian GLMs. Additionally, we provide a HiBayES software package [4] (Beta\nversion) for out-of-the-box implementation."
                },
                "authors": [
                    {
                        "name": "Lennart Luettgau"
                    },
                    {
                        "name": "Harry Coppock"
                    },
                    {
                        "name": "Magda Dubois"
                    },
                    {
                        "name": "Christopher Summerfield"
                    },
                    {
                        "name": "Cozmin Ududec"
                    }
                ],
                "author_detail": {
                    "name": "Cozmin Ududec"
                },
                "author": "Cozmin Ududec",
                "arxiv_comment": "23 pages, 9 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.05602v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.05602v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.AP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.05880v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.05880v1",
                "updated": "2025-07-08T11:04:17Z",
                "updated_parsed": [
                    2025,
                    7,
                    8,
                    11,
                    4,
                    17,
                    1,
                    189,
                    0
                ],
                "published": "2025-07-08T11:04:17Z",
                "published_parsed": [
                    2025,
                    7,
                    8,
                    11,
                    4,
                    17,
                    1,
                    189,
                    0
                ],
                "title": "RecRankerEval: A Flexible and Extensible Framework for Top-k LLM-based\n  Recommendation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RecRankerEval: A Flexible and Extensible Framework for Top-k LLM-based\n  Recommendation"
                },
                "summary": "A recent Large language model (LLM)-based recommendation model, called\nRecRanker, has demonstrated a superior performance in the top-k recommendation\ntask compared to other models. In particular, RecRanker samples users via\nclustering, generates an initial ranking list using an initial recommendation\nmodel, and fine-tunes an LLM through hybrid instruction tuning to infer user\npreferences. However, the contribution of each core component remains\nunderexplored. In this work, we inspect the reproducibility of RecRanker, and\nstudy the impact and role of its various components. We begin by reproducing\nthe RecRanker pipeline through the implementation of all its key components.\nOur reproduction shows that the pairwise and listwise methods achieve a\nperformance comparable to that reported in the original paper. For the\npointwise method, while we are also able to reproduce the original paper's\nresults, further analysis shows that the performance is abnormally high due to\ndata leakage from the inclusion of ground-truth information in the prompts. To\nenable a fair and comprehensive evaluation of LLM-based top-k recommendations,\nwe propose RecRankerEval, an extensible framework that covers five key\ndimensions: user sampling strategy, initial recommendation model, LLM backbone,\ndataset selection, and instruction tuning method. Using the RecRankerEval\nframework, we show that the original results of RecRanker can be reproduced on\nthe ML-100K and ML-1M datasets, as well as the additional Amazon-Music dataset,\nbut not on BookCrossing due to the lack of timestamp information in the\noriginal RecRanker paper. Furthermore, we demonstrate that RecRanker's\nperformance can be improved by employing alternative user sampling methods,\nstronger initial recommenders, and more capable LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A recent Large language model (LLM)-based recommendation model, called\nRecRanker, has demonstrated a superior performance in the top-k recommendation\ntask compared to other models. In particular, RecRanker samples users via\nclustering, generates an initial ranking list using an initial recommendation\nmodel, and fine-tunes an LLM through hybrid instruction tuning to infer user\npreferences. However, the contribution of each core component remains\nunderexplored. In this work, we inspect the reproducibility of RecRanker, and\nstudy the impact and role of its various components. We begin by reproducing\nthe RecRanker pipeline through the implementation of all its key components.\nOur reproduction shows that the pairwise and listwise methods achieve a\nperformance comparable to that reported in the original paper. For the\npointwise method, while we are also able to reproduce the original paper's\nresults, further analysis shows that the performance is abnormally high due to\ndata leakage from the inclusion of ground-truth information in the prompts. To\nenable a fair and comprehensive evaluation of LLM-based top-k recommendations,\nwe propose RecRankerEval, an extensible framework that covers five key\ndimensions: user sampling strategy, initial recommendation model, LLM backbone,\ndataset selection, and instruction tuning method. Using the RecRankerEval\nframework, we show that the original results of RecRanker can be reproduced on\nthe ML-100K and ML-1M datasets, as well as the additional Amazon-Music dataset,\nbut not on BookCrossing due to the lack of timestamp information in the\noriginal RecRanker paper. Furthermore, we demonstrate that RecRanker's\nperformance can be improved by employing alternative user sampling methods,\nstronger initial recommenders, and more capable LLMs."
                },
                "authors": [
                    {
                        "name": "Zeyuan Meng"
                    },
                    {
                        "name": "Zixuan Yi"
                    },
                    {
                        "name": "Iadh Ounis"
                    }
                ],
                "author_detail": {
                    "name": "Iadh Ounis"
                },
                "author": "Iadh Ounis",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.05880v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.05880v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.11211v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.11211v2",
                "updated": "2025-07-08T10:51:36Z",
                "updated_parsed": [
                    2025,
                    7,
                    8,
                    10,
                    51,
                    36,
                    1,
                    189,
                    0
                ],
                "published": "2025-05-16T13:06:25Z",
                "published_parsed": [
                    2025,
                    5,
                    16,
                    13,
                    6,
                    25,
                    4,
                    136,
                    0
                ],
                "title": "Bayesian Hierarchical Invariant Prediction",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Bayesian Hierarchical Invariant Prediction"
                },
                "summary": "We propose Bayesian Hierarchical Invariant Prediction (BHIP) reframing\nInvariant Causal Prediction (ICP) through the lens of Hierarchical Bayes. We\nleverage the hierarchical structure to explicitly test invariance of causal\nmechanisms under heterogeneous data, resulting in improved computational\nscalability for a larger number of predictors compared to ICP. Moreover, given\nits Bayesian nature BHIP enables the use of prior information. In this paper,\nwe test two sparsity inducing priors: horseshoe and spike-and-slab, both of\nwhich allow us a more reliable identification of causal features. We test BHIP\nin synthetic and real-world data showing its potential as an alternative\ninference method to ICP.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We propose Bayesian Hierarchical Invariant Prediction (BHIP) reframing\nInvariant Causal Prediction (ICP) through the lens of Hierarchical Bayes. We\nleverage the hierarchical structure to explicitly test invariance of causal\nmechanisms under heterogeneous data, resulting in improved computational\nscalability for a larger number of predictors compared to ICP. Moreover, given\nits Bayesian nature BHIP enables the use of prior information. In this paper,\nwe test two sparsity inducing priors: horseshoe and spike-and-slab, both of\nwhich allow us a more reliable identification of causal features. We test BHIP\nin synthetic and real-world data showing its potential as an alternative\ninference method to ICP."
                },
                "authors": [
                    {
                        "name": "Francisco Madaleno"
                    },
                    {
                        "name": "Pernille Julie Viuff Sand"
                    },
                    {
                        "name": "Francisco C. Pereira"
                    },
                    {
                        "name": "Sergio Hernan Garrido Mejia"
                    }
                ],
                "author_detail": {
                    "name": "Sergio Hernan Garrido Mejia"
                },
                "author": "Sergio Hernan Garrido Mejia",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.11211v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.11211v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.05866v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.05866v1",
                "updated": "2025-07-08T10:47:10Z",
                "updated_parsed": [
                    2025,
                    7,
                    8,
                    10,
                    47,
                    10,
                    1,
                    189,
                    0
                ],
                "published": "2025-07-08T10:47:10Z",
                "published_parsed": [
                    2025,
                    7,
                    8,
                    10,
                    47,
                    10,
                    1,
                    189,
                    0
                ],
                "title": "Understanding support for AI regulation: A Bayesian network perspective",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Understanding support for AI regulation: A Bayesian network perspective"
                },
                "summary": "As artificial intelligence (AI) becomes increasingly embedded in public and\nprivate life, understanding how citizens perceive its risks, benefits, and\nregulatory needs is essential. To inform ongoing regulatory efforts such as the\nEuropean Union's proposed AI Act, this study models public attitudes using\nBayesian networks learned from the nationally representative 2023 German survey\nCurrent Questions on AI. The survey includes variables on AI interest,\nexposure, perceived threats and opportunities, awareness of EU regulation, and\nsupport for legal restrictions, along with key demographic and political\nindicators. We estimate probabilistic models that reveal how personal\nengagement and techno-optimism shape public perceptions, and how political\norientation and age influence regulatory attitudes. Sobol indices and\nconditional inference identify belief patterns and scenario-specific responses\nacross population profiles. We show that awareness of regulation is driven by\ninformation-seeking behavior, while support for legal requirements depends\nstrongly on perceived policy adequacy and political alignment. Our approach\noffers a transparent, data-driven framework for identifying which public\nsegments are most responsive to AI policy initiatives, providing insights to\ninform risk communication and governance strategies. We illustrate this through\na focused analysis of support for AI regulation, quantifying the influence of\npolitical ideology, perceived risks, and regulatory awareness under different\nscenarios.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As artificial intelligence (AI) becomes increasingly embedded in public and\nprivate life, understanding how citizens perceive its risks, benefits, and\nregulatory needs is essential. To inform ongoing regulatory efforts such as the\nEuropean Union's proposed AI Act, this study models public attitudes using\nBayesian networks learned from the nationally representative 2023 German survey\nCurrent Questions on AI. The survey includes variables on AI interest,\nexposure, perceived threats and opportunities, awareness of EU regulation, and\nsupport for legal restrictions, along with key demographic and political\nindicators. We estimate probabilistic models that reveal how personal\nengagement and techno-optimism shape public perceptions, and how political\norientation and age influence regulatory attitudes. Sobol indices and\nconditional inference identify belief patterns and scenario-specific responses\nacross population profiles. We show that awareness of regulation is driven by\ninformation-seeking behavior, while support for legal requirements depends\nstrongly on perceived policy adequacy and political alignment. Our approach\noffers a transparent, data-driven framework for identifying which public\nsegments are most responsive to AI policy initiatives, providing insights to\ninform risk communication and governance strategies. We illustrate this through\na focused analysis of support for AI regulation, quantifying the influence of\npolitical ideology, perceived risks, and regulatory awareness under different\nscenarios."
                },
                "authors": [
                    {
                        "name": "Andrea Cremaschi"
                    },
                    {
                        "name": "Dae-Jin Lee"
                    },
                    {
                        "name": "Manuele Leonelli"
                    }
                ],
                "author_detail": {
                    "name": "Manuele Leonelli"
                },
                "author": "Manuele Leonelli",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.05866v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.05866v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.05863v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.05863v1",
                "updated": "2025-07-08T10:44:27Z",
                "updated_parsed": [
                    2025,
                    7,
                    8,
                    10,
                    44,
                    27,
                    1,
                    189,
                    0
                ],
                "published": "2025-07-08T10:44:27Z",
                "published_parsed": [
                    2025,
                    7,
                    8,
                    10,
                    44,
                    27,
                    1,
                    189,
                    0
                ],
                "title": "KERAG_R: Knowledge-Enhanced Retrieval-Augmented Generation for\n  Recommendation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "KERAG_R: Knowledge-Enhanced Retrieval-Augmented Generation for\n  Recommendation"
                },
                "summary": "Large Language Models (LLMs) have shown strong potential in recommender\nsystems due to their contextual learning and generalisation capabilities.\nExisting LLM-based recommendation approaches typically formulate the\nrecommendation task using specialised prompts designed to leverage their\ncontextual abilities, and aligning their outputs closely with human preferences\nto yield an improved recommendation performance. However, the use of LLMs for\nrecommendation tasks is limited by the absence of domain-specific knowledge.\nThis lack of relevant relational knowledge about the items to be recommended in\nthe LLM's pre-training corpus can lead to inaccuracies or hallucinations,\nresulting in incorrect or misleading recommendations. Moreover, directly using\ninformation from the knowledge graph introduces redundant and noisy\ninformation, which can affect the LLM's reasoning process or exceed its input\ncontext length, thereby reducing the performance of LLM-based recommendations.\nTo address the lack of domain-specific knowledge, we propose a novel model\ncalled Knowledge-Enhanced Retrieval-Augmented Generation for Recommendation\n(KERAG_R). Specifically, we leverage a graph retrieval-augmented generation\n(GraphRAG) component to integrate additional information from a knowledge graph\n(KG) into instructions, enabling the LLM to collaboratively exploit\nrecommendation signals from both text-based user interactions and the knowledge\ngraph to better estimate the users' preferences in a recommendation context. In\nparticular, we perform graph RAG by pre-training a graph attention network\n(GAT) to select the most relevant triple for the target users for the used LLM,\nthereby enhancing the LLM while reducing redundant and noisy information. Our\nextensive experiments on three public datasets show that our proposed KERAG_R\nmodel significantly outperforms ten existing state-of-the-art recommendation\nmethods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have shown strong potential in recommender\nsystems due to their contextual learning and generalisation capabilities.\nExisting LLM-based recommendation approaches typically formulate the\nrecommendation task using specialised prompts designed to leverage their\ncontextual abilities, and aligning their outputs closely with human preferences\nto yield an improved recommendation performance. However, the use of LLMs for\nrecommendation tasks is limited by the absence of domain-specific knowledge.\nThis lack of relevant relational knowledge about the items to be recommended in\nthe LLM's pre-training corpus can lead to inaccuracies or hallucinations,\nresulting in incorrect or misleading recommendations. Moreover, directly using\ninformation from the knowledge graph introduces redundant and noisy\ninformation, which can affect the LLM's reasoning process or exceed its input\ncontext length, thereby reducing the performance of LLM-based recommendations.\nTo address the lack of domain-specific knowledge, we propose a novel model\ncalled Knowledge-Enhanced Retrieval-Augmented Generation for Recommendation\n(KERAG_R). Specifically, we leverage a graph retrieval-augmented generation\n(GraphRAG) component to integrate additional information from a knowledge graph\n(KG) into instructions, enabling the LLM to collaboratively exploit\nrecommendation signals from both text-based user interactions and the knowledge\ngraph to better estimate the users' preferences in a recommendation context. In\nparticular, we perform graph RAG by pre-training a graph attention network\n(GAT) to select the most relevant triple for the target users for the used LLM,\nthereby enhancing the LLM while reducing redundant and noisy information. Our\nextensive experiments on three public datasets show that our proposed KERAG_R\nmodel significantly outperforms ten existing state-of-the-art recommendation\nmethods."
                },
                "authors": [
                    {
                        "name": "Zeyuan Meng"
                    },
                    {
                        "name": "Zixuan Yi"
                    },
                    {
                        "name": "Iadh Ounis"
                    }
                ],
                "author_detail": {
                    "name": "Iadh Ounis"
                },
                "author": "Iadh Ounis",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.05863v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.05863v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.03160v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.03160v3",
                "updated": "2025-07-09T06:49:35Z",
                "updated_parsed": [
                    2025,
                    7,
                    9,
                    6,
                    49,
                    35,
                    2,
                    190,
                    0
                ],
                "published": "2025-07-03T20:32:36Z",
                "published_parsed": [
                    2025,
                    7,
                    3,
                    20,
                    32,
                    36,
                    3,
                    184,
                    0
                ],
                "title": "Assessing Small Language Models for Code Generation: An Empirical Study\n  with Benchmarks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Assessing Small Language Models for Code Generation: An Empirical Study\n  with Benchmarks"
                },
                "summary": "The recent advancements of Small Language Models (SLMs) have opened new\npossibilities for efficient code generation. SLMs offer lightweight and\ncost-effective alternatives to Large Language Models (LLMs), making them\nattractive for use in resource-constrained environments. However, empirical\nunderstanding of SLMs, particularly their capabilities, limitations, and\nperformance trade-offs in code generation remains limited. This study presents\na comprehensive empirical evaluation of 20 open-source SLMs ranging from 0.4B\nto 10B parameters on five diverse code-related benchmarks (HumanEval, MBPP,\nMercury, HumanEvalPack, and CodeXGLUE). The models are assessed along three\ndimensions: i) functional correctness of generated code, ii) computational\nefficiency and iii) performance across multiple programming languages. The\nfindings of this study reveal that several compact SLMs achieve competitive\nresults while maintaining a balance between performance and efficiency, making\nthem viable for deployment in resource-constrained environments. However,\nachieving further improvements in accuracy requires switching to larger models.\nThese models generally outperform their smaller counterparts, but they require\nmuch more computational power. We observe that for 10% performance\nimprovements, models can require nearly a 4x increase in VRAM consumption,\nhighlighting a trade-off between effectiveness and scalability. Besides, the\nmultilingual performance analysis reveals that SLMs tend to perform better in\nlanguages such as Python, Java, and PHP, while exhibiting relatively weaker\nperformance in Go, C++, and Ruby. However, statistical analysis suggests these\ndifferences are not significant, indicating a generalizability of SLMs across\nprogramming languages. Based on the findings, this work provides insights into\nthe design and selection of SLMs for real-world code generation tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The recent advancements of Small Language Models (SLMs) have opened new\npossibilities for efficient code generation. SLMs offer lightweight and\ncost-effective alternatives to Large Language Models (LLMs), making them\nattractive for use in resource-constrained environments. However, empirical\nunderstanding of SLMs, particularly their capabilities, limitations, and\nperformance trade-offs in code generation remains limited. This study presents\na comprehensive empirical evaluation of 20 open-source SLMs ranging from 0.4B\nto 10B parameters on five diverse code-related benchmarks (HumanEval, MBPP,\nMercury, HumanEvalPack, and CodeXGLUE). The models are assessed along three\ndimensions: i) functional correctness of generated code, ii) computational\nefficiency and iii) performance across multiple programming languages. The\nfindings of this study reveal that several compact SLMs achieve competitive\nresults while maintaining a balance between performance and efficiency, making\nthem viable for deployment in resource-constrained environments. However,\nachieving further improvements in accuracy requires switching to larger models.\nThese models generally outperform their smaller counterparts, but they require\nmuch more computational power. We observe that for 10% performance\nimprovements, models can require nearly a 4x increase in VRAM consumption,\nhighlighting a trade-off between effectiveness and scalability. Besides, the\nmultilingual performance analysis reveals that SLMs tend to perform better in\nlanguages such as Python, Java, and PHP, while exhibiting relatively weaker\nperformance in Go, C++, and Ruby. However, statistical analysis suggests these\ndifferences are not significant, indicating a generalizability of SLMs across\nprogramming languages. Based on the findings, this work provides insights into\nthe design and selection of SLMs for real-world code generation tasks."
                },
                "authors": [
                    {
                        "name": "Md Mahade Hasan"
                    },
                    {
                        "name": "Muhammad Waseem"
                    },
                    {
                        "name": "Kai-Kristian Kemell"
                    },
                    {
                        "name": "Jussi Rasku"
                    },
                    {
                        "name": "Juha Ala-Rantala"
                    },
                    {
                        "name": "Pekka Abrahamsson"
                    }
                ],
                "author_detail": {
                    "name": "Pekka Abrahamsson"
                },
                "author": "Pekka Abrahamsson",
                "arxiv_comment": "17 pages, 10 Tables, 57 figures. Includes benchmarks and multilingual\n  evaluation. Submitted to the Journal of Systems and Software",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.03160v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.03160v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.05829v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.05829v1",
                "updated": "2025-07-08T09:50:57Z",
                "updated_parsed": [
                    2025,
                    7,
                    8,
                    9,
                    50,
                    57,
                    1,
                    189,
                    0
                ],
                "published": "2025-07-08T09:50:57Z",
                "published_parsed": [
                    2025,
                    7,
                    8,
                    9,
                    50,
                    57,
                    1,
                    189,
                    0
                ],
                "title": "Intra-DP: A High Performance Collaborative Inference System for Mobile\n  Edge Computing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Intra-DP: A High Performance Collaborative Inference System for Mobile\n  Edge Computing"
                },
                "summary": "Deploying deep neural networks (DNNs) on resource-constrained mobile devices\npresents significant challenges, particularly in achieving real-time\nperformance while simultaneously coping with limited computational resources\nand battery life. While Mobile Edge Computing (MEC) offers collaborative\ninference with GPU servers as a promising solution, existing approaches\nprimarily rely on layer-wise model partitioning and undergo significant\ntransmission bottlenecks caused by the sequential execution of DNN operations.\nTo address this challenge, we present Intra-DP, a high-performance\ncollaborative inference system optimized for DNN inference on MEC. Intra DP\nemploys a novel parallel computing technique based on local operators (i.e.,\noperators whose minimum unit input is not the entire input tensor, such as the\nconvolution kernel). By decomposing their computations (operations) into\nseveral independent sub-operations and overlapping the computation and\ntransmission of different sub-operations through parallel execution, Intra-DP\nmitigates transmission bottlenecks in MEC, achieving fast and energy-efficient\ninference. The evaluation demonstrates that Intra-DP reduces per-inference\nlatency by up to 50% and energy consumption by up to 75% compared to\nstate-of-the-art baselines, without sacrificing accuracy.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Deploying deep neural networks (DNNs) on resource-constrained mobile devices\npresents significant challenges, particularly in achieving real-time\nperformance while simultaneously coping with limited computational resources\nand battery life. While Mobile Edge Computing (MEC) offers collaborative\ninference with GPU servers as a promising solution, existing approaches\nprimarily rely on layer-wise model partitioning and undergo significant\ntransmission bottlenecks caused by the sequential execution of DNN operations.\nTo address this challenge, we present Intra-DP, a high-performance\ncollaborative inference system optimized for DNN inference on MEC. Intra DP\nemploys a novel parallel computing technique based on local operators (i.e.,\noperators whose minimum unit input is not the entire input tensor, such as the\nconvolution kernel). By decomposing their computations (operations) into\nseveral independent sub-operations and overlapping the computation and\ntransmission of different sub-operations through parallel execution, Intra-DP\nmitigates transmission bottlenecks in MEC, achieving fast and energy-efficient\ninference. The evaluation demonstrates that Intra-DP reduces per-inference\nlatency by up to 50% and energy consumption by up to 75% compared to\nstate-of-the-art baselines, without sacrificing accuracy."
                },
                "authors": [
                    {
                        "name": "Zekai Sun"
                    },
                    {
                        "name": "Xiuxian Guan"
                    },
                    {
                        "name": "Zheng Lin"
                    },
                    {
                        "name": "Zihan Fang"
                    },
                    {
                        "name": "Xiangming Cai"
                    },
                    {
                        "name": "Zhe Chen"
                    },
                    {
                        "name": "Fangming Liu"
                    },
                    {
                        "name": "Heming Cui"
                    },
                    {
                        "name": "Jie Xiong"
                    },
                    {
                        "name": "Wei Ni"
                    },
                    {
                        "name": "Chau Yuen"
                    }
                ],
                "author_detail": {
                    "name": "Chau Yuen"
                },
                "author": "Chau Yuen",
                "arxiv_comment": "14 pages, 19 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.05829v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.05829v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.18099v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.18099v2",
                "updated": "2025-07-08T09:49:57Z",
                "updated_parsed": [
                    2025,
                    7,
                    8,
                    9,
                    49,
                    57,
                    1,
                    189,
                    0
                ],
                "published": "2025-01-30T02:21:59Z",
                "published_parsed": [
                    2025,
                    1,
                    30,
                    2,
                    21,
                    59,
                    3,
                    30,
                    0
                ],
                "title": "Learning to Plan & Reason for Evaluation with Thinking-LLM-as-a-Judge",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Learning to Plan & Reason for Evaluation with Thinking-LLM-as-a-Judge"
                },
                "summary": "LLM-as-a-Judge models generate chain-of-thought (CoT) sequences intended to\ncapture the step-bystep reasoning process that underlies the final evaluation\nof a response. However, due to the lack of human annotated CoTs for evaluation,\nthe required components and structure of effective reasoning traces remain\nunderstudied. Consequently, previous approaches often (1) constrain reasoning\ntraces to hand-designed components, such as a list of criteria, reference\nanswers, or verification questions and (2) structure them such that planning is\nintertwined with the reasoning for evaluation. In this work, we propose\nEvalPlanner, a preference optimization algorithm for Thinking-LLM-as-a-Judge\nthat first generates an unconstrained evaluation plan, followed by its\nexecution, and then the final judgment. In a self-training loop, EvalPlanner\niteratively optimizes over synthetically constructed evaluation plans and\nexecutions, leading to better final verdicts. Our method achieves a new\nstate-of-the-art performance for generative reward models on RewardBench (with\na score of 93.9), despite being trained on fewer amount of, and synthetically\ngenerated, preference pairs. Additional experiments on other benchmarks like\nRM-Bench, JudgeBench, and FollowBenchEval further highlight the utility of both\nplanning and reasoning for building robust LLM-as-a-Judge reasoning models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM-as-a-Judge models generate chain-of-thought (CoT) sequences intended to\ncapture the step-bystep reasoning process that underlies the final evaluation\nof a response. However, due to the lack of human annotated CoTs for evaluation,\nthe required components and structure of effective reasoning traces remain\nunderstudied. Consequently, previous approaches often (1) constrain reasoning\ntraces to hand-designed components, such as a list of criteria, reference\nanswers, or verification questions and (2) structure them such that planning is\nintertwined with the reasoning for evaluation. In this work, we propose\nEvalPlanner, a preference optimization algorithm for Thinking-LLM-as-a-Judge\nthat first generates an unconstrained evaluation plan, followed by its\nexecution, and then the final judgment. In a self-training loop, EvalPlanner\niteratively optimizes over synthetically constructed evaluation plans and\nexecutions, leading to better final verdicts. Our method achieves a new\nstate-of-the-art performance for generative reward models on RewardBench (with\na score of 93.9), despite being trained on fewer amount of, and synthetically\ngenerated, preference pairs. Additional experiments on other benchmarks like\nRM-Bench, JudgeBench, and FollowBenchEval further highlight the utility of both\nplanning and reasoning for building robust LLM-as-a-Judge reasoning models."
                },
                "authors": [
                    {
                        "name": "Swarnadeep Saha"
                    },
                    {
                        "name": "Xian Li"
                    },
                    {
                        "name": "Marjan Ghazvininejad"
                    },
                    {
                        "name": "Jason Weston"
                    },
                    {
                        "name": "Tianlu Wang"
                    }
                ],
                "author_detail": {
                    "name": "Tianlu Wang"
                },
                "author": "Tianlu Wang",
                "arxiv_comment": "ICML 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.18099v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.18099v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.04436v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.04436v2",
                "updated": "2025-07-08T09:48:54Z",
                "updated_parsed": [
                    2025,
                    7,
                    8,
                    9,
                    48,
                    54,
                    1,
                    189,
                    0
                ],
                "published": "2025-04-06T10:22:37Z",
                "published_parsed": [
                    2025,
                    4,
                    6,
                    10,
                    22,
                    37,
                    6,
                    96,
                    0
                ],
                "title": "Intertwined geometries in collective modes of two dimensional Dirac\n  fermions",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Intertwined geometries in collective modes of two dimensional Dirac\n  fermions"
                },
                "summary": "It is well known that the time-dependent response of a correlated system can\nbe inferred from its spectral correlation functions. As a textbook example, the\nzero sound collective modes of a Fermi liquid appear as poles of its\nparticle-hole susceptibilities. However, the Fermi liquid's interactions endow\nthese response functions with a complex analytic structure, so that this\ntime/frequency relationship is no longer straightforward. We study how the\ngeometry of this structure is modified by a nontrivial band geometry, via a\ncalculation of the zero sound spectrum of a Dirac cone in two dimensions. We\nfind that the chiral wavefunctions, that encode the band geometry,\nfundamentally change the analytic structure of the response functions, which\nencode its Riemannian geometry. As a result, isotropic interactions can give\nrise to a variety of unconventional zero sound modes, that, due to the geometry\nof the functions in frequency space, can only be identified via time-resolved\nprobes. These modes are absent in a conventional Fermi liquid with similar\ninteractions, so that these modes can be used as a sensitive probe for the\nexistence of Dirac points in a band-structure.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "It is well known that the time-dependent response of a correlated system can\nbe inferred from its spectral correlation functions. As a textbook example, the\nzero sound collective modes of a Fermi liquid appear as poles of its\nparticle-hole susceptibilities. However, the Fermi liquid's interactions endow\nthese response functions with a complex analytic structure, so that this\ntime/frequency relationship is no longer straightforward. We study how the\ngeometry of this structure is modified by a nontrivial band geometry, via a\ncalculation of the zero sound spectrum of a Dirac cone in two dimensions. We\nfind that the chiral wavefunctions, that encode the band geometry,\nfundamentally change the analytic structure of the response functions, which\nencode its Riemannian geometry. As a result, isotropic interactions can give\nrise to a variety of unconventional zero sound modes, that, due to the geometry\nof the functions in frequency space, can only be identified via time-resolved\nprobes. These modes are absent in a conventional Fermi liquid with similar\ninteractions, so that these modes can be used as a sensitive probe for the\nexistence of Dirac points in a band-structure."
                },
                "authors": [
                    {
                        "name": "Ankan Biswas"
                    },
                    {
                        "name": "Avraham Klein"
                    }
                ],
                "author_detail": {
                    "name": "Avraham Klein"
                },
                "author": "Avraham Klein",
                "arxiv_doi": "10.1103/bh3c-4sng",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1103/bh3c-4sng",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2504.04436v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.04436v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "19 pages, 7 figures",
                "arxiv_primary_category": {
                    "term": "cond-mat.str-el",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cond-mat.str-el",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.20545v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.20545v2",
                "updated": "2025-07-08T09:46:27Z",
                "updated_parsed": [
                    2025,
                    7,
                    8,
                    9,
                    46,
                    27,
                    1,
                    189,
                    0
                ],
                "published": "2024-12-29T18:34:10Z",
                "published_parsed": [
                    2024,
                    12,
                    29,
                    18,
                    34,
                    10,
                    6,
                    364,
                    0
                ],
                "title": "The Impact of Prompt Programming on Function-Level Code Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Impact of Prompt Programming on Function-Level Code Generation"
                },
                "summary": "Large Language Models (LLMs) are increasingly used by software engineers for\ncode generation. However, limitations of LLMs such as irrelevant or incorrect\ncode have highlighted the need for prompt programming (or prompt engineering)\nwhere engineers apply specific prompt techniques (e.g., chain-of-thought or\ninput-output examples) to improve the generated code. While some prompt\ntechniques have been studied, the impact of different techniques -- and their\ninteractions -- on code generation is still not fully understood. In this\nstudy, we introduce CodePromptEval, a dataset of 7072 prompts designed to\nevaluate five prompt techniques (few-shot, persona, chain-of-thought, function\nsignature, list of packages) and their effect on the correctness, similarity,\nand quality of complete functions generated by three LLMs (GPT-4o, Llama3, and\nMistral). Our findings show that while certain prompt techniques significantly\ninfluence the generated code, combining multiple techniques does not\nnecessarily improve the outcome. Additionally, we observed a trade-off between\ncorrectness and quality when using prompt techniques. Our dataset and\nreplication package enable future research on improving LLM-generated code and\nevaluating new prompt techniques.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) are increasingly used by software engineers for\ncode generation. However, limitations of LLMs such as irrelevant or incorrect\ncode have highlighted the need for prompt programming (or prompt engineering)\nwhere engineers apply specific prompt techniques (e.g., chain-of-thought or\ninput-output examples) to improve the generated code. While some prompt\ntechniques have been studied, the impact of different techniques -- and their\ninteractions -- on code generation is still not fully understood. In this\nstudy, we introduce CodePromptEval, a dataset of 7072 prompts designed to\nevaluate five prompt techniques (few-shot, persona, chain-of-thought, function\nsignature, list of packages) and their effect on the correctness, similarity,\nand quality of complete functions generated by three LLMs (GPT-4o, Llama3, and\nMistral). Our findings show that while certain prompt techniques significantly\ninfluence the generated code, combining multiple techniques does not\nnecessarily improve the outcome. Additionally, we observed a trade-off between\ncorrectness and quality when using prompt techniques. Our dataset and\nreplication package enable future research on improving LLM-generated code and\nevaluating new prompt techniques."
                },
                "authors": [
                    {
                        "name": "Ranim Khojah"
                    },
                    {
                        "name": "Francisco Gomes de Oliveira Neto"
                    },
                    {
                        "name": "Mazen Mohamad"
                    },
                    {
                        "name": "Philipp Leitner"
                    }
                ],
                "author_detail": {
                    "name": "Philipp Leitner"
                },
                "author": "Philipp Leitner",
                "arxiv_comment": "Accepted at Transactions on Software Engineering (TSE).\n  CodePromptEval dataset and replication package on GitHub:\n  https://github.com/icetlab/CodePromptEval",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.20545v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.20545v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.05822v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.05822v1",
                "updated": "2025-07-08T09:43:17Z",
                "updated_parsed": [
                    2025,
                    7,
                    8,
                    9,
                    43,
                    17,
                    1,
                    189,
                    0
                ],
                "published": "2025-07-08T09:43:17Z",
                "published_parsed": [
                    2025,
                    7,
                    8,
                    9,
                    43,
                    17,
                    1,
                    189,
                    0
                ],
                "title": "Video Event Reasoning and Prediction by Fusing World Knowledge from LLMs\n  with Vision Foundation Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Video Event Reasoning and Prediction by Fusing World Knowledge from LLMs\n  with Vision Foundation Models"
                },
                "summary": "Current video understanding models excel at recognizing \"what\" is happening\nbut fall short in high-level cognitive tasks like causal reasoning and future\nprediction, a limitation rooted in their lack of commonsense world knowledge.\nTo bridge this cognitive gap, we propose a novel framework that synergistically\nfuses a powerful Vision Foundation Model (VFM) for deep visual perception with\na Large Language Model (LLM) serving as a knowledge-driven reasoning core. Our\nkey technical innovation is a sophisticated fusion module, inspired by the\nQ-Former architecture, which distills complex spatiotemporal and object-centric\nvisual features into a concise, language-aligned representation. This enables\nthe LLM to effectively ground its inferential processes in direct visual\nevidence. The model is trained via a two-stage strategy, beginning with\nlarge-scale alignment pre-training on video-text data, followed by targeted\ninstruction fine-tuning on a curated dataset designed to elicit advanced\nreasoning and prediction skills. Extensive experiments demonstrate that our\nmodel achieves state-of-the-art performance on multiple challenging benchmarks.\nNotably, it exhibits remarkable zero-shot generalization to unseen reasoning\ntasks, and our in-depth ablation studies validate the critical contribution of\neach architectural component. This work pushes the boundary of machine\nperception from simple recognition towards genuine cognitive understanding,\npaving the way for more intelligent and capable AI systems in robotics,\nhuman-computer interaction, and beyond.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Current video understanding models excel at recognizing \"what\" is happening\nbut fall short in high-level cognitive tasks like causal reasoning and future\nprediction, a limitation rooted in their lack of commonsense world knowledge.\nTo bridge this cognitive gap, we propose a novel framework that synergistically\nfuses a powerful Vision Foundation Model (VFM) for deep visual perception with\na Large Language Model (LLM) serving as a knowledge-driven reasoning core. Our\nkey technical innovation is a sophisticated fusion module, inspired by the\nQ-Former architecture, which distills complex spatiotemporal and object-centric\nvisual features into a concise, language-aligned representation. This enables\nthe LLM to effectively ground its inferential processes in direct visual\nevidence. The model is trained via a two-stage strategy, beginning with\nlarge-scale alignment pre-training on video-text data, followed by targeted\ninstruction fine-tuning on a curated dataset designed to elicit advanced\nreasoning and prediction skills. Extensive experiments demonstrate that our\nmodel achieves state-of-the-art performance on multiple challenging benchmarks.\nNotably, it exhibits remarkable zero-shot generalization to unseen reasoning\ntasks, and our in-depth ablation studies validate the critical contribution of\neach architectural component. This work pushes the boundary of machine\nperception from simple recognition towards genuine cognitive understanding,\npaving the way for more intelligent and capable AI systems in robotics,\nhuman-computer interaction, and beyond."
                },
                "authors": [
                    {
                        "name": "L'ea Dubois"
                    },
                    {
                        "name": "Klaus Schmidt"
                    },
                    {
                        "name": "Chengyu Wang"
                    },
                    {
                        "name": "Ji-Hoon Park"
                    },
                    {
                        "name": "Lin Wang"
                    },
                    {
                        "name": "Santiago Munoz"
                    }
                ],
                "author_detail": {
                    "name": "Santiago Munoz"
                },
                "author": "Santiago Munoz",
                "arxiv_comment": "22 pages, 4 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.05822v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.05822v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "CS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.10",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.05820v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.05820v1",
                "updated": "2025-07-08T09:39:02Z",
                "updated_parsed": [
                    2025,
                    7,
                    8,
                    9,
                    39,
                    2,
                    1,
                    189,
                    0
                ],
                "published": "2025-07-08T09:39:02Z",
                "published_parsed": [
                    2025,
                    7,
                    8,
                    9,
                    39,
                    2,
                    1,
                    189,
                    0
                ],
                "title": "Constella: Supporting Storywriters' Interconnected Character Creation\n  through LLM-based Multi-Agents",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Constella: Supporting Storywriters' Interconnected Character Creation\n  through LLM-based Multi-Agents"
                },
                "summary": "Creating a cast of characters by attending to their relational dynamics is a\ncritical aspect of most long-form storywriting. However, our formative study\n(N=14) reveals that writers struggle to envision new characters that could\ninfluence existing ones, to balance similarities and differences among\ncharacters, and to intricately flesh out their relationships. Based on these\nobservations, we designed Constella, an LLM-based multi-agent tool that\nsupports storywriters' interconnected character creation process. Constella\nsuggests related characters (FRIENDS DISCOVERY feature), reveals the inner\nmindscapes of several characters simultaneously (JOURNALS feature), and\nmanifests relationships through inter-character responses (COMMENTS feature).\nOur 7-8 day deployment study with storywriters (N=11) shows that Constella\nenabled the creation of expansive communities composed of related characters,\nfacilitated the comparison of characters' thoughts and emotions, and deepened\nwriters' understanding of character relationships. We conclude by discussing\nhow multi-agent interactions can help distribute writers' attention and effort\nacross the character cast.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Creating a cast of characters by attending to their relational dynamics is a\ncritical aspect of most long-form storywriting. However, our formative study\n(N=14) reveals that writers struggle to envision new characters that could\ninfluence existing ones, to balance similarities and differences among\ncharacters, and to intricately flesh out their relationships. Based on these\nobservations, we designed Constella, an LLM-based multi-agent tool that\nsupports storywriters' interconnected character creation process. Constella\nsuggests related characters (FRIENDS DISCOVERY feature), reveals the inner\nmindscapes of several characters simultaneously (JOURNALS feature), and\nmanifests relationships through inter-character responses (COMMENTS feature).\nOur 7-8 day deployment study with storywriters (N=11) shows that Constella\nenabled the creation of expansive communities composed of related characters,\nfacilitated the comparison of characters' thoughts and emotions, and deepened\nwriters' understanding of character relationships. We conclude by discussing\nhow multi-agent interactions can help distribute writers' attention and effort\nacross the character cast."
                },
                "authors": [
                    {
                        "name": "Syemin Park"
                    },
                    {
                        "name": "Soobin Park"
                    },
                    {
                        "name": "Youn-kyung Lim"
                    }
                ],
                "author_detail": {
                    "name": "Youn-kyung Lim"
                },
                "author": "Youn-kyung Lim",
                "arxiv_comment": "50 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.05820v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.05820v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2302.01584v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2302.01584v2",
                "updated": "2025-07-08T09:38:05Z",
                "updated_parsed": [
                    2025,
                    7,
                    8,
                    9,
                    38,
                    5,
                    1,
                    189,
                    0
                ],
                "published": "2023-02-03T07:32:23Z",
                "published_parsed": [
                    2023,
                    2,
                    3,
                    7,
                    32,
                    23,
                    4,
                    34,
                    0
                ],
                "title": "TT-TFHE: a Torus Fully Homomorphic Encryption-Friendly Neural Network\n  Architecture",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TT-TFHE: a Torus Fully Homomorphic Encryption-Friendly Neural Network\n  Architecture"
                },
                "summary": "This paper presents TT-TFHE, a deep neural network Fully Homomorphic\nEncryption (FHE) framework that effectively scales Torus FHE (TFHE) usage to\ntabular and image datasets using a recent family of convolutional neural\nnetworks called Truth-Table Neural Networks (TTnet). The proposed framework\nprovides an easy-to-implement, automated TTnet-based design toolbox with an\nunderlying (python-based) open-source Concrete implementation (CPU-based and\nimplementing lookup tables) for inference over encrypted data. Experimental\nevaluation shows that TT-TFHE greatly outperforms in terms of time and accuracy\nall Homomorphic Encryption (HE) set-ups on three tabular datasets, all other\nfeatures being equal. On image datasets such as MNIST and CIFAR-10, we show\nthat TT-TFHE consistently and largely outperforms other TFHE set-ups and is\ncompetitive against other HE variants such as BFV or CKKS (while maintaining\nthe same level of 128-bit encryption security guarantees). In addition, our\nsolutions present a very low memory footprint (down to dozens of MBs for\nMNIST), which is in sharp contrast with other HE set-ups that typically require\ntens to hundreds of GBs of memory per user (in addition to their communication\noverheads). This is the first work presenting a fully practical solution of\nprivate inference (i.e. a few seconds for inference time and a few dozens MBs\nof memory) on both tabular datasets and MNIST, that can easily scale to\nmultiple threads and users on server side.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper presents TT-TFHE, a deep neural network Fully Homomorphic\nEncryption (FHE) framework that effectively scales Torus FHE (TFHE) usage to\ntabular and image datasets using a recent family of convolutional neural\nnetworks called Truth-Table Neural Networks (TTnet). The proposed framework\nprovides an easy-to-implement, automated TTnet-based design toolbox with an\nunderlying (python-based) open-source Concrete implementation (CPU-based and\nimplementing lookup tables) for inference over encrypted data. Experimental\nevaluation shows that TT-TFHE greatly outperforms in terms of time and accuracy\nall Homomorphic Encryption (HE) set-ups on three tabular datasets, all other\nfeatures being equal. On image datasets such as MNIST and CIFAR-10, we show\nthat TT-TFHE consistently and largely outperforms other TFHE set-ups and is\ncompetitive against other HE variants such as BFV or CKKS (while maintaining\nthe same level of 128-bit encryption security guarantees). In addition, our\nsolutions present a very low memory footprint (down to dozens of MBs for\nMNIST), which is in sharp contrast with other HE set-ups that typically require\ntens to hundreds of GBs of memory per user (in addition to their communication\noverheads). This is the first work presenting a fully practical solution of\nprivate inference (i.e. a few seconds for inference time and a few dozens MBs\nof memory) on both tabular datasets and MNIST, that can easily scale to\nmultiple threads and users on server side."
                },
                "authors": [
                    {
                        "name": "Adrien Benamira"
                    },
                    {
                        "name": "Tristan GuÃ©rand"
                    },
                    {
                        "name": "Thomas Peyrin"
                    },
                    {
                        "name": "Sayandeep Saha"
                    }
                ],
                "author_detail": {
                    "name": "Sayandeep Saha"
                },
                "author": "Sayandeep Saha",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2302.01584v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2302.01584v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.05816v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.05816v1",
                "updated": "2025-07-08T09:36:14Z",
                "updated_parsed": [
                    2025,
                    7,
                    8,
                    9,
                    36,
                    14,
                    1,
                    189,
                    0
                ],
                "published": "2025-07-08T09:36:14Z",
                "published_parsed": [
                    2025,
                    7,
                    8,
                    9,
                    36,
                    14,
                    1,
                    189,
                    0
                ],
                "title": "Affective-ROPTester: Capability and Bias Analysis of LLMs in Predicting\n  Retinopathy of Prematurity",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Affective-ROPTester: Capability and Bias Analysis of LLMs in Predicting\n  Retinopathy of Prematurity"
                },
                "summary": "Despite the remarkable progress of large language models (LLMs) across\nvarious domains, their capacity to predict retinopathy of prematurity (ROP)\nrisk remains largely unexplored. To address this gap, we introduce a novel\nChinese benchmark dataset, termed CROP, comprising 993 admission records\nannotated with low, medium, and high-risk labels. To systematically examine the\npredictive capabilities and affective biases of LLMs in ROP risk\nstratification, we propose Affective-ROPTester, an automated evaluation\nframework incorporating three prompting strategies: Instruction-based,\nChain-of-Thought (CoT), and In-Context Learning (ICL). The Instruction scheme\nassesses LLMs' intrinsic knowledge and associated biases, whereas the CoT and\nICL schemes leverage external medical knowledge to enhance predictive accuracy.\nCrucially, we integrate emotional elements at the prompt level to investigate\nhow different affective framings influence the model's ability to predict ROP\nand its bias patterns. Empirical results derived from the CROP dataset yield\ntwo principal observations. First, LLMs demonstrate limited efficacy in ROP\nrisk prediction when operating solely on intrinsic knowledge, yet exhibit\nmarked performance gains when augmented with structured external inputs.\nSecond, affective biases are evident in the model outputs, with a consistent\ninclination toward overestimating medium- and high-risk cases. Third, compared\nto negative emotions, positive emotional framing contributes to mitigating\npredictive bias in model outputs. These findings highlight the critical role of\naffect-sensitive prompt engineering in enhancing diagnostic reliability and\nemphasize the utility of Affective-ROPTester as a framework for evaluating and\nmitigating affective bias in clinical language modeling systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Despite the remarkable progress of large language models (LLMs) across\nvarious domains, their capacity to predict retinopathy of prematurity (ROP)\nrisk remains largely unexplored. To address this gap, we introduce a novel\nChinese benchmark dataset, termed CROP, comprising 993 admission records\nannotated with low, medium, and high-risk labels. To systematically examine the\npredictive capabilities and affective biases of LLMs in ROP risk\nstratification, we propose Affective-ROPTester, an automated evaluation\nframework incorporating three prompting strategies: Instruction-based,\nChain-of-Thought (CoT), and In-Context Learning (ICL). The Instruction scheme\nassesses LLMs' intrinsic knowledge and associated biases, whereas the CoT and\nICL schemes leverage external medical knowledge to enhance predictive accuracy.\nCrucially, we integrate emotional elements at the prompt level to investigate\nhow different affective framings influence the model's ability to predict ROP\nand its bias patterns. Empirical results derived from the CROP dataset yield\ntwo principal observations. First, LLMs demonstrate limited efficacy in ROP\nrisk prediction when operating solely on intrinsic knowledge, yet exhibit\nmarked performance gains when augmented with structured external inputs.\nSecond, affective biases are evident in the model outputs, with a consistent\ninclination toward overestimating medium- and high-risk cases. Third, compared\nto negative emotions, positive emotional framing contributes to mitigating\npredictive bias in model outputs. These findings highlight the critical role of\naffect-sensitive prompt engineering in enhancing diagnostic reliability and\nemphasize the utility of Affective-ROPTester as a framework for evaluating and\nmitigating affective bias in clinical language modeling systems."
                },
                "authors": [
                    {
                        "name": "Shuai Zhao"
                    },
                    {
                        "name": "Yulin Zhang"
                    },
                    {
                        "name": "Luwei Xiao"
                    },
                    {
                        "name": "Xinyi Wu"
                    },
                    {
                        "name": "Yanhao Jia"
                    },
                    {
                        "name": "Zhongliang Guo"
                    },
                    {
                        "name": "Xiaobao Wu"
                    },
                    {
                        "name": "Cong-Duy Nguyen"
                    },
                    {
                        "name": "Guoming Zhang"
                    },
                    {
                        "name": "Anh Tuan Luu"
                    }
                ],
                "author_detail": {
                    "name": "Anh Tuan Luu"
                },
                "author": "Anh Tuan Luu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.05816v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.05816v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.15790v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.15790v2",
                "updated": "2025-07-08T09:31:28Z",
                "updated_parsed": [
                    2025,
                    7,
                    8,
                    9,
                    31,
                    28,
                    1,
                    189,
                    0
                ],
                "published": "2025-06-18T18:18:19Z",
                "published_parsed": [
                    2025,
                    6,
                    18,
                    18,
                    18,
                    19,
                    2,
                    169,
                    0
                ],
                "title": "ETrace:Event-Driven Vulnerability Detection in Smart Contracts via\n  LLM-Based Trace Analysis",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ETrace:Event-Driven Vulnerability Detection in Smart Contracts via\n  LLM-Based Trace Analysis"
                },
                "summary": "With the advance application of blockchain technology in various fields,\nensuring the security and stability of smart contracts has emerged as a\ncritical challenge. Current security analysis methodologies in vulnerability\ndetection can be categorized into static analysis and dynamic analysis\nmethods.However, these existing traditional vulnerability detection methods\npredominantly rely on analyzing original contract code, not all smart contracts\nprovide accessible code.We present ETrace, a novel event-driven vulnerability\ndetection framework for smart contracts, which uniquely identifies potential\nvulnerabilities through LLM-powered trace analysis without requiring source\ncode access. By extracting fine-grained event sequences from transaction logs,\nthe framework leverages Large Language Models (LLMs) as adaptive semantic\ninterpreters to reconstruct event analysis through chain-of-thought reasoning.\nETrace implements pattern-matching to establish causal links between\ntransaction behavior patterns and known attack behaviors. Furthermore, we\nvalidate the effectiveness of ETrace through preliminary experimental results.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the advance application of blockchain technology in various fields,\nensuring the security and stability of smart contracts has emerged as a\ncritical challenge. Current security analysis methodologies in vulnerability\ndetection can be categorized into static analysis and dynamic analysis\nmethods.However, these existing traditional vulnerability detection methods\npredominantly rely on analyzing original contract code, not all smart contracts\nprovide accessible code.We present ETrace, a novel event-driven vulnerability\ndetection framework for smart contracts, which uniquely identifies potential\nvulnerabilities through LLM-powered trace analysis without requiring source\ncode access. By extracting fine-grained event sequences from transaction logs,\nthe framework leverages Large Language Models (LLMs) as adaptive semantic\ninterpreters to reconstruct event analysis through chain-of-thought reasoning.\nETrace implements pattern-matching to establish causal links between\ntransaction behavior patterns and known attack behaviors. Furthermore, we\nvalidate the effectiveness of ETrace through preliminary experimental results."
                },
                "authors": [
                    {
                        "name": "Chenyang Peng"
                    },
                    {
                        "name": "Haijun Wang"
                    },
                    {
                        "name": "Yin Wu"
                    },
                    {
                        "name": "Hao Wu"
                    },
                    {
                        "name": "Ming Fan"
                    },
                    {
                        "name": "Yitao Zhao"
                    },
                    {
                        "name": "Ting Liu"
                    }
                ],
                "author_detail": {
                    "name": "Ting Liu"
                },
                "author": "Ting Liu",
                "arxiv_comment": "4 pages, 1 figure. Submitted to the 16th Asia-Pacific Symposium on\n  Internetware (Internetware 2025)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.15790v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.15790v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "68N01",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "D.2.0",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.01348v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.01348v2",
                "updated": "2025-07-08T09:21:24Z",
                "updated_parsed": [
                    2025,
                    7,
                    8,
                    9,
                    21,
                    24,
                    1,
                    189,
                    0
                ],
                "published": "2025-07-02T04:30:23Z",
                "published_parsed": [
                    2025,
                    7,
                    2,
                    4,
                    30,
                    23,
                    2,
                    183,
                    0
                ],
                "title": "SpeechAccentLLM: A Unified Framework for Foreign Accent Conversion and\n  Text to Speech",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SpeechAccentLLM: A Unified Framework for Foreign Accent Conversion and\n  Text to Speech"
                },
                "summary": "Foreign accent conversion (FAC) in speech processing remains a challenging\ntask. Building on the remarkable success of large language models (LLMs) in\nText-to-Speech (TTS) tasks, this study investigates the adaptation of LLM-based\ntechniques for FAC, which we term SpeechAccentLLM. At the core of this\nframework, we introduce SpeechCodeVAE, the first model to integrate\nconnectionist temporal classification (CTC) directly into codebook\ndiscretization for speech content tokenization. This novel architecture\ngenerates tokens with a unique \"locality\" property, as validated by experiments\ndemonstrating optimal trade-offs among content faithfulness, temporal\ncoherence, and structural recoverability. Then, to address data scarcity for\nthe FAC module, we adopted a multitask learning strategy that jointly trains\nthe FAC and TTS modules. Beyond mitigating data limitations, this approach\nyielded accelerated convergence and superior speech quality compared to\nstandalone FAC training. Moreover, leveraging the salient properties of our\ndiscrete speech representations, we introduce SpeechRestorer, a postprocessing\narchitecture designed to refine LLM-generated outputs. This module effectively\nmitigates stochastic errors prevalent in LLM inference pipelines while\nenhancing prosodic continuity, as validated by ablation experiments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Foreign accent conversion (FAC) in speech processing remains a challenging\ntask. Building on the remarkable success of large language models (LLMs) in\nText-to-Speech (TTS) tasks, this study investigates the adaptation of LLM-based\ntechniques for FAC, which we term SpeechAccentLLM. At the core of this\nframework, we introduce SpeechCodeVAE, the first model to integrate\nconnectionist temporal classification (CTC) directly into codebook\ndiscretization for speech content tokenization. This novel architecture\ngenerates tokens with a unique \"locality\" property, as validated by experiments\ndemonstrating optimal trade-offs among content faithfulness, temporal\ncoherence, and structural recoverability. Then, to address data scarcity for\nthe FAC module, we adopted a multitask learning strategy that jointly trains\nthe FAC and TTS modules. Beyond mitigating data limitations, this approach\nyielded accelerated convergence and superior speech quality compared to\nstandalone FAC training. Moreover, leveraging the salient properties of our\ndiscrete speech representations, we introduce SpeechRestorer, a postprocessing\narchitecture designed to refine LLM-generated outputs. This module effectively\nmitigates stochastic errors prevalent in LLM inference pipelines while\nenhancing prosodic continuity, as validated by ablation experiments."
                },
                "authors": [
                    {
                        "name": "Zhuangfei Cheng"
                    },
                    {
                        "name": "Guangyan Zhang"
                    },
                    {
                        "name": "Zehai Tu"
                    },
                    {
                        "name": "Yangyang Song"
                    },
                    {
                        "name": "Shuiyang Mao"
                    },
                    {
                        "name": "Xiaoqi Jiao"
                    },
                    {
                        "name": "Jingyu Li"
                    },
                    {
                        "name": "Yiwen Guo"
                    },
                    {
                        "name": "Jiasong Wu"
                    }
                ],
                "author_detail": {
                    "name": "Jiasong Wu"
                },
                "author": "Jiasong Wu",
                "arxiv_comment": "10 pages, includes references, 4 figures, 4 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.01348v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.01348v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.AS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.7",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.00582v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.00582v2",
                "updated": "2025-07-08T09:07:07Z",
                "updated_parsed": [
                    2025,
                    7,
                    8,
                    9,
                    7,
                    7,
                    1,
                    189,
                    0
                ],
                "published": "2025-07-01T09:03:06Z",
                "published_parsed": [
                    2025,
                    7,
                    1,
                    9,
                    3,
                    6,
                    1,
                    182,
                    0
                ],
                "title": "Bridging Classical and Learning-based Iterative Registration through\n  Deep Equilibrium Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Bridging Classical and Learning-based Iterative Registration through\n  Deep Equilibrium Models"
                },
                "summary": "Deformable medical image registration is traditionally formulated as an\noptimization problem. While classical methods solve this problem iteratively,\nrecent learning-based approaches use recurrent neural networks (RNNs) to mimic\nthis process by unrolling the prediction of deformation fields in a fixed\nnumber of steps. However, classical methods typically converge after sufficient\niterations, but learning-based unrolling methods lack a theoretical convergence\nguarantee and show instability empirically. In addition, unrolling methods have\na practical bottleneck at training time: GPU memory usage grows linearly with\nthe unrolling steps due to backpropagation through time (BPTT). To address both\ntheoretical and practical challenges, we propose DEQReg, a novel registration\nframework based on Deep Equilibrium Models (DEQ), which formulates registration\nas an equilibrium-seeking problem, establishing a natural connection between\nclassical optimization and learning-based unrolling methods. DEQReg maintains\nconstant memory usage, enabling theoretically unlimited iteration steps.\nThrough extensive evaluation on the public brain MRI and lung CT datasets, we\nshow that DEQReg can achieve competitive registration performance, while\nsubstantially reducing memory consumption compared to state-of-the-art\nunrolling methods. We also reveal an intriguing phenomenon: the performance of\nexisting unrolling methods first increases slightly then degrades irreversibly\nwhen the inference steps go beyond the training configuration. In contrast,\nDEQReg achieves stable convergence with its inbuilt equilibrium-seeking\nmechanism, bridging the gap between classical optimization-based and modern\nlearning-based registration methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Deformable medical image registration is traditionally formulated as an\noptimization problem. While classical methods solve this problem iteratively,\nrecent learning-based approaches use recurrent neural networks (RNNs) to mimic\nthis process by unrolling the prediction of deformation fields in a fixed\nnumber of steps. However, classical methods typically converge after sufficient\niterations, but learning-based unrolling methods lack a theoretical convergence\nguarantee and show instability empirically. In addition, unrolling methods have\na practical bottleneck at training time: GPU memory usage grows linearly with\nthe unrolling steps due to backpropagation through time (BPTT). To address both\ntheoretical and practical challenges, we propose DEQReg, a novel registration\nframework based on Deep Equilibrium Models (DEQ), which formulates registration\nas an equilibrium-seeking problem, establishing a natural connection between\nclassical optimization and learning-based unrolling methods. DEQReg maintains\nconstant memory usage, enabling theoretically unlimited iteration steps.\nThrough extensive evaluation on the public brain MRI and lung CT datasets, we\nshow that DEQReg can achieve competitive registration performance, while\nsubstantially reducing memory consumption compared to state-of-the-art\nunrolling methods. We also reveal an intriguing phenomenon: the performance of\nexisting unrolling methods first increases slightly then degrades irreversibly\nwhen the inference steps go beyond the training configuration. In contrast,\nDEQReg achieves stable convergence with its inbuilt equilibrium-seeking\nmechanism, bridging the gap between classical optimization-based and modern\nlearning-based registration methods."
                },
                "authors": [
                    {
                        "name": "Yi Zhang"
                    },
                    {
                        "name": "Yidong Zhao"
                    },
                    {
                        "name": "Qian Tao"
                    }
                ],
                "author_detail": {
                    "name": "Qian Tao"
                },
                "author": "Qian Tao",
                "arxiv_comment": "Submitted version. Accepted by MICCAI 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.00582v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.00582v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.IV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.IV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.13918v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.13918v4",
                "updated": "2025-07-08T09:00:41Z",
                "updated_parsed": [
                    2025,
                    7,
                    8,
                    9,
                    0,
                    41,
                    1,
                    189,
                    0
                ],
                "published": "2024-11-21T08:13:24Z",
                "published_parsed": [
                    2024,
                    11,
                    21,
                    8,
                    13,
                    24,
                    3,
                    326,
                    0
                ],
                "title": "Quantization without Tears",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Quantization without Tears"
                },
                "summary": "Deep neural networks, while achieving remarkable success across diverse\ntasks, demand significant resources, including computation, GPU memory,\nbandwidth, storage, and energy. Network quantization, as a standard compression\nand acceleration technique, reduces storage costs and enables potential\ninference acceleration by discretizing network weights and activations into a\nfinite set of integer values. However, current quantization methods are often\ncomplex and sensitive, requiring extensive task-specific hyperparameters, where\neven a single misconfiguration can impair model performance, limiting\ngenerality across different models and tasks. In this paper, we propose\nQuantization without Tears (QwT), a method that simultaneously achieves\nquantization speed, accuracy, simplicity, and generality. The key insight of\nQwT is to incorporate a lightweight additional structure into the quantized\nnetwork to mitigate information loss during quantization. This structure\nconsists solely of a small set of linear layers, keeping the method simple and\nefficient. More importantly, it provides a closed-form solution, allowing us to\nimprove accuracy effortlessly under 2 minutes. Extensive experiments across\nvarious vision, language, and multimodal tasks demonstrate that QwT is both\nhighly effective and versatile. In fact, our approach offers a robust solution\nfor network quantization that combines simplicity, accuracy, and adaptability,\nwhich provides new insights for the design of novel quantization paradigms. The\ncode is publicly available at https://github.com/wujx2001/QwT",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Deep neural networks, while achieving remarkable success across diverse\ntasks, demand significant resources, including computation, GPU memory,\nbandwidth, storage, and energy. Network quantization, as a standard compression\nand acceleration technique, reduces storage costs and enables potential\ninference acceleration by discretizing network weights and activations into a\nfinite set of integer values. However, current quantization methods are often\ncomplex and sensitive, requiring extensive task-specific hyperparameters, where\neven a single misconfiguration can impair model performance, limiting\ngenerality across different models and tasks. In this paper, we propose\nQuantization without Tears (QwT), a method that simultaneously achieves\nquantization speed, accuracy, simplicity, and generality. The key insight of\nQwT is to incorporate a lightweight additional structure into the quantized\nnetwork to mitigate information loss during quantization. This structure\nconsists solely of a small set of linear layers, keeping the method simple and\nefficient. More importantly, it provides a closed-form solution, allowing us to\nimprove accuracy effortlessly under 2 minutes. Extensive experiments across\nvarious vision, language, and multimodal tasks demonstrate that QwT is both\nhighly effective and versatile. In fact, our approach offers a robust solution\nfor network quantization that combines simplicity, accuracy, and adaptability,\nwhich provides new insights for the design of novel quantization paradigms. The\ncode is publicly available at https://github.com/wujx2001/QwT"
                },
                "authors": [
                    {
                        "name": "Minghao Fu"
                    },
                    {
                        "name": "Hao Yu"
                    },
                    {
                        "name": "Jie Shao"
                    },
                    {
                        "name": "Junjie Zhou"
                    },
                    {
                        "name": "Ke Zhu"
                    },
                    {
                        "name": "Jianxin Wu"
                    }
                ],
                "author_detail": {
                    "name": "Jianxin Wu"
                },
                "author": "Jianxin Wu",
                "arxiv_comment": "CVPR 2025. The code is publicly available at\n  https://github.com/wujx2001/QwT",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.13918v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.13918v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.08938v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.08938v2",
                "updated": "2025-07-08T08:59:27Z",
                "updated_parsed": [
                    2025,
                    7,
                    8,
                    8,
                    59,
                    27,
                    1,
                    189,
                    0
                ],
                "published": "2025-06-10T16:02:54Z",
                "published_parsed": [
                    2025,
                    6,
                    10,
                    16,
                    2,
                    54,
                    1,
                    161,
                    0
                ],
                "title": "FaithfulRAG: Fact-Level Conflict Modeling for Context-Faithful\n  Retrieval-Augmented Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FaithfulRAG: Fact-Level Conflict Modeling for Context-Faithful\n  Retrieval-Augmented Generation"
                },
                "summary": "Large language models (LLMs) augmented with retrieval systems have\ndemonstrated significant potential in handling knowledge-intensive tasks.\nHowever, these models often struggle with unfaithfulness issues, generating\noutputs that either ignore the retrieved context or inconsistently blend it\nwith the LLM`s parametric knowledge. This issue is particularly severe in cases\nof knowledge conflict, where the retrieved context conflicts with the model`s\nparametric knowledge. While existing faithful RAG approaches enforce strict\ncontext adherence through well-designed prompts or modified decoding\nstrategies, our analysis reveals a critical limitation: they achieve\nfaithfulness by forcibly suppressing the model`s parametric knowledge, which\nundermines the model`s internal knowledge structure and increases the risk of\nmisinterpreting the context. To this end, this paper proposes FaithfulRAG, a\nnovel framework that resolves knowledge conflicts by explicitly modeling\ndiscrepancies between the model`s parametric knowledge and retrieved context.\nSpecifically, FaithfulRAG identifies conflicting knowledge at the fact level\nand designs a self-thinking process, allowing LLMs to reason about and\nintegrate conflicting facts before generating responses. Extensive experiments\ndemonstrate that our method outperforms state-of-the-art methods. The code is\navailable at https://github.com/DeepLearnXMU/Faithful-RAG",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) augmented with retrieval systems have\ndemonstrated significant potential in handling knowledge-intensive tasks.\nHowever, these models often struggle with unfaithfulness issues, generating\noutputs that either ignore the retrieved context or inconsistently blend it\nwith the LLM`s parametric knowledge. This issue is particularly severe in cases\nof knowledge conflict, where the retrieved context conflicts with the model`s\nparametric knowledge. While existing faithful RAG approaches enforce strict\ncontext adherence through well-designed prompts or modified decoding\nstrategies, our analysis reveals a critical limitation: they achieve\nfaithfulness by forcibly suppressing the model`s parametric knowledge, which\nundermines the model`s internal knowledge structure and increases the risk of\nmisinterpreting the context. To this end, this paper proposes FaithfulRAG, a\nnovel framework that resolves knowledge conflicts by explicitly modeling\ndiscrepancies between the model`s parametric knowledge and retrieved context.\nSpecifically, FaithfulRAG identifies conflicting knowledge at the fact level\nand designs a self-thinking process, allowing LLMs to reason about and\nintegrate conflicting facts before generating responses. Extensive experiments\ndemonstrate that our method outperforms state-of-the-art methods. The code is\navailable at https://github.com/DeepLearnXMU/Faithful-RAG"
                },
                "authors": [
                    {
                        "name": "Qinggang Zhang"
                    },
                    {
                        "name": "Zhishang Xiang"
                    },
                    {
                        "name": "Yilin Xiao"
                    },
                    {
                        "name": "Le Wang"
                    },
                    {
                        "name": "Junhui Li"
                    },
                    {
                        "name": "Xinrun Wang"
                    },
                    {
                        "name": "Jinsong Su"
                    }
                ],
                "author_detail": {
                    "name": "Jinsong Su"
                },
                "author": "Jinsong Su",
                "arxiv_comment": "Accepted to ACL 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.08938v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.08938v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            }
        ]
    },
    {
        "keyword": "LLM Deployment",
        "arxiv_results": [
            {
                "id": "http://arxiv.org/abs/2507.06223v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.06223v1",
                "updated": "2025-07-08T17:56:28Z",
                "updated_parsed": [
                    2025,
                    7,
                    8,
                    17,
                    56,
                    28,
                    1,
                    189,
                    0
                ],
                "published": "2025-07-08T17:56:28Z",
                "published_parsed": [
                    2025,
                    7,
                    8,
                    17,
                    56,
                    28,
                    1,
                    189,
                    0
                ],
                "title": "Efficiency-Effectiveness Reranking FLOPs for LLM-based Rerankers",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficiency-Effectiveness Reranking FLOPs for LLM-based Rerankers"
                },
                "summary": "Large Language Models (LLMs) have recently been applied to reranking tasks in\ninformation retrieval, achieving strong performance. However, their high\ncomputational demands often hinder practical deployment. Existing studies\nevaluate the efficiency of LLM-based rerankers using proxy metrics such as\nlatency, the number of forward passes, input tokens, and output tokens.\nHowever, these metrics depend on hardware and running-time choices (\\eg\nparallel or not, batch size, etc), and often fail to account for model size,\nmaking it difficult to interpret and obscuring the evaluation of the\nefficiency-effectiveness tradeoff. To address this issue, we propose\nE\\textsuperscript{2}R-FLOPs, for LLM-based rerankers: ranking metrics per\nPetaFLOP (RPP) for relevance per compute and queries per PetaFLOP (QPP) for\nhardware-agnostic throughput. Companied with the new metrics, an interpretable\nFLOPs estimator is built to estimate the FLOPs of an LLM-based reranker even\nwithout running any experiments. Based on the proposed metrics, we conduct\ncomprehensive experiments to evaluate a wide range of LLM-based rerankers with\ndifferent architecture, studying the efficiency-effectiveness trade-off and\nbringing this issue to the attention of the research community.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have recently been applied to reranking tasks in\ninformation retrieval, achieving strong performance. However, their high\ncomputational demands often hinder practical deployment. Existing studies\nevaluate the efficiency of LLM-based rerankers using proxy metrics such as\nlatency, the number of forward passes, input tokens, and output tokens.\nHowever, these metrics depend on hardware and running-time choices (\\eg\nparallel or not, batch size, etc), and often fail to account for model size,\nmaking it difficult to interpret and obscuring the evaluation of the\nefficiency-effectiveness tradeoff. To address this issue, we propose\nE\\textsuperscript{2}R-FLOPs, for LLM-based rerankers: ranking metrics per\nPetaFLOP (RPP) for relevance per compute and queries per PetaFLOP (QPP) for\nhardware-agnostic throughput. Companied with the new metrics, an interpretable\nFLOPs estimator is built to estimate the FLOPs of an LLM-based reranker even\nwithout running any experiments. Based on the proposed metrics, we conduct\ncomprehensive experiments to evaluate a wide range of LLM-based rerankers with\ndifferent architecture, studying the efficiency-effectiveness trade-off and\nbringing this issue to the attention of the research community."
                },
                "authors": [
                    {
                        "name": "Zhiyuan Peng"
                    },
                    {
                        "name": "Ting-ruen Wei"
                    },
                    {
                        "name": "Tingyu Song"
                    },
                    {
                        "name": "Yilun Zhao"
                    },
                    {
                        "name": "Yi Fang"
                    }
                ],
                "author_detail": {
                    "name": "Yi Fang"
                },
                "author": "Yi Fang",
                "arxiv_comment": "under review",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.06223v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.06223v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.06222v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.06222v1",
                "updated": "2025-07-08T17:55:54Z",
                "updated_parsed": [
                    2025,
                    7,
                    8,
                    17,
                    55,
                    54,
                    1,
                    189,
                    0
                ],
                "published": "2025-07-08T17:55:54Z",
                "published_parsed": [
                    2025,
                    7,
                    8,
                    17,
                    55,
                    54,
                    1,
                    189,
                    0
                ],
                "title": "Deep Learning Optimization of Two-State Pinching Antennas Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Deep Learning Optimization of Two-State Pinching Antennas Systems"
                },
                "summary": "The evolution of wireless communication systems requires flexible,\nenergy-efficient, and cost-effective antenna technologies. Pinching antennas\n(PAs), which can dynamically control electromagnetic wave propagation through\nbinary activation states, have recently emerged as a promising candidate. In\nthis work, we investigate the problem of optimally selecting a subset of\nfixed-position PAs to activate in a waveguide, when the aim is to maximize the\ncommunication rate at a user terminal. Due to the complex interplay between\nantenna activation, waveguide-induced phase shifts, and power division, this\nproblem is formulated as a combinatorial fractional 0-1 quadratic program. To\nefficiently solve this challenging problem, we use neural network architectures\nof varying complexity to learn activation policies directly from data,\nleveraging spatial features and signal structure. Furthermore, we incorporate\nuser location uncertainty into our training and evaluation pipeline to simulate\nrealistic deployment conditions. Simulation results demonstrate the\neffectiveness and robustness of the proposed models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The evolution of wireless communication systems requires flexible,\nenergy-efficient, and cost-effective antenna technologies. Pinching antennas\n(PAs), which can dynamically control electromagnetic wave propagation through\nbinary activation states, have recently emerged as a promising candidate. In\nthis work, we investigate the problem of optimally selecting a subset of\nfixed-position PAs to activate in a waveguide, when the aim is to maximize the\ncommunication rate at a user terminal. Due to the complex interplay between\nantenna activation, waveguide-induced phase shifts, and power division, this\nproblem is formulated as a combinatorial fractional 0-1 quadratic program. To\nefficiently solve this challenging problem, we use neural network architectures\nof varying complexity to learn activation policies directly from data,\nleveraging spatial features and signal structure. Furthermore, we incorporate\nuser location uncertainty into our training and evaluation pipeline to simulate\nrealistic deployment conditions. Simulation results demonstrate the\neffectiveness and robustness of the proposed models."
                },
                "authors": [
                    {
                        "name": "Odysseas G. Karagiannidis"
                    },
                    {
                        "name": "Victoria E. Galanopoulou"
                    },
                    {
                        "name": "Panagiotis D. Diamantoulakis"
                    },
                    {
                        "name": "Zhiguo Ding"
                    },
                    {
                        "name": "Octavia Dobre"
                    }
                ],
                "author_detail": {
                    "name": "Octavia Dobre"
                },
                "author": "Octavia Dobre",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.06222v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.06222v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.13734v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.13734v2",
                "updated": "2025-07-08T17:48:59Z",
                "updated_parsed": [
                    2025,
                    7,
                    8,
                    17,
                    48,
                    59,
                    1,
                    189,
                    0
                ],
                "published": "2025-06-16T17:42:35Z",
                "published_parsed": [
                    2025,
                    6,
                    16,
                    17,
                    42,
                    35,
                    0,
                    167,
                    0
                ],
                "title": "Instruction Following by Boosting Attention of Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Instruction Following by Boosting Attention of Large Language Models"
                },
                "summary": "Controlling the generation of large language models (LLMs) remains a central\nchallenge to ensure their safe and reliable deployment. While prompt\nengineering and finetuning are common approaches, recent work has explored\nlatent steering, a lightweight technique that alters LLM internal activations\nto guide generation. However, subsequent studies revealed latent steering's\neffectiveness to be limited, often underperforming simple instruction\nprompting. To address this limitation, we first establish a benchmark across\ndiverse behaviors for standardized evaluation of steering techniques. Building\non insights from this benchmark, we introduce Instruction Attention Boosting\n(InstABoost), a latent steering method that boosts the strength of instruction\nprompting by altering the model's attention during generation. InstABoost\ncombines the strengths of existing approaches and is theoretically supported by\nprior work that suggests that in-context rule following in transformer-based\nmodels can be controlled by manipulating attention on instructions.\nEmpirically, InstABoost demonstrates superior control success compared to both\ntraditional prompting and latent steering.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Controlling the generation of large language models (LLMs) remains a central\nchallenge to ensure their safe and reliable deployment. While prompt\nengineering and finetuning are common approaches, recent work has explored\nlatent steering, a lightweight technique that alters LLM internal activations\nto guide generation. However, subsequent studies revealed latent steering's\neffectiveness to be limited, often underperforming simple instruction\nprompting. To address this limitation, we first establish a benchmark across\ndiverse behaviors for standardized evaluation of steering techniques. Building\non insights from this benchmark, we introduce Instruction Attention Boosting\n(InstABoost), a latent steering method that boosts the strength of instruction\nprompting by altering the model's attention during generation. InstABoost\ncombines the strengths of existing approaches and is theoretically supported by\nprior work that suggests that in-context rule following in transformer-based\nmodels can be controlled by manipulating attention on instructions.\nEmpirically, InstABoost demonstrates superior control success compared to both\ntraditional prompting and latent steering."
                },
                "authors": [
                    {
                        "name": "Vitoria Guardieiro"
                    },
                    {
                        "name": "Adam Stein"
                    },
                    {
                        "name": "Avishree Khare"
                    },
                    {
                        "name": "Eric Wong"
                    }
                ],
                "author_detail": {
                    "name": "Eric Wong"
                },
                "author": "Eric Wong",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.13734v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.13734v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.06205v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.06205v1",
                "updated": "2025-07-08T17:30:18Z",
                "updated_parsed": [
                    2025,
                    7,
                    8,
                    17,
                    30,
                    18,
                    1,
                    189,
                    0
                ],
                "published": "2025-07-08T17:30:18Z",
                "published_parsed": [
                    2025,
                    7,
                    8,
                    17,
                    30,
                    18,
                    1,
                    189,
                    0
                ],
                "title": "DS@GT at CheckThat! 2025: Ensemble Methods for Detection of Scientific\n  Discourse on Social Media",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DS@GT at CheckThat! 2025: Ensemble Methods for Detection of Scientific\n  Discourse on Social Media"
                },
                "summary": "In this paper, we, as the DS@GT team for CLEF 2025 CheckThat! Task 4a\nScientific Web Discourse Detection, present the methods we explored for this\ntask. For this multiclass classification task, we determined if a tweet\ncontained a scientific claim, a reference to a scientific study or publication,\nand/or mentions of scientific entities, such as a university or a scientist. We\npresent 3 modeling approaches for this task: transformer finetuning, few-shot\nprompting of LLMs, and a combined ensemble model whose design was informed by\nearlier experiments. Our team placed 7th in the competition, achieving a\nmacro-averaged F1 score of 0.8611, an improvement over the DeBERTaV3 baseline\nof 0.8375. Our code is available on Github at\nhttps://github.com/dsgt-arc/checkthat-2025-swd/tree/main/subtask-4a.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this paper, we, as the DS@GT team for CLEF 2025 CheckThat! Task 4a\nScientific Web Discourse Detection, present the methods we explored for this\ntask. For this multiclass classification task, we determined if a tweet\ncontained a scientific claim, a reference to a scientific study or publication,\nand/or mentions of scientific entities, such as a university or a scientist. We\npresent 3 modeling approaches for this task: transformer finetuning, few-shot\nprompting of LLMs, and a combined ensemble model whose design was informed by\nearlier experiments. Our team placed 7th in the competition, achieving a\nmacro-averaged F1 score of 0.8611, an improvement over the DeBERTaV3 baseline\nof 0.8375. Our code is available on Github at\nhttps://github.com/dsgt-arc/checkthat-2025-swd/tree/main/subtask-4a."
                },
                "authors": [
                    {
                        "name": "Ayush Parikh"
                    },
                    {
                        "name": "Hoang Thanh Thanh Truong"
                    },
                    {
                        "name": "Jeanette Schofield"
                    },
                    {
                        "name": "Maximilian Heil"
                    }
                ],
                "author_detail": {
                    "name": "Maximilian Heil"
                },
                "author": "Maximilian Heil",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.06205v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.06205v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.06204v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.06204v1",
                "updated": "2025-07-08T17:30:14Z",
                "updated_parsed": [
                    2025,
                    7,
                    8,
                    17,
                    30,
                    14,
                    1,
                    189,
                    0
                ],
                "published": "2025-07-08T17:30:14Z",
                "published_parsed": [
                    2025,
                    7,
                    8,
                    17,
                    30,
                    14,
                    1,
                    189,
                    0
                ],
                "title": "Differential Mamba",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Differential Mamba"
                },
                "summary": "Sequence models like Transformers and RNNs often overallocate attention to\nirrelevant context, leading to noisy intermediate representations. This\ndegrades LLM capabilities by promoting hallucinations, weakening long-range and\nretrieval abilities, and reducing robustness. Recent work has shown that\ndifferential design can mitigate this issue in Transformers, improving their\neffectiveness across various applications. In this paper, we explore whether\nthese techniques, originally developed for Transformers, can be applied to\nMamba, a recent architecture based on selective state-space layers that\nachieves Transformer-level performance with greater efficiency. We show that a\nnaive adaptation of differential design to Mamba is insufficient and requires\ncareful architectural modifications. To address this, we introduce a novel\ndifferential mechanism for Mamba, empirically validated on language modeling\nbenchmarks, demonstrating improved retrieval capabilities and superior\nperformance over vanilla Mamba. Finally, we conduct extensive ablation studies\nand empirical analyses to justify our design choices and provide evidence that\nour approach effectively mitigates the overallocation problem in Mamba-based\nmodels. Our code is publicly available.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Sequence models like Transformers and RNNs often overallocate attention to\nirrelevant context, leading to noisy intermediate representations. This\ndegrades LLM capabilities by promoting hallucinations, weakening long-range and\nretrieval abilities, and reducing robustness. Recent work has shown that\ndifferential design can mitigate this issue in Transformers, improving their\neffectiveness across various applications. In this paper, we explore whether\nthese techniques, originally developed for Transformers, can be applied to\nMamba, a recent architecture based on selective state-space layers that\nachieves Transformer-level performance with greater efficiency. We show that a\nnaive adaptation of differential design to Mamba is insufficient and requires\ncareful architectural modifications. To address this, we introduce a novel\ndifferential mechanism for Mamba, empirically validated on language modeling\nbenchmarks, demonstrating improved retrieval capabilities and superior\nperformance over vanilla Mamba. Finally, we conduct extensive ablation studies\nand empirical analyses to justify our design choices and provide evidence that\nour approach effectively mitigates the overallocation problem in Mamba-based\nmodels. Our code is publicly available."
                },
                "authors": [
                    {
                        "name": "Nadav Schneider"
                    },
                    {
                        "name": "Itamar Zimerman"
                    },
                    {
                        "name": "Eliya Nachmani"
                    }
                ],
                "author_detail": {
                    "name": "Eliya Nachmani"
                },
                "author": "Eliya Nachmani",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.06204v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.06204v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.06203v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.06203v1",
                "updated": "2025-07-08T17:29:07Z",
                "updated_parsed": [
                    2025,
                    7,
                    8,
                    17,
                    29,
                    7,
                    1,
                    189,
                    0
                ],
                "published": "2025-07-08T17:29:07Z",
                "published_parsed": [
                    2025,
                    7,
                    8,
                    17,
                    29,
                    7,
                    1,
                    189,
                    0
                ],
                "title": "A Survey on Latent Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Survey on Latent Reasoning"
                },
                "summary": "Large Language Models (LLMs) have demonstrated impressive reasoning\ncapabilities, especially when guided by explicit chain-of-thought (CoT)\nreasoning that verbalizes intermediate steps. While CoT improves both\ninterpretability and accuracy, its dependence on natural language reasoning\nlimits the model's expressive bandwidth. Latent reasoning tackles this\nbottleneck by performing multi-step inference entirely in the model's\ncontinuous hidden state, eliminating token-level supervision. To advance latent\nreasoning research, this survey provides a comprehensive overview of the\nemerging field of latent reasoning. We begin by examining the foundational role\nof neural network layers as the computational substrate for reasoning,\nhighlighting how hierarchical representations support complex transformations.\nNext, we explore diverse latent reasoning methodologies, including\nactivation-based recurrence, hidden state propagation, and fine-tuning\nstrategies that compress or internalize explicit reasoning traces. Finally, we\ndiscuss advanced paradigms such as infinite-depth latent reasoning via masked\ndiffusion models, which enable globally consistent and reversible reasoning\nprocesses. By unifying these perspectives, we aim to clarify the conceptual\nlandscape of latent reasoning and chart future directions for research at the\nfrontier of LLM cognition. An associated GitHub repository collecting the\nlatest papers and repos is available at:\nhttps://github.com/multimodal-art-projection/LatentCoT-Horizon/.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have demonstrated impressive reasoning\ncapabilities, especially when guided by explicit chain-of-thought (CoT)\nreasoning that verbalizes intermediate steps. While CoT improves both\ninterpretability and accuracy, its dependence on natural language reasoning\nlimits the model's expressive bandwidth. Latent reasoning tackles this\nbottleneck by performing multi-step inference entirely in the model's\ncontinuous hidden state, eliminating token-level supervision. To advance latent\nreasoning research, this survey provides a comprehensive overview of the\nemerging field of latent reasoning. We begin by examining the foundational role\nof neural network layers as the computational substrate for reasoning,\nhighlighting how hierarchical representations support complex transformations.\nNext, we explore diverse latent reasoning methodologies, including\nactivation-based recurrence, hidden state propagation, and fine-tuning\nstrategies that compress or internalize explicit reasoning traces. Finally, we\ndiscuss advanced paradigms such as infinite-depth latent reasoning via masked\ndiffusion models, which enable globally consistent and reversible reasoning\nprocesses. By unifying these perspectives, we aim to clarify the conceptual\nlandscape of latent reasoning and chart future directions for research at the\nfrontier of LLM cognition. An associated GitHub repository collecting the\nlatest papers and repos is available at:\nhttps://github.com/multimodal-art-projection/LatentCoT-Horizon/."
                },
                "authors": [
                    {
                        "name": "Rui-Jie Zhu"
                    },
                    {
                        "name": "Tianhao Peng"
                    },
                    {
                        "name": "Tianhao Cheng"
                    },
                    {
                        "name": "Xingwei Qu"
                    },
                    {
                        "name": "Jinfa Huang"
                    },
                    {
                        "name": "Dawei Zhu"
                    },
                    {
                        "name": "Hao Wang"
                    },
                    {
                        "name": "Kaiwen Xue"
                    },
                    {
                        "name": "Xuanliang Zhang"
                    },
                    {
                        "name": "Yong Shan"
                    },
                    {
                        "name": "Tianle Cai"
                    },
                    {
                        "name": "Taylor Kergan"
                    },
                    {
                        "name": "Assel Kembay"
                    },
                    {
                        "name": "Andrew Smith"
                    },
                    {
                        "name": "Chenghua Lin"
                    },
                    {
                        "name": "Binh Nguyen"
                    },
                    {
                        "name": "Yuqi Pan"
                    },
                    {
                        "name": "Yuhong Chou"
                    },
                    {
                        "name": "Zefan Cai"
                    },
                    {
                        "name": "Zhenhe Wu"
                    },
                    {
                        "name": "Yongchi Zhao"
                    },
                    {
                        "name": "Tianyu Liu"
                    },
                    {
                        "name": "Jian Yang"
                    },
                    {
                        "name": "Wangchunshu Zhou"
                    },
                    {
                        "name": "Chujie Zheng"
                    },
                    {
                        "name": "Chongxuan Li"
                    },
                    {
                        "name": "Yuyin Zhou"
                    },
                    {
                        "name": "Zhoujun Li"
                    },
                    {
                        "name": "Zhaoxiang Zhang"
                    },
                    {
                        "name": "Jiaheng Liu"
                    },
                    {
                        "name": "Ge Zhang"
                    },
                    {
                        "name": "Wenhao Huang"
                    },
                    {
                        "name": "Jason Eshraghian"
                    }
                ],
                "author_detail": {
                    "name": "Jason Eshraghian"
                },
                "author": "Jason Eshraghian",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.06203v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.06203v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.21898v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.21898v2",
                "updated": "2025-07-08T17:26:59Z",
                "updated_parsed": [
                    2025,
                    7,
                    8,
                    17,
                    26,
                    59,
                    1,
                    189,
                    0
                ],
                "published": "2025-06-27T04:35:52Z",
                "published_parsed": [
                    2025,
                    6,
                    27,
                    4,
                    35,
                    52,
                    4,
                    178,
                    0
                ],
                "title": "Bias, Accuracy, and Trust: Gender-Diverse Perspectives on Large Language\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Bias, Accuracy, and Trust: Gender-Diverse Perspectives on Large Language\n  Models"
                },
                "summary": "Large language models (LLMs) are becoming increasingly ubiquitous in our\ndaily lives, but numerous concerns about bias in LLMs exist. This study\nexamines how gender-diverse populations perceive bias, accuracy, and\ntrustworthiness in LLMs, specifically ChatGPT. Through 25 in-depth interviews\nwith non-binary/transgender, male, and female participants, we investigate how\ngendered and neutral prompts influence model responses and how users evaluate\nthese responses. Our findings reveal that gendered prompts elicit more\nidentity-specific responses, with non-binary participants particularly\nsusceptible to condescending and stereotypical portrayals. Perceived accuracy\nwas consistent across gender groups, with errors most noted in technical topics\nand creative tasks. Trustworthiness varied by gender, with men showing higher\ntrust, especially in performance, and non-binary participants demonstrating\nhigher performance-based trust. Additionally, participants suggested improving\nthe LLMs by diversifying training data, ensuring equal depth in gendered\nresponses, and incorporating clarifying questions. This research contributes to\nthe CSCW/HCI field by highlighting the need for gender-diverse perspectives in\nLLM development in particular and AI in general, to foster more inclusive and\ntrustworthy systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) are becoming increasingly ubiquitous in our\ndaily lives, but numerous concerns about bias in LLMs exist. This study\nexamines how gender-diverse populations perceive bias, accuracy, and\ntrustworthiness in LLMs, specifically ChatGPT. Through 25 in-depth interviews\nwith non-binary/transgender, male, and female participants, we investigate how\ngendered and neutral prompts influence model responses and how users evaluate\nthese responses. Our findings reveal that gendered prompts elicit more\nidentity-specific responses, with non-binary participants particularly\nsusceptible to condescending and stereotypical portrayals. Perceived accuracy\nwas consistent across gender groups, with errors most noted in technical topics\nand creative tasks. Trustworthiness varied by gender, with men showing higher\ntrust, especially in performance, and non-binary participants demonstrating\nhigher performance-based trust. Additionally, participants suggested improving\nthe LLMs by diversifying training data, ensuring equal depth in gendered\nresponses, and incorporating clarifying questions. This research contributes to\nthe CSCW/HCI field by highlighting the need for gender-diverse perspectives in\nLLM development in particular and AI in general, to foster more inclusive and\ntrustworthy systems."
                },
                "authors": [
                    {
                        "name": "Aimen Gaba"
                    },
                    {
                        "name": "Emily Wall"
                    },
                    {
                        "name": "Tejas Ramkumar Babu"
                    },
                    {
                        "name": "Yuriy Brun"
                    },
                    {
                        "name": "Kyle Hall"
                    },
                    {
                        "name": "Cindy Xiong Bearfield"
                    }
                ],
                "author_detail": {
                    "name": "Cindy Xiong Bearfield"
                },
                "author": "Cindy Xiong Bearfield",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.21898v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.21898v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.20815v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.20815v2",
                "updated": "2025-07-08T17:25:34Z",
                "updated_parsed": [
                    2025,
                    7,
                    8,
                    17,
                    25,
                    34,
                    1,
                    189,
                    0
                ],
                "published": "2025-06-25T20:29:46Z",
                "published_parsed": [
                    2025,
                    6,
                    25,
                    20,
                    29,
                    46,
                    2,
                    176,
                    0
                ],
                "title": "Dynamic Context-Aware Prompt Recommendation for Domain-Specific AI\n  Applications",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Dynamic Context-Aware Prompt Recommendation for Domain-Specific AI\n  Applications"
                },
                "summary": "LLM-powered applications are highly susceptible to the quality of user\nprompts, and crafting high-quality prompts can often be challenging especially\nfor domain-specific applications. This paper presents a novel dynamic\ncontext-aware prompt recommendation system for domain-specific AI applications.\nOur solution combines contextual query analysis, retrieval-augmented knowledge\ngrounding, hierarchical skill organization, and adaptive skill ranking to\ngenerate relevant and actionable prompt suggestions.\n  The system leverages behavioral telemetry and a two-stage hierarchical\nreasoning process to dynamically select and rank relevant skills, and\nsynthesizes prompts using both predefined and adaptive templates enhanced with\nfew-shot learning. Experiments on real-world datasets demonstrate that our\napproach achieves high usefulness and relevance, as validated by both automated\nand expert evaluations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM-powered applications are highly susceptible to the quality of user\nprompts, and crafting high-quality prompts can often be challenging especially\nfor domain-specific applications. This paper presents a novel dynamic\ncontext-aware prompt recommendation system for domain-specific AI applications.\nOur solution combines contextual query analysis, retrieval-augmented knowledge\ngrounding, hierarchical skill organization, and adaptive skill ranking to\ngenerate relevant and actionable prompt suggestions.\n  The system leverages behavioral telemetry and a two-stage hierarchical\nreasoning process to dynamically select and rank relevant skills, and\nsynthesizes prompts using both predefined and adaptive templates enhanced with\nfew-shot learning. Experiments on real-world datasets demonstrate that our\napproach achieves high usefulness and relevance, as validated by both automated\nand expert evaluations."
                },
                "authors": [
                    {
                        "name": "Xinye Tang"
                    },
                    {
                        "name": "Haijun Zhai"
                    },
                    {
                        "name": "Chaitanya Belwal"
                    },
                    {
                        "name": "Vineeth Thayanithi"
                    },
                    {
                        "name": "Philip Baumann"
                    },
                    {
                        "name": "Yogesh K Roy"
                    }
                ],
                "author_detail": {
                    "name": "Yogesh K Roy"
                },
                "author": "Yogesh K Roy",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.20815v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.20815v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.06196v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.06196v1",
                "updated": "2025-07-08T17:22:32Z",
                "updated_parsed": [
                    2025,
                    7,
                    8,
                    17,
                    22,
                    32,
                    1,
                    189,
                    0
                ],
                "published": "2025-07-08T17:22:32Z",
                "published_parsed": [
                    2025,
                    7,
                    8,
                    17,
                    22,
                    32,
                    1,
                    189,
                    0
                ],
                "title": "UQLM: A Python Package for Uncertainty Quantification in Large Language\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "UQLM: A Python Package for Uncertainty Quantification in Large Language\n  Models"
                },
                "summary": "Hallucinations, defined as instances where Large Language Models (LLMs)\ngenerate false or misleading content, pose a significant challenge that impacts\nthe safety and trust of downstream applications. We introduce UQLM, a Python\npackage for LLM hallucination detection using state-of-the-art uncertainty\nquantification (UQ) techniques. This toolkit offers a suite of UQ-based scorers\nthat compute response-level confidence scores ranging from 0 to 1. This library\nprovides an off-the-shelf solution for UQ-based hallucination detection that\ncan be easily integrated to enhance the reliability of LLM outputs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Hallucinations, defined as instances where Large Language Models (LLMs)\ngenerate false or misleading content, pose a significant challenge that impacts\nthe safety and trust of downstream applications. We introduce UQLM, a Python\npackage for LLM hallucination detection using state-of-the-art uncertainty\nquantification (UQ) techniques. This toolkit offers a suite of UQ-based scorers\nthat compute response-level confidence scores ranging from 0 to 1. This library\nprovides an off-the-shelf solution for UQ-based hallucination detection that\ncan be easily integrated to enhance the reliability of LLM outputs."
                },
                "authors": [
                    {
                        "name": "Dylan Bouchard"
                    },
                    {
                        "name": "Mohit Singh Chauhan"
                    },
                    {
                        "name": "David Skarbrevik"
                    },
                    {
                        "name": "Ho-Kyeong Ra"
                    },
                    {
                        "name": "Viren Bajaj"
                    },
                    {
                        "name": "Zeya Ahmad"
                    }
                ],
                "author_detail": {
                    "name": "Zeya Ahmad"
                },
                "author": "Zeya Ahmad",
                "arxiv_comment": "Submitted to Journal of Machine Learning Research (MLOSS); UQLM\n  Repository: https://github.com/cvs-health/uqlm",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.06196v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.06196v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.06192v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.06192v1",
                "updated": "2025-07-08T17:20:34Z",
                "updated_parsed": [
                    2025,
                    7,
                    8,
                    17,
                    20,
                    34,
                    1,
                    189,
                    0
                ],
                "published": "2025-07-08T17:20:34Z",
                "published_parsed": [
                    2025,
                    7,
                    8,
                    17,
                    20,
                    34,
                    1,
                    189,
                    0
                ],
                "title": "SQLBarber: A System Leveraging Large Language Models to Generate\n  Customized and Realistic SQL Workloads",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SQLBarber: A System Leveraging Large Language Models to Generate\n  Customized and Realistic SQL Workloads"
                },
                "summary": "Database research and development often require a large number of SQL queries\nfor benchmarking purposes. However, acquiring real-world SQL queries is\nchallenging due to privacy concerns, and existing SQL generation methods are\nlimited in customization and in satisfying realistic constraints. To address\nthis issue, we present SQLBarber, a system based on Large Language Models\n(LLMs) to generate customized and realistic SQL workloads. SQLBarber (i)\neliminates the need for users to manually craft SQL templates in advance, while\nproviding the flexibility to accept natural language specifications to\nconstrain SQL templates, (ii) scales efficiently to generate large volumes of\nqueries matching any user-defined cost distribution (e.g., cardinality and\nexecution plan cost), and (iii) uses execution statistics from Amazon Redshift\nand Snowflake to derive SQL template specifications and query cost\ndistributions that reflect real-world query characteristics. SQLBarber\nintroduces (i) a declarative interface for users to effortlessly generate\ncustomized SQL templates, (ii) an LLM-powered pipeline augmented with a\nself-correction module that profiles, refines, and prunes SQL templates based\non query costs, and (iii) a Bayesian Optimizer to efficiently explore different\npredicate values and identify a set of queries that satisfy the target cost\ndistribution. We construct and open-source ten benchmarks of varying difficulty\nlevels and target query cost distributions based on real-world statistics from\nSnowflake and Amazon Redshift. Extensive experiments on these benchmarks show\nthat SQLBarber is the only system that can generate customized SQL templates.\nIt reduces query generation time by one to three orders of magnitude, and\nsignificantly improves alignment with the target cost distribution, compared\nwith existing methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Database research and development often require a large number of SQL queries\nfor benchmarking purposes. However, acquiring real-world SQL queries is\nchallenging due to privacy concerns, and existing SQL generation methods are\nlimited in customization and in satisfying realistic constraints. To address\nthis issue, we present SQLBarber, a system based on Large Language Models\n(LLMs) to generate customized and realistic SQL workloads. SQLBarber (i)\neliminates the need for users to manually craft SQL templates in advance, while\nproviding the flexibility to accept natural language specifications to\nconstrain SQL templates, (ii) scales efficiently to generate large volumes of\nqueries matching any user-defined cost distribution (e.g., cardinality and\nexecution plan cost), and (iii) uses execution statistics from Amazon Redshift\nand Snowflake to derive SQL template specifications and query cost\ndistributions that reflect real-world query characteristics. SQLBarber\nintroduces (i) a declarative interface for users to effortlessly generate\ncustomized SQL templates, (ii) an LLM-powered pipeline augmented with a\nself-correction module that profiles, refines, and prunes SQL templates based\non query costs, and (iii) a Bayesian Optimizer to efficiently explore different\npredicate values and identify a set of queries that satisfy the target cost\ndistribution. We construct and open-source ten benchmarks of varying difficulty\nlevels and target query cost distributions based on real-world statistics from\nSnowflake and Amazon Redshift. Extensive experiments on these benchmarks show\nthat SQLBarber is the only system that can generate customized SQL templates.\nIt reduces query generation time by one to three orders of magnitude, and\nsignificantly improves alignment with the target cost distribution, compared\nwith existing methods."
                },
                "authors": [
                    {
                        "name": "Jiale Lao"
                    },
                    {
                        "name": "Immanuel Trummer"
                    }
                ],
                "author_detail": {
                    "name": "Immanuel Trummer"
                },
                "author": "Immanuel Trummer",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.06192v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.06192v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.03711v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.03711v3",
                "updated": "2025-07-09T02:09:05Z",
                "updated_parsed": [
                    2025,
                    7,
                    9,
                    2,
                    9,
                    5,
                    2,
                    190,
                    0
                ],
                "published": "2025-07-04T16:50:40Z",
                "published_parsed": [
                    2025,
                    7,
                    4,
                    16,
                    50,
                    40,
                    4,
                    185,
                    0
                ],
                "title": "Can LLMs Play Ã Än Quan Game? A Study of Multi-Step Planning and\n  Decision Making",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Can LLMs Play Ã Än Quan Game? A Study of Multi-Step Planning and\n  Decision Making"
                },
                "summary": "In this paper, we explore the ability of large language models (LLMs) to plan\nand make decisions through the lens of the traditional Vietnamese board game,\n\\^O \\u{A}n Quan. This game, which involves a series of strategic token\nmovements and captures, offers a unique environment for evaluating the\ndecision-making and strategic capabilities of LLMs. Specifically, we develop\nvarious agent personas, ranging from aggressive to defensive, and employ the\n\\^O \\u{A}n Quan game as a testbed for assessing LLM performance across\ndifferent strategies. Through experimentation with models like\nLlama-3.2-3B-Instruct, Llama-3.1-8B-Instruct, and Llama-3.3-70B-Instruct, we\naim to understand how these models execute strategic decision-making, plan\nmoves, and manage dynamic game states. The results will offer insights into the\nstrengths and weaknesses of LLMs in terms of reasoning and strategy,\ncontributing to a deeper understanding of their general capabilities.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this paper, we explore the ability of large language models (LLMs) to plan\nand make decisions through the lens of the traditional Vietnamese board game,\n\\^O \\u{A}n Quan. This game, which involves a series of strategic token\nmovements and captures, offers a unique environment for evaluating the\ndecision-making and strategic capabilities of LLMs. Specifically, we develop\nvarious agent personas, ranging from aggressive to defensive, and employ the\n\\^O \\u{A}n Quan game as a testbed for assessing LLM performance across\ndifferent strategies. Through experimentation with models like\nLlama-3.2-3B-Instruct, Llama-3.1-8B-Instruct, and Llama-3.3-70B-Instruct, we\naim to understand how these models execute strategic decision-making, plan\nmoves, and manage dynamic game states. The results will offer insights into the\nstrengths and weaknesses of LLMs in terms of reasoning and strategy,\ncontributing to a deeper understanding of their general capabilities."
                },
                "authors": [
                    {
                        "name": "Sang Quang Nguyen"
                    },
                    {
                        "name": "Kiet Van Nguyen"
                    },
                    {
                        "name": "Vinh-Tiep Nguyen"
                    },
                    {
                        "name": "Thanh Duc Ngo"
                    },
                    {
                        "name": "Ngan Luu-Thuy Nguyen"
                    },
                    {
                        "name": "Duy-Dinh Le"
                    }
                ],
                "author_detail": {
                    "name": "Duy-Dinh Le"
                },
                "author": "Duy-Dinh Le",
                "arxiv_comment": "Accepted paper at MAPR 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.03711v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.03711v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.06185v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.06185v1",
                "updated": "2025-07-08T17:11:13Z",
                "updated_parsed": [
                    2025,
                    7,
                    8,
                    17,
                    11,
                    13,
                    1,
                    189,
                    0
                ],
                "published": "2025-07-08T17:11:13Z",
                "published_parsed": [
                    2025,
                    7,
                    8,
                    17,
                    11,
                    13,
                    1,
                    189,
                    0
                ],
                "title": "Hidden Prompts in Manuscripts Exploit AI-Assisted Peer Review",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Hidden Prompts in Manuscripts Exploit AI-Assisted Peer Review"
                },
                "summary": "In July 2025, 18 academic manuscripts on the preprint website arXiv were\nfound to contain hidden instructions known as prompts designed to manipulate\nAI-assisted peer review. Instructions such as \"GIVE A POSITIVE REVIEW ONLY\"\nwere concealed using techniques like white-colored text. Author responses\nvaried: one planned to withdraw the affected paper, while another defended the\npractice as legitimate testing of reviewer compliance. This commentary analyzes\nthis practice as a novel form of research misconduct. We examine the technique\nof prompt injection in large language models (LLMs), revealing four types of\nhidden prompts, ranging from simple positive review commands to detailed\nevaluation frameworks. The defense that prompts served as \"honeypots\" to detect\nreviewers improperly using AI fails under examination--the consistently\nself-serving nature of prompt instructions indicates intent to manipulate.\nPublishers maintain inconsistent policies: Elsevier prohibits AI use in peer\nreview entirely, while Springer Nature permits limited use with disclosure\nrequirements. The incident exposes systematic vulnerabilities extending beyond\npeer review to any automated system processing scholarly texts, including\nplagiarism detection and citation indexing. Our analysis underscores the need\nfor coordinated technical screening at submission portals and harmonized\npolicies governing generative AI (GenAI) use in academic evaluation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In July 2025, 18 academic manuscripts on the preprint website arXiv were\nfound to contain hidden instructions known as prompts designed to manipulate\nAI-assisted peer review. Instructions such as \"GIVE A POSITIVE REVIEW ONLY\"\nwere concealed using techniques like white-colored text. Author responses\nvaried: one planned to withdraw the affected paper, while another defended the\npractice as legitimate testing of reviewer compliance. This commentary analyzes\nthis practice as a novel form of research misconduct. We examine the technique\nof prompt injection in large language models (LLMs), revealing four types of\nhidden prompts, ranging from simple positive review commands to detailed\nevaluation frameworks. The defense that prompts served as \"honeypots\" to detect\nreviewers improperly using AI fails under examination--the consistently\nself-serving nature of prompt instructions indicates intent to manipulate.\nPublishers maintain inconsistent policies: Elsevier prohibits AI use in peer\nreview entirely, while Springer Nature permits limited use with disclosure\nrequirements. The incident exposes systematic vulnerabilities extending beyond\npeer review to any automated system processing scholarly texts, including\nplagiarism detection and citation indexing. Our analysis underscores the need\nfor coordinated technical screening at submission portals and harmonized\npolicies governing generative AI (GenAI) use in academic evaluation."
                },
                "authors": [
                    {
                        "name": "Zhicheng Lin"
                    }
                ],
                "author_detail": {
                    "name": "Zhicheng Lin"
                },
                "author": "Zhicheng Lin",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.06185v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.06185v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.05201v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.05201v2",
                "updated": "2025-07-08T17:08:06Z",
                "updated_parsed": [
                    2025,
                    7,
                    8,
                    17,
                    8,
                    6,
                    1,
                    189,
                    0
                ],
                "published": "2025-07-07T17:01:44Z",
                "published_parsed": [
                    2025,
                    7,
                    7,
                    17,
                    1,
                    44,
                    0,
                    188,
                    0
                ],
                "title": "MedGemma Technical Report",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MedGemma Technical Report"
                },
                "summary": "Artificial intelligence (AI) has significant potential in healthcare\napplications, but its training and deployment faces challenges due to\nhealthcare's diverse data, complex tasks, and the need to preserve privacy.\nFoundation models that perform well on medical tasks and require less\ntask-specific tuning data are critical to accelerate the development of\nhealthcare AI applications. We introduce MedGemma, a collection of medical\nvision-language foundation models based on Gemma 3 4B and 27B. MedGemma\ndemonstrates advanced medical understanding and reasoning on images and text,\nsignificantly exceeding the performance of similar-sized generative models and\napproaching the performance of task-specific models, while maintaining the\ngeneral capabilities of the Gemma 3 base models. For out-of-distribution tasks,\nMedGemma achieves 2.6-10% improvement on medical multimodal question answering,\n15.5-18.1% improvement on chest X-ray finding classification, and 10.8%\nimprovement on agentic evaluations compared to the base models. Fine-tuning\nMedGemma further improves performance in subdomains, reducing errors in\nelectronic health record information retrieval by 50% and reaching comparable\nperformance to existing specialized state-of-the-art methods for pneumothorax\nclassification and histopathology patch classification. We additionally\nintroduce MedSigLIP, a medically-tuned vision encoder derived from SigLIP.\nMedSigLIP powers the visual understanding capabilities of MedGemma and as an\nencoder achieves comparable or better performance than specialized medical\nimage encoders. Taken together, the MedGemma collection provides a strong\nfoundation of medical image and text capabilities, with potential to\nsignificantly accelerate medical research and development of downstream\napplications. The MedGemma collection, including tutorials and model weights,\ncan be found at https://goo.gle/medgemma.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Artificial intelligence (AI) has significant potential in healthcare\napplications, but its training and deployment faces challenges due to\nhealthcare's diverse data, complex tasks, and the need to preserve privacy.\nFoundation models that perform well on medical tasks and require less\ntask-specific tuning data are critical to accelerate the development of\nhealthcare AI applications. We introduce MedGemma, a collection of medical\nvision-language foundation models based on Gemma 3 4B and 27B. MedGemma\ndemonstrates advanced medical understanding and reasoning on images and text,\nsignificantly exceeding the performance of similar-sized generative models and\napproaching the performance of task-specific models, while maintaining the\ngeneral capabilities of the Gemma 3 base models. For out-of-distribution tasks,\nMedGemma achieves 2.6-10% improvement on medical multimodal question answering,\n15.5-18.1% improvement on chest X-ray finding classification, and 10.8%\nimprovement on agentic evaluations compared to the base models. Fine-tuning\nMedGemma further improves performance in subdomains, reducing errors in\nelectronic health record information retrieval by 50% and reaching comparable\nperformance to existing specialized state-of-the-art methods for pneumothorax\nclassification and histopathology patch classification. We additionally\nintroduce MedSigLIP, a medically-tuned vision encoder derived from SigLIP.\nMedSigLIP powers the visual understanding capabilities of MedGemma and as an\nencoder achieves comparable or better performance than specialized medical\nimage encoders. Taken together, the MedGemma collection provides a strong\nfoundation of medical image and text capabilities, with potential to\nsignificantly accelerate medical research and development of downstream\napplications. The MedGemma collection, including tutorials and model weights,\ncan be found at https://goo.gle/medgemma."
                },
                "authors": [
                    {
                        "name": "Andrew Sellergren"
                    },
                    {
                        "name": "Sahar Kazemzadeh"
                    },
                    {
                        "name": "Tiam Jaroensri"
                    },
                    {
                        "name": "Atilla Kiraly"
                    },
                    {
                        "name": "Madeleine Traverse"
                    },
                    {
                        "name": "Timo Kohlberger"
                    },
                    {
                        "name": "Shawn Xu"
                    },
                    {
                        "name": "Fayaz Jamil"
                    },
                    {
                        "name": "CÃ­an Hughes"
                    },
                    {
                        "name": "Charles Lau"
                    },
                    {
                        "name": "Justin Chen"
                    },
                    {
                        "name": "Fereshteh Mahvar"
                    },
                    {
                        "name": "Liron Yatziv"
                    },
                    {
                        "name": "Tiffany Chen"
                    },
                    {
                        "name": "Bram Sterling"
                    },
                    {
                        "name": "Stefanie Anna Baby"
                    },
                    {
                        "name": "Susanna Maria Baby"
                    },
                    {
                        "name": "Jeremy Lai"
                    },
                    {
                        "name": "Samuel Schmidgall"
                    },
                    {
                        "name": "Lu Yang"
                    },
                    {
                        "name": "Kejia Chen"
                    },
                    {
                        "name": "Per Bjornsson"
                    },
                    {
                        "name": "Shashir Reddy"
                    },
                    {
                        "name": "Ryan Brush"
                    },
                    {
                        "name": "Kenneth Philbrick"
                    },
                    {
                        "name": "Howard Hu"
                    },
                    {
                        "name": "Howard Yang"
                    },
                    {
                        "name": "Richa Tiwari"
                    },
                    {
                        "name": "Sunny Jansen"
                    },
                    {
                        "name": "Preeti Singh"
                    },
                    {
                        "name": "Yun Liu"
                    },
                    {
                        "name": "Shekoofeh Azizi"
                    },
                    {
                        "name": "Aishwarya Kamath"
                    },
                    {
                        "name": "Johan Ferret"
                    },
                    {
                        "name": "Shreya Pathak"
                    },
                    {
                        "name": "Nino Vieillard"
                    },
                    {
                        "name": "Ramona Merhej"
                    },
                    {
                        "name": "Sarah Perrin"
                    },
                    {
                        "name": "Tatiana Matejovicova"
                    },
                    {
                        "name": "Alexandre RamÃ©"
                    },
                    {
                        "name": "Morgane Riviere"
                    },
                    {
                        "name": "Louis Rouillard"
                    },
                    {
                        "name": "Thomas Mesnard"
                    },
                    {
                        "name": "Geoffrey Cideron"
                    },
                    {
                        "name": "Jean-bastien Grill"
                    },
                    {
                        "name": "Sabela Ramos"
                    },
                    {
                        "name": "Edouard Yvinec"
                    },
                    {
                        "name": "Michelle Casbon"
                    },
                    {
                        "name": "Elena Buchatskaya"
                    },
                    {
                        "name": "Jean-Baptiste Alayrac"
                    },
                    {
                        "name": "Dmitry Lepikhin"
                    },
                    {
                        "name": "Vlad Feinberg"
                    },
                    {
                        "name": "Sebastian Borgeaud"
                    },
                    {
                        "name": "Alek Andreev"
                    },
                    {
                        "name": "Cassidy Hardin"
                    },
                    {
                        "name": "Robert Dadashi"
                    },
                    {
                        "name": "LÃ©onard Hussenot"
                    },
                    {
                        "name": "Armand Joulin"
                    },
                    {
                        "name": "Olivier Bachem"
                    },
                    {
                        "name": "Yossi Matias"
                    },
                    {
                        "name": "Katherine Chou"
                    },
                    {
                        "name": "Avinatan Hassidim"
                    },
                    {
                        "name": "Kavi Goel"
                    },
                    {
                        "name": "Clement Farabet"
                    },
                    {
                        "name": "Joelle Barral"
                    },
                    {
                        "name": "Tris Warkentin"
                    },
                    {
                        "name": "Jonathon Shlens"
                    },
                    {
                        "name": "David Fleet"
                    },
                    {
                        "name": "Victor Cotruta"
                    },
                    {
                        "name": "Omar Sanseviero"
                    },
                    {
                        "name": "Gus Martins"
                    },
                    {
                        "name": "Phoebe Kirk"
                    },
                    {
                        "name": "Anand Rao"
                    },
                    {
                        "name": "Shravya Shetty"
                    },
                    {
                        "name": "David F. Steiner"
                    },
                    {
                        "name": "Can Kirmizibayrak"
                    },
                    {
                        "name": "Rory Pilgrim"
                    },
                    {
                        "name": "Daniel Golden"
                    },
                    {
                        "name": "Lin Yang"
                    }
                ],
                "author_detail": {
                    "name": "Lin Yang"
                },
                "author": "Lin Yang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.05201v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.05201v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.06179v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.06179v1",
                "updated": "2025-07-08T17:00:05Z",
                "updated_parsed": [
                    2025,
                    7,
                    8,
                    17,
                    0,
                    5,
                    1,
                    189,
                    0
                ],
                "published": "2025-07-08T17:00:05Z",
                "published_parsed": [
                    2025,
                    7,
                    8,
                    17,
                    0,
                    5,
                    1,
                    189,
                    0
                ],
                "title": "Dynamic Slimmable Networks for Efficient Speech Separation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Dynamic Slimmable Networks for Efficient Speech Separation"
                },
                "summary": "Recent progress in speech separation has been largely driven by advances in\ndeep neural networks, yet their high computational and memory requirements\nhinder deployment on resource-constrained devices. A significant inefficiency\nin conventional systems arises from using static network architectures that\nmaintain constant computational complexity across all input segments,\nregardless of their characteristics. This approach is sub-optimal for simpler\nsegments that do not require intensive processing, such as silence or\nnon-overlapping speech. To address this limitation, we propose a dynamic\nslimmable network (DSN) for speech separation that adaptively adjusts its\ncomputational complexity based on the input signal. The DSN combines a\nslimmable network, which can operate at different network widths, with a\nlightweight gating module that dynamically determines the required width by\nanalyzing the local input characteristics. To balance performance and\nefficiency, we introduce a signal-dependent complexity loss that penalizes\nunnecessary computation based on segmental reconstruction error. Experiments on\nclean and noisy two-speaker mixtures from the WSJ0-2mix and WHAM! datasets show\nthat the DSN achieves a better performance-efficiency trade-off than\nindividually trained static networks of different sizes.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent progress in speech separation has been largely driven by advances in\ndeep neural networks, yet their high computational and memory requirements\nhinder deployment on resource-constrained devices. A significant inefficiency\nin conventional systems arises from using static network architectures that\nmaintain constant computational complexity across all input segments,\nregardless of their characteristics. This approach is sub-optimal for simpler\nsegments that do not require intensive processing, such as silence or\nnon-overlapping speech. To address this limitation, we propose a dynamic\nslimmable network (DSN) for speech separation that adaptively adjusts its\ncomputational complexity based on the input signal. The DSN combines a\nslimmable network, which can operate at different network widths, with a\nlightweight gating module that dynamically determines the required width by\nanalyzing the local input characteristics. To balance performance and\nefficiency, we introduce a signal-dependent complexity loss that penalizes\nunnecessary computation based on segmental reconstruction error. Experiments on\nclean and noisy two-speaker mixtures from the WSJ0-2mix and WHAM! datasets show\nthat the DSN achieves a better performance-efficiency trade-off than\nindividually trained static networks of different sizes."
                },
                "authors": [
                    {
                        "name": "Mohamed Elminshawi"
                    },
                    {
                        "name": "Srikanth Raj Chetupalli"
                    },
                    {
                        "name": "EmanuÃ«l A. P. Habets"
                    }
                ],
                "author_detail": {
                    "name": "EmanuÃ«l A. P. Habets"
                },
                "author": "EmanuÃ«l A. P. Habets",
                "arxiv_comment": "This manuscript has been submitted to IEEE Transactions on Audio,\n  Speech and Language Processing",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.06179v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.06179v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.AS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.06171v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.06171v1",
                "updated": "2025-07-08T16:52:37Z",
                "updated_parsed": [
                    2025,
                    7,
                    8,
                    16,
                    52,
                    37,
                    1,
                    189,
                    0
                ],
                "published": "2025-07-08T16:52:37Z",
                "published_parsed": [
                    2025,
                    7,
                    8,
                    16,
                    52,
                    37,
                    1,
                    189,
                    0
                ],
                "title": "Data-Semantics-Aware Recommendation of Diverse Pivot Tables",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Data-Semantics-Aware Recommendation of Diverse Pivot Tables"
                },
                "summary": "Data summarization is essential to discover insights from large datasets. In\na spreadsheets, pivot tables offer a convenient way to summarize tabular data\nby computing aggregates over some attributes, grouped by others. However,\nidentifying attribute combinations that will result in useful pivot tables\nremains a challenge, especially for high-dimensional datasets. We formalize the\nproblem of automatically recommending insightful and interpretable pivot\ntables, eliminating the tedious manual process. A crucial aspect of\nrecommending a set of pivot tables is to diversify them. Traditional works\ninadequately address the table-diversification problem, which leads us to\nconsider the problem of pivot table diversification.\n  We present SAGE, a data-semantics-aware system for recommending k-budgeted\ndiverse pivot tables, overcoming the shortcomings of prior work for top-k\nrecommendations that cause redundancy. SAGE ensures that each pivot table is\ninsightful, interpretable, and adaptive to the user's actions and preferences,\nwhile also guaranteeing that the set of pivot tables are different from each\nother, offering a diverse recommendation. We make two key technical\ncontributions: (1) a data-semantics-aware model to measure the utility of a\nsingle pivot table and the diversity of a set of pivot tables, and (2) a\nscalable greedy algorithm that can efficiently select a set of diverse pivot\ntables of high utility, by leveraging data semantics to significantly reduce\nthe combinatorial search space. Our extensive experiments on three real-world\ndatasets show that SAGE outperforms alternative approaches, and efficiently\nscales to accommodate high-dimensional datasets. Additionally, we present\nseveral case studies to highlight SAGE's qualitative effectiveness over\ncommercial software and Large Language Models (LLMs).",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Data summarization is essential to discover insights from large datasets. In\na spreadsheets, pivot tables offer a convenient way to summarize tabular data\nby computing aggregates over some attributes, grouped by others. However,\nidentifying attribute combinations that will result in useful pivot tables\nremains a challenge, especially for high-dimensional datasets. We formalize the\nproblem of automatically recommending insightful and interpretable pivot\ntables, eliminating the tedious manual process. A crucial aspect of\nrecommending a set of pivot tables is to diversify them. Traditional works\ninadequately address the table-diversification problem, which leads us to\nconsider the problem of pivot table diversification.\n  We present SAGE, a data-semantics-aware system for recommending k-budgeted\ndiverse pivot tables, overcoming the shortcomings of prior work for top-k\nrecommendations that cause redundancy. SAGE ensures that each pivot table is\ninsightful, interpretable, and adaptive to the user's actions and preferences,\nwhile also guaranteeing that the set of pivot tables are different from each\nother, offering a diverse recommendation. We make two key technical\ncontributions: (1) a data-semantics-aware model to measure the utility of a\nsingle pivot table and the diversity of a set of pivot tables, and (2) a\nscalable greedy algorithm that can efficiently select a set of diverse pivot\ntables of high utility, by leveraging data semantics to significantly reduce\nthe combinatorial search space. Our extensive experiments on three real-world\ndatasets show that SAGE outperforms alternative approaches, and efficiently\nscales to accommodate high-dimensional datasets. Additionally, we present\nseveral case studies to highlight SAGE's qualitative effectiveness over\ncommercial software and Large Language Models (LLMs)."
                },
                "authors": [
                    {
                        "name": "Whanhee Cho"
                    },
                    {
                        "name": "Anna Fariha"
                    }
                ],
                "author_detail": {
                    "name": "Anna Fariha"
                },
                "author": "Anna Fariha",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.06171v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.06171v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.04687v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.04687v2",
                "updated": "2025-07-08T16:51:53Z",
                "updated_parsed": [
                    2025,
                    7,
                    8,
                    16,
                    51,
                    53,
                    1,
                    189,
                    0
                ],
                "published": "2025-07-07T06:08:45Z",
                "published_parsed": [
                    2025,
                    7,
                    7,
                    6,
                    8,
                    45,
                    0,
                    188,
                    0
                ],
                "title": "LAKEGEN: A LLM-based Tabular Corpus Generator for Evaluating Dataset\n  Discovery in Data Lakes",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LAKEGEN: A LLM-based Tabular Corpus Generator for Evaluating Dataset\n  Discovery in Data Lakes"
                },
                "summary": "How to generate a large, realistic set of tables along with joinability\nrelationships, to stress-test dataset discovery methods? Dataset discovery\nmethods aim to automatically identify related data assets in a data lake. The\ndevelopment and evaluation of such solutions for customers from a wide range of\nbusiness domains, relies on diverse, high quality and domain-specific tabular\nbenchmarks. Large language models (LLMs) are trained on a wide variety of text\ndata, which can provide a strong foundation of general and domain-specific\nknowledge. In this paper, we ask the question -- \\textit{can we leverage LLMs\nto generate a tabular benchmark adequate for evaluating the dataset discovery\nsolutions?} In particular, we focus on the task of finding joinable tables\nwhich is the cornerstone of virtually every dataset discovery method. Current\ncorpora for evaluating dataset discovery methods are mainly based on subsets of\nopen data, and they suffer from three important issues: $i)$ they focus on very\ncommon and generic data types (e.g., address, id, name, etc.); $ii)$ they do\nnot contain human-annotated column pairs; instead, practitioners synthesize\nground truth using table splits (e.g., horizontal for table union search and\nvertical ones for joinability) and $iii)$ they do not focus on semantic column\nrelationships.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "How to generate a large, realistic set of tables along with joinability\nrelationships, to stress-test dataset discovery methods? Dataset discovery\nmethods aim to automatically identify related data assets in a data lake. The\ndevelopment and evaluation of such solutions for customers from a wide range of\nbusiness domains, relies on diverse, high quality and domain-specific tabular\nbenchmarks. Large language models (LLMs) are trained on a wide variety of text\ndata, which can provide a strong foundation of general and domain-specific\nknowledge. In this paper, we ask the question -- \\textit{can we leverage LLMs\nto generate a tabular benchmark adequate for evaluating the dataset discovery\nsolutions?} In particular, we focus on the task of finding joinable tables\nwhich is the cornerstone of virtually every dataset discovery method. Current\ncorpora for evaluating dataset discovery methods are mainly based on subsets of\nopen data, and they suffer from three important issues: $i)$ they focus on very\ncommon and generic data types (e.g., address, id, name, etc.); $ii)$ they do\nnot contain human-annotated column pairs; instead, practitioners synthesize\nground truth using table splits (e.g., horizontal for table union search and\nvertical ones for joinability) and $iii)$ they do not focus on semantic column\nrelationships."
                },
                "authors": [
                    {
                        "name": "Zhenwei Dai"
                    },
                    {
                        "name": "Chuan Lei"
                    },
                    {
                        "name": "Asterios Katsifodimos"
                    },
                    {
                        "name": "Xiao Qin"
                    },
                    {
                        "name": "Christos Faloutsos"
                    },
                    {
                        "name": "Huzefa Rangwala"
                    }
                ],
                "author_detail": {
                    "name": "Huzefa Rangwala"
                },
                "author": "Huzefa Rangwala",
                "arxiv_comment": "13 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.04687v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.04687v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.06167v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.06167v2",
                "updated": "2025-07-09T01:36:17Z",
                "updated_parsed": [
                    2025,
                    7,
                    9,
                    1,
                    36,
                    17,
                    2,
                    190,
                    0
                ],
                "published": "2025-07-08T16:47:16Z",
                "published_parsed": [
                    2025,
                    7,
                    8,
                    16,
                    47,
                    16,
                    1,
                    189,
                    0
                ],
                "title": "Skywork-R1V3 Technical Report",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Skywork-R1V3 Technical Report"
                },
                "summary": "We introduce Skywork-R1V3, an advanced, open-source vision-language model\n(VLM) that pioneers a new approach to visual reasoning. Its key innovation lies\nin effectively transferring reasoning skills from text-only Large Language\nModels (LLMs) to visual tasks. The strong performance of Skywork-R1V3 primarily\nstems from our elaborate post-training RL framework, which effectively\nactivates and enhances the model's reasoning ability, without the need for\nadditional continue pre-training. Through this framework, we further uncover\nthe fundamental role of the connector module in achieving robust cross-modal\nalignment for multimodal reasoning models. In addition, we introduce a unique\nindicator of reasoning capability, the entropy of critical reasoning tokens,\nwhich has proven highly effective for checkpoint selection during RL training.\nSkywork-R1V3 achieves state-of-the-art results on MMMU, significantly improving\nfrom 64.3% to 76.0%. This performance matches entry-level human capabilities.\nRemarkably, our RL-powered post-training approach enables even the 38B\nparameter model to rival top closed-source VLMs. The implementation\nsuccessfully transfers mathematical reasoning to other subject-related\nreasoning tasks. We also include an analysis of curriculum learning and\nreinforcement finetuning strategies, along with a broader discussion on\nmultimodal reasoning. Skywork-R1V3 represents a significant leap in multimodal\nreasoning, showcasing RL as a powerful engine for advancing open-source VLM\ncapabilities.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce Skywork-R1V3, an advanced, open-source vision-language model\n(VLM) that pioneers a new approach to visual reasoning. Its key innovation lies\nin effectively transferring reasoning skills from text-only Large Language\nModels (LLMs) to visual tasks. The strong performance of Skywork-R1V3 primarily\nstems from our elaborate post-training RL framework, which effectively\nactivates and enhances the model's reasoning ability, without the need for\nadditional continue pre-training. Through this framework, we further uncover\nthe fundamental role of the connector module in achieving robust cross-modal\nalignment for multimodal reasoning models. In addition, we introduce a unique\nindicator of reasoning capability, the entropy of critical reasoning tokens,\nwhich has proven highly effective for checkpoint selection during RL training.\nSkywork-R1V3 achieves state-of-the-art results on MMMU, significantly improving\nfrom 64.3% to 76.0%. This performance matches entry-level human capabilities.\nRemarkably, our RL-powered post-training approach enables even the 38B\nparameter model to rival top closed-source VLMs. The implementation\nsuccessfully transfers mathematical reasoning to other subject-related\nreasoning tasks. We also include an analysis of curriculum learning and\nreinforcement finetuning strategies, along with a broader discussion on\nmultimodal reasoning. Skywork-R1V3 represents a significant leap in multimodal\nreasoning, showcasing RL as a powerful engine for advancing open-source VLM\ncapabilities."
                },
                "authors": [
                    {
                        "name": "Wei Shen"
                    },
                    {
                        "name": "Jiangbo Pei"
                    },
                    {
                        "name": "Yi Peng"
                    },
                    {
                        "name": "Xuchen Song"
                    },
                    {
                        "name": "Yang Liu"
                    },
                    {
                        "name": "Jian Peng"
                    },
                    {
                        "name": "Haofeng Sun"
                    },
                    {
                        "name": "Yunzhuo Hao"
                    },
                    {
                        "name": "Peiyu Wang"
                    },
                    {
                        "name": "Jianhao Zhang"
                    },
                    {
                        "name": "Yahui Zhou"
                    }
                ],
                "author_detail": {
                    "name": "Yahui Zhou"
                },
                "author": "Yahui Zhou",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.06167v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.06167v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.06141v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.06141v1",
                "updated": "2025-07-08T16:22:52Z",
                "updated_parsed": [
                    2025,
                    7,
                    8,
                    16,
                    22,
                    52,
                    1,
                    189,
                    0
                ],
                "published": "2025-07-08T16:22:52Z",
                "published_parsed": [
                    2025,
                    7,
                    8,
                    16,
                    22,
                    52,
                    1,
                    189,
                    0
                ],
                "title": "Large Language Models Predict Human Well-being -- But Not Equally\n  Everywhere",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models Predict Human Well-being -- But Not Equally\n  Everywhere"
                },
                "summary": "Subjective well-being is a key metric in economic, medical, and policy\ndecision-making. As artificial intelligence provides scalable tools for\nmodelling human outcomes, it is crucial to evaluate whether large language\nmodels (LLMs) can accurately predict well-being across diverse global\npopulations. We evaluate four leading LLMs using data from 64,000 individuals\nin 64 countries. While LLMs capture broad correlates such as income and health,\ntheir predictive accuracy decreases in countries underrepresented in the\ntraining data, highlighting systematic biases rooted in global digital and\neconomic inequality. A pre-registered experiment demonstrates that LLMs rely on\nsurface-level linguistic similarity rather than conceptual understanding,\nleading to systematic misestimations in unfamiliar or resource-limited\nsettings. Injecting findings from underrepresented contexts substantially\nenhances performance, but a significant gap remains. These results highlight\nboth the promise and limitations of LLMs in predicting global well-being,\nunderscoring the importance of robust validation prior to their implementation\nacross these areas.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Subjective well-being is a key metric in economic, medical, and policy\ndecision-making. As artificial intelligence provides scalable tools for\nmodelling human outcomes, it is crucial to evaluate whether large language\nmodels (LLMs) can accurately predict well-being across diverse global\npopulations. We evaluate four leading LLMs using data from 64,000 individuals\nin 64 countries. While LLMs capture broad correlates such as income and health,\ntheir predictive accuracy decreases in countries underrepresented in the\ntraining data, highlighting systematic biases rooted in global digital and\neconomic inequality. A pre-registered experiment demonstrates that LLMs rely on\nsurface-level linguistic similarity rather than conceptual understanding,\nleading to systematic misestimations in unfamiliar or resource-limited\nsettings. Injecting findings from underrepresented contexts substantially\nenhances performance, but a significant gap remains. These results highlight\nboth the promise and limitations of LLMs in predicting global well-being,\nunderscoring the importance of robust validation prior to their implementation\nacross these areas."
                },
                "authors": [
                    {
                        "name": "Pat Pataranutaporn"
                    },
                    {
                        "name": "Nattavudh Powdthavee"
                    },
                    {
                        "name": "Chayapatr Archiwaranguprok"
                    },
                    {
                        "name": "Pattie Maes"
                    }
                ],
                "author_detail": {
                    "name": "Pattie Maes"
                },
                "author": "Pattie Maes",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.06141v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.06141v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.06138v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.06138v1",
                "updated": "2025-07-08T16:20:43Z",
                "updated_parsed": [
                    2025,
                    7,
                    8,
                    16,
                    20,
                    43,
                    1,
                    189,
                    0
                ],
                "published": "2025-07-08T16:20:43Z",
                "published_parsed": [
                    2025,
                    7,
                    8,
                    16,
                    20,
                    43,
                    1,
                    189,
                    0
                ],
                "title": "Coding Triangle: How Does Large Language Model Understand Code?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Coding Triangle: How Does Large Language Model Understand Code?"
                },
                "summary": "Large language models (LLMs) have achieved remarkable progress in code\ngeneration, yet their true programming competence remains underexplored. We\nintroduce the Code Triangle framework, which systematically evaluates LLMs\nacross three fundamental dimensions: editorial analysis, code implementation,\nand test case generation. Through extensive experiments on competitive\nprogramming benchmarks, we reveal that while LLMs can form a self-consistent\nsystem across these dimensions, their solutions often lack the diversity and\nrobustness of human programmers. We identify a significant distribution shift\nbetween model cognition and human expertise, with model errors tending to\ncluster due to training data biases and limited reasoning transfer. Our study\ndemonstrates that incorporating human-generated editorials, solutions, and\ndiverse test cases, as well as leveraging model mixtures, can substantially\nenhance both the performance and robustness of LLMs. Furthermore, we reveal\nboth the consistency and inconsistency in the cognition of LLMs that may\nfacilitate self-reflection and self-improvement, providing a potential\ndirection for developing more powerful coding models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have achieved remarkable progress in code\ngeneration, yet their true programming competence remains underexplored. We\nintroduce the Code Triangle framework, which systematically evaluates LLMs\nacross three fundamental dimensions: editorial analysis, code implementation,\nand test case generation. Through extensive experiments on competitive\nprogramming benchmarks, we reveal that while LLMs can form a self-consistent\nsystem across these dimensions, their solutions often lack the diversity and\nrobustness of human programmers. We identify a significant distribution shift\nbetween model cognition and human expertise, with model errors tending to\ncluster due to training data biases and limited reasoning transfer. Our study\ndemonstrates that incorporating human-generated editorials, solutions, and\ndiverse test cases, as well as leveraging model mixtures, can substantially\nenhance both the performance and robustness of LLMs. Furthermore, we reveal\nboth the consistency and inconsistency in the cognition of LLMs that may\nfacilitate self-reflection and self-improvement, providing a potential\ndirection for developing more powerful coding models."
                },
                "authors": [
                    {
                        "name": "Taolin Zhang"
                    },
                    {
                        "name": "Zihan Ma"
                    },
                    {
                        "name": "Maosong Cao"
                    },
                    {
                        "name": "Junnan Liu"
                    },
                    {
                        "name": "Songyang Zhang"
                    },
                    {
                        "name": "Kai Chen"
                    }
                ],
                "author_detail": {
                    "name": "Kai Chen"
                },
                "author": "Kai Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.06138v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.06138v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.06137v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.06137v1",
                "updated": "2025-07-08T16:19:45Z",
                "updated_parsed": [
                    2025,
                    7,
                    8,
                    16,
                    19,
                    45,
                    1,
                    189,
                    0
                ],
                "published": "2025-07-08T16:19:45Z",
                "published_parsed": [
                    2025,
                    7,
                    8,
                    16,
                    19,
                    45,
                    1,
                    189,
                    0
                ],
                "title": "NeoBabel: A Multilingual Open Tower for Visual Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "NeoBabel: A Multilingual Open Tower for Visual Generation"
                },
                "summary": "Text-to-image generation advancements have been predominantly\nEnglish-centric, creating barriers for non-English speakers and perpetuating\ndigital inequities. While existing systems rely on translation pipelines, these\nintroduce semantic drift, computational overhead, and cultural misalignment. We\nintroduce NeoBabel, a novel multilingual image generation framework that sets a\nnew Pareto frontier in performance, efficiency and inclusivity, supporting six\nlanguages: English, Chinese, Dutch, French, Hindi, and Persian. The model is\ntrained using a combination of large-scale multilingual pretraining and\nhigh-resolution instruction tuning. To evaluate its capabilities, we expand two\nEnglish-only benchmarks to multilingual equivalents: m-GenEval and m-DPG.\nNeoBabel achieves state-of-the-art multilingual performance while retaining\nstrong English capability, scoring 0.75 on m-GenEval and 0.68 on m-DPG.\nNotably, it performs on par with leading models on English tasks while\noutperforming them by +0.11 and +0.09 on multilingual benchmarks, even though\nthese models are built on multilingual base LLMs. This demonstrates the\neffectiveness of our targeted alignment training for preserving and extending\ncrosslingual generalization. We further introduce two new metrics to rigorously\nassess multilingual alignment and robustness to code-mixed prompts. Notably,\nNeoBabel matches or exceeds English-only models while being 2-4x smaller. We\nrelease an open toolkit, including all code, model checkpoints, a curated\ndataset of 124M multilingual text-image pairs, and standardized multilingual\nevaluation protocols, to advance inclusive AI research. Our work demonstrates\nthat multilingual capability is not a trade-off but a catalyst for improved\nrobustness, efficiency, and cultural fidelity in generative AI.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Text-to-image generation advancements have been predominantly\nEnglish-centric, creating barriers for non-English speakers and perpetuating\ndigital inequities. While existing systems rely on translation pipelines, these\nintroduce semantic drift, computational overhead, and cultural misalignment. We\nintroduce NeoBabel, a novel multilingual image generation framework that sets a\nnew Pareto frontier in performance, efficiency and inclusivity, supporting six\nlanguages: English, Chinese, Dutch, French, Hindi, and Persian. The model is\ntrained using a combination of large-scale multilingual pretraining and\nhigh-resolution instruction tuning. To evaluate its capabilities, we expand two\nEnglish-only benchmarks to multilingual equivalents: m-GenEval and m-DPG.\nNeoBabel achieves state-of-the-art multilingual performance while retaining\nstrong English capability, scoring 0.75 on m-GenEval and 0.68 on m-DPG.\nNotably, it performs on par with leading models on English tasks while\noutperforming them by +0.11 and +0.09 on multilingual benchmarks, even though\nthese models are built on multilingual base LLMs. This demonstrates the\neffectiveness of our targeted alignment training for preserving and extending\ncrosslingual generalization. We further introduce two new metrics to rigorously\nassess multilingual alignment and robustness to code-mixed prompts. Notably,\nNeoBabel matches or exceeds English-only models while being 2-4x smaller. We\nrelease an open toolkit, including all code, model checkpoints, a curated\ndataset of 124M multilingual text-image pairs, and standardized multilingual\nevaluation protocols, to advance inclusive AI research. Our work demonstrates\nthat multilingual capability is not a trade-off but a catalyst for improved\nrobustness, efficiency, and cultural fidelity in generative AI."
                },
                "authors": [
                    {
                        "name": "Mohammad Mahdi Derakhshani"
                    },
                    {
                        "name": "Dheeraj Varghese"
                    },
                    {
                        "name": "Marzieh Fadaee"
                    },
                    {
                        "name": "Cees G. M. Snoek"
                    }
                ],
                "author_detail": {
                    "name": "Cees G. M. Snoek"
                },
                "author": "Cees G. M. Snoek",
                "arxiv_comment": "34 pages, 12 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.06137v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.06137v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.06134v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.06134v1",
                "updated": "2025-07-08T16:18:54Z",
                "updated_parsed": [
                    2025,
                    7,
                    8,
                    16,
                    18,
                    54,
                    1,
                    189,
                    0
                ],
                "published": "2025-07-08T16:18:54Z",
                "published_parsed": [
                    2025,
                    7,
                    8,
                    16,
                    18,
                    54,
                    1,
                    189,
                    0
                ],
                "title": "OpenAgentSafety: A Comprehensive Framework for Evaluating Real-World AI\n  Agent Safety",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "OpenAgentSafety: A Comprehensive Framework for Evaluating Real-World AI\n  Agent Safety"
                },
                "summary": "Recent advances in AI agents capable of solving complex, everyday tasks, from\nscheduling to customer service, have enabled deployment in real-world settings,\nbut their possibilities for unsafe behavior demands rigorous evaluation. While\nprior benchmarks have attempted to assess agent safety, most fall short by\nrelying on simulated environments, narrow task domains, or unrealistic tool\nabstractions. We introduce OpenAgentSafety, a comprehensive and modular\nframework for evaluating agent behavior across eight critical risk categories.\nUnlike prior work, our framework evaluates agents that interact with real\ntools, including web browsers, code execution environments, file systems, bash\nshells, and messaging platforms; and supports over 350 multi-turn, multi-user\ntasks spanning both benign and adversarial user intents. OpenAgentSafety is\ndesigned for extensibility, allowing researchers to add tools, tasks, websites,\nand adversarial strategies with minimal effort. It combines rule-based analysis\nwith LLM-as-judge assessments to detect both overt and subtle unsafe behaviors.\nEmpirical analysis of five prominent LLMs in agentic scenarios reveals unsafe\nbehavior in 51.2% of safety-vulnerable tasks with Claude-Sonnet-3.7, to 72.7%\nwith o3-mini, highlighting critical safety vulnerabilities and the need for\nstronger safeguards before real-world deployment.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advances in AI agents capable of solving complex, everyday tasks, from\nscheduling to customer service, have enabled deployment in real-world settings,\nbut their possibilities for unsafe behavior demands rigorous evaluation. While\nprior benchmarks have attempted to assess agent safety, most fall short by\nrelying on simulated environments, narrow task domains, or unrealistic tool\nabstractions. We introduce OpenAgentSafety, a comprehensive and modular\nframework for evaluating agent behavior across eight critical risk categories.\nUnlike prior work, our framework evaluates agents that interact with real\ntools, including web browsers, code execution environments, file systems, bash\nshells, and messaging platforms; and supports over 350 multi-turn, multi-user\ntasks spanning both benign and adversarial user intents. OpenAgentSafety is\ndesigned for extensibility, allowing researchers to add tools, tasks, websites,\nand adversarial strategies with minimal effort. It combines rule-based analysis\nwith LLM-as-judge assessments to detect both overt and subtle unsafe behaviors.\nEmpirical analysis of five prominent LLMs in agentic scenarios reveals unsafe\nbehavior in 51.2% of safety-vulnerable tasks with Claude-Sonnet-3.7, to 72.7%\nwith o3-mini, highlighting critical safety vulnerabilities and the need for\nstronger safeguards before real-world deployment."
                },
                "authors": [
                    {
                        "name": "Sanidhya Vijayvargiya"
                    },
                    {
                        "name": "Aditya Bharat Soni"
                    },
                    {
                        "name": "Xuhui Zhou"
                    },
                    {
                        "name": "Zora Zhiruo Wang"
                    },
                    {
                        "name": "Nouha Dziri"
                    },
                    {
                        "name": "Graham Neubig"
                    },
                    {
                        "name": "Maarten Sap"
                    }
                ],
                "author_detail": {
                    "name": "Maarten Sap"
                },
                "author": "Maarten Sap",
                "arxiv_comment": "19 pages, 10 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.06134v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.06134v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.06127v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.06127v1",
                "updated": "2025-07-08T16:14:17Z",
                "updated_parsed": [
                    2025,
                    7,
                    8,
                    16,
                    14,
                    17,
                    1,
                    189,
                    0
                ],
                "published": "2025-07-08T16:14:17Z",
                "published_parsed": [
                    2025,
                    7,
                    8,
                    16,
                    14,
                    17,
                    1,
                    189,
                    0
                ],
                "title": "PrefixAgent: An LLM-Powered Design Framework for Efficient Prefix Adder\n  Optimization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PrefixAgent: An LLM-Powered Design Framework for Efficient Prefix Adder\n  Optimization"
                },
                "summary": "Prefix adders are fundamental arithmetic circuits, but their design space\ngrows exponentially with bit-width, posing significant optimization challenges.\nPrevious works face limitations in performance, generalization, and\nscalability. To address these challenges, we propose PrefixAgent, a large\nlanguage model (LLM)-powered framework that enables efficient prefix adder\noptimization. Specifically, PrefixAgent reformulates the problem into subtasks\nincluding backbone synthesis and structure refinement, which effectively\nreduces the search space. More importantly, this new design perspective enables\nus to efficiently collect enormous high-quality data and reasoning traces with\nE-graph, which further results in an effective fine-tuning of LLM. Experimental\nresults show that PrefixAgent synthesizes prefix adders with consistently\nsmaller areas compared to baseline methods, while maintaining scalability and\ngeneralization in commercial EDA flows.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Prefix adders are fundamental arithmetic circuits, but their design space\ngrows exponentially with bit-width, posing significant optimization challenges.\nPrevious works face limitations in performance, generalization, and\nscalability. To address these challenges, we propose PrefixAgent, a large\nlanguage model (LLM)-powered framework that enables efficient prefix adder\noptimization. Specifically, PrefixAgent reformulates the problem into subtasks\nincluding backbone synthesis and structure refinement, which effectively\nreduces the search space. More importantly, this new design perspective enables\nus to efficiently collect enormous high-quality data and reasoning traces with\nE-graph, which further results in an effective fine-tuning of LLM. Experimental\nresults show that PrefixAgent synthesizes prefix adders with consistently\nsmaller areas compared to baseline methods, while maintaining scalability and\ngeneralization in commercial EDA flows."
                },
                "authors": [
                    {
                        "name": "Dongsheng Zuo"
                    },
                    {
                        "name": "Jiadong Zhu"
                    },
                    {
                        "name": "Yang Luo"
                    },
                    {
                        "name": "Yuzhe Ma"
                    }
                ],
                "author_detail": {
                    "name": "Yuzhe Ma"
                },
                "author": "Yuzhe Ma",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.06127v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.06127v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.06112v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.06112v1",
                "updated": "2025-07-08T15:52:17Z",
                "updated_parsed": [
                    2025,
                    7,
                    8,
                    15,
                    52,
                    17,
                    1,
                    189,
                    0
                ],
                "published": "2025-07-08T15:52:17Z",
                "published_parsed": [
                    2025,
                    7,
                    8,
                    15,
                    52,
                    17,
                    1,
                    189,
                    0
                ],
                "title": "Fun with flags: How Compilers Break and Fix Constant-Time Code",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Fun with flags: How Compilers Break and Fix Constant-Time Code"
                },
                "summary": "Developers rely on constant-time programming to prevent timing side-channel\nattacks. But these efforts can be undone by compilers, whose optimizations may\nsilently reintroduce leaks. While recent works have measured the extent of such\nleakage, they leave developers without actionable insights: which optimization\npasses are responsible, and how to disable them without modifying the compiler\nremains unclear.\n  In this paper, we conduct a qualitative analysis of how compiler\noptimizations break constant-time code. We construct a dataset of\ncompiler-introduced constant-time violations and analyze the internals of two\nwidely used compilers, GCC and LLVM, to identify the specific optimization\npasses responsible. Our key insight is that a small set of passes are at the\nroot of most leaks. To the best of our knowledge, we are also the first to\ncharacterize how the interactions between these passes contribute to leakage.\nBased on this analysis, we propose an original and practical mitigation that\nrequires no source code modification or custom compiler: disabling selected\noptimization passes via compiler flags. We show that this approach\nsignificantly reduces leakage with minimal performance overhead, offering an\nimmediately deployable defense for developers.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Developers rely on constant-time programming to prevent timing side-channel\nattacks. But these efforts can be undone by compilers, whose optimizations may\nsilently reintroduce leaks. While recent works have measured the extent of such\nleakage, they leave developers without actionable insights: which optimization\npasses are responsible, and how to disable them without modifying the compiler\nremains unclear.\n  In this paper, we conduct a qualitative analysis of how compiler\noptimizations break constant-time code. We construct a dataset of\ncompiler-introduced constant-time violations and analyze the internals of two\nwidely used compilers, GCC and LLVM, to identify the specific optimization\npasses responsible. Our key insight is that a small set of passes are at the\nroot of most leaks. To the best of our knowledge, we are also the first to\ncharacterize how the interactions between these passes contribute to leakage.\nBased on this analysis, we propose an original and practical mitigation that\nrequires no source code modification or custom compiler: disabling selected\noptimization passes via compiler flags. We show that this approach\nsignificantly reduces leakage with minimal performance overhead, offering an\nimmediately deployable defense for developers."
                },
                "authors": [
                    {
                        "name": "Antoine Geimer"
                    },
                    {
                        "name": "Clementine Maurice"
                    }
                ],
                "author_detail": {
                    "name": "Clementine Maurice"
                },
                "author": "Clementine Maurice",
                "arxiv_comment": "11 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.06112v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.06112v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.00406v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.00406v2",
                "updated": "2025-07-08T15:49:01Z",
                "updated_parsed": [
                    2025,
                    7,
                    8,
                    15,
                    49,
                    1,
                    1,
                    189,
                    0
                ],
                "published": "2025-02-01T11:45:44Z",
                "published_parsed": [
                    2025,
                    2,
                    1,
                    11,
                    45,
                    44,
                    5,
                    32,
                    0
                ],
                "title": "Agents Are All You Need for LLM Unlearning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Agents Are All You Need for LLM Unlearning"
                },
                "summary": "Information removal or suppression in large language models (LLMs) is a\ndesired functionality, useful in AI regulation, legal compliance, safety, and\nprivacy. LLM unlearning methods aim to remove information on demand from LLMs.\nCurrent LLM unlearning methods struggle to balance the unlearning efficacy and\nutility due to the competing nature of these objectives. Keeping the unlearning\nprocess computationally feasible without assuming access to the model weights\nis an overlooked area. In this work we show that \\textit{agents might be all we\nneed for effective and practical inference-time LLM unlearning}. We present the\nfirst agentic LLM unlearning (\\texttt{ALU}) method, a multi-agent,\nretrain-free, model-agnostic approach to LLM unlearning that achieves effective\nunlearning while preserving the utility. Our \\texttt{ALU} framework unlearns by\ninvolving multiple LLM agents, each designed for a specific step in the\nunlearning process, without the need to update model weights for any of the\nagents in the framework. Users can easily request any set of unlearning\ninstances in any sequence, and \\texttt{ALU} seamlessly adapts in real time.\nThis is facilitated without requiring any changes in the underlying LLM model.\nThrough extensive experiments on established benchmarks (TOFU, WMDP, WPU) and\njailbreaking techniques (many shot, target masking, other languages), we\ndemonstrate that \\texttt{ALU} consistently stands out as the most robust\ninference-time LLM unlearning framework among current state-of-the-art methods\nwhile incurring time cost that remains effectively constant regardless of the\nnumber of unlearning targets. We further highlight \\texttt{ALU}'s superior\nperformance compared to existing methods when evaluated at scale. Specifically,\n\\texttt{ALU} is assessed on up to 1000 unlearning targets, exceeding the\nevaluation scope of all previously proposed LLM unlearning methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Information removal or suppression in large language models (LLMs) is a\ndesired functionality, useful in AI regulation, legal compliance, safety, and\nprivacy. LLM unlearning methods aim to remove information on demand from LLMs.\nCurrent LLM unlearning methods struggle to balance the unlearning efficacy and\nutility due to the competing nature of these objectives. Keeping the unlearning\nprocess computationally feasible without assuming access to the model weights\nis an overlooked area. In this work we show that \\textit{agents might be all we\nneed for effective and practical inference-time LLM unlearning}. We present the\nfirst agentic LLM unlearning (\\texttt{ALU}) method, a multi-agent,\nretrain-free, model-agnostic approach to LLM unlearning that achieves effective\nunlearning while preserving the utility. Our \\texttt{ALU} framework unlearns by\ninvolving multiple LLM agents, each designed for a specific step in the\nunlearning process, without the need to update model weights for any of the\nagents in the framework. Users can easily request any set of unlearning\ninstances in any sequence, and \\texttt{ALU} seamlessly adapts in real time.\nThis is facilitated without requiring any changes in the underlying LLM model.\nThrough extensive experiments on established benchmarks (TOFU, WMDP, WPU) and\njailbreaking techniques (many shot, target masking, other languages), we\ndemonstrate that \\texttt{ALU} consistently stands out as the most robust\ninference-time LLM unlearning framework among current state-of-the-art methods\nwhile incurring time cost that remains effectively constant regardless of the\nnumber of unlearning targets. We further highlight \\texttt{ALU}'s superior\nperformance compared to existing methods when evaluated at scale. Specifically,\n\\texttt{ALU} is assessed on up to 1000 unlearning targets, exceeding the\nevaluation scope of all previously proposed LLM unlearning methods."
                },
                "authors": [
                    {
                        "name": "Debdeep Sanyal"
                    },
                    {
                        "name": "Murari Mandal"
                    }
                ],
                "author_detail": {
                    "name": "Murari Mandal"
                },
                "author": "Murari Mandal",
                "arxiv_comment": "Accepted to COLM 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.00406v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.00406v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.06107v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.06107v1",
                "updated": "2025-07-08T15:47:39Z",
                "updated_parsed": [
                    2025,
                    7,
                    8,
                    15,
                    47,
                    39,
                    1,
                    189,
                    0
                ],
                "published": "2025-07-08T15:47:39Z",
                "published_parsed": [
                    2025,
                    7,
                    8,
                    15,
                    47,
                    39,
                    1,
                    189,
                    0
                ],
                "title": "A Unified Ontology for Scalable Knowledge Graph-Driven Operational Data\n  Analytics in High-Performance Computing Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Unified Ontology for Scalable Knowledge Graph-Driven Operational Data\n  Analytics in High-Performance Computing Systems"
                },
                "summary": "Modern high-performance computing (HPC) systems generate massive volumes of\nheterogeneous telemetry data from millions of sensors monitoring compute,\nmemory, power, cooling, and storage subsystems. As HPC infrastructures scale to\nsupport increasingly complex workloads-including generative AI-the need for\nefficient, reliable, and interoperable telemetry analysis becomes critical.\nOperational Data Analytics (ODA) has emerged to address these demands; however,\nthe reliance on schema-less storage solutions limits data accessibility and\nsemantic integration. Ontologies and knowledge graphs (KG) provide an effective\nway to enable efficient and expressive data querying by capturing domain\nsemantics, but they face challenges such as significant storage overhead and\nthe limited applicability of existing ontologies, which are often tailored to\nspecific HPC systems only. In this paper, we present the first unified ontology\nfor ODA in HPC systems, designed to enable semantic interoperability across\nheterogeneous data centers. Our ontology models telemetry data from the two\nlargest publicly available ODA datasets-M100 (Cineca, Italy) and F-DATA\n(Fugaku, Japan)-within a single data model. The ontology is validated through\n36 competency questions reflecting real-world stakeholder requirements, and we\nintroduce modeling optimizations that reduce knowledge graph (KG) storage\noverhead by up to 38.84% compared to a previous approach, with an additional\n26.82% reduction depending on the desired deployment configuration. This work\npaves the way for scalable ODA KGs and supports not only analysis within\nindividual systems, but also cross-system analysis across heterogeneous HPC\nsystems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Modern high-performance computing (HPC) systems generate massive volumes of\nheterogeneous telemetry data from millions of sensors monitoring compute,\nmemory, power, cooling, and storage subsystems. As HPC infrastructures scale to\nsupport increasingly complex workloads-including generative AI-the need for\nefficient, reliable, and interoperable telemetry analysis becomes critical.\nOperational Data Analytics (ODA) has emerged to address these demands; however,\nthe reliance on schema-less storage solutions limits data accessibility and\nsemantic integration. Ontologies and knowledge graphs (KG) provide an effective\nway to enable efficient and expressive data querying by capturing domain\nsemantics, but they face challenges such as significant storage overhead and\nthe limited applicability of existing ontologies, which are often tailored to\nspecific HPC systems only. In this paper, we present the first unified ontology\nfor ODA in HPC systems, designed to enable semantic interoperability across\nheterogeneous data centers. Our ontology models telemetry data from the two\nlargest publicly available ODA datasets-M100 (Cineca, Italy) and F-DATA\n(Fugaku, Japan)-within a single data model. The ontology is validated through\n36 competency questions reflecting real-world stakeholder requirements, and we\nintroduce modeling optimizations that reduce knowledge graph (KG) storage\noverhead by up to 38.84% compared to a previous approach, with an additional\n26.82% reduction depending on the desired deployment configuration. This work\npaves the way for scalable ODA KGs and supports not only analysis within\nindividual systems, but also cross-system analysis across heterogeneous HPC\nsystems."
                },
                "authors": [
                    {
                        "name": "Junaid Ahmed Khan"
                    },
                    {
                        "name": "Andrea Bartolini"
                    }
                ],
                "author_detail": {
                    "name": "Andrea Bartolini"
                },
                "author": "Andrea Bartolini",
                "arxiv_comment": "12 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.06107v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.06107v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.02986v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.02986v2",
                "updated": "2025-07-08T15:44:49Z",
                "updated_parsed": [
                    2025,
                    7,
                    8,
                    15,
                    44,
                    49,
                    1,
                    189,
                    0
                ],
                "published": "2025-07-01T10:01:21Z",
                "published_parsed": [
                    2025,
                    7,
                    1,
                    10,
                    1,
                    21,
                    1,
                    182,
                    0
                ],
                "title": "GAF-Guard: An Agentic Framework for Risk Management and Governance in\n  Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GAF-Guard: An Agentic Framework for Risk Management and Governance in\n  Large Language Models"
                },
                "summary": "As Large Language Models (LLMs) continue to be increasingly applied across\nvarious domains, their widespread adoption necessitates rigorous monitoring to\nprevent unintended negative consequences and ensure robustness. Furthermore,\nLLMs must be designed to align with human values, like preventing harmful\ncontent and ensuring responsible usage. The current automated systems and\nsolutions for monitoring LLMs in production are primarily centered on\nLLM-specific concerns like hallucination etc, with little consideration given\nto the requirements of specific use-cases and user preferences. This paper\nintroduces GAF-Guard, a novel agentic framework for LLM governance that places\nthe user, the use-case, and the model itself at the center. The framework is\ndesigned to detect and monitor risks associated with the deployment of LLM\nbased applications. The approach models autonomous agents that identify risks,\nactivate risk detection tools, within specific use-cases and facilitate\ncontinuous monitoring and reporting to enhance AI safety, and user\nexpectations. The code is available at\nhttps://github.com/IBM/risk-atlas-nexus-demos/tree/main/gaf-guard.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As Large Language Models (LLMs) continue to be increasingly applied across\nvarious domains, their widespread adoption necessitates rigorous monitoring to\nprevent unintended negative consequences and ensure robustness. Furthermore,\nLLMs must be designed to align with human values, like preventing harmful\ncontent and ensuring responsible usage. The current automated systems and\nsolutions for monitoring LLMs in production are primarily centered on\nLLM-specific concerns like hallucination etc, with little consideration given\nto the requirements of specific use-cases and user preferences. This paper\nintroduces GAF-Guard, a novel agentic framework for LLM governance that places\nthe user, the use-case, and the model itself at the center. The framework is\ndesigned to detect and monitor risks associated with the deployment of LLM\nbased applications. The approach models autonomous agents that identify risks,\nactivate risk detection tools, within specific use-cases and facilitate\ncontinuous monitoring and reporting to enhance AI safety, and user\nexpectations. The code is available at\nhttps://github.com/IBM/risk-atlas-nexus-demos/tree/main/gaf-guard."
                },
                "authors": [
                    {
                        "name": "Seshu Tirupathi"
                    },
                    {
                        "name": "Dhaval Salwala"
                    },
                    {
                        "name": "Elizabeth Daly"
                    },
                    {
                        "name": "Inge Vejsbjerg"
                    }
                ],
                "author_detail": {
                    "name": "Inge Vejsbjerg"
                },
                "author": "Inge Vejsbjerg",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.02986v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.02986v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.06092v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.06092v1",
                "updated": "2025-07-08T15:34:45Z",
                "updated_parsed": [
                    2025,
                    7,
                    8,
                    15,
                    34,
                    45,
                    1,
                    189,
                    0
                ],
                "published": "2025-07-08T15:34:45Z",
                "published_parsed": [
                    2025,
                    7,
                    8,
                    15,
                    34,
                    45,
                    1,
                    189,
                    0
                ],
                "title": "Taming Data Challenges in ML-based Security Tasks: Lessons from\n  Integrating Generative AI",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Taming Data Challenges in ML-based Security Tasks: Lessons from\n  Integrating Generative AI"
                },
                "summary": "Machine learning-based supervised classifiers are widely used for security\ntasks, and their improvement has been largely focused on algorithmic\nadvancements. We argue that data challenges that negatively impact the\nperformance of these classifiers have received limited attention. We address\nthe following research question: Can developments in Generative AI (GenAI)\naddress these data challenges and improve classifier performance? We propose\naugmenting training datasets with synthetic data generated using GenAI\ntechniques to improve classifier generalization. We evaluate this approach\nacross 7 diverse security tasks using 6 state-of-the-art GenAI methods and\nintroduce a novel GenAI scheme called Nimai that enables highly controlled data\nsynthesis. We find that GenAI techniques can significantly improve the\nperformance of security classifiers, achieving improvements of up to 32.6% even\nin severely data-constrained settings (only ~180 training samples).\nFurthermore, we demonstrate that GenAI can facilitate rapid adaptation to\nconcept drift post-deployment, requiring minimal labeling in the adjustment\nprocess. Despite successes, our study finds that some GenAI schemes struggle to\ninitialize (train and produce data) on certain security tasks. We also identify\ncharacteristics of specific tasks, such as noisy labels, overlapping class\ndistributions, and sparse feature vectors, which hinder performance boost using\nGenAI. We believe that our study will drive the development of future GenAI\ntools designed for security tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Machine learning-based supervised classifiers are widely used for security\ntasks, and their improvement has been largely focused on algorithmic\nadvancements. We argue that data challenges that negatively impact the\nperformance of these classifiers have received limited attention. We address\nthe following research question: Can developments in Generative AI (GenAI)\naddress these data challenges and improve classifier performance? We propose\naugmenting training datasets with synthetic data generated using GenAI\ntechniques to improve classifier generalization. We evaluate this approach\nacross 7 diverse security tasks using 6 state-of-the-art GenAI methods and\nintroduce a novel GenAI scheme called Nimai that enables highly controlled data\nsynthesis. We find that GenAI techniques can significantly improve the\nperformance of security classifiers, achieving improvements of up to 32.6% even\nin severely data-constrained settings (only ~180 training samples).\nFurthermore, we demonstrate that GenAI can facilitate rapid adaptation to\nconcept drift post-deployment, requiring minimal labeling in the adjustment\nprocess. Despite successes, our study finds that some GenAI schemes struggle to\ninitialize (train and produce data) on certain security tasks. We also identify\ncharacteristics of specific tasks, such as noisy labels, overlapping class\ndistributions, and sparse feature vectors, which hinder performance boost using\nGenAI. We believe that our study will drive the development of future GenAI\ntools designed for security tasks."
                },
                "authors": [
                    {
                        "name": "Shravya Kanchi"
                    },
                    {
                        "name": "Neal Mangaokar"
                    },
                    {
                        "name": "Aravind Cheruvu"
                    },
                    {
                        "name": "Sifat Muhammad Abdullah"
                    },
                    {
                        "name": "Shirin Nilizadeh"
                    },
                    {
                        "name": "Atul Prakash"
                    },
                    {
                        "name": "Bimal Viswanath"
                    }
                ],
                "author_detail": {
                    "name": "Bimal Viswanath"
                },
                "author": "Bimal Viswanath",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.06092v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.06092v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.22827v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.22827v2",
                "updated": "2025-07-08T15:24:43Z",
                "updated_parsed": [
                    2025,
                    7,
                    8,
                    15,
                    24,
                    43,
                    1,
                    189,
                    0
                ],
                "published": "2025-06-28T09:39:37Z",
                "published_parsed": [
                    2025,
                    6,
                    28,
                    9,
                    39,
                    37,
                    5,
                    179,
                    0
                ],
                "title": "Hierarchical Vision-Language Planning for Multi-Step Humanoid\n  Manipulation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Hierarchical Vision-Language Planning for Multi-Step Humanoid\n  Manipulation"
                },
                "summary": "Enabling humanoid robots to reliably execute complex multi-step manipulation\ntasks is crucial for their effective deployment in industrial and household\nenvironments. This paper presents a hierarchical planning and control framework\ndesigned to achieve reliable multi-step humanoid manipulation. The proposed\nsystem comprises three layers: (1) a low-level RL-based controller responsible\nfor tracking whole-body motion targets; (2) a mid-level set of skill policies\ntrained via imitation learning that produce motion targets for different steps\nof a task; and (3) a high-level vision-language planning module that determines\nwhich skills should be executed and also monitors their completion in real-time\nusing pretrained vision-language models (VLMs). Experimental validation is\nperformed on a Unitree G1 humanoid robot executing a non-prehensile\npick-and-place task. Over 40 real-world trials, the hierarchical system\nachieved a 73% success rate in completing the full manipulation sequence. These\nexperiments confirm the feasibility of the proposed hierarchical system,\nhighlighting the benefits of VLM-based skill planning and monitoring for\nmulti-step manipulation scenarios. See https://vlp-humanoid.github.io/ for\nvideo demonstrations of the policy rollout.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Enabling humanoid robots to reliably execute complex multi-step manipulation\ntasks is crucial for their effective deployment in industrial and household\nenvironments. This paper presents a hierarchical planning and control framework\ndesigned to achieve reliable multi-step humanoid manipulation. The proposed\nsystem comprises three layers: (1) a low-level RL-based controller responsible\nfor tracking whole-body motion targets; (2) a mid-level set of skill policies\ntrained via imitation learning that produce motion targets for different steps\nof a task; and (3) a high-level vision-language planning module that determines\nwhich skills should be executed and also monitors their completion in real-time\nusing pretrained vision-language models (VLMs). Experimental validation is\nperformed on a Unitree G1 humanoid robot executing a non-prehensile\npick-and-place task. Over 40 real-world trials, the hierarchical system\nachieved a 73% success rate in completing the full manipulation sequence. These\nexperiments confirm the feasibility of the proposed hierarchical system,\nhighlighting the benefits of VLM-based skill planning and monitoring for\nmulti-step manipulation scenarios. See https://vlp-humanoid.github.io/ for\nvideo demonstrations of the policy rollout."
                },
                "authors": [
                    {
                        "name": "AndrÃ© Schakkal"
                    },
                    {
                        "name": "Ben Zandonati"
                    },
                    {
                        "name": "Zhutian Yang"
                    },
                    {
                        "name": "Navid Azizan"
                    }
                ],
                "author_detail": {
                    "name": "Navid Azizan"
                },
                "author": "Navid Azizan",
                "arxiv_comment": "Accepted at the RSS 2025 Workshop on Robot Planning in the Era of\n  Foundation Models",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.22827v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.22827v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.08199v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.08199v2",
                "updated": "2025-07-08T15:19:50Z",
                "updated_parsed": [
                    2025,
                    7,
                    8,
                    15,
                    19,
                    50,
                    1,
                    189,
                    0
                ],
                "published": "2025-03-11T09:08:04Z",
                "published_parsed": [
                    2025,
                    3,
                    11,
                    9,
                    8,
                    4,
                    1,
                    70,
                    0
                ],
                "title": "A Cascading Cooperative Multi-agent Framework for On-ramp Merging\n  Control Integrating Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Cascading Cooperative Multi-agent Framework for On-ramp Merging\n  Control Integrating Large Language Models"
                },
                "summary": "Traditional Reinforcement Learning (RL) suffers from replicating human-like\nbehaviors, generalizing effectively in multi-agent scenarios, and overcoming\ninherent interpretability issues.These tasks are compounded when deep\nenvironment understanding, agent coordination and dynamic optimization are\nrequired. While Large Language Model (LLM) enhanced methods have shown promise\nin generalization and interoperability, they often neglect necessary\nmulti-agent coordination. Therefore, we introduce the Cascading Cooperative\nMulti-agent (CCMA) framework, integrating RL for individual interactions, a\nfine-tuned LLM for regional cooperation, a reward function for global\noptimization, and the Retrieval-augmented Generation mechanism to dynamically\noptimize decision-making across complex driving scenarios. Our experiments\ndemonstrate that the CCMA outperforms existing RL methods, demonstrating\nsignificant improvements in both micro and macro-level performance in complex\ndriving environments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Traditional Reinforcement Learning (RL) suffers from replicating human-like\nbehaviors, generalizing effectively in multi-agent scenarios, and overcoming\ninherent interpretability issues.These tasks are compounded when deep\nenvironment understanding, agent coordination and dynamic optimization are\nrequired. While Large Language Model (LLM) enhanced methods have shown promise\nin generalization and interoperability, they often neglect necessary\nmulti-agent coordination. Therefore, we introduce the Cascading Cooperative\nMulti-agent (CCMA) framework, integrating RL for individual interactions, a\nfine-tuned LLM for regional cooperation, a reward function for global\noptimization, and the Retrieval-augmented Generation mechanism to dynamically\noptimize decision-making across complex driving scenarios. Our experiments\ndemonstrate that the CCMA outperforms existing RL methods, demonstrating\nsignificant improvements in both micro and macro-level performance in complex\ndriving environments."
                },
                "authors": [
                    {
                        "name": "Miao Zhang"
                    },
                    {
                        "name": "Zhenlong Fang"
                    },
                    {
                        "name": "Tianyi Wang"
                    },
                    {
                        "name": "Qian Zhang"
                    },
                    {
                        "name": "Shuai Lu"
                    },
                    {
                        "name": "Junfeng Jiao"
                    },
                    {
                        "name": "Tianyu Shi"
                    }
                ],
                "author_detail": {
                    "name": "Tianyu Shi"
                },
                "author": "Tianyu Shi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.08199v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.08199v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.06079v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.06079v1",
                "updated": "2025-07-08T15:19:14Z",
                "updated_parsed": [
                    2025,
                    7,
                    8,
                    15,
                    19,
                    14,
                    1,
                    189,
                    0
                ],
                "published": "2025-07-08T15:19:14Z",
                "published_parsed": [
                    2025,
                    7,
                    8,
                    15,
                    19,
                    14,
                    1,
                    189,
                    0
                ],
                "title": "QS4D: Quantization-aware training for efficient hardware deployment of\n  structured state-space sequential models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "QS4D: Quantization-aware training for efficient hardware deployment of\n  structured state-space sequential models"
                },
                "summary": "Structured State Space models (SSM) have recently emerged as a new class of\ndeep learning models, particularly well-suited for processing long sequences.\nTheir constant memory footprint, in contrast to the linearly scaling memory\ndemands of Transformers, makes them attractive candidates for deployment on\nresource-constrained edge-computing devices. While recent works have explored\nthe effect of quantization-aware training (QAT) on SSMs, they typically do not\naddress its implications for specialized edge hardware, for example, analog\nin-memory computing (AIMC) chips. In this work, we demonstrate that QAT can\nsignificantly reduce the complexity of SSMs by up to two orders of magnitude\nacross various performance metrics. We analyze the relation between model size\nand numerical precision, and show that QAT enhances robustness to analog noise\nand enables structural pruning. Finally, we integrate these techniques to\ndeploy SSMs on a memristive analog in-memory computing substrate and highlight\nthe resulting benefits in terms of computational efficiency.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Structured State Space models (SSM) have recently emerged as a new class of\ndeep learning models, particularly well-suited for processing long sequences.\nTheir constant memory footprint, in contrast to the linearly scaling memory\ndemands of Transformers, makes them attractive candidates for deployment on\nresource-constrained edge-computing devices. While recent works have explored\nthe effect of quantization-aware training (QAT) on SSMs, they typically do not\naddress its implications for specialized edge hardware, for example, analog\nin-memory computing (AIMC) chips. In this work, we demonstrate that QAT can\nsignificantly reduce the complexity of SSMs by up to two orders of magnitude\nacross various performance metrics. We analyze the relation between model size\nand numerical precision, and show that QAT enhances robustness to analog noise\nand enables structural pruning. Finally, we integrate these techniques to\ndeploy SSMs on a memristive analog in-memory computing substrate and highlight\nthe resulting benefits in terms of computational efficiency."
                },
                "authors": [
                    {
                        "name": "Sebastian Siegel"
                    },
                    {
                        "name": "Ming-Jay Yang"
                    },
                    {
                        "name": "Younes Bouhadjar"
                    },
                    {
                        "name": "Maxime Fabre"
                    },
                    {
                        "name": "Emre Neftci"
                    },
                    {
                        "name": "John Paul Strachan"
                    }
                ],
                "author_detail": {
                    "name": "John Paul Strachan"
                },
                "author": "John Paul Strachan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.06079v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.06079v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.06077v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.06077v1",
                "updated": "2025-07-08T15:16:50Z",
                "updated_parsed": [
                    2025,
                    7,
                    8,
                    15,
                    16,
                    50,
                    1,
                    189,
                    0
                ],
                "published": "2025-07-08T15:16:50Z",
                "published_parsed": [
                    2025,
                    7,
                    8,
                    15,
                    16,
                    50,
                    1,
                    189,
                    0
                ],
                "title": "AI-Based Demand Forecasting and Load Balancing for Optimising Energy use\n  in Healthcare Systems: A real case study",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AI-Based Demand Forecasting and Load Balancing for Optimising Energy use\n  in Healthcare Systems: A real case study"
                },
                "summary": "This paper tackles the urgent need for efficient energy management in\nhealthcare facilities, where fluctuating demands challenge operational\nefficiency and sustainability. Traditional methods often prove inadequate,\ncausing inefficiencies and higher costs. To address this, the study presents an\nAI-based framework combining Long Short-Term Memory (LSTM), genetic algorithm\n(GA), and SHAP (Shapley Additive Explanations), specifically designed for\nhealthcare energy management. Although LSTM is widely used for time-series\nforecasting, its application in healthcare energy prediction remains\nunderexplored. The results reveal that LSTM significantly outperforms ARIMA and\nProphet models in forecasting complex, non-linear demand patterns. LSTM\nachieves a Mean Absolute Error (MAE) of 21.69 and Root Mean Square Error (RMSE)\nof 29.96, far better than Prophet (MAE: 59.78, RMSE: 81.22) and ARIMA (MAE:\n87.73, RMSE: 125.22), demonstrating superior performance. The genetic algorithm\nis applied to optimize model parameters and improve load balancing strategies,\nenabling adaptive responses to real-time energy fluctuations. SHAP analysis\nfurther enhances model transparency by explaining the influence of different\nfeatures on predictions, fostering trust in decision-making processes. This\nintegrated LSTM-GA-SHAP approach offers a robust solution for improving\nforecasting accuracy, boosting energy efficiency, and advancing sustainability\nin healthcare facilities. Future research may explore real-time deployment and\nhybridization with reinforcement learning for continuous optimization. Overall,\nthe study establishes a solid foundation for using AI in healthcare energy\nmanagement, highlighting its scalability, efficiency, and resilience potential.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper tackles the urgent need for efficient energy management in\nhealthcare facilities, where fluctuating demands challenge operational\nefficiency and sustainability. Traditional methods often prove inadequate,\ncausing inefficiencies and higher costs. To address this, the study presents an\nAI-based framework combining Long Short-Term Memory (LSTM), genetic algorithm\n(GA), and SHAP (Shapley Additive Explanations), specifically designed for\nhealthcare energy management. Although LSTM is widely used for time-series\nforecasting, its application in healthcare energy prediction remains\nunderexplored. The results reveal that LSTM significantly outperforms ARIMA and\nProphet models in forecasting complex, non-linear demand patterns. LSTM\nachieves a Mean Absolute Error (MAE) of 21.69 and Root Mean Square Error (RMSE)\nof 29.96, far better than Prophet (MAE: 59.78, RMSE: 81.22) and ARIMA (MAE:\n87.73, RMSE: 125.22), demonstrating superior performance. The genetic algorithm\nis applied to optimize model parameters and improve load balancing strategies,\nenabling adaptive responses to real-time energy fluctuations. SHAP analysis\nfurther enhances model transparency by explaining the influence of different\nfeatures on predictions, fostering trust in decision-making processes. This\nintegrated LSTM-GA-SHAP approach offers a robust solution for improving\nforecasting accuracy, boosting energy efficiency, and advancing sustainability\nin healthcare facilities. Future research may explore real-time deployment and\nhybridization with reinforcement learning for continuous optimization. Overall,\nthe study establishes a solid foundation for using AI in healthcare energy\nmanagement, highlighting its scalability, efficiency, and resilience potential."
                },
                "authors": [
                    {
                        "name": "Iman Rahimi"
                    },
                    {
                        "name": "Isha Patel"
                    }
                ],
                "author_detail": {
                    "name": "Isha Patel"
                },
                "author": "Isha Patel",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.06077v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.06077v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.18162v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.18162v3",
                "updated": "2025-07-08T15:16:05Z",
                "updated_parsed": [
                    2025,
                    7,
                    8,
                    15,
                    16,
                    5,
                    1,
                    189,
                    0
                ],
                "published": "2024-09-26T17:19:25Z",
                "published_parsed": [
                    2024,
                    9,
                    26,
                    17,
                    19,
                    25,
                    3,
                    270,
                    0
                ],
                "title": "The Nexus of AR/VR, AI, UI/UX, and Robotics Technologies in Enhancing\n  Learning and Social Interaction for Children with Autism Spectrum Disorders:\n  A Systematic Review",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Nexus of AR/VR, AI, UI/UX, and Robotics Technologies in Enhancing\n  Learning and Social Interaction for Children with Autism Spectrum Disorders:\n  A Systematic Review"
                },
                "summary": "The emergence of large language models (LLMs), augmented reality (AR), and\nuser interface/user experience (UI/UX) design in therapies for children,\nespecially with disorders like autism spectrum disorder (ASD), is studied in\ndetail in this review study. 150 publications were collected by a thorough\nliterature search throughout PubMed, ACM, IEEE Xplore, Elsevier, and Google\nScholar; 60 of them were chosen based on their methodological rigor and\nrelevance to the focus area. Three of the primary areas are studied and covered\nin this review: how AR can improve social and learning results, how LLMs can\nsupport communication, and how UI/UX design affects how effective these\ntechnologies can be. Results show that while LLMs can provide individualized\nlearning and communication support, AR has shown promise in enhancing social\nskills, motivation, and attention. For children with ASD, accessible and\nengaging interventions rely heavily on effective UI/UX design, but there is\nstill a significant lack of robotics-based education and therapeutic programs\nspecifically tailored for autistic children. To optimize the benefits of these\ntechnologies in ASD therapies and immersive education, the study emphasizes the\nneed for additional research to address difficulties related to customization,\naccessibility, and integration.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The emergence of large language models (LLMs), augmented reality (AR), and\nuser interface/user experience (UI/UX) design in therapies for children,\nespecially with disorders like autism spectrum disorder (ASD), is studied in\ndetail in this review study. 150 publications were collected by a thorough\nliterature search throughout PubMed, ACM, IEEE Xplore, Elsevier, and Google\nScholar; 60 of them were chosen based on their methodological rigor and\nrelevance to the focus area. Three of the primary areas are studied and covered\nin this review: how AR can improve social and learning results, how LLMs can\nsupport communication, and how UI/UX design affects how effective these\ntechnologies can be. Results show that while LLMs can provide individualized\nlearning and communication support, AR has shown promise in enhancing social\nskills, motivation, and attention. For children with ASD, accessible and\nengaging interventions rely heavily on effective UI/UX design, but there is\nstill a significant lack of robotics-based education and therapeutic programs\nspecifically tailored for autistic children. To optimize the benefits of these\ntechnologies in ASD therapies and immersive education, the study emphasizes the\nneed for additional research to address difficulties related to customization,\naccessibility, and integration."
                },
                "authors": [
                    {
                        "name": "Biplov Paneru"
                    }
                ],
                "author_detail": {
                    "name": "Biplov Paneru"
                },
                "author": "Biplov Paneru",
                "arxiv_comment": "none",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.18162v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.18162v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.23519v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.23519v2",
                "updated": "2025-07-08T15:10:51Z",
                "updated_parsed": [
                    2025,
                    7,
                    8,
                    15,
                    10,
                    51,
                    1,
                    189,
                    0
                ],
                "published": "2025-03-30T17:02:26Z",
                "published_parsed": [
                    2025,
                    3,
                    30,
                    17,
                    2,
                    26,
                    6,
                    89,
                    0
                ],
                "title": "BoundMatch: Boundary detection applied to semi-supervised segmentation\n  for urban-driving scenes",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "BoundMatch: Boundary detection applied to semi-supervised segmentation\n  for urban-driving scenes"
                },
                "summary": "Semi-supervised semantic segmentation (SS-SS) aims to mitigate the heavy\nannotation burden of dense pixel labeling by leveraging abundant unlabeled\nimages alongside a small labeled set. While current consistency regularization\nmethods achieve strong results, they often overlook a critical challenge: the\nprecise delineation of object boundaries. In this paper, we propose BoundMatch,\na novel multi-task SS-SS framework that explicitly integrates semantic boundary\ndetection into a teacher-student consistency regularization pipeline. Our core\nmechanism, Boundary Consistency Regularized Multi-Task Learning (BCRM),\nenforces prediction agreement between teacher and student models on both\nsegmentation masks and detailed semantic boundaries. To further enhance\nperformance and sharpen boundaries, BoundMatch incorporates two lightweight\nfusion modules: Boundary-Semantic Fusion (BSF) injects learned boundary cues\ninto the segmentation decoder, while Spatial Gradient Fusion (SGF) refines\nboundary predictions using mask gradients, leading to higher-quality boundary\npseudo-labels. This framework is built upon SAMTH, a strong teacher-student\nbaseline featuring a Harmonious Batch Normalization (HBN) update strategy for\nimproved stability. Extensive experiments on diverse urban-driving scene\ndatasets including Cityscapes, BDD100K, and SYNTHIA show that BoundMatch\nachieves competitive performance against current state-of-the-art methods. Our\napproach achieves state-of-the-art results on the new benchmark with DINOv2\nfoundation model. We further validate our approach's generalizability on Pascal\nVOC and ADE20K datasets. Ablation studies highlight BoundMatch's ability to\nimprove boundary-specific evaluation metrics, its effectiveness in realistic\nlarge-scale unlabeled data scenarios, and applicability to lightweight\narchitectures for mobile deployment.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Semi-supervised semantic segmentation (SS-SS) aims to mitigate the heavy\nannotation burden of dense pixel labeling by leveraging abundant unlabeled\nimages alongside a small labeled set. While current consistency regularization\nmethods achieve strong results, they often overlook a critical challenge: the\nprecise delineation of object boundaries. In this paper, we propose BoundMatch,\na novel multi-task SS-SS framework that explicitly integrates semantic boundary\ndetection into a teacher-student consistency regularization pipeline. Our core\nmechanism, Boundary Consistency Regularized Multi-Task Learning (BCRM),\nenforces prediction agreement between teacher and student models on both\nsegmentation masks and detailed semantic boundaries. To further enhance\nperformance and sharpen boundaries, BoundMatch incorporates two lightweight\nfusion modules: Boundary-Semantic Fusion (BSF) injects learned boundary cues\ninto the segmentation decoder, while Spatial Gradient Fusion (SGF) refines\nboundary predictions using mask gradients, leading to higher-quality boundary\npseudo-labels. This framework is built upon SAMTH, a strong teacher-student\nbaseline featuring a Harmonious Batch Normalization (HBN) update strategy for\nimproved stability. Extensive experiments on diverse urban-driving scene\ndatasets including Cityscapes, BDD100K, and SYNTHIA show that BoundMatch\nachieves competitive performance against current state-of-the-art methods. Our\napproach achieves state-of-the-art results on the new benchmark with DINOv2\nfoundation model. We further validate our approach's generalizability on Pascal\nVOC and ADE20K datasets. Ablation studies highlight BoundMatch's ability to\nimprove boundary-specific evaluation metrics, its effectiveness in realistic\nlarge-scale unlabeled data scenarios, and applicability to lightweight\narchitectures for mobile deployment."
                },
                "authors": [
                    {
                        "name": "Haruya Ishikawa"
                    },
                    {
                        "name": "Yoshimitsu Aoki"
                    }
                ],
                "author_detail": {
                    "name": "Yoshimitsu Aoki"
                },
                "author": "Yoshimitsu Aoki",
                "arxiv_comment": "20 pages, 18 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.23519v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.23519v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.08324v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.08324v2",
                "updated": "2025-07-08T15:08:52Z",
                "updated_parsed": [
                    2025,
                    7,
                    8,
                    15,
                    8,
                    52,
                    1,
                    189,
                    0
                ],
                "published": "2024-11-13T04:20:20Z",
                "published_parsed": [
                    2024,
                    11,
                    13,
                    4,
                    20,
                    20,
                    2,
                    318,
                    0
                ],
                "title": "Are LLMs Prescient? A Continuous Evaluation using Daily News as the\n  Oracle",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Are LLMs Prescient? A Continuous Evaluation using Daily News as the\n  Oracle"
                },
                "summary": "Many existing evaluation benchmarks for Large Language Models (LLMs) quickly\nbecome outdated due to the emergence of new models and training data. These\nbenchmarks also fall short in assessing how LLM performance changes over time,\nas they consist of a static set of questions without a temporal dimension. To\naddress these limitations, we propose using future event prediction as a\ncontinuous evaluation method to assess LLMs' temporal generalization and\nforecasting abilities. Our benchmark, Daily Oracle, automatically generates\nquestion-answer (QA) pairs from daily news, challenging LLMs to predict\n\"future\" event outcomes. Our findings reveal that as pre-training data becomes\noutdated, LLM performance degrades over time. While Retrieval Augmented\nGeneration (RAG) has the potential to enhance prediction accuracy, the\nperformance degradation pattern persists, highlighting the need for continuous\nmodel updates. Code and data are available at\nhttps://agenticlearning.ai/daily-oracle.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Many existing evaluation benchmarks for Large Language Models (LLMs) quickly\nbecome outdated due to the emergence of new models and training data. These\nbenchmarks also fall short in assessing how LLM performance changes over time,\nas they consist of a static set of questions without a temporal dimension. To\naddress these limitations, we propose using future event prediction as a\ncontinuous evaluation method to assess LLMs' temporal generalization and\nforecasting abilities. Our benchmark, Daily Oracle, automatically generates\nquestion-answer (QA) pairs from daily news, challenging LLMs to predict\n\"future\" event outcomes. Our findings reveal that as pre-training data becomes\noutdated, LLM performance degrades over time. While Retrieval Augmented\nGeneration (RAG) has the potential to enhance prediction accuracy, the\nperformance degradation pattern persists, highlighting the need for continuous\nmodel updates. Code and data are available at\nhttps://agenticlearning.ai/daily-oracle."
                },
                "authors": [
                    {
                        "name": "Hui Dai"
                    },
                    {
                        "name": "Ryan Teehan"
                    },
                    {
                        "name": "Mengye Ren"
                    }
                ],
                "author_detail": {
                    "name": "Mengye Ren"
                },
                "author": "Mengye Ren",
                "arxiv_comment": "ICML 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.08324v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.08324v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.21432v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.21432v4",
                "updated": "2025-07-08T15:03:11Z",
                "updated_parsed": [
                    2025,
                    7,
                    8,
                    15,
                    3,
                    11,
                    1,
                    189,
                    0
                ],
                "published": "2025-05-27T17:04:21Z",
                "published_parsed": [
                    2025,
                    5,
                    27,
                    17,
                    4,
                    21,
                    1,
                    147,
                    0
                ],
                "title": "Hume: Introducing System-2 Thinking in Visual-Language-Action Model",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Hume: Introducing System-2 Thinking in Visual-Language-Action Model"
                },
                "summary": "Humans practice slow thinking before performing actual actions when handling\ncomplex tasks in the physical world. This thinking paradigm, recently, has\nachieved remarkable advancement in boosting Large Language Models (LLMs) to\nsolve complex tasks in digital domains. However, the potential of slow thinking\nremains largely unexplored for robotic foundation models interacting with the\nphysical world. In this work, we propose Hume: a dual-system\nVision-Language-Action (VLA) model with value-guided System-2 thinking and\ncascaded action denoising, exploring human-like thinking capabilities of\nVision-Language-Action models for dexterous robot control. System 2 of Hume\nimplements value-Guided thinking by extending a Vision-Language-Action Model\nbackbone with a novel value-query head to estimate the state-action value of\npredicted actions. The value-guided thinking is conducted by repeat sampling\nmultiple action candidates and selecting one according to state-action value.\nSystem 1 of Hume is a lightweight reactive visuomotor policy that takes System\n2 selected action and performs cascaded action denoising for dexterous robot\ncontrol. At deployment time, System 2 performs value-guided thinking at a low\nfrequency while System 1 asynchronously receives the System 2 selected action\ncandidate and predicts fluid actions in real time. We show that Hume\noutperforms the existing state-of-the-art Vision-Language-Action models across\nmultiple simulation benchmark and real-robot deployments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Humans practice slow thinking before performing actual actions when handling\ncomplex tasks in the physical world. This thinking paradigm, recently, has\nachieved remarkable advancement in boosting Large Language Models (LLMs) to\nsolve complex tasks in digital domains. However, the potential of slow thinking\nremains largely unexplored for robotic foundation models interacting with the\nphysical world. In this work, we propose Hume: a dual-system\nVision-Language-Action (VLA) model with value-guided System-2 thinking and\ncascaded action denoising, exploring human-like thinking capabilities of\nVision-Language-Action models for dexterous robot control. System 2 of Hume\nimplements value-Guided thinking by extending a Vision-Language-Action Model\nbackbone with a novel value-query head to estimate the state-action value of\npredicted actions. The value-guided thinking is conducted by repeat sampling\nmultiple action candidates and selecting one according to state-action value.\nSystem 1 of Hume is a lightweight reactive visuomotor policy that takes System\n2 selected action and performs cascaded action denoising for dexterous robot\ncontrol. At deployment time, System 2 performs value-guided thinking at a low\nfrequency while System 1 asynchronously receives the System 2 selected action\ncandidate and predicts fluid actions in real time. We show that Hume\noutperforms the existing state-of-the-art Vision-Language-Action models across\nmultiple simulation benchmark and real-robot deployments."
                },
                "authors": [
                    {
                        "name": "Haoming Song"
                    },
                    {
                        "name": "Delin Qu"
                    },
                    {
                        "name": "Yuanqi Yao"
                    },
                    {
                        "name": "Qizhi Chen"
                    },
                    {
                        "name": "Qi Lv"
                    },
                    {
                        "name": "Yiwen Tang"
                    },
                    {
                        "name": "Modi Shi"
                    },
                    {
                        "name": "Guanghui Ren"
                    },
                    {
                        "name": "Maoqing Yao"
                    },
                    {
                        "name": "Bin Zhao"
                    },
                    {
                        "name": "Dong Wang"
                    },
                    {
                        "name": "Xuelong Li"
                    }
                ],
                "author_detail": {
                    "name": "Xuelong Li"
                },
                "author": "Xuelong Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.21432v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.21432v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.12961v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.12961v2",
                "updated": "2025-07-08T15:02:59Z",
                "updated_parsed": [
                    2025,
                    7,
                    8,
                    15,
                    2,
                    59,
                    1,
                    189,
                    0
                ],
                "published": "2025-02-18T15:45:01Z",
                "published_parsed": [
                    2025,
                    2,
                    18,
                    15,
                    45,
                    1,
                    1,
                    49,
                    0
                ],
                "title": "Adaptive Tool Use in Large Language Models with Meta-Cognition Trigger",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Adaptive Tool Use in Large Language Models with Meta-Cognition Trigger"
                },
                "summary": "Large language models (LLMs) have shown remarkable emergent capabilities,\ntransforming the execution of functional tasks by leveraging external tools for\ncomplex problems that require specialized processing or up-to-date data. While\nexisting research expands LLMs access to diverse tools (e.g., program\ninterpreters, search engines, calculators), the necessity of using these tools\nis often overlooked, leading to indiscriminate tool invocation. This naive\napproach raises two key issues: increased latency due to unnecessary tool\ncalls, and potential errors resulting from faulty interactions with external\ntools. In this paper, we introduce meta-cognition as a proxy for LLMs\nself-assessment of their capabilities, reflecting the model's awareness of its\nown limitations. Based on this, we propose MeCo, an adaptive decision-making\nstrategy for external tool use. MeCo quantifies metacognitive scores by\ncapturing high-level cognitive signals in the representation space, guiding\nwhen to invoke tools. Notably, MeCo is fine-tuning-free and incurs minimal\ncost. Experiments across multiple backbone models and benchmarks show that MeCo\nreliably detects LLMs' internal cognitive signals and significantly improves\ntool-use decision-making.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have shown remarkable emergent capabilities,\ntransforming the execution of functional tasks by leveraging external tools for\ncomplex problems that require specialized processing or up-to-date data. While\nexisting research expands LLMs access to diverse tools (e.g., program\ninterpreters, search engines, calculators), the necessity of using these tools\nis often overlooked, leading to indiscriminate tool invocation. This naive\napproach raises two key issues: increased latency due to unnecessary tool\ncalls, and potential errors resulting from faulty interactions with external\ntools. In this paper, we introduce meta-cognition as a proxy for LLMs\nself-assessment of their capabilities, reflecting the model's awareness of its\nown limitations. Based on this, we propose MeCo, an adaptive decision-making\nstrategy for external tool use. MeCo quantifies metacognitive scores by\ncapturing high-level cognitive signals in the representation space, guiding\nwhen to invoke tools. Notably, MeCo is fine-tuning-free and incurs minimal\ncost. Experiments across multiple backbone models and benchmarks show that MeCo\nreliably detects LLMs' internal cognitive signals and significantly improves\ntool-use decision-making."
                },
                "authors": [
                    {
                        "name": "Wenjun Li"
                    },
                    {
                        "name": "Dexun Li"
                    },
                    {
                        "name": "Kuicai Dong"
                    },
                    {
                        "name": "Cong Zhang"
                    },
                    {
                        "name": "Hao Zhang"
                    },
                    {
                        "name": "Weiwen Liu"
                    },
                    {
                        "name": "Yasheng Wang"
                    },
                    {
                        "name": "Ruiming Tang"
                    },
                    {
                        "name": "Yong Liu"
                    }
                ],
                "author_detail": {
                    "name": "Yong Liu"
                },
                "author": "Yong Liu",
                "arxiv_comment": "25 pages, camera ready version for ACL-2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.12961v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.12961v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.06057v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.06057v2",
                "updated": "2025-07-09T07:06:36Z",
                "updated_parsed": [
                    2025,
                    7,
                    9,
                    7,
                    6,
                    36,
                    2,
                    190,
                    0
                ],
                "published": "2025-07-08T14:59:46Z",
                "published_parsed": [
                    2025,
                    7,
                    8,
                    14,
                    59,
                    46,
                    1,
                    189,
                    0
                ],
                "title": "FEVO: Financial Knowledge Expansion and Reasoning Evolution for Large\n  Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FEVO: Financial Knowledge Expansion and Reasoning Evolution for Large\n  Language Models"
                },
                "summary": "Advancements in reasoning for large language models (LLMs) have lead to\nsignificant performance improvements for LLMs in various fields such as\nmathematics and programming. However, research applying these advances to the\nfinancial domain, where considerable domain-specific knowledge is necessary to\ncomplete tasks, remains limited. To address this gap, we introduce FEVO\n(Financial Evolution), a multi-stage enhancement framework developed to enhance\nLLM performance in the financial domain. FEVO systemically enhances LLM\nperformance by using continued pre-training (CPT) to expand financial domain\nknowledge, supervised fine-tuning (SFT) to instill structured, elaborate\nreasoning patterns, and reinforcement learning (RL) to further integrate the\nexpanded financial domain knowledge with the learned structured reasoning. To\nensure effective and efficient training, we leverage frontier reasoning models\nand rule-based filtering to curate FEVO-Train, high-quality datasets\nspecifically designed for the different post-training phases. Using our\nframework, we train the FEVO series of models - C32B, S32B, R32B - from\nQwen2.5-32B and evaluate them on seven benchmarks to assess financial and\ngeneral capabilities, with results showing that FEVO-R32B achieves\nstate-of-the-art performance on five financial benchmarks against much larger\nmodels as well as specialist models. More significantly, FEVO-R32B demonstrates\nmarkedly better performance than FEVO-R32B-0 (trained from Qwen2.5-32B-Instruct\nusing only RL), thus validating the effectiveness of financial domain knowledge\nexpansion and structured, logical reasoning distillation",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Advancements in reasoning for large language models (LLMs) have lead to\nsignificant performance improvements for LLMs in various fields such as\nmathematics and programming. However, research applying these advances to the\nfinancial domain, where considerable domain-specific knowledge is necessary to\ncomplete tasks, remains limited. To address this gap, we introduce FEVO\n(Financial Evolution), a multi-stage enhancement framework developed to enhance\nLLM performance in the financial domain. FEVO systemically enhances LLM\nperformance by using continued pre-training (CPT) to expand financial domain\nknowledge, supervised fine-tuning (SFT) to instill structured, elaborate\nreasoning patterns, and reinforcement learning (RL) to further integrate the\nexpanded financial domain knowledge with the learned structured reasoning. To\nensure effective and efficient training, we leverage frontier reasoning models\nand rule-based filtering to curate FEVO-Train, high-quality datasets\nspecifically designed for the different post-training phases. Using our\nframework, we train the FEVO series of models - C32B, S32B, R32B - from\nQwen2.5-32B and evaluate them on seven benchmarks to assess financial and\ngeneral capabilities, with results showing that FEVO-R32B achieves\nstate-of-the-art performance on five financial benchmarks against much larger\nmodels as well as specialist models. More significantly, FEVO-R32B demonstrates\nmarkedly better performance than FEVO-R32B-0 (trained from Qwen2.5-32B-Instruct\nusing only RL), thus validating the effectiveness of financial domain knowledge\nexpansion and structured, logical reasoning distillation"
                },
                "authors": [
                    {
                        "name": "Bo Pang"
                    },
                    {
                        "name": "Yalu Ouyang"
                    },
                    {
                        "name": "Hangfei Xu"
                    },
                    {
                        "name": "Ziqi Jia"
                    },
                    {
                        "name": "Panpan Li"
                    },
                    {
                        "name": "Shengzhao Wen"
                    },
                    {
                        "name": "Lu Wang"
                    },
                    {
                        "name": "Shiyong Li"
                    },
                    {
                        "name": "Yanpeng Wang"
                    }
                ],
                "author_detail": {
                    "name": "Yanpeng Wang"
                },
                "author": "Yanpeng Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.06057v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.06057v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.06056v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.06056v1",
                "updated": "2025-07-08T14:58:28Z",
                "updated_parsed": [
                    2025,
                    7,
                    8,
                    14,
                    58,
                    28,
                    1,
                    189,
                    0
                ],
                "published": "2025-07-08T14:58:28Z",
                "published_parsed": [
                    2025,
                    7,
                    8,
                    14,
                    58,
                    28,
                    1,
                    189,
                    0
                ],
                "title": "Entropy-Memorization Law: Evaluating Memorization Difficulty of Data in\n  LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Entropy-Memorization Law: Evaluating Memorization Difficulty of Data in\n  LLMs"
                },
                "summary": "Large Language Models (LLMs) are known to memorize portions of their training\ndata, sometimes reproducing content verbatim when prompted appropriately. In\nthis work, we investigate a fundamental yet under-explored question in the\ndomain of memorization: How to characterize memorization difficulty of training\ndata in LLMs? Through empirical experiments on OLMo, a family of open models,\nwe present the Entropy-Memorization Law. It suggests that data entropy is\nlinearly correlated with memorization score. Moreover, in a case study of\nmemorizing highly randomized strings, or \"gibberish\", we observe that such\nsequences, despite their apparent randomness, exhibit unexpectedly low\nempirical entropy compared to the broader training corpus. Adopting the same\nstrategy to discover Entropy-Memorization Law, we derive a simple yet effective\napproach to distinguish training and testing data, enabling Dataset Inference\n(DI).",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) are known to memorize portions of their training\ndata, sometimes reproducing content verbatim when prompted appropriately. In\nthis work, we investigate a fundamental yet under-explored question in the\ndomain of memorization: How to characterize memorization difficulty of training\ndata in LLMs? Through empirical experiments on OLMo, a family of open models,\nwe present the Entropy-Memorization Law. It suggests that data entropy is\nlinearly correlated with memorization score. Moreover, in a case study of\nmemorizing highly randomized strings, or \"gibberish\", we observe that such\nsequences, despite their apparent randomness, exhibit unexpectedly low\nempirical entropy compared to the broader training corpus. Adopting the same\nstrategy to discover Entropy-Memorization Law, we derive a simple yet effective\napproach to distinguish training and testing data, enabling Dataset Inference\n(DI)."
                },
                "authors": [
                    {
                        "name": "Yizhan Huang"
                    },
                    {
                        "name": "Zhe Yang"
                    },
                    {
                        "name": "Meifang Chen"
                    },
                    {
                        "name": "Jianping Zhang"
                    },
                    {
                        "name": "Michael R. Lyu"
                    }
                ],
                "author_detail": {
                    "name": "Michael R. Lyu"
                },
                "author": "Michael R. Lyu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.06056v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.06056v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.19652v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.19652v2",
                "updated": "2025-07-08T14:47:33Z",
                "updated_parsed": [
                    2025,
                    7,
                    8,
                    14,
                    47,
                    33,
                    1,
                    189,
                    0
                ],
                "published": "2025-06-24T14:15:26Z",
                "published_parsed": [
                    2025,
                    6,
                    24,
                    14,
                    15,
                    26,
                    1,
                    175,
                    0
                ],
                "title": "Tailored Conversations beyond LLMs: A RL-Based Dialogue Manager",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Tailored Conversations beyond LLMs: A RL-Based Dialogue Manager"
                },
                "summary": "In this work, we propose a novel framework that integrates large language\nmodels (LLMs) with an RL-based dialogue manager for open-ended dialogue with a\nspecific goal. By leveraging hierarchical reinforcement learning to model the\nstructured phases of dialogue and employ meta-learning to enhance adaptability\nacross diverse user profiles, our approach enhances adaptability and\nefficiency, enabling the system to learn from limited data, transition fluidly\nbetween dialogue phases, and personalize responses to heterogeneous patient\nneeds. We apply our framework to Motivational Interviews, aiming to foster\nbehavior change, and demonstrate that the proposed dialogue manager outperforms\na state-of-the-art LLM baseline in terms of reward, showing a potential benefit\nof conditioning LLMs to create open-ended dialogue systems with specific goals.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this work, we propose a novel framework that integrates large language\nmodels (LLMs) with an RL-based dialogue manager for open-ended dialogue with a\nspecific goal. By leveraging hierarchical reinforcement learning to model the\nstructured phases of dialogue and employ meta-learning to enhance adaptability\nacross diverse user profiles, our approach enhances adaptability and\nefficiency, enabling the system to learn from limited data, transition fluidly\nbetween dialogue phases, and personalize responses to heterogeneous patient\nneeds. We apply our framework to Motivational Interviews, aiming to foster\nbehavior change, and demonstrate that the proposed dialogue manager outperforms\na state-of-the-art LLM baseline in terms of reward, showing a potential benefit\nof conditioning LLMs to create open-ended dialogue systems with specific goals."
                },
                "authors": [
                    {
                        "name": "Lucie Galland"
                    },
                    {
                        "name": "Catherine Pelachaud"
                    },
                    {
                        "name": "Florian Pecune"
                    }
                ],
                "author_detail": {
                    "name": "Florian Pecune"
                },
                "author": "Florian Pecune",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.19652v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.19652v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.06044v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.06044v1",
                "updated": "2025-07-08T14:45:47Z",
                "updated_parsed": [
                    2025,
                    7,
                    8,
                    14,
                    45,
                    47,
                    1,
                    189,
                    0
                ],
                "published": "2025-07-08T14:45:47Z",
                "published_parsed": [
                    2025,
                    7,
                    8,
                    14,
                    45,
                    47,
                    1,
                    189,
                    0
                ],
                "title": "Hierarchical Interaction Summarization and Contrastive Prompting for\n  Explainable Recommendations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Hierarchical Interaction Summarization and Contrastive Prompting for\n  Explainable Recommendations"
                },
                "summary": "Explainable recommendations, which use the information of user and item with\ninteraction to generate a explanation for why the user would interact with the\nitem, are crucial for improving user trust and decision transparency to the\nrecommender system. Existing methods primarily rely on encoding features of\nusers and items to embeddings, which often leads to information loss due to\ndimensionality reduction, sparse interactions, and so on. With the advancements\nof large language models (LLMs) in language comprehension, some methods use\nembeddings as LLM inputs for explanation generation. However, since embeddings\nlack inherent semantics, LLMs must adjust or extend their parameters to\ninterpret them, a process that inevitably incurs information loss. To address\nthis issue, we propose a novel approach combining profile generation via\nhierarchical interaction summarization (PGHIS), which leverages a pretrained\nLLM to hierarchically summarize user-item interactions, generating structured\ntextual profiles as explicit representations of user and item characteristics.\nAdditionally, we propose contrastive prompting for explanation generation\n(CPEG) which employs contrastive learning to guide another reasoning language\nmodels in producing high-quality ground truth recommendation explanations.\nFinally, we use the textual profiles of user and item as input and high-quality\nexplanation as output to fine-tune a LLM for generating explanations.\nExperimental results on multiple datasets demonstrate that our approach\noutperforms existing state-of-the-art methods, achieving a great improvement on\nmetrics about explainability (e.g., 5% on GPTScore) and text quality.\nFurthermore, our generated ground truth explanations achieve a significantly\nhigher win rate compared to user-written reviews and those produced by other\nmethods, demonstrating the effectiveness of CPEG in generating high-quality\nground truths.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Explainable recommendations, which use the information of user and item with\ninteraction to generate a explanation for why the user would interact with the\nitem, are crucial for improving user trust and decision transparency to the\nrecommender system. Existing methods primarily rely on encoding features of\nusers and items to embeddings, which often leads to information loss due to\ndimensionality reduction, sparse interactions, and so on. With the advancements\nof large language models (LLMs) in language comprehension, some methods use\nembeddings as LLM inputs for explanation generation. However, since embeddings\nlack inherent semantics, LLMs must adjust or extend their parameters to\ninterpret them, a process that inevitably incurs information loss. To address\nthis issue, we propose a novel approach combining profile generation via\nhierarchical interaction summarization (PGHIS), which leverages a pretrained\nLLM to hierarchically summarize user-item interactions, generating structured\ntextual profiles as explicit representations of user and item characteristics.\nAdditionally, we propose contrastive prompting for explanation generation\n(CPEG) which employs contrastive learning to guide another reasoning language\nmodels in producing high-quality ground truth recommendation explanations.\nFinally, we use the textual profiles of user and item as input and high-quality\nexplanation as output to fine-tune a LLM for generating explanations.\nExperimental results on multiple datasets demonstrate that our approach\noutperforms existing state-of-the-art methods, achieving a great improvement on\nmetrics about explainability (e.g., 5% on GPTScore) and text quality.\nFurthermore, our generated ground truth explanations achieve a significantly\nhigher win rate compared to user-written reviews and those produced by other\nmethods, demonstrating the effectiveness of CPEG in generating high-quality\nground truths."
                },
                "authors": [
                    {
                        "name": "Yibin Liu"
                    },
                    {
                        "name": "Ang Li"
                    },
                    {
                        "name": "Shijian Li"
                    }
                ],
                "author_detail": {
                    "name": "Shijian Li"
                },
                "author": "Shijian Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.06044v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.06044v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.06043v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.06043v1",
                "updated": "2025-07-08T14:45:21Z",
                "updated_parsed": [
                    2025,
                    7,
                    8,
                    14,
                    45,
                    21,
                    1,
                    189,
                    0
                ],
                "published": "2025-07-08T14:45:21Z",
                "published_parsed": [
                    2025,
                    7,
                    8,
                    14,
                    45,
                    21,
                    1,
                    189,
                    0
                ],
                "title": "CAVGAN: Unifying Jailbreak and Defense of LLMs via Generative\n  Adversarial Attacks on their Internal Representations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CAVGAN: Unifying Jailbreak and Defense of LLMs via Generative\n  Adversarial Attacks on their Internal Representations"
                },
                "summary": "Security alignment enables the Large Language Model (LLM) to gain the\nprotection against malicious queries, but various jailbreak attack methods\nreveal the vulnerability of this security mechanism. Previous studies have\nisolated LLM jailbreak attacks and defenses. We analyze the security protection\nmechanism of the LLM, and propose a framework that combines attack and defense.\nOur method is based on the linearly separable property of LLM intermediate\nlayer embedding, as well as the essence of jailbreak attack, which aims to\nembed harmful problems and transfer them to the safe area. We utilize\ngenerative adversarial network (GAN) to learn the security judgment boundary\ninside the LLM to achieve efficient jailbreak attack and defense. The\nexperimental results indicate that our method achieves an average jailbreak\nsuccess rate of 88.85\\% across three popular LLMs, while the defense success\nrate on the state-of-the-art jailbreak dataset reaches an average of 84.17\\%.\nThis not only validates the effectiveness of our approach but also sheds light\non the internal security mechanisms of LLMs, offering new insights for\nenhancing model security The code and data are available at\nhttps://github.com/NLPGM/CAVGAN.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Security alignment enables the Large Language Model (LLM) to gain the\nprotection against malicious queries, but various jailbreak attack methods\nreveal the vulnerability of this security mechanism. Previous studies have\nisolated LLM jailbreak attacks and defenses. We analyze the security protection\nmechanism of the LLM, and propose a framework that combines attack and defense.\nOur method is based on the linearly separable property of LLM intermediate\nlayer embedding, as well as the essence of jailbreak attack, which aims to\nembed harmful problems and transfer them to the safe area. We utilize\ngenerative adversarial network (GAN) to learn the security judgment boundary\ninside the LLM to achieve efficient jailbreak attack and defense. The\nexperimental results indicate that our method achieves an average jailbreak\nsuccess rate of 88.85\\% across three popular LLMs, while the defense success\nrate on the state-of-the-art jailbreak dataset reaches an average of 84.17\\%.\nThis not only validates the effectiveness of our approach but also sheds light\non the internal security mechanisms of LLMs, offering new insights for\nenhancing model security The code and data are available at\nhttps://github.com/NLPGM/CAVGAN."
                },
                "authors": [
                    {
                        "name": "Xiaohu Li"
                    },
                    {
                        "name": "Yunfeng Ning"
                    },
                    {
                        "name": "Zepeng Bao"
                    },
                    {
                        "name": "Mayi Xu"
                    },
                    {
                        "name": "Jianhao Chen"
                    },
                    {
                        "name": "Tieyun Qian"
                    }
                ],
                "author_detail": {
                    "name": "Tieyun Qian"
                },
                "author": "Tieyun Qian",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.06043v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.06043v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.10251v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.10251v3",
                "updated": "2025-07-08T14:44:04Z",
                "updated_parsed": [
                    2025,
                    7,
                    8,
                    14,
                    44,
                    4,
                    1,
                    189,
                    0
                ],
                "published": "2025-05-15T13:04:53Z",
                "published_parsed": [
                    2025,
                    5,
                    15,
                    13,
                    4,
                    53,
                    3,
                    135,
                    0
                ],
                "title": "SRT-H: A Hierarchical Framework for Autonomous Surgery via Language\n  Conditioned Imitation Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SRT-H: A Hierarchical Framework for Autonomous Surgery via Language\n  Conditioned Imitation Learning"
                },
                "summary": "Research on autonomous surgery has largely focused on simple task automation\nin controlled environments. However, real-world surgical applications demand\ndexterous manipulation over extended durations and generalization to the\ninherent variability of human tissue. These challenges remain difficult to\naddress using existing logic-based or conventional end-to-end learning\napproaches. To address this gap, we propose a hierarchical framework for\nperforming dexterous, long-horizon surgical steps. Our approach utilizes a\nhigh-level policy for task planning and a low-level policy for generating robot\ntrajectories. The high-level planner plans in language space, generating\ntask-level or corrective instructions that guide the robot through the\nlong-horizon steps and correct for the low-level policy's errors. We validate\nour framework through ex vivo experiments on cholecystectomy, a\ncommonly-practiced minimally invasive procedure, and conduct ablation studies\nto evaluate key components of the system. Our method achieves a 100\\% success\nrate across eight unseen ex vivo gallbladders, operating fully autonomously\nwithout human intervention. This work demonstrates step-level autonomy in a\nsurgical procedure, marking a milestone toward clinical deployment of\nautonomous surgical systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Research on autonomous surgery has largely focused on simple task automation\nin controlled environments. However, real-world surgical applications demand\ndexterous manipulation over extended durations and generalization to the\ninherent variability of human tissue. These challenges remain difficult to\naddress using existing logic-based or conventional end-to-end learning\napproaches. To address this gap, we propose a hierarchical framework for\nperforming dexterous, long-horizon surgical steps. Our approach utilizes a\nhigh-level policy for task planning and a low-level policy for generating robot\ntrajectories. The high-level planner plans in language space, generating\ntask-level or corrective instructions that guide the robot through the\nlong-horizon steps and correct for the low-level policy's errors. We validate\nour framework through ex vivo experiments on cholecystectomy, a\ncommonly-practiced minimally invasive procedure, and conduct ablation studies\nto evaluate key components of the system. Our method achieves a 100\\% success\nrate across eight unseen ex vivo gallbladders, operating fully autonomously\nwithout human intervention. This work demonstrates step-level autonomy in a\nsurgical procedure, marking a milestone toward clinical deployment of\nautonomous surgical systems."
                },
                "authors": [
                    {
                        "name": "Ji Woong Kim"
                    },
                    {
                        "name": "Juo-Tung Chen"
                    },
                    {
                        "name": "Pascal Hansen"
                    },
                    {
                        "name": "Lucy X. Shi"
                    },
                    {
                        "name": "Antony Goldenberg"
                    },
                    {
                        "name": "Samuel Schmidgall"
                    },
                    {
                        "name": "Paul Maria Scheikl"
                    },
                    {
                        "name": "Anton Deguet"
                    },
                    {
                        "name": "Brandon M. White"
                    },
                    {
                        "name": "De Ru Tsai"
                    },
                    {
                        "name": "Richard Cha"
                    },
                    {
                        "name": "Jeffrey Jopling"
                    },
                    {
                        "name": "Chelsea Finn"
                    },
                    {
                        "name": "Axel Krieger"
                    }
                ],
                "author_detail": {
                    "name": "Axel Krieger"
                },
                "author": "Axel Krieger",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.10251v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.10251v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.03724v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.03724v2",
                "updated": "2025-07-08T14:30:24Z",
                "updated_parsed": [
                    2025,
                    7,
                    8,
                    14,
                    30,
                    24,
                    1,
                    189,
                    0
                ],
                "published": "2025-07-04T17:21:46Z",
                "published_parsed": [
                    2025,
                    7,
                    4,
                    17,
                    21,
                    46,
                    4,
                    185,
                    0
                ],
                "title": "MemOS: A Memory OS for AI System",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MemOS: A Memory OS for AI System"
                },
                "summary": "Large Language Models (LLMs) have become an essential infrastructure for\nArtificial General Intelligence (AGI), yet their lack of well-defined memory\nmanagement systems hinders the development of long-context reasoning, continual\npersonalization, and knowledge consistency.Existing models mainly rely on\nstatic parameters and short-lived contextual states, limiting their ability to\ntrack user preferences or update knowledge over extended periods.While\nRetrieval-Augmented Generation (RAG) introduces external knowledge in plain\ntext, it remains a stateless workaround without lifecycle control or\nintegration with persistent representations.Recent work has modeled the\ntraining and inference cost of LLMs from a memory hierarchy perspective,\nshowing that introducing an explicit memory layer between parameter memory and\nexternal retrieval can substantially reduce these costs by externalizing\nspecific knowledge. Beyond computational efficiency, LLMs face broader\nchallenges arising from how information is distributed over time and context,\nrequiring systems capable of managing heterogeneous knowledge spanning\ndifferent temporal scales and sources. To address this challenge, we propose\nMemOS, a memory operating system that treats memory as a manageable system\nresource. It unifies the representation, scheduling, and evolution of\nplaintext, activation-based, and parameter-level memories, enabling\ncost-efficient storage and retrieval. As the basic unit, a MemCube encapsulates\nboth memory content and metadata such as provenance and versioning. MemCubes\ncan be composed, migrated, and fused over time, enabling flexible transitions\nbetween memory types and bridging retrieval with parameter-based learning.\nMemOS establishes a memory-centric system framework that brings\ncontrollability, plasticity, and evolvability to LLMs, laying the foundation\nfor continual learning and personalized modeling.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have become an essential infrastructure for\nArtificial General Intelligence (AGI), yet their lack of well-defined memory\nmanagement systems hinders the development of long-context reasoning, continual\npersonalization, and knowledge consistency.Existing models mainly rely on\nstatic parameters and short-lived contextual states, limiting their ability to\ntrack user preferences or update knowledge over extended periods.While\nRetrieval-Augmented Generation (RAG) introduces external knowledge in plain\ntext, it remains a stateless workaround without lifecycle control or\nintegration with persistent representations.Recent work has modeled the\ntraining and inference cost of LLMs from a memory hierarchy perspective,\nshowing that introducing an explicit memory layer between parameter memory and\nexternal retrieval can substantially reduce these costs by externalizing\nspecific knowledge. Beyond computational efficiency, LLMs face broader\nchallenges arising from how information is distributed over time and context,\nrequiring systems capable of managing heterogeneous knowledge spanning\ndifferent temporal scales and sources. To address this challenge, we propose\nMemOS, a memory operating system that treats memory as a manageable system\nresource. It unifies the representation, scheduling, and evolution of\nplaintext, activation-based, and parameter-level memories, enabling\ncost-efficient storage and retrieval. As the basic unit, a MemCube encapsulates\nboth memory content and metadata such as provenance and versioning. MemCubes\ncan be composed, migrated, and fused over time, enabling flexible transitions\nbetween memory types and bridging retrieval with parameter-based learning.\nMemOS establishes a memory-centric system framework that brings\ncontrollability, plasticity, and evolvability to LLMs, laying the foundation\nfor continual learning and personalized modeling."
                },
                "authors": [
                    {
                        "name": "Zhiyu Li"
                    },
                    {
                        "name": "Shichao Song"
                    },
                    {
                        "name": "Chenyang Xi"
                    },
                    {
                        "name": "Hanyu Wang"
                    },
                    {
                        "name": "Chen Tang"
                    },
                    {
                        "name": "Simin Niu"
                    },
                    {
                        "name": "Ding Chen"
                    },
                    {
                        "name": "Jiawei Yang"
                    },
                    {
                        "name": "Chunyu Li"
                    },
                    {
                        "name": "Qingchen Yu"
                    },
                    {
                        "name": "Jihao Zhao"
                    },
                    {
                        "name": "Yezhaohui Wang"
                    },
                    {
                        "name": "Peng Liu"
                    },
                    {
                        "name": "Zehao Lin"
                    },
                    {
                        "name": "Pengyuan Wang"
                    },
                    {
                        "name": "Jiahao Huo"
                    },
                    {
                        "name": "Tianyi Chen"
                    },
                    {
                        "name": "Kai Chen"
                    },
                    {
                        "name": "Kehang Li"
                    },
                    {
                        "name": "Zhen Tao"
                    },
                    {
                        "name": "Junpeng Ren"
                    },
                    {
                        "name": "Huayi Lai"
                    },
                    {
                        "name": "Hao Wu"
                    },
                    {
                        "name": "Bo Tang"
                    },
                    {
                        "name": "Zhenren Wang"
                    },
                    {
                        "name": "Zhaoxin Fan"
                    },
                    {
                        "name": "Ningyu Zhang"
                    },
                    {
                        "name": "Linfeng Zhang"
                    },
                    {
                        "name": "Junchi Yan"
                    },
                    {
                        "name": "Mingchuan Yang"
                    },
                    {
                        "name": "Tong Xu"
                    },
                    {
                        "name": "Wei Xu"
                    },
                    {
                        "name": "Huajun Chen"
                    },
                    {
                        "name": "Haofeng Wang"
                    },
                    {
                        "name": "Hongkang Yang"
                    },
                    {
                        "name": "Wentao Zhang"
                    },
                    {
                        "name": "Zhi-Qin John Xu"
                    },
                    {
                        "name": "Siheng Chen"
                    },
                    {
                        "name": "Feiyu Xiong"
                    }
                ],
                "author_detail": {
                    "name": "Feiyu Xiong"
                },
                "author": "Feiyu Xiong",
                "arxiv_comment": "36 pages, 10 figures, 5 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.03724v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.03724v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.06016v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.06016v1",
                "updated": "2025-07-08T14:23:41Z",
                "updated_parsed": [
                    2025,
                    7,
                    8,
                    14,
                    23,
                    41,
                    1,
                    189,
                    0
                ],
                "published": "2025-07-08T14:23:41Z",
                "published_parsed": [
                    2025,
                    7,
                    8,
                    14,
                    23,
                    41,
                    1,
                    189,
                    0
                ],
                "title": "Conditional Multi-Stage Failure Recovery for Embodied Agents",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Conditional Multi-Stage Failure Recovery for Embodied Agents"
                },
                "summary": "Embodied agents performing complex tasks are susceptible to execution\nfailures, motivating the need for effective failure recovery mechanisms. In\nthis work, we introduce a conditional multistage failure recovery framework\nthat employs zero-shot chain prompting. The framework is structured into four\nerror-handling stages, with three operating during task execution and one\nfunctioning as a post-execution reflection phase. Our approach utilises the\nreasoning capabilities of LLMs to analyse execution challenges within their\nenvironmental context and devise strategic solutions. We evaluate our method on\nthe TfD benchmark of the TEACH dataset and achieve state-of-the-art\nperformance, outperforming a baseline without error recovery by 11.5% and\nsurpassing the strongest existing model by 19%.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Embodied agents performing complex tasks are susceptible to execution\nfailures, motivating the need for effective failure recovery mechanisms. In\nthis work, we introduce a conditional multistage failure recovery framework\nthat employs zero-shot chain prompting. The framework is structured into four\nerror-handling stages, with three operating during task execution and one\nfunctioning as a post-execution reflection phase. Our approach utilises the\nreasoning capabilities of LLMs to analyse execution challenges within their\nenvironmental context and devise strategic solutions. We evaluate our method on\nthe TfD benchmark of the TEACH dataset and achieve state-of-the-art\nperformance, outperforming a baseline without error recovery by 11.5% and\nsurpassing the strongest existing model by 19%."
                },
                "authors": [
                    {
                        "name": "Youmna Farag"
                    },
                    {
                        "name": "Svetlana Stoyanchev"
                    },
                    {
                        "name": "Mohan Li"
                    },
                    {
                        "name": "Simon Keizer"
                    },
                    {
                        "name": "Rama Doddipatla"
                    }
                ],
                "author_detail": {
                    "name": "Rama Doddipatla"
                },
                "author": "Rama Doddipatla",
                "arxiv_comment": "Accepted at REALM 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.06016v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.06016v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.20866v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.20866v2",
                "updated": "2025-07-08T14:22:54Z",
                "updated_parsed": [
                    2025,
                    7,
                    8,
                    14,
                    22,
                    54,
                    1,
                    189,
                    0
                ],
                "published": "2024-12-30T11:10:22Z",
                "published_parsed": [
                    2024,
                    12,
                    30,
                    11,
                    10,
                    22,
                    0,
                    365,
                    0
                ],
                "title": "ContractTrace: Retracing Smart Contract Versions for Security Analyses",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ContractTrace: Retracing Smart Contract Versions for Security Analyses"
                },
                "summary": "Due to the inherent immutability of blockchain technology, smart contract\nupdates require their deployment at new addresses rather than modifying\nexisting ones, thus fragmenting version histories and creating critical blind\nspots for analyses. Indeed, for example, this fragmentation severely hinders\nsecurity researchers ability to track vulnerability lifecycles across contract\nversions. While platforms like Etherscan provide detailed information about\nEthereum smart contracts, they lack crucial functionality to trace\npredecessor-successor relationships within smart contract lineages, preventing\nsystematic analysis of how vulnerabilities emerge, propagate, and potentially\nremain unresolved across versions.To address the challenge of tracing smart\ncontract lineages, we adopt a Design Science Research (DSR) approach and\nintroduce ContractTrace, an automated infrastructure that accurately identifies\nand links versions of smart contracts into coherent lineages. This tool enables\nthe construction of lineageSet, an up-to-date, open-source dataset specifically\ndesigned to support security research on vulnerability, defect or any other\nproperty evolution patterns in smart contracts. Through a security-focused case\nstudy we demonstrate how ContractTrace reveals previously obscured\nvulnerability life-cycles within smart contract lineages, tracking whether\ncritical security flaws persist or get resolved across versions. This\ncapability is essential for understanding vulnerability propagation patterns\nand evaluating the effectiveness of security patches in blockchain\nenvironments. In the evaluation phase of our DSR approach, we validated our\nlineage detection methodology against an alternative approach using\nLocality-Sensitive Hashing (LSH) to cluster contract versions, confirming the\nsecurity relevance and accuracy of our technique.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Due to the inherent immutability of blockchain technology, smart contract\nupdates require their deployment at new addresses rather than modifying\nexisting ones, thus fragmenting version histories and creating critical blind\nspots for analyses. Indeed, for example, this fragmentation severely hinders\nsecurity researchers ability to track vulnerability lifecycles across contract\nversions. While platforms like Etherscan provide detailed information about\nEthereum smart contracts, they lack crucial functionality to trace\npredecessor-successor relationships within smart contract lineages, preventing\nsystematic analysis of how vulnerabilities emerge, propagate, and potentially\nremain unresolved across versions.To address the challenge of tracing smart\ncontract lineages, we adopt a Design Science Research (DSR) approach and\nintroduce ContractTrace, an automated infrastructure that accurately identifies\nand links versions of smart contracts into coherent lineages. This tool enables\nthe construction of lineageSet, an up-to-date, open-source dataset specifically\ndesigned to support security research on vulnerability, defect or any other\nproperty evolution patterns in smart contracts. Through a security-focused case\nstudy we demonstrate how ContractTrace reveals previously obscured\nvulnerability life-cycles within smart contract lineages, tracking whether\ncritical security flaws persist or get resolved across versions. This\ncapability is essential for understanding vulnerability propagation patterns\nand evaluating the effectiveness of security patches in blockchain\nenvironments. In the evaluation phase of our DSR approach, we validated our\nlineage detection methodology against an alternative approach using\nLocality-Sensitive Hashing (LSH) to cluster contract versions, confirming the\nsecurity relevance and accuracy of our technique."
                },
                "authors": [
                    {
                        "name": "Fatou Ndiaye Mbodji"
                    },
                    {
                        "name": "Vinny Adjibi"
                    },
                    {
                        "name": "Moustapha Awwalou Diouf"
                    },
                    {
                        "name": "Gervais Mendy"
                    },
                    {
                        "name": "Kui Liu"
                    },
                    {
                        "name": "Jacques Klein"
                    },
                    {
                        "name": "Tegawende Bissyande"
                    }
                ],
                "author_detail": {
                    "name": "Tegawende Bissyande"
                },
                "author": "Tegawende Bissyande",
                "arxiv_comment": "10 pages, 4 figures, 4 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.20866v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.20866v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.06013v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.06013v1",
                "updated": "2025-07-08T14:17:07Z",
                "updated_parsed": [
                    2025,
                    7,
                    8,
                    14,
                    17,
                    7,
                    1,
                    189,
                    0
                ],
                "published": "2025-07-08T14:17:07Z",
                "published_parsed": [
                    2025,
                    7,
                    8,
                    14,
                    17,
                    7,
                    1,
                    189,
                    0
                ],
                "title": "CogniSQL-R1-Zero: Lightweight Reinforced Reasoning for Efficient SQL\n  Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CogniSQL-R1-Zero: Lightweight Reinforced Reasoning for Efficient SQL\n  Generation"
                },
                "summary": "Translating natural language into SQL (Text-to-SQL) remains a core challenge\nat the intersection of language understanding and structured data access.\nAlthough large language models (LLMs) have improved fluency, generating correct\nand executable SQL, especially for complex queries, continues to be\nchallenging. We introduce CogniSQL-R1-Zero, a reinforcement learning (RL)\nframework and model that produces accurate SQL using a lightweight reward\nsignal based on execution correctness and format-tag compliance. By avoiding\nintermediate supervision, hybrid pipelines and complex reward shaping, our\nmethod encourages stable learning and stronger alignment with the ultimate task\nobjective-producing executable programs. CogniSQL-R1-Zero achieves\nstate-of-the-art execution accuracy on Text2SQL benchmark; BIRD bench,\noutperforming prior supervised and instruction-tuned baselines including SFT\nCodeS-7B, DeepSeek-Coder 236B, and Mistral 123B-despite being trained on a\nsignificantly smaller 7B backbone. This result underscores the scalability and\nefficiency of our RL-based approach when trained on just four NVIDIA A100 GPUs\n(40 GB VRAM each). To support further research in efficient and interpretable\nText-to-SQL modeling, we release two curated datasets: (i) a collection of\n5,024 reasoning traces with varying context lengths, and (ii) a\npositive-sampled corpus of 36,356 corpus of weakly supervised queries, each\nannotated with six semantically diverse reasoning paths. Together, these\ncontributions advance scalable, execution-aligned Text-to-SQL generation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Translating natural language into SQL (Text-to-SQL) remains a core challenge\nat the intersection of language understanding and structured data access.\nAlthough large language models (LLMs) have improved fluency, generating correct\nand executable SQL, especially for complex queries, continues to be\nchallenging. We introduce CogniSQL-R1-Zero, a reinforcement learning (RL)\nframework and model that produces accurate SQL using a lightweight reward\nsignal based on execution correctness and format-tag compliance. By avoiding\nintermediate supervision, hybrid pipelines and complex reward shaping, our\nmethod encourages stable learning and stronger alignment with the ultimate task\nobjective-producing executable programs. CogniSQL-R1-Zero achieves\nstate-of-the-art execution accuracy on Text2SQL benchmark; BIRD bench,\noutperforming prior supervised and instruction-tuned baselines including SFT\nCodeS-7B, DeepSeek-Coder 236B, and Mistral 123B-despite being trained on a\nsignificantly smaller 7B backbone. This result underscores the scalability and\nefficiency of our RL-based approach when trained on just four NVIDIA A100 GPUs\n(40 GB VRAM each). To support further research in efficient and interpretable\nText-to-SQL modeling, we release two curated datasets: (i) a collection of\n5,024 reasoning traces with varying context lengths, and (ii) a\npositive-sampled corpus of 36,356 corpus of weakly supervised queries, each\nannotated with six semantically diverse reasoning paths. Together, these\ncontributions advance scalable, execution-aligned Text-to-SQL generation."
                },
                "authors": [
                    {
                        "name": "Kushal Gajjar"
                    },
                    {
                        "name": "Harshit Sikchi"
                    },
                    {
                        "name": "Arpit Singh Gautam"
                    },
                    {
                        "name": "Marc Hammons"
                    },
                    {
                        "name": "Saurabh Jha"
                    }
                ],
                "author_detail": {
                    "name": "Saurabh Jha"
                },
                "author": "Saurabh Jha",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.06013v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.06013v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.05997v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.05997v1",
                "updated": "2025-07-08T13:55:25Z",
                "updated_parsed": [
                    2025,
                    7,
                    8,
                    13,
                    55,
                    25,
                    1,
                    189,
                    0
                ],
                "published": "2025-07-08T13:55:25Z",
                "published_parsed": [
                    2025,
                    7,
                    8,
                    13,
                    55,
                    25,
                    1,
                    189,
                    0
                ],
                "title": "DocIE@XLLM25: In-Context Learning for Information Extraction using Fully\n  Synthetic Demonstrations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DocIE@XLLM25: In-Context Learning for Information Extraction using Fully\n  Synthetic Demonstrations"
                },
                "summary": "Large, high-quality annotated corpora remain scarce in document-level entity\nand relation extraction in zero-shot or few-shot settings. In this paper, we\npresent a fully automatic, LLM-based pipeline for synthetic data generation and\nin-context learning for document-level entity and relation extraction. In\ncontrast to existing approaches that rely on manually annotated demonstrations\nor direct zero-shot inference, our method combines synthetic data generation\nwith retrieval-based in-context learning, using a reasoning-optimized language\nmodel. This allows us to build a high-quality demonstration database without\nmanual annotation and to dynamically retrieve relevant examples at inference\ntime. Based on our approach we produce a synthetic dataset of over $5k$\nWikipedia abstracts with approximately $59k$ entities and $30k$ relation\ntriples. Finally, we evaluate in-context learning performance on the DocIE\nshared task, extracting entities and relations from long documents in a\nzero-shot setting. We find that in-context joint entity and relation extraction\nat document-level remains a challenging task, even for state-of-the-art large\nlanguage models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large, high-quality annotated corpora remain scarce in document-level entity\nand relation extraction in zero-shot or few-shot settings. In this paper, we\npresent a fully automatic, LLM-based pipeline for synthetic data generation and\nin-context learning for document-level entity and relation extraction. In\ncontrast to existing approaches that rely on manually annotated demonstrations\nor direct zero-shot inference, our method combines synthetic data generation\nwith retrieval-based in-context learning, using a reasoning-optimized language\nmodel. This allows us to build a high-quality demonstration database without\nmanual annotation and to dynamically retrieve relevant examples at inference\ntime. Based on our approach we produce a synthetic dataset of over $5k$\nWikipedia abstracts with approximately $59k$ entities and $30k$ relation\ntriples. Finally, we evaluate in-context learning performance on the DocIE\nshared task, extracting entities and relations from long documents in a\nzero-shot setting. We find that in-context joint entity and relation extraction\nat document-level remains a challenging task, even for state-of-the-art large\nlanguage models."
                },
                "authors": [
                    {
                        "name": "Nicholas PopoviÄ"
                    },
                    {
                        "name": "Ashish Kangen"
                    },
                    {
                        "name": "Tim Schopf"
                    },
                    {
                        "name": "Michael FÃ¤rber"
                    }
                ],
                "author_detail": {
                    "name": "Michael FÃ¤rber"
                },
                "author": "Michael FÃ¤rber",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.05997v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.05997v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.05991v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.05991v1",
                "updated": "2025-07-08T13:52:45Z",
                "updated_parsed": [
                    2025,
                    7,
                    8,
                    13,
                    52,
                    45,
                    1,
                    189,
                    0
                ],
                "published": "2025-07-08T13:52:45Z",
                "published_parsed": [
                    2025,
                    7,
                    8,
                    13,
                    52,
                    45,
                    1,
                    189,
                    0
                ],
                "title": "Evolution without Large Models: Training Language Model with Task\n  Principles",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Evolution without Large Models: Training Language Model with Task\n  Principles"
                },
                "summary": "A common training approach for language models involves using a large-scale\nlanguage model to expand a human-provided dataset, which is subsequently used\nfor model training.This method significantly reduces training costs by\neliminating the need for extensive human data annotation. However, it still\nfaces challenges such as high carbon emissions during data augmentation and the\nrisk of data leakage when we use closed-source LLMs. To address these issues,\nwe propose a self-evolution method for language models. First, we introduce the\nMulti-level Principle Generation, which enables a large-scale model to\nsummarize task-completion principles based on a small amount of task data.\nThen, we propose the Principle-based Instance Generation, in which a\nsmaller-scale language model uses these task principles to generate a large\namount of data. This data is then used for model training. Experimental results\nshow that our proposed method significantly improves model performance compared\nto directly using a smaller-scale language model to generate data.\nAdditionally, since we only use the large-scale language model to generate the\ntask-completion principles, the carbon emissions associated with training the\nmodel are greatly reduced.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A common training approach for language models involves using a large-scale\nlanguage model to expand a human-provided dataset, which is subsequently used\nfor model training.This method significantly reduces training costs by\neliminating the need for extensive human data annotation. However, it still\nfaces challenges such as high carbon emissions during data augmentation and the\nrisk of data leakage when we use closed-source LLMs. To address these issues,\nwe propose a self-evolution method for language models. First, we introduce the\nMulti-level Principle Generation, which enables a large-scale model to\nsummarize task-completion principles based on a small amount of task data.\nThen, we propose the Principle-based Instance Generation, in which a\nsmaller-scale language model uses these task principles to generate a large\namount of data. This data is then used for model training. Experimental results\nshow that our proposed method significantly improves model performance compared\nto directly using a smaller-scale language model to generate data.\nAdditionally, since we only use the large-scale language model to generate the\ntask-completion principles, the carbon emissions associated with training the\nmodel are greatly reduced."
                },
                "authors": [
                    {
                        "name": "Minghang Zhu"
                    },
                    {
                        "name": "Shen Gao"
                    },
                    {
                        "name": "Zhengliang Shi"
                    },
                    {
                        "name": "Jiabao Fang"
                    },
                    {
                        "name": "Pengjie Ren"
                    },
                    {
                        "name": "Zhaochun Ren"
                    },
                    {
                        "name": "Zhumin Chen"
                    },
                    {
                        "name": "Shuo Shang"
                    }
                ],
                "author_detail": {
                    "name": "Shuo Shang"
                },
                "author": "Shuo Shang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.05991v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.05991v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.05984v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.05984v1",
                "updated": "2025-07-08T13:41:22Z",
                "updated_parsed": [
                    2025,
                    7,
                    8,
                    13,
                    41,
                    22,
                    1,
                    189,
                    0
                ],
                "published": "2025-07-08T13:41:22Z",
                "published_parsed": [
                    2025,
                    7,
                    8,
                    13,
                    41,
                    22,
                    1,
                    189,
                    0
                ],
                "title": "Development and Evaluation of HopeBot: an LLM-based chatbot for\n  structured and interactive PHQ-9 depression screening",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Development and Evaluation of HopeBot: an LLM-based chatbot for\n  structured and interactive PHQ-9 depression screening"
                },
                "summary": "Static tools like the Patient Health Questionnaire-9 (PHQ-9) effectively\nscreen depression but lack interactivity and adaptability. We developed\nHopeBot, a chatbot powered by a large language model (LLM) that administers the\nPHQ-9 using retrieval-augmented generation and real-time clarification. In a\nwithin-subject study, 132 adults in the United Kingdom and China completed both\nself-administered and chatbot versions. Scores demonstrated strong agreement\n(ICC = 0.91; 45% identical). Among 75 participants providing comparative\nfeedback, 71% reported greater trust in the chatbot, highlighting clearer\nstructure, interpretive guidance, and a supportive tone. Mean ratings (0-10)\nwere 8.4 for comfort, 7.7 for voice clarity, 7.6 for handling sensitive topics,\nand 7.4 for recommendation helpfulness; the latter varied significantly by\nemployment status and prior mental-health service use (p < 0.05). Overall,\n87.1% expressed willingness to reuse or recommend HopeBot. These findings\ndemonstrate voice-based LLM chatbots can feasibly serve as scalable, low-burden\nadjuncts for routine depression screening.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Static tools like the Patient Health Questionnaire-9 (PHQ-9) effectively\nscreen depression but lack interactivity and adaptability. We developed\nHopeBot, a chatbot powered by a large language model (LLM) that administers the\nPHQ-9 using retrieval-augmented generation and real-time clarification. In a\nwithin-subject study, 132 adults in the United Kingdom and China completed both\nself-administered and chatbot versions. Scores demonstrated strong agreement\n(ICC = 0.91; 45% identical). Among 75 participants providing comparative\nfeedback, 71% reported greater trust in the chatbot, highlighting clearer\nstructure, interpretive guidance, and a supportive tone. Mean ratings (0-10)\nwere 8.4 for comfort, 7.7 for voice clarity, 7.6 for handling sensitive topics,\nand 7.4 for recommendation helpfulness; the latter varied significantly by\nemployment status and prior mental-health service use (p < 0.05). Overall,\n87.1% expressed willingness to reuse or recommend HopeBot. These findings\ndemonstrate voice-based LLM chatbots can feasibly serve as scalable, low-burden\nadjuncts for routine depression screening."
                },
                "authors": [
                    {
                        "name": "Zhijun Guo"
                    },
                    {
                        "name": "Alvina Lai"
                    },
                    {
                        "name": "Julia Ive"
                    },
                    {
                        "name": "Alexandru Petcu"
                    },
                    {
                        "name": "Yutong Wang"
                    },
                    {
                        "name": "Luyuan Qi"
                    },
                    {
                        "name": "Johan H Thygesen"
                    },
                    {
                        "name": "Kezhi Li"
                    }
                ],
                "author_detail": {
                    "name": "Kezhi Li"
                },
                "author": "Kezhi Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.05984v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.05984v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.05981v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.05981v1",
                "updated": "2025-07-08T13:37:59Z",
                "updated_parsed": [
                    2025,
                    7,
                    8,
                    13,
                    37,
                    59,
                    1,
                    189,
                    0
                ],
                "published": "2025-07-08T13:37:59Z",
                "published_parsed": [
                    2025,
                    7,
                    8,
                    13,
                    37,
                    59,
                    1,
                    189,
                    0
                ],
                "title": "Multi-Agent Debate Strategies to Enhance Requirements Engineering with\n  Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-Agent Debate Strategies to Enhance Requirements Engineering with\n  Large Language Models"
                },
                "summary": "Context: Large Language Model (LLM) agents are becoming widely used for\nvarious Requirements Engineering (RE) tasks. Research on improving their\naccuracy mainly focuses on prompt engineering, model fine-tuning, and retrieval\naugmented generation. However, these methods often treat models as isolated\nblack boxes - relying on single-pass outputs without iterative refinement or\ncollaboration, limiting robustness and adaptability. Objective: We propose\nthat, just as human debates enhance accuracy and reduce bias in RE tasks by\nincorporating diverse perspectives, different LLM agents debating and\ncollaborating may achieve similar improvements. Our goal is to investigate\nwhether Multi-Agent Debate (MAD) strategies can enhance RE performance. Method:\nWe conducted a systematic study of existing MAD strategies across various\ndomains to identify their key characteristics. To assess their applicability in\nRE, we implemented and tested a preliminary MAD-based framework for RE\nclassification. Results: Our study identified and categorized several MAD\nstrategies, leading to a taxonomy outlining their core attributes. Our\npreliminary evaluation demonstrated the feasibility of applying MAD to RE\nclassification. Conclusions: MAD presents a promising approach for improving\nLLM accuracy in RE tasks. This study provides a foundational understanding of\nMAD strategies, offering insights for future research and refinements in RE\napplications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Context: Large Language Model (LLM) agents are becoming widely used for\nvarious Requirements Engineering (RE) tasks. Research on improving their\naccuracy mainly focuses on prompt engineering, model fine-tuning, and retrieval\naugmented generation. However, these methods often treat models as isolated\nblack boxes - relying on single-pass outputs without iterative refinement or\ncollaboration, limiting robustness and adaptability. Objective: We propose\nthat, just as human debates enhance accuracy and reduce bias in RE tasks by\nincorporating diverse perspectives, different LLM agents debating and\ncollaborating may achieve similar improvements. Our goal is to investigate\nwhether Multi-Agent Debate (MAD) strategies can enhance RE performance. Method:\nWe conducted a systematic study of existing MAD strategies across various\ndomains to identify their key characteristics. To assess their applicability in\nRE, we implemented and tested a preliminary MAD-based framework for RE\nclassification. Results: Our study identified and categorized several MAD\nstrategies, leading to a taxonomy outlining their core attributes. Our\npreliminary evaluation demonstrated the feasibility of applying MAD to RE\nclassification. Conclusions: MAD presents a promising approach for improving\nLLM accuracy in RE tasks. This study provides a foundational understanding of\nMAD strategies, offering insights for future research and refinements in RE\napplications."
                },
                "authors": [
                    {
                        "name": "Marc Oriol"
                    },
                    {
                        "name": "Quim Motger"
                    },
                    {
                        "name": "Jordi Marco"
                    },
                    {
                        "name": "Xavier Franch"
                    }
                ],
                "author_detail": {
                    "name": "Xavier Franch"
                },
                "author": "Xavier Franch",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.05981v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.05981v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.05980v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.05980v1",
                "updated": "2025-07-08T13:37:25Z",
                "updated_parsed": [
                    2025,
                    7,
                    8,
                    13,
                    37,
                    25,
                    1,
                    189,
                    0
                ],
                "published": "2025-07-08T13:37:25Z",
                "published_parsed": [
                    2025,
                    7,
                    8,
                    13,
                    37,
                    25,
                    1,
                    189,
                    0
                ],
                "title": "RabakBench: Scaling Human Annotations to Construct Localized\n  Multilingual Safety Benchmarks for Low-Resource Languages",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RabakBench: Scaling Human Annotations to Construct Localized\n  Multilingual Safety Benchmarks for Low-Resource Languages"
                },
                "summary": "Large language models (LLMs) and their safety classifiers often perform\npoorly on low-resource languages due to limited training data and evaluation\nbenchmarks. This paper introduces RabakBench, a new multilingual safety\nbenchmark localized to Singapore's unique linguistic context, covering\nSinglish, Chinese, Malay, and Tamil. RabakBench is constructed through a\nscalable three-stage pipeline: (i) Generate - adversarial example generation by\naugmenting real Singlish web content with LLM-driven red teaming; (ii) Label -\nsemi-automated multi-label safety annotation using majority-voted LLM labelers\naligned with human judgments; and (iii) Translate - high-fidelity translation\npreserving linguistic nuance and toxicity across languages. The final dataset\ncomprises over 5,000 safety-labeled examples across four languages and six\nfine-grained safety categories with severity levels. Evaluations of 11 popular\nopen-source and closed-source guardrail classifiers reveal significant\nperformance degradation. RabakBench not only enables robust safety evaluation\nin Southeast Asian multilingual settings but also offers a reproducible\nframework for building localized safety datasets in low-resource environments.\nThe benchmark dataset, including the human-verified translations, and\nevaluation code are publicly available.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) and their safety classifiers often perform\npoorly on low-resource languages due to limited training data and evaluation\nbenchmarks. This paper introduces RabakBench, a new multilingual safety\nbenchmark localized to Singapore's unique linguistic context, covering\nSinglish, Chinese, Malay, and Tamil. RabakBench is constructed through a\nscalable three-stage pipeline: (i) Generate - adversarial example generation by\naugmenting real Singlish web content with LLM-driven red teaming; (ii) Label -\nsemi-automated multi-label safety annotation using majority-voted LLM labelers\naligned with human judgments; and (iii) Translate - high-fidelity translation\npreserving linguistic nuance and toxicity across languages. The final dataset\ncomprises over 5,000 safety-labeled examples across four languages and six\nfine-grained safety categories with severity levels. Evaluations of 11 popular\nopen-source and closed-source guardrail classifiers reveal significant\nperformance degradation. RabakBench not only enables robust safety evaluation\nin Southeast Asian multilingual settings but also offers a reproducible\nframework for building localized safety datasets in low-resource environments.\nThe benchmark dataset, including the human-verified translations, and\nevaluation code are publicly available."
                },
                "authors": [
                    {
                        "name": "Gabriel Chua"
                    },
                    {
                        "name": "Leanne Tan"
                    },
                    {
                        "name": "Ziyu Ge"
                    },
                    {
                        "name": "Roy Ka-Wei Lee"
                    }
                ],
                "author_detail": {
                    "name": "Roy Ka-Wei Lee"
                },
                "author": "Roy Ka-Wei Lee",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.05980v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.05980v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.05970v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.05970v1",
                "updated": "2025-07-08T13:24:05Z",
                "updated_parsed": [
                    2025,
                    7,
                    8,
                    13,
                    24,
                    5,
                    1,
                    189,
                    0
                ],
                "published": "2025-07-08T13:24:05Z",
                "published_parsed": [
                    2025,
                    7,
                    8,
                    13,
                    24,
                    5,
                    1,
                    189,
                    0
                ],
                "title": "Automatic Synthesis of High-Quality Triplet Data for Composed Image\n  Retrieval",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Automatic Synthesis of High-Quality Triplet Data for Composed Image\n  Retrieval"
                },
                "summary": "As a challenging vision-language (VL) task, Composed Image Retrieval (CIR)\naims to retrieve target images using multimodal (image+text) queries. Although\nmany existing CIR methods have attained promising performance, their reliance\non costly, manually labeled triplets hinders scalability and zero-shot\ncapability. To address this issue, we propose a scalable pipeline for automatic\ntriplet generation, along with a fully synthetic dataset named Composed Image\nRetrieval on High-quality Synthetic Triplets (CIRHS). Our pipeline leverages a\nlarge language model (LLM) to generate diverse prompts, controlling a\ntext-to-image generative model to produce image pairs with identical elements\nin each pair, which are then filtered and reorganized to form the CIRHS\ndataset. In addition, we introduce Hybrid Contextual Alignment (CoAlign), a\nnovel CIR framework, which can accomplish global alignment and local reasoning\nwithin a broader context, enabling the model to learn more robust and\ninformative representations. By utilizing the synthetic CIRHS dataset, CoAlign\nachieves outstanding zero-shot performance on three commonly used benchmarks,\ndemonstrating for the first time the feasibility of training CIR models on a\nfully synthetic dataset. Furthermore, under supervised training, our method\noutperforms all the state-of-the-art supervised CIR approaches, validating the\neffectiveness of our proposed retrieval framework. The code and the CIRHS\ndataset will be released soon.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As a challenging vision-language (VL) task, Composed Image Retrieval (CIR)\naims to retrieve target images using multimodal (image+text) queries. Although\nmany existing CIR methods have attained promising performance, their reliance\non costly, manually labeled triplets hinders scalability and zero-shot\ncapability. To address this issue, we propose a scalable pipeline for automatic\ntriplet generation, along with a fully synthetic dataset named Composed Image\nRetrieval on High-quality Synthetic Triplets (CIRHS). Our pipeline leverages a\nlarge language model (LLM) to generate diverse prompts, controlling a\ntext-to-image generative model to produce image pairs with identical elements\nin each pair, which are then filtered and reorganized to form the CIRHS\ndataset. In addition, we introduce Hybrid Contextual Alignment (CoAlign), a\nnovel CIR framework, which can accomplish global alignment and local reasoning\nwithin a broader context, enabling the model to learn more robust and\ninformative representations. By utilizing the synthetic CIRHS dataset, CoAlign\nachieves outstanding zero-shot performance on three commonly used benchmarks,\ndemonstrating for the first time the feasibility of training CIR models on a\nfully synthetic dataset. Furthermore, under supervised training, our method\noutperforms all the state-of-the-art supervised CIR approaches, validating the\neffectiveness of our proposed retrieval framework. The code and the CIRHS\ndataset will be released soon."
                },
                "authors": [
                    {
                        "name": "Haiwen Li"
                    },
                    {
                        "name": "Delong Liu"
                    },
                    {
                        "name": "Zhaohui Hou"
                    },
                    {
                        "name": "Zhicheng Zhao"
                    },
                    {
                        "name": "Fei Su"
                    }
                ],
                "author_detail": {
                    "name": "Fei Su"
                },
                "author": "Fei Su",
                "arxiv_comment": "This paper was originally submitted to ACM MM 2025 on April 12, 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.05970v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.05970v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.13575v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.13575v2",
                "updated": "2025-07-08T13:20:21Z",
                "updated_parsed": [
                    2025,
                    7,
                    8,
                    13,
                    20,
                    21,
                    1,
                    189,
                    0
                ],
                "published": "2025-03-17T13:40:46Z",
                "published_parsed": [
                    2025,
                    3,
                    17,
                    13,
                    40,
                    46,
                    0,
                    76,
                    0
                ],
                "title": "Analytic Subspace Routing: How Recursive Least Squares Works in\n  Continual Learning of Large Language Model",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Analytic Subspace Routing: How Recursive Least Squares Works in\n  Continual Learning of Large Language Model"
                },
                "summary": "Large Language Models (LLMs) possess encompassing capabilities that can\nprocess diverse language-related tasks. However, finetuning on LLMs will\ndiminish this general skills and continual finetuning will further cause severe\ndegradation on accumulated knowledge. Recently, Continual Learning (CL) in\nLarge Language Models (LLMs) arises which aims to continually adapt the LLMs to\nnew tasks while maintaining previously learned knowledge and inheriting general\nskills. Existing techniques either leverage previous data to replay, leading to\nextra computational costs, or utilize a single parameter-efficient module to\nlearn the downstream task, constraining new knowledge absorption with\ninterference between different tasks. Toward these issues, this paper proposes\nAnalytic Subspace Routing(ASR) to address these challenges. For each task, we\nisolate the learning within a subspace of deep layers' features via low-rank\nadaptation, eliminating knowledge interference between different tasks.\nAdditionally, we propose an analytic routing mechanism to properly utilize\nknowledge learned in different subspaces. Our approach employs Recursive Least\nSquares to train a multi-task router model, allowing the router to dynamically\nadapt to incoming data without requiring access to historical data. Also, the\nrouter effectively assigns the current task to an appropriate subspace and has\na non-forgetting property of previously learned tasks with a solid theoretical\nguarantee. Experimental results demonstrate that our method achieves\nnear-perfect retention of prior knowledge while seamlessly integrating new\ninformation, effectively overcoming the core limitations of existing methods.\nOur code will be released after acceptance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) possess encompassing capabilities that can\nprocess diverse language-related tasks. However, finetuning on LLMs will\ndiminish this general skills and continual finetuning will further cause severe\ndegradation on accumulated knowledge. Recently, Continual Learning (CL) in\nLarge Language Models (LLMs) arises which aims to continually adapt the LLMs to\nnew tasks while maintaining previously learned knowledge and inheriting general\nskills. Existing techniques either leverage previous data to replay, leading to\nextra computational costs, or utilize a single parameter-efficient module to\nlearn the downstream task, constraining new knowledge absorption with\ninterference between different tasks. Toward these issues, this paper proposes\nAnalytic Subspace Routing(ASR) to address these challenges. For each task, we\nisolate the learning within a subspace of deep layers' features via low-rank\nadaptation, eliminating knowledge interference between different tasks.\nAdditionally, we propose an analytic routing mechanism to properly utilize\nknowledge learned in different subspaces. Our approach employs Recursive Least\nSquares to train a multi-task router model, allowing the router to dynamically\nadapt to incoming data without requiring access to historical data. Also, the\nrouter effectively assigns the current task to an appropriate subspace and has\na non-forgetting property of previously learned tasks with a solid theoretical\nguarantee. Experimental results demonstrate that our method achieves\nnear-perfect retention of prior knowledge while seamlessly integrating new\ninformation, effectively overcoming the core limitations of existing methods.\nOur code will be released after acceptance."
                },
                "authors": [
                    {
                        "name": "Kai Tong"
                    },
                    {
                        "name": "Kang Pan"
                    },
                    {
                        "name": "Xiao Zhang"
                    },
                    {
                        "name": "Erli Meng"
                    },
                    {
                        "name": "Run He"
                    },
                    {
                        "name": "Yawen Cui"
                    },
                    {
                        "name": "Nuoyan Guo"
                    },
                    {
                        "name": "Huiping Zhuang"
                    }
                ],
                "author_detail": {
                    "name": "Huiping Zhuang"
                },
                "author": "Huiping Zhuang",
                "arxiv_comment": "11 pages, 4 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.13575v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.13575v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.04649v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.04649v2",
                "updated": "2025-07-08T13:19:25Z",
                "updated_parsed": [
                    2025,
                    7,
                    8,
                    13,
                    19,
                    25,
                    1,
                    189,
                    0
                ],
                "published": "2025-05-06T18:50:02Z",
                "published_parsed": [
                    2025,
                    5,
                    6,
                    18,
                    50,
                    2,
                    1,
                    126,
                    0
                ],
                "title": "FRAME: Feedback-Refined Agent Methodology for Enhancing Medical Research\n  Insights",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FRAME: Feedback-Refined Agent Methodology for Enhancing Medical Research\n  Insights"
                },
                "summary": "The automation of scientific research through large language models (LLMs)\npresents significant opportunities but faces critical challenges in knowledge\nsynthesis and quality assurance. We introduce Feedback-Refined Agent\nMethodology (FRAME), a novel framework that enhances medical paper generation\nthrough iterative refinement and structured feedback. Our approach comprises\nthree key innovations: (1) A structured dataset construction method that\ndecomposes 4,287 medical papers into essential research components through\niterative refinement; (2) A tripartite architecture integrating Generator,\nEvaluator, and Reflector agents that progressively improve content quality\nthrough metric-driven feedback; and (3) A comprehensive evaluation framework\nthat combines statistical metrics with human-grounded benchmarks. Experimental\nresults demonstrate FRAME's effectiveness, achieving significant improvements\nover conventional approaches across multiple models (9.91% average gain with\nDeepSeek V3, comparable improvements with GPT-4o Mini) and evaluation\ndimensions. Human evaluation confirms that FRAME-generated papers achieve\nquality comparable to human-authored works, with particular strength in\nsynthesizing future research directions. The results demonstrated our work\ncould efficiently assist medical research by building a robust foundation for\nautomated medical research paper generation while maintaining rigorous academic\nstandards.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The automation of scientific research through large language models (LLMs)\npresents significant opportunities but faces critical challenges in knowledge\nsynthesis and quality assurance. We introduce Feedback-Refined Agent\nMethodology (FRAME), a novel framework that enhances medical paper generation\nthrough iterative refinement and structured feedback. Our approach comprises\nthree key innovations: (1) A structured dataset construction method that\ndecomposes 4,287 medical papers into essential research components through\niterative refinement; (2) A tripartite architecture integrating Generator,\nEvaluator, and Reflector agents that progressively improve content quality\nthrough metric-driven feedback; and (3) A comprehensive evaluation framework\nthat combines statistical metrics with human-grounded benchmarks. Experimental\nresults demonstrate FRAME's effectiveness, achieving significant improvements\nover conventional approaches across multiple models (9.91% average gain with\nDeepSeek V3, comparable improvements with GPT-4o Mini) and evaluation\ndimensions. Human evaluation confirms that FRAME-generated papers achieve\nquality comparable to human-authored works, with particular strength in\nsynthesizing future research directions. The results demonstrated our work\ncould efficiently assist medical research by building a robust foundation for\nautomated medical research paper generation while maintaining rigorous academic\nstandards."
                },
                "authors": [
                    {
                        "name": "Chengzhang Yu"
                    },
                    {
                        "name": "Yiming Zhang"
                    },
                    {
                        "name": "Zhixin Liu"
                    },
                    {
                        "name": "Zenghui Ding"
                    },
                    {
                        "name": "Yining Sun"
                    },
                    {
                        "name": "Zhanpeng Jin"
                    }
                ],
                "author_detail": {
                    "name": "Zhanpeng Jin"
                },
                "author": "Zhanpeng Jin",
                "arxiv_comment": "12 pages, 4 figures, 5 table",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.04649v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.04649v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.05965v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.05965v1",
                "updated": "2025-07-08T13:19:00Z",
                "updated_parsed": [
                    2025,
                    7,
                    8,
                    13,
                    19,
                    0,
                    1,
                    189,
                    0
                ],
                "published": "2025-07-08T13:19:00Z",
                "published_parsed": [
                    2025,
                    7,
                    8,
                    13,
                    19,
                    0,
                    1,
                    189,
                    0
                ],
                "title": "OpenFActScore: Open-Source Atomic Evaluation of Factuality in Text\n  Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "OpenFActScore: Open-Source Atomic Evaluation of Factuality in Text\n  Generation"
                },
                "summary": "We introduce OpenFActScore, an open-source implementation of the FActScore\nframework for evaluating the factuality of text generated by large language\nmodels (LLMs). FActScore evaluates the factual accuracy of long-form text by\nusing Atomic Fact Generation (AFG) to extract individual factual claims and\nAtomic Fact Validation (AFV) to verify each claim against a trusted knowledge\nsource. While the original FActScore relies on closed-source and commercial\nmodels such as InstructGPT and ChatGPT, OpenFActScore enables the use of any\nHugging Face-compatible model for both AFG and AFV. We provide a detailed\ntechnical overview of our implementation, highlighting design choices and\nmodifications made to support open models. We evaluate multiple open-source\nLLMs on both AFG and AFV using the original FActScore benchmark, reporting\nBERTScore-F1 for AFG and Error Rate relative to human annotations for AFV. Our\nresults show that open models can approximate the performance of closed-source\nsystems, with Gemma achieving the best overall performance, and our final setup\nobtains a 0.99 Pearson correlation with the original FActScore experiments.\nOpenFActScore promotes transparency, reproducibility, and cost-effective\nevaluation, and is available at: https://github.com/lflage/OpenFActScore.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce OpenFActScore, an open-source implementation of the FActScore\nframework for evaluating the factuality of text generated by large language\nmodels (LLMs). FActScore evaluates the factual accuracy of long-form text by\nusing Atomic Fact Generation (AFG) to extract individual factual claims and\nAtomic Fact Validation (AFV) to verify each claim against a trusted knowledge\nsource. While the original FActScore relies on closed-source and commercial\nmodels such as InstructGPT and ChatGPT, OpenFActScore enables the use of any\nHugging Face-compatible model for both AFG and AFV. We provide a detailed\ntechnical overview of our implementation, highlighting design choices and\nmodifications made to support open models. We evaluate multiple open-source\nLLMs on both AFG and AFV using the original FActScore benchmark, reporting\nBERTScore-F1 for AFG and Error Rate relative to human annotations for AFV. Our\nresults show that open models can approximate the performance of closed-source\nsystems, with Gemma achieving the best overall performance, and our final setup\nobtains a 0.99 Pearson correlation with the original FActScore experiments.\nOpenFActScore promotes transparency, reproducibility, and cost-effective\nevaluation, and is available at: https://github.com/lflage/OpenFActScore."
                },
                "authors": [
                    {
                        "name": "Lucas Fonseca Lage"
                    },
                    {
                        "name": "Simon Ostermann"
                    }
                ],
                "author_detail": {
                    "name": "Simon Ostermann"
                },
                "author": "Simon Ostermann",
                "arxiv_comment": "Submitted to EMNLP 2025 System Demonstrations track",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.05965v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.05965v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.11459v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.11459v2",
                "updated": "2025-07-08T13:14:01Z",
                "updated_parsed": [
                    2025,
                    7,
                    8,
                    13,
                    14,
                    1,
                    1,
                    189,
                    0
                ],
                "published": "2024-12-16T05:33:05Z",
                "published_parsed": [
                    2024,
                    12,
                    16,
                    5,
                    33,
                    5,
                    0,
                    351,
                    0
                ],
                "title": "Rethinking Associative Memory Mechanism in Induction Head",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Rethinking Associative Memory Mechanism in Induction Head"
                },
                "summary": "Induction head mechanism is a part of the computational circuits for\nin-context learning (ICL) that enable large language models (LLMs) to adapt to\nnew tasks without fine-tuning. Most existing work explains the training\ndynamics behind acquiring such a powerful mechanism. However, the model's\nability to coordinate in-context information over long contexts and global\nknowledge acquired during pretraining remains poorly understood. This paper\ninvestigates how a two-layer transformer thoroughly captures in-context\ninformation and balances it with pretrained bigram knowledge in next token\nprediction, from the viewpoint of associative memory. We theoretically analyze\nthe representation of weight matrices in attention layers and the resulting\nlogits when a transformer is given prompts generated by a bigram model. In the\nexperiments, we design specific prompts to evaluate whether the outputs of the\ntrained transformer align with the theoretical results.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Induction head mechanism is a part of the computational circuits for\nin-context learning (ICL) that enable large language models (LLMs) to adapt to\nnew tasks without fine-tuning. Most existing work explains the training\ndynamics behind acquiring such a powerful mechanism. However, the model's\nability to coordinate in-context information over long contexts and global\nknowledge acquired during pretraining remains poorly understood. This paper\ninvestigates how a two-layer transformer thoroughly captures in-context\ninformation and balances it with pretrained bigram knowledge in next token\nprediction, from the viewpoint of associative memory. We theoretically analyze\nthe representation of weight matrices in attention layers and the resulting\nlogits when a transformer is given prompts generated by a bigram model. In the\nexperiments, we design specific prompts to evaluate whether the outputs of the\ntrained transformer align with the theoretical results."
                },
                "authors": [
                    {
                        "name": "Shuo Wang"
                    },
                    {
                        "name": "Issei Sato"
                    }
                ],
                "author_detail": {
                    "name": "Issei Sato"
                },
                "author": "Issei Sato",
                "arxiv_comment": "COLM 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.11459v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.11459v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.22968v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.22968v4",
                "updated": "2025-07-08T13:13:04Z",
                "updated_parsed": [
                    2025,
                    7,
                    8,
                    13,
                    13,
                    4,
                    1,
                    189,
                    0
                ],
                "published": "2025-03-29T04:17:58Z",
                "published_parsed": [
                    2025,
                    3,
                    29,
                    4,
                    17,
                    58,
                    5,
                    88,
                    0
                ],
                "title": "Redefining Evaluation Standards: A Unified Framework for Evaluating the\n  Korean Capabilities of Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Redefining Evaluation Standards: A Unified Framework for Evaluating the\n  Korean Capabilities of Language Models"
                },
                "summary": "Recent advancements in Korean large language models (LLMs) have driven\nnumerous benchmarks and evaluation methods, yet inconsistent protocols cause up\nto 10 p.p performance gaps across institutions. Overcoming these\nreproducibility gaps does not mean enforcing a one-size-fits-all evaluation.\nRather, effective benchmarking requires diverse experimental approaches and a\nframework robust enough to support them. To this end, we introduce HRET (Haerae\nEvaluation Toolkit), an open-source, registry-based framework that unifies\nKorean LLM assessment. HRET integrates major Korean benchmarks, multiple\ninference backends, and multi-method evaluation, with language consistency\nenforcement to ensure genuine Korean outputs. Its modular registry design also\nenables rapid incorporation of new datasets, methods, and backends, ensuring\nthe toolkit adapts to evolving research needs. Beyond standard accuracy\nmetrics, HRET incorporates Korean-focused output analyses-morphology-aware\nType-Token Ratio (TTR) for evaluating lexical diversity and systematic\nkeyword-omission detection for identifying missing concepts-to provide\ndiagnostic insights into language-specific behaviors. These targeted analyses\nhelp researchers pinpoint morphological and semantic shortcomings in model\noutputs, guiding focused improvements in Korean LLM development.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advancements in Korean large language models (LLMs) have driven\nnumerous benchmarks and evaluation methods, yet inconsistent protocols cause up\nto 10 p.p performance gaps across institutions. Overcoming these\nreproducibility gaps does not mean enforcing a one-size-fits-all evaluation.\nRather, effective benchmarking requires diverse experimental approaches and a\nframework robust enough to support them. To this end, we introduce HRET (Haerae\nEvaluation Toolkit), an open-source, registry-based framework that unifies\nKorean LLM assessment. HRET integrates major Korean benchmarks, multiple\ninference backends, and multi-method evaluation, with language consistency\nenforcement to ensure genuine Korean outputs. Its modular registry design also\nenables rapid incorporation of new datasets, methods, and backends, ensuring\nthe toolkit adapts to evolving research needs. Beyond standard accuracy\nmetrics, HRET incorporates Korean-focused output analyses-morphology-aware\nType-Token Ratio (TTR) for evaluating lexical diversity and systematic\nkeyword-omission detection for identifying missing concepts-to provide\ndiagnostic insights into language-specific behaviors. These targeted analyses\nhelp researchers pinpoint morphological and semantic shortcomings in model\noutputs, guiding focused improvements in Korean LLM development."
                },
                "authors": [
                    {
                        "name": "Hanwool Lee"
                    },
                    {
                        "name": "Dasol Choi"
                    },
                    {
                        "name": "Sooyong Kim"
                    },
                    {
                        "name": "Ilgyun Jung"
                    },
                    {
                        "name": "Sangwon Baek"
                    },
                    {
                        "name": "Guijin Son"
                    },
                    {
                        "name": "Inseon Hwang"
                    },
                    {
                        "name": "Naeun Lee"
                    },
                    {
                        "name": "Seunghyeok Hong"
                    }
                ],
                "author_detail": {
                    "name": "Seunghyeok Hong"
                },
                "author": "Seunghyeok Hong",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.22968v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.22968v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.05962v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.05962v1",
                "updated": "2025-07-08T13:10:32Z",
                "updated_parsed": [
                    2025,
                    7,
                    8,
                    13,
                    10,
                    32,
                    1,
                    189,
                    0
                ],
                "published": "2025-07-08T13:10:32Z",
                "published_parsed": [
                    2025,
                    7,
                    8,
                    13,
                    10,
                    32,
                    1,
                    189,
                    0
                ],
                "title": "Evaluation of Large Language Model-Driven AutoML in Data and Model\n  Management from Human-Centered Perspective",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Evaluation of Large Language Model-Driven AutoML in Data and Model\n  Management from Human-Centered Perspective"
                },
                "summary": "As organizations increasingly seek to leverage machine learning (ML)\ncapabilities, the technical complexity of implementing ML solutions creates\nsignificant barriers to adoption and impacts operational efficiency. This\nresearch examines how Large Language Models (LLMs) can transform the\naccessibility of ML technologies within organizations through a human-centered\nAutomated Machine Learning (AutoML) approach. Through a comprehensive user\nstudy involving 15 professionals across various roles and technical\nbackgrounds, we evaluate the organizational impact of an LLM-based AutoML\nframework compared to traditional implementation methods. Our research offers\nfour significant contributions to both management practice and technical\ninnovation: First, we present pioneering evidence that LLM-based interfaces can\ndramatically improve ML implementation success rates, with 93.34% of users\nachieved superior performance in the LLM condition, with 46.67% showing higher\naccuracy (10-25% improvement over baseline) and 46.67% demonstrating\nsignificantly higher accuracy (>25% improvement over baseline), while 6.67%\nmaintained comparable performance levels; and 60% reporting substantially\nreduced development time. Second, we demonstrate how natural language\ninterfaces can effectively bridge the technical skills gap in organizations,\ncutting implementation time by 50% while improving accuracy across all\nexpertise levels. Third, we provide valuable insights for organizations\ndesigning human-AI collaborative systems, showing that our approach reduced\nerror resolution time by 73% and significantly accelerated employee learning\ncurves. Finally, we establish empirical support for natural language as an\neffective interface for complex technical systems, offering organizations a\npath to democratize ML capabilities without compromising quality or\nperformance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As organizations increasingly seek to leverage machine learning (ML)\ncapabilities, the technical complexity of implementing ML solutions creates\nsignificant barriers to adoption and impacts operational efficiency. This\nresearch examines how Large Language Models (LLMs) can transform the\naccessibility of ML technologies within organizations through a human-centered\nAutomated Machine Learning (AutoML) approach. Through a comprehensive user\nstudy involving 15 professionals across various roles and technical\nbackgrounds, we evaluate the organizational impact of an LLM-based AutoML\nframework compared to traditional implementation methods. Our research offers\nfour significant contributions to both management practice and technical\ninnovation: First, we present pioneering evidence that LLM-based interfaces can\ndramatically improve ML implementation success rates, with 93.34% of users\nachieved superior performance in the LLM condition, with 46.67% showing higher\naccuracy (10-25% improvement over baseline) and 46.67% demonstrating\nsignificantly higher accuracy (>25% improvement over baseline), while 6.67%\nmaintained comparable performance levels; and 60% reporting substantially\nreduced development time. Second, we demonstrate how natural language\ninterfaces can effectively bridge the technical skills gap in organizations,\ncutting implementation time by 50% while improving accuracy across all\nexpertise levels. Third, we provide valuable insights for organizations\ndesigning human-AI collaborative systems, showing that our approach reduced\nerror resolution time by 73% and significantly accelerated employee learning\ncurves. Finally, we establish empirical support for natural language as an\neffective interface for complex technical systems, offering organizations a\npath to democratize ML capabilities without compromising quality or\nperformance."
                },
                "authors": [
                    {
                        "name": "Jiapeng Yao"
                    },
                    {
                        "name": "Lantian Zhang"
                    },
                    {
                        "name": "Jiping Huang"
                    }
                ],
                "author_detail": {
                    "name": "Jiping Huang"
                },
                "author": "Jiping Huang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.05962v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.05962v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2403.12980v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2403.12980v3",
                "updated": "2025-07-08T12:58:07Z",
                "updated_parsed": [
                    2025,
                    7,
                    8,
                    12,
                    58,
                    7,
                    1,
                    189,
                    0
                ],
                "published": "2024-03-01T17:42:02Z",
                "published_parsed": [
                    2024,
                    3,
                    1,
                    17,
                    42,
                    2,
                    4,
                    61,
                    0
                ],
                "title": "Containerization in Multi-Cloud Environment: Roles, Strategies,\n  Challenges, and Solutions for Effective Implementation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Containerization in Multi-Cloud Environment: Roles, Strategies,\n  Challenges, and Solutions for Effective Implementation"
                },
                "summary": "Containerization in multi-cloud environments has received significant\nattention in recent years both from academic research and industrial\ndevelopment perspectives. However, there exists no effort to systematically\ninvestigate the state of research on this topic. The aim of this research is to\nsystematically identify and categorize the multiple aspects of containerization\nin multi-cloud environment. We conducted the Systematic Mapping Study (SMS) on\nthe literature published between January 2013 and July 2024. One hundred twenty\none studies were selected and the key results are: (1) Four leading themes on\ncontainerization in multi-cloud environment are identified: 'Scalability and\nHigh Availability', 'Performance and Optimization', 'Security and Privacy', and\n'Multi-Cloud Container Monitoring and Adaptation'. (2) Ninety-eight patterns\nand strategies for containerization in multicloud environment were classified\nacross 10 subcategories and 4 categories. (3) Ten quality attributes considered\nwere identified with 47 associated tactics. (4) Four catalogs consisting of\nchallenges and solutions related to security, automation, deployment, and\nmonitoring were introduced. The results of this SMS will assist researchers and\npractitioners in pursuing further studies on containerization in multi-cloud\nenvironment and developing specialized solutions for containerization\napplications in multi-cloud environment.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Containerization in multi-cloud environments has received significant\nattention in recent years both from academic research and industrial\ndevelopment perspectives. However, there exists no effort to systematically\ninvestigate the state of research on this topic. The aim of this research is to\nsystematically identify and categorize the multiple aspects of containerization\nin multi-cloud environment. We conducted the Systematic Mapping Study (SMS) on\nthe literature published between January 2013 and July 2024. One hundred twenty\none studies were selected and the key results are: (1) Four leading themes on\ncontainerization in multi-cloud environment are identified: 'Scalability and\nHigh Availability', 'Performance and Optimization', 'Security and Privacy', and\n'Multi-Cloud Container Monitoring and Adaptation'. (2) Ninety-eight patterns\nand strategies for containerization in multicloud environment were classified\nacross 10 subcategories and 4 categories. (3) Ten quality attributes considered\nwere identified with 47 associated tactics. (4) Four catalogs consisting of\nchallenges and solutions related to security, automation, deployment, and\nmonitoring were introduced. The results of this SMS will assist researchers and\npractitioners in pursuing further studies on containerization in multi-cloud\nenvironment and developing specialized solutions for containerization\napplications in multi-cloud environment."
                },
                "authors": [
                    {
                        "name": "Muhammad Waseem"
                    },
                    {
                        "name": "Aakash Ahmad"
                    },
                    {
                        "name": "Peng Liang"
                    },
                    {
                        "name": "Muhammad Azeem Akbar"
                    },
                    {
                        "name": "Arif Ali Khan"
                    },
                    {
                        "name": "Iftikhar Ahmad"
                    },
                    {
                        "name": "Manu SetÃ¤lÃ¤"
                    },
                    {
                        "name": "Tommi Mikkonen"
                    }
                ],
                "author_detail": {
                    "name": "Tommi Mikkonen"
                },
                "author": "Tommi Mikkonen",
                "arxiv_comment": "Preprint accepted for publication in Journal of Systems and Software,\n  2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2403.12980v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2403.12980v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.23782v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.23782v2",
                "updated": "2025-07-08T12:34:43Z",
                "updated_parsed": [
                    2025,
                    7,
                    8,
                    12,
                    34,
                    43,
                    1,
                    189,
                    0
                ],
                "published": "2025-06-30T12:23:57Z",
                "published_parsed": [
                    2025,
                    6,
                    30,
                    12,
                    23,
                    57,
                    0,
                    181,
                    0
                ],
                "title": "WATS: Calibrating Graph Neural Networks with Wavelet-Aware Temperature\n  Scaling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "WATS: Calibrating Graph Neural Networks with Wavelet-Aware Temperature\n  Scaling"
                },
                "summary": "Graph Neural Networks (GNNs) have demonstrated strong predictive performance\non relational data; however, their confidence estimates often misalign with\nactual predictive correctness, posing significant limitations for deployment in\nsafety-critical settings. While existing graph-aware calibration methods seek\nto mitigate this limitation, they primarily depend on coarse one-hop\nstatistics, such as neighbor-predicted confidence, or latent node embeddings,\nthereby neglecting the fine-grained structural heterogeneity inherent in graph\ntopology. In this work, we propose Wavelet-Aware Temperature Scaling (WATS), a\npost-hoc calibration framework that assigns node-specific temperatures based on\ntunable heat-kernel graph wavelet features. Specifically, WATS harnesses the\nscalability and topology sensitivity of graph wavelets to refine confidence\nestimates, all without necessitating model retraining or access to neighboring\nlogits or predictions. Extensive evaluations across seven benchmark datasets\nwith varying graph structures and two GNN backbones demonstrate that WATS\nachieves the lowest Expected Calibration Error (ECE) among all compared\nmethods, outperforming both classical and graph-specific baselines by up to\n42.3\\% in ECE and reducing calibration variance by 17.24\\% on average compared\nwith graph-specific methods. Moreover, WATS remains computationally efficient,\nscaling well across graphs of diverse sizes and densities. Code will be\nreleased based on publication.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Graph Neural Networks (GNNs) have demonstrated strong predictive performance\non relational data; however, their confidence estimates often misalign with\nactual predictive correctness, posing significant limitations for deployment in\nsafety-critical settings. While existing graph-aware calibration methods seek\nto mitigate this limitation, they primarily depend on coarse one-hop\nstatistics, such as neighbor-predicted confidence, or latent node embeddings,\nthereby neglecting the fine-grained structural heterogeneity inherent in graph\ntopology. In this work, we propose Wavelet-Aware Temperature Scaling (WATS), a\npost-hoc calibration framework that assigns node-specific temperatures based on\ntunable heat-kernel graph wavelet features. Specifically, WATS harnesses the\nscalability and topology sensitivity of graph wavelets to refine confidence\nestimates, all without necessitating model retraining or access to neighboring\nlogits or predictions. Extensive evaluations across seven benchmark datasets\nwith varying graph structures and two GNN backbones demonstrate that WATS\nachieves the lowest Expected Calibration Error (ECE) among all compared\nmethods, outperforming both classical and graph-specific baselines by up to\n42.3\\% in ECE and reducing calibration variance by 17.24\\% on average compared\nwith graph-specific methods. Moreover, WATS remains computationally efficient,\nscaling well across graphs of diverse sizes and densities. Code will be\nreleased based on publication."
                },
                "authors": [
                    {
                        "name": "Xiaoyang Li"
                    },
                    {
                        "name": "Linwei Tao"
                    },
                    {
                        "name": "Haohui Lu"
                    },
                    {
                        "name": "Minjing Dong"
                    },
                    {
                        "name": "Junbin Gao"
                    },
                    {
                        "name": "Chang Xu"
                    }
                ],
                "author_detail": {
                    "name": "Chang Xu"
                },
                "author": "Chang Xu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.23782v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.23782v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.05934v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.05934v1",
                "updated": "2025-07-08T12:34:10Z",
                "updated_parsed": [
                    2025,
                    7,
                    8,
                    12,
                    34,
                    10,
                    1,
                    189,
                    0
                ],
                "published": "2025-07-08T12:34:10Z",
                "published_parsed": [
                    2025,
                    7,
                    8,
                    12,
                    34,
                    10,
                    1,
                    189,
                    0
                ],
                "title": "BlueLM-2.5-3B Technical Report",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "BlueLM-2.5-3B Technical Report"
                },
                "summary": "We present BlueLM-2.5-3B, a compact and unified dense Multimodal Large\nLanguage Model (MLLM) designed for efficient edge-device deployment, offering\nstrong general-purpose and reasoning capabilities. To the best of our\nknowledge, this is the first 3B-scale MLLM to support both thinking and\nnon-thinking modes, while also enabling explicit control over thinking token\nbudget. BlueLM-2.5-3B is developed through diversified data curation, key data\nresampling, hybrid heterogeneous reinforcement learning, and a high-performance\ntraining infrastructure. Our model achieves superior multimodal capacity while\npreserving competitive pure-text performance with only 2.9 billion parameters.\nWe conduct comprehensive evaluations across a broad range of multimodal and\ntext-only benchmarks. In thinking mode, BlueLM-2.5-3B achieves comparable\nperformance to Qwen3-4B on text-only benchmarks, and trails the larger\nKimi-VL-A3B-16B by only about 5% on average across multimodal evaluations. In\nnon-thinking mode, it outperforms Qwen2.5-VL-3B on the majority of multimodal\nbenchmarks. Additionally, BlueLM-2.5-3B exhibits exceptional data efficiency.\nAll of the aforementioned performance is achieved with substantially less total\ntraining data than Qwen2.5-VL-3B and Qwen3-4B. We hope our work contributes to\nthe advancement of high-performance, on-device MLLMs and provides meaningful\ninsights to the research community.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present BlueLM-2.5-3B, a compact and unified dense Multimodal Large\nLanguage Model (MLLM) designed for efficient edge-device deployment, offering\nstrong general-purpose and reasoning capabilities. To the best of our\nknowledge, this is the first 3B-scale MLLM to support both thinking and\nnon-thinking modes, while also enabling explicit control over thinking token\nbudget. BlueLM-2.5-3B is developed through diversified data curation, key data\nresampling, hybrid heterogeneous reinforcement learning, and a high-performance\ntraining infrastructure. Our model achieves superior multimodal capacity while\npreserving competitive pure-text performance with only 2.9 billion parameters.\nWe conduct comprehensive evaluations across a broad range of multimodal and\ntext-only benchmarks. In thinking mode, BlueLM-2.5-3B achieves comparable\nperformance to Qwen3-4B on text-only benchmarks, and trails the larger\nKimi-VL-A3B-16B by only about 5% on average across multimodal evaluations. In\nnon-thinking mode, it outperforms Qwen2.5-VL-3B on the majority of multimodal\nbenchmarks. Additionally, BlueLM-2.5-3B exhibits exceptional data efficiency.\nAll of the aforementioned performance is achieved with substantially less total\ntraining data than Qwen2.5-VL-3B and Qwen3-4B. We hope our work contributes to\nthe advancement of high-performance, on-device MLLMs and provides meaningful\ninsights to the research community."
                },
                "authors": [
                    {
                        "name": "Baojiao Xiong"
                    },
                    {
                        "name": "Boheng Chen"
                    },
                    {
                        "name": "Chengzhi Wang"
                    },
                    {
                        "name": "Daxiong Luo"
                    },
                    {
                        "name": "Dongsheng Xu"
                    },
                    {
                        "name": "Dongyang Liu"
                    },
                    {
                        "name": "Fan Yang"
                    },
                    {
                        "name": "Fangyuan Li"
                    },
                    {
                        "name": "Fei Teng"
                    },
                    {
                        "name": "Feng Wang"
                    },
                    {
                        "name": "Fukang Qin"
                    },
                    {
                        "name": "Fuquan Peng"
                    },
                    {
                        "name": "Guanxin Tan"
                    },
                    {
                        "name": "Guozhi Wang"
                    },
                    {
                        "name": "Haibo Yu"
                    },
                    {
                        "name": "Haohao Gao"
                    },
                    {
                        "name": "Heng Liu"
                    },
                    {
                        "name": "Hongbo Yang"
                    },
                    {
                        "name": "Hongjian Zou"
                    },
                    {
                        "name": "Houzheng Shen"
                    },
                    {
                        "name": "Hu Meng"
                    },
                    {
                        "name": "Huan Li"
                    },
                    {
                        "name": "Hui Tan"
                    },
                    {
                        "name": "Jiali Chen"
                    },
                    {
                        "name": "Jianzhao Chen"
                    },
                    {
                        "name": "Jinliang Zhu"
                    },
                    {
                        "name": "Kai Wang"
                    },
                    {
                        "name": "Lei Wu"
                    },
                    {
                        "name": "Liangbing Liu"
                    },
                    {
                        "name": "Liuyang Bian"
                    },
                    {
                        "name": "Liyan He"
                    },
                    {
                        "name": "Long Liu"
                    },
                    {
                        "name": "Peiwen Li"
                    },
                    {
                        "name": "Penggang Shi"
                    },
                    {
                        "name": "Qi Ding"
                    },
                    {
                        "name": "Rui Hu"
                    },
                    {
                        "name": "Shuai Cao"
                    },
                    {
                        "name": "Shuai Ren"
                    },
                    {
                        "name": "Shuang Peng"
                    },
                    {
                        "name": "Teng Xie"
                    },
                    {
                        "name": "Weiji Chen"
                    },
                    {
                        "name": "Weilin Xiang"
                    },
                    {
                        "name": "Weixin Wu"
                    },
                    {
                        "name": "Xi Yin"
                    },
                    {
                        "name": "Xiaoxin Chen"
                    },
                    {
                        "name": "Xu Chen"
                    },
                    {
                        "name": "Yafei Wen"
                    },
                    {
                        "name": "Yan Hu"
                    },
                    {
                        "name": "Yanzhou Yang"
                    },
                    {
                        "name": "Yina Xie"
                    },
                    {
                        "name": "Yinghao Chen"
                    },
                    {
                        "name": "Yixuan Liao"
                    },
                    {
                        "name": "Yu Geng"
                    },
                    {
                        "name": "Yuanjiang Ouyang"
                    },
                    {
                        "name": "Yuanzhuo Yang"
                    },
                    {
                        "name": "Yuehua He"
                    },
                    {
                        "name": "Yushuai Peng"
                    },
                    {
                        "name": "Zhaoxiong Wang"
                    },
                    {
                        "name": "Zheng Wang"
                    },
                    {
                        "name": "Zhibo Zhou"
                    },
                    {
                        "name": "Ziyang Wu"
                    }
                ],
                "author_detail": {
                    "name": "Ziyang Wu"
                },
                "author": "Ziyang Wu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.05934v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.05934v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.05911v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.05911v1",
                "updated": "2025-07-08T11:57:16Z",
                "updated_parsed": [
                    2025,
                    7,
                    8,
                    11,
                    57,
                    16,
                    1,
                    189,
                    0
                ],
                "published": "2025-07-08T11:57:16Z",
                "published_parsed": [
                    2025,
                    7,
                    8,
                    11,
                    57,
                    16,
                    1,
                    189,
                    0
                ],
                "title": "Differentiable Reward Optimization for LLM based TTS system",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Differentiable Reward Optimization for LLM based TTS system"
                },
                "summary": "This paper proposes a novel Differentiable Reward Optimization (DiffRO)\nmethod aimed at enhancing the performance of neural codec language models based\ntext-to-speech (TTS) systems. In contrast to conventional reinforcement\nlearning from human feedback (RLHF) approaches applied to TTS, DiffRO directly\ncompute the rewards based on neural codec tokens, rather than relying on\nsynthesized audio. Furthermore, we employ the Gumbel-Softmax technique to\nrender the reward function differentiable, thereby streamlining the RLHF\ntraining process. Additionally, we introduce a multi-task reward (MTR) model\nwhich can provide feedback from different perspectives and find that it can\naugment the system's capability to follow instructions effectively.Experimental\nresults indicate that DiffRO significantly improves the pronunciation accuracy\nof the TTS system, achieving state-of-the-art (SOTA) WER results on the\nseed-tts-eval benchmark. Moreover, with the integration of the MTR model, we\ndemonstrate the ability to control emotional and quality attributes in a\nzero-shot manner.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper proposes a novel Differentiable Reward Optimization (DiffRO)\nmethod aimed at enhancing the performance of neural codec language models based\ntext-to-speech (TTS) systems. In contrast to conventional reinforcement\nlearning from human feedback (RLHF) approaches applied to TTS, DiffRO directly\ncompute the rewards based on neural codec tokens, rather than relying on\nsynthesized audio. Furthermore, we employ the Gumbel-Softmax technique to\nrender the reward function differentiable, thereby streamlining the RLHF\ntraining process. Additionally, we introduce a multi-task reward (MTR) model\nwhich can provide feedback from different perspectives and find that it can\naugment the system's capability to follow instructions effectively.Experimental\nresults indicate that DiffRO significantly improves the pronunciation accuracy\nof the TTS system, achieving state-of-the-art (SOTA) WER results on the\nseed-tts-eval benchmark. Moreover, with the integration of the MTR model, we\ndemonstrate the ability to control emotional and quality attributes in a\nzero-shot manner."
                },
                "authors": [
                    {
                        "name": "Changfeng Gao"
                    },
                    {
                        "name": "Zhihao Du"
                    },
                    {
                        "name": "Shiliang Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Shiliang Zhang"
                },
                "author": "Shiliang Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.05911v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.05911v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SD",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.06382v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.06382v3",
                "updated": "2025-07-08T11:43:16Z",
                "updated_parsed": [
                    2025,
                    7,
                    8,
                    11,
                    43,
                    16,
                    1,
                    189,
                    0
                ],
                "published": "2025-06-04T23:28:39Z",
                "published_parsed": [
                    2025,
                    6,
                    4,
                    23,
                    28,
                    39,
                    2,
                    155,
                    0
                ],
                "title": "On the Fundamental Impossibility of Hallucination Control in Large\n  Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "On the Fundamental Impossibility of Hallucination Control in Large\n  Language Models"
                },
                "summary": "We prove that perfect hallucination control in large language models is\nmathematically impossible. No LLM inference mechanism can simultaneously\nachieve truthful response generation, semantic information conservation,\nrelevant knowledge revelation, and knowledge-constrained optimality. This\nimpossibility is fundamental, arising from the mathematical structure of\ninformation aggregation itself rather than engineering limitations. The proof\nspans three mathematical frameworks: auction theory, proper scoring theory for\nprobabilistic predictions, and log-sum-exp analysis for transformer\narchitectures. In each setting, we demonstrate that information aggregation\ncreates unavoidable violations of conservation principles. The Jensen gap in\ntransformer probability aggregation provides a direct measure of this\nimpossibility. These results reframe hallucination from an engineering bug to\nan inevitable mathematical feature of distributed intelligence. There are\nfundamental trade-offs between truthfulness, knowledge utilization, and\nresponse completeness, providing principled foundations for managing rather\nthan eliminating hallucination. This work reveals deep connections between\nneural network inference, philosophy of knowledge and reasoning, and classical\nresults in game theory and information theory, opening new research directions\nfor developing beneficial AI systems within mathematical constraints.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We prove that perfect hallucination control in large language models is\nmathematically impossible. No LLM inference mechanism can simultaneously\nachieve truthful response generation, semantic information conservation,\nrelevant knowledge revelation, and knowledge-constrained optimality. This\nimpossibility is fundamental, arising from the mathematical structure of\ninformation aggregation itself rather than engineering limitations. The proof\nspans three mathematical frameworks: auction theory, proper scoring theory for\nprobabilistic predictions, and log-sum-exp analysis for transformer\narchitectures. In each setting, we demonstrate that information aggregation\ncreates unavoidable violations of conservation principles. The Jensen gap in\ntransformer probability aggregation provides a direct measure of this\nimpossibility. These results reframe hallucination from an engineering bug to\nan inevitable mathematical feature of distributed intelligence. There are\nfundamental trade-offs between truthfulness, knowledge utilization, and\nresponse completeness, providing principled foundations for managing rather\nthan eliminating hallucination. This work reveals deep connections between\nneural network inference, philosophy of knowledge and reasoning, and classical\nresults in game theory and information theory, opening new research directions\nfor developing beneficial AI systems within mathematical constraints."
                },
                "authors": [
                    {
                        "name": "MichaÅ P. Karpowicz"
                    }
                ],
                "author_detail": {
                    "name": "MichaÅ P. Karpowicz"
                },
                "author": "MichaÅ P. Karpowicz",
                "arxiv_comment": "transformer example extended, discussion and speculation section\n  added",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.06382v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.06382v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ML",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.GT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.05890v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.05890v1",
                "updated": "2025-07-08T11:26:03Z",
                "updated_parsed": [
                    2025,
                    7,
                    8,
                    11,
                    26,
                    3,
                    1,
                    189,
                    0
                ],
                "published": "2025-07-08T11:26:03Z",
                "published_parsed": [
                    2025,
                    7,
                    8,
                    11,
                    26,
                    3,
                    1,
                    189,
                    0
                ],
                "title": "Psychometric Item Validation Using Virtual Respondents with\n  Trait-Response Mediators",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Psychometric Item Validation Using Virtual Respondents with\n  Trait-Response Mediators"
                },
                "summary": "As psychometric surveys are increasingly used to assess the traits of large\nlanguage models (LLMs), the need for scalable survey item generation suited for\nLLMs has also grown. A critical challenge here is ensuring the construct\nvalidity of generated items, i.e., whether they truly measure the intended\ntrait. Traditionally, this requires costly, large-scale human data collection.\nTo make it efficient, we present a framework for virtual respondent simulation\nusing LLMs. Our central idea is to account for mediators: factors through which\nthe same trait can give rise to varying responses to a survey item. By\nsimulating respondents with diverse mediators, we identify survey items that\nrobustly measure intended traits. Experiments on three psychological trait\ntheories (Big5, Schwartz, VIA) show that our mediator generation methods and\nsimulation framework effectively identify high-validity items. LLMs demonstrate\nthe ability to generate plausible mediators from trait definitions and to\nsimulate respondent behavior for item validation. Our problem formulation,\nmetrics, methodology, and dataset open a new direction for cost-effective\nsurvey development and a deeper understanding of how LLMs replicate human-like\nbehavior. We will publicly release our dataset and code to support future work.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As psychometric surveys are increasingly used to assess the traits of large\nlanguage models (LLMs), the need for scalable survey item generation suited for\nLLMs has also grown. A critical challenge here is ensuring the construct\nvalidity of generated items, i.e., whether they truly measure the intended\ntrait. Traditionally, this requires costly, large-scale human data collection.\nTo make it efficient, we present a framework for virtual respondent simulation\nusing LLMs. Our central idea is to account for mediators: factors through which\nthe same trait can give rise to varying responses to a survey item. By\nsimulating respondents with diverse mediators, we identify survey items that\nrobustly measure intended traits. Experiments on three psychological trait\ntheories (Big5, Schwartz, VIA) show that our mediator generation methods and\nsimulation framework effectively identify high-validity items. LLMs demonstrate\nthe ability to generate plausible mediators from trait definitions and to\nsimulate respondent behavior for item validation. Our problem formulation,\nmetrics, methodology, and dataset open a new direction for cost-effective\nsurvey development and a deeper understanding of how LLMs replicate human-like\nbehavior. We will publicly release our dataset and code to support future work."
                },
                "authors": [
                    {
                        "name": "Sungjib Lim"
                    },
                    {
                        "name": "Woojung Song"
                    },
                    {
                        "name": "Eun-Ju Lee"
                    },
                    {
                        "name": "Yohan Jo"
                    }
                ],
                "author_detail": {
                    "name": "Yohan Jo"
                },
                "author": "Yohan Jo",
                "arxiv_comment": "17 pages, 7 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.05890v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.05890v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.05886v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.05886v1",
                "updated": "2025-07-08T11:19:09Z",
                "updated_parsed": [
                    2025,
                    7,
                    8,
                    11,
                    19,
                    9,
                    1,
                    189,
                    0
                ],
                "published": "2025-07-08T11:19:09Z",
                "published_parsed": [
                    2025,
                    7,
                    8,
                    11,
                    19,
                    9,
                    1,
                    189,
                    0
                ],
                "title": "Current Practices for Building LLM-Powered Reasoning Tools Are Ad Hoc --\n  and We Can Do Better",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Current Practices for Building LLM-Powered Reasoning Tools Are Ad Hoc --\n  and We Can Do Better"
                },
                "summary": "There is growing excitement about building software verifiers, synthesizers,\nand other Automated Reasoning (AR) tools by combining traditional symbolic\nalgorithms and Large Language Models (LLMs). Unfortunately, the current\npractice for constructing such neurosymbolic AR systems is an ad hoc\nprogramming model that does not have the strong guarantees of traditional\nsymbolic algorithms, nor a deep enough synchronization of neural networks and\nsymbolic reasoning to unlock the full potential of LLM-powered reasoning. I\npropose Neurosymbolic Transition Systems as a principled computational model\nthat can underlie infrastructure for building neurosymbolic AR tools. In this\nmodel, symbolic state is paired with intuition, and state transitions operate\nover symbols and intuition in parallel. I argue why this new paradigm can scale\nlogical reasoning beyond current capabilities while retaining the strong\nguarantees of symbolic algorithms, and I sketch out how the computational model\nI propose can be reified in a logic programming language.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "There is growing excitement about building software verifiers, synthesizers,\nand other Automated Reasoning (AR) tools by combining traditional symbolic\nalgorithms and Large Language Models (LLMs). Unfortunately, the current\npractice for constructing such neurosymbolic AR systems is an ad hoc\nprogramming model that does not have the strong guarantees of traditional\nsymbolic algorithms, nor a deep enough synchronization of neural networks and\nsymbolic reasoning to unlock the full potential of LLM-powered reasoning. I\npropose Neurosymbolic Transition Systems as a principled computational model\nthat can underlie infrastructure for building neurosymbolic AR tools. In this\nmodel, symbolic state is paired with intuition, and state transitions operate\nover symbols and intuition in parallel. I argue why this new paradigm can scale\nlogical reasoning beyond current capabilities while retaining the strong\nguarantees of symbolic algorithms, and I sketch out how the computational model\nI propose can be reified in a logic programming language."
                },
                "authors": [
                    {
                        "name": "Aaron Bembenek"
                    }
                ],
                "author_detail": {
                    "name": "Aaron Bembenek"
                },
                "arxiv_affiliation": "The University of Melbourne",
                "author": "Aaron Bembenek",
                "arxiv_comment": "6 pages, 4 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.05886v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.05886v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.05602v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.05602v2",
                "updated": "2025-07-08T11:06:22Z",
                "updated_parsed": [
                    2025,
                    7,
                    8,
                    11,
                    6,
                    22,
                    1,
                    189,
                    0
                ],
                "published": "2025-05-08T19:05:02Z",
                "published_parsed": [
                    2025,
                    5,
                    8,
                    19,
                    5,
                    2,
                    3,
                    128,
                    0
                ],
                "title": "HiBayES: A Hierarchical Bayesian Modeling Framework for AI Evaluation\n  Statistics",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "HiBayES: A Hierarchical Bayesian Modeling Framework for AI Evaluation\n  Statistics"
                },
                "summary": "As Large Language Models (LLMs) and other AI systems evolve, robustly\nestimating their capabilities from inherently stochastic outputs while\nsystematically quantifying uncertainty in these estimates becomes increasingly\nimportant. Further, advanced AI evaluations often have a nested hierarchical\nstructure, exhibit high levels of complexity, and come with high costs in\ntesting the most advanced AI systems. To address these challenges, we introduce\nHiBayES, a generalizable Hierarchical Bayesian modeling framework for AI\nEvaluation Statistics. HiBayES supports robust inferences in classical\nquestion-answer benchmarks and advanced agentic evaluations, particularly in\nlow-data scenarios (e.g., < 20 data points per evaluation). Built on\nGeneralized Linear Models (GLMs), Bayesian data analysis, and formal model\ncomparison, HiBayES provides principled uncertainty quantification and robust\nparameter estimation. This paper offers a comprehensive introduction to\nHiBayES, including illustrative examples, comparisons to conventional\nstatistical methods, and practical guidance for implementing multilevel\nBayesian GLMs. Additionally, we provide a HiBayES software package [4] (Beta\nversion) for out-of-the-box implementation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As Large Language Models (LLMs) and other AI systems evolve, robustly\nestimating their capabilities from inherently stochastic outputs while\nsystematically quantifying uncertainty in these estimates becomes increasingly\nimportant. Further, advanced AI evaluations often have a nested hierarchical\nstructure, exhibit high levels of complexity, and come with high costs in\ntesting the most advanced AI systems. To address these challenges, we introduce\nHiBayES, a generalizable Hierarchical Bayesian modeling framework for AI\nEvaluation Statistics. HiBayES supports robust inferences in classical\nquestion-answer benchmarks and advanced agentic evaluations, particularly in\nlow-data scenarios (e.g., < 20 data points per evaluation). Built on\nGeneralized Linear Models (GLMs), Bayesian data analysis, and formal model\ncomparison, HiBayES provides principled uncertainty quantification and robust\nparameter estimation. This paper offers a comprehensive introduction to\nHiBayES, including illustrative examples, comparisons to conventional\nstatistical methods, and practical guidance for implementing multilevel\nBayesian GLMs. Additionally, we provide a HiBayES software package [4] (Beta\nversion) for out-of-the-box implementation."
                },
                "authors": [
                    {
                        "name": "Lennart Luettgau"
                    },
                    {
                        "name": "Harry Coppock"
                    },
                    {
                        "name": "Magda Dubois"
                    },
                    {
                        "name": "Christopher Summerfield"
                    },
                    {
                        "name": "Cozmin Ududec"
                    }
                ],
                "author_detail": {
                    "name": "Cozmin Ududec"
                },
                "author": "Cozmin Ududec",
                "arxiv_comment": "23 pages, 9 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.05602v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.05602v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.AP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.05880v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.05880v1",
                "updated": "2025-07-08T11:04:17Z",
                "updated_parsed": [
                    2025,
                    7,
                    8,
                    11,
                    4,
                    17,
                    1,
                    189,
                    0
                ],
                "published": "2025-07-08T11:04:17Z",
                "published_parsed": [
                    2025,
                    7,
                    8,
                    11,
                    4,
                    17,
                    1,
                    189,
                    0
                ],
                "title": "RecRankerEval: A Flexible and Extensible Framework for Top-k LLM-based\n  Recommendation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RecRankerEval: A Flexible and Extensible Framework for Top-k LLM-based\n  Recommendation"
                },
                "summary": "A recent Large language model (LLM)-based recommendation model, called\nRecRanker, has demonstrated a superior performance in the top-k recommendation\ntask compared to other models. In particular, RecRanker samples users via\nclustering, generates an initial ranking list using an initial recommendation\nmodel, and fine-tunes an LLM through hybrid instruction tuning to infer user\npreferences. However, the contribution of each core component remains\nunderexplored. In this work, we inspect the reproducibility of RecRanker, and\nstudy the impact and role of its various components. We begin by reproducing\nthe RecRanker pipeline through the implementation of all its key components.\nOur reproduction shows that the pairwise and listwise methods achieve a\nperformance comparable to that reported in the original paper. For the\npointwise method, while we are also able to reproduce the original paper's\nresults, further analysis shows that the performance is abnormally high due to\ndata leakage from the inclusion of ground-truth information in the prompts. To\nenable a fair and comprehensive evaluation of LLM-based top-k recommendations,\nwe propose RecRankerEval, an extensible framework that covers five key\ndimensions: user sampling strategy, initial recommendation model, LLM backbone,\ndataset selection, and instruction tuning method. Using the RecRankerEval\nframework, we show that the original results of RecRanker can be reproduced on\nthe ML-100K and ML-1M datasets, as well as the additional Amazon-Music dataset,\nbut not on BookCrossing due to the lack of timestamp information in the\noriginal RecRanker paper. Furthermore, we demonstrate that RecRanker's\nperformance can be improved by employing alternative user sampling methods,\nstronger initial recommenders, and more capable LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A recent Large language model (LLM)-based recommendation model, called\nRecRanker, has demonstrated a superior performance in the top-k recommendation\ntask compared to other models. In particular, RecRanker samples users via\nclustering, generates an initial ranking list using an initial recommendation\nmodel, and fine-tunes an LLM through hybrid instruction tuning to infer user\npreferences. However, the contribution of each core component remains\nunderexplored. In this work, we inspect the reproducibility of RecRanker, and\nstudy the impact and role of its various components. We begin by reproducing\nthe RecRanker pipeline through the implementation of all its key components.\nOur reproduction shows that the pairwise and listwise methods achieve a\nperformance comparable to that reported in the original paper. For the\npointwise method, while we are also able to reproduce the original paper's\nresults, further analysis shows that the performance is abnormally high due to\ndata leakage from the inclusion of ground-truth information in the prompts. To\nenable a fair and comprehensive evaluation of LLM-based top-k recommendations,\nwe propose RecRankerEval, an extensible framework that covers five key\ndimensions: user sampling strategy, initial recommendation model, LLM backbone,\ndataset selection, and instruction tuning method. Using the RecRankerEval\nframework, we show that the original results of RecRanker can be reproduced on\nthe ML-100K and ML-1M datasets, as well as the additional Amazon-Music dataset,\nbut not on BookCrossing due to the lack of timestamp information in the\noriginal RecRanker paper. Furthermore, we demonstrate that RecRanker's\nperformance can be improved by employing alternative user sampling methods,\nstronger initial recommenders, and more capable LLMs."
                },
                "authors": [
                    {
                        "name": "Zeyuan Meng"
                    },
                    {
                        "name": "Zixuan Yi"
                    },
                    {
                        "name": "Iadh Ounis"
                    }
                ],
                "author_detail": {
                    "name": "Iadh Ounis"
                },
                "author": "Iadh Ounis",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.05880v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.05880v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.05863v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.05863v1",
                "updated": "2025-07-08T10:44:27Z",
                "updated_parsed": [
                    2025,
                    7,
                    8,
                    10,
                    44,
                    27,
                    1,
                    189,
                    0
                ],
                "published": "2025-07-08T10:44:27Z",
                "published_parsed": [
                    2025,
                    7,
                    8,
                    10,
                    44,
                    27,
                    1,
                    189,
                    0
                ],
                "title": "KERAG_R: Knowledge-Enhanced Retrieval-Augmented Generation for\n  Recommendation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "KERAG_R: Knowledge-Enhanced Retrieval-Augmented Generation for\n  Recommendation"
                },
                "summary": "Large Language Models (LLMs) have shown strong potential in recommender\nsystems due to their contextual learning and generalisation capabilities.\nExisting LLM-based recommendation approaches typically formulate the\nrecommendation task using specialised prompts designed to leverage their\ncontextual abilities, and aligning their outputs closely with human preferences\nto yield an improved recommendation performance. However, the use of LLMs for\nrecommendation tasks is limited by the absence of domain-specific knowledge.\nThis lack of relevant relational knowledge about the items to be recommended in\nthe LLM's pre-training corpus can lead to inaccuracies or hallucinations,\nresulting in incorrect or misleading recommendations. Moreover, directly using\ninformation from the knowledge graph introduces redundant and noisy\ninformation, which can affect the LLM's reasoning process or exceed its input\ncontext length, thereby reducing the performance of LLM-based recommendations.\nTo address the lack of domain-specific knowledge, we propose a novel model\ncalled Knowledge-Enhanced Retrieval-Augmented Generation for Recommendation\n(KERAG_R). Specifically, we leverage a graph retrieval-augmented generation\n(GraphRAG) component to integrate additional information from a knowledge graph\n(KG) into instructions, enabling the LLM to collaboratively exploit\nrecommendation signals from both text-based user interactions and the knowledge\ngraph to better estimate the users' preferences in a recommendation context. In\nparticular, we perform graph RAG by pre-training a graph attention network\n(GAT) to select the most relevant triple for the target users for the used LLM,\nthereby enhancing the LLM while reducing redundant and noisy information. Our\nextensive experiments on three public datasets show that our proposed KERAG_R\nmodel significantly outperforms ten existing state-of-the-art recommendation\nmethods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have shown strong potential in recommender\nsystems due to their contextual learning and generalisation capabilities.\nExisting LLM-based recommendation approaches typically formulate the\nrecommendation task using specialised prompts designed to leverage their\ncontextual abilities, and aligning their outputs closely with human preferences\nto yield an improved recommendation performance. However, the use of LLMs for\nrecommendation tasks is limited by the absence of domain-specific knowledge.\nThis lack of relevant relational knowledge about the items to be recommended in\nthe LLM's pre-training corpus can lead to inaccuracies or hallucinations,\nresulting in incorrect or misleading recommendations. Moreover, directly using\ninformation from the knowledge graph introduces redundant and noisy\ninformation, which can affect the LLM's reasoning process or exceed its input\ncontext length, thereby reducing the performance of LLM-based recommendations.\nTo address the lack of domain-specific knowledge, we propose a novel model\ncalled Knowledge-Enhanced Retrieval-Augmented Generation for Recommendation\n(KERAG_R). Specifically, we leverage a graph retrieval-augmented generation\n(GraphRAG) component to integrate additional information from a knowledge graph\n(KG) into instructions, enabling the LLM to collaboratively exploit\nrecommendation signals from both text-based user interactions and the knowledge\ngraph to better estimate the users' preferences in a recommendation context. In\nparticular, we perform graph RAG by pre-training a graph attention network\n(GAT) to select the most relevant triple for the target users for the used LLM,\nthereby enhancing the LLM while reducing redundant and noisy information. Our\nextensive experiments on three public datasets show that our proposed KERAG_R\nmodel significantly outperforms ten existing state-of-the-art recommendation\nmethods."
                },
                "authors": [
                    {
                        "name": "Zeyuan Meng"
                    },
                    {
                        "name": "Zixuan Yi"
                    },
                    {
                        "name": "Iadh Ounis"
                    }
                ],
                "author_detail": {
                    "name": "Iadh Ounis"
                },
                "author": "Iadh Ounis",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.05863v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.05863v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.03160v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.03160v3",
                "updated": "2025-07-09T06:49:35Z",
                "updated_parsed": [
                    2025,
                    7,
                    9,
                    6,
                    49,
                    35,
                    2,
                    190,
                    0
                ],
                "published": "2025-07-03T20:32:36Z",
                "published_parsed": [
                    2025,
                    7,
                    3,
                    20,
                    32,
                    36,
                    3,
                    184,
                    0
                ],
                "title": "Assessing Small Language Models for Code Generation: An Empirical Study\n  with Benchmarks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Assessing Small Language Models for Code Generation: An Empirical Study\n  with Benchmarks"
                },
                "summary": "The recent advancements of Small Language Models (SLMs) have opened new\npossibilities for efficient code generation. SLMs offer lightweight and\ncost-effective alternatives to Large Language Models (LLMs), making them\nattractive for use in resource-constrained environments. However, empirical\nunderstanding of SLMs, particularly their capabilities, limitations, and\nperformance trade-offs in code generation remains limited. This study presents\na comprehensive empirical evaluation of 20 open-source SLMs ranging from 0.4B\nto 10B parameters on five diverse code-related benchmarks (HumanEval, MBPP,\nMercury, HumanEvalPack, and CodeXGLUE). The models are assessed along three\ndimensions: i) functional correctness of generated code, ii) computational\nefficiency and iii) performance across multiple programming languages. The\nfindings of this study reveal that several compact SLMs achieve competitive\nresults while maintaining a balance between performance and efficiency, making\nthem viable for deployment in resource-constrained environments. However,\nachieving further improvements in accuracy requires switching to larger models.\nThese models generally outperform their smaller counterparts, but they require\nmuch more computational power. We observe that for 10% performance\nimprovements, models can require nearly a 4x increase in VRAM consumption,\nhighlighting a trade-off between effectiveness and scalability. Besides, the\nmultilingual performance analysis reveals that SLMs tend to perform better in\nlanguages such as Python, Java, and PHP, while exhibiting relatively weaker\nperformance in Go, C++, and Ruby. However, statistical analysis suggests these\ndifferences are not significant, indicating a generalizability of SLMs across\nprogramming languages. Based on the findings, this work provides insights into\nthe design and selection of SLMs for real-world code generation tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The recent advancements of Small Language Models (SLMs) have opened new\npossibilities for efficient code generation. SLMs offer lightweight and\ncost-effective alternatives to Large Language Models (LLMs), making them\nattractive for use in resource-constrained environments. However, empirical\nunderstanding of SLMs, particularly their capabilities, limitations, and\nperformance trade-offs in code generation remains limited. This study presents\na comprehensive empirical evaluation of 20 open-source SLMs ranging from 0.4B\nto 10B parameters on five diverse code-related benchmarks (HumanEval, MBPP,\nMercury, HumanEvalPack, and CodeXGLUE). The models are assessed along three\ndimensions: i) functional correctness of generated code, ii) computational\nefficiency and iii) performance across multiple programming languages. The\nfindings of this study reveal that several compact SLMs achieve competitive\nresults while maintaining a balance between performance and efficiency, making\nthem viable for deployment in resource-constrained environments. However,\nachieving further improvements in accuracy requires switching to larger models.\nThese models generally outperform their smaller counterparts, but they require\nmuch more computational power. We observe that for 10% performance\nimprovements, models can require nearly a 4x increase in VRAM consumption,\nhighlighting a trade-off between effectiveness and scalability. Besides, the\nmultilingual performance analysis reveals that SLMs tend to perform better in\nlanguages such as Python, Java, and PHP, while exhibiting relatively weaker\nperformance in Go, C++, and Ruby. However, statistical analysis suggests these\ndifferences are not significant, indicating a generalizability of SLMs across\nprogramming languages. Based on the findings, this work provides insights into\nthe design and selection of SLMs for real-world code generation tasks."
                },
                "authors": [
                    {
                        "name": "Md Mahade Hasan"
                    },
                    {
                        "name": "Muhammad Waseem"
                    },
                    {
                        "name": "Kai-Kristian Kemell"
                    },
                    {
                        "name": "Jussi Rasku"
                    },
                    {
                        "name": "Juha Ala-Rantala"
                    },
                    {
                        "name": "Pekka Abrahamsson"
                    }
                ],
                "author_detail": {
                    "name": "Pekka Abrahamsson"
                },
                "author": "Pekka Abrahamsson",
                "arxiv_comment": "17 pages, 10 Tables, 57 figures. Includes benchmarks and multilingual\n  evaluation. Submitted to the Journal of Systems and Software",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.03160v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.03160v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.18099v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.18099v2",
                "updated": "2025-07-08T09:49:57Z",
                "updated_parsed": [
                    2025,
                    7,
                    8,
                    9,
                    49,
                    57,
                    1,
                    189,
                    0
                ],
                "published": "2025-01-30T02:21:59Z",
                "published_parsed": [
                    2025,
                    1,
                    30,
                    2,
                    21,
                    59,
                    3,
                    30,
                    0
                ],
                "title": "Learning to Plan & Reason for Evaluation with Thinking-LLM-as-a-Judge",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Learning to Plan & Reason for Evaluation with Thinking-LLM-as-a-Judge"
                },
                "summary": "LLM-as-a-Judge models generate chain-of-thought (CoT) sequences intended to\ncapture the step-bystep reasoning process that underlies the final evaluation\nof a response. However, due to the lack of human annotated CoTs for evaluation,\nthe required components and structure of effective reasoning traces remain\nunderstudied. Consequently, previous approaches often (1) constrain reasoning\ntraces to hand-designed components, such as a list of criteria, reference\nanswers, or verification questions and (2) structure them such that planning is\nintertwined with the reasoning for evaluation. In this work, we propose\nEvalPlanner, a preference optimization algorithm for Thinking-LLM-as-a-Judge\nthat first generates an unconstrained evaluation plan, followed by its\nexecution, and then the final judgment. In a self-training loop, EvalPlanner\niteratively optimizes over synthetically constructed evaluation plans and\nexecutions, leading to better final verdicts. Our method achieves a new\nstate-of-the-art performance for generative reward models on RewardBench (with\na score of 93.9), despite being trained on fewer amount of, and synthetically\ngenerated, preference pairs. Additional experiments on other benchmarks like\nRM-Bench, JudgeBench, and FollowBenchEval further highlight the utility of both\nplanning and reasoning for building robust LLM-as-a-Judge reasoning models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM-as-a-Judge models generate chain-of-thought (CoT) sequences intended to\ncapture the step-bystep reasoning process that underlies the final evaluation\nof a response. However, due to the lack of human annotated CoTs for evaluation,\nthe required components and structure of effective reasoning traces remain\nunderstudied. Consequently, previous approaches often (1) constrain reasoning\ntraces to hand-designed components, such as a list of criteria, reference\nanswers, or verification questions and (2) structure them such that planning is\nintertwined with the reasoning for evaluation. In this work, we propose\nEvalPlanner, a preference optimization algorithm for Thinking-LLM-as-a-Judge\nthat first generates an unconstrained evaluation plan, followed by its\nexecution, and then the final judgment. In a self-training loop, EvalPlanner\niteratively optimizes over synthetically constructed evaluation plans and\nexecutions, leading to better final verdicts. Our method achieves a new\nstate-of-the-art performance for generative reward models on RewardBench (with\na score of 93.9), despite being trained on fewer amount of, and synthetically\ngenerated, preference pairs. Additional experiments on other benchmarks like\nRM-Bench, JudgeBench, and FollowBenchEval further highlight the utility of both\nplanning and reasoning for building robust LLM-as-a-Judge reasoning models."
                },
                "authors": [
                    {
                        "name": "Swarnadeep Saha"
                    },
                    {
                        "name": "Xian Li"
                    },
                    {
                        "name": "Marjan Ghazvininejad"
                    },
                    {
                        "name": "Jason Weston"
                    },
                    {
                        "name": "Tianlu Wang"
                    }
                ],
                "author_detail": {
                    "name": "Tianlu Wang"
                },
                "author": "Tianlu Wang",
                "arxiv_comment": "ICML 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.18099v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.18099v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.20545v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.20545v2",
                "updated": "2025-07-08T09:46:27Z",
                "updated_parsed": [
                    2025,
                    7,
                    8,
                    9,
                    46,
                    27,
                    1,
                    189,
                    0
                ],
                "published": "2024-12-29T18:34:10Z",
                "published_parsed": [
                    2024,
                    12,
                    29,
                    18,
                    34,
                    10,
                    6,
                    364,
                    0
                ],
                "title": "The Impact of Prompt Programming on Function-Level Code Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Impact of Prompt Programming on Function-Level Code Generation"
                },
                "summary": "Large Language Models (LLMs) are increasingly used by software engineers for\ncode generation. However, limitations of LLMs such as irrelevant or incorrect\ncode have highlighted the need for prompt programming (or prompt engineering)\nwhere engineers apply specific prompt techniques (e.g., chain-of-thought or\ninput-output examples) to improve the generated code. While some prompt\ntechniques have been studied, the impact of different techniques -- and their\ninteractions -- on code generation is still not fully understood. In this\nstudy, we introduce CodePromptEval, a dataset of 7072 prompts designed to\nevaluate five prompt techniques (few-shot, persona, chain-of-thought, function\nsignature, list of packages) and their effect on the correctness, similarity,\nand quality of complete functions generated by three LLMs (GPT-4o, Llama3, and\nMistral). Our findings show that while certain prompt techniques significantly\ninfluence the generated code, combining multiple techniques does not\nnecessarily improve the outcome. Additionally, we observed a trade-off between\ncorrectness and quality when using prompt techniques. Our dataset and\nreplication package enable future research on improving LLM-generated code and\nevaluating new prompt techniques.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) are increasingly used by software engineers for\ncode generation. However, limitations of LLMs such as irrelevant or incorrect\ncode have highlighted the need for prompt programming (or prompt engineering)\nwhere engineers apply specific prompt techniques (e.g., chain-of-thought or\ninput-output examples) to improve the generated code. While some prompt\ntechniques have been studied, the impact of different techniques -- and their\ninteractions -- on code generation is still not fully understood. In this\nstudy, we introduce CodePromptEval, a dataset of 7072 prompts designed to\nevaluate five prompt techniques (few-shot, persona, chain-of-thought, function\nsignature, list of packages) and their effect on the correctness, similarity,\nand quality of complete functions generated by three LLMs (GPT-4o, Llama3, and\nMistral). Our findings show that while certain prompt techniques significantly\ninfluence the generated code, combining multiple techniques does not\nnecessarily improve the outcome. Additionally, we observed a trade-off between\ncorrectness and quality when using prompt techniques. Our dataset and\nreplication package enable future research on improving LLM-generated code and\nevaluating new prompt techniques."
                },
                "authors": [
                    {
                        "name": "Ranim Khojah"
                    },
                    {
                        "name": "Francisco Gomes de Oliveira Neto"
                    },
                    {
                        "name": "Mazen Mohamad"
                    },
                    {
                        "name": "Philipp Leitner"
                    }
                ],
                "author_detail": {
                    "name": "Philipp Leitner"
                },
                "author": "Philipp Leitner",
                "arxiv_comment": "Accepted at Transactions on Software Engineering (TSE).\n  CodePromptEval dataset and replication package on GitHub:\n  https://github.com/icetlab/CodePromptEval",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.20545v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.20545v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.05822v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.05822v1",
                "updated": "2025-07-08T09:43:17Z",
                "updated_parsed": [
                    2025,
                    7,
                    8,
                    9,
                    43,
                    17,
                    1,
                    189,
                    0
                ],
                "published": "2025-07-08T09:43:17Z",
                "published_parsed": [
                    2025,
                    7,
                    8,
                    9,
                    43,
                    17,
                    1,
                    189,
                    0
                ],
                "title": "Video Event Reasoning and Prediction by Fusing World Knowledge from LLMs\n  with Vision Foundation Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Video Event Reasoning and Prediction by Fusing World Knowledge from LLMs\n  with Vision Foundation Models"
                },
                "summary": "Current video understanding models excel at recognizing \"what\" is happening\nbut fall short in high-level cognitive tasks like causal reasoning and future\nprediction, a limitation rooted in their lack of commonsense world knowledge.\nTo bridge this cognitive gap, we propose a novel framework that synergistically\nfuses a powerful Vision Foundation Model (VFM) for deep visual perception with\na Large Language Model (LLM) serving as a knowledge-driven reasoning core. Our\nkey technical innovation is a sophisticated fusion module, inspired by the\nQ-Former architecture, which distills complex spatiotemporal and object-centric\nvisual features into a concise, language-aligned representation. This enables\nthe LLM to effectively ground its inferential processes in direct visual\nevidence. The model is trained via a two-stage strategy, beginning with\nlarge-scale alignment pre-training on video-text data, followed by targeted\ninstruction fine-tuning on a curated dataset designed to elicit advanced\nreasoning and prediction skills. Extensive experiments demonstrate that our\nmodel achieves state-of-the-art performance on multiple challenging benchmarks.\nNotably, it exhibits remarkable zero-shot generalization to unseen reasoning\ntasks, and our in-depth ablation studies validate the critical contribution of\neach architectural component. This work pushes the boundary of machine\nperception from simple recognition towards genuine cognitive understanding,\npaving the way for more intelligent and capable AI systems in robotics,\nhuman-computer interaction, and beyond.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Current video understanding models excel at recognizing \"what\" is happening\nbut fall short in high-level cognitive tasks like causal reasoning and future\nprediction, a limitation rooted in their lack of commonsense world knowledge.\nTo bridge this cognitive gap, we propose a novel framework that synergistically\nfuses a powerful Vision Foundation Model (VFM) for deep visual perception with\na Large Language Model (LLM) serving as a knowledge-driven reasoning core. Our\nkey technical innovation is a sophisticated fusion module, inspired by the\nQ-Former architecture, which distills complex spatiotemporal and object-centric\nvisual features into a concise, language-aligned representation. This enables\nthe LLM to effectively ground its inferential processes in direct visual\nevidence. The model is trained via a two-stage strategy, beginning with\nlarge-scale alignment pre-training on video-text data, followed by targeted\ninstruction fine-tuning on a curated dataset designed to elicit advanced\nreasoning and prediction skills. Extensive experiments demonstrate that our\nmodel achieves state-of-the-art performance on multiple challenging benchmarks.\nNotably, it exhibits remarkable zero-shot generalization to unseen reasoning\ntasks, and our in-depth ablation studies validate the critical contribution of\neach architectural component. This work pushes the boundary of machine\nperception from simple recognition towards genuine cognitive understanding,\npaving the way for more intelligent and capable AI systems in robotics,\nhuman-computer interaction, and beyond."
                },
                "authors": [
                    {
                        "name": "L'ea Dubois"
                    },
                    {
                        "name": "Klaus Schmidt"
                    },
                    {
                        "name": "Chengyu Wang"
                    },
                    {
                        "name": "Ji-Hoon Park"
                    },
                    {
                        "name": "Lin Wang"
                    },
                    {
                        "name": "Santiago Munoz"
                    }
                ],
                "author_detail": {
                    "name": "Santiago Munoz"
                },
                "author": "Santiago Munoz",
                "arxiv_comment": "22 pages, 4 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.05822v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.05822v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "CS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.10",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.05820v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.05820v1",
                "updated": "2025-07-08T09:39:02Z",
                "updated_parsed": [
                    2025,
                    7,
                    8,
                    9,
                    39,
                    2,
                    1,
                    189,
                    0
                ],
                "published": "2025-07-08T09:39:02Z",
                "published_parsed": [
                    2025,
                    7,
                    8,
                    9,
                    39,
                    2,
                    1,
                    189,
                    0
                ],
                "title": "Constella: Supporting Storywriters' Interconnected Character Creation\n  through LLM-based Multi-Agents",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Constella: Supporting Storywriters' Interconnected Character Creation\n  through LLM-based Multi-Agents"
                },
                "summary": "Creating a cast of characters by attending to their relational dynamics is a\ncritical aspect of most long-form storywriting. However, our formative study\n(N=14) reveals that writers struggle to envision new characters that could\ninfluence existing ones, to balance similarities and differences among\ncharacters, and to intricately flesh out their relationships. Based on these\nobservations, we designed Constella, an LLM-based multi-agent tool that\nsupports storywriters' interconnected character creation process. Constella\nsuggests related characters (FRIENDS DISCOVERY feature), reveals the inner\nmindscapes of several characters simultaneously (JOURNALS feature), and\nmanifests relationships through inter-character responses (COMMENTS feature).\nOur 7-8 day deployment study with storywriters (N=11) shows that Constella\nenabled the creation of expansive communities composed of related characters,\nfacilitated the comparison of characters' thoughts and emotions, and deepened\nwriters' understanding of character relationships. We conclude by discussing\nhow multi-agent interactions can help distribute writers' attention and effort\nacross the character cast.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Creating a cast of characters by attending to their relational dynamics is a\ncritical aspect of most long-form storywriting. However, our formative study\n(N=14) reveals that writers struggle to envision new characters that could\ninfluence existing ones, to balance similarities and differences among\ncharacters, and to intricately flesh out their relationships. Based on these\nobservations, we designed Constella, an LLM-based multi-agent tool that\nsupports storywriters' interconnected character creation process. Constella\nsuggests related characters (FRIENDS DISCOVERY feature), reveals the inner\nmindscapes of several characters simultaneously (JOURNALS feature), and\nmanifests relationships through inter-character responses (COMMENTS feature).\nOur 7-8 day deployment study with storywriters (N=11) shows that Constella\nenabled the creation of expansive communities composed of related characters,\nfacilitated the comparison of characters' thoughts and emotions, and deepened\nwriters' understanding of character relationships. We conclude by discussing\nhow multi-agent interactions can help distribute writers' attention and effort\nacross the character cast."
                },
                "authors": [
                    {
                        "name": "Syemin Park"
                    },
                    {
                        "name": "Soobin Park"
                    },
                    {
                        "name": "Youn-kyung Lim"
                    }
                ],
                "author_detail": {
                    "name": "Youn-kyung Lim"
                },
                "author": "Youn-kyung Lim",
                "arxiv_comment": "50 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.05820v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.05820v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.05816v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.05816v1",
                "updated": "2025-07-08T09:36:14Z",
                "updated_parsed": [
                    2025,
                    7,
                    8,
                    9,
                    36,
                    14,
                    1,
                    189,
                    0
                ],
                "published": "2025-07-08T09:36:14Z",
                "published_parsed": [
                    2025,
                    7,
                    8,
                    9,
                    36,
                    14,
                    1,
                    189,
                    0
                ],
                "title": "Affective-ROPTester: Capability and Bias Analysis of LLMs in Predicting\n  Retinopathy of Prematurity",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Affective-ROPTester: Capability and Bias Analysis of LLMs in Predicting\n  Retinopathy of Prematurity"
                },
                "summary": "Despite the remarkable progress of large language models (LLMs) across\nvarious domains, their capacity to predict retinopathy of prematurity (ROP)\nrisk remains largely unexplored. To address this gap, we introduce a novel\nChinese benchmark dataset, termed CROP, comprising 993 admission records\nannotated with low, medium, and high-risk labels. To systematically examine the\npredictive capabilities and affective biases of LLMs in ROP risk\nstratification, we propose Affective-ROPTester, an automated evaluation\nframework incorporating three prompting strategies: Instruction-based,\nChain-of-Thought (CoT), and In-Context Learning (ICL). The Instruction scheme\nassesses LLMs' intrinsic knowledge and associated biases, whereas the CoT and\nICL schemes leverage external medical knowledge to enhance predictive accuracy.\nCrucially, we integrate emotional elements at the prompt level to investigate\nhow different affective framings influence the model's ability to predict ROP\nand its bias patterns. Empirical results derived from the CROP dataset yield\ntwo principal observations. First, LLMs demonstrate limited efficacy in ROP\nrisk prediction when operating solely on intrinsic knowledge, yet exhibit\nmarked performance gains when augmented with structured external inputs.\nSecond, affective biases are evident in the model outputs, with a consistent\ninclination toward overestimating medium- and high-risk cases. Third, compared\nto negative emotions, positive emotional framing contributes to mitigating\npredictive bias in model outputs. These findings highlight the critical role of\naffect-sensitive prompt engineering in enhancing diagnostic reliability and\nemphasize the utility of Affective-ROPTester as a framework for evaluating and\nmitigating affective bias in clinical language modeling systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Despite the remarkable progress of large language models (LLMs) across\nvarious domains, their capacity to predict retinopathy of prematurity (ROP)\nrisk remains largely unexplored. To address this gap, we introduce a novel\nChinese benchmark dataset, termed CROP, comprising 993 admission records\nannotated with low, medium, and high-risk labels. To systematically examine the\npredictive capabilities and affective biases of LLMs in ROP risk\nstratification, we propose Affective-ROPTester, an automated evaluation\nframework incorporating three prompting strategies: Instruction-based,\nChain-of-Thought (CoT), and In-Context Learning (ICL). The Instruction scheme\nassesses LLMs' intrinsic knowledge and associated biases, whereas the CoT and\nICL schemes leverage external medical knowledge to enhance predictive accuracy.\nCrucially, we integrate emotional elements at the prompt level to investigate\nhow different affective framings influence the model's ability to predict ROP\nand its bias patterns. Empirical results derived from the CROP dataset yield\ntwo principal observations. First, LLMs demonstrate limited efficacy in ROP\nrisk prediction when operating solely on intrinsic knowledge, yet exhibit\nmarked performance gains when augmented with structured external inputs.\nSecond, affective biases are evident in the model outputs, with a consistent\ninclination toward overestimating medium- and high-risk cases. Third, compared\nto negative emotions, positive emotional framing contributes to mitigating\npredictive bias in model outputs. These findings highlight the critical role of\naffect-sensitive prompt engineering in enhancing diagnostic reliability and\nemphasize the utility of Affective-ROPTester as a framework for evaluating and\nmitigating affective bias in clinical language modeling systems."
                },
                "authors": [
                    {
                        "name": "Shuai Zhao"
                    },
                    {
                        "name": "Yulin Zhang"
                    },
                    {
                        "name": "Luwei Xiao"
                    },
                    {
                        "name": "Xinyi Wu"
                    },
                    {
                        "name": "Yanhao Jia"
                    },
                    {
                        "name": "Zhongliang Guo"
                    },
                    {
                        "name": "Xiaobao Wu"
                    },
                    {
                        "name": "Cong-Duy Nguyen"
                    },
                    {
                        "name": "Guoming Zhang"
                    },
                    {
                        "name": "Anh Tuan Luu"
                    }
                ],
                "author_detail": {
                    "name": "Anh Tuan Luu"
                },
                "author": "Anh Tuan Luu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.05816v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.05816v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.05815v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.05815v1",
                "updated": "2025-07-08T09:36:12Z",
                "updated_parsed": [
                    2025,
                    7,
                    8,
                    9,
                    36,
                    12,
                    1,
                    189,
                    0
                ],
                "published": "2025-07-08T09:36:12Z",
                "published_parsed": [
                    2025,
                    7,
                    8,
                    9,
                    36,
                    12,
                    1,
                    189,
                    0
                ],
                "title": "Just Say Better or Worse: A Human-AI Collaborative Framework for Medical\n  Image Segmentation Without Manual Annotations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Just Say Better or Worse: A Human-AI Collaborative Framework for Medical\n  Image Segmentation Without Manual Annotations"
                },
                "summary": "Manual annotation of medical images is a labor-intensive and time-consuming\nprocess, posing a significant bottleneck in the development and deployment of\nrobust medical imaging AI systems. This paper introduces a novel Human-AI\ncollaborative framework for medical image segmentation that substantially\nreduces the annotation burden by eliminating the need for explicit manual\npixel-level labeling. The core innovation lies in a preference learning\nparadigm, where human experts provide minimal, intuitive feedback -- simply\nindicating whether an AI-generated segmentation is better or worse than a\nprevious version. The framework comprises four key components: (1) an adaptable\nfoundation model (FM) for feature extraction, (2) label propagation based on\nfeature similarity, (3) a clicking agent that learns from human better-or-worse\nfeedback to decide where to click and with which label, and (4) a multi-round\nsegmentation learning procedure that trains a state-of-the-art segmentation\nnetwork using pseudo-labels generated by the clicking agent and FM-based label\npropagation. Experiments on three public datasets demonstrate that the proposed\napproach achieves competitive segmentation performance using only binary\npreference feedback, without requiring experts to directly manually annotate\nthe images.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Manual annotation of medical images is a labor-intensive and time-consuming\nprocess, posing a significant bottleneck in the development and deployment of\nrobust medical imaging AI systems. This paper introduces a novel Human-AI\ncollaborative framework for medical image segmentation that substantially\nreduces the annotation burden by eliminating the need for explicit manual\npixel-level labeling. The core innovation lies in a preference learning\nparadigm, where human experts provide minimal, intuitive feedback -- simply\nindicating whether an AI-generated segmentation is better or worse than a\nprevious version. The framework comprises four key components: (1) an adaptable\nfoundation model (FM) for feature extraction, (2) label propagation based on\nfeature similarity, (3) a clicking agent that learns from human better-or-worse\nfeedback to decide where to click and with which label, and (4) a multi-round\nsegmentation learning procedure that trains a state-of-the-art segmentation\nnetwork using pseudo-labels generated by the clicking agent and FM-based label\npropagation. Experiments on three public datasets demonstrate that the proposed\napproach achieves competitive segmentation performance using only binary\npreference feedback, without requiring experts to directly manually annotate\nthe images."
                },
                "authors": [
                    {
                        "name": "Yizhe Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Yizhe Zhang"
                },
                "author": "Yizhe Zhang",
                "arxiv_comment": "10 pages, 4 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.05815v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.05815v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.IV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.IV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.15790v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.15790v2",
                "updated": "2025-07-08T09:31:28Z",
                "updated_parsed": [
                    2025,
                    7,
                    8,
                    9,
                    31,
                    28,
                    1,
                    189,
                    0
                ],
                "published": "2025-06-18T18:18:19Z",
                "published_parsed": [
                    2025,
                    6,
                    18,
                    18,
                    18,
                    19,
                    2,
                    169,
                    0
                ],
                "title": "ETrace:Event-Driven Vulnerability Detection in Smart Contracts via\n  LLM-Based Trace Analysis",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ETrace:Event-Driven Vulnerability Detection in Smart Contracts via\n  LLM-Based Trace Analysis"
                },
                "summary": "With the advance application of blockchain technology in various fields,\nensuring the security and stability of smart contracts has emerged as a\ncritical challenge. Current security analysis methodologies in vulnerability\ndetection can be categorized into static analysis and dynamic analysis\nmethods.However, these existing traditional vulnerability detection methods\npredominantly rely on analyzing original contract code, not all smart contracts\nprovide accessible code.We present ETrace, a novel event-driven vulnerability\ndetection framework for smart contracts, which uniquely identifies potential\nvulnerabilities through LLM-powered trace analysis without requiring source\ncode access. By extracting fine-grained event sequences from transaction logs,\nthe framework leverages Large Language Models (LLMs) as adaptive semantic\ninterpreters to reconstruct event analysis through chain-of-thought reasoning.\nETrace implements pattern-matching to establish causal links between\ntransaction behavior patterns and known attack behaviors. Furthermore, we\nvalidate the effectiveness of ETrace through preliminary experimental results.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the advance application of blockchain technology in various fields,\nensuring the security and stability of smart contracts has emerged as a\ncritical challenge. Current security analysis methodologies in vulnerability\ndetection can be categorized into static analysis and dynamic analysis\nmethods.However, these existing traditional vulnerability detection methods\npredominantly rely on analyzing original contract code, not all smart contracts\nprovide accessible code.We present ETrace, a novel event-driven vulnerability\ndetection framework for smart contracts, which uniquely identifies potential\nvulnerabilities through LLM-powered trace analysis without requiring source\ncode access. By extracting fine-grained event sequences from transaction logs,\nthe framework leverages Large Language Models (LLMs) as adaptive semantic\ninterpreters to reconstruct event analysis through chain-of-thought reasoning.\nETrace implements pattern-matching to establish causal links between\ntransaction behavior patterns and known attack behaviors. Furthermore, we\nvalidate the effectiveness of ETrace through preliminary experimental results."
                },
                "authors": [
                    {
                        "name": "Chenyang Peng"
                    },
                    {
                        "name": "Haijun Wang"
                    },
                    {
                        "name": "Yin Wu"
                    },
                    {
                        "name": "Hao Wu"
                    },
                    {
                        "name": "Ming Fan"
                    },
                    {
                        "name": "Yitao Zhao"
                    },
                    {
                        "name": "Ting Liu"
                    }
                ],
                "author_detail": {
                    "name": "Ting Liu"
                },
                "author": "Ting Liu",
                "arxiv_comment": "4 pages, 1 figure. Submitted to the 16th Asia-Pacific Symposium on\n  Internetware (Internetware 2025)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.15790v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.15790v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "68N01",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "D.2.0",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.01348v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.01348v2",
                "updated": "2025-07-08T09:21:24Z",
                "updated_parsed": [
                    2025,
                    7,
                    8,
                    9,
                    21,
                    24,
                    1,
                    189,
                    0
                ],
                "published": "2025-07-02T04:30:23Z",
                "published_parsed": [
                    2025,
                    7,
                    2,
                    4,
                    30,
                    23,
                    2,
                    183,
                    0
                ],
                "title": "SpeechAccentLLM: A Unified Framework for Foreign Accent Conversion and\n  Text to Speech",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SpeechAccentLLM: A Unified Framework for Foreign Accent Conversion and\n  Text to Speech"
                },
                "summary": "Foreign accent conversion (FAC) in speech processing remains a challenging\ntask. Building on the remarkable success of large language models (LLMs) in\nText-to-Speech (TTS) tasks, this study investigates the adaptation of LLM-based\ntechniques for FAC, which we term SpeechAccentLLM. At the core of this\nframework, we introduce SpeechCodeVAE, the first model to integrate\nconnectionist temporal classification (CTC) directly into codebook\ndiscretization for speech content tokenization. This novel architecture\ngenerates tokens with a unique \"locality\" property, as validated by experiments\ndemonstrating optimal trade-offs among content faithfulness, temporal\ncoherence, and structural recoverability. Then, to address data scarcity for\nthe FAC module, we adopted a multitask learning strategy that jointly trains\nthe FAC and TTS modules. Beyond mitigating data limitations, this approach\nyielded accelerated convergence and superior speech quality compared to\nstandalone FAC training. Moreover, leveraging the salient properties of our\ndiscrete speech representations, we introduce SpeechRestorer, a postprocessing\narchitecture designed to refine LLM-generated outputs. This module effectively\nmitigates stochastic errors prevalent in LLM inference pipelines while\nenhancing prosodic continuity, as validated by ablation experiments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Foreign accent conversion (FAC) in speech processing remains a challenging\ntask. Building on the remarkable success of large language models (LLMs) in\nText-to-Speech (TTS) tasks, this study investigates the adaptation of LLM-based\ntechniques for FAC, which we term SpeechAccentLLM. At the core of this\nframework, we introduce SpeechCodeVAE, the first model to integrate\nconnectionist temporal classification (CTC) directly into codebook\ndiscretization for speech content tokenization. This novel architecture\ngenerates tokens with a unique \"locality\" property, as validated by experiments\ndemonstrating optimal trade-offs among content faithfulness, temporal\ncoherence, and structural recoverability. Then, to address data scarcity for\nthe FAC module, we adopted a multitask learning strategy that jointly trains\nthe FAC and TTS modules. Beyond mitigating data limitations, this approach\nyielded accelerated convergence and superior speech quality compared to\nstandalone FAC training. Moreover, leveraging the salient properties of our\ndiscrete speech representations, we introduce SpeechRestorer, a postprocessing\narchitecture designed to refine LLM-generated outputs. This module effectively\nmitigates stochastic errors prevalent in LLM inference pipelines while\nenhancing prosodic continuity, as validated by ablation experiments."
                },
                "authors": [
                    {
                        "name": "Zhuangfei Cheng"
                    },
                    {
                        "name": "Guangyan Zhang"
                    },
                    {
                        "name": "Zehai Tu"
                    },
                    {
                        "name": "Yangyang Song"
                    },
                    {
                        "name": "Shuiyang Mao"
                    },
                    {
                        "name": "Xiaoqi Jiao"
                    },
                    {
                        "name": "Jingyu Li"
                    },
                    {
                        "name": "Yiwen Guo"
                    },
                    {
                        "name": "Jiasong Wu"
                    }
                ],
                "author_detail": {
                    "name": "Jiasong Wu"
                },
                "author": "Jiasong Wu",
                "arxiv_comment": "10 pages, includes references, 4 figures, 4 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.01348v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.01348v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.AS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.7",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.08938v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.08938v2",
                "updated": "2025-07-08T08:59:27Z",
                "updated_parsed": [
                    2025,
                    7,
                    8,
                    8,
                    59,
                    27,
                    1,
                    189,
                    0
                ],
                "published": "2025-06-10T16:02:54Z",
                "published_parsed": [
                    2025,
                    6,
                    10,
                    16,
                    2,
                    54,
                    1,
                    161,
                    0
                ],
                "title": "FaithfulRAG: Fact-Level Conflict Modeling for Context-Faithful\n  Retrieval-Augmented Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FaithfulRAG: Fact-Level Conflict Modeling for Context-Faithful\n  Retrieval-Augmented Generation"
                },
                "summary": "Large language models (LLMs) augmented with retrieval systems have\ndemonstrated significant potential in handling knowledge-intensive tasks.\nHowever, these models often struggle with unfaithfulness issues, generating\noutputs that either ignore the retrieved context or inconsistently blend it\nwith the LLM`s parametric knowledge. This issue is particularly severe in cases\nof knowledge conflict, where the retrieved context conflicts with the model`s\nparametric knowledge. While existing faithful RAG approaches enforce strict\ncontext adherence through well-designed prompts or modified decoding\nstrategies, our analysis reveals a critical limitation: they achieve\nfaithfulness by forcibly suppressing the model`s parametric knowledge, which\nundermines the model`s internal knowledge structure and increases the risk of\nmisinterpreting the context. To this end, this paper proposes FaithfulRAG, a\nnovel framework that resolves knowledge conflicts by explicitly modeling\ndiscrepancies between the model`s parametric knowledge and retrieved context.\nSpecifically, FaithfulRAG identifies conflicting knowledge at the fact level\nand designs a self-thinking process, allowing LLMs to reason about and\nintegrate conflicting facts before generating responses. Extensive experiments\ndemonstrate that our method outperforms state-of-the-art methods. The code is\navailable at https://github.com/DeepLearnXMU/Faithful-RAG",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) augmented with retrieval systems have\ndemonstrated significant potential in handling knowledge-intensive tasks.\nHowever, these models often struggle with unfaithfulness issues, generating\noutputs that either ignore the retrieved context or inconsistently blend it\nwith the LLM`s parametric knowledge. This issue is particularly severe in cases\nof knowledge conflict, where the retrieved context conflicts with the model`s\nparametric knowledge. While existing faithful RAG approaches enforce strict\ncontext adherence through well-designed prompts or modified decoding\nstrategies, our analysis reveals a critical limitation: they achieve\nfaithfulness by forcibly suppressing the model`s parametric knowledge, which\nundermines the model`s internal knowledge structure and increases the risk of\nmisinterpreting the context. To this end, this paper proposes FaithfulRAG, a\nnovel framework that resolves knowledge conflicts by explicitly modeling\ndiscrepancies between the model`s parametric knowledge and retrieved context.\nSpecifically, FaithfulRAG identifies conflicting knowledge at the fact level\nand designs a self-thinking process, allowing LLMs to reason about and\nintegrate conflicting facts before generating responses. Extensive experiments\ndemonstrate that our method outperforms state-of-the-art methods. The code is\navailable at https://github.com/DeepLearnXMU/Faithful-RAG"
                },
                "authors": [
                    {
                        "name": "Qinggang Zhang"
                    },
                    {
                        "name": "Zhishang Xiang"
                    },
                    {
                        "name": "Yilin Xiao"
                    },
                    {
                        "name": "Le Wang"
                    },
                    {
                        "name": "Junhui Li"
                    },
                    {
                        "name": "Xinrun Wang"
                    },
                    {
                        "name": "Jinsong Su"
                    }
                ],
                "author_detail": {
                    "name": "Jinsong Su"
                },
                "author": "Jinsong Su",
                "arxiv_comment": "Accepted to ACL 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.08938v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.08938v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.05795v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.05795v1",
                "updated": "2025-07-08T08:57:13Z",
                "updated_parsed": [
                    2025,
                    7,
                    8,
                    8,
                    57,
                    13,
                    1,
                    189,
                    0
                ],
                "published": "2025-07-08T08:57:13Z",
                "published_parsed": [
                    2025,
                    7,
                    8,
                    8,
                    57,
                    13,
                    1,
                    189,
                    0
                ],
                "title": "Creating a customisable freely-accessible Socratic AI physics tutor",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Creating a customisable freely-accessible Socratic AI physics tutor"
                },
                "summary": "This paper explores role engineering as an effective paradigm for customizing\nLarge Language Models (LLMs) into specialized AI tutors for physics education.\nWe demonstrate this methodology by designing a Socratic physics problem-solving\ntutor using Google's Gemini Gems feature, defining its pedagogical behavior\nthrough a detailed 'script' that specifies its role and persona. We present two\nillustrative use cases: the first demonstrates the Gem's multimodal ability to\nanalyze a student's hand-drawn force diagram and apply notational rules from a\n'Knowledge' file; the second showcases its capacity to guide conceptual\nreasoning in electromagnetism using its pre-trained knowledge without using\nspecific documents provided by the instructor. Our findings show that the\n'role-engineered' Gem successfully facilitates a Socratic dialogue, in stark\ncontrast to a standard Gemini model, which tends to immediately provide direct\nsolutions. We conclude that role engineering is a pivotal and accessible method\nfor educators to transform a general-purpose 'solution provider' into a\nreliable pedagogical tutor capable of engaging students in an active reflection\nprocess. This approach offers a powerful tool for both instructors and\nstudents, while also highlighting the importance of addressing the technology's\ninherent limitations, such as the potential for occasional inaccuracies.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper explores role engineering as an effective paradigm for customizing\nLarge Language Models (LLMs) into specialized AI tutors for physics education.\nWe demonstrate this methodology by designing a Socratic physics problem-solving\ntutor using Google's Gemini Gems feature, defining its pedagogical behavior\nthrough a detailed 'script' that specifies its role and persona. We present two\nillustrative use cases: the first demonstrates the Gem's multimodal ability to\nanalyze a student's hand-drawn force diagram and apply notational rules from a\n'Knowledge' file; the second showcases its capacity to guide conceptual\nreasoning in electromagnetism using its pre-trained knowledge without using\nspecific documents provided by the instructor. Our findings show that the\n'role-engineered' Gem successfully facilitates a Socratic dialogue, in stark\ncontrast to a standard Gemini model, which tends to immediately provide direct\nsolutions. We conclude that role engineering is a pivotal and accessible method\nfor educators to transform a general-purpose 'solution provider' into a\nreliable pedagogical tutor capable of engaging students in an active reflection\nprocess. This approach offers a powerful tool for both instructors and\nstudents, while also highlighting the importance of addressing the technology's\ninherent limitations, such as the potential for occasional inaccuracies."
                },
                "authors": [
                    {
                        "name": "Eugenio Tufino"
                    },
                    {
                        "name": "Bor Gregorcic"
                    }
                ],
                "author_detail": {
                    "name": "Bor Gregorcic"
                },
                "author": "Bor Gregorcic",
                "arxiv_comment": "7 pages, 3 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.05795v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.05795v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.ed-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.ed-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.05788v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.05788v1",
                "updated": "2025-07-08T08:50:47Z",
                "updated_parsed": [
                    2025,
                    7,
                    8,
                    8,
                    50,
                    47,
                    1,
                    189,
                    0
                ],
                "published": "2025-07-08T08:50:47Z",
                "published_parsed": [
                    2025,
                    7,
                    8,
                    8,
                    50,
                    47,
                    1,
                    189,
                    0
                ],
                "title": "Flippi: End To End GenAI Assistant for E-Commerce",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Flippi: End To End GenAI Assistant for E-Commerce"
                },
                "summary": "The emergence of conversational assistants has fundamentally reshaped user\ninteractions with digital platforms. This paper introduces Flippi-a\ncutting-edge, end-to-end conversational assistant powered by large language\nmodels (LLMs) and tailored for the e-commerce sector. Flippi addresses the\nchallenges posed by the vast and often overwhelming product landscape, enabling\ncustomers to discover products more efficiently through natural language\ndialogue. By accommodating both objective and subjective user requirements,\nFlippi delivers a personalized shopping experience that surpasses traditional\nsearch methods. This paper details how Flippi interprets customer queries to\nprovide precise product information, leveraging advanced NLP techniques such as\nQuery Reformulation, Intent Detection, Retrieval-Augmented Generation (RAG),\nNamed Entity Recognition (NER), and Context Reduction. Flippi's unique\ncapability to identify and present the most attractive offers on an e-commerce\nsite is also explored, demonstrating how it empowers users to make\ncost-effective decisions. Additionally, the paper discusses Flippi's\ncomparative analysis features, which help users make informed choices by\ncontrasting product features, prices, and other relevant attributes. The\nsystem's robust architecture is outlined, emphasizing its adaptability for\nintegration across various e-commerce platforms and the technological choices\nunderpinning its performance and accuracy. Finally, a comprehensive evaluation\nframework is presented, covering performance metrics, user satisfaction, and\nthe impact on customer engagement and conversion rates. By bridging the\nconvenience of online shopping with the personalized assistance traditionally\nfound in physical stores, Flippi sets a new standard for customer satisfaction\nand engagement in the digital marketplace.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The emergence of conversational assistants has fundamentally reshaped user\ninteractions with digital platforms. This paper introduces Flippi-a\ncutting-edge, end-to-end conversational assistant powered by large language\nmodels (LLMs) and tailored for the e-commerce sector. Flippi addresses the\nchallenges posed by the vast and often overwhelming product landscape, enabling\ncustomers to discover products more efficiently through natural language\ndialogue. By accommodating both objective and subjective user requirements,\nFlippi delivers a personalized shopping experience that surpasses traditional\nsearch methods. This paper details how Flippi interprets customer queries to\nprovide precise product information, leveraging advanced NLP techniques such as\nQuery Reformulation, Intent Detection, Retrieval-Augmented Generation (RAG),\nNamed Entity Recognition (NER), and Context Reduction. Flippi's unique\ncapability to identify and present the most attractive offers on an e-commerce\nsite is also explored, demonstrating how it empowers users to make\ncost-effective decisions. Additionally, the paper discusses Flippi's\ncomparative analysis features, which help users make informed choices by\ncontrasting product features, prices, and other relevant attributes. The\nsystem's robust architecture is outlined, emphasizing its adaptability for\nintegration across various e-commerce platforms and the technological choices\nunderpinning its performance and accuracy. Finally, a comprehensive evaluation\nframework is presented, covering performance metrics, user satisfaction, and\nthe impact on customer engagement and conversion rates. By bridging the\nconvenience of online shopping with the personalized assistance traditionally\nfound in physical stores, Flippi sets a new standard for customer satisfaction\nand engagement in the digital marketplace."
                },
                "authors": [
                    {
                        "name": "Anand A. Rajasekar"
                    },
                    {
                        "name": "Praveen Tangarajan"
                    },
                    {
                        "name": "Anjali Nainani"
                    },
                    {
                        "name": "Amogh Batwal"
                    },
                    {
                        "name": "Vinay Rao Dandin"
                    },
                    {
                        "name": "Anusua Trivedi"
                    },
                    {
                        "name": "Ozan Ersoy"
                    }
                ],
                "author_detail": {
                    "name": "Ozan Ersoy"
                },
                "author": "Ozan Ersoy",
                "arxiv_comment": "10 pages, 2 figures, 7 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.05788v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.05788v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.7; H.3.3",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.05785v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.05785v1",
                "updated": "2025-07-08T08:43:29Z",
                "updated_parsed": [
                    2025,
                    7,
                    8,
                    8,
                    43,
                    29,
                    1,
                    189,
                    0
                ],
                "published": "2025-07-08T08:43:29Z",
                "published_parsed": [
                    2025,
                    7,
                    8,
                    8,
                    43,
                    29,
                    1,
                    189,
                    0
                ],
                "title": "Robust Bandwidth Estimation for Real-Time Communication with Offline\n  Reinforcement Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Robust Bandwidth Estimation for Real-Time Communication with Offline\n  Reinforcement Learning"
                },
                "summary": "Accurate bandwidth estimation (BWE) is critical for real-time communication\n(RTC) systems. Traditional heuristic approaches offer limited adaptability\nunder dynamic networks, while online reinforcement learning (RL) suffers from\nhigh exploration costs and potential service disruptions. Offline RL, which\nleverages high-quality data collected from real-world environments, offers a\npromising alternative. However, challenges such as out-of-distribution (OOD)\nactions, policy extraction from behaviorally diverse datasets, and reliable\ndeployment in production systems remain unsolved. We propose RBWE, a robust\nbandwidth estimation framework based on offline RL that integrates Q-ensemble\n(an ensemble of Q-functions) with a Gaussian mixture policy to mitigate OOD\nrisks and enhance policy learning. A fallback mechanism ensures deployment\nstability by switching to heuristic methods under high uncertainty.\nExperimental results show that RBWE reduces overestimation errors by 18% and\nimproves the 10th percentile Quality of Experience (QoE) by 18.6%,\ndemonstrating its practical effectiveness in real-world RTC applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Accurate bandwidth estimation (BWE) is critical for real-time communication\n(RTC) systems. Traditional heuristic approaches offer limited adaptability\nunder dynamic networks, while online reinforcement learning (RL) suffers from\nhigh exploration costs and potential service disruptions. Offline RL, which\nleverages high-quality data collected from real-world environments, offers a\npromising alternative. However, challenges such as out-of-distribution (OOD)\nactions, policy extraction from behaviorally diverse datasets, and reliable\ndeployment in production systems remain unsolved. We propose RBWE, a robust\nbandwidth estimation framework based on offline RL that integrates Q-ensemble\n(an ensemble of Q-functions) with a Gaussian mixture policy to mitigate OOD\nrisks and enhance policy learning. A fallback mechanism ensures deployment\nstability by switching to heuristic methods under high uncertainty.\nExperimental results show that RBWE reduces overestimation errors by 18% and\nimproves the 10th percentile Quality of Experience (QoE) by 18.6%,\ndemonstrating its practical effectiveness in real-world RTC applications."
                },
                "authors": [
                    {
                        "name": "Jian Kai"
                    },
                    {
                        "name": "Tianwei Zhang"
                    },
                    {
                        "name": "Zihan Ling"
                    },
                    {
                        "name": "Yang Cao"
                    },
                    {
                        "name": "Can Shen"
                    }
                ],
                "author_detail": {
                    "name": "Can Shen"
                },
                "author": "Can Shen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.05785v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.05785v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.05784v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.05784v1",
                "updated": "2025-07-08T08:43:08Z",
                "updated_parsed": [
                    2025,
                    7,
                    8,
                    8,
                    43,
                    8,
                    1,
                    189,
                    0
                ],
                "published": "2025-07-08T08:43:08Z",
                "published_parsed": [
                    2025,
                    7,
                    8,
                    8,
                    43,
                    8,
                    1,
                    189,
                    0
                ],
                "title": "Does Movable Antenna Present A Dual-edged Nature? From the Perspective\n  of Physical Layer Security: A Joint Design of Fixed-position Antenna and\n  Movable Antenna",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Does Movable Antenna Present A Dual-edged Nature? From the Perspective\n  of Physical Layer Security: A Joint Design of Fixed-position Antenna and\n  Movable Antenna"
                },
                "summary": "In conventional artificial noise (AN)-aided physical-layer security systems,\nfixed-position antenna (FPA) arrays exhibit inherent vulnerability to coverage\ngaps due to their static spatial configuration. Adversarial eavesdroppers can\nstrategically exploit their mobility to infiltrate these spatial nulls of AN\nradiation patterns, thereby evading interference suppression and successfully\nintercepting the confidential communication. To overcome this limitation, in\nthis paper, we investigate a hybrid antenna deployment framework integrating\nFPA arrays and movable antenna (MA) arrays (denoted by FMA co-design) to\naddress the security performance in dynamic wireless environments, based on the\nfact that MA arrays enable channel reconfiguration through localized antenna\nrepositioning, achieving more higher spatial degree of freedom (DoF). Enabled\nby FMA co-design framework, FPA arrays ensure baseline connectivity for\nlegitimate links while MA arrays function as dynamic security enhancers,\nreplacing conventional static AN generation. Furthermore, we formulate a\nnon-convex optimization problem of the secrecy rate maximization through\njointly optimizing MA positioning, FPA beamforming, and MA beamforming under\npractical constraints. the solution employs a dual-algorithm approach: Nesterov\nmomentum-based projected gradient ascent (NMPGA) accelerates convergence in\ncontinuous position optimization, while alternating optimization (AO) handles\ncoupled beamforming design. Experimental evaluations demonstrate that the\nproposed FMA co-design framework achieves significant secrecy performance gains\nover individual optimization benchmarks, yielding 42.34% and 9.12% improvements\nin secrecy rate compared to isolated FPA for AN generation and MA for\nconfidential information baselines, respectively.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In conventional artificial noise (AN)-aided physical-layer security systems,\nfixed-position antenna (FPA) arrays exhibit inherent vulnerability to coverage\ngaps due to their static spatial configuration. Adversarial eavesdroppers can\nstrategically exploit their mobility to infiltrate these spatial nulls of AN\nradiation patterns, thereby evading interference suppression and successfully\nintercepting the confidential communication. To overcome this limitation, in\nthis paper, we investigate a hybrid antenna deployment framework integrating\nFPA arrays and movable antenna (MA) arrays (denoted by FMA co-design) to\naddress the security performance in dynamic wireless environments, based on the\nfact that MA arrays enable channel reconfiguration through localized antenna\nrepositioning, achieving more higher spatial degree of freedom (DoF). Enabled\nby FMA co-design framework, FPA arrays ensure baseline connectivity for\nlegitimate links while MA arrays function as dynamic security enhancers,\nreplacing conventional static AN generation. Furthermore, we formulate a\nnon-convex optimization problem of the secrecy rate maximization through\njointly optimizing MA positioning, FPA beamforming, and MA beamforming under\npractical constraints. the solution employs a dual-algorithm approach: Nesterov\nmomentum-based projected gradient ascent (NMPGA) accelerates convergence in\ncontinuous position optimization, while alternating optimization (AO) handles\ncoupled beamforming design. Experimental evaluations demonstrate that the\nproposed FMA co-design framework achieves significant secrecy performance gains\nover individual optimization benchmarks, yielding 42.34% and 9.12% improvements\nin secrecy rate compared to isolated FPA for AN generation and MA for\nconfidential information baselines, respectively."
                },
                "authors": [
                    {
                        "name": "Kan Yu"
                    },
                    {
                        "name": "Wenxu Wang"
                    },
                    {
                        "name": "Xiaowu Liu"
                    },
                    {
                        "name": "Yujia Zhao"
                    },
                    {
                        "name": "Qixun Zhang"
                    },
                    {
                        "name": "Zhiyong Feng"
                    },
                    {
                        "name": "Dong Li"
                    }
                ],
                "author_detail": {
                    "name": "Dong Li"
                },
                "author": "Dong Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.05784v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.05784v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IT",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.14569v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.14569v2",
                "updated": "2025-07-08T08:34:51Z",
                "updated_parsed": [
                    2025,
                    7,
                    8,
                    8,
                    34,
                    51,
                    1,
                    189,
                    0
                ],
                "published": "2025-04-20T11:00:29Z",
                "published_parsed": [
                    2025,
                    4,
                    20,
                    11,
                    0,
                    29,
                    6,
                    110,
                    0
                ],
                "title": "NoWag: A Unified Framework for Shape Preserving Compression of Large\n  Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "NoWag: A Unified Framework for Shape Preserving Compression of Large\n  Language Models"
                },
                "summary": "Large language models (LLMs) exhibit remarkable performance across various\nnatural language processing tasks but suffer from immense computational and\nmemory demands, limiting their deployment in resource-constrained environments.\nTo address this challenge, we propose NoWag: (Normalized Weight and Activation\nGuided Compression), a unified framework for zero-shot shape preserving\ncompression algorithms. We compressed Llama-2 7B/13B/70B and Llama-3 8/70BB\nmodels, using two popular forms of shape-preserving compression, vector\nquantization NoWag-VQ (NoWag for Vector Quantization), and\nunstructured/semi-structured pruning NoWag-P (NoWag for Pruning). We found that\nNoWag-VQ significantly outperforms state-of-the-art zero shot VQ, and that\nNoWag-P performs competitively against state-of-the-art methods. These results\nsuggest commonalities between these compression paradigms that could inspire\nfuture work. Our code is available at https://github.com/LawrenceRLiu/NoWag",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) exhibit remarkable performance across various\nnatural language processing tasks but suffer from immense computational and\nmemory demands, limiting their deployment in resource-constrained environments.\nTo address this challenge, we propose NoWag: (Normalized Weight and Activation\nGuided Compression), a unified framework for zero-shot shape preserving\ncompression algorithms. We compressed Llama-2 7B/13B/70B and Llama-3 8/70BB\nmodels, using two popular forms of shape-preserving compression, vector\nquantization NoWag-VQ (NoWag for Vector Quantization), and\nunstructured/semi-structured pruning NoWag-P (NoWag for Pruning). We found that\nNoWag-VQ significantly outperforms state-of-the-art zero shot VQ, and that\nNoWag-P performs competitively against state-of-the-art methods. These results\nsuggest commonalities between these compression paradigms that could inspire\nfuture work. Our code is available at https://github.com/LawrenceRLiu/NoWag"
                },
                "authors": [
                    {
                        "name": "Lawrence Liu"
                    },
                    {
                        "name": "Inesh Chakrabarti"
                    },
                    {
                        "name": "Yixiao Li"
                    },
                    {
                        "name": "Mengdi Wang"
                    },
                    {
                        "name": "Tuo Zhao"
                    },
                    {
                        "name": "Lin F. Yang"
                    }
                ],
                "author_detail": {
                    "name": "Lin F. Yang"
                },
                "author": "Lin F. Yang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.14569v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.14569v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.05755v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.05755v1",
                "updated": "2025-07-08T07:58:52Z",
                "updated_parsed": [
                    2025,
                    7,
                    8,
                    7,
                    58,
                    52,
                    1,
                    189,
                    0
                ],
                "published": "2025-07-08T07:58:52Z",
                "published_parsed": [
                    2025,
                    7,
                    8,
                    7,
                    58,
                    52,
                    1,
                    189,
                    0
                ],
                "title": "An autonomous agent for auditing and improving the reliability of\n  clinical AI models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "An autonomous agent for auditing and improving the reliability of\n  clinical AI models"
                },
                "summary": "The deployment of AI models in clinical practice faces a critical challenge:\nmodels achieving expert-level performance on benchmarks can fail\ncatastrophically when confronted with real-world variations in medical imaging.\nMinor shifts in scanner hardware, lighting or demographics can erode accuracy,\nbut currently reliability auditing to identify such catastrophic failure cases\nbefore deployment is a bespoke and time-consuming process. Practitioners lack\naccessible and interpretable tools to expose and repair hidden failure modes.\nHere we introduce ModelAuditor, a self-reflective agent that converses with\nusers, selects task-specific metrics, and simulates context-dependent,\nclinically relevant distribution shifts. ModelAuditor then generates\ninterpretable reports explaining how much performance likely degrades during\ndeployment, discussing specific likely failure modes and identifying root\ncauses and mitigation strategies. Our comprehensive evaluation across three\nreal-world clinical scenarios - inter-institutional variation in\nhistopathology, demographic shifts in dermatology, and equipment heterogeneity\nin chest radiography - demonstrates that ModelAuditor is able correctly\nidentify context-specific failure modes of state-of-the-art models such as the\nestablished SIIM-ISIC melanoma classifier. Its targeted recommendations recover\n15-25% of performance lost under real-world distribution shift, substantially\noutperforming both baseline models and state-of-the-art augmentation methods.\nThese improvements are achieved through a multi-agent architecture and execute\non consumer hardware in under 10 minutes, costing less than US$0.50 per audit.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The deployment of AI models in clinical practice faces a critical challenge:\nmodels achieving expert-level performance on benchmarks can fail\ncatastrophically when confronted with real-world variations in medical imaging.\nMinor shifts in scanner hardware, lighting or demographics can erode accuracy,\nbut currently reliability auditing to identify such catastrophic failure cases\nbefore deployment is a bespoke and time-consuming process. Practitioners lack\naccessible and interpretable tools to expose and repair hidden failure modes.\nHere we introduce ModelAuditor, a self-reflective agent that converses with\nusers, selects task-specific metrics, and simulates context-dependent,\nclinically relevant distribution shifts. ModelAuditor then generates\ninterpretable reports explaining how much performance likely degrades during\ndeployment, discussing specific likely failure modes and identifying root\ncauses and mitigation strategies. Our comprehensive evaluation across three\nreal-world clinical scenarios - inter-institutional variation in\nhistopathology, demographic shifts in dermatology, and equipment heterogeneity\nin chest radiography - demonstrates that ModelAuditor is able correctly\nidentify context-specific failure modes of state-of-the-art models such as the\nestablished SIIM-ISIC melanoma classifier. Its targeted recommendations recover\n15-25% of performance lost under real-world distribution shift, substantially\noutperforming both baseline models and state-of-the-art augmentation methods.\nThese improvements are achieved through a multi-agent architecture and execute\non consumer hardware in under 10 minutes, costing less than US$0.50 per audit."
                },
                "authors": [
                    {
                        "name": "Lukas Kuhn"
                    },
                    {
                        "name": "Florian Buettner"
                    }
                ],
                "author_detail": {
                    "name": "Florian Buettner"
                },
                "author": "Florian Buettner",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.05755v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.05755v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.05754v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.05754v1",
                "updated": "2025-07-08T07:58:29Z",
                "updated_parsed": [
                    2025,
                    7,
                    8,
                    7,
                    58,
                    29,
                    1,
                    189,
                    0
                ],
                "published": "2025-07-08T07:58:29Z",
                "published_parsed": [
                    2025,
                    7,
                    8,
                    7,
                    58,
                    29,
                    1,
                    189,
                    0
                ],
                "title": "LeAD: The LLM Enhanced Planning System Converged with End-to-end\n  Autonomous Driving",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LeAD: The LLM Enhanced Planning System Converged with End-to-end\n  Autonomous Driving"
                },
                "summary": "A principal barrier to large-scale deployment of urban autonomous driving\nsystems lies in the prevalence of complex scenarios and edge cases. Existing\nsystems fail to effectively interpret semantic information within traffic\ncontexts and discern intentions of other participants, consequently generating\ndecisions misaligned with skilled drivers' reasoning patterns. We present LeAD,\na dual-rate autonomous driving architecture integrating imitation\nlearning-based end-to-end (E2E) frameworks with large language model (LLM)\naugmentation. The high-frequency E2E subsystem maintains real-time\nperception-planning-control cycles, while the low-frequency LLM module enhances\nscenario comprehension through multi-modal perception fusion with HD maps and\nderives optimal decisions via chain-of-thought (CoT) reasoning when baseline\nplanners encounter capability limitations. Our experimental evaluation in the\nCARLA Simulator demonstrates LeAD's superior handling of unconventional\nscenarios, achieving 71 points on Leaderboard V1 benchmark, with a route\ncompletion of 93%.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A principal barrier to large-scale deployment of urban autonomous driving\nsystems lies in the prevalence of complex scenarios and edge cases. Existing\nsystems fail to effectively interpret semantic information within traffic\ncontexts and discern intentions of other participants, consequently generating\ndecisions misaligned with skilled drivers' reasoning patterns. We present LeAD,\na dual-rate autonomous driving architecture integrating imitation\nlearning-based end-to-end (E2E) frameworks with large language model (LLM)\naugmentation. The high-frequency E2E subsystem maintains real-time\nperception-planning-control cycles, while the low-frequency LLM module enhances\nscenario comprehension through multi-modal perception fusion with HD maps and\nderives optimal decisions via chain-of-thought (CoT) reasoning when baseline\nplanners encounter capability limitations. Our experimental evaluation in the\nCARLA Simulator demonstrates LeAD's superior handling of unconventional\nscenarios, achieving 71 points on Leaderboard V1 benchmark, with a route\ncompletion of 93%."
                },
                "authors": [
                    {
                        "name": "Yuhang Zhang"
                    },
                    {
                        "name": "Jiaqi Liu"
                    },
                    {
                        "name": "Chengkai Xu"
                    },
                    {
                        "name": "Peng Hang"
                    },
                    {
                        "name": "Jian Sun"
                    }
                ],
                "author_detail": {
                    "name": "Jian Sun"
                },
                "author": "Jian Sun",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.05754v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.05754v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.08780v6",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.08780v6",
                "updated": "2025-07-08T07:52:51Z",
                "updated_parsed": [
                    2025,
                    7,
                    8,
                    7,
                    52,
                    51,
                    1,
                    189,
                    0
                ],
                "published": "2024-08-16T14:49:04Z",
                "published_parsed": [
                    2024,
                    8,
                    16,
                    14,
                    49,
                    4,
                    4,
                    229,
                    0
                ],
                "title": "Large Language Models Might Not Care What You Are Saying: Prompt Format\n  Beats Descriptions",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models Might Not Care What You Are Saying: Prompt Format\n  Beats Descriptions"
                },
                "summary": "With the help of in-context learning (ICL), large language models (LLMs) have\nachieved impressive performance across various tasks. However, the function of\ndescriptive instructions during ICL remains under-explored. In this work, we\npropose an ensemble prompt framework to describe the selection criteria of\nmultiple in-context examples, and preliminary experiments on machine\ntranslation (MT) across six translation directions confirm that this framework\nboosts ICL performance. But to our surprise, LLMs might not care what the\ndescriptions actually say, and the performance gain is primarily caused by the\nensemble format, since it could lead to improvement even with random\ndescriptive nouns. We further apply this new ensemble framework on a range of\ncommonsense, math, logical reasoning and hallucination tasks with three LLMs\nand achieve promising results, suggesting again that designing a proper prompt\nformat would be much more effective and efficient than paying effort into\nspecific descriptions. Our code is available at\nhttps://github.com/JamyDon/Format-Beats-Descriptions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the help of in-context learning (ICL), large language models (LLMs) have\nachieved impressive performance across various tasks. However, the function of\ndescriptive instructions during ICL remains under-explored. In this work, we\npropose an ensemble prompt framework to describe the selection criteria of\nmultiple in-context examples, and preliminary experiments on machine\ntranslation (MT) across six translation directions confirm that this framework\nboosts ICL performance. But to our surprise, LLMs might not care what the\ndescriptions actually say, and the performance gain is primarily caused by the\nensemble format, since it could lead to improvement even with random\ndescriptive nouns. We further apply this new ensemble framework on a range of\ncommonsense, math, logical reasoning and hallucination tasks with three LLMs\nand achieve promising results, suggesting again that designing a proper prompt\nformat would be much more effective and efficient than paying effort into\nspecific descriptions. Our code is available at\nhttps://github.com/JamyDon/Format-Beats-Descriptions."
                },
                "authors": [
                    {
                        "name": "Chenming Tang"
                    },
                    {
                        "name": "Zhixiang Wang"
                    },
                    {
                        "name": "Hao Sun"
                    },
                    {
                        "name": "Yunfang Wu"
                    }
                ],
                "author_detail": {
                    "name": "Yunfang Wu"
                },
                "author": "Yunfang Wu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.08780v6",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.08780v6",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.05750v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.05750v1",
                "updated": "2025-07-08T07:52:12Z",
                "updated_parsed": [
                    2025,
                    7,
                    8,
                    7,
                    52,
                    12,
                    1,
                    189,
                    0
                ],
                "published": "2025-07-08T07:52:12Z",
                "published_parsed": [
                    2025,
                    7,
                    8,
                    7,
                    52,
                    12,
                    1,
                    189,
                    0
                ],
                "title": "DocTalk: Scalable Graph-based Dialogue Synthesis for Enhancing LLM\n  Conversational Capabilities",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DocTalk: Scalable Graph-based Dialogue Synthesis for Enhancing LLM\n  Conversational Capabilities"
                },
                "summary": "Large Language Models (LLMs) are increasingly employed in multi-turn\nconversational tasks, yet their pre-training data predominantly consists of\ncontinuous prose, creating a potential mismatch between required capabilities\nand training paradigms. We introduce a novel approach to address this\ndiscrepancy by synthesizing conversational data from existing text corpora. We\npresent a pipeline that transforms a cluster of multiple related documents into\nan extended multi-turn, multi-topic information-seeking dialogue. Applying our\npipeline to Wikipedia articles, we curate DocTalk, a multi-turn pre-training\ndialogue corpus consisting of over 730k long conversations. We hypothesize that\nexposure to such synthesized conversational structures during pre-training can\nenhance the fundamental multi-turn capabilities of LLMs, such as context memory\nand understanding. Empirically, we show that incorporating DocTalk during\npre-training results in up to 40% gain in context memory and understanding,\nwithout compromising base performance. DocTalk is available at\nhttps://huggingface.co/datasets/AmazonScience/DocTalk.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) are increasingly employed in multi-turn\nconversational tasks, yet their pre-training data predominantly consists of\ncontinuous prose, creating a potential mismatch between required capabilities\nand training paradigms. We introduce a novel approach to address this\ndiscrepancy by synthesizing conversational data from existing text corpora. We\npresent a pipeline that transforms a cluster of multiple related documents into\nan extended multi-turn, multi-topic information-seeking dialogue. Applying our\npipeline to Wikipedia articles, we curate DocTalk, a multi-turn pre-training\ndialogue corpus consisting of over 730k long conversations. We hypothesize that\nexposure to such synthesized conversational structures during pre-training can\nenhance the fundamental multi-turn capabilities of LLMs, such as context memory\nand understanding. Empirically, we show that incorporating DocTalk during\npre-training results in up to 40% gain in context memory and understanding,\nwithout compromising base performance. DocTalk is available at\nhttps://huggingface.co/datasets/AmazonScience/DocTalk."
                },
                "authors": [
                    {
                        "name": "Jing Yang Lee"
                    },
                    {
                        "name": "Hamed Bonab"
                    },
                    {
                        "name": "Nasser Zalmout"
                    },
                    {
                        "name": "Ming Zeng"
                    },
                    {
                        "name": "Sanket Lokegaonkar"
                    },
                    {
                        "name": "Colin Lockard"
                    },
                    {
                        "name": "Binxuan Huang"
                    },
                    {
                        "name": "Ritesh Sarkhel"
                    },
                    {
                        "name": "Haodong Wang"
                    }
                ],
                "author_detail": {
                    "name": "Haodong Wang"
                },
                "author": "Haodong Wang",
                "arxiv_comment": "Accepted at SIGDIAL 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.05750v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.05750v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.08414v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.08414v2",
                "updated": "2025-07-08T07:45:50Z",
                "updated_parsed": [
                    2025,
                    7,
                    8,
                    7,
                    45,
                    50,
                    1,
                    189,
                    0
                ],
                "published": "2025-06-10T03:37:45Z",
                "published_parsed": [
                    2025,
                    6,
                    10,
                    3,
                    37,
                    45,
                    1,
                    161,
                    0
                ],
                "title": "Theoretical Foundations of Waste Factor and Waste Figure with\n  Applications to Fixed Wireless Access and Relay Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Theoretical Foundations of Waste Factor and Waste Figure with\n  Applications to Fixed Wireless Access and Relay Systems"
                },
                "summary": "The exponential rise in energy consumption across wireless communication\nsystems, particularly in anticipation of next-generation wireless systems,\nnecessitates rigorous frameworks for evaluating and optimizing energy\nefficiency. This paper revisits and expands the concept of the Waste Factor\n(W), or Waste Figure (WF) in decibel scale, as a unifying metric that captures\nboth utilized and wasted power in cascaded communication systems. Building upon\nits foundation in system-level power modeling, we integrate the Waste Factor\ninto a refined formulation of the Consumption Factor (CF), the ratio of data\nrate to total consumed power, linking it directly to Shannon's theoretical\nlimit on energy per bit. This analysis introduces additive energy waste into\nthe classical energy-per-bit derivation through the Waste Factor term.\n  We derive closed-form expressions for energy-per-bit expenditure in both\ndirect and relay-assisted links and develop a decision rule to determine which\ncommunication path is more energy efficient under given conditions. While not\nmodeled explicitly, Reflective Intelligent Surfaces (RIS) can be interpreted as\na special case of relay-based architectures within this unified formulation,\nsuggesting broader applicability of the Waste Factor framework to emerging 6G\nuse cases. The framework is then extended to a Fixed Wireless Access (FWA)\nscenario, where uplink and downlink asymmetries, traffic directionality, and\ncomponent inefficiencies are jointly considered to analyze energy-optimal\ndeployment strategies.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The exponential rise in energy consumption across wireless communication\nsystems, particularly in anticipation of next-generation wireless systems,\nnecessitates rigorous frameworks for evaluating and optimizing energy\nefficiency. This paper revisits and expands the concept of the Waste Factor\n(W), or Waste Figure (WF) in decibel scale, as a unifying metric that captures\nboth utilized and wasted power in cascaded communication systems. Building upon\nits foundation in system-level power modeling, we integrate the Waste Factor\ninto a refined formulation of the Consumption Factor (CF), the ratio of data\nrate to total consumed power, linking it directly to Shannon's theoretical\nlimit on energy per bit. This analysis introduces additive energy waste into\nthe classical energy-per-bit derivation through the Waste Factor term.\n  We derive closed-form expressions for energy-per-bit expenditure in both\ndirect and relay-assisted links and develop a decision rule to determine which\ncommunication path is more energy efficient under given conditions. While not\nmodeled explicitly, Reflective Intelligent Surfaces (RIS) can be interpreted as\na special case of relay-based architectures within this unified formulation,\nsuggesting broader applicability of the Waste Factor framework to emerging 6G\nuse cases. The framework is then extended to a Fixed Wireless Access (FWA)\nscenario, where uplink and downlink asymmetries, traffic directionality, and\ncomponent inefficiencies are jointly considered to analyze energy-optimal\ndeployment strategies."
                },
                "authors": [
                    {
                        "name": "Nurullah Sevim"
                    },
                    {
                        "name": "Mostafa Ibrahim"
                    },
                    {
                        "name": "Sabit Ekin"
                    },
                    {
                        "name": "Theodore S. Rappaport"
                    }
                ],
                "author_detail": {
                    "name": "Theodore S. Rappaport"
                },
                "author": "Theodore S. Rappaport",
                "arxiv_comment": "Accepted at Nature npj Wireless Technology, July 1st, 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.08414v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.08414v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.05740v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.05740v1",
                "updated": "2025-07-08T07:37:12Z",
                "updated_parsed": [
                    2025,
                    7,
                    8,
                    7,
                    37,
                    12,
                    1,
                    189,
                    0
                ],
                "published": "2025-07-08T07:37:12Z",
                "published_parsed": [
                    2025,
                    7,
                    8,
                    7,
                    37,
                    12,
                    1,
                    189,
                    0
                ],
                "title": "GPTKB v1.5: A Massive Knowledge Base for Exploring Factual LLM Knowledge",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GPTKB v1.5: A Massive Knowledge Base for Exploring Factual LLM Knowledge"
                },
                "summary": "Language models are powerful tools, yet their factual knowledge is still\npoorly understood, and inaccessible to ad-hoc browsing and scalable statistical\nanalysis. This demonstration introduces GPTKB v1.5, a densely interlinked\n100-million-triple knowledge base (KB) built for $14,000 from GPT-4.1, using\nthe GPTKB methodology for massive-recursive LLM knowledge materialization (Hu\net al., ACL 2025). The demonstration experience focuses on three use cases: (1)\nlink-traversal-based LLM knowledge exploration, (2) SPARQL-based structured LLM\nknowledge querying, (3) comparative exploration of the strengths and weaknesses\nof LLM knowledge. Massive-recursive LLM knowledge materialization is a\ngroundbreaking opportunity both for the research area of systematic analysis of\nLLM knowledge, as well as for automated KB construction. The GPTKB demonstrator\nis accessible at https://gptkb.org.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Language models are powerful tools, yet their factual knowledge is still\npoorly understood, and inaccessible to ad-hoc browsing and scalable statistical\nanalysis. This demonstration introduces GPTKB v1.5, a densely interlinked\n100-million-triple knowledge base (KB) built for $14,000 from GPT-4.1, using\nthe GPTKB methodology for massive-recursive LLM knowledge materialization (Hu\net al., ACL 2025). The demonstration experience focuses on three use cases: (1)\nlink-traversal-based LLM knowledge exploration, (2) SPARQL-based structured LLM\nknowledge querying, (3) comparative exploration of the strengths and weaknesses\nof LLM knowledge. Massive-recursive LLM knowledge materialization is a\ngroundbreaking opportunity both for the research area of systematic analysis of\nLLM knowledge, as well as for automated KB construction. The GPTKB demonstrator\nis accessible at https://gptkb.org."
                },
                "authors": [
                    {
                        "name": "Yujia Hu"
                    },
                    {
                        "name": "Tuan-Phong Nguyen"
                    },
                    {
                        "name": "Shrestha Ghosh"
                    },
                    {
                        "name": "Moritz MÃ¼ller"
                    },
                    {
                        "name": "Simon Razniewski"
                    }
                ],
                "author_detail": {
                    "name": "Simon Razniewski"
                },
                "author": "Simon Razniewski",
                "arxiv_comment": "7 pages, 6 figures, 1 table",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.05740v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.05740v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.05733v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.05733v1",
                "updated": "2025-07-08T07:26:55Z",
                "updated_parsed": [
                    2025,
                    7,
                    8,
                    7,
                    26,
                    55,
                    1,
                    189,
                    0
                ],
                "published": "2025-07-08T07:26:55Z",
                "published_parsed": [
                    2025,
                    7,
                    8,
                    7,
                    26,
                    55,
                    1,
                    189,
                    0
                ],
                "title": "When Transformers Meet Recommenders: Integrating Self-Attentive\n  Sequential Recommendation with Fine-Tuned LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "When Transformers Meet Recommenders: Integrating Self-Attentive\n  Sequential Recommendation with Fine-Tuned LLMs"
                },
                "summary": "Self-Attentive Sequential Recommendation (SASRec) effectively captures\nlong-term user preferences by applying attention mechanisms to historical\ninteractions. Concurrently, the rise of Large Language Models (LLMs) has\nmotivated research into LLM-based recommendation, which leverages their\npowerful generalization and language understanding capabilities. However, LLMs\noften lack the domain-specific knowledge and collaborative signals essential\nfor high-quality recommendations when relying solely on textual prompts. To\naddress this limitation, this study proposes SASRecLLM, a novel framework that\nintegrates SASRec as a collaborative encoder with an LLM fine-tuned using\nLow-Rank Adaptation (LoRA). The components are connected via a mapping layer to\nalign their dimensional spaces, and three targeted training strategies are\ndesigned to optimize the hybrid architecture. Extensive experiments on multiple\ndatasets demonstrate that SASRecLLM achieves robust and consistent improvements\nover strong baselines in both cold-start and warm-start scenarios. This work\nadvances the field of LLM-based recommendation by presenting a modular and\neffective paradigm for fusing structured collaborative filtering with the\nsemantic power of fine-tuned LLMs. The implementation is available on GitHub:\nhttps://github.com/kechenkristin/RecLLM",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Self-Attentive Sequential Recommendation (SASRec) effectively captures\nlong-term user preferences by applying attention mechanisms to historical\ninteractions. Concurrently, the rise of Large Language Models (LLMs) has\nmotivated research into LLM-based recommendation, which leverages their\npowerful generalization and language understanding capabilities. However, LLMs\noften lack the domain-specific knowledge and collaborative signals essential\nfor high-quality recommendations when relying solely on textual prompts. To\naddress this limitation, this study proposes SASRecLLM, a novel framework that\nintegrates SASRec as a collaborative encoder with an LLM fine-tuned using\nLow-Rank Adaptation (LoRA). The components are connected via a mapping layer to\nalign their dimensional spaces, and three targeted training strategies are\ndesigned to optimize the hybrid architecture. Extensive experiments on multiple\ndatasets demonstrate that SASRecLLM achieves robust and consistent improvements\nover strong baselines in both cold-start and warm-start scenarios. This work\nadvances the field of LLM-based recommendation by presenting a modular and\neffective paradigm for fusing structured collaborative filtering with the\nsemantic power of fine-tuned LLMs. The implementation is available on GitHub:\nhttps://github.com/kechenkristin/RecLLM"
                },
                "authors": [
                    {
                        "name": "Kechen Liu"
                    }
                ],
                "author_detail": {
                    "name": "Kechen Liu"
                },
                "author": "Kechen Liu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.05733v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.05733v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.02233v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.02233v3",
                "updated": "2025-07-08T07:23:07Z",
                "updated_parsed": [
                    2025,
                    7,
                    8,
                    7,
                    23,
                    7,
                    1,
                    189,
                    0
                ],
                "published": "2025-03-04T03:16:02Z",
                "published_parsed": [
                    2025,
                    3,
                    4,
                    3,
                    16,
                    2,
                    1,
                    63,
                    0
                ],
                "title": "Enhancing LLM Reliability via Explicit Knowledge Boundary Modeling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Enhancing LLM Reliability via Explicit Knowledge Boundary Modeling"
                },
                "summary": "Large language models (LLMs) are prone to hallucination stemming from\nmisaligned self-awareness, particularly when processing queries exceeding their\nknowledge boundaries. While existing mitigation strategies employ uncertainty\nestimation or query rejection mechanisms, they suffer from computational\nefficiency and sacrificed helpfulness. To address these issues, we propose the\nExplicit Knowledge Boundary Modeling (EKBM) framework, integrating fast and\nslow reasoning systems to harmonize reliability and usability. The framework\nfirst employs a fast-thinking model to generate confidence-labeled responses,\nenabling immediate utilization of high-confidence outputs, whereas uncertain\npredictions trigger a slow refinement model for accuracy improvement. To align\nmodel behavior with our proposed object, we propose a hybrid training pipeline,\nenhancing self-awareness without degrading task performance. Evaluations on\ndialogue state tracking tasks demonstrate that EKBM achieves superior model\nreliability over uncertainty-based baselines. Further analysis reveals that\nrefinement substantially boosts accuracy while maintaining low computational\noverhead. The framework establishes a scalable paradigm for deploying reliable\nLLMs in error-sensitive applications, effectively balancing accuracy and\npractical utility.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) are prone to hallucination stemming from\nmisaligned self-awareness, particularly when processing queries exceeding their\nknowledge boundaries. While existing mitigation strategies employ uncertainty\nestimation or query rejection mechanisms, they suffer from computational\nefficiency and sacrificed helpfulness. To address these issues, we propose the\nExplicit Knowledge Boundary Modeling (EKBM) framework, integrating fast and\nslow reasoning systems to harmonize reliability and usability. The framework\nfirst employs a fast-thinking model to generate confidence-labeled responses,\nenabling immediate utilization of high-confidence outputs, whereas uncertain\npredictions trigger a slow refinement model for accuracy improvement. To align\nmodel behavior with our proposed object, we propose a hybrid training pipeline,\nenhancing self-awareness without degrading task performance. Evaluations on\ndialogue state tracking tasks demonstrate that EKBM achieves superior model\nreliability over uncertainty-based baselines. Further analysis reveals that\nrefinement substantially boosts accuracy while maintaining low computational\noverhead. The framework establishes a scalable paradigm for deploying reliable\nLLMs in error-sensitive applications, effectively balancing accuracy and\npractical utility."
                },
                "authors": [
                    {
                        "name": "Hang Zheng"
                    },
                    {
                        "name": "Hongshen Xu"
                    },
                    {
                        "name": "Yuncong Liu"
                    },
                    {
                        "name": "Lu Chen"
                    },
                    {
                        "name": "Pascale Fung"
                    },
                    {
                        "name": "Kai Yu"
                    }
                ],
                "author_detail": {
                    "name": "Kai Yu"
                },
                "author": "Kai Yu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.02233v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.02233v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.05727v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.05727v1",
                "updated": "2025-07-08T07:21:20Z",
                "updated_parsed": [
                    2025,
                    7,
                    8,
                    7,
                    21,
                    20,
                    1,
                    189,
                    0
                ],
                "published": "2025-07-08T07:21:20Z",
                "published_parsed": [
                    2025,
                    7,
                    8,
                    7,
                    21,
                    20,
                    1,
                    189,
                    0
                ],
                "title": "ContextASR-Bench: A Massive Contextual Speech Recognition Benchmark",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ContextASR-Bench: A Massive Contextual Speech Recognition Benchmark"
                },
                "summary": "Automatic Speech Recognition (ASR) has been extensively investigated, yet\nprior evaluative efforts have largely been restricted to contextless paradigms.\nThis constraint stems from the limited proficiency of conventional ASR models\nin context modeling and their deficiency in memory and reasoning based on world\nknowledge. Recent breakthroughs in the development of Large Language Models\n(LLMs) and corresponding Large Audio Language Models (LALMs) have markedly\nenhanced the visibility of general artificial intelligence capabilities.\nConsequently, there exists a compelling need for a benchmark that can evaluate\nboth the generality and intelligence of ASR systems. To address this gap, we\npropose ContextASR-Bench: a comprehensive, large-scale benchmark designed to\nassess contextual speech recognition. This benchmark encompasses up to 40,000\ndata entries across over 10 domains, enabling a thorough evaluation of model\nperformance in scenarios that omit or incorporate coarse-grained or\nfine-grained contextual information. Moreover, diverging from conventional ASR\nevaluations, our benchmark includes an analysis of model efficacy in\nrecognizing named entities mentioned within the auditory input. Our extensive\nevaluation highlights that LALMs, with strong world knowledge and context\nlearning capabilities, outperform conventional ASR models by a large margin.\nThe dataset and evaluation code have been released at\nhttps://github.com/MrSupW/ContextASR-Bench.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Automatic Speech Recognition (ASR) has been extensively investigated, yet\nprior evaluative efforts have largely been restricted to contextless paradigms.\nThis constraint stems from the limited proficiency of conventional ASR models\nin context modeling and their deficiency in memory and reasoning based on world\nknowledge. Recent breakthroughs in the development of Large Language Models\n(LLMs) and corresponding Large Audio Language Models (LALMs) have markedly\nenhanced the visibility of general artificial intelligence capabilities.\nConsequently, there exists a compelling need for a benchmark that can evaluate\nboth the generality and intelligence of ASR systems. To address this gap, we\npropose ContextASR-Bench: a comprehensive, large-scale benchmark designed to\nassess contextual speech recognition. This benchmark encompasses up to 40,000\ndata entries across over 10 domains, enabling a thorough evaluation of model\nperformance in scenarios that omit or incorporate coarse-grained or\nfine-grained contextual information. Moreover, diverging from conventional ASR\nevaluations, our benchmark includes an analysis of model efficacy in\nrecognizing named entities mentioned within the auditory input. Our extensive\nevaluation highlights that LALMs, with strong world knowledge and context\nlearning capabilities, outperform conventional ASR models by a large margin.\nThe dataset and evaluation code have been released at\nhttps://github.com/MrSupW/ContextASR-Bench."
                },
                "authors": [
                    {
                        "name": "He Wang"
                    },
                    {
                        "name": "Linhan Ma"
                    },
                    {
                        "name": "Dake Guo"
                    },
                    {
                        "name": "Xiong Wang"
                    },
                    {
                        "name": "Lei Xie"
                    },
                    {
                        "name": "Jin Xu"
                    },
                    {
                        "name": "Junyang Lin"
                    }
                ],
                "author_detail": {
                    "name": "Junyang Lin"
                },
                "author": "Junyang Lin",
                "arxiv_comment": "18 pages, 4 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.05727v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.05727v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.AS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.12182v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.12182v3",
                "updated": "2025-07-08T07:21:15Z",
                "updated_parsed": [
                    2025,
                    7,
                    8,
                    7,
                    21,
                    15,
                    1,
                    189,
                    0
                ],
                "published": "2025-05-18T00:47:21Z",
                "published_parsed": [
                    2025,
                    5,
                    18,
                    0,
                    47,
                    21,
                    6,
                    138,
                    0
                ],
                "title": "Truth Neurons",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Truth Neurons"
                },
                "summary": "Despite their remarkable success and deployment across diverse workflows,\nlanguage models sometimes produce untruthful responses. Our limited\nunderstanding of how truthfulness is mechanistically encoded within these\nmodels jeopardizes their reliability and safety. In this paper, we propose a\nmethod for identifying representations of truthfulness at the neuron level. We\nshow that language models contain truth neurons, which encode truthfulness in a\nsubject-agnostic manner. Experiments conducted across models of varying scales\nvalidate the existence of truth neurons, confirming that the encoding of\ntruthfulness at the neuron level is a property shared by many language models.\nThe distribution patterns of truth neurons over layers align with prior\nfindings on the geometry of truthfulness. Selectively suppressing the\nactivations of truth neurons found through the TruthfulQA dataset degrades\nperformance both on TruthfulQA and on other benchmarks, showing that the\ntruthfulness mechanisms are not tied to a specific dataset. Our results offer\nnovel insights into the mechanisms underlying truthfulness in language models\nand highlight potential directions toward improving their trustworthiness and\nreliability.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Despite their remarkable success and deployment across diverse workflows,\nlanguage models sometimes produce untruthful responses. Our limited\nunderstanding of how truthfulness is mechanistically encoded within these\nmodels jeopardizes their reliability and safety. In this paper, we propose a\nmethod for identifying representations of truthfulness at the neuron level. We\nshow that language models contain truth neurons, which encode truthfulness in a\nsubject-agnostic manner. Experiments conducted across models of varying scales\nvalidate the existence of truth neurons, confirming that the encoding of\ntruthfulness at the neuron level is a property shared by many language models.\nThe distribution patterns of truth neurons over layers align with prior\nfindings on the geometry of truthfulness. Selectively suppressing the\nactivations of truth neurons found through the TruthfulQA dataset degrades\nperformance both on TruthfulQA and on other benchmarks, showing that the\ntruthfulness mechanisms are not tied to a specific dataset. Our results offer\nnovel insights into the mechanisms underlying truthfulness in language models\nand highlight potential directions toward improving their trustworthiness and\nreliability."
                },
                "authors": [
                    {
                        "name": "Haohang Li"
                    },
                    {
                        "name": "Yupeng Cao"
                    },
                    {
                        "name": "Yangyang Yu"
                    },
                    {
                        "name": "Jordan W. Suchow"
                    },
                    {
                        "name": "Zining Zhu"
                    }
                ],
                "author_detail": {
                    "name": "Zining Zhu"
                },
                "author": "Zining Zhu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.12182v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.12182v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.05723v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.05723v1",
                "updated": "2025-07-08T07:17:24Z",
                "updated_parsed": [
                    2025,
                    7,
                    8,
                    7,
                    17,
                    24,
                    1,
                    189,
                    0
                ],
                "published": "2025-07-08T07:17:24Z",
                "published_parsed": [
                    2025,
                    7,
                    8,
                    7,
                    17,
                    24,
                    1,
                    189,
                    0
                ],
                "title": "Large Language Models for Agent-Based Modelling: Current and possible\n  uses across the modelling cycle",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models for Agent-Based Modelling: Current and possible\n  uses across the modelling cycle"
                },
                "summary": "The emergence of Large Language Models (LLMs) with increasingly sophisticated\nnatural language understanding and generative capabilities has sparked interest\nin the Agent-based Modelling (ABM) community. With their ability to summarize,\ngenerate, analyze, categorize, transcribe and translate text, answer questions,\npropose explanations, sustain dialogue, extract information from unstructured\ntext, and perform logical reasoning and problem-solving tasks, LLMs have a good\npotential to contribute to the modelling process. After reviewing the current\nuse of LLMs in ABM, this study reflects on the opportunities and challenges of\nthe potential use of LLMs in ABM. It does so by following the modelling cycle,\nfrom problem formulation to documentation and communication of model results,\nand holding a critical stance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The emergence of Large Language Models (LLMs) with increasingly sophisticated\nnatural language understanding and generative capabilities has sparked interest\nin the Agent-based Modelling (ABM) community. With their ability to summarize,\ngenerate, analyze, categorize, transcribe and translate text, answer questions,\npropose explanations, sustain dialogue, extract information from unstructured\ntext, and perform logical reasoning and problem-solving tasks, LLMs have a good\npotential to contribute to the modelling process. After reviewing the current\nuse of LLMs in ABM, this study reflects on the opportunities and challenges of\nthe potential use of LLMs in ABM. It does so by following the modelling cycle,\nfrom problem formulation to documentation and communication of model results,\nand holding a critical stance."
                },
                "authors": [
                    {
                        "name": "LoÃ¯s VanhÃ©e"
                    },
                    {
                        "name": "Melania Borit"
                    },
                    {
                        "name": "Peer-Olaf Siebers"
                    },
                    {
                        "name": "Roger Cremades"
                    },
                    {
                        "name": "Christopher Frantz"
                    },
                    {
                        "name": "Ãnder GÃ¼rcan"
                    },
                    {
                        "name": "FrantiÅ¡ek Kalvas"
                    },
                    {
                        "name": "Denisa Reshef Kera"
                    },
                    {
                        "name": "Vivek Nallur"
                    },
                    {
                        "name": "Kavin Narasimhan"
                    },
                    {
                        "name": "Martin Neumann"
                    }
                ],
                "author_detail": {
                    "name": "Martin Neumann"
                },
                "author": "Martin Neumann",
                "arxiv_comment": "18 pages, including 2 pages of appendix, accepted for publication at\n  the Social Simulation Conference 2025 (https://ssc2025.tbm.tudelft.nl/)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.05723v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.05723v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.MA",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.05722v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.05722v1",
                "updated": "2025-07-08T07:10:52Z",
                "updated_parsed": [
                    2025,
                    7,
                    8,
                    7,
                    10,
                    52,
                    1,
                    189,
                    0
                ],
                "published": "2025-07-08T07:10:52Z",
                "published_parsed": [
                    2025,
                    7,
                    8,
                    7,
                    10,
                    52,
                    1,
                    189,
                    0
                ],
                "title": "Hierarchical Task Offloading for UAV-Assisted Vehicular Edge Computing\n  via Deep Reinforcement Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Hierarchical Task Offloading for UAV-Assisted Vehicular Edge Computing\n  via Deep Reinforcement Learning"
                },
                "summary": "With the emergence of compute-intensive and delay-sensitive applications in\nvehicular networks, unmanned aerial vehicles (UAVs) have emerged as a promising\ncomplement for vehicular edge computing due to the high mobility and flexible\ndeployment. However, the existing UAV-assisted offloading strategies are\ninsufficient in coordinating heterogeneous computing resources and adapting to\ndynamic network conditions. Hence, this paper proposes a dual-layer\nUAV-assisted edge computing architecture based on partial offloading, composed\nof the relay capability of high-altitude UAVs and the computing support of\nlow-altitude UAVs. The proposed architecture enables efficient integration and\ncoordination of heterogeneous resources. A joint optimization problem is\nformulated to minimize the system delay and energy consumption while ensuring\nthe task completion rate. To solve the high-dimensional decision problem, we\nreformulate the problem as a Markov decision process and propose a hierarchical\noffloading scheme based on the soft actor-critic algorithm. The method\ndecouples global and local decisions, where the global decisions integrate\noffloading ratios and trajectory planning into continuous actions, while the\nlocal scheduling is handled via designing a priority-based mechanism.\nSimulations are conducted and demonstrate that the proposed approach\noutperforms several baselines in task completion rate, system efficiency, and\nconvergence speed, showing strong robustness and applicability in dynamic\nvehicular environments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the emergence of compute-intensive and delay-sensitive applications in\nvehicular networks, unmanned aerial vehicles (UAVs) have emerged as a promising\ncomplement for vehicular edge computing due to the high mobility and flexible\ndeployment. However, the existing UAV-assisted offloading strategies are\ninsufficient in coordinating heterogeneous computing resources and adapting to\ndynamic network conditions. Hence, this paper proposes a dual-layer\nUAV-assisted edge computing architecture based on partial offloading, composed\nof the relay capability of high-altitude UAVs and the computing support of\nlow-altitude UAVs. The proposed architecture enables efficient integration and\ncoordination of heterogeneous resources. A joint optimization problem is\nformulated to minimize the system delay and energy consumption while ensuring\nthe task completion rate. To solve the high-dimensional decision problem, we\nreformulate the problem as a Markov decision process and propose a hierarchical\noffloading scheme based on the soft actor-critic algorithm. The method\ndecouples global and local decisions, where the global decisions integrate\noffloading ratios and trajectory planning into continuous actions, while the\nlocal scheduling is handled via designing a priority-based mechanism.\nSimulations are conducted and demonstrate that the proposed approach\noutperforms several baselines in task completion rate, system efficiency, and\nconvergence speed, showing strong robustness and applicability in dynamic\nvehicular environments."
                },
                "authors": [
                    {
                        "name": "Hongbao Li"
                    },
                    {
                        "name": "Ziye Jia"
                    },
                    {
                        "name": "Sijie He"
                    },
                    {
                        "name": "Kun Guo"
                    },
                    {
                        "name": "Qihui Wu"
                    }
                ],
                "author_detail": {
                    "name": "Qihui Wu"
                },
                "author": "Qihui Wu",
                "arxiv_comment": "6 pages, 5 figures, conference",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.05722v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.05722v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.05713v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.05713v1",
                "updated": "2025-07-08T06:52:43Z",
                "updated_parsed": [
                    2025,
                    7,
                    8,
                    6,
                    52,
                    43,
                    1,
                    189,
                    0
                ],
                "published": "2025-07-08T06:52:43Z",
                "published_parsed": [
                    2025,
                    7,
                    8,
                    6,
                    52,
                    43,
                    1,
                    189,
                    0
                ],
                "title": "DRAGON: Dynamic RAG Benchmark On News",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DRAGON: Dynamic RAG Benchmark On News"
                },
                "summary": "Retrieval-Augmented Generation (RAG) is a widely adopted approach for\nimproving the factuality of large language models (LLMs) by incorporating\nexternal knowledge at inference time. Although there exist multiple RAG\nbenchmarks for English, evaluation resources for other languages, including\nRussian, remain scarce and static, failing to capture the dynamic nature of\nreal-world deployments.\n  In this work, we present DRAGON (Dynamic RAG Benchmark On News), the first\ndynamic benchmark for evaluating RAG systems in Russian on a changing news\ncorpora. DRAGON is built upon a regularly updated corpus of Russian news and\npublic documents and supports comprehensive evaluation of both the retriever\nand generator components. Question generation is performed automatically with\nthe use of Knowledge Graph constructed from the corpus and enables the\nextraction of four core question types aligned with distinct subgraph patterns.\nWe release a complete evaluation framework comprising the pipeline for\nautomatic question generation, evaluation scripts, which are potentially\nreusable for other languages and multilingual settings, and benchmark data. We\nalso launch a public leaderboard to encourage community participation and\ncomparison.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Retrieval-Augmented Generation (RAG) is a widely adopted approach for\nimproving the factuality of large language models (LLMs) by incorporating\nexternal knowledge at inference time. Although there exist multiple RAG\nbenchmarks for English, evaluation resources for other languages, including\nRussian, remain scarce and static, failing to capture the dynamic nature of\nreal-world deployments.\n  In this work, we present DRAGON (Dynamic RAG Benchmark On News), the first\ndynamic benchmark for evaluating RAG systems in Russian on a changing news\ncorpora. DRAGON is built upon a regularly updated corpus of Russian news and\npublic documents and supports comprehensive evaluation of both the retriever\nand generator components. Question generation is performed automatically with\nthe use of Knowledge Graph constructed from the corpus and enables the\nextraction of four core question types aligned with distinct subgraph patterns.\nWe release a complete evaluation framework comprising the pipeline for\nautomatic question generation, evaluation scripts, which are potentially\nreusable for other languages and multilingual settings, and benchmark data. We\nalso launch a public leaderboard to encourage community participation and\ncomparison."
                },
                "authors": [
                    {
                        "name": "Fedor Chernogorskii"
                    },
                    {
                        "name": "Sergei Averkiev"
                    },
                    {
                        "name": "Liliya Kudraleeva"
                    },
                    {
                        "name": "Zaven Martirosian"
                    },
                    {
                        "name": "Maria Tikhonova"
                    },
                    {
                        "name": "Valentin Malykh"
                    },
                    {
                        "name": "Alena Fenogenova"
                    }
                ],
                "author_detail": {
                    "name": "Alena Fenogenova"
                },
                "author": "Alena Fenogenova",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.05713v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.05713v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.02962v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.02962v2",
                "updated": "2025-07-08T06:38:26Z",
                "updated_parsed": [
                    2025,
                    7,
                    8,
                    6,
                    38,
                    26,
                    1,
                    189,
                    0
                ],
                "published": "2025-06-30T09:02:45Z",
                "published_parsed": [
                    2025,
                    6,
                    30,
                    9,
                    2,
                    45,
                    0,
                    181,
                    0
                ],
                "title": "RAG-R1 : Incentivize the Search and Reasoning Capabilities of LLMs\n  through Multi-query Parallelism",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RAG-R1 : Incentivize the Search and Reasoning Capabilities of LLMs\n  through Multi-query Parallelism"
                },
                "summary": "Large Language Models (LLMs) have demonstrated remarkable capabilities across\nvarious tasks, while they remain prone to generating hallucinated or outdated\nresponses due to their static internal knowledge. Recent advancements in\nRetrieval-Augmented Generation (RAG) methods have explored enhancing models'\nsearch and reasoning capabilities through reinforcement learning (RL). Although\nthese methods demonstrate promising results, they face challenges in training\nstability and encounter issues such as substantial inference time and\nrestricted capabilities due to the single-query mode. In this paper, we propose\nRAG-R1, a novel training framework designed to enable LLMs to adaptively\nleverage internal and external knowledge during the reasoning process. We\nfurther expand the generation and retrieval processes within the framework from\nsingle-query mode to multi-query parallelism, aimed at reducing inference time\nand enhancing the model's capabilities. Extensive experiments on seven\nquestion-answering benchmarks demonstrate that our method outperforms the\nstrongest baseline by up to 13.2% and decreases inference time by 11.1%.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have demonstrated remarkable capabilities across\nvarious tasks, while they remain prone to generating hallucinated or outdated\nresponses due to their static internal knowledge. Recent advancements in\nRetrieval-Augmented Generation (RAG) methods have explored enhancing models'\nsearch and reasoning capabilities through reinforcement learning (RL). Although\nthese methods demonstrate promising results, they face challenges in training\nstability and encounter issues such as substantial inference time and\nrestricted capabilities due to the single-query mode. In this paper, we propose\nRAG-R1, a novel training framework designed to enable LLMs to adaptively\nleverage internal and external knowledge during the reasoning process. We\nfurther expand the generation and retrieval processes within the framework from\nsingle-query mode to multi-query parallelism, aimed at reducing inference time\nand enhancing the model's capabilities. Extensive experiments on seven\nquestion-answering benchmarks demonstrate that our method outperforms the\nstrongest baseline by up to 13.2% and decreases inference time by 11.1%."
                },
                "authors": [
                    {
                        "name": "Zhiwen Tan"
                    },
                    {
                        "name": "Jiaming Huang"
                    },
                    {
                        "name": "Qintong Wu"
                    },
                    {
                        "name": "Hongxuan Zhang"
                    },
                    {
                        "name": "Chenyi Zhuang"
                    },
                    {
                        "name": "Jinjie Gu"
                    }
                ],
                "author_detail": {
                    "name": "Jinjie Gu"
                },
                "author": "Jinjie Gu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.02962v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.02962v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.04942v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.04942v2",
                "updated": "2025-07-08T06:37:05Z",
                "updated_parsed": [
                    2025,
                    7,
                    8,
                    6,
                    37,
                    5,
                    1,
                    189,
                    0
                ],
                "published": "2025-07-07T12:38:53Z",
                "published_parsed": [
                    2025,
                    7,
                    7,
                    12,
                    38,
                    53,
                    0,
                    188,
                    0
                ],
                "title": "SIGIR 2025 -- LiveRAG Challenge Report",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SIGIR 2025 -- LiveRAG Challenge Report"
                },
                "summary": "The LiveRAG Challenge at SIGIR 2025, held between March and May 2025,\nprovided a competitive platform for advancing Retrieval-Augmented Generation\n(RAG) technologies. Participants from academia and industry were invited to\ndevelop a RAG-based question-answering system using a fixed corpus\n(Fineweb-10BT) and a common open-source LLM (Falcon3-10B-Instruct). The goal\nwas to facilitate challenging comparisons of retrieval and prompting\nstrategies. During the Live Challenge Day, 70 teams from 27 different countries\nprovided answers and supportive information to 500 unseen questions within a\nstrict two-hour time window. Evaluation was conducted in two stages: first an\nautomated LLM-as-a-judge approach was used to compute correctness and\nfaithfulness score, then a manual review of top ranked submissions was\nconducted. The finalists were announced on June 12, 2025, with prizes awarded\nduring the LiveRAG Workshop at SIGIR 2025 in Padua, Italy.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The LiveRAG Challenge at SIGIR 2025, held between March and May 2025,\nprovided a competitive platform for advancing Retrieval-Augmented Generation\n(RAG) technologies. Participants from academia and industry were invited to\ndevelop a RAG-based question-answering system using a fixed corpus\n(Fineweb-10BT) and a common open-source LLM (Falcon3-10B-Instruct). The goal\nwas to facilitate challenging comparisons of retrieval and prompting\nstrategies. During the Live Challenge Day, 70 teams from 27 different countries\nprovided answers and supportive information to 500 unseen questions within a\nstrict two-hour time window. Evaluation was conducted in two stages: first an\nautomated LLM-as-a-judge approach was used to compute correctness and\nfaithfulness score, then a manual review of top ranked submissions was\nconducted. The finalists were announced on June 12, 2025, with prizes awarded\nduring the LiveRAG Workshop at SIGIR 2025 in Padua, Italy."
                },
                "authors": [
                    {
                        "name": "David Carmel"
                    },
                    {
                        "name": "Simone Filice"
                    },
                    {
                        "name": "Guy Horowitz"
                    },
                    {
                        "name": "Yoelle Maarek"
                    },
                    {
                        "name": "Oren Somekh"
                    },
                    {
                        "name": "Ran Tavory"
                    },
                    {
                        "name": "Mehdi Ghissassi"
                    },
                    {
                        "name": "Edo Liberty"
                    },
                    {
                        "name": "Roy Miara"
                    }
                ],
                "author_detail": {
                    "name": "Roy Miara"
                },
                "author": "Roy Miara",
                "arxiv_comment": "9 pages, 5 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.04942v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.04942v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "H.3.3",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.13299v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.13299v2",
                "updated": "2025-07-08T06:24:53Z",
                "updated_parsed": [
                    2025,
                    7,
                    8,
                    6,
                    24,
                    53,
                    1,
                    189,
                    0
                ],
                "published": "2025-03-17T15:44:09Z",
                "published_parsed": [
                    2025,
                    3,
                    17,
                    15,
                    44,
                    9,
                    0,
                    76,
                    0
                ],
                "title": "A Survey on Transformer Context Extension: Approaches and Evaluation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Survey on Transformer Context Extension: Approaches and Evaluation"
                },
                "summary": "Large language models (LLMs) based on Transformer have been widely applied in\nthe filed of natural language processing (NLP), demonstrating strong\nperformance, particularly in handling short text tasks. However, when it comes\nto long context scenarios, the performance of LLMs degrades due to some\nchallenges. To alleviate this phenomenon, there is a number of work proposed\nrecently. In this survey, we first list the challenges of applying pre-trained\nLLMs to process long contexts. Then systematically review the approaches\nrelated to long context and propose our taxonomy categorizing them into four\nmain types: positional encoding, context compression, retrieval augmented, and\nattention pattern. In addition to the approaches, we focus on the evaluation of\nlong context, organizing relevant data, tasks, and metrics based on existing\nlong context benchmarks. Finally, we summarize unresolved issues in the long\ncontext domain and put forward our views on future developments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) based on Transformer have been widely applied in\nthe filed of natural language processing (NLP), demonstrating strong\nperformance, particularly in handling short text tasks. However, when it comes\nto long context scenarios, the performance of LLMs degrades due to some\nchallenges. To alleviate this phenomenon, there is a number of work proposed\nrecently. In this survey, we first list the challenges of applying pre-trained\nLLMs to process long contexts. Then systematically review the approaches\nrelated to long context and propose our taxonomy categorizing them into four\nmain types: positional encoding, context compression, retrieval augmented, and\nattention pattern. In addition to the approaches, we focus on the evaluation of\nlong context, organizing relevant data, tasks, and metrics based on existing\nlong context benchmarks. Finally, we summarize unresolved issues in the long\ncontext domain and put forward our views on future developments."
                },
                "authors": [
                    {
                        "name": "Yijun Liu"
                    },
                    {
                        "name": "Jinzheng Yu"
                    },
                    {
                        "name": "Yang Xu"
                    },
                    {
                        "name": "Zhongyang Li"
                    },
                    {
                        "name": "Qingfu Zhu"
                    }
                ],
                "author_detail": {
                    "name": "Qingfu Zhu"
                },
                "author": "Qingfu Zhu",
                "arxiv_comment": "preprint",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.13299v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.13299v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.03861v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.03861v2",
                "updated": "2025-07-08T06:24:13Z",
                "updated_parsed": [
                    2025,
                    7,
                    8,
                    6,
                    24,
                    13,
                    1,
                    189,
                    0
                ],
                "published": "2025-06-04T11:48:51Z",
                "published_parsed": [
                    2025,
                    6,
                    4,
                    11,
                    48,
                    51,
                    2,
                    155,
                    0
                ],
                "title": "PulseReddit: A Novel Reddit Dataset for Benchmarking MAS in\n  High-Frequency Cryptocurrency Trading",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PulseReddit: A Novel Reddit Dataset for Benchmarking MAS in\n  High-Frequency Cryptocurrency Trading"
                },
                "summary": "High-Frequency Trading (HFT) is pivotal in cryptocurrency markets, demanding\nrapid decision-making. Social media platforms like Reddit offer valuable, yet\nunderexplored, information for such high-frequency, short-term trading. This\npaper introduces \\textbf{PulseReddit}, a novel dataset that is the first to\nalign large-scale Reddit discussion data with high-frequency cryptocurrency\nmarket statistics for short-term trading analysis. We conduct an extensive\nempirical study using Large Language Model (LLM)-based Multi-Agent Systems\n(MAS) to investigate the impact of social sentiment from PulseReddit on trading\nperformance. Our experiments conclude that MAS augmented with PulseReddit data\nachieve superior trading outcomes compared to traditional baselines,\nparticularly in bull markets, and demonstrate robust adaptability across\ndifferent market regimes. Furthermore, our research provides conclusive\ninsights into the performance-efficiency trade-offs of different LLMs,\ndetailing significant considerations for practical model selection in HFT\napplications. PulseReddit and our findings establish a foundation for advanced\nMAS research in HFT, demonstrating the tangible benefits of integrating social\nmedia.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "High-Frequency Trading (HFT) is pivotal in cryptocurrency markets, demanding\nrapid decision-making. Social media platforms like Reddit offer valuable, yet\nunderexplored, information for such high-frequency, short-term trading. This\npaper introduces \\textbf{PulseReddit}, a novel dataset that is the first to\nalign large-scale Reddit discussion data with high-frequency cryptocurrency\nmarket statistics for short-term trading analysis. We conduct an extensive\nempirical study using Large Language Model (LLM)-based Multi-Agent Systems\n(MAS) to investigate the impact of social sentiment from PulseReddit on trading\nperformance. Our experiments conclude that MAS augmented with PulseReddit data\nachieve superior trading outcomes compared to traditional baselines,\nparticularly in bull markets, and demonstrate robust adaptability across\ndifferent market regimes. Furthermore, our research provides conclusive\ninsights into the performance-efficiency trade-offs of different LLMs,\ndetailing significant considerations for practical model selection in HFT\napplications. PulseReddit and our findings establish a foundation for advanced\nMAS research in HFT, demonstrating the tangible benefits of integrating social\nmedia."
                },
                "authors": [
                    {
                        "name": "Qiuhan Han"
                    },
                    {
                        "name": "Qian Wang"
                    },
                    {
                        "name": "Atsushi Yoshikawa"
                    },
                    {
                        "name": "Masayuki Yamamura"
                    }
                ],
                "author_detail": {
                    "name": "Masayuki Yamamura"
                },
                "author": "Masayuki Yamamura",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.03861v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.03861v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            }
        ]
    }
]