[
    {
        "keyword": "kv cache",
        "arxiv_results": [
            {
                "id": "http://arxiv.org/abs/2409.11326v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.11326v2",
                "updated": "2024-09-18T17:09:42Z",
                "updated_parsed": [
                    2024,
                    9,
                    18,
                    17,
                    9,
                    42,
                    2,
                    262,
                    0
                ],
                "published": "2024-09-17T16:22:49Z",
                "published_parsed": [
                    2024,
                    9,
                    17,
                    16,
                    22,
                    49,
                    1,
                    261,
                    0
                ],
                "title": "Autonomous Navigation in Ice-Covered Waters with Learned Predictions on\n  Ship-Ice Interactions",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Autonomous Navigation in Ice-Covered Waters with Learned Predictions on\n  Ship-Ice Interactions"
                },
                "summary": "Autonomous navigation in ice-covered waters poses significant challenges due\nto the frequent lack of viable collision-free trajectories. When complete\nobstacle avoidance is infeasible, it becomes imperative for the navigation\nstrategy to minimize collisions. Additionally, the dynamic nature of ice, which\nmoves in response to ship maneuvers, complicates the path planning process. To\naddress these challenges, we propose a novel deep learning model to estimate\nthe coarse dynamics of ice movements triggered by ship actions through\noccupancy estimation. To ensure real-time applicability, we propose a novel\napproach that caches intermediate prediction results and seamlessly integrates\nthe predictive model into a graph search planner. We evaluate the proposed\nplanner both in simulation and in a physical testbed against existing\napproaches and show that our planner significantly reduces collisions with ice\nwhen compared to the state-of-the-art. Codes and demos of this work are\navailable at https://github.com/IvanIZ/predictive-asv-planner.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Autonomous navigation in ice-covered waters poses significant challenges due\nto the frequent lack of viable collision-free trajectories. When complete\nobstacle avoidance is infeasible, it becomes imperative for the navigation\nstrategy to minimize collisions. Additionally, the dynamic nature of ice, which\nmoves in response to ship maneuvers, complicates the path planning process. To\naddress these challenges, we propose a novel deep learning model to estimate\nthe coarse dynamics of ice movements triggered by ship actions through\noccupancy estimation. To ensure real-time applicability, we propose a novel\napproach that caches intermediate prediction results and seamlessly integrates\nthe predictive model into a graph search planner. We evaluate the proposed\nplanner both in simulation and in a physical testbed against existing\napproaches and show that our planner significantly reduces collisions with ice\nwhen compared to the state-of-the-art. Codes and demos of this work are\navailable at https://github.com/IvanIZ/predictive-asv-planner."
                },
                "authors": [
                    {
                        "name": "Ninghan Zhong"
                    },
                    {
                        "name": "Alessandro Potenza"
                    },
                    {
                        "name": "Stephen L. Smith"
                    }
                ],
                "author_detail": {
                    "name": "Stephen L. Smith"
                },
                "author": "Stephen L. Smith",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.11326v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.11326v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.12021v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.12021v1",
                "updated": "2024-09-18T14:31:33Z",
                "updated_parsed": [
                    2024,
                    9,
                    18,
                    14,
                    31,
                    33,
                    2,
                    262,
                    0
                ],
                "published": "2024-09-18T14:31:33Z",
                "published_parsed": [
                    2024,
                    9,
                    18,
                    14,
                    31,
                    33,
                    2,
                    262,
                    0
                ],
                "title": "Optimal Offline ORAM with Perfect Security via Simple Oblivious Priority\n  Queues",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Optimal Offline ORAM with Perfect Security via Simple Oblivious Priority\n  Queues"
                },
                "summary": "Oblivious RAM (ORAM) is a well-researched primitive to hide the memory access\npattern of a RAM computation; it has a variety of applications in trusted\ncomputing, outsourced storage, and multiparty computation. In this paper, we\nstudy the so-called offline ORAM in which the sequence of memory access\nlocations to be hidden is known in advance. Apart from their theoretical\nsignificance, offline ORAMs can be used to construct efficient oblivious\nalgorithms.\n  We obtain the first optimal offline ORAM with perfect security from oblivious\npriority queues via time-forward processing. For this, we present a simple\nconstruction of an oblivious priority queue with perfect security. Our\nconstruction achieves an asymptotically optimal (amortized) runtime of\n$\\Theta(\\log N)$ per operation for a capacity of $N$ elements and is of\nindependent interest.\n  Building on our construction, we additionally present efficient\nexternal-memory instantiations of our oblivious, perfectly-secure construction:\nFor the cache-aware setting, we match the optimal I/O complexity of\n$\\Theta(\\frac{1}{B} \\log \\frac{N}{M})$ per operation (amortized), and for the\ncache-oblivious setting we achieve a near-optimal I/O complexity of\n$O(\\frac{1}{B} \\log \\frac{N}{M} \\log\\log_M N)$ per operation (amortized).",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Oblivious RAM (ORAM) is a well-researched primitive to hide the memory access\npattern of a RAM computation; it has a variety of applications in trusted\ncomputing, outsourced storage, and multiparty computation. In this paper, we\nstudy the so-called offline ORAM in which the sequence of memory access\nlocations to be hidden is known in advance. Apart from their theoretical\nsignificance, offline ORAMs can be used to construct efficient oblivious\nalgorithms.\n  We obtain the first optimal offline ORAM with perfect security from oblivious\npriority queues via time-forward processing. For this, we present a simple\nconstruction of an oblivious priority queue with perfect security. Our\nconstruction achieves an asymptotically optimal (amortized) runtime of\n$\\Theta(\\log N)$ per operation for a capacity of $N$ elements and is of\nindependent interest.\n  Building on our construction, we additionally present efficient\nexternal-memory instantiations of our oblivious, perfectly-secure construction:\nFor the cache-aware setting, we match the optimal I/O complexity of\n$\\Theta(\\frac{1}{B} \\log \\frac{N}{M})$ per operation (amortized), and for the\ncache-oblivious setting we achieve a near-optimal I/O complexity of\n$O(\\frac{1}{B} \\log \\frac{N}{M} \\log\\log_M N)$ per operation (amortized)."
                },
                "authors": [
                    {
                        "name": "Thore Thießen"
                    },
                    {
                        "name": "Jan Vahrenhold"
                    }
                ],
                "author_detail": {
                    "name": "Jan Vahrenhold"
                },
                "author": "Jan Vahrenhold",
                "arxiv_doi": "10.4230/LIPIcs.ISAAC.2024.36",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.4230/LIPIcs.ISAAC.2024.36",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2409.12021v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.12021v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "23 pages, full version of the paper to appear in ISAAC 2024",
                "arxiv_journal_ref": "Thore Thie{\\ss}en and Jan Vahrenhold. Optimal offline ORAM with\n  perfect security via simple oblivious priority queues. In 35th International\n  Symposium on Algorithms and Computation (ISAAC 2024), 19 pages. 2024",
                "arxiv_primary_category": {
                    "term": "cs.DS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.10516v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.10516v2",
                "updated": "2024-09-18T13:11:13Z",
                "updated_parsed": [
                    2024,
                    9,
                    18,
                    13,
                    11,
                    13,
                    2,
                    262,
                    0
                ],
                "published": "2024-09-16T17:59:52Z",
                "published_parsed": [
                    2024,
                    9,
                    16,
                    17,
                    59,
                    52,
                    0,
                    260,
                    0
                ],
                "title": "RetrievalAttention: Accelerating Long-Context LLM Inference via Vector\n  Retrieval",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RetrievalAttention: Accelerating Long-Context LLM Inference via Vector\n  Retrieval"
                },
                "summary": "Transformer-based Large Language Models (LLMs) have become increasingly\nimportant. However, due to the quadratic time complexity of attention\ncomputation, scaling LLMs to longer contexts incurs extremely slow inference\nlatency and high GPU memory consumption for caching key-value (KV) vectors.\nThis paper proposes RetrievalAttention, a training-free approach to both\naccelerate attention computation and reduce GPU memory consumption. By\nleveraging the dynamic sparsity of attention mechanism, RetrievalAttention\nproposes to use approximate nearest neighbor search (ANNS) indexes for KV\nvectors in CPU memory and retrieves the most relevant ones with vector search\nduring generation. Unfortunately, we observe that the off-the-shelf ANNS\nindexes are often ineffective for such retrieval tasks due to the\nout-of-distribution (OOD) between query vectors and key vectors in attention\nmechanism. RetrievalAttention addresses the OOD challenge by designing an\nattention-aware vector search algorithm that can adapt to the distribution of\nquery vectors. Our evaluation shows that RetrievalAttention only needs to\naccess 1--3% of data while maintaining high model accuracy. This leads to\nsignificant reduction in the inference cost of long-context LLMs with much\nlower GPU memory footprint. In particular, RetrievalAttention only needs a\nsingle NVIDIA RTX4090 (24GB) for serving 128K tokens in LLMs with 8B\nparameters, which is capable of generating one token in 0.188 seconds.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Transformer-based Large Language Models (LLMs) have become increasingly\nimportant. However, due to the quadratic time complexity of attention\ncomputation, scaling LLMs to longer contexts incurs extremely slow inference\nlatency and high GPU memory consumption for caching key-value (KV) vectors.\nThis paper proposes RetrievalAttention, a training-free approach to both\naccelerate attention computation and reduce GPU memory consumption. By\nleveraging the dynamic sparsity of attention mechanism, RetrievalAttention\nproposes to use approximate nearest neighbor search (ANNS) indexes for KV\nvectors in CPU memory and retrieves the most relevant ones with vector search\nduring generation. Unfortunately, we observe that the off-the-shelf ANNS\nindexes are often ineffective for such retrieval tasks due to the\nout-of-distribution (OOD) between query vectors and key vectors in attention\nmechanism. RetrievalAttention addresses the OOD challenge by designing an\nattention-aware vector search algorithm that can adapt to the distribution of\nquery vectors. Our evaluation shows that RetrievalAttention only needs to\naccess 1--3% of data while maintaining high model accuracy. This leads to\nsignificant reduction in the inference cost of long-context LLMs with much\nlower GPU memory footprint. In particular, RetrievalAttention only needs a\nsingle NVIDIA RTX4090 (24GB) for serving 128K tokens in LLMs with 8B\nparameters, which is capable of generating one token in 0.188 seconds."
                },
                "authors": [
                    {
                        "name": "Di Liu"
                    },
                    {
                        "name": "Meng Chen"
                    },
                    {
                        "name": "Baotong Lu"
                    },
                    {
                        "name": "Huiqiang Jiang"
                    },
                    {
                        "name": "Zhenhua Han"
                    },
                    {
                        "name": "Qianxi Zhang"
                    },
                    {
                        "name": "Qi Chen"
                    },
                    {
                        "name": "Chengruidong Zhang"
                    },
                    {
                        "name": "Bailu Ding"
                    },
                    {
                        "name": "Kai Zhang"
                    },
                    {
                        "name": "Chen Chen"
                    },
                    {
                        "name": "Fan Yang"
                    },
                    {
                        "name": "Yuqing Yang"
                    },
                    {
                        "name": "Lili Qiu"
                    }
                ],
                "author_detail": {
                    "name": "Lili Qiu"
                },
                "author": "Lili Qiu",
                "arxiv_comment": "16 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.10516v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.10516v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.10687v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.10687v2",
                "updated": "2024-09-18T08:22:23Z",
                "updated_parsed": [
                    2024,
                    9,
                    18,
                    8,
                    22,
                    23,
                    2,
                    262,
                    0
                ],
                "published": "2024-05-17T10:40:33Z",
                "published_parsed": [
                    2024,
                    5,
                    17,
                    10,
                    40,
                    33,
                    4,
                    138,
                    0
                ],
                "title": "Proportional scintillation in liquid xenon: demonstration in a\n  single-phase liquid-only time projection chamber",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Proportional scintillation in liquid xenon: demonstration in a\n  single-phase liquid-only time projection chamber"
                },
                "summary": "The largest direct dark matter search experiments to date employ dual-phase\ntime projection chambers (TPCs) with liquid noble gas targets. These detect\nboth the primary photons generated by particle interactions in the liquid\ntarget, as well as proportional secondary scintillation light created by the\nionization electrons in a strong electric field in the gas phase between the\nliquid-gas interface and the anode. In this work, we describe the detection of\ncharge signals in a small-scale single-phase liquid-xenon-only TPC, that\nfeatures the well-established TPC geometry with light readout above and below a\ncylindrical target. In the single-phase TPC, the proportional scintillation\nlight (S2) is generated in liquid xenon in close proximity to 10 {\\mu}m\ndiameter anode wires. The detector was characterized and the proportional\nscintillation process was studied using the 32.1 keV and 9.4 keV signals from\n83mKr decays. A charge gain factor g2 of up to (1.9 $\\pm$ 0.3) PE/electron was\nreached at an anode voltage 4.4 kV higher than the gate electrode 5 mm below\nit, corresponding to (29 $\\pm$ 6) photons emitted per ionization electron. The\nduration of S2 signals is dominated by electron diffusion and approaches the\nxenon de-excitation timescale for very short electron drift times. The electron\ndrift velocity and the longitudinal diffusion constant were measured at a drift\nfield of 470 V/cm. The results agree with the literature and demonstrate that a\nsingle-phase TPC can be operated successfully.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The largest direct dark matter search experiments to date employ dual-phase\ntime projection chambers (TPCs) with liquid noble gas targets. These detect\nboth the primary photons generated by particle interactions in the liquid\ntarget, as well as proportional secondary scintillation light created by the\nionization electrons in a strong electric field in the gas phase between the\nliquid-gas interface and the anode. In this work, we describe the detection of\ncharge signals in a small-scale single-phase liquid-xenon-only TPC, that\nfeatures the well-established TPC geometry with light readout above and below a\ncylindrical target. In the single-phase TPC, the proportional scintillation\nlight (S2) is generated in liquid xenon in close proximity to 10 {\\mu}m\ndiameter anode wires. The detector was characterized and the proportional\nscintillation process was studied using the 32.1 keV and 9.4 keV signals from\n83mKr decays. A charge gain factor g2 of up to (1.9 $\\pm$ 0.3) PE/electron was\nreached at an anode voltage 4.4 kV higher than the gate electrode 5 mm below\nit, corresponding to (29 $\\pm$ 6) photons emitted per ionization electron. The\nduration of S2 signals is dominated by electron diffusion and approaches the\nxenon de-excitation timescale for very short electron drift times. The electron\ndrift velocity and the longitudinal diffusion constant were measured at a drift\nfield of 470 V/cm. The results agree with the literature and demonstrate that a\nsingle-phase TPC can be operated successfully."
                },
                "authors": [
                    {
                        "name": "Florian Tönnies"
                    },
                    {
                        "name": "Adam Brown"
                    },
                    {
                        "name": "Baris Kiyim"
                    },
                    {
                        "name": "Fabian Kuger"
                    },
                    {
                        "name": "Sebastian Lindemann"
                    },
                    {
                        "name": "Patrick Meinhardt"
                    },
                    {
                        "name": "Marc Schumann"
                    },
                    {
                        "name": "Andrew Stevens"
                    }
                ],
                "author_detail": {
                    "name": "Andrew Stevens"
                },
                "author": "Andrew Stevens",
                "arxiv_comment": "20 pages, 14 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.10687v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.10687v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.ins-det",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.ins-det",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2312.05821v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2312.05821v3",
                "updated": "2024-09-18T04:53:46Z",
                "updated_parsed": [
                    2024,
                    9,
                    18,
                    4,
                    53,
                    46,
                    2,
                    262,
                    0
                ],
                "published": "2023-12-10T08:41:24Z",
                "published_parsed": [
                    2023,
                    12,
                    10,
                    8,
                    41,
                    24,
                    6,
                    344,
                    0
                ],
                "title": "ASVD: Activation-aware Singular Value Decomposition for Compressing\n  Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ASVD: Activation-aware Singular Value Decomposition for Compressing\n  Large Language Models"
                },
                "summary": "In this paper, we introduce a new post-training compression paradigm for\nLarge Language Models (LLMs) to facilitate their wider adoption. We delve into\nLLM weight low-rank factorization, and find that the challenges of this task\nstem from the outlier phenomenon in the LLM activations and the sensitivity\ndifference among various kinds of layers. To address these issues, we propose a\ntraining-free approach called Activation-aware Singular Value Decomposition\n(ASVD). Specifically, ASVD manages activation outliers by scaling the weight\nmatrix based on the activation distribution, thereby enhancing decomposition\naccuracy. Additionally, we propose an efficient iterative calibration process\nto optimize layer-specific decomposition by addressing the varying sensitivity\nof different LLM layers. ASVD can compress a network by 10-20%, without\ncompromising the performance of LLMs. Based on the success of the low-rank\ndecomposition of projection matrices in the self-attention module, we further\nintroduce ASVD to compress the KV cache. By reducing the channel dimension of\nKV activations, memory requirements for KV cache can be largely reduced. Thanks\nto the 50-75% reduction in the rank of the KV projection matrices, ASVD can\nfurther achieve 50% KV cache reductions without performance drop in a\ntraining-free manner.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this paper, we introduce a new post-training compression paradigm for\nLarge Language Models (LLMs) to facilitate their wider adoption. We delve into\nLLM weight low-rank factorization, and find that the challenges of this task\nstem from the outlier phenomenon in the LLM activations and the sensitivity\ndifference among various kinds of layers. To address these issues, we propose a\ntraining-free approach called Activation-aware Singular Value Decomposition\n(ASVD). Specifically, ASVD manages activation outliers by scaling the weight\nmatrix based on the activation distribution, thereby enhancing decomposition\naccuracy. Additionally, we propose an efficient iterative calibration process\nto optimize layer-specific decomposition by addressing the varying sensitivity\nof different LLM layers. ASVD can compress a network by 10-20%, without\ncompromising the performance of LLMs. Based on the success of the low-rank\ndecomposition of projection matrices in the self-attention module, we further\nintroduce ASVD to compress the KV cache. By reducing the channel dimension of\nKV activations, memory requirements for KV cache can be largely reduced. Thanks\nto the 50-75% reduction in the rank of the KV projection matrices, ASVD can\nfurther achieve 50% KV cache reductions without performance drop in a\ntraining-free manner."
                },
                "authors": [
                    {
                        "name": "Zhihang Yuan"
                    },
                    {
                        "name": "Yuzhang Shang"
                    },
                    {
                        "name": "Yue Song"
                    },
                    {
                        "name": "Qiang Wu"
                    },
                    {
                        "name": "Yan Yan"
                    },
                    {
                        "name": "Guangyu Sun"
                    }
                ],
                "author_detail": {
                    "name": "Guangyu Sun"
                },
                "author": "Guangyu Sun",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2312.05821v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2312.05821v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.11600v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.11600v1",
                "updated": "2024-09-17T23:15:39Z",
                "updated_parsed": [
                    2024,
                    9,
                    17,
                    23,
                    15,
                    39,
                    1,
                    261,
                    0
                ],
                "published": "2024-09-17T23:15:39Z",
                "published_parsed": [
                    2024,
                    9,
                    17,
                    23,
                    15,
                    39,
                    1,
                    261,
                    0
                ],
                "title": "No Saved Kaleidosope: an 100% Jitted Neural Network Coding Language with\n  Pythonic Syntax",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "No Saved Kaleidosope: an 100% Jitted Neural Network Coding Language with\n  Pythonic Syntax"
                },
                "summary": "We developed a jitted compiler for training Artificial Neural Networks using\nC++, LLVM and Cuda. It features object-oriented characteristics, strong typing,\nparallel workers for data pre-processing, pythonic syntax for expressions,\nPyTorch like model declaration and Automatic Differentiation. We implement the\nmechanisms of cache and pooling in order to manage VRAM, cuBLAS for high\nperformance matrix multiplication and cuDNN for convolutional layers. Our\nexperiments with Residual Convolutional Neural Networks on ImageNet, we reach\nsimilar speed but degraded performance. Also, the GRU network experiments show\nsimilar accuracy, but our compiler have degraded speed in that task. However,\nour compiler demonstrates promising results at the CIFAR-10 benchmark, in which\nwe reach the same performance and about the same speed as PyTorch. We make the\ncode publicly available at: https://github.com/NoSavedDATA/NoSavedKaleidoscope",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We developed a jitted compiler for training Artificial Neural Networks using\nC++, LLVM and Cuda. It features object-oriented characteristics, strong typing,\nparallel workers for data pre-processing, pythonic syntax for expressions,\nPyTorch like model declaration and Automatic Differentiation. We implement the\nmechanisms of cache and pooling in order to manage VRAM, cuBLAS for high\nperformance matrix multiplication and cuDNN for convolutional layers. Our\nexperiments with Residual Convolutional Neural Networks on ImageNet, we reach\nsimilar speed but degraded performance. Also, the GRU network experiments show\nsimilar accuracy, but our compiler have degraded speed in that task. However,\nour compiler demonstrates promising results at the CIFAR-10 benchmark, in which\nwe reach the same performance and about the same speed as PyTorch. We make the\ncode publicly available at: https://github.com/NoSavedDATA/NoSavedKaleidoscope"
                },
                "authors": [
                    {
                        "name": "Augusto Seben da Rosa"
                    },
                    {
                        "name": "Marlon Daniel Angeli"
                    },
                    {
                        "name": "Jorge Aikes Junior"
                    },
                    {
                        "name": "Alef Iury Ferreira"
                    },
                    {
                        "name": "Lucas Rafael Gris"
                    },
                    {
                        "name": "Anderson da Silva Soares"
                    },
                    {
                        "name": "Arnaldo Candido Junior"
                    },
                    {
                        "name": "Frederico Santos de Oliveira"
                    },
                    {
                        "name": "Gabriel Trevisan Damke"
                    },
                    {
                        "name": "Rafael Teixeira Sousa"
                    }
                ],
                "author_detail": {
                    "name": "Rafael Teixeira Sousa"
                },
                "author": "Rafael Teixeira Sousa",
                "arxiv_comment": "12 pages, 3 figures and 3 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.11600v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.11600v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.PL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.PL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "68T07",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "D.3; I.2; I.4; I.7",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.11258v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.11258v1",
                "updated": "2024-09-17T15:07:05Z",
                "updated_parsed": [
                    2024,
                    9,
                    17,
                    15,
                    7,
                    5,
                    1,
                    261,
                    0
                ],
                "published": "2024-09-17T15:07:05Z",
                "published_parsed": [
                    2024,
                    9,
                    17,
                    15,
                    7,
                    5,
                    1,
                    261,
                    0
                ],
                "title": "Attacking Slicing Network via Side-channel Reinforcement Learning Attack",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Attacking Slicing Network via Side-channel Reinforcement Learning Attack"
                },
                "summary": "Network slicing in 5G and the future 6G networks will enable the creation of\nmultiple virtualized networks on a shared physical infrastructure. This\ninnovative approach enables the provision of tailored networks to accommodate\nspecific business types or industry users, thus delivering more customized and\nefficient services. However, the shared memory and cache in network slicing\nintroduce security vulnerabilities that have yet to be fully addressed. In this\npaper, we introduce a reinforcement learning-based side-channel cache attack\nframework specifically designed for network slicing environments. Unlike\ntraditional cache attack methods, our framework leverages reinforcement\nlearning to dynamically identify and exploit cache locations storing sensitive\ninformation, such as authentication keys and user registration data. We assume\nthat one slice network is compromised and demonstrate how the attacker can\ninduce another shared slice to send registration requests, thereby estimating\nthe cache locations of critical data. By formulating the cache timing channel\nattack as a reinforcement learning-driven guessing game between the attack\nslice and the victim slice, our model efficiently explores possible actions to\npinpoint memory blocks containing sensitive information. Experimental results\nshowcase the superiority of our approach, achieving a success rate of\napproximately 95\\% to 98\\% in accurately identifying the storage locations of\nsensitive data. This high level of accuracy underscores the potential risks in\nshared network slicing environments and highlights the need for robust security\nmeasures to safeguard against such advanced side-channel attacks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Network slicing in 5G and the future 6G networks will enable the creation of\nmultiple virtualized networks on a shared physical infrastructure. This\ninnovative approach enables the provision of tailored networks to accommodate\nspecific business types or industry users, thus delivering more customized and\nefficient services. However, the shared memory and cache in network slicing\nintroduce security vulnerabilities that have yet to be fully addressed. In this\npaper, we introduce a reinforcement learning-based side-channel cache attack\nframework specifically designed for network slicing environments. Unlike\ntraditional cache attack methods, our framework leverages reinforcement\nlearning to dynamically identify and exploit cache locations storing sensitive\ninformation, such as authentication keys and user registration data. We assume\nthat one slice network is compromised and demonstrate how the attacker can\ninduce another shared slice to send registration requests, thereby estimating\nthe cache locations of critical data. By formulating the cache timing channel\nattack as a reinforcement learning-driven guessing game between the attack\nslice and the victim slice, our model efficiently explores possible actions to\npinpoint memory blocks containing sensitive information. Experimental results\nshowcase the superiority of our approach, achieving a success rate of\napproximately 95\\% to 98\\% in accurately identifying the storage locations of\nsensitive data. This high level of accuracy underscores the potential risks in\nshared network slicing environments and highlights the need for robust security\nmeasures to safeguard against such advanced side-channel attacks."
                },
                "authors": [
                    {
                        "name": "Wei Shao"
                    },
                    {
                        "name": "Chandra Thapa"
                    },
                    {
                        "name": "Rayne Holland"
                    },
                    {
                        "name": "Sarah Ali Siddiqui"
                    },
                    {
                        "name": "Seyit Camtepe"
                    }
                ],
                "author_detail": {
                    "name": "Seyit Camtepe"
                },
                "author": "Seyit Camtepe",
                "arxiv_comment": "9 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.11258v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.11258v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.11102v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.11102v1",
                "updated": "2024-09-17T11:54:24Z",
                "updated_parsed": [
                    2024,
                    9,
                    17,
                    11,
                    54,
                    24,
                    1,
                    261,
                    0
                ],
                "published": "2024-09-17T11:54:24Z",
                "published_parsed": [
                    2024,
                    9,
                    17,
                    11,
                    54,
                    24,
                    1,
                    261,
                    0
                ],
                "title": "Electron-beam-induced adatom-vacancy-complexes in mono- and bilayer\n  phosphorene",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Electron-beam-induced adatom-vacancy-complexes in mono- and bilayer\n  phosphorene"
                },
                "summary": "Phosphorene, a puckered two-dimensional allotrope of phosphorus, has sparked\nconsiderable interest in recent years due to its potential especially for\noptoelectronic applications with its layer-number-dependant direct band gap and\nstrongly bound excitons. However, detailed experimental characterization of its\nintrinsic defects as well as its defect creation characteristics under electron\nirradiation are scarce. Here, we report on the creation and stability of a\nvariety of defect configurations under 60 kV electron irradiation in mono- and\nbilayer phosphorene including the first experimental reports of stable\nadatom-vacancy-complexes. Displacement cross section measurements in bilayer\nphosphorene yield a value of 7.7 +- 1.4 barn with an estimated lifetime of\nadatom-vacancy-complexes of 19.9 +- 0.7 s, while some are stable for up to 68 s\nunder continuous electron irradiation. Surprisingly, ab initio-based\nsimulations indicate that the complexes should readily recombine, even in\nstructures strained by up to 3 %. The presented results will help to improve\nthe understanding of the wide variety of defects in phosphorene, their\ncreation, and their stability, which may enable new pathways for defect\nengineered phosphorene devices.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Phosphorene, a puckered two-dimensional allotrope of phosphorus, has sparked\nconsiderable interest in recent years due to its potential especially for\noptoelectronic applications with its layer-number-dependant direct band gap and\nstrongly bound excitons. However, detailed experimental characterization of its\nintrinsic defects as well as its defect creation characteristics under electron\nirradiation are scarce. Here, we report on the creation and stability of a\nvariety of defect configurations under 60 kV electron irradiation in mono- and\nbilayer phosphorene including the first experimental reports of stable\nadatom-vacancy-complexes. Displacement cross section measurements in bilayer\nphosphorene yield a value of 7.7 +- 1.4 barn with an estimated lifetime of\nadatom-vacancy-complexes of 19.9 +- 0.7 s, while some are stable for up to 68 s\nunder continuous electron irradiation. Surprisingly, ab initio-based\nsimulations indicate that the complexes should readily recombine, even in\nstructures strained by up to 3 %. The presented results will help to improve\nthe understanding of the wide variety of defects in phosphorene, their\ncreation, and their stability, which may enable new pathways for defect\nengineered phosphorene devices."
                },
                "authors": [
                    {
                        "name": "Carsten Speckmann"
                    },
                    {
                        "name": "Andrea Angeletti"
                    },
                    {
                        "name": "Lukáš Kývala"
                    },
                    {
                        "name": "David Lamprecht"
                    },
                    {
                        "name": "Felix Herterich"
                    },
                    {
                        "name": "Clemens Mangler"
                    },
                    {
                        "name": "Lado Filipovic"
                    },
                    {
                        "name": "Christoph Dellago"
                    },
                    {
                        "name": "Cesare Franchini"
                    },
                    {
                        "name": "Jani Kotakoski"
                    }
                ],
                "author_detail": {
                    "name": "Jani Kotakoski"
                },
                "author": "Jani Kotakoski",
                "arxiv_comment": "11 pages, 7 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.11102v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.11102v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cond-mat.mes-hall",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cond-mat.mes-hall",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cond-mat.mtrl-sci",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.11057v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.11057v1",
                "updated": "2024-09-17T10:35:30Z",
                "updated_parsed": [
                    2024,
                    9,
                    17,
                    10,
                    35,
                    30,
                    1,
                    261,
                    0
                ],
                "published": "2024-09-17T10:35:30Z",
                "published_parsed": [
                    2024,
                    9,
                    17,
                    10,
                    35,
                    30,
                    1,
                    261,
                    0
                ],
                "title": "KVPruner: Structural Pruning for Faster and Memory-Efficient Large\n  Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "KVPruner: Structural Pruning for Faster and Memory-Efficient Large\n  Language Models"
                },
                "summary": "The bottleneck associated with the key-value(KV) cache presents a significant\nchallenge during the inference processes of large language models. While depth\npruning accelerates inference, it requires extensive recovery training, which\ncan take up to two weeks. On the other hand, width pruning retains much of the\nperformance but offers slight speed gains. To tackle these challenges, we\npropose KVPruner to improve model efficiency while maintaining performance. Our\nmethod uses global perplexity-based analysis to determine the importance ratio\nfor each block and provides multiple strategies to prune non-essential KV\nchannels within blocks. Compared to the original model, KVPruner reduces\nruntime memory usage by 50% and boosts throughput by over 35%. Additionally,\nour method requires only two hours of LoRA fine-tuning on small datasets to\nrecover most of the performance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The bottleneck associated with the key-value(KV) cache presents a significant\nchallenge during the inference processes of large language models. While depth\npruning accelerates inference, it requires extensive recovery training, which\ncan take up to two weeks. On the other hand, width pruning retains much of the\nperformance but offers slight speed gains. To tackle these challenges, we\npropose KVPruner to improve model efficiency while maintaining performance. Our\nmethod uses global perplexity-based analysis to determine the importance ratio\nfor each block and provides multiple strategies to prune non-essential KV\nchannels within blocks. Compared to the original model, KVPruner reduces\nruntime memory usage by 50% and boosts throughput by over 35%. Additionally,\nour method requires only two hours of LoRA fine-tuning on small datasets to\nrecover most of the performance."
                },
                "authors": [
                    {
                        "name": "Bo Lv"
                    },
                    {
                        "name": "Quan Zhou"
                    },
                    {
                        "name": "Xuanang Ding"
                    },
                    {
                        "name": "Yan Wang"
                    },
                    {
                        "name": "Zeming Ma"
                    }
                ],
                "author_detail": {
                    "name": "Zeming Ma"
                },
                "author": "Zeming Ma",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.11057v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.11057v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.10946v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.10946v1",
                "updated": "2024-09-17T07:28:56Z",
                "updated_parsed": [
                    2024,
                    9,
                    17,
                    7,
                    28,
                    56,
                    1,
                    261,
                    0
                ],
                "published": "2024-09-17T07:28:56Z",
                "published_parsed": [
                    2024,
                    9,
                    17,
                    7,
                    28,
                    56,
                    1,
                    261,
                    0
                ],
                "title": "Skip TLB flushes for reused pages within mmap's",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Skip TLB flushes for reused pages within mmap's"
                },
                "summary": "Memory access efficiency is significantly enhanced by caching recent address\ntranslations in the CPUs' Translation Lookaside Buffers (TLBs). However, since\nthe operating system is not aware of which core is using a particular mapping,\nit flushes TLB entries across all cores where the application runs whenever\naddresses are unmapped, ensuring security and consistency. These TLB flushes,\nknown as TLB shootdowns, are costly and create a performance and scalability\nbottleneck. A key contributor to TLB shootdowns is memory-mapped I/O,\nparticularly during mmap-munmap cycles and page cache evictions. Often, the\nsame physical pages are reassigned to the same process post-eviction,\npresenting an opportunity for the operating system to reduce the frequency of\nTLB shootdowns. We demonstrate, that by slightly extending the mmap function,\nTLB shootdowns for these \"recycled pages\" can be avoided.\n  Therefore we introduce and implement the \"fast page recycling\" (FPR) feature\nwithin the mmap system call. FPR-mmaps maintain security by only triggering TLB\nshootdowns when a page exits its recycling cycle and is allocated to a\ndifferent process. To ensure consistency when FPR-mmap pointers are used, we\nmade minor adjustments to virtual memory management to avoid the ABA problem.\nUnlike previous methods to mitigate shootdown effects, our approach does not\nrequire any hardware modifications and operates transparently within the\nexisting Linux virtual memory framework.\n  Our evaluations across a variety of CPU, memory, and storage setups,\nincluding persistent memory and Optane SSDs, demonstrate that FPR delivers\nnotable performance gains, with improvements of up to 28% in real-world\napplications and 92% in micro-benchmarks. Additionally, we show that TLB\nshootdowns are a significant source of bottlenecks, previously misattributed to\nother components of the Linux kernel.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Memory access efficiency is significantly enhanced by caching recent address\ntranslations in the CPUs' Translation Lookaside Buffers (TLBs). However, since\nthe operating system is not aware of which core is using a particular mapping,\nit flushes TLB entries across all cores where the application runs whenever\naddresses are unmapped, ensuring security and consistency. These TLB flushes,\nknown as TLB shootdowns, are costly and create a performance and scalability\nbottleneck. A key contributor to TLB shootdowns is memory-mapped I/O,\nparticularly during mmap-munmap cycles and page cache evictions. Often, the\nsame physical pages are reassigned to the same process post-eviction,\npresenting an opportunity for the operating system to reduce the frequency of\nTLB shootdowns. We demonstrate, that by slightly extending the mmap function,\nTLB shootdowns for these \"recycled pages\" can be avoided.\n  Therefore we introduce and implement the \"fast page recycling\" (FPR) feature\nwithin the mmap system call. FPR-mmaps maintain security by only triggering TLB\nshootdowns when a page exits its recycling cycle and is allocated to a\ndifferent process. To ensure consistency when FPR-mmap pointers are used, we\nmade minor adjustments to virtual memory management to avoid the ABA problem.\nUnlike previous methods to mitigate shootdown effects, our approach does not\nrequire any hardware modifications and operates transparently within the\nexisting Linux virtual memory framework.\n  Our evaluations across a variety of CPU, memory, and storage setups,\nincluding persistent memory and Optane SSDs, demonstrate that FPR delivers\nnotable performance gains, with improvements of up to 28% in real-world\napplications and 92% in micro-benchmarks. Additionally, we show that TLB\nshootdowns are a significant source of bottlenecks, previously misattributed to\nother components of the Linux kernel."
                },
                "authors": [
                    {
                        "name": "Frederic Schimmelpfennig"
                    },
                    {
                        "name": "André Brinkmann"
                    },
                    {
                        "name": "Hossein Asadi"
                    },
                    {
                        "name": "Reza Salkhordeh"
                    }
                ],
                "author_detail": {
                    "name": "Reza Salkhordeh"
                },
                "author": "Reza Salkhordeh",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.10946v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.10946v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.OS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.OS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.09417v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.09417v2",
                "updated": "2024-09-17T04:39:04Z",
                "updated_parsed": [
                    2024,
                    9,
                    17,
                    4,
                    39,
                    4,
                    1,
                    261,
                    0
                ],
                "published": "2024-09-14T11:15:38Z",
                "published_parsed": [
                    2024,
                    9,
                    14,
                    11,
                    15,
                    38,
                    5,
                    258,
                    0
                ],
                "title": "Resources on the Move for Smart City: A Disruptive Perspective on the\n  Grand Convergence of Sensing, Communications, Computing, Storage, and\n  Intelligence",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Resources on the Move for Smart City: A Disruptive Perspective on the\n  Grand Convergence of Sensing, Communications, Computing, Storage, and\n  Intelligence"
                },
                "summary": "The most commonly seen things on streets in any city are vehicles. However,\nmost of them are used to transport people or goods. What if they also carry\nresources and capabilities for sensing, communications, computing, storage, and\nintelligence (SCCSI)? We will have a web of sensors to monitor the city, a\nnetwork of powerful communicators to transport data around, a grid of computing\npower to conduct data analytics and machine learning (ML), a network of\ndistributed storage to buffer/cache data/job for optimization, and a set of\nmovable AI/ML toolboxes made available for specialized smart applications. This\nperspective article presents how to leverage SCCSI-empowered vehicles to design\nsuch a service network, simply called SCCSI network, to help build a smart city\nwith a cost-effective and sustainable solution. It showcases how\nmulti-dimensional technologies, namely, sensing, communications, computing,\nstorage, and intelligence, converge to a unifying technology to solve grand\nchallenges for resource demands from emerging large-scale applications. Thus,\nwith SCCSI-empowered vehicles on the ground, over the air, and on the sea,\nSCCSI network can make resources and capabilities on the move, practically\npushing SCCSI services to the edge! We hope this article serves as a spark to\nstimulate more disruptive thinking to address grand challenges of paramount\nimportance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The most commonly seen things on streets in any city are vehicles. However,\nmost of them are used to transport people or goods. What if they also carry\nresources and capabilities for sensing, communications, computing, storage, and\nintelligence (SCCSI)? We will have a web of sensors to monitor the city, a\nnetwork of powerful communicators to transport data around, a grid of computing\npower to conduct data analytics and machine learning (ML), a network of\ndistributed storage to buffer/cache data/job for optimization, and a set of\nmovable AI/ML toolboxes made available for specialized smart applications. This\nperspective article presents how to leverage SCCSI-empowered vehicles to design\nsuch a service network, simply called SCCSI network, to help build a smart city\nwith a cost-effective and sustainable solution. It showcases how\nmulti-dimensional technologies, namely, sensing, communications, computing,\nstorage, and intelligence, converge to a unifying technology to solve grand\nchallenges for resource demands from emerging large-scale applications. Thus,\nwith SCCSI-empowered vehicles on the ground, over the air, and on the sea,\nSCCSI network can make resources and capabilities on the move, practically\npushing SCCSI services to the edge! We hope this article serves as a spark to\nstimulate more disruptive thinking to address grand challenges of paramount\nimportance."
                },
                "authors": [
                    {
                        "name": "Yuguang Fang"
                    },
                    {
                        "name": "Yiqin Deng"
                    },
                    {
                        "name": "Xianhao Chen"
                    }
                ],
                "author_detail": {
                    "name": "Xianhao Chen"
                },
                "author": "Xianhao Chen",
                "arxiv_comment": "8 pages, 3 figures. Accepted by IEEE Communications Magazine",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.09417v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.09417v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.10593v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.10593v2",
                "updated": "2024-09-21T13:01:43Z",
                "updated_parsed": [
                    2024,
                    9,
                    21,
                    13,
                    1,
                    43,
                    5,
                    265,
                    0
                ],
                "published": "2024-09-16T17:36:50Z",
                "published_parsed": [
                    2024,
                    9,
                    16,
                    17,
                    36,
                    50,
                    0,
                    260,
                    0
                ],
                "title": "CSKV: Training-Efficient Channel Shrinking for KV Cache in Long-Context\n  Scenarios",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CSKV: Training-Efficient Channel Shrinking for KV Cache in Long-Context\n  Scenarios"
                },
                "summary": "Large Language Models (LLMs) have been widely adopted to process long-context\ntasks. However, the large memory overhead of the key-value (KV) cache poses\nsignificant challenges in long-context scenarios. Existing training-free KV\ncache compression methods typically focus on quantization and token pruning,\nwhich have compression limits, and excessive sparsity can lead to severe\nperformance degradation. Other methods design new architectures with less KV\noverhead but require significant training overhead. To address the above two\ndrawbacks, we further explore the redundancy in the channel dimension and apply\nan architecture-level design with minor training costs. Therefore, we introduce\nCSKV, a training-efficient Channel Shrinking technique for KV cache\ncompression: (1) We first analyze the singular value distribution of the KV\ncache, revealing significant redundancy and compression potential along the\nchannel dimension. Based on this observation, we propose using low-rank\ndecomposition for key and value layers and storing the low-dimension features.\n(2) To preserve model performance, we introduce a bi-branch KV cache, including\na window-based full-precision KV cache and a low-precision compressed KV cache.\n(3) To reduce the training costs, we minimize the layer-wise reconstruction\nloss for the compressed KV cache instead of retraining the entire LLMs.\nExtensive experiments show that CSKV can reduce the memory overhead of the KV\ncache by 80% while maintaining the model's long-context capability. Moreover,\nwe show that our method can be seamlessly combined with quantization to further\nreduce the memory overhead, achieving a compression ratio of up to 95%.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have been widely adopted to process long-context\ntasks. However, the large memory overhead of the key-value (KV) cache poses\nsignificant challenges in long-context scenarios. Existing training-free KV\ncache compression methods typically focus on quantization and token pruning,\nwhich have compression limits, and excessive sparsity can lead to severe\nperformance degradation. Other methods design new architectures with less KV\noverhead but require significant training overhead. To address the above two\ndrawbacks, we further explore the redundancy in the channel dimension and apply\nan architecture-level design with minor training costs. Therefore, we introduce\nCSKV, a training-efficient Channel Shrinking technique for KV cache\ncompression: (1) We first analyze the singular value distribution of the KV\ncache, revealing significant redundancy and compression potential along the\nchannel dimension. Based on this observation, we propose using low-rank\ndecomposition for key and value layers and storing the low-dimension features.\n(2) To preserve model performance, we introduce a bi-branch KV cache, including\na window-based full-precision KV cache and a low-precision compressed KV cache.\n(3) To reduce the training costs, we minimize the layer-wise reconstruction\nloss for the compressed KV cache instead of retraining the entire LLMs.\nExtensive experiments show that CSKV can reduce the memory overhead of the KV\ncache by 80% while maintaining the model's long-context capability. Moreover,\nwe show that our method can be seamlessly combined with quantization to further\nreduce the memory overhead, achieving a compression ratio of up to 95%."
                },
                "authors": [
                    {
                        "name": "Luning Wang"
                    },
                    {
                        "name": "Shiyao Li"
                    },
                    {
                        "name": "Xuefei Ning"
                    },
                    {
                        "name": "Zhihang Yuan"
                    },
                    {
                        "name": "Shengen Yan"
                    },
                    {
                        "name": "Guohao Dai"
                    },
                    {
                        "name": "Yu Wang"
                    }
                ],
                "author_detail": {
                    "name": "Yu Wang"
                },
                "author": "Yu Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.10593v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.10593v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.10287v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.10287v1",
                "updated": "2024-09-16T13:52:46Z",
                "updated_parsed": [
                    2024,
                    9,
                    16,
                    13,
                    52,
                    46,
                    0,
                    260,
                    0
                ],
                "published": "2024-09-16T13:52:46Z",
                "published_parsed": [
                    2024,
                    9,
                    16,
                    13,
                    52,
                    46,
                    0,
                    260,
                    0
                ],
                "title": "Ejected Particles after Impact Splash on Mars: Electrification",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Ejected Particles after Impact Splash on Mars: Electrification"
                },
                "summary": "Within the RoadMap project we investigated the microphysical aspects of\nparticle collisions during saltation on the Martian surface in laboratory\nexperiments. Following the size distribution of ejected particles, their\naerodynamic properties and aggregation status upon ejection, we now focus on\nthe electrification and charge distribution of ejected particles. We analyzed\nrebound and ejection trajectories of grains in a vacuum setup with a strong\nelectric field of 100 kV/m and deduced particle charges from their\nacceleration. The ejected particles have sizes of about 10 to 100 microns. They\ncarry charges up to $10^5$ e or charge densities up to $> 10^7$ e/mm$^2$.\nWithin the given size range, we find a small bias towards positive charges.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Within the RoadMap project we investigated the microphysical aspects of\nparticle collisions during saltation on the Martian surface in laboratory\nexperiments. Following the size distribution of ejected particles, their\naerodynamic properties and aggregation status upon ejection, we now focus on\nthe electrification and charge distribution of ejected particles. We analyzed\nrebound and ejection trajectories of grains in a vacuum setup with a strong\nelectric field of 100 kV/m and deduced particle charges from their\nacceleration. The ejected particles have sizes of about 10 to 100 microns. They\ncarry charges up to $10^5$ e or charge densities up to $> 10^7$ e/mm$^2$.\nWithin the given size range, we find a small bias towards positive charges."
                },
                "authors": [
                    {
                        "name": "T. Becker"
                    },
                    {
                        "name": "F. C. Onyeagusi"
                    },
                    {
                        "name": "J. Teiser"
                    },
                    {
                        "name": "T. Jardiel"
                    },
                    {
                        "name": "M. Peiteado"
                    },
                    {
                        "name": "O. Munoz"
                    },
                    {
                        "name": "J. Martikainen"
                    },
                    {
                        "name": "J. C. Gomez Martin"
                    },
                    {
                        "name": "J. Merrison"
                    },
                    {
                        "name": "G. Wurm"
                    }
                ],
                "author_detail": {
                    "name": "G. Wurm"
                },
                "author": "G. Wurm",
                "arxiv_comment": "Preprint, 7 pages, 3 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.10287v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.10287v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.EP",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.EP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "astro-ph.IM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.10207v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.10207v1",
                "updated": "2024-09-16T11:56:09Z",
                "updated_parsed": [
                    2024,
                    9,
                    16,
                    11,
                    56,
                    9,
                    0,
                    260,
                    0
                ],
                "published": "2024-09-16T11:56:09Z",
                "published_parsed": [
                    2024,
                    9,
                    16,
                    11,
                    56,
                    9,
                    0,
                    260,
                    0
                ],
                "title": "Decoupling DNS Update Timing from TTL Values",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Decoupling DNS Update Timing from TTL Values"
                },
                "summary": "A relatively simple safety-belt mechanism for improving DNS system\navailability and efficiency is proposed here. While it may seem ambitious, a\ncareful examination shows it is both feasible and beneficial for the DNS\nsystem. The mechanism called \"DNS Real-time Update\" (DNSRU), a service that\nfacilitates real-time and secure updates of cached domain records in DNS\nresolvers worldwide, even before the expiration of the corresponding Time To\nLive (TTL) values. This service allows Internet domain owners to quickly\nrectify any erroneous global IP address distribution, even if a long TTL value\nis associated with it. By addressing this critical DNS high availability issue,\nDNSRU eliminates the need for short TTL values and their associated drawbacks.\nTherefore, DNSRU DNSRU reduces the traffic load on authoritative servers while\nenhancing the system's fault tolerance. In this paper we show that our DNSRU\ndesign is backward compatible, supports gradual deployment, secure, efficient,\nand feasible.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A relatively simple safety-belt mechanism for improving DNS system\navailability and efficiency is proposed here. While it may seem ambitious, a\ncareful examination shows it is both feasible and beneficial for the DNS\nsystem. The mechanism called \"DNS Real-time Update\" (DNSRU), a service that\nfacilitates real-time and secure updates of cached domain records in DNS\nresolvers worldwide, even before the expiration of the corresponding Time To\nLive (TTL) values. This service allows Internet domain owners to quickly\nrectify any erroneous global IP address distribution, even if a long TTL value\nis associated with it. By addressing this critical DNS high availability issue,\nDNSRU eliminates the need for short TTL values and their associated drawbacks.\nTherefore, DNSRU DNSRU reduces the traffic load on authoritative servers while\nenhancing the system's fault tolerance. In this paper we show that our DNSRU\ndesign is backward compatible, supports gradual deployment, secure, efficient,\nand feasible."
                },
                "authors": [
                    {
                        "name": "Yehuda Afek"
                    },
                    {
                        "name": "Ariel Litmanovich"
                    }
                ],
                "author_detail": {
                    "name": "Ariel Litmanovich"
                },
                "author": "Ariel Litmanovich",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.10207v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.10207v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.09753v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.09753v1",
                "updated": "2024-09-15T14:49:30Z",
                "updated_parsed": [
                    2024,
                    9,
                    15,
                    14,
                    49,
                    30,
                    6,
                    259,
                    0
                ],
                "published": "2024-09-15T14:49:30Z",
                "published_parsed": [
                    2024,
                    9,
                    15,
                    14,
                    49,
                    30,
                    6,
                    259,
                    0
                ],
                "title": "DARDA: Domain-Aware Real-Time Dynamic Neural Network Adaptation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DARDA: Domain-Aware Real-Time Dynamic Neural Network Adaptation"
                },
                "summary": "Test Time Adaptation (TTA) has emerged as a practical solution to mitigate\nthe performance degradation of Deep Neural Networks (DNNs) in the presence of\ncorruption/ noise affecting inputs. Existing approaches in TTA continuously\nadapt the DNN, leading to excessive resource consumption and performance\ndegradation due to accumulation of error stemming from lack of supervision. In\nthis work, we propose Domain-Aware Real-Time Dynamic Adaptation (DARDA) to\naddress such issues. Our key approach is to proactively learn latent\nrepresentations of some corruption types, each one associated with a\nsub-network state tailored to correctly classify inputs affected by that\ncorruption. After deployment, DARDA adapts the DNN to previously unseen\ncorruptions in an unsupervised fashion by (i) estimating the latent\nrepresentation of the ongoing corruption; (ii) selecting the sub-network whose\nassociated corruption is the closest in the latent space to the ongoing\ncorruption; and (iii) adapting DNN state, so that its representation matches\nthe ongoing corruption. This way, DARDA is more resource efficient and can\nswiftly adapt to new distributions caused by different corruptions without\nrequiring a large variety of input data. Through experiments with two popular\nmobile edge devices - Raspberry Pi and NVIDIA Jetson Nano - we show that DARDA\nreduces energy consumption and average cache memory footprint respectively by\n1.74x and 2.64x with respect to the state of the art, while increasing the\nperformance by 10.4%, 5.7% and 4.4% on CIFAR-10, CIFAR-100 and TinyImagenet.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Test Time Adaptation (TTA) has emerged as a practical solution to mitigate\nthe performance degradation of Deep Neural Networks (DNNs) in the presence of\ncorruption/ noise affecting inputs. Existing approaches in TTA continuously\nadapt the DNN, leading to excessive resource consumption and performance\ndegradation due to accumulation of error stemming from lack of supervision. In\nthis work, we propose Domain-Aware Real-Time Dynamic Adaptation (DARDA) to\naddress such issues. Our key approach is to proactively learn latent\nrepresentations of some corruption types, each one associated with a\nsub-network state tailored to correctly classify inputs affected by that\ncorruption. After deployment, DARDA adapts the DNN to previously unseen\ncorruptions in an unsupervised fashion by (i) estimating the latent\nrepresentation of the ongoing corruption; (ii) selecting the sub-network whose\nassociated corruption is the closest in the latent space to the ongoing\ncorruption; and (iii) adapting DNN state, so that its representation matches\nthe ongoing corruption. This way, DARDA is more resource efficient and can\nswiftly adapt to new distributions caused by different corruptions without\nrequiring a large variety of input data. Through experiments with two popular\nmobile edge devices - Raspberry Pi and NVIDIA Jetson Nano - we show that DARDA\nreduces energy consumption and average cache memory footprint respectively by\n1.74x and 2.64x with respect to the state of the art, while increasing the\nperformance by 10.4%, 5.7% and 4.4% on CIFAR-10, CIFAR-100 and TinyImagenet."
                },
                "authors": [
                    {
                        "name": "Shahriar Rifat"
                    },
                    {
                        "name": "Jonathan Ashdown"
                    },
                    {
                        "name": "Francesco Restuccia"
                    }
                ],
                "author_detail": {
                    "name": "Francesco Restuccia"
                },
                "author": "Francesco Restuccia",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.09753v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.09753v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.09398v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.09398v1",
                "updated": "2024-09-14T10:15:37Z",
                "updated_parsed": [
                    2024,
                    9,
                    14,
                    10,
                    15,
                    37,
                    5,
                    258,
                    0
                ],
                "published": "2024-09-14T10:15:37Z",
                "published_parsed": [
                    2024,
                    9,
                    14,
                    10,
                    15,
                    37,
                    5,
                    258,
                    0
                ],
                "title": "Language-Queried Target Sound Extraction Without Parallel Training Data",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Language-Queried Target Sound Extraction Without Parallel Training Data"
                },
                "summary": "Language-queried target sound extraction (TSE) aims to extract specific\nsounds from mixtures based on language queries. Traditional fully-supervised\ntraining schemes require extensively annotated parallel audio-text data, which\nare labor-intensive. We introduce a language-free training scheme, requiring\nonly unlabelled audio clips for TSE model training by utilizing the multi-modal\nrepresentation alignment nature of the contrastive language-audio pre-trained\nmodel (CLAP). In a vanilla language-free training stage, target audio is\nencoded using the pre-trained CLAP audio encoder to form a condition embedding\nfor the TSE model, while during inference, user language queries are encoded by\nCLAP text encoder. This straightforward approach faces challenges due to the\nmodality gap between training and inference queries and information leakage\nfrom direct exposure to target audio during training. To address this, we\npropose a retrieval-augmented strategy. Specifically, we create an embedding\ncache using audio captions generated by a large language model (LLM). During\ntraining, target audio embeddings retrieve text embeddings from this cache to\nuse as condition embeddings, ensuring consistent modalities between training\nand inference and eliminating information leakage. Extensive experiment results\nshow that our retrieval-augmented approach achieves consistent and notable\nperformance improvements over existing state-of-the-art with better\ngeneralizability.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Language-queried target sound extraction (TSE) aims to extract specific\nsounds from mixtures based on language queries. Traditional fully-supervised\ntraining schemes require extensively annotated parallel audio-text data, which\nare labor-intensive. We introduce a language-free training scheme, requiring\nonly unlabelled audio clips for TSE model training by utilizing the multi-modal\nrepresentation alignment nature of the contrastive language-audio pre-trained\nmodel (CLAP). In a vanilla language-free training stage, target audio is\nencoded using the pre-trained CLAP audio encoder to form a condition embedding\nfor the TSE model, while during inference, user language queries are encoded by\nCLAP text encoder. This straightforward approach faces challenges due to the\nmodality gap between training and inference queries and information leakage\nfrom direct exposure to target audio during training. To address this, we\npropose a retrieval-augmented strategy. Specifically, we create an embedding\ncache using audio captions generated by a large language model (LLM). During\ntraining, target audio embeddings retrieve text embeddings from this cache to\nuse as condition embeddings, ensuring consistent modalities between training\nand inference and eliminating information leakage. Extensive experiment results\nshow that our retrieval-augmented approach achieves consistent and notable\nperformance improvements over existing state-of-the-art with better\ngeneralizability."
                },
                "authors": [
                    {
                        "name": "Hao Ma"
                    },
                    {
                        "name": "Zhiyuan Peng"
                    },
                    {
                        "name": "Xu Li"
                    },
                    {
                        "name": "Yukai Li"
                    },
                    {
                        "name": "Mingjie Shao"
                    },
                    {
                        "name": "Qiuqiang Kong"
                    },
                    {
                        "name": "Ju Liu"
                    }
                ],
                "author_detail": {
                    "name": "Ju Liu"
                },
                "author": "Ju Liu",
                "arxiv_comment": "Submitted to ICASSP 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.09398v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.09398v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.AS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.09322v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.09322v1",
                "updated": "2024-09-14T05:51:50Z",
                "updated_parsed": [
                    2024,
                    9,
                    14,
                    5,
                    51,
                    50,
                    5,
                    258,
                    0
                ],
                "published": "2024-09-14T05:51:50Z",
                "published_parsed": [
                    2024,
                    9,
                    14,
                    5,
                    51,
                    50,
                    5,
                    258,
                    0
                ],
                "title": "A Compressive Memory-based Retrieval Approach for Event Argument\n  Extraction",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Compressive Memory-based Retrieval Approach for Event Argument\n  Extraction"
                },
                "summary": "Recent works have demonstrated the effectiveness of retrieval augmentation in\nthe Event Argument Extraction (EAE) task. However, existing retrieval-based EAE\nmethods have two main limitations: (1) input length constraints and (2) the gap\nbetween the retriever and the inference model. These issues limit the diversity\nand quality of the retrieved information. In this paper, we propose a\nCompressive Memory-based Retrieval (CMR) mechanism for EAE, which addresses the\ntwo limitations mentioned above. Our compressive memory, designed as a dynamic\nmatrix that effectively caches retrieved information and supports continuous\nupdates, overcomes the limitations of the input length. Additionally, after\npre-loading all candidate demonstrations into the compressive memory, the model\nfurther retrieves and filters relevant information from memory based on the\ninput query, bridging the gap between the retriever and the inference model.\nExtensive experiments show that our method achieves new state-of-the-art\nperformance on three public datasets (RAMS, WikiEvents, ACE05), significantly\noutperforming existing retrieval-based EAE methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent works have demonstrated the effectiveness of retrieval augmentation in\nthe Event Argument Extraction (EAE) task. However, existing retrieval-based EAE\nmethods have two main limitations: (1) input length constraints and (2) the gap\nbetween the retriever and the inference model. These issues limit the diversity\nand quality of the retrieved information. In this paper, we propose a\nCompressive Memory-based Retrieval (CMR) mechanism for EAE, which addresses the\ntwo limitations mentioned above. Our compressive memory, designed as a dynamic\nmatrix that effectively caches retrieved information and supports continuous\nupdates, overcomes the limitations of the input length. Additionally, after\npre-loading all candidate demonstrations into the compressive memory, the model\nfurther retrieves and filters relevant information from memory based on the\ninput query, bridging the gap between the retriever and the inference model.\nExtensive experiments show that our method achieves new state-of-the-art\nperformance on three public datasets (RAMS, WikiEvents, ACE05), significantly\noutperforming existing retrieval-based EAE methods."
                },
                "authors": [
                    {
                        "name": "Wanlong Liu"
                    },
                    {
                        "name": "Enqi Zhang"
                    },
                    {
                        "name": "Li Zhou"
                    },
                    {
                        "name": "Dingyi Zeng"
                    },
                    {
                        "name": "Shaohuan Cheng"
                    },
                    {
                        "name": "Chen Zhang"
                    },
                    {
                        "name": "Malu Zhang"
                    },
                    {
                        "name": "Wenyu Chen"
                    }
                ],
                "author_detail": {
                    "name": "Wenyu Chen"
                },
                "author": "Wenyu Chen",
                "arxiv_comment": "15 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.09322v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.09322v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.09202v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.09202v1",
                "updated": "2024-09-13T21:31:45Z",
                "updated_parsed": [
                    2024,
                    9,
                    13,
                    21,
                    31,
                    45,
                    4,
                    257,
                    0
                ],
                "published": "2024-09-13T21:31:45Z",
                "published_parsed": [
                    2024,
                    9,
                    13,
                    21,
                    31,
                    45,
                    4,
                    257,
                    0
                ],
                "title": "WarmSwap: Sharing Dependencies for Accelerating Cold Starts in\n  Serverless Functions",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "WarmSwap: Sharing Dependencies for Accelerating Cold Starts in\n  Serverless Functions"
                },
                "summary": "This work presents WarmSwap, a novel provider-side cold-start optimization\nfor serverless computing. This optimization reduces cold-start time when\nbooting and loading dependencies at runtime inside a function container.\nPrevious approaches to the optimization of cold starts tend to fall into two\ncategories: optimizing the infrastructure of serverless computing to benefit\nall serverless functions; or function-specific tuning for individual serverless\nfunctions. In contrast, WarmSwap offers a broad middle ground, which optimizes\nentire categories of serverless functions. WarmSwap eliminates the need to\ninitialize middleware or software dependencies when launching a new serverless\ncontainer, by migrating a pre-initialized live dependency image to the new\nfunction instance. WarmSwap respects the provider's cache constraints, as a\nsingle pre-warmed dependency image in the cache is shared among all serverless\nfunctions requiring that software dependency image. WarmSwap has been tested on\nseven representative functions from FunctionBench. The functions are chosen to\ncompare with previous work. In those tests, WarmSwap accelerates cold-start\nexecutions for those serverless functions with large dependency requirements by\na factor ranging from 1.2 to 2.2.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This work presents WarmSwap, a novel provider-side cold-start optimization\nfor serverless computing. This optimization reduces cold-start time when\nbooting and loading dependencies at runtime inside a function container.\nPrevious approaches to the optimization of cold starts tend to fall into two\ncategories: optimizing the infrastructure of serverless computing to benefit\nall serverless functions; or function-specific tuning for individual serverless\nfunctions. In contrast, WarmSwap offers a broad middle ground, which optimizes\nentire categories of serverless functions. WarmSwap eliminates the need to\ninitialize middleware or software dependencies when launching a new serverless\ncontainer, by migrating a pre-initialized live dependency image to the new\nfunction instance. WarmSwap respects the provider's cache constraints, as a\nsingle pre-warmed dependency image in the cache is shared among all serverless\nfunctions requiring that software dependency image. WarmSwap has been tested on\nseven representative functions from FunctionBench. The functions are chosen to\ncompare with previous work. In those tests, WarmSwap accelerates cold-start\nexecutions for those serverless functions with large dependency requirements by\na factor ranging from 1.2 to 2.2."
                },
                "authors": [
                    {
                        "name": "Rui Li"
                    },
                    {
                        "name": "Devesh Tiwari"
                    },
                    {
                        "name": "Gene Cooperman"
                    }
                ],
                "author_detail": {
                    "name": "Gene Cooperman"
                },
                "author": "Gene Cooperman",
                "arxiv_comment": "15 pages, 7 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.09202v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.09202v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.08141v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.08141v1",
                "updated": "2024-09-12T15:34:23Z",
                "updated_parsed": [
                    2024,
                    9,
                    12,
                    15,
                    34,
                    23,
                    3,
                    256,
                    0
                ],
                "published": "2024-09-12T15:34:23Z",
                "published_parsed": [
                    2024,
                    9,
                    12,
                    15,
                    34,
                    23,
                    3,
                    256,
                    0
                ],
                "title": "Rethinking Programmed I/O for Fast Devices, Cheap Cores, and Coherent\n  Interconnects",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Rethinking Programmed I/O for Fast Devices, Cheap Cores, and Coherent\n  Interconnects"
                },
                "summary": "Conventional wisdom holds that an efficient interface between an OS running\non a CPU and a high-bandwidth I/O device should be based on Direct Memory\nAccess (DMA), descriptor rings, and interrupts: DMA offloads transfers from the\nCPU, descriptor rings provide buffering and queuing, and interrupts facilitate\nasynchronous interaction between cores and device with a lightweight\nnotification mechanism. In this paper we question this wisdom in the light of\nmodern hardware and workloads, particularly in cloud servers. We argue that the\nassumptions that led to this model are obsolete, and in many use-cases use of\nprogrammed I/O, where the CPU explicitly transfers data and control information\nto and from a device via loads and stores, actually results in a more efficient\nsystem. We quantitatively demonstrate these advantages using three use-cases:\nfine-grained RPC-style invocation of functions on an accelerator, offloading of\noperators in a streaming dataflow engine, and a network interface targeting for\nserverless functions. Moreover, we show that while these advantages are\nsignificant over a modern PCIe peripheral bus, a truly cache-coherent\ninterconnect offers significant additional efficiency gains.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Conventional wisdom holds that an efficient interface between an OS running\non a CPU and a high-bandwidth I/O device should be based on Direct Memory\nAccess (DMA), descriptor rings, and interrupts: DMA offloads transfers from the\nCPU, descriptor rings provide buffering and queuing, and interrupts facilitate\nasynchronous interaction between cores and device with a lightweight\nnotification mechanism. In this paper we question this wisdom in the light of\nmodern hardware and workloads, particularly in cloud servers. We argue that the\nassumptions that led to this model are obsolete, and in many use-cases use of\nprogrammed I/O, where the CPU explicitly transfers data and control information\nto and from a device via loads and stores, actually results in a more efficient\nsystem. We quantitatively demonstrate these advantages using three use-cases:\nfine-grained RPC-style invocation of functions on an accelerator, offloading of\noperators in a streaming dataflow engine, and a network interface targeting for\nserverless functions. Moreover, we show that while these advantages are\nsignificant over a modern PCIe peripheral bus, a truly cache-coherent\ninterconnect offers significant additional efficiency gains."
                },
                "authors": [
                    {
                        "name": "Anastasiia Ruzhanskaia"
                    },
                    {
                        "name": "Pengcheng Xu"
                    },
                    {
                        "name": "David Cock"
                    },
                    {
                        "name": "Timothy Roscoe"
                    }
                ],
                "author_detail": {
                    "name": "Timothy Roscoe"
                },
                "author": "Timothy Roscoe",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.08141v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.08141v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.OS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2303.01699v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2303.01699v5",
                "updated": "2024-09-12T10:35:15Z",
                "updated_parsed": [
                    2024,
                    9,
                    12,
                    10,
                    35,
                    15,
                    3,
                    256,
                    0
                ],
                "published": "2023-03-03T04:03:28Z",
                "published_parsed": [
                    2023,
                    3,
                    3,
                    4,
                    3,
                    28,
                    4,
                    62,
                    0
                ],
                "title": "Light Induced Orbital Magnetism in Metals via Inverse Faraday Effect",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Light Induced Orbital Magnetism in Metals via Inverse Faraday Effect"
                },
                "summary": "We present a microscopic calculation of the inverse Faraday effect in metals.\nWe derive a static local magnetic moment induced on the application of\nhigh-frequency light, using the Eilenberger formulation of quasiclassical\ntheory. We include the effect of disorder and formulate a theory applicable\nacross the entire temperature range, in the absence of external applied fields.\nFor light-induced electric fields of amplitude $\\sim 100 kV/cm$, the induced\nfields are large, $\\sim 0.1 T$ for metallic Nb! The predictions of our theory\nagree with recent experimental and theoretical results [1]. An extension of\nthis approach to superconductors would open a new route of inducing orbital\nmagnetic field and potentially vortices in superconductors.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present a microscopic calculation of the inverse Faraday effect in metals.\nWe derive a static local magnetic moment induced on the application of\nhigh-frequency light, using the Eilenberger formulation of quasiclassical\ntheory. We include the effect of disorder and formulate a theory applicable\nacross the entire temperature range, in the absence of external applied fields.\nFor light-induced electric fields of amplitude $\\sim 100 kV/cm$, the induced\nfields are large, $\\sim 0.1 T$ for metallic Nb! The predictions of our theory\nagree with recent experimental and theoretical results [1]. An extension of\nthis approach to superconductors would open a new route of inducing orbital\nmagnetic field and potentially vortices in superconductors."
                },
                "authors": [
                    {
                        "name": "Priya Sharma"
                    },
                    {
                        "name": "Alexander V. Balatsky"
                    }
                ],
                "author_detail": {
                    "name": "Alexander V. Balatsky"
                },
                "author": "Alexander V. Balatsky",
                "arxiv_doi": "10.1103/PhysRevB.110.094302",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1103/PhysRevB.110.094302",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2303.01699v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2303.01699v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_journal_ref": "Phys. Rev. B 110, 094302 (2024)",
                "arxiv_primary_category": {
                    "term": "cond-mat.supr-con",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cond-mat.supr-con",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.07704v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.07704v1",
                "updated": "2024-09-12T02:13:57Z",
                "updated_parsed": [
                    2024,
                    9,
                    12,
                    2,
                    13,
                    57,
                    3,
                    256,
                    0
                ],
                "published": "2024-09-12T02:13:57Z",
                "published_parsed": [
                    2024,
                    9,
                    12,
                    2,
                    13,
                    57,
                    3,
                    256,
                    0
                ],
                "title": "Super Monotonic Alignment Search",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Super Monotonic Alignment Search"
                },
                "summary": "Monotonic alignment search (MAS), introduced by Glow-TTS, is one of the most\npopular algorithm in TTS to estimate unknown alignments between text and\nspeech. Since this algorithm needs to search for the most probable alignment\nwith dynamic programming by caching all paths, the time complexity of the\nalgorithm is $O(T \\times S)$. The authors of Glow-TTS run this algorithm on\nCPU, and while they mentioned it is difficult to parallelize, we found that MAS\ncan be parallelized in text-length dimension and CPU execution consumes an\ninordinate amount of time for inter-device copy. Therefore, we implemented a\nTriton kernel and PyTorch JIT script to accelerate MAS on GPU without\ninter-device copy. As a result, Super-MAS Triton kernel is up to 72 times\nfaster in the extreme-length case. The code is available at\n\\url{https://github.com/supertone-inc/super-monotonic-align}.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Monotonic alignment search (MAS), introduced by Glow-TTS, is one of the most\npopular algorithm in TTS to estimate unknown alignments between text and\nspeech. Since this algorithm needs to search for the most probable alignment\nwith dynamic programming by caching all paths, the time complexity of the\nalgorithm is $O(T \\times S)$. The authors of Glow-TTS run this algorithm on\nCPU, and while they mentioned it is difficult to parallelize, we found that MAS\ncan be parallelized in text-length dimension and CPU execution consumes an\ninordinate amount of time for inter-device copy. Therefore, we implemented a\nTriton kernel and PyTorch JIT script to accelerate MAS on GPU without\ninter-device copy. As a result, Super-MAS Triton kernel is up to 72 times\nfaster in the extreme-length case. The code is available at\n\\url{https://github.com/supertone-inc/super-monotonic-align}."
                },
                "authors": [
                    {
                        "name": "Junhyeok Lee"
                    },
                    {
                        "name": "Hyeongju Kim"
                    }
                ],
                "author_detail": {
                    "name": "Hyeongju Kim"
                },
                "author": "Hyeongju Kim",
                "arxiv_comment": "Technical Report",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.07704v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.07704v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.AS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.07331v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.07331v1",
                "updated": "2024-09-11T15:11:39Z",
                "updated_parsed": [
                    2024,
                    9,
                    11,
                    15,
                    11,
                    39,
                    2,
                    255,
                    0
                ],
                "published": "2024-09-11T15:11:39Z",
                "published_parsed": [
                    2024,
                    9,
                    11,
                    15,
                    11,
                    39,
                    2,
                    255,
                    0
                ],
                "title": "Learning to Compress Contexts for Efficient Knowledge-based Visual\n  Question Answering",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Learning to Compress Contexts for Efficient Knowledge-based Visual\n  Question Answering"
                },
                "summary": "Multimodal Large Language Models (MLLMs) have demonstrated great zero-shot\nperformance on visual question answering (VQA). However, when it comes to\nknowledge-based VQA (KB-VQA), MLLMs may lack human commonsense or specialized\ndomain knowledge to answer such questions and require obtaining necessary\ninformation from external knowledge sources. Previous works like\nRetrival-Augmented VQA-v2 (RAVQA-v2) focus on utilizing as much input\ninformation, such as image-based textual descriptions and retrieved knowledge,\nas possible to improve performance, but they all overlook the issue that with\nthe number of input tokens increasing, inference efficiency significantly\ndecreases, which contradicts the demands of practical applications. To address\nthis issue, we propose Retrieval-Augmented MLLM with Compressed Contexts\n(RACC). RACC learns to compress and aggregate retrieved contexts, from which it\ngenerates a compact modulation in the form of Key-Value (KV) cache. This\nmodulation is then used to adapt the downstream frozen MLLM, thereby achieving\neffective and efficient inference. RACC achieves a state-of-the-art (SOTA)\nperformance of 62.9% on OK-VQA. Moreover, it significantly reduces inference\nlatency by 22.0%-59.7% compared to the prominent RAVQA-v2. Abundant experiments\nshow RACC's broad applicability. It is compatible with various off-the-shelf\nMLLMs and can also handle different knowledge sources including textual and\nmultimodal documents.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multimodal Large Language Models (MLLMs) have demonstrated great zero-shot\nperformance on visual question answering (VQA). However, when it comes to\nknowledge-based VQA (KB-VQA), MLLMs may lack human commonsense or specialized\ndomain knowledge to answer such questions and require obtaining necessary\ninformation from external knowledge sources. Previous works like\nRetrival-Augmented VQA-v2 (RAVQA-v2) focus on utilizing as much input\ninformation, such as image-based textual descriptions and retrieved knowledge,\nas possible to improve performance, but they all overlook the issue that with\nthe number of input tokens increasing, inference efficiency significantly\ndecreases, which contradicts the demands of practical applications. To address\nthis issue, we propose Retrieval-Augmented MLLM with Compressed Contexts\n(RACC). RACC learns to compress and aggregate retrieved contexts, from which it\ngenerates a compact modulation in the form of Key-Value (KV) cache. This\nmodulation is then used to adapt the downstream frozen MLLM, thereby achieving\neffective and efficient inference. RACC achieves a state-of-the-art (SOTA)\nperformance of 62.9% on OK-VQA. Moreover, it significantly reduces inference\nlatency by 22.0%-59.7% compared to the prominent RAVQA-v2. Abundant experiments\nshow RACC's broad applicability. It is compatible with various off-the-shelf\nMLLMs and can also handle different knowledge sources including textual and\nmultimodal documents."
                },
                "authors": [
                    {
                        "name": "Weixi Weng"
                    },
                    {
                        "name": "Jieming Zhu"
                    },
                    {
                        "name": "Hao Zhang"
                    },
                    {
                        "name": "Xiaojun Meng"
                    },
                    {
                        "name": "Rui Zhang"
                    },
                    {
                        "name": "Chun Yuan"
                    }
                ],
                "author_detail": {
                    "name": "Chun Yuan"
                },
                "author": "Chun Yuan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.07331v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.07331v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.09086v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.09086v1",
                "updated": "2024-09-11T12:44:12Z",
                "updated_parsed": [
                    2024,
                    9,
                    11,
                    12,
                    44,
                    12,
                    2,
                    255,
                    0
                ],
                "published": "2024-09-11T12:44:12Z",
                "published_parsed": [
                    2024,
                    9,
                    11,
                    12,
                    44,
                    12,
                    2,
                    255,
                    0
                ],
                "title": "Inf-MLLM: Efficient Streaming Inference of Multimodal Large Language\n  Models on a Single GPU",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Inf-MLLM: Efficient Streaming Inference of Multimodal Large Language\n  Models on a Single GPU"
                },
                "summary": "Multimodal Large Language Models (MLLMs) are distinguished by their\nmultimodal comprehensive ability and widely used in many real-world\napplications including GPT-4o, autonomous driving and robotics. Despite their\nimpressive performance, the multimodal inputs always incur long context. The\ninference under long context requires caching massive Key and Value states (KV\ncache) of previous tokens, which introduces high latency and excessive memory\nconsumption. Due to this reason, it is challenging to deploy streaming\ninference of MLLMs on edge devices, which largely constrains the power and\nusage of MLLMs in real-world applications. In this paper, we introduce\nInf-MLLM, an efficient inference framework for MLLMs, which enable streaming\ninference of MLLM on a single GPU with infinite context. Inf-MLLM is based on\nour key observation of the attention pattern in both LLMs and MLLMs called\n\"attention saddles\". Thanks to the newly discovered attention pattern, Inf-MLLM\nmaintains a size-constrained KV cache by dynamically caching recent tokens and\nrelevant tokens. Furthermore, Inf-MLLM proposes attention bias, a novel\napproach to enable MLLMs to capture long-term dependency. We show that Inf-MLLM\nenables multiple LLMs and MLLMs to achieve stable performance over 4M-token\nlong texts and multi-round conversations with 1-hour-long videos on a single\nGPU. In addition, Inf-MLLM exhibits superior streaming reasoning quality than\nexisting methods such as StreamingLLM and 2x speedup than H2O.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multimodal Large Language Models (MLLMs) are distinguished by their\nmultimodal comprehensive ability and widely used in many real-world\napplications including GPT-4o, autonomous driving and robotics. Despite their\nimpressive performance, the multimodal inputs always incur long context. The\ninference under long context requires caching massive Key and Value states (KV\ncache) of previous tokens, which introduces high latency and excessive memory\nconsumption. Due to this reason, it is challenging to deploy streaming\ninference of MLLMs on edge devices, which largely constrains the power and\nusage of MLLMs in real-world applications. In this paper, we introduce\nInf-MLLM, an efficient inference framework for MLLMs, which enable streaming\ninference of MLLM on a single GPU with infinite context. Inf-MLLM is based on\nour key observation of the attention pattern in both LLMs and MLLMs called\n\"attention saddles\". Thanks to the newly discovered attention pattern, Inf-MLLM\nmaintains a size-constrained KV cache by dynamically caching recent tokens and\nrelevant tokens. Furthermore, Inf-MLLM proposes attention bias, a novel\napproach to enable MLLMs to capture long-term dependency. We show that Inf-MLLM\nenables multiple LLMs and MLLMs to achieve stable performance over 4M-token\nlong texts and multi-round conversations with 1-hour-long videos on a single\nGPU. In addition, Inf-MLLM exhibits superior streaming reasoning quality than\nexisting methods such as StreamingLLM and 2x speedup than H2O."
                },
                "authors": [
                    {
                        "name": "Zhenyu Ning"
                    },
                    {
                        "name": "Jieru Zhao"
                    },
                    {
                        "name": "Qihao Jin"
                    },
                    {
                        "name": "Wenchao Ding"
                    },
                    {
                        "name": "Minyi Guo"
                    }
                ],
                "author_detail": {
                    "name": "Minyi Guo"
                },
                "author": "Minyi Guo",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.09086v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.09086v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.07196v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.07196v1",
                "updated": "2024-09-11T11:40:23Z",
                "updated_parsed": [
                    2024,
                    9,
                    11,
                    11,
                    40,
                    23,
                    2,
                    255,
                    0
                ],
                "published": "2024-09-11T11:40:23Z",
                "published_parsed": [
                    2024,
                    9,
                    11,
                    11,
                    40,
                    23,
                    2,
                    255,
                    0
                ],
                "title": "Sub-cycle Nanotip Field Emission of Electrons Driven by Air Plasma\n  Generated THz Pulses",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Sub-cycle Nanotip Field Emission of Electrons Driven by Air Plasma\n  Generated THz Pulses"
                },
                "summary": "Terahertz pulses generated by two-color laser plasmas have reported peak\nfield strengths exceeding MV/cm, and when illuminating metal nanotips the\nnear-field enhancement at the tip apex should result in extremely high bunch\ncharges and electron energies via sub-cycle cold field emission. Here, electron\nemission from tungsten nanotips driven by THz pulses generated by a long\nfilament air-plasma are reported. Electron energies up to 1.1 keV and bunch\ncharges up to 2x$10^5$ electrons per pulse were detected, well below values\nexpected for peak field calculated via the time averaged Poynting vector.\nInvestigations revealed a failure in the use of the time-averaged Poynting\nvector when applied to long filament THz pulses, due to spatio-temporal\nrestructuring of the THz pulse in the focus. Accounting for this restructuring\nsignificantly reduces the field strength to approximately 160 ~kV/cm,\nconsistent with the observed electron bunch charges, peak energies and their\ndependence on the tip position in the THz focus. Despite these findings, our\nresults surpass previous THz plasma-driven electron generation by an order of\nmagnitude in both electron energy and bunch charge and a path to increasing\nthese by an additional order of magnitude by modification of the THz optics is\nproposed.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Terahertz pulses generated by two-color laser plasmas have reported peak\nfield strengths exceeding MV/cm, and when illuminating metal nanotips the\nnear-field enhancement at the tip apex should result in extremely high bunch\ncharges and electron energies via sub-cycle cold field emission. Here, electron\nemission from tungsten nanotips driven by THz pulses generated by a long\nfilament air-plasma are reported. Electron energies up to 1.1 keV and bunch\ncharges up to 2x$10^5$ electrons per pulse were detected, well below values\nexpected for peak field calculated via the time averaged Poynting vector.\nInvestigations revealed a failure in the use of the time-averaged Poynting\nvector when applied to long filament THz pulses, due to spatio-temporal\nrestructuring of the THz pulse in the focus. Accounting for this restructuring\nsignificantly reduces the field strength to approximately 160 ~kV/cm,\nconsistent with the observed electron bunch charges, peak energies and their\ndependence on the tip position in the THz focus. Despite these findings, our\nresults surpass previous THz plasma-driven electron generation by an order of\nmagnitude in both electron energy and bunch charge and a path to increasing\nthese by an additional order of magnitude by modification of the THz optics is\nproposed."
                },
                "authors": [
                    {
                        "name": "Benjamin Colmey"
                    },
                    {
                        "name": "Rodrigo T. Paulino"
                    },
                    {
                        "name": "David G. Cooke"
                    }
                ],
                "author_detail": {
                    "name": "David G. Cooke"
                },
                "author": "David G. Cooke",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.07196v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.07196v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cond-mat.mtrl-sci",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cond-mat.mtrl-sci",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "physics.plasm-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.10926v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.10926v2",
                "updated": "2024-09-11T08:12:55Z",
                "updated_parsed": [
                    2024,
                    9,
                    11,
                    8,
                    12,
                    55,
                    2,
                    255,
                    0
                ],
                "published": "2024-07-15T17:25:42Z",
                "published_parsed": [
                    2024,
                    7,
                    15,
                    17,
                    25,
                    42,
                    0,
                    197,
                    0
                ],
                "title": "In-Loop Filtering via Trained Look-Up Tables",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In-Loop Filtering via Trained Look-Up Tables"
                },
                "summary": "In-loop filtering (ILF) is a key technology for removing the artifacts in\nimage/video coding standards. Recently, neural network-based in-loop filtering\nmethods achieve remarkable coding gains beyond the capability of advanced video\ncoding standards, which becomes a powerful coding tool candidate for future\nvideo coding standards. However, the utilization of deep neural networks brings\nheavy time and computational complexity, and high demands of high-performance\nhardware, which is challenging to apply to the general uses of coding scene. To\naddress this limitation, inspired by explorations in image restoration, we\npropose an efficient and practical in-loop filtering scheme by adopting the\nLook-up Table (LUT). We train the DNN of in-loop filtering within a fixed\nfiltering reference range, and cache the output values of the DNN into a LUT\nvia traversing all possible inputs. At testing time in the coding process, the\nfiltered pixel is generated by locating input pixels (to-be-filtered pixel with\nreference pixels) and interpolating cached filtered pixel values. To further\nenable the large filtering reference range with the limited storage cost of\nLUT, we introduce the enhanced indexing mechanism in the filtering process, and\nclipping/finetuning mechanism in the training. The proposed method is\nimplemented into the Versatile Video Coding (VVC) reference software, VTM-11.0.\nExperimental results show that the ultrafast, very fast, and fast mode of the\nproposed method achieves on average 0.13%/0.34%/0.51%, and 0.10%/0.27%/0.39%\nBD-rate reduction, under the all intra (AI) and random access (RA)\nconfigurations. Especially, our method has friendly time and computational\ncomplexity, only 101%/102%-104%/108% time increase with 0.13-0.93 kMACs/pixel,\nand only 164-1148 KB storage cost for a single model. Our solution may shed\nlight on the journey of practical neural network-based coding tool evolution.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In-loop filtering (ILF) is a key technology for removing the artifacts in\nimage/video coding standards. Recently, neural network-based in-loop filtering\nmethods achieve remarkable coding gains beyond the capability of advanced video\ncoding standards, which becomes a powerful coding tool candidate for future\nvideo coding standards. However, the utilization of deep neural networks brings\nheavy time and computational complexity, and high demands of high-performance\nhardware, which is challenging to apply to the general uses of coding scene. To\naddress this limitation, inspired by explorations in image restoration, we\npropose an efficient and practical in-loop filtering scheme by adopting the\nLook-up Table (LUT). We train the DNN of in-loop filtering within a fixed\nfiltering reference range, and cache the output values of the DNN into a LUT\nvia traversing all possible inputs. At testing time in the coding process, the\nfiltered pixel is generated by locating input pixels (to-be-filtered pixel with\nreference pixels) and interpolating cached filtered pixel values. To further\nenable the large filtering reference range with the limited storage cost of\nLUT, we introduce the enhanced indexing mechanism in the filtering process, and\nclipping/finetuning mechanism in the training. The proposed method is\nimplemented into the Versatile Video Coding (VVC) reference software, VTM-11.0.\nExperimental results show that the ultrafast, very fast, and fast mode of the\nproposed method achieves on average 0.13%/0.34%/0.51%, and 0.10%/0.27%/0.39%\nBD-rate reduction, under the all intra (AI) and random access (RA)\nconfigurations. Especially, our method has friendly time and computational\ncomplexity, only 101%/102%-104%/108% time increase with 0.13-0.93 kMACs/pixel,\nand only 164-1148 KB storage cost for a single model. Our solution may shed\nlight on the journey of practical neural network-based coding tool evolution."
                },
                "authors": [
                    {
                        "name": "Zhuoyuan Li"
                    },
                    {
                        "name": "Jiacheng Li"
                    },
                    {
                        "name": "Yao Li"
                    },
                    {
                        "name": "Li Li"
                    },
                    {
                        "name": "Dong Liu"
                    },
                    {
                        "name": "Feng Wu"
                    }
                ],
                "author_detail": {
                    "name": "Feng Wu"
                },
                "author": "Feng Wu",
                "arxiv_comment": "11 pages, 6 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.10926v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.10926v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.IV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.IV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2208.12453v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2208.12453v2",
                "updated": "2024-09-11T02:33:06Z",
                "updated_parsed": [
                    2024,
                    9,
                    11,
                    2,
                    33,
                    6,
                    2,
                    255,
                    0
                ],
                "published": "2022-08-26T06:28:08Z",
                "published_parsed": [
                    2022,
                    8,
                    26,
                    6,
                    28,
                    8,
                    4,
                    238,
                    0
                ],
                "title": "Exploiting Deep Reinforcement Learning for Edge Caching in Cell-Free\n  Massive MIMO Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Exploiting Deep Reinforcement Learning for Edge Caching in Cell-Free\n  Massive MIMO Systems"
                },
                "summary": "Cell-free massive multiple-input-multiple-output is promising to meet the\nstringent quality-of-experience (QoE) requirements of railway wireless\ncommunications by coordinating many successional access points (APs) to serve\nthe onboard users coherently. A key challenge is how to deliver the desired\ncontents timely due to the radical changing propagation environment caused by\nthe growing train speed. In this paper, we propose to proactively cache the\nlikely-requesting contents at the upcoming APs which perform the coherent\ntransmission to reduce end-to-end delay. A long-term QoE-maximization problem\nis formulated and two cache placement algorithms are proposed. One is based on\nheuristic convex optimization (HCO) and the other exploits deep reinforcement\nlearning (DRL) with soft actor-critic (SAC). Compared to the conventional\nbenchmark, numerical results show the advantage of our proposed algorithms on\nQoE and hit probability. With the advanced DRL model, SAC outperforms HCO on\nQoE by predicting the user requests accurately.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cell-free massive multiple-input-multiple-output is promising to meet the\nstringent quality-of-experience (QoE) requirements of railway wireless\ncommunications by coordinating many successional access points (APs) to serve\nthe onboard users coherently. A key challenge is how to deliver the desired\ncontents timely due to the radical changing propagation environment caused by\nthe growing train speed. In this paper, we propose to proactively cache the\nlikely-requesting contents at the upcoming APs which perform the coherent\ntransmission to reduce end-to-end delay. A long-term QoE-maximization problem\nis formulated and two cache placement algorithms are proposed. One is based on\nheuristic convex optimization (HCO) and the other exploits deep reinforcement\nlearning (DRL) with soft actor-critic (SAC). Compared to the conventional\nbenchmark, numerical results show the advantage of our proposed algorithms on\nQoE and hit probability. With the advanced DRL model, SAC outperforms HCO on\nQoE by predicting the user requests accurately."
                },
                "authors": [
                    {
                        "name": "Yu Zhang"
                    },
                    {
                        "name": "Shuaifei Chen"
                    },
                    {
                        "name": "Jiayi Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Jiayi Zhang"
                },
                "author": "Jiayi Zhang",
                "arxiv_comment": "The focus of the research has shifted, and the current submission is\n  no longer aligned with our objectives",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2208.12453v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2208.12453v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IT",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2401.11504v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2401.11504v3",
                "updated": "2024-09-11T02:22:58Z",
                "updated_parsed": [
                    2024,
                    9,
                    11,
                    2,
                    22,
                    58,
                    2,
                    255,
                    0
                ],
                "published": "2024-01-21T14:28:41Z",
                "published_parsed": [
                    2024,
                    1,
                    21,
                    14,
                    28,
                    41,
                    6,
                    21,
                    0
                ],
                "title": "With Greater Text Comes Greater Necessity: Inference-Time Training Helps\n  Long Text Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With Greater Text Comes Greater Necessity: Inference-Time Training Helps\n  Long Text Generation"
                },
                "summary": "Long text generation, such as novel writing and discourse-level translation\nwith extremely long contexts, presents significant challenges to current\nlanguage models. Existing methods mainly focus on extending the model's context\nwindow through strategies like length extrapolation. However, these approaches\ndemand substantial hardware resources during the training and/or inference\nphases. Our proposed method, Temp-Lora, introduces an alternative concept.\nInstead of relying on the KV cache to store all context information, we embeds\nthis information directly into a temporary Lora module. In the process of long\ntext generation, this module is progressively trained with text generated\npreviously. This approach not only efficiently preserves contextual knowledge\nbut also prevents any permanent alteration to the model's parameters given that\nthe module is discarded post-generation. Extensive experiments on the PG19\nlanguage modeling benchmark and the GuoFeng discourse-level translation\nbenchmark validate the effectiveness of Temp-Lora. Our results show that: 1)\nTemp-Lora substantially enhances generation quality for long text, as indicated\nby a 13.2% decrease in perplexity (PPL) on a subset of PG19, and a 29.3%\ndecrease in PPL along with a 113.2% increase in BLEU score on a subset of\nGuoFeng, 2) Temp-Lora is compatible with and enhances most existing long text\ngeneration methods, and 3) Temp-Lora can greatly reduce computational costs by\nshortening the context window. For example, we can ensure a moderate\nimprovement in generation quality (a decrease of 3.8% in PPL) while enabling a\n51.5% memory usage reduction and a 60.0% decrease in latency for inference.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Long text generation, such as novel writing and discourse-level translation\nwith extremely long contexts, presents significant challenges to current\nlanguage models. Existing methods mainly focus on extending the model's context\nwindow through strategies like length extrapolation. However, these approaches\ndemand substantial hardware resources during the training and/or inference\nphases. Our proposed method, Temp-Lora, introduces an alternative concept.\nInstead of relying on the KV cache to store all context information, we embeds\nthis information directly into a temporary Lora module. In the process of long\ntext generation, this module is progressively trained with text generated\npreviously. This approach not only efficiently preserves contextual knowledge\nbut also prevents any permanent alteration to the model's parameters given that\nthe module is discarded post-generation. Extensive experiments on the PG19\nlanguage modeling benchmark and the GuoFeng discourse-level translation\nbenchmark validate the effectiveness of Temp-Lora. Our results show that: 1)\nTemp-Lora substantially enhances generation quality for long text, as indicated\nby a 13.2% decrease in perplexity (PPL) on a subset of PG19, and a 29.3%\ndecrease in PPL along with a 113.2% increase in BLEU score on a subset of\nGuoFeng, 2) Temp-Lora is compatible with and enhances most existing long text\ngeneration methods, and 3) Temp-Lora can greatly reduce computational costs by\nshortening the context window. For example, we can ensure a moderate\nimprovement in generation quality (a decrease of 3.8% in PPL) while enabling a\n51.5% memory usage reduction and a 60.0% decrease in latency for inference."
                },
                "authors": [
                    {
                        "name": "Y. Wang"
                    },
                    {
                        "name": "D. Ma"
                    },
                    {
                        "name": "D. Cai"
                    }
                ],
                "author_detail": {
                    "name": "D. Cai"
                },
                "author": "D. Cai",
                "arxiv_comment": "COLM 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2401.11504v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2401.11504v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.06217v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.06217v1",
                "updated": "2024-09-10T04:58:48Z",
                "updated_parsed": [
                    2024,
                    9,
                    10,
                    4,
                    58,
                    48,
                    1,
                    254,
                    0
                ],
                "published": "2024-09-10T04:58:48Z",
                "published_parsed": [
                    2024,
                    9,
                    10,
                    4,
                    58,
                    48,
                    1,
                    254,
                    0
                ],
                "title": "DACAT: Dual-stream Adaptive Clip-aware Time Modeling for Robust Online\n  Surgical Phase Recognition",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DACAT: Dual-stream Adaptive Clip-aware Time Modeling for Robust Online\n  Surgical Phase Recognition"
                },
                "summary": "Surgical phase recognition has become a crucial requirement in laparoscopic\nsurgery, enabling various clinical applications like surgical risk forecasting.\nCurrent methods typically identify the surgical phase using individual\nframe-wise embeddings as the fundamental unit for time modeling. However, this\napproach is overly sensitive to current observations, often resulting in\ndiscontinuous and erroneous predictions within a complete surgical phase. In\nthis paper, we propose DACAT, a novel dual-stream model that adaptively learns\nclip-aware context information to enhance the temporal relationship. In one\nstream, DACAT pretrains a frame encoder, caching all historical frame-wise\nfeatures. In the other stream, DACAT fine-tunes a new frame encoder to extract\nthe frame-wise feature at the current moment. Additionally, a max clip-response\nread-out (Max-R) module is introduced to bridge the two streams by using the\ncurrent frame-wise feature to adaptively fetch the most relevant past clip from\nthe feature cache. The clip-aware context feature is then encoded via\ncross-attention between the current frame and its fetched adaptive clip, and\nfurther utilized to enhance the time modeling for accurate online surgical\nphase recognition. The benchmark results on three public datasets, i.e.,\nCholec80, M2CAI16, and AutoLaparo, demonstrate the superiority of our proposed\nDACAT over existing state-of-the-art methods, with improvements in Jaccard\nscores of at least 4.5%, 4.6%, and 2.7%, respectively. Our code and models have\nbeen released at https://github.com/kk42yy/DACAT.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Surgical phase recognition has become a crucial requirement in laparoscopic\nsurgery, enabling various clinical applications like surgical risk forecasting.\nCurrent methods typically identify the surgical phase using individual\nframe-wise embeddings as the fundamental unit for time modeling. However, this\napproach is overly sensitive to current observations, often resulting in\ndiscontinuous and erroneous predictions within a complete surgical phase. In\nthis paper, we propose DACAT, a novel dual-stream model that adaptively learns\nclip-aware context information to enhance the temporal relationship. In one\nstream, DACAT pretrains a frame encoder, caching all historical frame-wise\nfeatures. In the other stream, DACAT fine-tunes a new frame encoder to extract\nthe frame-wise feature at the current moment. Additionally, a max clip-response\nread-out (Max-R) module is introduced to bridge the two streams by using the\ncurrent frame-wise feature to adaptively fetch the most relevant past clip from\nthe feature cache. The clip-aware context feature is then encoded via\ncross-attention between the current frame and its fetched adaptive clip, and\nfurther utilized to enhance the time modeling for accurate online surgical\nphase recognition. The benchmark results on three public datasets, i.e.,\nCholec80, M2CAI16, and AutoLaparo, demonstrate the superiority of our proposed\nDACAT over existing state-of-the-art methods, with improvements in Jaccard\nscores of at least 4.5%, 4.6%, and 2.7%, respectively. Our code and models have\nbeen released at https://github.com/kk42yy/DACAT."
                },
                "authors": [
                    {
                        "name": "Kaixiang Yang"
                    },
                    {
                        "name": "Qiang Li"
                    },
                    {
                        "name": "Zhiwei Wang"
                    }
                ],
                "author_detail": {
                    "name": "Zhiwei Wang"
                },
                "author": "Zhiwei Wang",
                "arxiv_comment": "5 pages, 4 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.06217v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.06217v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.06207v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.06207v1",
                "updated": "2024-09-10T04:24:22Z",
                "updated_parsed": [
                    2024,
                    9,
                    10,
                    4,
                    24,
                    22,
                    1,
                    254,
                    0
                ],
                "published": "2024-09-10T04:24:22Z",
                "published_parsed": [
                    2024,
                    9,
                    10,
                    4,
                    24,
                    22,
                    1,
                    254,
                    0
                ],
                "title": "Design and Implementation of Online Live Streaming System Using A 3D\n  Engine",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Design and Implementation of Online Live Streaming System Using A 3D\n  Engine"
                },
                "summary": "With the growing demand for live video streaming, there is an increasing need\nfor low-latency and high-quality transmission, especially with the advent of 5G\nnetworks. While 5G offers hardware-level improvements, effective software\nsolutions for minimizing latency remain essential. Current methods, such as\nmulti-channel streaming, fail to address latency issues fundamentally, often\nonly adding new channels without optimizing overall performance. This thesis\nproposes a novel approach using a 3D engine (e.g., Unity 3D) to stream\nmulti-input video data through a single channel with reduced latency. By\nleveraging 3D engine capabilities, such as World/Screen Space Cameras, 3D\nCanvases, and Webcam Textures, the proposed system consolidates video streams\nfrom multiple external cameras into a unified, low-latency output. The\naffiliated project of this thesis demonstrates the implementation of a\nlow-latency multi-channel live video streaming system. It employs the RTSP\nprotocol and examines video encoding techniques, alongside a client-side\napplication based on Unity 3D. The system architecture includes a WebSocket\nserver for persistent connections, an HTTP server for communication, a MySQL\ndatabase for storage, Redis for caching, and Nginx for load balancing. Each\nmodule operates independently, ensuring flexibility and scalability in the\nsystem's design. A key innovation of this system is its use of a 3D scene to\nmap multiple video inputs onto a virtual canvas, recorded by an in-engine\ncamera for transmission. This design minimizes redundant data, enabling an\nefficient and director-guided live streaming network. The thesis concludes by\ndiscussing challenges encountered during the project and provides solutions for\nfuture improvement.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the growing demand for live video streaming, there is an increasing need\nfor low-latency and high-quality transmission, especially with the advent of 5G\nnetworks. While 5G offers hardware-level improvements, effective software\nsolutions for minimizing latency remain essential. Current methods, such as\nmulti-channel streaming, fail to address latency issues fundamentally, often\nonly adding new channels without optimizing overall performance. This thesis\nproposes a novel approach using a 3D engine (e.g., Unity 3D) to stream\nmulti-input video data through a single channel with reduced latency. By\nleveraging 3D engine capabilities, such as World/Screen Space Cameras, 3D\nCanvases, and Webcam Textures, the proposed system consolidates video streams\nfrom multiple external cameras into a unified, low-latency output. The\naffiliated project of this thesis demonstrates the implementation of a\nlow-latency multi-channel live video streaming system. It employs the RTSP\nprotocol and examines video encoding techniques, alongside a client-side\napplication based on Unity 3D. The system architecture includes a WebSocket\nserver for persistent connections, an HTTP server for communication, a MySQL\ndatabase for storage, Redis for caching, and Nginx for load balancing. Each\nmodule operates independently, ensuring flexibility and scalability in the\nsystem's design. A key innovation of this system is its use of a 3D scene to\nmap multiple video inputs onto a virtual canvas, recorded by an in-engine\ncamera for transmission. This design minimizes redundant data, enabling an\nefficient and director-guided live streaming network. The thesis concludes by\ndiscussing challenges encountered during the project and provides solutions for\nfuture improvement."
                },
                "authors": [
                    {
                        "name": "Aizierjiang Aiersilan"
                    }
                ],
                "author_detail": {
                    "name": "Aizierjiang Aiersilan"
                },
                "author": "Aizierjiang Aiersilan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.06207v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.06207v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.05867v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.05867v1",
                "updated": "2024-09-09T17:59:57Z",
                "updated_parsed": [
                    2024,
                    9,
                    9,
                    17,
                    59,
                    57,
                    0,
                    253,
                    0
                ],
                "published": "2024-09-09T17:59:57Z",
                "published_parsed": [
                    2024,
                    9,
                    9,
                    17,
                    59,
                    57,
                    0,
                    253,
                    0
                ],
                "title": "Flash Cache: Reducing Bias in Radiance Cache Based Inverse Rendering",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Flash Cache: Reducing Bias in Radiance Cache Based Inverse Rendering"
                },
                "summary": "State-of-the-art techniques for 3D reconstruction are largely based on\nvolumetric scene representations, which require sampling multiple points to\ncompute the color arriving along a ray. Using these representations for more\ngeneral inverse rendering -- reconstructing geometry, materials, and lighting\nfrom observed images -- is challenging because recursively path-tracing such\nvolumetric representations is expensive. Recent works alleviate this issue\nthrough the use of radiance caches: data structures that store the\nsteady-state, infinite-bounce radiance arriving at any point from any\ndirection. However, these solutions rely on approximations that introduce bias\ninto the renderings and, more importantly, into the gradients used for\noptimization. We present a method that avoids these approximations while\nremaining computationally efficient. In particular, we leverage two techniques\nto reduce variance for unbiased estimators of the rendering equation: (1) an\nocclusion-aware importance sampler for incoming illumination and (2) a fast\ncache architecture that can be used as a control variate for the radiance from\na high-quality, but more expensive, volumetric cache. We show that by removing\nthese biases our approach improves the generality of radiance cache based\ninverse rendering, as well as increasing quality in the presence of challenging\nlight transport effects such as specular reflections.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "State-of-the-art techniques for 3D reconstruction are largely based on\nvolumetric scene representations, which require sampling multiple points to\ncompute the color arriving along a ray. Using these representations for more\ngeneral inverse rendering -- reconstructing geometry, materials, and lighting\nfrom observed images -- is challenging because recursively path-tracing such\nvolumetric representations is expensive. Recent works alleviate this issue\nthrough the use of radiance caches: data structures that store the\nsteady-state, infinite-bounce radiance arriving at any point from any\ndirection. However, these solutions rely on approximations that introduce bias\ninto the renderings and, more importantly, into the gradients used for\noptimization. We present a method that avoids these approximations while\nremaining computationally efficient. In particular, we leverage two techniques\nto reduce variance for unbiased estimators of the rendering equation: (1) an\nocclusion-aware importance sampler for incoming illumination and (2) a fast\ncache architecture that can be used as a control variate for the radiance from\na high-quality, but more expensive, volumetric cache. We show that by removing\nthese biases our approach improves the generality of radiance cache based\ninverse rendering, as well as increasing quality in the presence of challenging\nlight transport effects such as specular reflections."
                },
                "authors": [
                    {
                        "name": "Benjamin Attal"
                    },
                    {
                        "name": "Dor Verbin"
                    },
                    {
                        "name": "Ben Mildenhall"
                    },
                    {
                        "name": "Peter Hedman"
                    },
                    {
                        "name": "Jonathan T. Barron"
                    },
                    {
                        "name": "Matthew O'Toole"
                    },
                    {
                        "name": "Pratul P. Srinivasan"
                    }
                ],
                "author_detail": {
                    "name": "Pratul P. Srinivasan"
                },
                "author": "Pratul P. Srinivasan",
                "arxiv_comment": "Website: https://benattal.github.io/flash-cache/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.05867v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.05867v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.GR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.GR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.03753v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.03753v2",
                "updated": "2024-09-09T10:04:00Z",
                "updated_parsed": [
                    2024,
                    9,
                    9,
                    10,
                    4,
                    0,
                    0,
                    253,
                    0
                ],
                "published": "2024-09-05T17:59:15Z",
                "published_parsed": [
                    2024,
                    9,
                    5,
                    17,
                    59,
                    15,
                    3,
                    249,
                    0
                ],
                "title": "WildVis: Open Source Visualizer for Million-Scale Chat Logs in the Wild",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "WildVis: Open Source Visualizer for Million-Scale Chat Logs in the Wild"
                },
                "summary": "The increasing availability of real-world conversation data offers exciting\nopportunities for researchers to study user-chatbot interactions. However, the\nsheer volume of this data makes manually examining individual conversations\nimpractical. To overcome this challenge, we introduce WildVis, an interactive\ntool that enables fast, versatile, and large-scale conversation analysis.\nWildVis provides search and visualization capabilities in the text and\nembedding spaces based on a list of criteria. To manage million-scale datasets,\nwe implemented optimizations including search index construction, embedding\nprecomputation and compression, and caching to ensure responsive user\ninteractions within seconds. We demonstrate WildVis' utility through three case\nstudies: facilitating chatbot misuse research, visualizing and comparing topic\ndistributions across datasets, and characterizing user-specific conversation\npatterns. WildVis is open-source and designed to be extendable, supporting\nadditional datasets and customized search and visualization functionalities.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The increasing availability of real-world conversation data offers exciting\nopportunities for researchers to study user-chatbot interactions. However, the\nsheer volume of this data makes manually examining individual conversations\nimpractical. To overcome this challenge, we introduce WildVis, an interactive\ntool that enables fast, versatile, and large-scale conversation analysis.\nWildVis provides search and visualization capabilities in the text and\nembedding spaces based on a list of criteria. To manage million-scale datasets,\nwe implemented optimizations including search index construction, embedding\nprecomputation and compression, and caching to ensure responsive user\ninteractions within seconds. We demonstrate WildVis' utility through three case\nstudies: facilitating chatbot misuse research, visualizing and comparing topic\ndistributions across datasets, and characterizing user-specific conversation\npatterns. WildVis is open-source and designed to be extendable, supporting\nadditional datasets and customized search and visualization functionalities."
                },
                "authors": [
                    {
                        "name": "Yuntian Deng"
                    },
                    {
                        "name": "Wenting Zhao"
                    },
                    {
                        "name": "Jack Hessel"
                    },
                    {
                        "name": "Xiang Ren"
                    },
                    {
                        "name": "Claire Cardie"
                    },
                    {
                        "name": "Yejin Choi"
                    }
                ],
                "author_detail": {
                    "name": "Yejin Choi"
                },
                "author": "Yejin Choi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.03753v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.03753v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.05025v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.05025v1",
                "updated": "2024-09-08T08:39:50Z",
                "updated_parsed": [
                    2024,
                    9,
                    8,
                    8,
                    39,
                    50,
                    6,
                    252,
                    0
                ],
                "published": "2024-09-08T08:39:50Z",
                "published_parsed": [
                    2024,
                    9,
                    8,
                    8,
                    39,
                    50,
                    6,
                    252,
                    0
                ],
                "title": "Cooperative Learning-Based Framework for VNF Caching and Placement\n  Optimization over Low Earth Orbit Satellite Networks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cooperative Learning-Based Framework for VNF Caching and Placement\n  Optimization over Low Earth Orbit Satellite Networks"
                },
                "summary": "Low Earth Orbit Satellite Networks (LSNs) are integral to supporting a broad\nrange of modern applications, which are typically modeled as Service Function\nChains (SFCs). Each SFC is composed of Virtual Network Functions (VNFs), where\neach VNF performs a specific task. In this work, we tackle two key challenges\nin deploying SFCs across an LSN. Firstly, we aim to optimize the long-term\nsystem performance by minimizing the average end-to-end SFC execution delay,\ngiven that each satellite comes with a pre-installed/cached subset of VNFs. To\nachieve optimal SFC placement, we formulate an offline Dynamic Programming (DP)\nequation. To overcome the challenges associated with DP, such as its\ncomplexity, the need for probability knowledge, and centralized\ndecision-making, we put forth an online Multi-Agent Q-Learning (MAQL) solution.\nOur MAQL approach addresses convergence issues in the non-stationary LSN\nenvironment by enabling satellites to share learning parameters and update\ntheir Q-tables based on distinct rules for their selected actions. Secondly, to\ndetermine the optimal VNF subsets for satellite caching, we develop a Bayesian\nOptimization (BO)-based learning mechanism that operates both offline and\ncontinuously in the background during runtime. Extensive experiments\ndemonstrate that our MAQL approach achieves near-optimal performance comparable\nto the DP model and significantly outperforms existing baselines. Moreover, the\nBO-based approach effectively enhances the request serving rate over time.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Low Earth Orbit Satellite Networks (LSNs) are integral to supporting a broad\nrange of modern applications, which are typically modeled as Service Function\nChains (SFCs). Each SFC is composed of Virtual Network Functions (VNFs), where\neach VNF performs a specific task. In this work, we tackle two key challenges\nin deploying SFCs across an LSN. Firstly, we aim to optimize the long-term\nsystem performance by minimizing the average end-to-end SFC execution delay,\ngiven that each satellite comes with a pre-installed/cached subset of VNFs. To\nachieve optimal SFC placement, we formulate an offline Dynamic Programming (DP)\nequation. To overcome the challenges associated with DP, such as its\ncomplexity, the need for probability knowledge, and centralized\ndecision-making, we put forth an online Multi-Agent Q-Learning (MAQL) solution.\nOur MAQL approach addresses convergence issues in the non-stationary LSN\nenvironment by enabling satellites to share learning parameters and update\ntheir Q-tables based on distinct rules for their selected actions. Secondly, to\ndetermine the optimal VNF subsets for satellite caching, we develop a Bayesian\nOptimization (BO)-based learning mechanism that operates both offline and\ncontinuously in the background during runtime. Extensive experiments\ndemonstrate that our MAQL approach achieves near-optimal performance comparable\nto the DP model and significantly outperforms existing baselines. Moreover, the\nBO-based approach effectively enhances the request serving rate over time."
                },
                "authors": [
                    {
                        "name": "Khai Doan"
                    },
                    {
                        "name": "Marios Avgeris"
                    },
                    {
                        "name": "Aris Leivadeas"
                    },
                    {
                        "name": "Ioannis Lambadaris"
                    },
                    {
                        "name": "Wonjae Shin"
                    }
                ],
                "author_detail": {
                    "name": "Wonjae Shin"
                },
                "author": "Wonjae Shin",
                "arxiv_comment": "40 pages, 11 figure, 3 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.05025v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.05025v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IT",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.04992v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.04992v1",
                "updated": "2024-09-08T06:06:44Z",
                "updated_parsed": [
                    2024,
                    9,
                    8,
                    6,
                    6,
                    44,
                    6,
                    252,
                    0
                ],
                "published": "2024-09-08T06:06:44Z",
                "published_parsed": [
                    2024,
                    9,
                    8,
                    6,
                    6,
                    44,
                    6,
                    252,
                    0
                ],
                "title": "InstInfer: In-Storage Attention Offloading for Cost-Effective\n  Long-Context LLM Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "InstInfer: In-Storage Attention Offloading for Cost-Effective\n  Long-Context LLM Inference"
                },
                "summary": "The widespread of Large Language Models (LLMs) marks a significant milestone\nin generative AI. Nevertheless, the increasing context length and batch size in\noffline LLM inference escalate the memory requirement of the key-value (KV)\ncache, which imposes a huge burden on the GPU VRAM, especially for\nresource-constraint scenarios (e.g., edge computing and personal devices).\nSeveral cost-effective solutions leverage host memory or SSDs to reduce storage\ncosts for offline inference scenarios and improve the throughput. Nevertheless,\nthey suffer from significant performance penalties imposed by intensive KV\ncache accesses due to limited PCIe bandwidth. To address these issues, we\npropose InstInfer, a novel LLM inference system that offloads the most\nperformance-critical computation (i.e., attention in decoding phase) and data\n(i.e., KV cache) parts to Computational Storage Drives (CSDs), which minimize\nthe enormous KV transfer overheads. InstInfer designs a dedicated flash-aware\nin-storage attention engine with KV cache management mechanisms to exploit the\nhigh internal bandwidths of CSDs instead of being limited by the PCIe\nbandwidth. The optimized P2P transmission between GPU and CSDs further reduces\ndata migration overheads. Experimental results demonstrate that for a 13B model\nusing an NVIDIA A6000 GPU, InstInfer improves throughput for long-sequence\ninference by up to 11.1$\\times$, compared to existing SSD-based solutions such\nas FlexGen.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The widespread of Large Language Models (LLMs) marks a significant milestone\nin generative AI. Nevertheless, the increasing context length and batch size in\noffline LLM inference escalate the memory requirement of the key-value (KV)\ncache, which imposes a huge burden on the GPU VRAM, especially for\nresource-constraint scenarios (e.g., edge computing and personal devices).\nSeveral cost-effective solutions leverage host memory or SSDs to reduce storage\ncosts for offline inference scenarios and improve the throughput. Nevertheless,\nthey suffer from significant performance penalties imposed by intensive KV\ncache accesses due to limited PCIe bandwidth. To address these issues, we\npropose InstInfer, a novel LLM inference system that offloads the most\nperformance-critical computation (i.e., attention in decoding phase) and data\n(i.e., KV cache) parts to Computational Storage Drives (CSDs), which minimize\nthe enormous KV transfer overheads. InstInfer designs a dedicated flash-aware\nin-storage attention engine with KV cache management mechanisms to exploit the\nhigh internal bandwidths of CSDs instead of being limited by the PCIe\nbandwidth. The optimized P2P transmission between GPU and CSDs further reduces\ndata migration overheads. Experimental results demonstrate that for a 13B model\nusing an NVIDIA A6000 GPU, InstInfer improves throughput for long-sequence\ninference by up to 11.1$\\times$, compared to existing SSD-based solutions such\nas FlexGen."
                },
                "authors": [
                    {
                        "name": "Xiurui Pan"
                    },
                    {
                        "name": "Endian Li"
                    },
                    {
                        "name": "Qiao Li"
                    },
                    {
                        "name": "Shengwen Liang"
                    },
                    {
                        "name": "Yizhou Shan"
                    },
                    {
                        "name": "Ke Zhou"
                    },
                    {
                        "name": "Yingwei Luo"
                    },
                    {
                        "name": "Xiaolin Wang"
                    },
                    {
                        "name": "Jie Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Jie Zhang"
                },
                "author": "Jie Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.04992v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.04992v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.04750v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.04750v1",
                "updated": "2024-09-07T07:50:13Z",
                "updated_parsed": [
                    2024,
                    9,
                    7,
                    7,
                    50,
                    13,
                    5,
                    251,
                    0
                ],
                "published": "2024-09-07T07:50:13Z",
                "published_parsed": [
                    2024,
                    9,
                    7,
                    7,
                    50,
                    13,
                    5,
                    251,
                    0
                ],
                "title": "Training-Free Style Consistent Image Synthesis with Condition and Mask\n  Guidance in E-Commerce",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Training-Free Style Consistent Image Synthesis with Condition and Mask\n  Guidance in E-Commerce"
                },
                "summary": "Generating style-consistent images is a common task in the e-commerce field,\nand current methods are largely based on diffusion models, which have achieved\nexcellent results. This paper introduces the concept of the QKV\n(query/key/value) level, referring to modifications in the attention maps\n(self-attention and cross-attention) when integrating UNet with image\nconditions. Without disrupting the product's main composition in e-commerce\nimages, we aim to use a train-free method guided by pre-set conditions. This\ninvolves using shared KV to enhance similarity in cross-attention and\ngenerating mask guidance from the attention map to cleverly direct the\ngeneration of style-consistent images. Our method has shown promising results\nin practical applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Generating style-consistent images is a common task in the e-commerce field,\nand current methods are largely based on diffusion models, which have achieved\nexcellent results. This paper introduces the concept of the QKV\n(query/key/value) level, referring to modifications in the attention maps\n(self-attention and cross-attention) when integrating UNet with image\nconditions. Without disrupting the product's main composition in e-commerce\nimages, we aim to use a train-free method guided by pre-set conditions. This\ninvolves using shared KV to enhance similarity in cross-attention and\ngenerating mask guidance from the attention map to cleverly direct the\ngeneration of style-consistent images. Our method has shown promising results\nin practical applications."
                },
                "authors": [
                    {
                        "name": "Guandong Li"
                    }
                ],
                "author_detail": {
                    "name": "Guandong Li"
                },
                "author": "Guandong Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.04750v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.04750v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.14366v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.14366v2",
                "updated": "2024-09-07T02:52:29Z",
                "updated_parsed": [
                    2024,
                    9,
                    7,
                    2,
                    52,
                    29,
                    5,
                    251,
                    0
                ],
                "published": "2024-05-23T09:43:52Z",
                "published_parsed": [
                    2024,
                    5,
                    23,
                    9,
                    43,
                    52,
                    3,
                    144,
                    0
                ],
                "title": "MiniCache: KV Cache Compression in Depth Dimension for Large Language\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MiniCache: KV Cache Compression in Depth Dimension for Large Language\n  Models"
                },
                "summary": "A critical approach for efficiently deploying computationally demanding large\nlanguage models (LLMs) is Key-Value (KV) caching. The KV cache stores key-value\nstates of previously generated tokens, significantly reducing the need for\nrepetitive computations and thereby lowering latency in autoregressive\ngeneration. However, the size of the KV cache grows linearly with sequence\nlength, posing challenges for applications requiring long context input and\nextensive sequence generation. In this paper, we present a simple yet effective\napproach, called MiniCache, to compress the KV cache across layers from a novel\ndepth perspective, significantly reducing the memory footprint for LLM\ninference. Our approach is based on the observation that KV cache states\nexhibit high similarity between the adjacent layers in the middle-to-deep\nportion of LLMs. To facilitate merging, we propose disentangling the states\ninto the magnitude and direction components, interpolating the directions of\nthe state vectors while preserving their lengths unchanged. Furthermore, we\nintroduce a token retention strategy to keep highly distinct state pairs\nunmerged, thus preserving the information with minimal additional storage\noverhead. Our MiniCache is training-free and general, complementing existing KV\ncache compression strategies, such as quantization and sparsity. We conduct a\ncomprehensive evaluation of MiniCache utilizing various models including\nLLaMA-2, LLaMA-3, Phi-3, Mistral, and Mixtral across multiple benchmarks,\ndemonstrating its exceptional performance in achieving superior compression\nratios and high throughput. On the ShareGPT dataset, LLaMA-2-7B with 4-bit\nMiniCache achieves a remarkable compression ratio of up to 5.02x, enhances\ninference throughput by approximately 5x, and reduces the memory footprint by\n41% compared to the FP16 full cache baseline, all while maintaining\nnear-lossless performance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A critical approach for efficiently deploying computationally demanding large\nlanguage models (LLMs) is Key-Value (KV) caching. The KV cache stores key-value\nstates of previously generated tokens, significantly reducing the need for\nrepetitive computations and thereby lowering latency in autoregressive\ngeneration. However, the size of the KV cache grows linearly with sequence\nlength, posing challenges for applications requiring long context input and\nextensive sequence generation. In this paper, we present a simple yet effective\napproach, called MiniCache, to compress the KV cache across layers from a novel\ndepth perspective, significantly reducing the memory footprint for LLM\ninference. Our approach is based on the observation that KV cache states\nexhibit high similarity between the adjacent layers in the middle-to-deep\nportion of LLMs. To facilitate merging, we propose disentangling the states\ninto the magnitude and direction components, interpolating the directions of\nthe state vectors while preserving their lengths unchanged. Furthermore, we\nintroduce a token retention strategy to keep highly distinct state pairs\nunmerged, thus preserving the information with minimal additional storage\noverhead. Our MiniCache is training-free and general, complementing existing KV\ncache compression strategies, such as quantization and sparsity. We conduct a\ncomprehensive evaluation of MiniCache utilizing various models including\nLLaMA-2, LLaMA-3, Phi-3, Mistral, and Mixtral across multiple benchmarks,\ndemonstrating its exceptional performance in achieving superior compression\nratios and high throughput. On the ShareGPT dataset, LLaMA-2-7B with 4-bit\nMiniCache achieves a remarkable compression ratio of up to 5.02x, enhances\ninference throughput by approximately 5x, and reduces the memory footprint by\n41% compared to the FP16 full cache baseline, all while maintaining\nnear-lossless performance."
                },
                "authors": [
                    {
                        "name": "Akide Liu"
                    },
                    {
                        "name": "Jing Liu"
                    },
                    {
                        "name": "Zizheng Pan"
                    },
                    {
                        "name": "Yefei He"
                    },
                    {
                        "name": "Gholamreza Haffari"
                    },
                    {
                        "name": "Bohan Zhuang"
                    }
                ],
                "author_detail": {
                    "name": "Bohan Zhuang"
                },
                "author": "Bohan Zhuang",
                "arxiv_comment": "Project is available at https://minicache.vmv.re",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.14366v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.14366v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.03637v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.03637v4",
                "updated": "2024-09-06T08:28:01Z",
                "updated_parsed": [
                    2024,
                    9,
                    6,
                    8,
                    28,
                    1,
                    4,
                    250,
                    0
                ],
                "published": "2024-07-04T05:13:58Z",
                "published_parsed": [
                    2024,
                    7,
                    4,
                    5,
                    13,
                    58,
                    3,
                    186,
                    0
                ],
                "title": "QET: Enhancing Quantized LLM Parameters and KV cache Compression through\n  Element Substitution and Residual Clustering",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "QET: Enhancing Quantized LLM Parameters and KV cache Compression through\n  Element Substitution and Residual Clustering"
                },
                "summary": "The matrix quantization entails representing matrix elements in a more\nspace-efficient form to reduce storage usage, with dequantization restoring the\noriginal matrix for use. We formulate the Quantization Error Minimization (QEM)\nproblem as minimizing the distance between a matrix before and after\nquantization, under the condition that the quantized matrix occupies the same\nmemory space. Matrix quantization is crucial in various applications, including\nLarge Language Models (LLMs) weight quantization, vector databases, KV cache\nquantization, graph compression, and image compression. Recent advancements in\nLLMs, such as GPT-4 and BERT, have highlighted the importance of matrix\ncompression due to the large size of parameters and KV cache, which are stored\nas matrices.\n  We propose Quantum Entanglement Trees (QET) to address the QEM problem by\nleveraging the local orderliness of matrix elements, involving iterative\nelement swapping to form a locally ordered matrix. This matrix is then grouped\nand quantized by columns. To enhance QET, we introduce two optimizations:\nfurther quantizing residuals to reduce MSE, and using masking and batch\nprocessing to accelerate the algorithm.\n  Experimental results demonstrate that QET can effectively reduce MSE to\n5.05%, 13.33%, and 11.89% of the current best method on the LLM dataset, K\ncache, and V cache, respectively. Our contributions include the abstraction of\nthe QEM problem, the design of the QET algorithm, and the proposal of two\noptimizations to improve accuracy and speed.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The matrix quantization entails representing matrix elements in a more\nspace-efficient form to reduce storage usage, with dequantization restoring the\noriginal matrix for use. We formulate the Quantization Error Minimization (QEM)\nproblem as minimizing the distance between a matrix before and after\nquantization, under the condition that the quantized matrix occupies the same\nmemory space. Matrix quantization is crucial in various applications, including\nLarge Language Models (LLMs) weight quantization, vector databases, KV cache\nquantization, graph compression, and image compression. Recent advancements in\nLLMs, such as GPT-4 and BERT, have highlighted the importance of matrix\ncompression due to the large size of parameters and KV cache, which are stored\nas matrices.\n  We propose Quantum Entanglement Trees (QET) to address the QEM problem by\nleveraging the local orderliness of matrix elements, involving iterative\nelement swapping to form a locally ordered matrix. This matrix is then grouped\nand quantized by columns. To enhance QET, we introduce two optimizations:\nfurther quantizing residuals to reduce MSE, and using masking and batch\nprocessing to accelerate the algorithm.\n  Experimental results demonstrate that QET can effectively reduce MSE to\n5.05%, 13.33%, and 11.89% of the current best method on the LLM dataset, K\ncache, and V cache, respectively. Our contributions include the abstraction of\nthe QEM problem, the design of the QET algorithm, and the proposal of two\noptimizations to improve accuracy and speed."
                },
                "authors": [
                    {
                        "name": "Yanshu Wang"
                    },
                    {
                        "name": "Wang Li"
                    },
                    {
                        "name": "Zhaoqian Yao"
                    },
                    {
                        "name": "Tong Yang"
                    }
                ],
                "author_detail": {
                    "name": "Tong Yang"
                },
                "author": "Tong Yang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.03637v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.03637v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.04040v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.04040v1",
                "updated": "2024-09-06T06:16:55Z",
                "updated_parsed": [
                    2024,
                    9,
                    6,
                    6,
                    16,
                    55,
                    4,
                    250,
                    0
                ],
                "published": "2024-09-06T06:16:55Z",
                "published_parsed": [
                    2024,
                    9,
                    6,
                    6,
                    16,
                    55,
                    4,
                    250,
                    0
                ],
                "title": "A First Look At Efficient And Secure On-Device LLM Inference Against KV\n  Leakage",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A First Look At Efficient And Secure On-Device LLM Inference Against KV\n  Leakage"
                },
                "summary": "Running LLMs on end devices has garnered significant attention recently due\nto their advantages in privacy preservation. With the advent of lightweight LLM\nmodels and specially designed GPUs, on-device LLM inference has achieved the\nnecessary accuracy and performance metrics. However, we have identified that\nLLM inference on GPUs can leak privacy-sensitive intermediate information,\nspecifically the KV pairs. An attacker could exploit these KV pairs to\nreconstruct the entire user conversation, leading to significant\nvulnerabilities. Existing solutions, such as Fully Homomorphic Encryption (FHE)\nand Trusted Execution Environments (TEE), are either too computation-intensive\nor resource-limited. To address these issues, we designed KV-Shield, which\noperates in two phases. In the initialization phase, it permutes the weight\nmatrices so that all KV pairs are correspondingly permuted. During the runtime\nphase, the attention vector is inversely permuted to ensure the correctness of\nthe layer output. All permutation-related operations are executed within the\nTEE, ensuring that insecure GPUs cannot access the original KV pairs, thus\npreventing conversation reconstruction. Finally, we theoretically analyze the\ncorrectness of KV-Shield, along with its advantages and overhead.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Running LLMs on end devices has garnered significant attention recently due\nto their advantages in privacy preservation. With the advent of lightweight LLM\nmodels and specially designed GPUs, on-device LLM inference has achieved the\nnecessary accuracy and performance metrics. However, we have identified that\nLLM inference on GPUs can leak privacy-sensitive intermediate information,\nspecifically the KV pairs. An attacker could exploit these KV pairs to\nreconstruct the entire user conversation, leading to significant\nvulnerabilities. Existing solutions, such as Fully Homomorphic Encryption (FHE)\nand Trusted Execution Environments (TEE), are either too computation-intensive\nor resource-limited. To address these issues, we designed KV-Shield, which\noperates in two phases. In the initialization phase, it permutes the weight\nmatrices so that all KV pairs are correspondingly permuted. During the runtime\nphase, the attention vector is inversely permuted to ensure the correctness of\nthe layer output. All permutation-related operations are executed within the\nTEE, ensuring that insecure GPUs cannot access the original KV pairs, thus\npreventing conversation reconstruction. Finally, we theoretically analyze the\ncorrectness of KV-Shield, along with its advantages and overhead."
                },
                "authors": [
                    {
                        "name": "Huan Yang"
                    },
                    {
                        "name": "Deyu Zhang"
                    },
                    {
                        "name": "Yudong Zhao"
                    },
                    {
                        "name": "Yuanchun Li"
                    },
                    {
                        "name": "Yunxin Liu"
                    }
                ],
                "author_detail": {
                    "name": "Yunxin Liu"
                },
                "author": "Yunxin Liu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.04040v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.04040v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.03308v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.03308v2",
                "updated": "2024-09-05T20:21:54Z",
                "updated_parsed": [
                    2024,
                    9,
                    5,
                    20,
                    21,
                    54,
                    3,
                    249,
                    0
                ],
                "published": "2024-08-06T17:16:19Z",
                "published_parsed": [
                    2024,
                    8,
                    6,
                    17,
                    16,
                    19,
                    1,
                    219,
                    0
                ],
                "title": "Potential and Limitation of High-Frequency Cores and Caches",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Potential and Limitation of High-Frequency Cores and Caches"
                },
                "summary": "This paper explores the potential of cryogenic semiconductor computing and\nsuperconductor electronics as promising alternatives to traditional\nsemiconductor devices. As semiconductor devices face challenges such as\nincreased leakage currents and reduced performance at higher temperatures,\nthese novel technologies offer high performance and low power computation.\nConventional semiconductor electronics operating at cryogenic temperatures\n(below -150{\\deg}C or 123.15 K) can benefit from reduced leakage currents and\nimproved electron mobility. On the other hand, superconductor electronics,\noperating below 10 K, allow electrons to flow without resistance, offering the\npotential for ultra-low-power, high-speed computation. This study presents a\ncomprehensive performance modeling and analysis of these technologies and\nprovides insights into their potential benefits and limitations. We implement\nmodels of in-order and out-of-order cores operating at high clock frequencies\nassociated with superconductor electronics and cryogenic semiconductor\ncomputing in gem5. We evaluate the performance of these components using\nworkloads representative of real-world applications like NPB, SPEC CPU2006, and\nGAPBS. Our results show the potential speedups achievable by these components\nand the limitations posed by cache bandwidth. This work provides valuable\ninsights into the performance implications and design trade-offs associated\nwith cryogenic and superconductor technologies, laying the foundation for\nfuture research in this field using gem5.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper explores the potential of cryogenic semiconductor computing and\nsuperconductor electronics as promising alternatives to traditional\nsemiconductor devices. As semiconductor devices face challenges such as\nincreased leakage currents and reduced performance at higher temperatures,\nthese novel technologies offer high performance and low power computation.\nConventional semiconductor electronics operating at cryogenic temperatures\n(below -150{\\deg}C or 123.15 K) can benefit from reduced leakage currents and\nimproved electron mobility. On the other hand, superconductor electronics,\noperating below 10 K, allow electrons to flow without resistance, offering the\npotential for ultra-low-power, high-speed computation. This study presents a\ncomprehensive performance modeling and analysis of these technologies and\nprovides insights into their potential benefits and limitations. We implement\nmodels of in-order and out-of-order cores operating at high clock frequencies\nassociated with superconductor electronics and cryogenic semiconductor\ncomputing in gem5. We evaluate the performance of these components using\nworkloads representative of real-world applications like NPB, SPEC CPU2006, and\nGAPBS. Our results show the potential speedups achievable by these components\nand the limitations posed by cache bandwidth. This work provides valuable\ninsights into the performance implications and design trade-offs associated\nwith cryogenic and superconductor technologies, laying the foundation for\nfuture research in this field using gem5."
                },
                "authors": [
                    {
                        "name": "Kunal Pai"
                    },
                    {
                        "name": "Anusheel Nand"
                    },
                    {
                        "name": "Jason Lowe-Power"
                    }
                ],
                "author_detail": {
                    "name": "Jason Lowe-Power"
                },
                "author": "Jason Lowe-Power",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.03308v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.03308v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.03743v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.03743v1",
                "updated": "2024-09-05T17:56:19Z",
                "updated_parsed": [
                    2024,
                    9,
                    5,
                    17,
                    56,
                    19,
                    3,
                    249,
                    0
                ],
                "published": "2024-09-05T17:56:19Z",
                "published_parsed": [
                    2024,
                    9,
                    5,
                    17,
                    56,
                    19,
                    3,
                    249,
                    0
                ],
                "title": "Libra: Architectural Support For Principled, Secure And Efficient\n  Balanced Execution On High-End Processors (Extended Version)",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Libra: Architectural Support For Principled, Secure And Efficient\n  Balanced Execution On High-End Processors (Extended Version)"
                },
                "summary": "Control-flow leakage (CFL) attacks enable an attacker to expose control-flow\ndecisions of a victim program via side-channel observations. Linearization\n(i.e., elimination) of secret-dependent control flow is the main countermeasure\nagainst these attacks, yet it comes at a non-negligible cost. Conversely,\nbalancing secret-dependent branches often incurs a smaller overhead, but is\nnotoriously insecure on high-end processors. Hence, linearization has been\nwidely believed to be the only effective countermeasure against CFL attacks. In\nthis paper, we challenge this belief and investigate an unexplored alternative:\nhow to securely balance secret-dependent branches on higher-end processors?\n  We propose Libra, a generic and principled hardware-software codesign to\nefficiently address CFL on high-end processors. We perform a systematic\nclassification of hardware primitives leaking control flow from the literature,\nand provide guidelines to handle them with our design. Importantly, Libra\nenables secure control-flow balancing without the need to disable\nperformance-critical hardware such as the instruction cache and the prefetcher.\nWe formalize the semantics of Libra and propose a code transformation algorithm\nfor securing programs, which we prove correct and secure. Finally, we implement\nand evaluate Libra on an out-of-order RISC-V processor, showing performance\noverhead on par with insecure balanced code, and outperforming state-of-the-art\nlinearized code by 19.3%.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Control-flow leakage (CFL) attacks enable an attacker to expose control-flow\ndecisions of a victim program via side-channel observations. Linearization\n(i.e., elimination) of secret-dependent control flow is the main countermeasure\nagainst these attacks, yet it comes at a non-negligible cost. Conversely,\nbalancing secret-dependent branches often incurs a smaller overhead, but is\nnotoriously insecure on high-end processors. Hence, linearization has been\nwidely believed to be the only effective countermeasure against CFL attacks. In\nthis paper, we challenge this belief and investigate an unexplored alternative:\nhow to securely balance secret-dependent branches on higher-end processors?\n  We propose Libra, a generic and principled hardware-software codesign to\nefficiently address CFL on high-end processors. We perform a systematic\nclassification of hardware primitives leaking control flow from the literature,\nand provide guidelines to handle them with our design. Importantly, Libra\nenables secure control-flow balancing without the need to disable\nperformance-critical hardware such as the instruction cache and the prefetcher.\nWe formalize the semantics of Libra and propose a code transformation algorithm\nfor securing programs, which we prove correct and secure. Finally, we implement\nand evaluate Libra on an out-of-order RISC-V processor, showing performance\noverhead on par with insecure balanced code, and outperforming state-of-the-art\nlinearized code by 19.3%."
                },
                "authors": [
                    {
                        "name": "Hans Winderix"
                    },
                    {
                        "name": "Marton Bognar"
                    },
                    {
                        "name": "Lesly-Ann Daniel"
                    },
                    {
                        "name": "Frank Piessens"
                    }
                ],
                "author_detail": {
                    "name": "Frank Piessens"
                },
                "author": "Frank Piessens",
                "arxiv_doi": "10.1145/3658644.3690319",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3658644.3690319",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2409.03743v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.03743v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.03568v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.03568v1",
                "updated": "2024-09-05T14:22:02Z",
                "updated_parsed": [
                    2024,
                    9,
                    5,
                    14,
                    22,
                    2,
                    3,
                    249,
                    0
                ],
                "published": "2024-09-05T14:22:02Z",
                "published_parsed": [
                    2024,
                    9,
                    5,
                    14,
                    22,
                    2,
                    3,
                    249,
                    0
                ],
                "title": "Enabling Practical and Privacy-Preserving Image Processing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Enabling Practical and Privacy-Preserving Image Processing"
                },
                "summary": "Fully Homomorphic Encryption (FHE) enables computations on encrypted data,\npreserving confidentiality without the need for decryption. However, FHE is\noften hindered by significant performance overhead, particularly for\nhigh-precision and complex data like images. Due to serious efficiency issues,\ntraditional FHE methods often encrypt images by monolithic data blocks (such as\npixel rows), instead of pixels. However, this strategy compromises the\nadvantages of homomorphic operations and disables pixel-level image processing.\nIn this study, we address these challenges by proposing and implementing a\npixel-level homomorphic encryption approach, iCHEETAH, based on the CKKS\nscheme. To enhance computational efficiency, we introduce three novel caching\nmechanisms to pre-encrypt radix values or frequently occurring pixel values,\nsubstantially reducing redundant encryption operations. Extensive experiments\ndemonstrate that our approach achieves up to a 19-fold improvement in\nencryption speed compared to the original CKKS, while maintaining high image\nquality. Additionally, real-world image applications such as mean filtering,\nbrightness enhancement, image matching and watermarking are tested based on\nFHE, showcasing up to a 91.53% speed improvement. We also proved that our\nmethod is IND-CPA (Indistinguishability under Chosen Plaintext Attack) secure,\nproviding strong encryption security. These results underscore the practicality\nand efficiency of iCHEETAH, marking a significant advancement in\nprivacy-preserving image processing at scale.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Fully Homomorphic Encryption (FHE) enables computations on encrypted data,\npreserving confidentiality without the need for decryption. However, FHE is\noften hindered by significant performance overhead, particularly for\nhigh-precision and complex data like images. Due to serious efficiency issues,\ntraditional FHE methods often encrypt images by monolithic data blocks (such as\npixel rows), instead of pixels. However, this strategy compromises the\nadvantages of homomorphic operations and disables pixel-level image processing.\nIn this study, we address these challenges by proposing and implementing a\npixel-level homomorphic encryption approach, iCHEETAH, based on the CKKS\nscheme. To enhance computational efficiency, we introduce three novel caching\nmechanisms to pre-encrypt radix values or frequently occurring pixel values,\nsubstantially reducing redundant encryption operations. Extensive experiments\ndemonstrate that our approach achieves up to a 19-fold improvement in\nencryption speed compared to the original CKKS, while maintaining high image\nquality. Additionally, real-world image applications such as mean filtering,\nbrightness enhancement, image matching and watermarking are tested based on\nFHE, showcasing up to a 91.53% speed improvement. We also proved that our\nmethod is IND-CPA (Indistinguishability under Chosen Plaintext Attack) secure,\nproviding strong encryption security. These results underscore the practicality\nand efficiency of iCHEETAH, marking a significant advancement in\nprivacy-preserving image processing at scale."
                },
                "authors": [
                    {
                        "name": "Chao Wang"
                    },
                    {
                        "name": "Shubing Yang"
                    },
                    {
                        "name": "Xiaoyan Sun"
                    },
                    {
                        "name": "Jun Dai"
                    },
                    {
                        "name": "Dongfang Zhao"
                    }
                ],
                "author_detail": {
                    "name": "Dongfang Zhao"
                },
                "author": "Dongfang Zhao",
                "arxiv_comment": "16 pages, 10 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.03568v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.03568v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "C.2.0; K.6.5",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.02088v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.02088v2",
                "updated": "2024-09-05T01:12:04Z",
                "updated_parsed": [
                    2024,
                    9,
                    5,
                    1,
                    12,
                    4,
                    3,
                    249,
                    0
                ],
                "published": "2024-09-03T17:40:24Z",
                "published_parsed": [
                    2024,
                    9,
                    3,
                    17,
                    40,
                    24,
                    1,
                    247,
                    0
                ],
                "title": "SELCC: Coherent Caching over Compute-Limited Disaggregated Memory",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SELCC: Coherent Caching over Compute-Limited Disaggregated Memory"
                },
                "summary": "Disaggregating memory from compute offers the opportunity to better utilize\nstranded memory in data centers. It is important to cache data in the compute\nnodes and maintain cache coherence across multiple compute nodes to save on\nround-trip communication cost between the disaggregated memory and the compute\nnodes. However, the limited computing power on the disaggregated memory servers\nmakes it challenging to maintain cache coherence among multiple compute-side\ncaches over disaggregated shared memory. This paper introduces SELCC; a\nShared-Exclusive Latch Cache Coherence protocol that maintains cache coherence\nwithout imposing any computational burden on the remote memory side. SELCC\nbuilds on a one-sided shared-exclusive latch protocol by introducing lazy latch\nrelease and invalidation messages among the compute nodes so that it can\nguarantee both data access atomicity and cache coherence. SELCC minimizes\ncommunication round-trips by embedding the current cache copy holder IDs into\nRDMA latch words and prioritizes local concurrency control over global\nconcurrency control. We instantiate the SELCC protocol onto compute-sided\ncache, forming an abstraction layer over disaggregated memory. This abstraction\nlayer provides main-memory-like APIs to upper-level applications, and thus\nenabling existing data structures and algorithms to function over disaggregated\nmemory with minimal code change. To demonstrate the usability of SELCC, we\nimplement a B-tree and three transaction concurrency control algorithms over\nSELCC's APIs. Micro-benchmark results show that the SELCC protocol achieves\nbetter performance compared to RPC-based cache-coherence protocols.\nAdditionally, YCSB and TPC-C benchmarks indicate that applications over SELCC\ncan achieve comparable or superior performance against competitors over\ndisaggregated memory.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Disaggregating memory from compute offers the opportunity to better utilize\nstranded memory in data centers. It is important to cache data in the compute\nnodes and maintain cache coherence across multiple compute nodes to save on\nround-trip communication cost between the disaggregated memory and the compute\nnodes. However, the limited computing power on the disaggregated memory servers\nmakes it challenging to maintain cache coherence among multiple compute-side\ncaches over disaggregated shared memory. This paper introduces SELCC; a\nShared-Exclusive Latch Cache Coherence protocol that maintains cache coherence\nwithout imposing any computational burden on the remote memory side. SELCC\nbuilds on a one-sided shared-exclusive latch protocol by introducing lazy latch\nrelease and invalidation messages among the compute nodes so that it can\nguarantee both data access atomicity and cache coherence. SELCC minimizes\ncommunication round-trips by embedding the current cache copy holder IDs into\nRDMA latch words and prioritizes local concurrency control over global\nconcurrency control. We instantiate the SELCC protocol onto compute-sided\ncache, forming an abstraction layer over disaggregated memory. This abstraction\nlayer provides main-memory-like APIs to upper-level applications, and thus\nenabling existing data structures and algorithms to function over disaggregated\nmemory with minimal code change. To demonstrate the usability of SELCC, we\nimplement a B-tree and three transaction concurrency control algorithms over\nSELCC's APIs. Micro-benchmark results show that the SELCC protocol achieves\nbetter performance compared to RPC-based cache-coherence protocols.\nAdditionally, YCSB and TPC-C benchmarks indicate that applications over SELCC\ncan achieve comparable or superior performance against competitors over\ndisaggregated memory."
                },
                "authors": [
                    {
                        "name": "Ruihong Wang"
                    },
                    {
                        "name": "Jianguo Wang"
                    },
                    {
                        "name": "Walid G. Aref"
                    }
                ],
                "author_detail": {
                    "name": "Walid G. Aref"
                },
                "author": "Walid G. Aref",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.02088v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.02088v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.ET",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.10443v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.10443v3",
                "updated": "2024-09-05T01:06:40Z",
                "updated_parsed": [
                    2024,
                    9,
                    5,
                    1,
                    6,
                    40,
                    3,
                    249,
                    0
                ],
                "published": "2024-05-16T21:07:42Z",
                "published_parsed": [
                    2024,
                    5,
                    16,
                    21,
                    7,
                    42,
                    3,
                    137,
                    0
                ],
                "title": "Simultaneous Masking, Not Prompting Optimization: A Paradigm Shift in\n  Fine-tuning LLMs for Simultaneous Translation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Simultaneous Masking, Not Prompting Optimization: A Paradigm Shift in\n  Fine-tuning LLMs for Simultaneous Translation"
                },
                "summary": "Large language models (LLMs) have achieved state-of-the-art performance in\nvarious language processing tasks, motivating their adoption in simultaneous\ntranslation. Current fine-tuning methods to adapt LLMs for simultaneous\ntranslation focus on prompting optimization strategies using either data\naugmentation or prompt structure modifications. However, these methods suffer\nfrom several issues, such as unnecessarily expanded training sets,\ncomputational inefficiency from dumping the key and value cache, increased\nprompt sizes, or restriction to a single decision policy. To eliminate these\nissues, in this work, we propose SimulMask, a new paradigm for fine-tuning LLMs\nfor simultaneous translation. It utilizes a novel attention mask approach that\nmodels simultaneous translation during fine-tuning by masking attention for a\ndesired decision policy. Applying the proposed SimulMask on a Falcon LLM for\nthe IWSLT 2017 dataset, we have observed a significant translation quality\nimprovement compared to state-of-the-art prompting optimization strategies on\nfive language pairs while reducing the computational cost.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have achieved state-of-the-art performance in\nvarious language processing tasks, motivating their adoption in simultaneous\ntranslation. Current fine-tuning methods to adapt LLMs for simultaneous\ntranslation focus on prompting optimization strategies using either data\naugmentation or prompt structure modifications. However, these methods suffer\nfrom several issues, such as unnecessarily expanded training sets,\ncomputational inefficiency from dumping the key and value cache, increased\nprompt sizes, or restriction to a single decision policy. To eliminate these\nissues, in this work, we propose SimulMask, a new paradigm for fine-tuning LLMs\nfor simultaneous translation. It utilizes a novel attention mask approach that\nmodels simultaneous translation during fine-tuning by masking attention for a\ndesired decision policy. Applying the proposed SimulMask on a Falcon LLM for\nthe IWSLT 2017 dataset, we have observed a significant translation quality\nimprovement compared to state-of-the-art prompting optimization strategies on\nfive language pairs while reducing the computational cost."
                },
                "authors": [
                    {
                        "name": "Matthew Raffel"
                    },
                    {
                        "name": "Victor Agostinelli"
                    },
                    {
                        "name": "Lizhong Chen"
                    }
                ],
                "author_detail": {
                    "name": "Lizhong Chen"
                },
                "author": "Lizhong Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.10443v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.10443v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2312.04985v6",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2312.04985v6",
                "updated": "2024-09-04T10:04:52Z",
                "updated_parsed": [
                    2024,
                    9,
                    4,
                    10,
                    4,
                    52,
                    2,
                    248,
                    0
                ],
                "published": "2023-12-08T11:47:35Z",
                "published_parsed": [
                    2023,
                    12,
                    8,
                    11,
                    47,
                    35,
                    4,
                    342,
                    0
                ],
                "title": "SparQ Attention: Bandwidth-Efficient LLM Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SparQ Attention: Bandwidth-Efficient LLM Inference"
                },
                "summary": "The computational difficulties of large language model (LLM) inference remain\na significant obstacle to their widespread deployment. The need for many\napplications to support long input sequences and process them in large batches\ntypically causes token-generation to be bottlenecked by data transfer. For this\nreason, we introduce SparQ Attention, a technique for increasing the inference\nthroughput of LLMs by utilising memory bandwidth more efficiently within the\nattention layers, through selective fetching of the cached history. Our\nproposed technique can be applied directly to off-the-shelf LLMs during\ninference, without requiring any modification to the pre-training setup or\nadditional fine-tuning. We show that SparQ Attention brings up to 8x savings in\nattention data transfers without substantial drops in accuracy, by evaluating\nLlama 2 and 3, Mistral, Gemma and Pythia models on a wide range of downstream\ntasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The computational difficulties of large language model (LLM) inference remain\na significant obstacle to their widespread deployment. The need for many\napplications to support long input sequences and process them in large batches\ntypically causes token-generation to be bottlenecked by data transfer. For this\nreason, we introduce SparQ Attention, a technique for increasing the inference\nthroughput of LLMs by utilising memory bandwidth more efficiently within the\nattention layers, through selective fetching of the cached history. Our\nproposed technique can be applied directly to off-the-shelf LLMs during\ninference, without requiring any modification to the pre-training setup or\nadditional fine-tuning. We show that SparQ Attention brings up to 8x savings in\nattention data transfers without substantial drops in accuracy, by evaluating\nLlama 2 and 3, Mistral, Gemma and Pythia models on a wide range of downstream\ntasks."
                },
                "authors": [
                    {
                        "name": "Luka Ribar"
                    },
                    {
                        "name": "Ivan Chelombiev"
                    },
                    {
                        "name": "Luke Hudlass-Galley"
                    },
                    {
                        "name": "Charlie Blake"
                    },
                    {
                        "name": "Carlo Luschi"
                    },
                    {
                        "name": "Douglas Orr"
                    }
                ],
                "author_detail": {
                    "name": "Douglas Orr"
                },
                "author": "Douglas Orr",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2312.04985v6",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2312.04985v6",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.02480v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.02480v1",
                "updated": "2024-09-04T07:13:01Z",
                "updated_parsed": [
                    2024,
                    9,
                    4,
                    7,
                    13,
                    1,
                    2,
                    248,
                    0
                ],
                "published": "2024-09-04T07:13:01Z",
                "published_parsed": [
                    2024,
                    9,
                    4,
                    7,
                    13,
                    1,
                    2,
                    248,
                    0
                ],
                "title": "A brown dwarf orbiting around the planetary-nebula central binary KV Vel",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A brown dwarf orbiting around the planetary-nebula central binary KV Vel"
                },
                "summary": "KV Vel is a non-eclipsing short-period (P = 0.3571 days) close binary\ncontaining a very hot subdwarf primary (77000 K) and a cool low-mass secondary\nstar (3400 K) that is located at the center of the planetary nebula DS 1. The\nchanges in the orbital period of the close binary were analyzed based on 262\nnew times of light maximum together with those compiled from the literature. It\nis discovered that the O-C curve shows a small-amplitude (0.0034 days) cyclic\nperiod variation with a period of 29.55 years. The explanation by the\nsolar-type magnetic activity cycles of the cool component is ruled out because\nthe required energies are much larger than the total radiant energy of this\ncomponent in a whole cycle. Therefore, the cyclic variation was plausibly\nexplained as the light-travel time effect via the presence of a tertiary\ncomponent, which is supported by the periodic changes of the O-C curve and the\nrather symmetric and stable light curves obtained by TESS. The mass of the\ntertiary companion is determined to be M_3sini' = 0.060(7) M_sun. If the third\nbody is coplanar with the central binary (i.e., i' = 62.5{\\deg}), the mass of\nthe tertiary component is computed as M_3 ~ 0.068 M\\sun, and thus it would be\nbelow the stable hydrogen-burning limit and is a brown dwarf. The orbital\nseparation is shorter than 9.35 astronomical units (AU). KV Vel together with\nits surrounding planetary nebula and the brown-dwarf companion may be formed\nthrough the common-envelope evolution after the primary filled its Roche lobe\nduring the early asymptotic giant branch stage.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "KV Vel is a non-eclipsing short-period (P = 0.3571 days) close binary\ncontaining a very hot subdwarf primary (77000 K) and a cool low-mass secondary\nstar (3400 K) that is located at the center of the planetary nebula DS 1. The\nchanges in the orbital period of the close binary were analyzed based on 262\nnew times of light maximum together with those compiled from the literature. It\nis discovered that the O-C curve shows a small-amplitude (0.0034 days) cyclic\nperiod variation with a period of 29.55 years. The explanation by the\nsolar-type magnetic activity cycles of the cool component is ruled out because\nthe required energies are much larger than the total radiant energy of this\ncomponent in a whole cycle. Therefore, the cyclic variation was plausibly\nexplained as the light-travel time effect via the presence of a tertiary\ncomponent, which is supported by the periodic changes of the O-C curve and the\nrather symmetric and stable light curves obtained by TESS. The mass of the\ntertiary companion is determined to be M_3sini' = 0.060(7) M_sun. If the third\nbody is coplanar with the central binary (i.e., i' = 62.5{\\deg}), the mass of\nthe tertiary component is computed as M_3 ~ 0.068 M\\sun, and thus it would be\nbelow the stable hydrogen-burning limit and is a brown dwarf. The orbital\nseparation is shorter than 9.35 astronomical units (AU). KV Vel together with\nits surrounding planetary nebula and the brown-dwarf companion may be formed\nthrough the common-envelope evolution after the primary filled its Roche lobe\nduring the early asymptotic giant branch stage."
                },
                "authors": [
                    {
                        "name": "S. -B. Qian"
                    },
                    {
                        "name": "L. -Y. Zhu"
                    },
                    {
                        "name": "F. -X. Li"
                    },
                    {
                        "name": "L. -J. Li"
                    },
                    {
                        "name": "Z. -T. Han"
                    },
                    {
                        "name": "J. -J. He"
                    },
                    {
                        "name": "L. Zang"
                    },
                    {
                        "name": "L. -F. Chang"
                    },
                    {
                        "name": "Q. -B. Sun"
                    },
                    {
                        "name": "M. -Y. Li"
                    },
                    {
                        "name": "H. -T. Zhang"
                    },
                    {
                        "name": "F. -Z. Yan"
                    }
                ],
                "author_detail": {
                    "name": "F. -Z. Yan"
                },
                "author": "F. -Z. Yan",
                "arxiv_doi": "10.3847/1538-4357/ad631a",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.3847/1538-4357/ad631a",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2409.02480v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.02480v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.SR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.SR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.01990v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.01990v1",
                "updated": "2024-09-03T15:35:01Z",
                "updated_parsed": [
                    2024,
                    9,
                    3,
                    15,
                    35,
                    1,
                    1,
                    247,
                    0
                ],
                "published": "2024-09-03T15:35:01Z",
                "published_parsed": [
                    2024,
                    9,
                    3,
                    15,
                    35,
                    1,
                    1,
                    247,
                    0
                ],
                "title": "Contemporary Model Compression on Large Language Models Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Contemporary Model Compression on Large Language Models Inference"
                },
                "summary": "Large Language Models (LLMs) have revolutionized natural language processing\nby achieving state-of-the-art results across a variety of tasks. However, the\ncomputational demands of LLM inference, including high memory consumption and\nslow processing speeds, pose significant challenges for real-world\napplications, particularly on resource-constrained devices. Efficient inference\nis crucial for scaling the deployment of LLMs to a broader range of platforms,\nincluding mobile and edge devices.\n  This survey explores contemporary techniques in model compression that\naddress these challenges by reducing the size and computational requirements of\nLLMs while maintaining their performance. We focus on model-level compression\nmethods, including quantization, knowledge distillation, and pruning, as well\nas system-level optimizations like KV cache efficient design. Each of these\nmethodologies offers a unique approach to optimizing LLMs, from reducing\nnumerical precision to transferring knowledge between models and structurally\nsimplifying neural networks. Additionally, we discuss emerging trends in\nsystem-level design that further enhance the efficiency of LLM inference. This\nsurvey aims to provide a comprehensive overview of current advancements in\nmodel compression and their potential to make LLMs more accessible and\npractical for diverse applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have revolutionized natural language processing\nby achieving state-of-the-art results across a variety of tasks. However, the\ncomputational demands of LLM inference, including high memory consumption and\nslow processing speeds, pose significant challenges for real-world\napplications, particularly on resource-constrained devices. Efficient inference\nis crucial for scaling the deployment of LLMs to a broader range of platforms,\nincluding mobile and edge devices.\n  This survey explores contemporary techniques in model compression that\naddress these challenges by reducing the size and computational requirements of\nLLMs while maintaining their performance. We focus on model-level compression\nmethods, including quantization, knowledge distillation, and pruning, as well\nas system-level optimizations like KV cache efficient design. Each of these\nmethodologies offers a unique approach to optimizing LLMs, from reducing\nnumerical precision to transferring knowledge between models and structurally\nsimplifying neural networks. Additionally, we discuss emerging trends in\nsystem-level design that further enhance the efficiency of LLM inference. This\nsurvey aims to provide a comprehensive overview of current advancements in\nmodel compression and their potential to make LLMs more accessible and\npractical for diverse applications."
                },
                "authors": [
                    {
                        "name": "Dong Liu"
                    }
                ],
                "author_detail": {
                    "name": "Dong Liu"
                },
                "author": "Dong Liu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.01990v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.01990v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.01890v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.01890v1",
                "updated": "2024-09-03T13:29:13Z",
                "updated_parsed": [
                    2024,
                    9,
                    3,
                    13,
                    29,
                    13,
                    1,
                    247,
                    0
                ],
                "published": "2024-09-03T13:29:13Z",
                "published_parsed": [
                    2024,
                    9,
                    3,
                    13,
                    29,
                    13,
                    1,
                    247,
                    0
                ],
                "title": "A Fresh Take on Stale Embeddings: Improving Dense Retriever Training\n  with Corrector Networks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Fresh Take on Stale Embeddings: Improving Dense Retriever Training\n  with Corrector Networks"
                },
                "summary": "In dense retrieval, deep encoders provide embeddings for both inputs and\ntargets, and the softmax function is used to parameterize a distribution over a\nlarge number of candidate targets (e.g., textual passages for information\nretrieval). Significant challenges arise in training such encoders in the\nincreasingly prevalent scenario of (1) a large number of targets, (2) a\ncomputationally expensive target encoder model, (3) cached target embeddings\nthat are out-of-date due to ongoing training of target encoder parameters. This\npaper presents a simple and highly scalable response to these challenges by\ntraining a small parametric corrector network that adjusts stale cached target\nembeddings, enabling an accurate softmax approximation and thereby sampling of\nup-to-date high scoring \"hard negatives.\" We theoretically investigate the\ngeneralization properties of our proposed target corrector, relating the\ncomplexity of the network, staleness of cached representations, and the amount\nof training data. We present experimental results on large benchmark dense\nretrieval datasets as well as on QA with retrieval augmented language models.\nOur approach matches state-of-the-art results even when no target embedding\nupdates are made during training beyond an initial cache from the unsupervised\npre-trained model, providing a 4-80x reduction in re-embedding computational\ncost.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In dense retrieval, deep encoders provide embeddings for both inputs and\ntargets, and the softmax function is used to parameterize a distribution over a\nlarge number of candidate targets (e.g., textual passages for information\nretrieval). Significant challenges arise in training such encoders in the\nincreasingly prevalent scenario of (1) a large number of targets, (2) a\ncomputationally expensive target encoder model, (3) cached target embeddings\nthat are out-of-date due to ongoing training of target encoder parameters. This\npaper presents a simple and highly scalable response to these challenges by\ntraining a small parametric corrector network that adjusts stale cached target\nembeddings, enabling an accurate softmax approximation and thereby sampling of\nup-to-date high scoring \"hard negatives.\" We theoretically investigate the\ngeneralization properties of our proposed target corrector, relating the\ncomplexity of the network, staleness of cached representations, and the amount\nof training data. We present experimental results on large benchmark dense\nretrieval datasets as well as on QA with retrieval augmented language models.\nOur approach matches state-of-the-art results even when no target embedding\nupdates are made during training beyond an initial cache from the unsupervised\npre-trained model, providing a 4-80x reduction in re-embedding computational\ncost."
                },
                "authors": [
                    {
                        "name": "Nicholas Monath"
                    },
                    {
                        "name": "Will Grathwohl"
                    },
                    {
                        "name": "Michael Boratko"
                    },
                    {
                        "name": "Rob Fergus"
                    },
                    {
                        "name": "Andrew McCallum"
                    },
                    {
                        "name": "Manzil Zaheer"
                    }
                ],
                "author_detail": {
                    "name": "Manzil Zaheer"
                },
                "author": "Manzil Zaheer",
                "arxiv_comment": "ICML 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.01890v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.01890v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.02137v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.02137v1",
                "updated": "2024-09-02T15:07:05Z",
                "updated_parsed": [
                    2024,
                    9,
                    2,
                    15,
                    7,
                    5,
                    0,
                    246,
                    0
                ],
                "published": "2024-09-02T15:07:05Z",
                "published_parsed": [
                    2024,
                    9,
                    2,
                    15,
                    7,
                    5,
                    0,
                    246,
                    0
                ],
                "title": "Reward Augmentation in Reinforcement Learning for Testing Distributed\n  Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reward Augmentation in Reinforcement Learning for Testing Distributed\n  Systems"
                },
                "summary": "Bugs in popular distributed protocol implementations have been the source of\nmany downtimes in popular internet services. We describe a randomized testing\napproach for distributed protocol implementations based on reinforcement\nlearning. Since the natural reward structure is very sparse, the key to\nsuccessful exploration in reinforcement learning is reward augmentation. We\nshow two different techniques that build on one another. First, we provide a\ndecaying exploration bonus based on the discovery of new states -- the reward\ndecays as the same state is visited multiple times. The exploration bonus\ncaptures the intuition from coverage-guided fuzzing of prioritizing new\ncoverage points; in contrast to other schemes, we show that taking the maximum\nof the bonus and the Q-value leads to more effective exploration. Second, we\nprovide waypoints to the algorithm as a sequence of predicates that capture\ninteresting semantic scenarios. Waypoints exploit designer insight about the\nprotocol and guide the exploration to ``interesting'' parts of the state space.\nOur reward structure ensures that new episodes can reliably get to deep\ninteresting states even without execution caching. We have implemented our\nalgorithm in Go. Our evaluation on three large benchmarks (RedisRaft, Etcd, and\nRSL) shows that our algorithm can significantly outperform baseline approaches\nin terms of coverage and bug finding.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Bugs in popular distributed protocol implementations have been the source of\nmany downtimes in popular internet services. We describe a randomized testing\napproach for distributed protocol implementations based on reinforcement\nlearning. Since the natural reward structure is very sparse, the key to\nsuccessful exploration in reinforcement learning is reward augmentation. We\nshow two different techniques that build on one another. First, we provide a\ndecaying exploration bonus based on the discovery of new states -- the reward\ndecays as the same state is visited multiple times. The exploration bonus\ncaptures the intuition from coverage-guided fuzzing of prioritizing new\ncoverage points; in contrast to other schemes, we show that taking the maximum\nof the bonus and the Q-value leads to more effective exploration. Second, we\nprovide waypoints to the algorithm as a sequence of predicates that capture\ninteresting semantic scenarios. Waypoints exploit designer insight about the\nprotocol and guide the exploration to ``interesting'' parts of the state space.\nOur reward structure ensures that new episodes can reliably get to deep\ninteresting states even without execution caching. We have implemented our\nalgorithm in Go. Our evaluation on three large benchmarks (RedisRaft, Etcd, and\nRSL) shows that our algorithm can significantly outperform baseline approaches\nin terms of coverage and bug finding."
                },
                "authors": [
                    {
                        "name": "Andrea Borgarelli"
                    },
                    {
                        "name": "Constantin Enea"
                    },
                    {
                        "name": "Rupak Majumdar"
                    },
                    {
                        "name": "Srinidhi Nagendra"
                    }
                ],
                "author_detail": {
                    "name": "Srinidhi Nagendra"
                },
                "author": "Srinidhi Nagendra",
                "arxiv_doi": "10.1145/3689779",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3689779",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2409.02137v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.02137v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.01066v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.01066v1",
                "updated": "2024-09-02T08:41:45Z",
                "updated_parsed": [
                    2024,
                    9,
                    2,
                    8,
                    41,
                    45,
                    0,
                    246,
                    0
                ],
                "published": "2024-09-02T08:41:45Z",
                "published_parsed": [
                    2024,
                    9,
                    2,
                    8,
                    41,
                    45,
                    0,
                    246,
                    0
                ],
                "title": "Learning in Hybrid Active Inference Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Learning in Hybrid Active Inference Models"
                },
                "summary": "An open problem in artificial intelligence is how systems can flexibly learn\ndiscrete abstractions that are useful for solving inherently continuous\nproblems. Previous work in computational neuroscience has considered this\nfunctional integration of discrete and continuous variables during\ndecision-making under the formalism of active inference (Parr, Friston & de\nVries, 2017; Parr & Friston, 2018). However, their focus is on the expressive\nphysical implementation of categorical decisions and the hierarchical mixed\ngenerative model is assumed to be known. As a consequence, it is unclear how\nthis framework might be extended to learning. We therefore present a novel\nhierarchical hybrid active inference agent in which a high-level discrete\nactive inference planner sits above a low-level continuous active inference\ncontroller. We make use of recent work in recurrent switching linear dynamical\nsystems (rSLDS) which implement end-to-end learning of meaningful discrete\nrepresentations via the piecewise linear decomposition of complex continuous\ndynamics (Linderman et al., 2016). The representations learned by the rSLDS\ninform the structure of the hybrid decision-making agent and allow us to (1)\nspecify temporally-abstracted sub-goals in a method reminiscent of the options\nframework, (2) lift the exploration into discrete space allowing us to exploit\ninformation-theoretic exploration bonuses and (3) `cache' the approximate\nsolutions to low-level problems in the discrete planner. We apply our model to\nthe sparse Continuous Mountain Car task, demonstrating fast system\nidentification via enhanced exploration and successful planning through the\ndelineation of abstract sub-goals.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "An open problem in artificial intelligence is how systems can flexibly learn\ndiscrete abstractions that are useful for solving inherently continuous\nproblems. Previous work in computational neuroscience has considered this\nfunctional integration of discrete and continuous variables during\ndecision-making under the formalism of active inference (Parr, Friston & de\nVries, 2017; Parr & Friston, 2018). However, their focus is on the expressive\nphysical implementation of categorical decisions and the hierarchical mixed\ngenerative model is assumed to be known. As a consequence, it is unclear how\nthis framework might be extended to learning. We therefore present a novel\nhierarchical hybrid active inference agent in which a high-level discrete\nactive inference planner sits above a low-level continuous active inference\ncontroller. We make use of recent work in recurrent switching linear dynamical\nsystems (rSLDS) which implement end-to-end learning of meaningful discrete\nrepresentations via the piecewise linear decomposition of complex continuous\ndynamics (Linderman et al., 2016). The representations learned by the rSLDS\ninform the structure of the hybrid decision-making agent and allow us to (1)\nspecify temporally-abstracted sub-goals in a method reminiscent of the options\nframework, (2) lift the exploration into discrete space allowing us to exploit\ninformation-theoretic exploration bonuses and (3) `cache' the approximate\nsolutions to low-level problems in the discrete planner. We apply our model to\nthe sparse Continuous Mountain Car task, demonstrating fast system\nidentification via enhanced exploration and successful planning through the\ndelineation of abstract sub-goals."
                },
                "authors": [
                    {
                        "name": "Poppy Collis"
                    },
                    {
                        "name": "Ryan Singh"
                    },
                    {
                        "name": "Paul F Kinghorn"
                    },
                    {
                        "name": "Christopher L Buckley"
                    }
                ],
                "author_detail": {
                    "name": "Christopher L Buckley"
                },
                "author": "Christopher L Buckley",
                "arxiv_comment": "11 pages (+ appendix). Accepted to the International Workshop on\n  Active Inference 2024. arXiv admin note: substantial text overlap with\n  arXiv:2408.10970",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.01066v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.01066v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.00905v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.00905v1",
                "updated": "2024-09-02T02:36:22Z",
                "updated_parsed": [
                    2024,
                    9,
                    2,
                    2,
                    36,
                    22,
                    0,
                    246,
                    0
                ],
                "published": "2024-09-02T02:36:22Z",
                "published_parsed": [
                    2024,
                    9,
                    2,
                    2,
                    36,
                    22,
                    0,
                    246,
                    0
                ],
                "title": "Throughput Optimization in Cache-aided Networks: An Opportunistic\n  Probing and Scheduling Approach",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Throughput Optimization in Cache-aided Networks: An Opportunistic\n  Probing and Scheduling Approach"
                },
                "summary": "This paper addresses the challenges of throughput optimization in wireless\ncache-aided cooperative networks. We propose an opportunistic cooperative\nprobing and scheduling strategy for efficient content delivery. The strategy\ninvolves the base station probing the relaying channels and cache states of\nmultiple cooperative nodes, thereby enabling opportunistic user scheduling for\ncontent delivery. Leveraging the theory of Sequentially Planned Decision (SPD)\noptimization, we dynamically formulate decisions on cooperative probing and\nstopping time. Our proposed Reward Expected Thresholds (RET)-based strategy\noptimizes opportunistic probing and scheduling. This approach significantly\nenhances system throughput by exploiting gains from local caching, cooperative\ntransmission and time diversity. Simulations confirm the effectiveness and\npracticality of the proposed Media Access Control (MAC) strategy.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper addresses the challenges of throughput optimization in wireless\ncache-aided cooperative networks. We propose an opportunistic cooperative\nprobing and scheduling strategy for efficient content delivery. The strategy\ninvolves the base station probing the relaying channels and cache states of\nmultiple cooperative nodes, thereby enabling opportunistic user scheduling for\ncontent delivery. Leveraging the theory of Sequentially Planned Decision (SPD)\noptimization, we dynamically formulate decisions on cooperative probing and\nstopping time. Our proposed Reward Expected Thresholds (RET)-based strategy\noptimizes opportunistic probing and scheduling. This approach significantly\nenhances system throughput by exploiting gains from local caching, cooperative\ntransmission and time diversity. Simulations confirm the effectiveness and\npracticality of the proposed Media Access Control (MAC) strategy."
                },
                "authors": [
                    {
                        "name": "Zhou Zhang"
                    },
                    {
                        "name": "Saman Atapattu"
                    },
                    {
                        "name": "Yizhu Wang"
                    },
                    {
                        "name": "Marco Di Renzo"
                    }
                ],
                "author_detail": {
                    "name": "Marco Di Renzo"
                },
                "author": "Marco Di Renzo",
                "arxiv_comment": "2024 IEEE GLOBECOM, Cape Town, South Africa",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.00905v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.00905v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SP",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.00876v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.00876v1",
                "updated": "2024-09-02T00:05:20Z",
                "updated_parsed": [
                    2024,
                    9,
                    2,
                    0,
                    5,
                    20,
                    0,
                    246,
                    0
                ],
                "published": "2024-09-02T00:05:20Z",
                "published_parsed": [
                    2024,
                    9,
                    2,
                    0,
                    5,
                    20,
                    0,
                    246,
                    0
                ],
                "title": "Rapid GPU-Based Pangenome Graph Layout",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Rapid GPU-Based Pangenome Graph Layout"
                },
                "summary": "Computational Pangenomics is an emerging field that studies genetic variation\nusing a graph structure encompassing multiple genomes. Visualizing pangenome\ngraphs is vital for understanding genome diversity. Yet, handling large graphs\ncan be challenging due to the high computational demands of the graph layout\nprocess.\n  In this work, we conduct a thorough performance characterization of a\nstate-of-the-art pangenome graph layout algorithm, revealing significant\ndata-level parallelism, which makes GPUs a promising option for compute\nacceleration. However, irregular data access and the algorithm's memory-bound\nnature present significant hurdles. To overcome these challenges, we develop a\nsolution implementing three key optimizations: a cache-friendly data layout,\ncoalesced random states, and warp merging. Additionally, we propose a\nquantitative metric for scalable evaluation of pangenome layout quality.\n  Evaluated on 24 human whole-chromosome pangenomes, our GPU-based solution\nachieves a 57.3x speedup over the state-of-the-art multithreaded CPU baseline\nwithout layout quality loss, reducing execution time from hours to minutes.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Computational Pangenomics is an emerging field that studies genetic variation\nusing a graph structure encompassing multiple genomes. Visualizing pangenome\ngraphs is vital for understanding genome diversity. Yet, handling large graphs\ncan be challenging due to the high computational demands of the graph layout\nprocess.\n  In this work, we conduct a thorough performance characterization of a\nstate-of-the-art pangenome graph layout algorithm, revealing significant\ndata-level parallelism, which makes GPUs a promising option for compute\nacceleration. However, irregular data access and the algorithm's memory-bound\nnature present significant hurdles. To overcome these challenges, we develop a\nsolution implementing three key optimizations: a cache-friendly data layout,\ncoalesced random states, and warp merging. Additionally, we propose a\nquantitative metric for scalable evaluation of pangenome layout quality.\n  Evaluated on 24 human whole-chromosome pangenomes, our GPU-based solution\nachieves a 57.3x speedup over the state-of-the-art multithreaded CPU baseline\nwithout layout quality loss, reducing execution time from hours to minutes."
                },
                "authors": [
                    {
                        "name": "Jiajie Li"
                    },
                    {
                        "name": "Jan-Niklas Schmelzle"
                    },
                    {
                        "name": "Yixiao Du"
                    },
                    {
                        "name": "Simon Heumos"
                    },
                    {
                        "name": "Andrea Guarracino"
                    },
                    {
                        "name": "Giulia Guidi"
                    },
                    {
                        "name": "Pjotr Prins"
                    },
                    {
                        "name": "Erik Garrison"
                    },
                    {
                        "name": "Zhiru Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Zhiru Zhang"
                },
                "author": "Zhiru Zhang",
                "arxiv_comment": "SC 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.00876v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.00876v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.10539v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.10539v1",
                "updated": "2024-08-31T15:45:44Z",
                "updated_parsed": [
                    2024,
                    8,
                    31,
                    15,
                    45,
                    44,
                    5,
                    244,
                    0
                ],
                "published": "2024-08-31T15:45:44Z",
                "published_parsed": [
                    2024,
                    8,
                    31,
                    15,
                    45,
                    44,
                    5,
                    244,
                    0
                ],
                "title": "Towards 3D AI Hardware: Fine-Grain Hardware Characterization of 3D\n  Stacks for Heterogeneous System Integration & AI Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Towards 3D AI Hardware: Fine-Grain Hardware Characterization of 3D\n  Stacks for Heterogeneous System Integration & AI Systems"
                },
                "summary": "3D integration offers key advantages in improving system performance and\nefficiency for the End-of-Scaling era. It enables the incorporation of\nheterogeneous system components and disparate technologies, eliminates off-chip\ncommunication constraints, reduces on-chip latency and total power dissipation.\nMoreover, AIs demand for increased computational power, larger GPU cache\ncapacity, energy efficiency and low power custom AI hardware integration all\nserve as drivers for 3D integration. Although 3D advantages such as enhanced\ninterconnectivity and increased performance have been demonstrated through\nnumerous technology sites, heterogeneous 3D system design raises numerous\nunanswered questions. Among the primary challenges are the temperature and\nlifetime reliability issues caused by the complex interaction patterns among\nsystem components. Such interactions are harder to model with current modeling\ntools and require detailed hardware characterization. This study presents the\nlatest drivers for 3D integration and the resulting need for hardware emulation\nframeworks. It then presents a design to profile power, temperature, noise,\ninter-layer bandwidth and lifetime reliability characterization that can\nemulate a wide range of stacking alternatives. This framework allows for\ncontrolling activity levels at the macro-level, along with customized sensor\ninfrastructure to characterize heat propagation, inter-layer noise, power\ndelivery, reliability and inter-connectivity as well as the interactions among\ncritical design objectives.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "3D integration offers key advantages in improving system performance and\nefficiency for the End-of-Scaling era. It enables the incorporation of\nheterogeneous system components and disparate technologies, eliminates off-chip\ncommunication constraints, reduces on-chip latency and total power dissipation.\nMoreover, AIs demand for increased computational power, larger GPU cache\ncapacity, energy efficiency and low power custom AI hardware integration all\nserve as drivers for 3D integration. Although 3D advantages such as enhanced\ninterconnectivity and increased performance have been demonstrated through\nnumerous technology sites, heterogeneous 3D system design raises numerous\nunanswered questions. Among the primary challenges are the temperature and\nlifetime reliability issues caused by the complex interaction patterns among\nsystem components. Such interactions are harder to model with current modeling\ntools and require detailed hardware characterization. This study presents the\nlatest drivers for 3D integration and the resulting need for hardware emulation\nframeworks. It then presents a design to profile power, temperature, noise,\ninter-layer bandwidth and lifetime reliability characterization that can\nemulate a wide range of stacking alternatives. This framework allows for\ncontrolling activity levels at the macro-level, along with customized sensor\ninfrastructure to characterize heat propagation, inter-layer noise, power\ndelivery, reliability and inter-connectivity as well as the interactions among\ncritical design objectives."
                },
                "authors": [
                    {
                        "name": "Eren Kurshan"
                    },
                    {
                        "name": "Paul Franzon"
                    }
                ],
                "author_detail": {
                    "name": "Paul Franzon"
                },
                "author": "Paul Franzon",
                "arxiv_journal_ref": "IEEE 3D IC Conference 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.10539v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.10539v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.ET",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.ET",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.00364v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.00364v1",
                "updated": "2024-08-31T06:33:50Z",
                "updated_parsed": [
                    2024,
                    8,
                    31,
                    6,
                    33,
                    50,
                    5,
                    244,
                    0
                ],
                "published": "2024-08-31T06:33:50Z",
                "published_parsed": [
                    2024,
                    8,
                    31,
                    6,
                    33,
                    50,
                    5,
                    244,
                    0
                ],
                "title": "Resource Management for IRS-Assisted Full-Duplex Integrated Sensing,\n  Communication and Computing Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Resource Management for IRS-Assisted Full-Duplex Integrated Sensing,\n  Communication and Computing Systems"
                },
                "summary": "In this paper, we investigate an intelligent reflecting surface (IRS)\nassisted full-duplex (FD) integrated sensing, communication and computing\nsystem. Specifically, an FD base station (BS) provides service for uplink and\ndownlink transmission, and a local cache is connected to the BS through a\nbackhaul link to store data. Meanwhile, active sensing elements are deployed on\nthe IRS to receive target echo signals. On this basis, in order to evaluate the\noverall performance of the system under consideration, we propose a system\nutility maximization problem while ensuring the sensing quality, expressed as\nthe difference between the sum of communication throughput, total computation\nbits (offloading bits and local computation bits) and the total backhaul cost\nfor content delivery. This makes the problem difficult to solve due to the\nhighly non-convex coupling of the optimization variables. To effectively solve\nthis problem, we first design the most effective caching strategy. Then, we\ndevelop an algorithm based on weighted minimum mean square error, alternative\ndirection method of multipliers, majorization-minimization framework,\nsemi-definite relaxation techniques, and several complex transformations to\njointly solve the optimization variables. Finally, simulation results are\nprovided to verify the utility performance of the proposed algorithm and\ndemonstrate the advantages of the proposed scheme compared with the baseline\nscheme.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this paper, we investigate an intelligent reflecting surface (IRS)\nassisted full-duplex (FD) integrated sensing, communication and computing\nsystem. Specifically, an FD base station (BS) provides service for uplink and\ndownlink transmission, and a local cache is connected to the BS through a\nbackhaul link to store data. Meanwhile, active sensing elements are deployed on\nthe IRS to receive target echo signals. On this basis, in order to evaluate the\noverall performance of the system under consideration, we propose a system\nutility maximization problem while ensuring the sensing quality, expressed as\nthe difference between the sum of communication throughput, total computation\nbits (offloading bits and local computation bits) and the total backhaul cost\nfor content delivery. This makes the problem difficult to solve due to the\nhighly non-convex coupling of the optimization variables. To effectively solve\nthis problem, we first design the most effective caching strategy. Then, we\ndevelop an algorithm based on weighted minimum mean square error, alternative\ndirection method of multipliers, majorization-minimization framework,\nsemi-definite relaxation techniques, and several complex transformations to\njointly solve the optimization variables. Finally, simulation results are\nprovided to verify the utility performance of the proposed algorithm and\ndemonstrate the advantages of the proposed scheme compared with the baseline\nscheme."
                },
                "authors": [
                    {
                        "name": "Wanming Hao"
                    },
                    {
                        "name": "Xue Wu"
                    },
                    {
                        "name": "Xingwang Li"
                    },
                    {
                        "name": "Gangcan Sun"
                    },
                    {
                        "name": "Qingqing Wu"
                    },
                    {
                        "name": "Liang Yang"
                    }
                ],
                "author_detail": {
                    "name": "Liang Yang"
                },
                "author": "Liang Yang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.00364v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.00364v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IT",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.00344v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.00344v1",
                "updated": "2024-08-31T04:20:58Z",
                "updated_parsed": [
                    2024,
                    8,
                    31,
                    4,
                    20,
                    58,
                    5,
                    244,
                    0
                ],
                "published": "2024-08-31T04:20:58Z",
                "published_parsed": [
                    2024,
                    8,
                    31,
                    4,
                    20,
                    58,
                    5,
                    244,
                    0
                ],
                "title": ">3kV NiO/Ga2O3 Heterojunction Diodes with Space-Modulated Junction\n  Termination Extension and Sub-1V Turn-on",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": ">3kV NiO/Ga2O3 Heterojunction Diodes with Space-Modulated Junction\n  Termination Extension and Sub-1V Turn-on"
                },
                "summary": "This work demonstrates high-performance vertical NiO/Ga2O3 heterojunction\ndiodes (HJDs) with a 2-step space-modulated junction termination extension.\nDistinct from the current state-of-the-art Ga2O3 HJDs, we achieve breakdown\nvoltage exceeding 3 kV with a low turn on voltage (VON) of 0.8V, estimated at a\nforward current density (IF) of 1 A-cm-2. The measured devices exhibit\nexcellent turn-on characteristics achieving 100 A-cm-2 current density at a\nforward bias of 1.5V along with a low differential specific on-resistance\n(Ron,sp) of 4.4 m{\\Omega}-cm2. The SM-JTE was realized using concentric NiO\nrings with varying widths and spacing that approximates a gradual reduction in\nJTE charge. The unipolar figure of merit (FOM) calculated exceeds 2 GW-cm2 and\nis among the best reported for devices with a sub-1V turn-on. The fabricated\ndevices also displayed minimal change in forward I-V characteristics post\nreverse bias stress of 3 kV applied during breakdown voltage testing.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This work demonstrates high-performance vertical NiO/Ga2O3 heterojunction\ndiodes (HJDs) with a 2-step space-modulated junction termination extension.\nDistinct from the current state-of-the-art Ga2O3 HJDs, we achieve breakdown\nvoltage exceeding 3 kV with a low turn on voltage (VON) of 0.8V, estimated at a\nforward current density (IF) of 1 A-cm-2. The measured devices exhibit\nexcellent turn-on characteristics achieving 100 A-cm-2 current density at a\nforward bias of 1.5V along with a low differential specific on-resistance\n(Ron,sp) of 4.4 m{\\Omega}-cm2. The SM-JTE was realized using concentric NiO\nrings with varying widths and spacing that approximates a gradual reduction in\nJTE charge. The unipolar figure of merit (FOM) calculated exceeds 2 GW-cm2 and\nis among the best reported for devices with a sub-1V turn-on. The fabricated\ndevices also displayed minimal change in forward I-V characteristics post\nreverse bias stress of 3 kV applied during breakdown voltage testing."
                },
                "authors": [
                    {
                        "name": "Advait Gilankar"
                    },
                    {
                        "name": "Abishek Katta"
                    },
                    {
                        "name": "Nabasindhu Das"
                    },
                    {
                        "name": "Nidhin Kurian Kalarickal"
                    }
                ],
                "author_detail": {
                    "name": "Nidhin Kurian Kalarickal"
                },
                "author": "Nidhin Kurian Kalarickal",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.00344v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.00344v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.app-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.app-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.00184v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.00184v1",
                "updated": "2024-08-30T18:04:53Z",
                "updated_parsed": [
                    2024,
                    8,
                    30,
                    18,
                    4,
                    53,
                    4,
                    243,
                    0
                ],
                "published": "2024-08-30T18:04:53Z",
                "published_parsed": [
                    2024,
                    8,
                    30,
                    18,
                    4,
                    53,
                    4,
                    243,
                    0
                ],
                "title": "Adaptive Multi-Resolution Encoding for Interactive Large-Scale Volume\n  Visualization through Functional Approximation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Adaptive Multi-Resolution Encoding for Interactive Large-Scale Volume\n  Visualization through Functional Approximation"
                },
                "summary": "Functional approximation as a high-order continuous representation provides a\nmore accurate value and gradient query compared to the traditional discrete\nvolume representation. Volume visualization directly rendered from functional\napproximation generates high-quality rendering results without high-order\nartifacts caused by trilinear interpolations. However, querying an encoded\nfunctional approximation is computationally expensive, especially when the\ninput dataset is large, making functional approximation impractical for\ninteractive visualization. In this paper, we proposed a novel functional\napproximation multi-resolution representation, Adaptive-FAM, which is\nlightweight and fast to query. We also design a GPU-accelerated out-of-core\nmulti-resolution volume visualization framework that directly utilizes the\nAdaptive-FAM representation to generate high-quality rendering with interactive\nresponsiveness. Our method can not only dramatically decrease the caching time,\none of the main contributors to input latency, but also effectively improve the\ncache hit rate through prefetching. Our approach significantly outperforms the\ntraditional function approximation method in terms of input latency while\nmaintaining comparable rendering quality.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Functional approximation as a high-order continuous representation provides a\nmore accurate value and gradient query compared to the traditional discrete\nvolume representation. Volume visualization directly rendered from functional\napproximation generates high-quality rendering results without high-order\nartifacts caused by trilinear interpolations. However, querying an encoded\nfunctional approximation is computationally expensive, especially when the\ninput dataset is large, making functional approximation impractical for\ninteractive visualization. In this paper, we proposed a novel functional\napproximation multi-resolution representation, Adaptive-FAM, which is\nlightweight and fast to query. We also design a GPU-accelerated out-of-core\nmulti-resolution volume visualization framework that directly utilizes the\nAdaptive-FAM representation to generate high-quality rendering with interactive\nresponsiveness. Our method can not only dramatically decrease the caching time,\none of the main contributors to input latency, but also effectively improve the\ncache hit rate through prefetching. Our approach significantly outperforms the\ntraditional function approximation method in terms of input latency while\nmaintaining comparable rendering quality."
                },
                "authors": [
                    {
                        "name": "Jianxin Sun"
                    },
                    {
                        "name": "David Lenz"
                    },
                    {
                        "name": "Hongfeng Yu"
                    },
                    {
                        "name": "Tom Peterka"
                    }
                ],
                "author_detail": {
                    "name": "Tom Peterka"
                },
                "author": "Tom Peterka",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.00184v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.00184v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.GR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.GR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.17178v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.17178v1",
                "updated": "2024-08-30T10:26:50Z",
                "updated_parsed": [
                    2024,
                    8,
                    30,
                    10,
                    26,
                    50,
                    4,
                    243,
                    0
                ],
                "published": "2024-08-30T10:26:50Z",
                "published_parsed": [
                    2024,
                    8,
                    30,
                    10,
                    26,
                    50,
                    4,
                    243,
                    0
                ],
                "title": "Modelling the High-Voltage Grid Using Open Data for Europe and Beyond",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Modelling the High-Voltage Grid Using Open Data for Europe and Beyond"
                },
                "summary": "This paper provides the background, methodology and validation for\nconstructing a representation of the European high-voltage grid, including and\nabove 200 kV, based on public data provided by OpenStreetMap. The\nmodel-independent grid dataset is published under the Open Data Commons Open\nDatabase (ODbL 1.0) licence and can be used for large-scale electricity as well\nas energy system modelling. The dataset and workflow are provided as part of\nPyPSA-Eur -- an open-source, sector-coupled optimisation model of the European\nenergy system. By integrating with the codebase for initiatives such as\nPyPSA-Earth, the value of open and maintainable high-voltage grid data extends\nto the global context. By accessing the latest data through the the Overpass\nturbo API, the dataset can be easily reconstructed and updated within minutes.\nTo assess the data quality, this paper further compares the dataset with\nofficial statistics and representative model runs using PyPSA-Eur based on\ndifferent electricity grid representations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper provides the background, methodology and validation for\nconstructing a representation of the European high-voltage grid, including and\nabove 200 kV, based on public data provided by OpenStreetMap. The\nmodel-independent grid dataset is published under the Open Data Commons Open\nDatabase (ODbL 1.0) licence and can be used for large-scale electricity as well\nas energy system modelling. The dataset and workflow are provided as part of\nPyPSA-Eur -- an open-source, sector-coupled optimisation model of the European\nenergy system. By integrating with the codebase for initiatives such as\nPyPSA-Earth, the value of open and maintainable high-voltage grid data extends\nto the global context. By accessing the latest data through the the Overpass\nturbo API, the dataset can be easily reconstructed and updated within minutes.\nTo assess the data quality, this paper further compares the dataset with\nofficial statistics and representative model runs using PyPSA-Eur based on\ndifferent electricity grid representations."
                },
                "authors": [
                    {
                        "name": "Bobby Xiong"
                    },
                    {
                        "name": "Davide Fioriti"
                    },
                    {
                        "name": "Fabian Neumann"
                    },
                    {
                        "name": "Iegor Riepin"
                    },
                    {
                        "name": "Tom Brown"
                    }
                ],
                "author_detail": {
                    "name": "Tom Brown"
                },
                "author": "Tom Brown",
                "arxiv_comment": "20 pages, 15 figures, 8 tables. For associated prebuilt electricity\n  network, see https://doi.org/10.5281/zenodo.13358976",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.17178v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.17178v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.soc-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.soc-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.16967v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.16967v1",
                "updated": "2024-08-30T02:01:56Z",
                "updated_parsed": [
                    2024,
                    8,
                    30,
                    2,
                    1,
                    56,
                    4,
                    243,
                    0
                ],
                "published": "2024-08-30T02:01:56Z",
                "published_parsed": [
                    2024,
                    8,
                    30,
                    2,
                    1,
                    56,
                    4,
                    243,
                    0
                ],
                "title": "MemLong: Memory-Augmented Retrieval for Long Text Modeling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MemLong: Memory-Augmented Retrieval for Long Text Modeling"
                },
                "summary": "Recent advancements in Large Language Models (LLMs) have yielded remarkable\nsuccess across diverse fields. However, handling long contexts remains a\nsignificant challenge for LLMs due to the quadratic time and space complexity\nof attention mechanisms and the growing memory consumption of the key-value\ncache during generation. This work introduces MemLong: Memory-Augmented\nRetrieval for Long Text Generation, a method designed to enhance the\ncapabilities of long-context language modeling by utilizing an external\nretriever for historical information retrieval. MemLong combines a\nnon-differentiable ``ret-mem'' module with a partially trainable decoder-only\nlanguage model and introduces a fine-grained, controllable retrieval attention\nmechanism that leverages semantic-level relevant chunks. Comprehensive\nevaluations on multiple long-context language modeling benchmarks demonstrate\nthat MemLong consistently outperforms other state-of-the-art LLMs. More\nimportantly, MemLong can extend the context length on a single 3090 GPU from 4k\nup to 80k. Our code is available at https://github.com/Bui1dMySea/MemLong",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advancements in Large Language Models (LLMs) have yielded remarkable\nsuccess across diverse fields. However, handling long contexts remains a\nsignificant challenge for LLMs due to the quadratic time and space complexity\nof attention mechanisms and the growing memory consumption of the key-value\ncache during generation. This work introduces MemLong: Memory-Augmented\nRetrieval for Long Text Generation, a method designed to enhance the\ncapabilities of long-context language modeling by utilizing an external\nretriever for historical information retrieval. MemLong combines a\nnon-differentiable ``ret-mem'' module with a partially trainable decoder-only\nlanguage model and introduces a fine-grained, controllable retrieval attention\nmechanism that leverages semantic-level relevant chunks. Comprehensive\nevaluations on multiple long-context language modeling benchmarks demonstrate\nthat MemLong consistently outperforms other state-of-the-art LLMs. More\nimportantly, MemLong can extend the context length on a single 3090 GPU from 4k\nup to 80k. Our code is available at https://github.com/Bui1dMySea/MemLong"
                },
                "authors": [
                    {
                        "name": "Weijie Liu"
                    },
                    {
                        "name": "Zecheng Tang"
                    },
                    {
                        "name": "Juntao Li"
                    },
                    {
                        "name": "Kehai Chen"
                    },
                    {
                        "name": "Min Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Min Zhang"
                },
                "author": "Min Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.16967v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.16967v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2309.07975v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2309.07975v2",
                "updated": "2024-08-29T17:43:26Z",
                "updated_parsed": [
                    2024,
                    8,
                    29,
                    17,
                    43,
                    26,
                    3,
                    242,
                    0
                ],
                "published": "2023-09-14T18:18:10Z",
                "published_parsed": [
                    2023,
                    9,
                    14,
                    18,
                    18,
                    10,
                    3,
                    257,
                    0
                ],
                "title": "Smart Helper-Aided F-RANs: Improving Delay and Reducing Fronthaul Load",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Smart Helper-Aided F-RANs: Improving Delay and Reducing Fronthaul Load"
                },
                "summary": "In traditional Fog-Radio Access Networks (F-RANs), enhanced remote radio\nheads (eRRHs) are connected to a macro base station (MBS) through fronthaul\nlinks. Deploying a massive number of eRRHs is not always feasible due to site\nconstraints and the cost of fronthaul links. This paper introduces an\ninnovative concept of using smart helpers (SHs) in F-RANs. These SHs do not\nrequire fronthaul links and listen to the nearby eRRHs' communications. Then,\nthey smartly select and cache popular content. This capability enables SHs to\nserve users with frequent on-demand service requests potentially. As such,\nnetwork operators have the flexibility to easily deploy SHs in various\nscenarios, such as dense urban areas and temporary public events, to expand\ntheir F-RANs and improve the quality of service (QoS). To study the performance\nof the proposed SH-aided F-RAN, we formulate an optimization problem of\nminimizing the average transmission delay that jointly optimizes cache\nresources and user scheduling. To tackle the formulated problem, we develop an\ninnovative multi-stage algorithm that uses a reinforcement learning (RL)\nframework. Various performance measures, e.g., the average transmission delay,\nfronthaul load, and cache hit rate of the proposed SH-aided F-RAN are evaluated\nnumerically and compared with those of traditional F-RANs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In traditional Fog-Radio Access Networks (F-RANs), enhanced remote radio\nheads (eRRHs) are connected to a macro base station (MBS) through fronthaul\nlinks. Deploying a massive number of eRRHs is not always feasible due to site\nconstraints and the cost of fronthaul links. This paper introduces an\ninnovative concept of using smart helpers (SHs) in F-RANs. These SHs do not\nrequire fronthaul links and listen to the nearby eRRHs' communications. Then,\nthey smartly select and cache popular content. This capability enables SHs to\nserve users with frequent on-demand service requests potentially. As such,\nnetwork operators have the flexibility to easily deploy SHs in various\nscenarios, such as dense urban areas and temporary public events, to expand\ntheir F-RANs and improve the quality of service (QoS). To study the performance\nof the proposed SH-aided F-RAN, we formulate an optimization problem of\nminimizing the average transmission delay that jointly optimizes cache\nresources and user scheduling. To tackle the formulated problem, we develop an\ninnovative multi-stage algorithm that uses a reinforcement learning (RL)\nframework. Various performance measures, e.g., the average transmission delay,\nfronthaul load, and cache hit rate of the proposed SH-aided F-RAN are evaluated\nnumerically and compared with those of traditional F-RANs."
                },
                "authors": [
                    {
                        "name": "Hesameddin Mokhtarzadeh"
                    },
                    {
                        "name": "Mohammed S. Al-Abiad"
                    },
                    {
                        "name": "Md Jahangir Hossain"
                    },
                    {
                        "name": "Julian Cheng"
                    }
                ],
                "author_detail": {
                    "name": "Julian Cheng"
                },
                "author": "Julian Cheng",
                "arxiv_comment": "13 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2309.07975v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2309.07975v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IT",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.16730v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.16730v1",
                "updated": "2024-08-29T17:21:58Z",
                "updated_parsed": [
                    2024,
                    8,
                    29,
                    17,
                    21,
                    58,
                    3,
                    242,
                    0
                ],
                "published": "2024-08-29T17:21:58Z",
                "published_parsed": [
                    2024,
                    8,
                    29,
                    17,
                    21,
                    58,
                    3,
                    242,
                    0
                ],
                "title": "VideoLLM-MoD: Efficient Video-Language Streaming with Mixture-of-Depths\n  Vision Computation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "VideoLLM-MoD: Efficient Video-Language Streaming with Mixture-of-Depths\n  Vision Computation"
                },
                "summary": "A well-known dilemma in large vision-language models (e.g., GPT-4, LLaVA) is\nthat while increasing the number of vision tokens generally enhances visual\nunderstanding, it also significantly raises memory and computational costs,\nespecially in long-term, dense video frame streaming scenarios. Although\nlearnable approaches like Q-Former and Perceiver Resampler have been developed\nto reduce the vision token burden, they overlook the context causally modeled\nby LLMs (i.e., key-value cache), potentially leading to missed visual cues when\naddressing user queries. In this paper, we introduce a novel approach to reduce\nvision compute by leveraging redundant vision tokens \"skipping layers\" rather\nthan decreasing the number of vision tokens. Our method, VideoLLM-MoD, is\ninspired by mixture-of-depths LLMs and addresses the challenge of numerous\nvision tokens in long-term or streaming video. Specifically, for each\ntransformer layer, we learn to skip the computation for a high proportion\n(e.g., 80\\%) of vision tokens, passing them directly to the next layer. This\napproach significantly enhances model efficiency, achieving approximately\n\\textasciitilde42\\% time and \\textasciitilde30\\% memory savings for the entire\ntraining. Moreover, our method reduces the computation in the context and avoid\ndecreasing the vision tokens, thus preserving or even improving performance\ncompared to the vanilla model. We conduct extensive experiments to demonstrate\nthe effectiveness of VideoLLM-MoD, showing its state-of-the-art results on\nmultiple benchmarks, including narration, forecasting, and summarization tasks\nin COIN, Ego4D, and Ego-Exo4D datasets.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A well-known dilemma in large vision-language models (e.g., GPT-4, LLaVA) is\nthat while increasing the number of vision tokens generally enhances visual\nunderstanding, it also significantly raises memory and computational costs,\nespecially in long-term, dense video frame streaming scenarios. Although\nlearnable approaches like Q-Former and Perceiver Resampler have been developed\nto reduce the vision token burden, they overlook the context causally modeled\nby LLMs (i.e., key-value cache), potentially leading to missed visual cues when\naddressing user queries. In this paper, we introduce a novel approach to reduce\nvision compute by leveraging redundant vision tokens \"skipping layers\" rather\nthan decreasing the number of vision tokens. Our method, VideoLLM-MoD, is\ninspired by mixture-of-depths LLMs and addresses the challenge of numerous\nvision tokens in long-term or streaming video. Specifically, for each\ntransformer layer, we learn to skip the computation for a high proportion\n(e.g., 80\\%) of vision tokens, passing them directly to the next layer. This\napproach significantly enhances model efficiency, achieving approximately\n\\textasciitilde42\\% time and \\textasciitilde30\\% memory savings for the entire\ntraining. Moreover, our method reduces the computation in the context and avoid\ndecreasing the vision tokens, thus preserving or even improving performance\ncompared to the vanilla model. We conduct extensive experiments to demonstrate\nthe effectiveness of VideoLLM-MoD, showing its state-of-the-art results on\nmultiple benchmarks, including narration, forecasting, and summarization tasks\nin COIN, Ego4D, and Ego-Exo4D datasets."
                },
                "authors": [
                    {
                        "name": "Shiwei Wu"
                    },
                    {
                        "name": "Joya Chen"
                    },
                    {
                        "name": "Kevin Qinghong Lin"
                    },
                    {
                        "name": "Qimeng Wang"
                    },
                    {
                        "name": "Yan Gao"
                    },
                    {
                        "name": "Qianli Xu"
                    },
                    {
                        "name": "Tong Xu"
                    },
                    {
                        "name": "Yao Hu"
                    },
                    {
                        "name": "Enhong Chen"
                    },
                    {
                        "name": "Mike Zheng Shou"
                    }
                ],
                "author_detail": {
                    "name": "Mike Zheng Shou"
                },
                "author": "Mike Zheng Shou",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.16730v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.16730v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2403.05527v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2403.05527v3",
                "updated": "2024-08-29T16:48:58Z",
                "updated_parsed": [
                    2024,
                    8,
                    29,
                    16,
                    48,
                    58,
                    3,
                    242,
                    0
                ],
                "published": "2024-03-08T18:48:30Z",
                "published_parsed": [
                    2024,
                    3,
                    8,
                    18,
                    48,
                    30,
                    4,
                    68,
                    0
                ],
                "title": "GEAR: An Efficient KV Cache Compression Recipe for Near-Lossless\n  Generative Inference of LLM",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GEAR: An Efficient KV Cache Compression Recipe for Near-Lossless\n  Generative Inference of LLM"
                },
                "summary": "Key-value (KV) caching has become the de-facto to accelerate generation speed\nfor large language models (LLMs) inference. However, the growing cache demand\nwith increasing sequence length has transformed LLM inference to be a memory\nbound problem, significantly constraining the system throughput. Existing\nmethods rely on dropping unimportant tokens or quantizing all entries\nuniformly. Such methods, however, often incur high approximation errors to\nrepresent the compressed matrices. The autoregressive decoding process further\ncompounds the error of each step, resulting in critical deviation in model\ngeneration and deterioration of performance. To tackle this challenge, we\npropose GEAR, an efficient KV cache compression framework that achieves\nnear-lossless high-ratio compression. GEAR first applies quantization to\nmajority of entries of similar magnitudes to ultra-low precision. It then\nemploys a low rank matrix to approximate the quantization error, and a sparse\nmatrix to remedy individual errors from outlier entries. By adeptly integrating\nthree techniques, GEAR is able to fully exploit their synergistic potentials.\nOur experiments demonstrate that compared to alternatives, GEAR achieves\nnear-lossless 4-bit KV cache compression with up to 2.38x throughput\nimprovement, while reducing peak-memory size up to 2.29x. Our code is publicly\navailable at https://github.com/HaoKang-Timmy/GEAR.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Key-value (KV) caching has become the de-facto to accelerate generation speed\nfor large language models (LLMs) inference. However, the growing cache demand\nwith increasing sequence length has transformed LLM inference to be a memory\nbound problem, significantly constraining the system throughput. Existing\nmethods rely on dropping unimportant tokens or quantizing all entries\nuniformly. Such methods, however, often incur high approximation errors to\nrepresent the compressed matrices. The autoregressive decoding process further\ncompounds the error of each step, resulting in critical deviation in model\ngeneration and deterioration of performance. To tackle this challenge, we\npropose GEAR, an efficient KV cache compression framework that achieves\nnear-lossless high-ratio compression. GEAR first applies quantization to\nmajority of entries of similar magnitudes to ultra-low precision. It then\nemploys a low rank matrix to approximate the quantization error, and a sparse\nmatrix to remedy individual errors from outlier entries. By adeptly integrating\nthree techniques, GEAR is able to fully exploit their synergistic potentials.\nOur experiments demonstrate that compared to alternatives, GEAR achieves\nnear-lossless 4-bit KV cache compression with up to 2.38x throughput\nimprovement, while reducing peak-memory size up to 2.29x. Our code is publicly\navailable at https://github.com/HaoKang-Timmy/GEAR."
                },
                "authors": [
                    {
                        "name": "Hao Kang"
                    },
                    {
                        "name": "Qingru Zhang"
                    },
                    {
                        "name": "Souvik Kundu"
                    },
                    {
                        "name": "Geonhwa Jeong"
                    },
                    {
                        "name": "Zaoxing Liu"
                    },
                    {
                        "name": "Tushar Krishna"
                    },
                    {
                        "name": "Tuo Zhao"
                    }
                ],
                "author_detail": {
                    "name": "Tuo Zhao"
                },
                "author": "Tuo Zhao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2403.05527v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2403.05527v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.16220v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.16220v1",
                "updated": "2024-08-29T02:31:28Z",
                "updated_parsed": [
                    2024,
                    8,
                    29,
                    2,
                    31,
                    28,
                    3,
                    242,
                    0
                ],
                "published": "2024-08-29T02:31:28Z",
                "published_parsed": [
                    2024,
                    8,
                    29,
                    2,
                    31,
                    28,
                    3,
                    242,
                    0
                ],
                "title": "LightSLH: Provable and Low-Overhead Spectre v1 Mitigation through\n  Targeted Instruction Hardening",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LightSLH: Provable and Low-Overhead Spectre v1 Mitigation through\n  Targeted Instruction Hardening"
                },
                "summary": "Several software mitigations have been proposed to defend against Spectre\nvulnerabilities. However, these countermeasures often suffer from high\nperformance overhead, largely due to unnecessary protections. We propose\nLightSLH, designed to mitigate this overhead by hardening instructions only\nwhen they are under threat from Spectre vulnerabilities. LightSLH leverages\nprogram analysis techniques based on abstract interpretation to identify all\ninstructions that could potentially lead to Spectre vulnerabilities and\nprovides provable protection. To enhance analysis efficiency and precision,\nLightSLH employs novel taint and value domains. The taint domain enables\nbit-level taint tracking, while the value domain allows LightSLH to analyze\ncomplex program structures such as pointers and structures. Furthermore,\nLightSLH uses a two-stage abstract interpretation approach to circumvent\npotential analysis paralysis issues.\n  We demonstrate the security guarantees of LightSLH and evaluate its\nperformance on cryptographic algorithm implementations from OpenSSL. LightSLH\nsignificantly reduces the overhead associated with speculative-load-hardening\ntechniques. Our results show that LightSLH introduces no protection and thus no\noverhead on 4 out of the 7 studied algorithms, which contrasts with existing\ncountermeasures that introduce additional overhead due to unnecessary\nhardening. Additionally, LightSLH performs, for the first time, a rigorous\nanalysis of the security guarantees of RSA against Spectre v1, highlighting\nthat the memory access patterns generated by the scatter-gather algorithm\ndepend on secrets, even for observers at the cache line granularity,\nnecessitating protection for such accesses.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Several software mitigations have been proposed to defend against Spectre\nvulnerabilities. However, these countermeasures often suffer from high\nperformance overhead, largely due to unnecessary protections. We propose\nLightSLH, designed to mitigate this overhead by hardening instructions only\nwhen they are under threat from Spectre vulnerabilities. LightSLH leverages\nprogram analysis techniques based on abstract interpretation to identify all\ninstructions that could potentially lead to Spectre vulnerabilities and\nprovides provable protection. To enhance analysis efficiency and precision,\nLightSLH employs novel taint and value domains. The taint domain enables\nbit-level taint tracking, while the value domain allows LightSLH to analyze\ncomplex program structures such as pointers and structures. Furthermore,\nLightSLH uses a two-stage abstract interpretation approach to circumvent\npotential analysis paralysis issues.\n  We demonstrate the security guarantees of LightSLH and evaluate its\nperformance on cryptographic algorithm implementations from OpenSSL. LightSLH\nsignificantly reduces the overhead associated with speculative-load-hardening\ntechniques. Our results show that LightSLH introduces no protection and thus no\noverhead on 4 out of the 7 studied algorithms, which contrasts with existing\ncountermeasures that introduce additional overhead due to unnecessary\nhardening. Additionally, LightSLH performs, for the first time, a rigorous\nanalysis of the security guarantees of RSA against Spectre v1, highlighting\nthat the memory access patterns generated by the scatter-gather algorithm\ndepend on secrets, even for observers at the cache line granularity,\nnecessitating protection for such accesses."
                },
                "authors": [
                    {
                        "name": "Yiming Zhu"
                    },
                    {
                        "name": "Wenchao Huang"
                    },
                    {
                        "name": "Yan Xiong"
                    }
                ],
                "author_detail": {
                    "name": "Yan Xiong"
                },
                "author": "Yan Xiong",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.16220v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.16220v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.08286v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.08286v1",
                "updated": "2024-08-28T17:28:12Z",
                "updated_parsed": [
                    2024,
                    8,
                    28,
                    17,
                    28,
                    12,
                    2,
                    241,
                    0
                ],
                "published": "2024-08-28T17:28:12Z",
                "published_parsed": [
                    2024,
                    8,
                    28,
                    17,
                    28,
                    12,
                    2,
                    241,
                    0
                ],
                "title": "On the Impact of ISA Extension on Energy Consumption of I-Cache in\n  Extensible Processors",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "On the Impact of ISA Extension on Energy Consumption of I-Cache in\n  Extensible Processors"
                },
                "summary": "As is widely known, the computational speed and power consumption are two\ncritical parameters in microprocessor design. A solution for these issues is\nthe application specific instruction set processor (ASIP) methodology, which\ncan improve speed and reduce power consumption of the general purpose processor\n(GPP) technique. In ASIP, changing the instruction set architecture (ISA) of\nthe processor will lead to alter the number and the mean time of accesses to\nthe cache memory. This issue has a direct impact on the processor energy\nconsumption. In this work, we study the impacts of extended ISA on the energy\nconsumption of the extended ISA processor. Also, we demonstrate the extended\nISA let the designer to reduce the cache size in order to minimize the energy\nconsumption while meeting performance constraint.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As is widely known, the computational speed and power consumption are two\ncritical parameters in microprocessor design. A solution for these issues is\nthe application specific instruction set processor (ASIP) methodology, which\ncan improve speed and reduce power consumption of the general purpose processor\n(GPP) technique. In ASIP, changing the instruction set architecture (ISA) of\nthe processor will lead to alter the number and the mean time of accesses to\nthe cache memory. This issue has a direct impact on the processor energy\nconsumption. In this work, we study the impacts of extended ISA on the energy\nconsumption of the extended ISA processor. Also, we demonstrate the extended\nISA let the designer to reduce the cache size in order to minimize the energy\nconsumption while meeting performance constraint."
                },
                "authors": [
                    {
                        "name": "Noushin Behboudi"
                    },
                    {
                        "name": "Mehdi Kamal"
                    },
                    {
                        "name": "Ali Afzali-Kusha"
                    }
                ],
                "author_detail": {
                    "name": "Ali Afzali-Kusha"
                },
                "author": "Ali Afzali-Kusha",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.08286v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.08286v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2306.06942v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2306.06942v3",
                "updated": "2024-08-28T08:41:45Z",
                "updated_parsed": [
                    2024,
                    8,
                    28,
                    8,
                    41,
                    45,
                    2,
                    241,
                    0
                ],
                "published": "2023-06-12T08:24:14Z",
                "published_parsed": [
                    2023,
                    6,
                    12,
                    8,
                    24,
                    14,
                    0,
                    163,
                    0
                ],
                "title": "RIP Linked List",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RIP Linked List"
                },
                "summary": "Linked lists have long served as a valuable teaching tool in programming.\nHowever, the question arises: Are they truly practical for everyday program\nuse? In most cases, it appears that array-based data structures offer distinct\nadvantages, particularly in terms of memory efficiency and,more importantly,\nexecution speed. While it's relatively straightforward to calculate the\ncomplexity of operations, gauging actual execution efficiency remains a\nchallenge. This paper addresses this question by introducing a new benchmark.\nOur study compares various linked list implementations with several array-based\nalternatives. We also demonstrate the ease of incorporating memory caching for\nlinked lists, enhancing their performance. Additionally, we introduce a new\narray-based data structure designed to excel in a wide range of operations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Linked lists have long served as a valuable teaching tool in programming.\nHowever, the question arises: Are they truly practical for everyday program\nuse? In most cases, it appears that array-based data structures offer distinct\nadvantages, particularly in terms of memory efficiency and,more importantly,\nexecution speed. While it's relatively straightforward to calculate the\ncomplexity of operations, gauging actual execution efficiency remains a\nchallenge. This paper addresses this question by introducing a new benchmark.\nOur study compares various linked list implementations with several array-based\nalternatives. We also demonstrate the ease of incorporating memory caching for\nlinked lists, enhancing their performance. Additionally, we introduce a new\narray-based data structure designed to excel in a wide range of operations."
                },
                "authors": [
                    {
                        "name": "Benoît Sonntag"
                    },
                    {
                        "name": "Dominique Colnet"
                    }
                ],
                "author_detail": {
                    "name": "Dominique Colnet"
                },
                "arxiv_affiliation": "LORIA",
                "author": "Dominique Colnet",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2306.06942v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2306.06942v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.17678v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.17678v2",
                "updated": "2024-08-27T22:06:20Z",
                "updated_parsed": [
                    2024,
                    8,
                    27,
                    22,
                    6,
                    20,
                    1,
                    240,
                    0
                ],
                "published": "2024-07-25T00:27:07Z",
                "published_parsed": [
                    2024,
                    7,
                    25,
                    0,
                    27,
                    7,
                    3,
                    207,
                    0
                ],
                "title": "Efficient LLM Training and Serving with Heterogeneous Context Sharding\n  among Attention Heads",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient LLM Training and Serving with Heterogeneous Context Sharding\n  among Attention Heads"
                },
                "summary": "Existing LLM training and inference frameworks struggle in boosting\nefficiency with sparsity while maintaining the integrity of context and model\narchitecture. Inspired by the sharding concept in database and the fact that\nattention parallelizes over heads on accelerators, we propose Sparsely-Sharded\n(S2) Attention, an attention algorithm that allocates heterogeneous context\npartitions for different attention heads to divide and conquer. S2-Attention\nenforces each attention head to only attend to a partition of contexts\nfollowing a strided sparsity pattern, while the full context is preserved as\nthe union of all the shards. As attention heads are processed in separate\nthread blocks, the context reduction for each head can thus produce end-to-end\nspeed-up and memory reduction. At inference, LLMs trained with S2-Attention can\nthen take the KV cache reduction as free meals with guaranteed model quality\npreserve. In experiments, we show S2-Attentioncan provide as much as (1) 25.3X\nwall-clock attention speed-up over FlashAttention-2, resulting in 6X reduction\nin end-to-end training time and 10X inference latency, (2) on-par model\ntraining quality compared to default attention, (3)perfect needle retrieval\naccuracy over 32K context window. On top of the algorithm, we build DKernel, an\nLLM training and inference kernel library that allows users to customize\nsparsity patterns for their own models. We open-sourced DKerneland make it\ncompatible with Megatron, Pytorch, and vLLM.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Existing LLM training and inference frameworks struggle in boosting\nefficiency with sparsity while maintaining the integrity of context and model\narchitecture. Inspired by the sharding concept in database and the fact that\nattention parallelizes over heads on accelerators, we propose Sparsely-Sharded\n(S2) Attention, an attention algorithm that allocates heterogeneous context\npartitions for different attention heads to divide and conquer. S2-Attention\nenforces each attention head to only attend to a partition of contexts\nfollowing a strided sparsity pattern, while the full context is preserved as\nthe union of all the shards. As attention heads are processed in separate\nthread blocks, the context reduction for each head can thus produce end-to-end\nspeed-up and memory reduction. At inference, LLMs trained with S2-Attention can\nthen take the KV cache reduction as free meals with guaranteed model quality\npreserve. In experiments, we show S2-Attentioncan provide as much as (1) 25.3X\nwall-clock attention speed-up over FlashAttention-2, resulting in 6X reduction\nin end-to-end training time and 10X inference latency, (2) on-par model\ntraining quality compared to default attention, (3)perfect needle retrieval\naccuracy over 32K context window. On top of the algorithm, we build DKernel, an\nLLM training and inference kernel library that allows users to customize\nsparsity patterns for their own models. We open-sourced DKerneland make it\ncompatible with Megatron, Pytorch, and vLLM."
                },
                "authors": [
                    {
                        "name": "Xihui Lin"
                    },
                    {
                        "name": "Yunan Zhang"
                    },
                    {
                        "name": "Suyu Ge"
                    },
                    {
                        "name": "Barun Patra"
                    },
                    {
                        "name": "Vishrav Chaudhary"
                    },
                    {
                        "name": "Hao Peng"
                    },
                    {
                        "name": "Xia Song"
                    }
                ],
                "author_detail": {
                    "name": "Xia Song"
                },
                "author": "Xia Song",
                "arxiv_comment": "10 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.17678v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.17678v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2312.06893v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2312.06893v3",
                "updated": "2024-08-27T17:30:41Z",
                "updated_parsed": [
                    2024,
                    8,
                    27,
                    17,
                    30,
                    41,
                    1,
                    240,
                    0
                ],
                "published": "2023-12-11T23:34:23Z",
                "published_parsed": [
                    2023,
                    12,
                    11,
                    23,
                    34,
                    23,
                    0,
                    345,
                    0
                ],
                "title": "Styx: Transactional Stateful Functions on Streaming Dataflows",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Styx: Transactional Stateful Functions on Streaming Dataflows"
                },
                "summary": "Developing stateful cloud applications, such as low-latency workflows and\nmicroservices with strict consistency requirements, remains arduous for\nprogrammers. The Stateful Functions-as-a-Service (SFaaS) paradigm aims to serve\nthese use cases. However, existing approaches either provide serializable\ntransactional guarantees at the level of individual functions, or separate\napplication logic from the state and use inefficient transactional protocols.\nThese design choices increase the execution latency, limiting the adoption of\nSFaaS systems.\n  In this paper, we present Styx, a novel SFaaS runtime that executes\nserializable transactions across functions with exactly-once guarantees. Styx\nextends a deterministic transactional protocol to support an arbitrary call\ngraph of stateful functions. It introduces a transaction-execution\nacknowledgment scheme that allows tracking a transactional workflow's SFaaS\ncalls, guaranteeing atomicity and exactly-once processing. Finally, Styx\nfeatures a function-execution caching mechanism and early transactional commit\nreplies for optimized performance. Experiments with the YCSB-T, TPC-C, and\nDeathstar benchmarks show that Styx outperforms state-of-the-art approaches by\nachieving at least one order of magnitude higher throughput while exhibiting\nnear-linear scalability and low latency.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Developing stateful cloud applications, such as low-latency workflows and\nmicroservices with strict consistency requirements, remains arduous for\nprogrammers. The Stateful Functions-as-a-Service (SFaaS) paradigm aims to serve\nthese use cases. However, existing approaches either provide serializable\ntransactional guarantees at the level of individual functions, or separate\napplication logic from the state and use inefficient transactional protocols.\nThese design choices increase the execution latency, limiting the adoption of\nSFaaS systems.\n  In this paper, we present Styx, a novel SFaaS runtime that executes\nserializable transactions across functions with exactly-once guarantees. Styx\nextends a deterministic transactional protocol to support an arbitrary call\ngraph of stateful functions. It introduces a transaction-execution\nacknowledgment scheme that allows tracking a transactional workflow's SFaaS\ncalls, guaranteeing atomicity and exactly-once processing. Finally, Styx\nfeatures a function-execution caching mechanism and early transactional commit\nreplies for optimized performance. Experiments with the YCSB-T, TPC-C, and\nDeathstar benchmarks show that Styx outperforms state-of-the-art approaches by\nachieving at least one order of magnitude higher throughput while exhibiting\nnear-linear scalability and low latency."
                },
                "authors": [
                    {
                        "name": "Kyriakos Psarakis"
                    },
                    {
                        "name": "George Siachamis"
                    },
                    {
                        "name": "George Christodoulou"
                    },
                    {
                        "name": "Marios Fragkoulis"
                    },
                    {
                        "name": "Asterios Katsifodimos"
                    }
                ],
                "author_detail": {
                    "name": "Asterios Katsifodimos"
                },
                "author": "Asterios Katsifodimos",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2312.06893v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2312.06893v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.14906v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.14906v1",
                "updated": "2024-08-27T09:34:38Z",
                "updated_parsed": [
                    2024,
                    8,
                    27,
                    9,
                    34,
                    38,
                    1,
                    240,
                    0
                ],
                "published": "2024-08-27T09:34:38Z",
                "published_parsed": [
                    2024,
                    8,
                    27,
                    9,
                    34,
                    38,
                    1,
                    240,
                    0
                ],
                "title": "Writing in the Margins: Better Inference Pattern for Long Context\n  Retrieval",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Writing in the Margins: Better Inference Pattern for Long Context\n  Retrieval"
                },
                "summary": "In this paper, we introduce Writing in the Margins (WiM), a new inference\npattern for Large Language Models designed to optimize the handling of long\ninput sequences in retrieval-oriented tasks. This approach leverages the\nchunked prefill of the key-value cache to perform segment-wise inference, which\nenables efficient processing of extensive contexts along with the generation\nand classification of intermediate information (\"margins\") that guide the model\ntowards specific tasks. This method increases computational overhead marginally\nwhile significantly enhancing the performance of off-the-shelf models without\nthe need for fine-tuning. Specifically, we observe that WiM provides an average\nenhancement of 7.5% in accuracy for reasoning skills (HotpotQA, MultiHop-RAG)\nand more than a 30.0% increase in the F1-score for aggregation tasks (CWE).\nAdditionally, we show how the proposed pattern fits into an interactive\nretrieval design that provides end-users with ongoing updates about the\nprogress of context processing, and pinpoints the integration of relevant\ninformation into the final response. We release our implementation of WiM using\nHugging Face Transformers library at\nhttps://github.com/writer/writing-in-the-margins.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this paper, we introduce Writing in the Margins (WiM), a new inference\npattern for Large Language Models designed to optimize the handling of long\ninput sequences in retrieval-oriented tasks. This approach leverages the\nchunked prefill of the key-value cache to perform segment-wise inference, which\nenables efficient processing of extensive contexts along with the generation\nand classification of intermediate information (\"margins\") that guide the model\ntowards specific tasks. This method increases computational overhead marginally\nwhile significantly enhancing the performance of off-the-shelf models without\nthe need for fine-tuning. Specifically, we observe that WiM provides an average\nenhancement of 7.5% in accuracy for reasoning skills (HotpotQA, MultiHop-RAG)\nand more than a 30.0% increase in the F1-score for aggregation tasks (CWE).\nAdditionally, we show how the proposed pattern fits into an interactive\nretrieval design that provides end-users with ongoing updates about the\nprogress of context processing, and pinpoints the integration of relevant\ninformation into the final response. We release our implementation of WiM using\nHugging Face Transformers library at\nhttps://github.com/writer/writing-in-the-margins."
                },
                "authors": [
                    {
                        "name": "Melisa Russak"
                    },
                    {
                        "name": "Umar Jamil"
                    },
                    {
                        "name": "Christopher Bryant"
                    },
                    {
                        "name": "Kiran Kamble"
                    },
                    {
                        "name": "Axel Magnuson"
                    },
                    {
                        "name": "Mateusz Russak"
                    },
                    {
                        "name": "Waseem AlShikh"
                    }
                ],
                "author_detail": {
                    "name": "Waseem AlShikh"
                },
                "author": "Waseem AlShikh",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.14906v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.14906v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.14735v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.14735v1",
                "updated": "2024-08-27T02:03:36Z",
                "updated_parsed": [
                    2024,
                    8,
                    27,
                    2,
                    3,
                    36,
                    1,
                    240,
                    0
                ],
                "published": "2024-08-27T02:03:36Z",
                "published_parsed": [
                    2024,
                    8,
                    27,
                    2,
                    3,
                    36,
                    1,
                    240,
                    0
                ],
                "title": "PPVF: An Efficient Privacy-Preserving Online Video Fetching Framework\n  with Correlated Differential Privacy",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PPVF: An Efficient Privacy-Preserving Online Video Fetching Framework\n  with Correlated Differential Privacy"
                },
                "summary": "Online video streaming has evolved into an integral component of the\ncontemporary Internet landscape. Yet, the disclosure of user requests presents\nformidable privacy challenges. As users stream their preferred online videos,\ntheir requests are automatically seized by video content providers, potentially\nleaking users' privacy.\n  Unfortunately, current protection methods are not well-suited to preserving\nuser request privacy from content providers while maintaining high-quality\nonline video services. To tackle this challenge, we introduce a novel\nPrivacy-Preserving Video Fetching (PPVF) framework, which utilizes trusted edge\ndevices to pre-fetch and cache videos, ensuring the privacy of users' requests\nwhile optimizing the efficiency of edge caching. More specifically, we design\nPPVF with three core components: (1) \\textit{Online privacy budget scheduler},\nwhich employs a theoretically guaranteed online algorithm to select\nnon-requested videos as candidates with assigned privacy budgets. Alternative\nvideos are chosen by an online algorithm that is theoretically guaranteed to\nconsider both video utilities and available privacy budgets. (2) \\textit{Noisy\nvideo request generator}, which generates redundant video requests (in addition\nto original ones) utilizing correlated differential privacy to obfuscate\nrequest privacy. (3) \\textit{Online video utility predictor}, which leverages\nfederated learning to collaboratively evaluate video utility in an online\nfashion, aiding in video selection in (1) and noise generation in (2). Finally,\nwe conduct extensive experiments using real-world video request traces from\nTencent Video. The results demonstrate that PPVF effectively safeguards user\nrequest privacy while upholding high video caching performance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Online video streaming has evolved into an integral component of the\ncontemporary Internet landscape. Yet, the disclosure of user requests presents\nformidable privacy challenges. As users stream their preferred online videos,\ntheir requests are automatically seized by video content providers, potentially\nleaking users' privacy.\n  Unfortunately, current protection methods are not well-suited to preserving\nuser request privacy from content providers while maintaining high-quality\nonline video services. To tackle this challenge, we introduce a novel\nPrivacy-Preserving Video Fetching (PPVF) framework, which utilizes trusted edge\ndevices to pre-fetch and cache videos, ensuring the privacy of users' requests\nwhile optimizing the efficiency of edge caching. More specifically, we design\nPPVF with three core components: (1) \\textit{Online privacy budget scheduler},\nwhich employs a theoretically guaranteed online algorithm to select\nnon-requested videos as candidates with assigned privacy budgets. Alternative\nvideos are chosen by an online algorithm that is theoretically guaranteed to\nconsider both video utilities and available privacy budgets. (2) \\textit{Noisy\nvideo request generator}, which generates redundant video requests (in addition\nto original ones) utilizing correlated differential privacy to obfuscate\nrequest privacy. (3) \\textit{Online video utility predictor}, which leverages\nfederated learning to collaboratively evaluate video utility in an online\nfashion, aiding in video selection in (1) and noise generation in (2). Finally,\nwe conduct extensive experiments using real-world video request traces from\nTencent Video. The results demonstrate that PPVF effectively safeguards user\nrequest privacy while upholding high video caching performance."
                },
                "authors": [
                    {
                        "name": "Xianzhi Zhang"
                    },
                    {
                        "name": "Yipeng Zhou"
                    },
                    {
                        "name": "Di Wu"
                    },
                    {
                        "name": "Quan Z. Sheng"
                    },
                    {
                        "name": "Miao Hu"
                    },
                    {
                        "name": "Linchang Xiao"
                    }
                ],
                "author_detail": {
                    "name": "Linchang Xiao"
                },
                "author": "Linchang Xiao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.14735v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.14735v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.MM",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.MM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.10774v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.10774v2",
                "updated": "2024-08-26T21:01:02Z",
                "updated_parsed": [
                    2024,
                    8,
                    26,
                    21,
                    1,
                    2,
                    0,
                    239,
                    0
                ],
                "published": "2024-06-16T01:33:02Z",
                "published_parsed": [
                    2024,
                    6,
                    16,
                    1,
                    33,
                    2,
                    6,
                    168,
                    0
                ],
                "title": "Quest: Query-Aware Sparsity for Efficient Long-Context LLM Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Quest: Query-Aware Sparsity for Efficient Long-Context LLM Inference"
                },
                "summary": "As the demand for long-context large language models (LLMs) increases, models\nwith context windows of up to 128K or 1M tokens are becoming increasingly\nprevalent. However, long-context LLM inference is challenging since the\ninference speed decreases significantly as the sequence length grows. This\nslowdown is primarily caused by loading a large KV cache during self-attention.\nPrevious works have shown that a small portion of critical tokens will dominate\nthe attention outcomes. However, we observe the criticality of a token highly\ndepends on the query. To this end, we propose Quest, a query-aware KV cache\nselection algorithm. Quest keeps track of the minimal and maximal Key values in\nKV cache pages and estimates the criticality of a given page using Query\nvectors. By only loading the Top-K critical KV cache pages for attention, Quest\nsignificantly speeds up self-attention without sacrificing accuracy. We show\nthat Quest can achieve up to 2.23x self-attention speedup, which reduces\ninference latency by 7.03x while performing well on tasks with long\ndependencies with negligible accuracy loss. Code is available at\nhttp://github.com/mit-han-lab/Quest .",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As the demand for long-context large language models (LLMs) increases, models\nwith context windows of up to 128K or 1M tokens are becoming increasingly\nprevalent. However, long-context LLM inference is challenging since the\ninference speed decreases significantly as the sequence length grows. This\nslowdown is primarily caused by loading a large KV cache during self-attention.\nPrevious works have shown that a small portion of critical tokens will dominate\nthe attention outcomes. However, we observe the criticality of a token highly\ndepends on the query. To this end, we propose Quest, a query-aware KV cache\nselection algorithm. Quest keeps track of the minimal and maximal Key values in\nKV cache pages and estimates the criticality of a given page using Query\nvectors. By only loading the Top-K critical KV cache pages for attention, Quest\nsignificantly speeds up self-attention without sacrificing accuracy. We show\nthat Quest can achieve up to 2.23x self-attention speedup, which reduces\ninference latency by 7.03x while performing well on tasks with long\ndependencies with negligible accuracy loss. Code is available at\nhttp://github.com/mit-han-lab/Quest ."
                },
                "authors": [
                    {
                        "name": "Jiaming Tang"
                    },
                    {
                        "name": "Yilong Zhao"
                    },
                    {
                        "name": "Kan Zhu"
                    },
                    {
                        "name": "Guangxuan Xiao"
                    },
                    {
                        "name": "Baris Kasikci"
                    },
                    {
                        "name": "Song Han"
                    }
                ],
                "author_detail": {
                    "name": "Song Han"
                },
                "author": "Song Han",
                "arxiv_comment": "ICML 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.10774v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.10774v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.14434v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.14434v1",
                "updated": "2024-08-26T17:21:19Z",
                "updated_parsed": [
                    2024,
                    8,
                    26,
                    17,
                    21,
                    19,
                    0,
                    239,
                    0
                ],
                "published": "2024-08-26T17:21:19Z",
                "published_parsed": [
                    2024,
                    8,
                    26,
                    17,
                    21,
                    19,
                    0,
                    239,
                    0
                ],
                "title": "Employing Artificial Intelligence to Steer Exascale Workflows with\n  Colmena",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Employing Artificial Intelligence to Steer Exascale Workflows with\n  Colmena"
                },
                "summary": "Computational workflows are a common class of application on supercomputers,\nyet the loosely coupled and heterogeneous nature of workflows often fails to\ntake full advantage of their capabilities. We created Colmena to leverage the\nmassive parallelism of a supercomputer by using Artificial Intelligence (AI) to\nlearn from and adapt a workflow as it executes. Colmena allows scientists to\ndefine how their application should respond to events (e.g., task completion)\nas a series of cooperative agents. In this paper, we describe the design of\nColmena, the challenges we overcame while deploying applications on exascale\nsystems, and the science workflows we have enhanced through interweaving AI.\nThe scaling challenges we discuss include developing steering strategies that\nmaximize node utilization, introducing data fabrics that reduce communication\noverhead of data-intensive tasks, and implementing workflow tasks that cache\ncostly operations between invocations. These innovations coupled with a variety\nof application patterns accessible through our agent-based steering model have\nenabled science advances in chemistry, biophysics, and materials science using\ndifferent types of AI. Our vision is that Colmena will spur creative solutions\nthat harness AI across many domains of scientific computing.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Computational workflows are a common class of application on supercomputers,\nyet the loosely coupled and heterogeneous nature of workflows often fails to\ntake full advantage of their capabilities. We created Colmena to leverage the\nmassive parallelism of a supercomputer by using Artificial Intelligence (AI) to\nlearn from and adapt a workflow as it executes. Colmena allows scientists to\ndefine how their application should respond to events (e.g., task completion)\nas a series of cooperative agents. In this paper, we describe the design of\nColmena, the challenges we overcame while deploying applications on exascale\nsystems, and the science workflows we have enhanced through interweaving AI.\nThe scaling challenges we discuss include developing steering strategies that\nmaximize node utilization, introducing data fabrics that reduce communication\noverhead of data-intensive tasks, and implementing workflow tasks that cache\ncostly operations between invocations. These innovations coupled with a variety\nof application patterns accessible through our agent-based steering model have\nenabled science advances in chemistry, biophysics, and materials science using\ndifferent types of AI. Our vision is that Colmena will spur creative solutions\nthat harness AI across many domains of scientific computing."
                },
                "authors": [
                    {
                        "name": "Logan Ward"
                    },
                    {
                        "name": "J. Gregory Pauloski"
                    },
                    {
                        "name": "Valerie Hayot-Sasson"
                    },
                    {
                        "name": "Yadu Babuji"
                    },
                    {
                        "name": "Alexander Brace"
                    },
                    {
                        "name": "Ryan Chard"
                    },
                    {
                        "name": "Kyle Chard"
                    },
                    {
                        "name": "Rajeev Thakur"
                    },
                    {
                        "name": "Ian Foster"
                    }
                ],
                "author_detail": {
                    "name": "Ian Foster"
                },
                "author": "Ian Foster",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.14434v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.14434v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.06876v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.06876v2",
                "updated": "2024-08-26T11:29:07Z",
                "updated_parsed": [
                    2024,
                    8,
                    26,
                    11,
                    29,
                    7,
                    0,
                    239,
                    0
                ],
                "published": "2024-08-13T13:14:54Z",
                "published_parsed": [
                    2024,
                    8,
                    13,
                    13,
                    14,
                    54,
                    1,
                    226,
                    0
                ],
                "title": "Decision-Focused Learning to Predict Action Costs for Planning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Decision-Focused Learning to Predict Action Costs for Planning"
                },
                "summary": "In many automated planning applications, action costs can be hard to specify.\nAn example is the time needed to travel through a certain road segment, which\ndepends on many factors, such as the current weather conditions. A natural way\nto address this issue is to learn to predict these parameters based on input\nfeatures (e.g., weather forecasts) and use the predicted action costs in\nautomated planning afterward. Decision-Focused Learning (DFL) has been\nsuccessful in learning to predict the parameters of combinatorial optimization\nproblems in a way that optimizes solution quality rather than prediction\nquality. This approach yields better results than treating prediction and\noptimization as separate tasks. In this paper, we investigate for the first\ntime the challenges of implementing DFL for automated planning in order to\nlearn to predict the action costs. There are two main challenges to overcome:\n(1) planning systems are called during gradient descent learning, to solve\nplanning problems with negative action costs, which are not supported in\nplanning. We propose novel methods for gradient computation to avoid this\nissue. (2) DFL requires repeated planner calls during training, which can limit\nthe scalability of the method. We experiment with different methods\napproximating the optimal plan as well as an easy-to-implement caching\nmechanism to speed up the learning process. As the first work that addresses\nDFL for automated planning, we demonstrate that the proposed gradient\ncomputation consistently yields significantly better plans than predictions\naimed at minimizing prediction error; and that caching can temper the\ncomputation requirements.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In many automated planning applications, action costs can be hard to specify.\nAn example is the time needed to travel through a certain road segment, which\ndepends on many factors, such as the current weather conditions. A natural way\nto address this issue is to learn to predict these parameters based on input\nfeatures (e.g., weather forecasts) and use the predicted action costs in\nautomated planning afterward. Decision-Focused Learning (DFL) has been\nsuccessful in learning to predict the parameters of combinatorial optimization\nproblems in a way that optimizes solution quality rather than prediction\nquality. This approach yields better results than treating prediction and\noptimization as separate tasks. In this paper, we investigate for the first\ntime the challenges of implementing DFL for automated planning in order to\nlearn to predict the action costs. There are two main challenges to overcome:\n(1) planning systems are called during gradient descent learning, to solve\nplanning problems with negative action costs, which are not supported in\nplanning. We propose novel methods for gradient computation to avoid this\nissue. (2) DFL requires repeated planner calls during training, which can limit\nthe scalability of the method. We experiment with different methods\napproximating the optimal plan as well as an easy-to-implement caching\nmechanism to speed up the learning process. As the first work that addresses\nDFL for automated planning, we demonstrate that the proposed gradient\ncomputation consistently yields significantly better plans than predictions\naimed at minimizing prediction error; and that caching can temper the\ncomputation requirements."
                },
                "authors": [
                    {
                        "name": "Jayanta Mandi"
                    },
                    {
                        "name": "Marco Foschini"
                    },
                    {
                        "name": "Daniel Holler"
                    },
                    {
                        "name": "Sylvie Thiebaux"
                    },
                    {
                        "name": "Jorg Hoffmann"
                    },
                    {
                        "name": "Tias Guns"
                    }
                ],
                "author_detail": {
                    "name": "Tias Guns"
                },
                "author": "Tias Guns",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.06876v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.06876v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2402.16343v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2402.16343v2",
                "updated": "2024-08-26T07:26:27Z",
                "updated_parsed": [
                    2024,
                    8,
                    26,
                    7,
                    26,
                    27,
                    0,
                    239,
                    0
                ],
                "published": "2024-02-26T06:55:36Z",
                "published_parsed": [
                    2024,
                    2,
                    26,
                    6,
                    55,
                    36,
                    0,
                    57,
                    0
                ],
                "title": "Trimma: Trimming Metadata Storage and Latency for Hybrid Memory Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Trimma: Trimming Metadata Storage and Latency for Hybrid Memory Systems"
                },
                "summary": "Hybrid main memory systems combine both performance and capacity advantages\nfrom heterogeneous memory technologies. With larger capacities, higher\nassociativities, and finer granularities, hybrid memory systems currently\nexhibit significant metadata storage and lookup overheads for flexibly\nremapping data blocks between the two memory tiers. To alleviate the\ninefficiencies of existing designs, we propose Trimma, the combination of a\nmulti-level metadata structure and an efficient metadata cache design. Trimma\nuses a multi-level metadata table to only track truly necessary address remap\nentries. The saved memory space is effectively utilized as extra DRAM cache\ncapacity to improve performance. Trimma also uses separate formats to store the\nentries with non-identity and identity address mappings. This improves the\noverall remap cache hit rate, further boosting the performance. Trimma is\ntransparent to software and compatible with various types of hybrid memory\nsystems. When evaluated on a representative hybrid memory system with HBM3 and\nDDR5, Trimma achieves up to 1.68$\\times$ and on average 1.33$\\times$ speedup\nbenefits, compared to state-of-the-art hybrid memory designs. These results\nshow that Trimma effectively addresses metadata management overheads,\nespecially for future scalable large-scale hybrid memory architectures.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Hybrid main memory systems combine both performance and capacity advantages\nfrom heterogeneous memory technologies. With larger capacities, higher\nassociativities, and finer granularities, hybrid memory systems currently\nexhibit significant metadata storage and lookup overheads for flexibly\nremapping data blocks between the two memory tiers. To alleviate the\ninefficiencies of existing designs, we propose Trimma, the combination of a\nmulti-level metadata structure and an efficient metadata cache design. Trimma\nuses a multi-level metadata table to only track truly necessary address remap\nentries. The saved memory space is effectively utilized as extra DRAM cache\ncapacity to improve performance. Trimma also uses separate formats to store the\nentries with non-identity and identity address mappings. This improves the\noverall remap cache hit rate, further boosting the performance. Trimma is\ntransparent to software and compatible with various types of hybrid memory\nsystems. When evaluated on a representative hybrid memory system with HBM3 and\nDDR5, Trimma achieves up to 1.68$\\times$ and on average 1.33$\\times$ speedup\nbenefits, compared to state-of-the-art hybrid memory designs. These results\nshow that Trimma effectively addresses metadata management overheads,\nespecially for future scalable large-scale hybrid memory architectures."
                },
                "authors": [
                    {
                        "name": "Yiwei Li"
                    },
                    {
                        "name": "Boyu Tian"
                    },
                    {
                        "name": "Mingyu Gao"
                    }
                ],
                "author_detail": {
                    "name": "Mingyu Gao"
                },
                "author": "Mingyu Gao",
                "arxiv_comment": "Accepted by PACT 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2402.16343v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2402.16343v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.08795v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.08795v2",
                "updated": "2024-08-26T04:32:56Z",
                "updated_parsed": [
                    2024,
                    8,
                    26,
                    4,
                    32,
                    56,
                    0,
                    239,
                    0
                ],
                "published": "2024-08-16T15:11:12Z",
                "published_parsed": [
                    2024,
                    8,
                    16,
                    15,
                    11,
                    12,
                    4,
                    229,
                    0
                ],
                "title": "RollingCache: Using Runtime Behavior to Defend Against Cache Side\n  Channel Attacks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RollingCache: Using Runtime Behavior to Defend Against Cache Side\n  Channel Attacks"
                },
                "summary": "Shared caches are vulnerable to side channel attacks through contention in\ncache sets. Besides being a simple source of information leak, these side\nchannels form useful gadgets for more sophisticated attacks that compromise the\nsecurity of shared systems.\n  The fundamental design aspect that contention attacks exploit is the\ndeterministic nature of the set of addresses contending for a cache set. In\nthis paper, we present RollingCache, a cache design that defends against\ncontention attacks by dynamically changing the set of addresses contending for\ncache sets. Unlike prior defenses, RollingCache does not rely on address\nencryption/decryption, data relocation, or cache partitioning. We use one level\nof indirection to implement dynamic mapping controlled by the whole-cache\nruntime behavior. Our solution does not depend on having defined security\ndomains, and can defend against an attacker running on the same or another\ncore.\n  We evaluate RollingCache on ChampSim using the SPEC-2017 benchmark suite. Our\nsecurity evaluation shows that our dynamic mapping removes the deterministic\nability to identify the source of contention. The performance evaluation shows\nan impact of 1.67\\% over a mix of workloads, with a corresponding",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Shared caches are vulnerable to side channel attacks through contention in\ncache sets. Besides being a simple source of information leak, these side\nchannels form useful gadgets for more sophisticated attacks that compromise the\nsecurity of shared systems.\n  The fundamental design aspect that contention attacks exploit is the\ndeterministic nature of the set of addresses contending for a cache set. In\nthis paper, we present RollingCache, a cache design that defends against\ncontention attacks by dynamically changing the set of addresses contending for\ncache sets. Unlike prior defenses, RollingCache does not rely on address\nencryption/decryption, data relocation, or cache partitioning. We use one level\nof indirection to implement dynamic mapping controlled by the whole-cache\nruntime behavior. Our solution does not depend on having defined security\ndomains, and can defend against an attacker running on the same or another\ncore.\n  We evaluate RollingCache on ChampSim using the SPEC-2017 benchmark suite. Our\nsecurity evaluation shows that our dynamic mapping removes the deterministic\nability to identify the source of contention. The performance evaluation shows\nan impact of 1.67\\% over a mix of workloads, with a corresponding"
                },
                "authors": [
                    {
                        "name": "Divya Ojha"
                    },
                    {
                        "name": "Sandhya Dwarkadas"
                    }
                ],
                "author_detail": {
                    "name": "Sandhya Dwarkadas"
                },
                "author": "Sandhya Dwarkadas",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.08795v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.08795v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.14001v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.14001v1",
                "updated": "2024-08-26T03:58:20Z",
                "updated_parsed": [
                    2024,
                    8,
                    26,
                    3,
                    58,
                    20,
                    0,
                    239,
                    0
                ],
                "published": "2024-08-26T03:58:20Z",
                "published_parsed": [
                    2024,
                    8,
                    26,
                    3,
                    58,
                    20,
                    0,
                    239,
                    0
                ],
                "title": "Decentralized Federated Learning with Model Caching on Mobile Agents",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Decentralized Federated Learning with Model Caching on Mobile Agents"
                },
                "summary": "Federated Learning (FL) aims to train a shared model using data and\ncomputation power on distributed agents coordinated by a central server.\nDecentralized FL (DFL) utilizes local model exchange and aggregation between\nagents to reduce the communication and computation overheads on the central\nserver. However, when agents are mobile, the communication opportunity between\nagents can be sporadic, largely hindering the convergence and accuracy of DFL.\nIn this paper, we study delay-tolerant model spreading and aggregation enabled\nby model caching on mobile agents. Each agent stores not only its own model,\nbut also models of agents encountered in the recent past. When two agents meet,\nthey exchange their own models as well as the cached models. Local model\naggregation works on all models in the cache. We theoretically analyze the\nconvergence of DFL with cached models, explicitly taking into account the model\nstaleness introduced by caching. We design and compare different model caching\nalgorithms for different DFL and mobility scenarios. We conduct detailed case\nstudies in a vehicular network to systematically investigate the interplay\nbetween agent mobility, cache staleness, and model convergence. In our\nexperiments, cached DFL converges quickly, and significantly outperforms DFL\nwithout caching.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Federated Learning (FL) aims to train a shared model using data and\ncomputation power on distributed agents coordinated by a central server.\nDecentralized FL (DFL) utilizes local model exchange and aggregation between\nagents to reduce the communication and computation overheads on the central\nserver. However, when agents are mobile, the communication opportunity between\nagents can be sporadic, largely hindering the convergence and accuracy of DFL.\nIn this paper, we study delay-tolerant model spreading and aggregation enabled\nby model caching on mobile agents. Each agent stores not only its own model,\nbut also models of agents encountered in the recent past. When two agents meet,\nthey exchange their own models as well as the cached models. Local model\naggregation works on all models in the cache. We theoretically analyze the\nconvergence of DFL with cached models, explicitly taking into account the model\nstaleness introduced by caching. We design and compare different model caching\nalgorithms for different DFL and mobility scenarios. We conduct detailed case\nstudies in a vehicular network to systematically investigate the interplay\nbetween agent mobility, cache staleness, and model convergence. In our\nexperiments, cached DFL converges quickly, and significantly outperforms DFL\nwithout caching."
                },
                "authors": [
                    {
                        "name": "Xiaoyu Wang"
                    },
                    {
                        "name": "Guojun Xiong"
                    },
                    {
                        "name": "Houwei Cao"
                    },
                    {
                        "name": "Jian Li"
                    },
                    {
                        "name": "Yong Liu"
                    }
                ],
                "author_detail": {
                    "name": "Yong Liu"
                },
                "author": "Yong Liu",
                "arxiv_comment": "27 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.14001v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.14001v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.13605v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.13605v1",
                "updated": "2024-08-24T15:23:32Z",
                "updated_parsed": [
                    2024,
                    8,
                    24,
                    15,
                    23,
                    32,
                    5,
                    237,
                    0
                ],
                "published": "2024-08-24T15:23:32Z",
                "published_parsed": [
                    2024,
                    8,
                    24,
                    15,
                    23,
                    32,
                    5,
                    237,
                    0
                ],
                "title": "Mobile Edge Computing Networks: Online Low-Latency and Fresh Service\n  Provisioning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mobile Edge Computing Networks: Online Low-Latency and Fresh Service\n  Provisioning"
                },
                "summary": "Edge service caching can significantly mitigate latency and reduce\ncommunication and computing overhead by fetching and initializing services\n(applications) from clouds. The freshness of cached service data is critical\nwhen providing satisfactory services to users, but has been overlooked in\nexisting research efforts. In this paper, we study the online low-latency and\nfresh service provisioning in mobile edge computing (MEC) networks.\nSpecifically, we jointly optimize the service caching, task offloading, and\nresource allocation without knowledge of future system information, which is\nformulated as a joint online long-term optimization problem. This problem is\nNP-hard. To solve the problem, we design a Lyapunov-based online framework that\ndecouples the problem at temporal level into a series of per-time-slot\nsubproblems. For each subproblem, we propose an online integrated\noptimization-deep reinforcement learning (OIODRL) method, which contains an\noptimization stage including a quadratically constrained quadratic program\n(QCQP) transformation and a semidefinite relaxation (SDR) method, and a\nlearning stage including a deep reinforcement learning (DRL) algorithm.\nExtensive simulations show that the proposed OIODRL method achieves a\nnear-optimal solution and outperforms other benchmark methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Edge service caching can significantly mitigate latency and reduce\ncommunication and computing overhead by fetching and initializing services\n(applications) from clouds. The freshness of cached service data is critical\nwhen providing satisfactory services to users, but has been overlooked in\nexisting research efforts. In this paper, we study the online low-latency and\nfresh service provisioning in mobile edge computing (MEC) networks.\nSpecifically, we jointly optimize the service caching, task offloading, and\nresource allocation without knowledge of future system information, which is\nformulated as a joint online long-term optimization problem. This problem is\nNP-hard. To solve the problem, we design a Lyapunov-based online framework that\ndecouples the problem at temporal level into a series of per-time-slot\nsubproblems. For each subproblem, we propose an online integrated\noptimization-deep reinforcement learning (OIODRL) method, which contains an\noptimization stage including a quadratically constrained quadratic program\n(QCQP) transformation and a semidefinite relaxation (SDR) method, and a\nlearning stage including a deep reinforcement learning (DRL) algorithm.\nExtensive simulations show that the proposed OIODRL method achieves a\nnear-optimal solution and outperforms other benchmark methods."
                },
                "authors": [
                    {
                        "name": "Yuhan Yi"
                    },
                    {
                        "name": "Guanglin Zhang"
                    },
                    {
                        "name": "Hai Jiang"
                    }
                ],
                "author_detail": {
                    "name": "Hai Jiang"
                },
                "author": "Hai Jiang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.13605v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.13605v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IT",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.11049v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.11049v3",
                "updated": "2024-08-23T17:54:34Z",
                "updated_parsed": [
                    2024,
                    8,
                    23,
                    17,
                    54,
                    34,
                    4,
                    236,
                    0
                ],
                "published": "2024-08-20T17:57:31Z",
                "published_parsed": [
                    2024,
                    8,
                    20,
                    17,
                    57,
                    31,
                    1,
                    233,
                    0
                ],
                "title": "MagicDec: Breaking the Latency-Throughput Tradeoff for Long Context\n  Generation with Speculative Decoding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MagicDec: Breaking the Latency-Throughput Tradeoff for Long Context\n  Generation with Speculative Decoding"
                },
                "summary": "Large Language Models (LLMs) have become more prevalent in long-context\napplications such as interactive chatbots, document analysis, and agent\nworkflows, but it is challenging to serve long-context requests with low\nlatency and high throughput. Speculative decoding (SD) is a widely used\ntechnique to reduce latency without sacrificing performance but the\nconventional wisdom suggests that its efficacy is limited to small batch sizes.\nIn MagicDec, we show that surprisingly SD can achieve speedup even for a high\nthroughput inference regime for moderate to long sequences. More interestingly,\nan intelligent drafting strategy can achieve better speedup with increasing\nbatch size based on our rigorous analysis. MagicDec first identifies the\nbottleneck shifts with increasing batch size and sequence length, and uses\nthese insights to deploy speculative decoding more effectively for high\nthroughput inference. Then, it leverages draft models with sparse KV cache to\naddress the KV bottleneck that scales with both sequence length and batch size.\nThis finding underscores the broad applicability of speculative decoding in\nlong-context serving, as it can enhance throughput and reduce latency without\ncompromising accuracy. For moderate to long sequences, we demonstrate up to 2x\nspeedup for LLaMA-2-7B-32K and 1.84x speedup for LLaMA-3.1-8B when serving\nbatch sizes ranging from 32 to 256 on 8 NVIDIA A100 GPUs. The code is available\nat https://github.com/Infini-AI-Lab/MagicDec/.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have become more prevalent in long-context\napplications such as interactive chatbots, document analysis, and agent\nworkflows, but it is challenging to serve long-context requests with low\nlatency and high throughput. Speculative decoding (SD) is a widely used\ntechnique to reduce latency without sacrificing performance but the\nconventional wisdom suggests that its efficacy is limited to small batch sizes.\nIn MagicDec, we show that surprisingly SD can achieve speedup even for a high\nthroughput inference regime for moderate to long sequences. More interestingly,\nan intelligent drafting strategy can achieve better speedup with increasing\nbatch size based on our rigorous analysis. MagicDec first identifies the\nbottleneck shifts with increasing batch size and sequence length, and uses\nthese insights to deploy speculative decoding more effectively for high\nthroughput inference. Then, it leverages draft models with sparse KV cache to\naddress the KV bottleneck that scales with both sequence length and batch size.\nThis finding underscores the broad applicability of speculative decoding in\nlong-context serving, as it can enhance throughput and reduce latency without\ncompromising accuracy. For moderate to long sequences, we demonstrate up to 2x\nspeedup for LLaMA-2-7B-32K and 1.84x speedup for LLaMA-3.1-8B when serving\nbatch sizes ranging from 32 to 256 on 8 NVIDIA A100 GPUs. The code is available\nat https://github.com/Infini-AI-Lab/MagicDec/."
                },
                "authors": [
                    {
                        "name": "Jian Chen"
                    },
                    {
                        "name": "Vashisth Tiwari"
                    },
                    {
                        "name": "Ranajoy Sadhukhan"
                    },
                    {
                        "name": "Zhuoming Chen"
                    },
                    {
                        "name": "Jinyuan Shi"
                    },
                    {
                        "name": "Ian En-Hsu Yen"
                    },
                    {
                        "name": "Beidi Chen"
                    }
                ],
                "author_detail": {
                    "name": "Beidi Chen"
                },
                "author": "Beidi Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.11049v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.11049v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.13165v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.13165v1",
                "updated": "2024-08-23T15:39:20Z",
                "updated_parsed": [
                    2024,
                    8,
                    23,
                    15,
                    39,
                    20,
                    4,
                    236,
                    0
                ],
                "published": "2024-08-23T15:39:20Z",
                "published_parsed": [
                    2024,
                    8,
                    23,
                    15,
                    39,
                    20,
                    4,
                    236,
                    0
                ],
                "title": "Cyclic Wrap-Around Multi-Access Coded Caching with Private Caches",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cyclic Wrap-Around Multi-Access Coded Caching with Private Caches"
                },
                "summary": "We consider a variant of the coded caching problem where users connect to two\ntypes of caches, called private caches and access caches. The problem setting\nconsists of a server having a library of files and a set of access caches.\nEvery user, equipped with a private cache, connects to $L$ neighboring access\ncaches in a cyclic wrap-around fashion. The server populates the private and\naccess caches with file contents in either coded or uncoded format. For this\nsetting, we derive a lower bound on the optimal worst-case transmission rate\nusing cut-set arguments. This lower bound applies to both coded and uncoded\nplacements. We then provide an achievable scheme with uncoded placement and\nshow that our scheme specializes to the well-known Maddah-Ali-Niesen scheme for\nthe dedicated cache network in the absence of access caches. Finally, we show\nthat the proposed scheme achieves optimality in large memory regimes and\nprovide numerical plots comparing the rate of the proposed scheme with the\nderived lower bound, demonstrating the optimality of our scheme.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We consider a variant of the coded caching problem where users connect to two\ntypes of caches, called private caches and access caches. The problem setting\nconsists of a server having a library of files and a set of access caches.\nEvery user, equipped with a private cache, connects to $L$ neighboring access\ncaches in a cyclic wrap-around fashion. The server populates the private and\naccess caches with file contents in either coded or uncoded format. For this\nsetting, we derive a lower bound on the optimal worst-case transmission rate\nusing cut-set arguments. This lower bound applies to both coded and uncoded\nplacements. We then provide an achievable scheme with uncoded placement and\nshow that our scheme specializes to the well-known Maddah-Ali-Niesen scheme for\nthe dedicated cache network in the absence of access caches. Finally, we show\nthat the proposed scheme achieves optimality in large memory regimes and\nprovide numerical plots comparing the rate of the proposed scheme with the\nderived lower bound, demonstrating the optimality of our scheme."
                },
                "authors": [
                    {
                        "name": "Dhruv Pratap Singh"
                    },
                    {
                        "name": "Anjana A. Mahesh"
                    },
                    {
                        "name": "B. Sundar Rajan"
                    }
                ],
                "author_detail": {
                    "name": "B. Sundar Rajan"
                },
                "author": "B. Sundar Rajan",
                "arxiv_comment": "15 pages, 5 figures and one table. Some overlap of introductory and\n  background materials with our earlier submission arXiv:2407.00677v1 dated 30\n  June 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.13165v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.13165v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IT",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2305.05332v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2305.05332v5",
                "updated": "2024-08-23T13:25:07Z",
                "updated_parsed": [
                    2024,
                    8,
                    23,
                    13,
                    25,
                    7,
                    4,
                    236,
                    0
                ],
                "published": "2023-05-09T10:41:36Z",
                "published_parsed": [
                    2023,
                    5,
                    9,
                    10,
                    41,
                    36,
                    1,
                    129,
                    0
                ],
                "title": "Fundamental Limits of Multi-Message Private Computation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Fundamental Limits of Multi-Message Private Computation"
                },
                "summary": "In a typical formulation of the private information retrieval (PIR) problem,\na single user wishes to retrieve one out of $ K$ files from $N$ servers without\nrevealing the demanded file index to any server. This paper formulates an\nextended model of PIR, referred to as multi-message private computation\n(MM-PC), where instead of retrieving a single file, the user wishes to retrieve\n$P>1$ linear combinations of files while preserving the privacy of the demand\ninformation. The MM-PC problem is a generalization of the private computation\n(PC) problem (where the user requests one linear combination of the files), and\nthe multi-message private information retrieval (MM-PIR) problem (where the\nuser requests $P>1$ files). A baseline achievable scheme repeats the optimal PC\nscheme by Sun and Jafar $P$ times, or treats each possible demanded linear\ncombination as an independent file and then uses the near optimal MM-PIR scheme\nby Banawan and Ulukus. In this paper, we propose a new MM-PC scheme that\nsignificantly improves upon the baseline schemes. In doing so, we design the\nqueries inspired by the structure in the cache-aided scalar linear function\nretrieval scheme by Wan {\\it et al.}, which leverages the dependency between\nlinear functions to reduce the amount of communications. To ensure the\ndecodability of our scheme, we propose a new method to benefit from the\nexisting dependency, referred to as the sign assignment step. In the end, we\nuse Maximum Distance Separable matrices to code the queries, which allows the\nreduction of download from the servers, while preserving privacy. By the\nproposed schemes, we characterize the capacity within a multiplicative factor\nof $2$.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In a typical formulation of the private information retrieval (PIR) problem,\na single user wishes to retrieve one out of $ K$ files from $N$ servers without\nrevealing the demanded file index to any server. This paper formulates an\nextended model of PIR, referred to as multi-message private computation\n(MM-PC), where instead of retrieving a single file, the user wishes to retrieve\n$P>1$ linear combinations of files while preserving the privacy of the demand\ninformation. The MM-PC problem is a generalization of the private computation\n(PC) problem (where the user requests one linear combination of the files), and\nthe multi-message private information retrieval (MM-PIR) problem (where the\nuser requests $P>1$ files). A baseline achievable scheme repeats the optimal PC\nscheme by Sun and Jafar $P$ times, or treats each possible demanded linear\ncombination as an independent file and then uses the near optimal MM-PIR scheme\nby Banawan and Ulukus. In this paper, we propose a new MM-PC scheme that\nsignificantly improves upon the baseline schemes. In doing so, we design the\nqueries inspired by the structure in the cache-aided scalar linear function\nretrieval scheme by Wan {\\it et al.}, which leverages the dependency between\nlinear functions to reduce the amount of communications. To ensure the\ndecodability of our scheme, we propose a new method to benefit from the\nexisting dependency, referred to as the sign assignment step. In the end, we\nuse Maximum Distance Separable matrices to code the queries, which allows the\nreduction of download from the servers, while preserving privacy. By the\nproposed schemes, we characterize the capacity within a multiplicative factor\nof $2$."
                },
                "authors": [
                    {
                        "name": "Ali Gholami"
                    },
                    {
                        "name": "Kai Wan"
                    },
                    {
                        "name": "Tayyebeh Jahani-Nezhad"
                    },
                    {
                        "name": "Hua Sun"
                    },
                    {
                        "name": "Mingyue Ji"
                    },
                    {
                        "name": "Giuseppe Caire"
                    }
                ],
                "author_detail": {
                    "name": "Giuseppe Caire"
                },
                "author": "Giuseppe Caire",
                "arxiv_comment": "A version of this paper is submitted to IEEE Transactions on\n  Communications. A short version was accepted and presented at ISIT 2024 in\n  Athens",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2305.05332v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2305.05332v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IT",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.12947v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.12947v1",
                "updated": "2024-08-23T09:54:22Z",
                "updated_parsed": [
                    2024,
                    8,
                    23,
                    9,
                    54,
                    22,
                    4,
                    236,
                    0
                ],
                "published": "2024-08-23T09:54:22Z",
                "published_parsed": [
                    2024,
                    8,
                    23,
                    9,
                    54,
                    22,
                    4,
                    236,
                    0
                ],
                "title": "Which Part of the Heap is Useful? Improving Heap Liveness Analysis",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Which Part of the Heap is Useful? Improving Heap Liveness Analysis"
                },
                "summary": "With the growing sizes of data structures allocated in heap, understanding\nthe actual use of heap memory is critically important for minimizing cache\nmisses and reclaiming unused memory. A static analysis aimed at this is\ndifficult because the heap locations are unnamed. Using allocation sites to\nname them creates very few distinctions making it difficult to identify\nallocated heap locations that are not used. Heap liveness analysis using access\ngraphs solves this problem by (a) using a storeless model of heap memory by\nnaming the locations with access paths, and (b) representing the unbounded sets\nof access paths (which are regular languages) as finite automata.\n  We improve the scalability and efficiency of heap liveness analysis, and\nreduce the amount of computed heap liveness information by using deterministic\nautomata and by minimizing the inclusion of aliased access paths in the\nlanguage. Practically, our field-, flow-, context-sensitive liveness analysis\non SPEC CPU2006 benchmarks scales to 36 kLoC (existing analysis scales to 10.5\nkLoC) and improves efficiency even up to 99%. For some of the benchmarks, our\ntechnique shows multifold reduction in the computed liveness information,\nranging from 2 to 100 times (in terms of the number of live access paths),\nwithout compromising on soundness.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the growing sizes of data structures allocated in heap, understanding\nthe actual use of heap memory is critically important for minimizing cache\nmisses and reclaiming unused memory. A static analysis aimed at this is\ndifficult because the heap locations are unnamed. Using allocation sites to\nname them creates very few distinctions making it difficult to identify\nallocated heap locations that are not used. Heap liveness analysis using access\ngraphs solves this problem by (a) using a storeless model of heap memory by\nnaming the locations with access paths, and (b) representing the unbounded sets\nof access paths (which are regular languages) as finite automata.\n  We improve the scalability and efficiency of heap liveness analysis, and\nreduce the amount of computed heap liveness information by using deterministic\nautomata and by minimizing the inclusion of aliased access paths in the\nlanguage. Practically, our field-, flow-, context-sensitive liveness analysis\non SPEC CPU2006 benchmarks scales to 36 kLoC (existing analysis scales to 10.5\nkLoC) and improves efficiency even up to 99%. For some of the benchmarks, our\ntechnique shows multifold reduction in the computed liveness information,\nranging from 2 to 100 times (in terms of the number of live access paths),\nwithout compromising on soundness."
                },
                "authors": [
                    {
                        "name": "Vini Kanvar"
                    },
                    {
                        "name": "Uday P. Khedker"
                    }
                ],
                "author_detail": {
                    "name": "Uday P. Khedker"
                },
                "author": "Uday P. Khedker",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.12947v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.12947v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.PL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.PL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.12592v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.12592v1",
                "updated": "2024-08-22T17:56:29Z",
                "updated_parsed": [
                    2024,
                    8,
                    22,
                    17,
                    56,
                    29,
                    3,
                    235,
                    0
                ],
                "published": "2024-08-22T17:56:29Z",
                "published_parsed": [
                    2024,
                    8,
                    22,
                    17,
                    56,
                    29,
                    3,
                    235,
                    0
                ],
                "title": "Exposing Shadow Branches",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Exposing Shadow Branches"
                },
                "summary": "Modern processors implement a decoupled front-end in the form of Fetch\nDirected Instruction Prefetching (FDIP) to avoid front-end stalls. FDIP is\ndriven by the Branch Prediction Unit (BPU), relying on the BPU's accuracy and\nbranch target tracking structures to speculatively fetch instructions into the\nInstruction Cache (L1I). As data center applications become more complex, their\ncode footprints also grow, resulting in an increase in Branch Target Buffer\n(BTB) misses. FDIP can alleviate L1I cache misses, but when it encounters a BTB\nmiss, the BPU may not identify the current instruction as a branch to FDIP.\nThis can prevent FDIP from prefetching or cause it to speculate down the wrong\npath, further polluting the L1I cache. We observe that the vast majority, 75%,\nof BTB-missing, unidentified branches are actually present in instruction cache\nlines that FDIP has previously fetched but, these missing branches have not yet\nbeen decoded and inserted into the BTB. This is because the instruction line is\ndecoded from an entry point (which is the target of the previous taken branch)\ntill an exit point (the taken branch). Branch instructions present in the\nignored portion of the cache line we call them \"Shadow Branches\". Here we\npresent Skeia, a novel shadow branch decoding technique that identifies and\ndecodes unused bytes in cache lines fetched by FDIP, inserting them into a\nShadow Branch Buffer (SBB). The SBB is accessed in parallel with the BTB,\nallowing FDIP to speculate despite a BTB miss. With a minimal storage state of\n12.25KB, Skeia delivers a geomean speedup of ~5.7% over an 8K-entry BTB (78KB)\nand ~2% versus adding an equal amount of state to the BTB across 16 front-end\nbound applications. Since many branches stored in the SBB are unique compared\nto those in a similarly sized BTB, we consistently observe greater performance\ngains with Skeia across all examined sizes until saturation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Modern processors implement a decoupled front-end in the form of Fetch\nDirected Instruction Prefetching (FDIP) to avoid front-end stalls. FDIP is\ndriven by the Branch Prediction Unit (BPU), relying on the BPU's accuracy and\nbranch target tracking structures to speculatively fetch instructions into the\nInstruction Cache (L1I). As data center applications become more complex, their\ncode footprints also grow, resulting in an increase in Branch Target Buffer\n(BTB) misses. FDIP can alleviate L1I cache misses, but when it encounters a BTB\nmiss, the BPU may not identify the current instruction as a branch to FDIP.\nThis can prevent FDIP from prefetching or cause it to speculate down the wrong\npath, further polluting the L1I cache. We observe that the vast majority, 75%,\nof BTB-missing, unidentified branches are actually present in instruction cache\nlines that FDIP has previously fetched but, these missing branches have not yet\nbeen decoded and inserted into the BTB. This is because the instruction line is\ndecoded from an entry point (which is the target of the previous taken branch)\ntill an exit point (the taken branch). Branch instructions present in the\nignored portion of the cache line we call them \"Shadow Branches\". Here we\npresent Skeia, a novel shadow branch decoding technique that identifies and\ndecodes unused bytes in cache lines fetched by FDIP, inserting them into a\nShadow Branch Buffer (SBB). The SBB is accessed in parallel with the BTB,\nallowing FDIP to speculate despite a BTB miss. With a minimal storage state of\n12.25KB, Skeia delivers a geomean speedup of ~5.7% over an 8K-entry BTB (78KB)\nand ~2% versus adding an equal amount of state to the BTB across 16 front-end\nbound applications. Since many branches stored in the SBB are unique compared\nto those in a similarly sized BTB, we consistently observe greater performance\ngains with Skeia across all examined sizes until saturation."
                },
                "authors": [
                    {
                        "name": "Chrysanthos Pepi"
                    },
                    {
                        "name": "Bhargav Reddy Godala"
                    },
                    {
                        "name": "Krishnam Tibrewala"
                    },
                    {
                        "name": "Gino Chacon"
                    },
                    {
                        "name": "Paul V. Gratz"
                    },
                    {
                        "name": "Daniel A. Jiménez"
                    },
                    {
                        "name": "Gilles A. Pokam"
                    },
                    {
                        "name": "David I. August"
                    }
                ],
                "author_detail": {
                    "name": "David I. August"
                },
                "author": "David I. August",
                "arxiv_comment": "13 pages, 16 figures, Submitted to ASPLOS 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.12592v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.12592v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2309.14533v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2309.14533v2",
                "updated": "2024-08-22T17:47:49Z",
                "updated_parsed": [
                    2024,
                    8,
                    22,
                    17,
                    47,
                    49,
                    3,
                    235,
                    0
                ],
                "published": "2023-09-25T21:17:17Z",
                "published_parsed": [
                    2023,
                    9,
                    25,
                    21,
                    17,
                    17,
                    0,
                    268,
                    0
                ],
                "title": "Stable CoO$_2$ Nanoscrolls With Outstanding Electrical Properties",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Stable CoO$_2$ Nanoscrolls With Outstanding Electrical Properties"
                },
                "summary": "Layered CoO$_2$ is of great interest for its promising properties but is\nmeta-stable in its bulk form. CoO$_2$ was synthesized by converting the\nquasi-one-dimensional crystal structure of bulk Ca$_3$Co$_2$O$_6$ via a\nhydrothermal treatment. The resulting nanostructures were predominantly\nnanoscrolls with very thin walls, which exhibit long-term stability. A detailed\nstructural investigation reveals that the CoO$_2$ is found to crystallize in\nmonoclinic form, similar to the related CaCoO$_2$-CoO$_2$ misfit structure.\nIndividual nanoscrolls are characterized electrically and show a p-type\nsemiconducting nature with a high current-carrying capacity of 4$\\cdot$10$^5$ A\ncm$^{-2}$ and an extremely high breakdown voltage of up to 270 kV/cm. The\nresults demonstrate the possibility to stabilize meta-stable materials in\nlow-dimensional forms and a promising application of the nanoscrolls as\ninterconnect in high-voltage electronic circuitry.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Layered CoO$_2$ is of great interest for its promising properties but is\nmeta-stable in its bulk form. CoO$_2$ was synthesized by converting the\nquasi-one-dimensional crystal structure of bulk Ca$_3$Co$_2$O$_6$ via a\nhydrothermal treatment. The resulting nanostructures were predominantly\nnanoscrolls with very thin walls, which exhibit long-term stability. A detailed\nstructural investigation reveals that the CoO$_2$ is found to crystallize in\nmonoclinic form, similar to the related CaCoO$_2$-CoO$_2$ misfit structure.\nIndividual nanoscrolls are characterized electrically and show a p-type\nsemiconducting nature with a high current-carrying capacity of 4$\\cdot$10$^5$ A\ncm$^{-2}$ and an extremely high breakdown voltage of up to 270 kV/cm. The\nresults demonstrate the possibility to stabilize meta-stable materials in\nlow-dimensional forms and a promising application of the nanoscrolls as\ninterconnect in high-voltage electronic circuitry."
                },
                "authors": [
                    {
                        "name": "Simon Hettler"
                    },
                    {
                        "name": "Kankona Singha Roy"
                    },
                    {
                        "name": "Raul Arenal"
                    },
                    {
                        "name": "Leela S. Panchakarla"
                    }
                ],
                "author_detail": {
                    "name": "Leela S. Panchakarla"
                },
                "author": "Leela S. Panchakarla",
                "arxiv_doi": "10.1002/admi.202400317",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1002/admi.202400317",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2309.14533v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2309.14533v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_journal_ref": "Adv. Mater. Interfaces 2024, 2400317",
                "arxiv_primary_category": {
                    "term": "cond-mat.mtrl-sci",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cond-mat.mtrl-sci",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.11506v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.11506v1",
                "updated": "2024-08-21T10:26:26Z",
                "updated_parsed": [
                    2024,
                    8,
                    21,
                    10,
                    26,
                    26,
                    2,
                    234,
                    0
                ],
                "published": "2024-08-21T10:26:26Z",
                "published_parsed": [
                    2024,
                    8,
                    21,
                    10,
                    26,
                    26,
                    2,
                    234,
                    0
                ],
                "title": "Rheological behavior of molybdenum disulfide (MoS2) inks under electric\n  fields: influence of concentration and voltage",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Rheological behavior of molybdenum disulfide (MoS2) inks under electric\n  fields: influence of concentration and voltage"
                },
                "summary": "This work provides a complete rheological characterization of molybdenum\ndisulfide (MoS2) inks in the presence of electric fields. Several\nconcentrations of MoS2 are studied and dispersed in a viscoelastic fluid. The\nlubrication effects are present in the ink when the MoS2 concentration is\nhigher than 0.10% w/w. The dielectric properties show the impossibility of a\npositive electrorheological effect for all MoS2-inks studied. The formation of\nvortices and electromigration of MoS2 particles occur under the influence of an\nexternal electric field. These two phenomena affect the rheological behavior of\nMoS2-inks under shear flow condition. Relatively to the extensional rheology\nexperiments, the particle migration and the vortex formation promote anisotropy\non the rheological properties of the inks which affects the relaxation time,\nthe formation of beads-on-a-string and the uniaxial elongational flow condition\nis no longer valid. When the electric field strength is 1.5 kV/mm, the\nformation of Taylor's cone is observed and independent of MoS2 concentration.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This work provides a complete rheological characterization of molybdenum\ndisulfide (MoS2) inks in the presence of electric fields. Several\nconcentrations of MoS2 are studied and dispersed in a viscoelastic fluid. The\nlubrication effects are present in the ink when the MoS2 concentration is\nhigher than 0.10% w/w. The dielectric properties show the impossibility of a\npositive electrorheological effect for all MoS2-inks studied. The formation of\nvortices and electromigration of MoS2 particles occur under the influence of an\nexternal electric field. These two phenomena affect the rheological behavior of\nMoS2-inks under shear flow condition. Relatively to the extensional rheology\nexperiments, the particle migration and the vortex formation promote anisotropy\non the rheological properties of the inks which affects the relaxation time,\nthe formation of beads-on-a-string and the uniaxial elongational flow condition\nis no longer valid. When the electric field strength is 1.5 kV/mm, the\nformation of Taylor's cone is observed and independent of MoS2 concentration."
                },
                "authors": [
                    {
                        "name": "Pedro C Rijo"
                    },
                    {
                        "name": "Francisco J. Galindo-Rosales"
                    }
                ],
                "author_detail": {
                    "name": "Francisco J. Galindo-Rosales"
                },
                "author": "Francisco J. Galindo-Rosales",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.11506v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.11506v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.flu-dyn",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.flu-dyn",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cond-mat.soft",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2401.10685v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2401.10685v2",
                "updated": "2024-08-21T06:10:02Z",
                "updated_parsed": [
                    2024,
                    8,
                    21,
                    6,
                    10,
                    2,
                    2,
                    234,
                    0
                ],
                "published": "2024-01-19T13:32:55Z",
                "published_parsed": [
                    2024,
                    1,
                    19,
                    13,
                    32,
                    55,
                    4,
                    19,
                    0
                ],
                "title": "Towards End-to-End GPS Localization with Neural Pseudorange Correction",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Towards End-to-End GPS Localization with Neural Pseudorange Correction"
                },
                "summary": "The pseudorange error is one of the root causes of localization inaccuracy in\nGPS. Previous data-driven methods regress and eliminate pseudorange errors\nusing handcrafted intermediate labels. Unlike them, we propose an end-to-end\nGPS localization framework, E2E-PrNet, to train a neural network for\npseudorange correction (PrNet) directly using the final task loss calculated\nwith the ground truth of GPS receiver states. The gradients of the loss with\nrespect to learnable parameters are backpropagated through a Differentiable\nNonlinear Least Squares (DNLS) optimizer to PrNet. The feasibility of fusing\nthe data-driven neural network and the model-based DNLS module is verified with\nGPS data collected by Android phones, showing that E2E-PrNet outperforms the\nbaseline weighted least squares method and the state-of-the-art end-to-end\ndata-driven approach. Finally, we discuss the explainability of E2E-PrNet.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The pseudorange error is one of the root causes of localization inaccuracy in\nGPS. Previous data-driven methods regress and eliminate pseudorange errors\nusing handcrafted intermediate labels. Unlike them, we propose an end-to-end\nGPS localization framework, E2E-PrNet, to train a neural network for\npseudorange correction (PrNet) directly using the final task loss calculated\nwith the ground truth of GPS receiver states. The gradients of the loss with\nrespect to learnable parameters are backpropagated through a Differentiable\nNonlinear Least Squares (DNLS) optimizer to PrNet. The feasibility of fusing\nthe data-driven neural network and the model-based DNLS module is verified with\nGPS data collected by Android phones, showing that E2E-PrNet outperforms the\nbaseline weighted least squares method and the state-of-the-art end-to-end\ndata-driven approach. Finally, we discuss the explainability of E2E-PrNet."
                },
                "authors": [
                    {
                        "name": "Xu Weng"
                    },
                    {
                        "name": "KV Ling"
                    },
                    {
                        "name": "Haochen Liu"
                    },
                    {
                        "name": "Kun Cao"
                    }
                ],
                "author_detail": {
                    "name": "Kun Cao"
                },
                "author": "Kun Cao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2401.10685v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2401.10685v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.11325v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.11325v1",
                "updated": "2024-08-21T04:16:49Z",
                "updated_parsed": [
                    2024,
                    8,
                    21,
                    4,
                    16,
                    49,
                    2,
                    234,
                    0
                ],
                "published": "2024-08-21T04:16:49Z",
                "published_parsed": [
                    2024,
                    8,
                    21,
                    4,
                    16,
                    49,
                    2,
                    234,
                    0
                ],
                "title": "Telepathic Datacenters: Fast RPCs using Shared CXL Memory",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Telepathic Datacenters: Fast RPCs using Shared CXL Memory"
                },
                "summary": "Datacenter applications often rely on remote procedure calls (RPCs) for fast,\nefficient, and secure communication. However, RPCs are slow, inefficient, and\nhard to use as they require expensive serialization and compression to\ncommunicate over a packetized serial network link. Compute Express Link 3.0\n(CXL) offers an alternative solution, allowing applications to share data using\na cache-coherent, shared-memory interface across clusters of machines.\n  RPCool is a new framework that exploits CXL's shared memory capabilities.\nRPCool avoids serialization by passing pointers to data structures in shared\nmemory. While avoiding serialization is useful, directly sharing pointer-rich\ndata eliminates the isolation that copying data over traditional networks\nprovides, leaving the receiver vulnerable to invalid pointers and concurrent\nupdates to shared data by the sender. RPCool restores this safety with careful\nand efficient management of memory permissions. Another significant challenge\nwith CXL shared memory capabilities is that they are unlikely to scale to an\nentire datacenter. RPCool addresses this by falling back to RDMA-based\ncommunication.\n  Overall, RPCool reduces the round-trip latency by 1.93$\\times$ and\n7.2$\\times$ compared to state-of-the-art RDMA and CXL-based RPC mechanisms,\nrespectively. Moreover, RPCool performs either comparably or better than other\nRPC mechanisms across a range of workloads.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Datacenter applications often rely on remote procedure calls (RPCs) for fast,\nefficient, and secure communication. However, RPCs are slow, inefficient, and\nhard to use as they require expensive serialization and compression to\ncommunicate over a packetized serial network link. Compute Express Link 3.0\n(CXL) offers an alternative solution, allowing applications to share data using\na cache-coherent, shared-memory interface across clusters of machines.\n  RPCool is a new framework that exploits CXL's shared memory capabilities.\nRPCool avoids serialization by passing pointers to data structures in shared\nmemory. While avoiding serialization is useful, directly sharing pointer-rich\ndata eliminates the isolation that copying data over traditional networks\nprovides, leaving the receiver vulnerable to invalid pointers and concurrent\nupdates to shared data by the sender. RPCool restores this safety with careful\nand efficient management of memory permissions. Another significant challenge\nwith CXL shared memory capabilities is that they are unlikely to scale to an\nentire datacenter. RPCool addresses this by falling back to RDMA-based\ncommunication.\n  Overall, RPCool reduces the round-trip latency by 1.93$\\times$ and\n7.2$\\times$ compared to state-of-the-art RDMA and CXL-based RPC mechanisms,\nrespectively. Moreover, RPCool performs either comparably or better than other\nRPC mechanisms across a range of workloads."
                },
                "authors": [
                    {
                        "name": "Suyash Mahar"
                    },
                    {
                        "name": "Ehsan Hajyjasini"
                    },
                    {
                        "name": "Seungjin Lee"
                    },
                    {
                        "name": "Zifeng Zhang"
                    },
                    {
                        "name": "Mingyao Shen"
                    },
                    {
                        "name": "Steven Swanson"
                    }
                ],
                "author_detail": {
                    "name": "Steven Swanson"
                },
                "author": "Steven Swanson",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.11325v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.11325v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.OS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.10970v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.10970v1",
                "updated": "2024-08-20T16:02:54Z",
                "updated_parsed": [
                    2024,
                    8,
                    20,
                    16,
                    2,
                    54,
                    1,
                    233,
                    0
                ],
                "published": "2024-08-20T16:02:54Z",
                "published_parsed": [
                    2024,
                    8,
                    20,
                    16,
                    2,
                    54,
                    1,
                    233,
                    0
                ],
                "title": "Hybrid Recurrent Models Support Emergent Descriptions for Hierarchical\n  Planning and Control",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Hybrid Recurrent Models Support Emergent Descriptions for Hierarchical\n  Planning and Control"
                },
                "summary": "An open problem in artificial intelligence is how systems can flexibly learn\ndiscrete abstractions that are useful for solving inherently continuous\nproblems. Previous work has demonstrated that a class of hybrid state-space\nmodel known as recurrent switching linear dynamical systems (rSLDS) discover\nmeaningful behavioural units via the piecewise linear decomposition of complex\ncontinuous dynamics (Linderman et al., 2016). Furthermore, they model how the\nunderlying continuous states drive these discrete mode switches. We propose\nthat the rich representations formed by an rSLDS can provide useful\nabstractions for planning and control. We present a novel hierarchical\nmodel-based algorithm inspired by Active Inference in which a discrete MDP sits\nabove a low-level linear-quadratic controller. The recurrent transition\ndynamics learned by the rSLDS allow us to (1) specify temporally-abstracted\nsub-goals in a method reminiscent of the options framework, (2) lift the\nexploration into discrete space allowing us to exploit information-theoretic\nexploration bonuses and (3) `cache' the approximate solutions to low-level\nproblems in the discrete planner. We successfully apply our model to the sparse\nContinuous Mountain Car task, demonstrating fast system identification via\nenhanced exploration and non-trivial planning through the delineation of\nabstract sub-goals.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "An open problem in artificial intelligence is how systems can flexibly learn\ndiscrete abstractions that are useful for solving inherently continuous\nproblems. Previous work has demonstrated that a class of hybrid state-space\nmodel known as recurrent switching linear dynamical systems (rSLDS) discover\nmeaningful behavioural units via the piecewise linear decomposition of complex\ncontinuous dynamics (Linderman et al., 2016). Furthermore, they model how the\nunderlying continuous states drive these discrete mode switches. We propose\nthat the rich representations formed by an rSLDS can provide useful\nabstractions for planning and control. We present a novel hierarchical\nmodel-based algorithm inspired by Active Inference in which a discrete MDP sits\nabove a low-level linear-quadratic controller. The recurrent transition\ndynamics learned by the rSLDS allow us to (1) specify temporally-abstracted\nsub-goals in a method reminiscent of the options framework, (2) lift the\nexploration into discrete space allowing us to exploit information-theoretic\nexploration bonuses and (3) `cache' the approximate solutions to low-level\nproblems in the discrete planner. We successfully apply our model to the sparse\nContinuous Mountain Car task, demonstrating fast system identification via\nenhanced exploration and non-trivial planning through the delineation of\nabstract sub-goals."
                },
                "authors": [
                    {
                        "name": "Poppy Collis"
                    },
                    {
                        "name": "Ryan Singh"
                    },
                    {
                        "name": "Paul F Kinghorn"
                    },
                    {
                        "name": "Christopher L Buckley"
                    }
                ],
                "author_detail": {
                    "name": "Christopher L Buckley"
                },
                "author": "Christopher L Buckley",
                "arxiv_comment": "4 pages, 3 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.10970v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.10970v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.10746v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.10746v1",
                "updated": "2024-08-20T11:30:12Z",
                "updated_parsed": [
                    2024,
                    8,
                    20,
                    11,
                    30,
                    12,
                    1,
                    233,
                    0
                ],
                "published": "2024-08-20T11:30:12Z",
                "published_parsed": [
                    2024,
                    8,
                    20,
                    11,
                    30,
                    12,
                    1,
                    233,
                    0
                ],
                "title": "Pluto and Charon: A Time and Memory Efficient Collaborative Edge AI\n  Framework for Personal LLMs Fine-Tuning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Pluto and Charon: A Time and Memory Efficient Collaborative Edge AI\n  Framework for Personal LLMs Fine-Tuning"
                },
                "summary": "Large language models (LLMs) have unlocked a plethora of powerful\napplications at the network edge, such as intelligent personal assistants. Data\nprivacy and security concerns have prompted a shift towards edge-based\nfine-tuning of personal LLMs, away from cloud reliance. However, this raises\nissues of computational intensity and resource scarcity, hindering training\nefficiency and feasibility. While current studies investigate\nparameter-efficient fine-tuning (PEFT) techniques to mitigate resource\nconstraints, our analysis indicates that these techniques are not sufficiently\nresource-efficient for edge devices. To tackle these challenges, we propose\nPluto and Charon (PAC), a time and memory efficient collaborative edge AI\nframework for personal LLMs fine-tuning. PAC breaks the resource wall of\npersonal LLMs fine-tuning with a sophisticated algorithm-system co-design. (1)\nAlgorithmically, PAC implements a personal LLMs fine-tuning technique that is\nefficient in terms of parameters, time, and memory. It utilizes Parallel\nAdapters to circumvent the need for a full backward pass through the LLM\nbackbone. Additionally, an activation cache mechanism further streamlining the\nprocess by negating the necessity for repeated forward passes across multiple\nepochs. (2) Systematically, PAC leverages edge devices in close proximity,\npooling them as a collective resource for in-situ personal LLMs fine-tuning,\nutilizing a hybrid data and pipeline parallelism to orchestrate distributed\ntraining. The use of the activation cache eliminates the need for forward pass\nthrough the LLM backbone,enabling exclusive fine-tuning of the Parallel\nAdapters using data parallelism. Extensive evaluation based on prototype\nimplementation demonstrates that PAC remarkably outperforms state-of-the-art\napproaches, achieving up to 8.64x end-to-end speedup and up to 88.16% reduction\nin memory footprint.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have unlocked a plethora of powerful\napplications at the network edge, such as intelligent personal assistants. Data\nprivacy and security concerns have prompted a shift towards edge-based\nfine-tuning of personal LLMs, away from cloud reliance. However, this raises\nissues of computational intensity and resource scarcity, hindering training\nefficiency and feasibility. While current studies investigate\nparameter-efficient fine-tuning (PEFT) techniques to mitigate resource\nconstraints, our analysis indicates that these techniques are not sufficiently\nresource-efficient for edge devices. To tackle these challenges, we propose\nPluto and Charon (PAC), a time and memory efficient collaborative edge AI\nframework for personal LLMs fine-tuning. PAC breaks the resource wall of\npersonal LLMs fine-tuning with a sophisticated algorithm-system co-design. (1)\nAlgorithmically, PAC implements a personal LLMs fine-tuning technique that is\nefficient in terms of parameters, time, and memory. It utilizes Parallel\nAdapters to circumvent the need for a full backward pass through the LLM\nbackbone. Additionally, an activation cache mechanism further streamlining the\nprocess by negating the necessity for repeated forward passes across multiple\nepochs. (2) Systematically, PAC leverages edge devices in close proximity,\npooling them as a collective resource for in-situ personal LLMs fine-tuning,\nutilizing a hybrid data and pipeline parallelism to orchestrate distributed\ntraining. The use of the activation cache eliminates the need for forward pass\nthrough the LLM backbone,enabling exclusive fine-tuning of the Parallel\nAdapters using data parallelism. Extensive evaluation based on prototype\nimplementation demonstrates that PAC remarkably outperforms state-of-the-art\napproaches, achieving up to 8.64x end-to-end speedup and up to 88.16% reduction\nin memory footprint."
                },
                "authors": [
                    {
                        "name": "Bei Ouyang"
                    },
                    {
                        "name": "Shengyuan Ye"
                    },
                    {
                        "name": "Liekang Zeng"
                    },
                    {
                        "name": "Tianyi Qian"
                    },
                    {
                        "name": "Jingyi Li"
                    },
                    {
                        "name": "Xu Chen"
                    }
                ],
                "author_detail": {
                    "name": "Xu Chen"
                },
                "author": "Xu Chen",
                "arxiv_comment": "Accepted by The 53rd International Conference on Parallel Processing\n  (ICPP'24)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.10746v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.10746v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.09697v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.09697v2",
                "updated": "2024-08-20T04:46:18Z",
                "updated_parsed": [
                    2024,
                    8,
                    20,
                    4,
                    46,
                    18,
                    1,
                    233,
                    0
                ],
                "published": "2024-08-19T04:43:56Z",
                "published_parsed": [
                    2024,
                    8,
                    19,
                    4,
                    43,
                    56,
                    0,
                    232,
                    0
                ],
                "title": "Heta: Distributed Training of Heterogeneous Graph Neural Networks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Heta: Distributed Training of Heterogeneous Graph Neural Networks"
                },
                "summary": "Heterogeneous Graph Neural Networks (HGNNs) leverage diverse semantic\nrelationships in Heterogeneous Graphs (HetGs) and have demonstrated remarkable\nlearning performance in various applications. However, current distributed GNN\ntraining systems often overlook unique characteristics of HetGs, such as\nvarying feature dimensions and the prevalence of missing features among nodes,\nleading to suboptimal performance or even incompatibility with distributed HGNN\ntraining. We introduce Heta, a framework designed to address the communication\nbottleneck in distributed HGNN training. Heta leverages the inherent structure\nof HGNNs - independent relation-specific aggregations for each relation,\nfollowed by a cross-relation aggregation - and advocates for a novel\nRelation-Aggregation-First computation paradigm. It performs relation-specific\naggregations within graph partitions and then exchanges partial aggregations.\nThis design, coupled with a new graph partitioning method that divides a HetG\nbased on its graph schema and HGNN computation dependency, substantially\nreduces communication overhead. Heta further incorporates an innovative GPU\nfeature caching strategy that accounts for the different cache miss-penalties\nassociated with diverse node types. Comprehensive evaluations of various HGNN\nmodels and large heterogeneous graph datasets demonstrate that Heta outperforms\nstate-of-the-art systems like DGL and GraphLearn by up to 5.8x and 2.3x in\nend-to-end epoch time, respectively.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Heterogeneous Graph Neural Networks (HGNNs) leverage diverse semantic\nrelationships in Heterogeneous Graphs (HetGs) and have demonstrated remarkable\nlearning performance in various applications. However, current distributed GNN\ntraining systems often overlook unique characteristics of HetGs, such as\nvarying feature dimensions and the prevalence of missing features among nodes,\nleading to suboptimal performance or even incompatibility with distributed HGNN\ntraining. We introduce Heta, a framework designed to address the communication\nbottleneck in distributed HGNN training. Heta leverages the inherent structure\nof HGNNs - independent relation-specific aggregations for each relation,\nfollowed by a cross-relation aggregation - and advocates for a novel\nRelation-Aggregation-First computation paradigm. It performs relation-specific\naggregations within graph partitions and then exchanges partial aggregations.\nThis design, coupled with a new graph partitioning method that divides a HetG\nbased on its graph schema and HGNN computation dependency, substantially\nreduces communication overhead. Heta further incorporates an innovative GPU\nfeature caching strategy that accounts for the different cache miss-penalties\nassociated with diverse node types. Comprehensive evaluations of various HGNN\nmodels and large heterogeneous graph datasets demonstrate that Heta outperforms\nstate-of-the-art systems like DGL and GraphLearn by up to 5.8x and 2.3x in\nend-to-end epoch time, respectively."
                },
                "authors": [
                    {
                        "name": "Yuchen Zhong"
                    },
                    {
                        "name": "Junwei Su"
                    },
                    {
                        "name": "Chuan Wu"
                    },
                    {
                        "name": "Minjie Wang"
                    }
                ],
                "author_detail": {
                    "name": "Minjie Wang"
                },
                "author": "Minjie Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.09697v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.09697v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.10104v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.10104v1",
                "updated": "2024-08-19T15:47:17Z",
                "updated_parsed": [
                    2024,
                    8,
                    19,
                    15,
                    47,
                    17,
                    0,
                    232,
                    0
                ],
                "published": "2024-08-19T15:47:17Z",
                "published_parsed": [
                    2024,
                    8,
                    19,
                    15,
                    47,
                    17,
                    0,
                    232,
                    0
                ],
                "title": "Multi-Mode Lens for Momentum Microscopy and XPEEM: Theory",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-Mode Lens for Momentum Microscopy and XPEEM: Theory"
                },
                "summary": "The strong electric field between the sample and the extractor is the core of\ncathode lenses and a pivotal determinant of high resolution. Nevertheless,\nfields in the range of 3-8 kV/mm can be a source of complications. Local field\nenhancement at sharp edges or microscopic protrusions of cleaved samples may\nresult in field emission or flashovers. Moreover, slow background electrons are\ndrawn into the microscope column, where they contribute to space charge\neffects. A novel front lens configuration, optimized through ray-tracing\nsimulations, significantly reduces the field at the sample and allows even for\nzero field or retarding field, which serves to suppress space charge effects.\nOne or several annular electrodes, situated in a concentric position relative\nto the extractor, serve to form an additional lens within the gap between the\nsample and the extractor. The refractory power of this lens, and consequently\nthe field at the sample surface, can be modified by adjusting the potentials of\nthe annular electrodes. The imaging properties and aberrations of this gap lens\nhave been investigated with regard to momentum imaging and XPEEM. The study\nencompasses the energy range from the few-eV level for laser-ARPES to 6 keV,\nfor hard X-ray ARPES. The additional converging lens situated in close\nproximity to the sample exhibits a reduced field curvature of the k-image in\nthe backfocal plane. This allows for the acquisition of larger fields of view\nin both momentum and real-space imaging.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The strong electric field between the sample and the extractor is the core of\ncathode lenses and a pivotal determinant of high resolution. Nevertheless,\nfields in the range of 3-8 kV/mm can be a source of complications. Local field\nenhancement at sharp edges or microscopic protrusions of cleaved samples may\nresult in field emission or flashovers. Moreover, slow background electrons are\ndrawn into the microscope column, where they contribute to space charge\neffects. A novel front lens configuration, optimized through ray-tracing\nsimulations, significantly reduces the field at the sample and allows even for\nzero field or retarding field, which serves to suppress space charge effects.\nOne or several annular electrodes, situated in a concentric position relative\nto the extractor, serve to form an additional lens within the gap between the\nsample and the extractor. The refractory power of this lens, and consequently\nthe field at the sample surface, can be modified by adjusting the potentials of\nthe annular electrodes. The imaging properties and aberrations of this gap lens\nhave been investigated with regard to momentum imaging and XPEEM. The study\nencompasses the energy range from the few-eV level for laser-ARPES to 6 keV,\nfor hard X-ray ARPES. The additional converging lens situated in close\nproximity to the sample exhibits a reduced field curvature of the k-image in\nthe backfocal plane. This allows for the acquisition of larger fields of view\nin both momentum and real-space imaging."
                },
                "authors": [
                    {
                        "name": "Olena Tkach"
                    },
                    {
                        "name": "Gerd Schoenhense"
                    }
                ],
                "author_detail": {
                    "name": "Gerd Schoenhense"
                },
                "author": "Gerd Schoenhense",
                "arxiv_comment": "17 pages, 4 figures, 44 references",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.10104v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.10104v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.app-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.app-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cond-mat.mtrl-sci",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "physics.ins-det",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.09848v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.09848v1",
                "updated": "2024-08-19T09:50:35Z",
                "updated_parsed": [
                    2024,
                    8,
                    19,
                    9,
                    50,
                    35,
                    0,
                    232,
                    0
                ],
                "published": "2024-08-19T09:50:35Z",
                "published_parsed": [
                    2024,
                    8,
                    19,
                    9,
                    50,
                    35,
                    0,
                    232,
                    0
                ],
                "title": "Abstract Environment Trimming",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Abstract Environment Trimming"
                },
                "summary": "Variable sharing is a fundamental property in the static analysis of logic\nprograms, since it is instrumental for ensuring correctness and increasing\nprecision while inferring many useful program properties. Such properties\ninclude modes, determinacy, non-failure, cost, etc. This has motivated\nsignificant work on developing abstract domains to improve the precision and\nperformance of sharing analyses. Much of this work has centered around the\nfamily of set-sharing domains, because of the high precision they offer.\nHowever, this comes at a price: their scalability to a wide set of realistic\nprograms remains challenging and this hinders their wider adoption. In this\nwork, rather than defining new sharing abstract domains, we focus instead on\ndeveloping techniques which can be incorporated in the analyzers to address\naspects that are known to affect the efficiency of these domains, such as the\nnumber of variables, without affecting precision. These techniques are inspired\nin others used in the context of compiler optimizations, such as expression\nreassociation and variable trimming. We present several such techniques and\nprovide an extensive experimental evaluation of over 1100 program modules taken\nfrom both production code and classical benchmarks. This includes the\nSpectector cache analyzer, the s(CASP) system, the libraries of the Ciao\nsystem, the LPdoc documenter, the PLAI analyzer itself, etc. The experimental\nresults are quite encouraging: we have obtained significant speed-ups, and,\nmore importantly, the number of modules that require a timeout was cut in half.\nAs a result, many more programs can be analyzed precisely in reasonable times.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Variable sharing is a fundamental property in the static analysis of logic\nprograms, since it is instrumental for ensuring correctness and increasing\nprecision while inferring many useful program properties. Such properties\ninclude modes, determinacy, non-failure, cost, etc. This has motivated\nsignificant work on developing abstract domains to improve the precision and\nperformance of sharing analyses. Much of this work has centered around the\nfamily of set-sharing domains, because of the high precision they offer.\nHowever, this comes at a price: their scalability to a wide set of realistic\nprograms remains challenging and this hinders their wider adoption. In this\nwork, rather than defining new sharing abstract domains, we focus instead on\ndeveloping techniques which can be incorporated in the analyzers to address\naspects that are known to affect the efficiency of these domains, such as the\nnumber of variables, without affecting precision. These techniques are inspired\nin others used in the context of compiler optimizations, such as expression\nreassociation and variable trimming. We present several such techniques and\nprovide an extensive experimental evaluation of over 1100 program modules taken\nfrom both production code and classical benchmarks. This includes the\nSpectector cache analyzer, the s(CASP) system, the libraries of the Ciao\nsystem, the LPdoc documenter, the PLAI analyzer itself, etc. The experimental\nresults are quite encouraging: we have obtained significant speed-ups, and,\nmore importantly, the number of modules that require a timeout was cut in half.\nAs a result, many more programs can be analyzed precisely in reasonable times."
                },
                "authors": [
                    {
                        "name": "Daniel Jurjo-Rivas"
                    },
                    {
                        "name": "Jose F. Morales"
                    },
                    {
                        "name": "Pedro López-García"
                    },
                    {
                        "name": "Manuel V. Hermenegildo"
                    }
                ],
                "author_detail": {
                    "name": "Manuel V. Hermenegildo"
                },
                "author": "Manuel V. Hermenegildo",
                "arxiv_comment": "61 pages, 10 figures, 7 tables, submitted to ICLP 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.09848v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.09848v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.PL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.PL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.10284v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.10284v1",
                "updated": "2024-08-19T03:27:15Z",
                "updated_parsed": [
                    2024,
                    8,
                    19,
                    3,
                    27,
                    15,
                    0,
                    232,
                    0
                ],
                "published": "2024-08-19T03:27:15Z",
                "published_parsed": [
                    2024,
                    8,
                    19,
                    3,
                    27,
                    15,
                    0,
                    232,
                    0
                ],
                "title": "AdapMoE: Adaptive Sensitivity-based Expert Gating and Management for\n  Efficient MoE Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AdapMoE: Adaptive Sensitivity-based Expert Gating and Management for\n  Efficient MoE Inference"
                },
                "summary": "Mixture-of-Experts (MoE) models are designed to enhance the efficiency of\nlarge language models (LLMs) without proportionally increasing the\ncomputational demands. However, their deployment on edge devices still faces\nsignificant challenges due to high on-demand loading overheads from managing\nsparsely activated experts. This paper introduces AdapMoE, an algorithm-system\nco-design framework for efficient MoE inference. AdapMoE features adaptive\nexpert gating and management to reduce the on-demand loading overheads. We\nobserve the heterogeneity of experts loading across layers and tokens, based on\nwhich we propose a sensitivity-based strategy to adjust the number of activated\nexperts dynamically. Meanwhile, we also integrate advanced prefetching and\ncache management techniques to further reduce the loading latency. Through\ncomprehensive evaluations on various platforms, we demonstrate AdapMoE\nconsistently outperforms existing techniques, reducing the average number of\nactivated experts by 25% and achieving a 1.35x speedup without accuracy\ndegradation. Code is available at: https://github.com/PKU-SEC-Lab/AdapMoE.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mixture-of-Experts (MoE) models are designed to enhance the efficiency of\nlarge language models (LLMs) without proportionally increasing the\ncomputational demands. However, their deployment on edge devices still faces\nsignificant challenges due to high on-demand loading overheads from managing\nsparsely activated experts. This paper introduces AdapMoE, an algorithm-system\nco-design framework for efficient MoE inference. AdapMoE features adaptive\nexpert gating and management to reduce the on-demand loading overheads. We\nobserve the heterogeneity of experts loading across layers and tokens, based on\nwhich we propose a sensitivity-based strategy to adjust the number of activated\nexperts dynamically. Meanwhile, we also integrate advanced prefetching and\ncache management techniques to further reduce the loading latency. Through\ncomprehensive evaluations on various platforms, we demonstrate AdapMoE\nconsistently outperforms existing techniques, reducing the average number of\nactivated experts by 25% and achieving a 1.35x speedup without accuracy\ndegradation. Code is available at: https://github.com/PKU-SEC-Lab/AdapMoE."
                },
                "authors": [
                    {
                        "name": "Shuzhang Zhong"
                    },
                    {
                        "name": "Ling Liang"
                    },
                    {
                        "name": "Yuan Wang"
                    },
                    {
                        "name": "Runsheng Wang"
                    },
                    {
                        "name": "Ru Huang"
                    },
                    {
                        "name": "Meng Li"
                    }
                ],
                "author_detail": {
                    "name": "Meng Li"
                },
                "author": "Meng Li",
                "arxiv_doi": "10.1145/3676536.3676741",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3676536.3676741",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2408.10284v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.10284v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.07092v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.07092v2",
                "updated": "2024-08-18T17:27:17Z",
                "updated_parsed": [
                    2024,
                    8,
                    18,
                    17,
                    27,
                    17,
                    6,
                    231,
                    0
                ],
                "published": "2024-08-11T18:40:36Z",
                "published_parsed": [
                    2024,
                    8,
                    11,
                    18,
                    40,
                    36,
                    6,
                    224,
                    0
                ],
                "title": "Post-Training Sparse Attention with Double Sparsity",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Post-Training Sparse Attention with Double Sparsity"
                },
                "summary": "The inference process for large language models is slow and memory-intensive,\nwith one of the most critical bottlenecks being excessive Key-Value (KV) cache\naccesses. This paper introduces \"Double Sparsity,\" a novel post-training sparse\nattention technique designed to alleviate this bottleneck by reducing KV cache\naccess. Double Sparsity combines token sparsity, which focuses on utilizing\nonly the important tokens for computing self-attention, with channel sparsity,\nan approach that uses important feature channels for identifying important\ntokens. Our key insight is that the pattern of channel sparsity is relatively\nstatic, allowing us to use offline calibration to make it efficient at runtime,\nthereby enabling accurate and efficient identification of important tokens.\nMoreover, this method can be combined with offloading to achieve significant\nmemory usage reduction. Experimental results demonstrate that Double Sparsity\ncan achieve $\\frac{1}{16}$ token and channel sparsity with minimal impact on\naccuracy across various tasks, including wiki-2 perplexity, key-value\nretrieval, and long context benchmarks with models including Llama-2-7B,\nLlama-2-70B, and Mixtral-8x7B. It brings up to a 14.1$\\times$ acceleration in\nattention operations and a 1.9$\\times$ improvement in end-to-end inference on\nGPUs. With offloading, it achieves a decoding speed acceleration of\n16.3$\\times$ compared to state-of-the-art solutions at a sequence length of\n256K. Our code is publicly available at\nhttps://github.com/andy-yang-1/DoubleSparse.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The inference process for large language models is slow and memory-intensive,\nwith one of the most critical bottlenecks being excessive Key-Value (KV) cache\naccesses. This paper introduces \"Double Sparsity,\" a novel post-training sparse\nattention technique designed to alleviate this bottleneck by reducing KV cache\naccess. Double Sparsity combines token sparsity, which focuses on utilizing\nonly the important tokens for computing self-attention, with channel sparsity,\nan approach that uses important feature channels for identifying important\ntokens. Our key insight is that the pattern of channel sparsity is relatively\nstatic, allowing us to use offline calibration to make it efficient at runtime,\nthereby enabling accurate and efficient identification of important tokens.\nMoreover, this method can be combined with offloading to achieve significant\nmemory usage reduction. Experimental results demonstrate that Double Sparsity\ncan achieve $\\frac{1}{16}$ token and channel sparsity with minimal impact on\naccuracy across various tasks, including wiki-2 perplexity, key-value\nretrieval, and long context benchmarks with models including Llama-2-7B,\nLlama-2-70B, and Mixtral-8x7B. It brings up to a 14.1$\\times$ acceleration in\nattention operations and a 1.9$\\times$ improvement in end-to-end inference on\nGPUs. With offloading, it achieves a decoding speed acceleration of\n16.3$\\times$ compared to state-of-the-art solutions at a sequence length of\n256K. Our code is publicly available at\nhttps://github.com/andy-yang-1/DoubleSparse."
                },
                "authors": [
                    {
                        "name": "Shuo Yang"
                    },
                    {
                        "name": "Ying Sheng"
                    },
                    {
                        "name": "Joseph E. Gonzalez"
                    },
                    {
                        "name": "Ion Stoica"
                    },
                    {
                        "name": "Lianmin Zheng"
                    }
                ],
                "author_detail": {
                    "name": "Lianmin Zheng"
                },
                "author": "Lianmin Zheng",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.07092v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.07092v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.09483v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.09483v1",
                "updated": "2024-08-18T13:54:46Z",
                "updated_parsed": [
                    2024,
                    8,
                    18,
                    13,
                    54,
                    46,
                    6,
                    231,
                    0
                ],
                "published": "2024-08-18T13:54:46Z",
                "published_parsed": [
                    2024,
                    8,
                    18,
                    13,
                    54,
                    46,
                    6,
                    231,
                    0
                ],
                "title": "CMD: A Cache-assisted GPU Memory Deduplication Architecture",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CMD: A Cache-assisted GPU Memory Deduplication Architecture"
                },
                "summary": "Massive off-chip accesses in GPUs are the main performance bottleneck, and we\ndivided these accesses into three types: (1) Write, (2) Data-Read, and (3)\nRead-Only. Besides, We find that many writes are duplicate, and the duplication\ncan be inter-dup and intra-dup. While inter-dup means different memory blocks\nare identical, and intra-dup means all the 4B elements in a line are the same.\nIn this work, we propose a cache-assisted GPU memory deduplication architecture\nnamed CMD to reduce the off-chip accesses via utilizing the data duplication in\nGPU applications. CMD includes three key design contributions which aim to\nreduce the three kinds of accesses: (1) A novel GPU memory deduplication\narchitecture that removes the inter-dup and inter-dup lines. As for the\ninter-dup detection, we reduce the extra read requests caused by the\ntraditional read-verify hash process. Besides, we design several techniques to\nmanage duplicate blocks. (2) We propose a cache-assisted read scheme to reduce\nthe reads to duplicate data. When an L2 cache miss wants to read the duplicate\nblock, if the reference block has been fetched to L2 and it is clean, we can\ncopy it to the L2 missed block without accessing off-chip DRAM. As for the\nreads to intra-dup data, CMD uses the on-chip metadata cache to get the data.\n(3) When a cache line is evicted, the clean sectors in the line are invalidated\nwhile the dirty sectors are written back. However, most read-only victims are\nre-referenced from DRAM more than twice. Therefore, we add a full-associate\nFIFO to accommodate the read-only (it is also clean) victims to reduce the\nre-reference counts. Experiments show that CMD can decrease the off-chip\naccesses by 31.01%, reduce the energy by 32.78% and improve performance by\n37.79%. Besides, CMD can improve the performance of memory-intensive workloads\nby 50.18%.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Massive off-chip accesses in GPUs are the main performance bottleneck, and we\ndivided these accesses into three types: (1) Write, (2) Data-Read, and (3)\nRead-Only. Besides, We find that many writes are duplicate, and the duplication\ncan be inter-dup and intra-dup. While inter-dup means different memory blocks\nare identical, and intra-dup means all the 4B elements in a line are the same.\nIn this work, we propose a cache-assisted GPU memory deduplication architecture\nnamed CMD to reduce the off-chip accesses via utilizing the data duplication in\nGPU applications. CMD includes three key design contributions which aim to\nreduce the three kinds of accesses: (1) A novel GPU memory deduplication\narchitecture that removes the inter-dup and inter-dup lines. As for the\ninter-dup detection, we reduce the extra read requests caused by the\ntraditional read-verify hash process. Besides, we design several techniques to\nmanage duplicate blocks. (2) We propose a cache-assisted read scheme to reduce\nthe reads to duplicate data. When an L2 cache miss wants to read the duplicate\nblock, if the reference block has been fetched to L2 and it is clean, we can\ncopy it to the L2 missed block without accessing off-chip DRAM. As for the\nreads to intra-dup data, CMD uses the on-chip metadata cache to get the data.\n(3) When a cache line is evicted, the clean sectors in the line are invalidated\nwhile the dirty sectors are written back. However, most read-only victims are\nre-referenced from DRAM more than twice. Therefore, we add a full-associate\nFIFO to accommodate the read-only (it is also clean) victims to reduce the\nre-reference counts. Experiments show that CMD can decrease the off-chip\naccesses by 31.01%, reduce the energy by 32.78% and improve performance by\n37.79%. Besides, CMD can improve the performance of memory-intensive workloads\nby 50.18%."
                },
                "authors": [
                    {
                        "name": "Wei Zhao"
                    },
                    {
                        "name": "Dan Feng"
                    },
                    {
                        "name": "Wei Tong"
                    },
                    {
                        "name": "Xueliang Wei"
                    },
                    {
                        "name": "Bing Wu"
                    }
                ],
                "author_detail": {
                    "name": "Bing Wu"
                },
                "author": "Bing Wu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.09483v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.09483v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.11550v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.11550v3",
                "updated": "2024-08-16T08:46:33Z",
                "updated_parsed": [
                    2024,
                    8,
                    16,
                    8,
                    46,
                    33,
                    4,
                    229,
                    0
                ],
                "published": "2024-07-16T09:53:32Z",
                "published_parsed": [
                    2024,
                    7,
                    16,
                    9,
                    53,
                    32,
                    1,
                    198,
                    0
                ],
                "title": "Ada-KV: Optimizing KV Cache Eviction by Adaptive Budget Allocation for\n  Efficient LLM Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Ada-KV: Optimizing KV Cache Eviction by Adaptive Budget Allocation for\n  Efficient LLM Inference"
                },
                "summary": "Large Language Models have excelled in various fields but encounter\nchallenges in memory and time efficiency due to the expanding Key-Value (KV)\ncache required for long-sequence inference. Recent efforts try to reduce KV\ncache size to a given memory budget by evicting vast non-critical cache\nelements during runtime, while preserving generation quality. Our revisiting of\ncurrent eviction methods reveals that they fundamentally minimize an upper\nbound of the $L_1$ eviction loss between the pre- and post-eviction outputs of\nmulti-head self-attention mechanisms. Moreover, our analysis indicates that the\ncommon practices of uniformly assigning budgets across attention heads harm\ntheir post-eviction generation quality. In light of these findings, we propose\na simple yet effective adaptive budget allocation algorithm. This algorithm not\nonly optimizes the theoretical loss upper bound but also reduces the $L_1$\neviction loss in practice by aligning with the varied characteristics across\ndifferent heads. By integrating this algorithm into two state-of-the-art\nmethods, we demonstrate the effectiveness of using adaptive budget allocation\nto optimize KV cache eviction. Extensive evaluations on 16 datasets and the\nNeedle-in-a-Haystack test confirm significant performance improvements across\nvarious tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models have excelled in various fields but encounter\nchallenges in memory and time efficiency due to the expanding Key-Value (KV)\ncache required for long-sequence inference. Recent efforts try to reduce KV\ncache size to a given memory budget by evicting vast non-critical cache\nelements during runtime, while preserving generation quality. Our revisiting of\ncurrent eviction methods reveals that they fundamentally minimize an upper\nbound of the $L_1$ eviction loss between the pre- and post-eviction outputs of\nmulti-head self-attention mechanisms. Moreover, our analysis indicates that the\ncommon practices of uniformly assigning budgets across attention heads harm\ntheir post-eviction generation quality. In light of these findings, we propose\na simple yet effective adaptive budget allocation algorithm. This algorithm not\nonly optimizes the theoretical loss upper bound but also reduces the $L_1$\neviction loss in practice by aligning with the varied characteristics across\ndifferent heads. By integrating this algorithm into two state-of-the-art\nmethods, we demonstrate the effectiveness of using adaptive budget allocation\nto optimize KV cache eviction. Extensive evaluations on 16 datasets and the\nNeedle-in-a-Haystack test confirm significant performance improvements across\nvarious tasks."
                },
                "authors": [
                    {
                        "name": "Yuan Feng"
                    },
                    {
                        "name": "Junlin Lv"
                    },
                    {
                        "name": "Yukun Cao"
                    },
                    {
                        "name": "Xike Xie"
                    },
                    {
                        "name": "S. Kevin Zhou"
                    }
                ],
                "author_detail": {
                    "name": "S. Kevin Zhou"
                },
                "author": "S. Kevin Zhou",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.11550v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.11550v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.08545v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.08545v1",
                "updated": "2024-08-16T06:11:21Z",
                "updated_parsed": [
                    2024,
                    8,
                    16,
                    6,
                    11,
                    21,
                    4,
                    229,
                    0
                ],
                "published": "2024-08-16T06:11:21Z",
                "published_parsed": [
                    2024,
                    8,
                    16,
                    6,
                    11,
                    21,
                    4,
                    229,
                    0
                ],
                "title": "SelectLLM: Query-Aware Efficient Selection Algorithm for Large Language\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SelectLLM: Query-Aware Efficient Selection Algorithm for Large Language\n  Models"
                },
                "summary": "Large language models (LLMs) have gained increased popularity due to their\nremarkable success across various tasks, which has led to the active\ndevelopment of a large set of diverse LLMs. However, individual LLMs have\nlimitations when applied to complex tasks because of such factors as training\nbiases, model sizes, and the datasets used. A promising approach is to\nefficiently harness the diverse capabilities of LLMs to overcome these\nindividual limitations. Towards this goal, we introduce a novel LLM selection\nalgorithm called SelectLLM. This algorithm directs input queries to the most\nsuitable subset of LLMs from a large pool, ensuring they collectively provide\nthe correct response efficiently. SelectLLM uses a multi-label classifier,\nutilizing the classifier's predictions and confidence scores to design optimal\npolicies for selecting an optimal, query-aware, and lightweight subset of LLMs.\nOur findings show that the proposed model outperforms individual LLMs and\nachieves competitive performance compared to similarly sized, computationally\nexpensive top-performing LLM subsets. Specifically, with a similarly sized\ntop-performing LLM subset, we achieve a significant reduction in latency on two\nstandard reasoning benchmarks: 13% lower latency for GSM8K and 70% lower\nlatency for MMLU. Additionally, we conduct comprehensive analyses and ablation\nstudies, which validate the robustness of the proposed model.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have gained increased popularity due to their\nremarkable success across various tasks, which has led to the active\ndevelopment of a large set of diverse LLMs. However, individual LLMs have\nlimitations when applied to complex tasks because of such factors as training\nbiases, model sizes, and the datasets used. A promising approach is to\nefficiently harness the diverse capabilities of LLMs to overcome these\nindividual limitations. Towards this goal, we introduce a novel LLM selection\nalgorithm called SelectLLM. This algorithm directs input queries to the most\nsuitable subset of LLMs from a large pool, ensuring they collectively provide\nthe correct response efficiently. SelectLLM uses a multi-label classifier,\nutilizing the classifier's predictions and confidence scores to design optimal\npolicies for selecting an optimal, query-aware, and lightweight subset of LLMs.\nOur findings show that the proposed model outperforms individual LLMs and\nachieves competitive performance compared to similarly sized, computationally\nexpensive top-performing LLM subsets. Specifically, with a similarly sized\ntop-performing LLM subset, we achieve a significant reduction in latency on two\nstandard reasoning benchmarks: 13% lower latency for GSM8K and 70% lower\nlatency for MMLU. Additionally, we conduct comprehensive analyses and ablation\nstudies, which validate the robustness of the proposed model."
                },
                "authors": [
                    {
                        "name": "Kaushal Kumar Maurya"
                    },
                    {
                        "name": "KV Aditya Srivatsa"
                    },
                    {
                        "name": "Ekaterina Kochmar"
                    }
                ],
                "author_detail": {
                    "name": "Ekaterina Kochmar"
                },
                "author": "Ekaterina Kochmar",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.08545v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.08545v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.19291v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.19291v2",
                "updated": "2024-08-16T04:12:25Z",
                "updated_parsed": [
                    2024,
                    8,
                    16,
                    4,
                    12,
                    25,
                    4,
                    229,
                    0
                ],
                "published": "2024-07-27T16:20:21Z",
                "published_parsed": [
                    2024,
                    7,
                    27,
                    16,
                    20,
                    21,
                    5,
                    209,
                    0
                ],
                "title": "Symmetric Locality: Definition and Initial Results",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Symmetric Locality: Definition and Initial Results"
                },
                "summary": "In this short paper, we characterize symmetric locality. In designing\nalgorithms, compilers, and systems, data movement is a common bottleneck in\nhigh-performance computation, in which we improve cache and memory performance.\nWe study a special type of data reuse in the form of repeated traversals, or\nre-traversals, which are based on the symmetric group. The cyclic and sawtooth\ntraces are previously known results in symmetric locality, and in this work, we\nwould like to generalize this result for any re-traversal. Then, we also\nprovide an abstract framework for applications in compiler design and machine\nlearning models to improve the memory performance of certain programs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this short paper, we characterize symmetric locality. In designing\nalgorithms, compilers, and systems, data movement is a common bottleneck in\nhigh-performance computation, in which we improve cache and memory performance.\nWe study a special type of data reuse in the form of repeated traversals, or\nre-traversals, which are based on the symmetric group. The cyclic and sawtooth\ntraces are previously known results in symmetric locality, and in this work, we\nwould like to generalize this result for any re-traversal. Then, we also\nprovide an abstract framework for applications in compiler design and machine\nlearning models to improve the memory performance of certain programs."
                },
                "authors": [
                    {
                        "name": "Giordan Escalona"
                    },
                    {
                        "name": "Dylan McKellips"
                    },
                    {
                        "name": "Chen Ding"
                    }
                ],
                "author_detail": {
                    "name": "Chen Ding"
                },
                "author": "Chen Ding",
                "arxiv_comment": "6 pages, 2nd ver",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.19291v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.19291v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.04870v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.04870v4",
                "updated": "2024-09-20T15:51:17Z",
                "updated_parsed": [
                    2024,
                    9,
                    20,
                    15,
                    51,
                    17,
                    4,
                    264,
                    0
                ],
                "published": "2024-08-09T05:20:05Z",
                "published_parsed": [
                    2024,
                    8,
                    9,
                    5,
                    20,
                    5,
                    4,
                    222,
                    0
                ],
                "title": "ConfusedPilot: Confused Deputy Risks in RAG-based LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ConfusedPilot: Confused Deputy Risks in RAG-based LLMs"
                },
                "summary": "Retrieval augmented generation (RAG) is a process where a large language\nmodel (LLM) retrieves useful information from a database and then generates the\nresponses. It is becoming popular in enterprise settings for daily business\noperations. For example, Copilot for Microsoft 365 has accumulated millions of\nbusinesses. However, the security implications of adopting such RAG-based\nsystems are unclear.\n  In this paper, we introduce ConfusedPilot, a class of security\nvulnerabilities of RAG systems that confuse Copilot and cause integrity and\nconfidentiality violations in its responses. First, we investigate a\nvulnerability that embeds malicious text in the modified prompt in RAG,\ncorrupting the responses generated by the LLM. Second, we demonstrate a\nvulnerability that leaks secret data, which leverages the caching mechanism\nduring retrieval. Third, we investigate how both vulnerabilities can be\nexploited to propagate misinformation within the enterprise and ultimately\nimpact its operations, such as sales and manufacturing. We also discuss the\nroot cause of these attacks by investigating the architecture of a RAG-based\nsystem. This study highlights the security vulnerabilities in today's RAG-based\nsystems and proposes design guidelines to secure future RAG-based systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Retrieval augmented generation (RAG) is a process where a large language\nmodel (LLM) retrieves useful information from a database and then generates the\nresponses. It is becoming popular in enterprise settings for daily business\noperations. For example, Copilot for Microsoft 365 has accumulated millions of\nbusinesses. However, the security implications of adopting such RAG-based\nsystems are unclear.\n  In this paper, we introduce ConfusedPilot, a class of security\nvulnerabilities of RAG systems that confuse Copilot and cause integrity and\nconfidentiality violations in its responses. First, we investigate a\nvulnerability that embeds malicious text in the modified prompt in RAG,\ncorrupting the responses generated by the LLM. Second, we demonstrate a\nvulnerability that leaks secret data, which leverages the caching mechanism\nduring retrieval. Third, we investigate how both vulnerabilities can be\nexploited to propagate misinformation within the enterprise and ultimately\nimpact its operations, such as sales and manufacturing. We also discuss the\nroot cause of these attacks by investigating the architecture of a RAG-based\nsystem. This study highlights the security vulnerabilities in today's RAG-based\nsystems and proposes design guidelines to secure future RAG-based systems."
                },
                "authors": [
                    {
                        "name": "Ayush RoyChowdhury"
                    },
                    {
                        "name": "Mulong Luo"
                    },
                    {
                        "name": "Prateek Sahu"
                    },
                    {
                        "name": "Sarbartha Banerjee"
                    },
                    {
                        "name": "Mohit Tiwari"
                    }
                ],
                "author_detail": {
                    "name": "Mohit Tiwari"
                },
                "author": "Mohit Tiwari",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.04870v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.04870v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.07853v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.07853v1",
                "updated": "2024-08-14T23:42:46Z",
                "updated_parsed": [
                    2024,
                    8,
                    14,
                    23,
                    42,
                    46,
                    2,
                    227,
                    0
                ],
                "published": "2024-08-14T23:42:46Z",
                "published_parsed": [
                    2024,
                    8,
                    14,
                    23,
                    42,
                    46,
                    2,
                    227,
                    0
                ],
                "title": "A Case for Enabling Delegation of 5G Core Decisions to the RAN",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Case for Enabling Delegation of 5G Core Decisions to the RAN"
                },
                "summary": "Under conventional 5G system design, the authentication and continuous\nmonitoring of user equipment (UE) demands a reliable backhaul connection\nbetween the radio access network (RAN) and the core network functions (AMF,\nAUSF, UDM, etc.). This is not a given, especially in disaster response and\nmilitary operations. We propose that, in these scenarios, decisions made by\ncore functions can be effectively delegated to the RAN by leveraging the RAN's\ncomputing resources and the micro-service programmability of the O-RAN system\narchitecture. This paper presents several concrete designs of core-RAN decision\ndelegation, including caching of core decisions and replicating some of the\ncore decision logic. Each design has revealed interesting performance and\nsecurity trade-offs that warrant further investigation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Under conventional 5G system design, the authentication and continuous\nmonitoring of user equipment (UE) demands a reliable backhaul connection\nbetween the radio access network (RAN) and the core network functions (AMF,\nAUSF, UDM, etc.). This is not a given, especially in disaster response and\nmilitary operations. We propose that, in these scenarios, decisions made by\ncore functions can be effectively delegated to the RAN by leveraging the RAN's\ncomputing resources and the micro-service programmability of the O-RAN system\narchitecture. This paper presents several concrete designs of core-RAN decision\ndelegation, including caching of core decisions and replicating some of the\ncore decision logic. Each design has revealed interesting performance and\nsecurity trade-offs that warrant further investigation."
                },
                "authors": [
                    {
                        "name": "Lucas Vancina"
                    },
                    {
                        "name": "Geoffrey Xie"
                    }
                ],
                "author_detail": {
                    "name": "Geoffrey Xie"
                },
                "author": "Geoffrey Xie",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.07853v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.07853v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.15440v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.15440v2",
                "updated": "2024-08-14T09:18:02Z",
                "updated_parsed": [
                    2024,
                    8,
                    14,
                    9,
                    18,
                    2,
                    2,
                    227,
                    0
                ],
                "published": "2024-07-22T07:42:57Z",
                "published_parsed": [
                    2024,
                    7,
                    22,
                    7,
                    42,
                    57,
                    0,
                    204,
                    0
                ],
                "title": "The Bicameral Cache: a split cache for vector architectures",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Bicameral Cache: a split cache for vector architectures"
                },
                "summary": "The Bicameral Cache is a cache organization proposal for a vector\narchitecture that segregates data according to their access type,\ndistinguishing scalar from vector references. Its aim is to avoid both types of\nreferences from interfering in each other's data locality, with a special focus\non prioritizing the performance on vector references. The proposed system\nincorporates an additional, non-polluting prefetching mechanism to help\npopulate the long vector cache lines in advance to increase the hit rate by\nfurther exploiting the spatial locality on vector data. Its evaluation was\nconducted on the Cavatools simulator, comparing the performance to a standard\nconventional cache, over different typical vector benchmarks for several vector\nlengths. The results proved the proposed cache speeds up performance on\nstride-1 vector benchmarks, while hardly impacting non-stride-1's. In addition,\nthe prefetching feature consistently provided an additional value.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Bicameral Cache is a cache organization proposal for a vector\narchitecture that segregates data according to their access type,\ndistinguishing scalar from vector references. Its aim is to avoid both types of\nreferences from interfering in each other's data locality, with a special focus\non prioritizing the performance on vector references. The proposed system\nincorporates an additional, non-polluting prefetching mechanism to help\npopulate the long vector cache lines in advance to increase the hit rate by\nfurther exploiting the spatial locality on vector data. Its evaluation was\nconducted on the Cavatools simulator, comparing the performance to a standard\nconventional cache, over different typical vector benchmarks for several vector\nlengths. The results proved the proposed cache speeds up performance on\nstride-1 vector benchmarks, while hardly impacting non-stride-1's. In addition,\nthe prefetching feature consistently provided an additional value."
                },
                "authors": [
                    {
                        "name": "Susana Rebolledo"
                    },
                    {
                        "name": "Borja Perez"
                    },
                    {
                        "name": "Jose Luis Bosque"
                    },
                    {
                        "name": "Peter Hsu"
                    }
                ],
                "author_detail": {
                    "name": "Peter Hsu"
                },
                "author": "Peter Hsu",
                "arxiv_comment": "10 pages, 5 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.15440v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.15440v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.07304v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.07304v1",
                "updated": "2024-08-14T05:42:35Z",
                "updated_parsed": [
                    2024,
                    8,
                    14,
                    5,
                    42,
                    35,
                    2,
                    227,
                    0
                ],
                "published": "2024-08-14T05:42:35Z",
                "published_parsed": [
                    2024,
                    8,
                    14,
                    5,
                    42,
                    35,
                    2,
                    227,
                    0
                ],
                "title": "At Least Factor-of-Two Optimization for RWLE-Based Homomorphic\n  Encryption",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "At Least Factor-of-Two Optimization for RWLE-Based Homomorphic\n  Encryption"
                },
                "summary": "Many modern applications that deal with sensitive data, such as healthcare\nand government services, outsource computation to cloud platforms. In such\nuntrusted environments, privacy is of vital importance. One solution to this\nproblem is homomorphic encryption (HE), a family of cryptographic schemes that\nsupport certain algebraic operations on encrypted data without the need for\ndecryption. However, despite major advancements, encryption in modern HE\nschemes still comes with a non-trivial computational overhead that can hamper\ndata-intensive workloads. To resolve this, recent research has shown that\nleveraging caching techniques, such as Rache, can significantly enhance the\nperformance of HE schemes while maintaining security. Rache unfortunately\ndisplays a key limitation in the time complexity of its caching procedure,\nwhich scales with the size of the plaintext space. Smuche is another caching\nscheme that simultaneously improves the scalability of the caching procedure\nand turns the encryption process into a constant-time operation, utilizing only\na single scalar multiplication. Even still, more can be done. In this paper, we\npresent an encryption method we call ``Zinc\" which entirely forgoes the\nmultiple caching process, replacing it with a single scalar addition, and then\ninjecting randomness that takes constant time with respect to the plaintext\nspace. This injection of randomness is similar to Smuche, and a great\nimprovement from Rache, allowing Zinc to achieve efficiency without\ncompromising security. We implement the scheme using Microsoft SEAL and compare\nits performance to vanilla CKKS.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Many modern applications that deal with sensitive data, such as healthcare\nand government services, outsource computation to cloud platforms. In such\nuntrusted environments, privacy is of vital importance. One solution to this\nproblem is homomorphic encryption (HE), a family of cryptographic schemes that\nsupport certain algebraic operations on encrypted data without the need for\ndecryption. However, despite major advancements, encryption in modern HE\nschemes still comes with a non-trivial computational overhead that can hamper\ndata-intensive workloads. To resolve this, recent research has shown that\nleveraging caching techniques, such as Rache, can significantly enhance the\nperformance of HE schemes while maintaining security. Rache unfortunately\ndisplays a key limitation in the time complexity of its caching procedure,\nwhich scales with the size of the plaintext space. Smuche is another caching\nscheme that simultaneously improves the scalability of the caching procedure\nand turns the encryption process into a constant-time operation, utilizing only\na single scalar multiplication. Even still, more can be done. In this paper, we\npresent an encryption method we call ``Zinc\" which entirely forgoes the\nmultiple caching process, replacing it with a single scalar addition, and then\ninjecting randomness that takes constant time with respect to the plaintext\nspace. This injection of randomness is similar to Smuche, and a great\nimprovement from Rache, allowing Zinc to achieve efficiency without\ncompromising security. We implement the scheme using Microsoft SEAL and compare\nits performance to vanilla CKKS."
                },
                "authors": [
                    {
                        "name": "Jonathan Ly"
                    }
                ],
                "author_detail": {
                    "name": "Jonathan Ly"
                },
                "author": "Jonathan Ly",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.07304v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.07304v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.15743v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.15743v2",
                "updated": "2024-08-13T13:56:14Z",
                "updated_parsed": [
                    2024,
                    8,
                    13,
                    13,
                    56,
                    14,
                    1,
                    226,
                    0
                ],
                "published": "2024-07-22T15:42:59Z",
                "published_parsed": [
                    2024,
                    7,
                    22,
                    15,
                    42,
                    59,
                    0,
                    204,
                    0
                ],
                "title": "Cache-Aided MIMO Communications: DoF Analysis and Transmitter\n  Optimization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cache-Aided MIMO Communications: DoF Analysis and Transmitter\n  Optimization"
                },
                "summary": "Cache-aided MIMO communications aims to jointly exploit both coded\ncaching~(CC) and spatial multiplexing gains to enhance communication\nefficiency. In this paper, we first analyze the achievable degrees of\nfreedom~(DoF) in a MIMO-CC system with CC gain \\(t\\), where a server with \\(L\\)\ntransmit antennas communicates with \\(K\\) users, each equipped with \\(G\\)\nreceive antennas. We demonstrate that the enhanced achievable DoF is\n\\(\\max_{\\beta, \\Omega} \\Omega \\beta\\), where the number of users \\(\\Omega\\)\nserved in each transmission is fine-tuned to maximize DoF, and \\(\\beta \\le\n\\min\\big(G, \\nicefrac{L \\binom{\\Omega-1}{t}}{1 + (\\Omega - t -\n1)\\binom{\\Omega-1}{t}}\\big)\\) represents the number of parallel streams decoded\nby each user. Second, we introduce an effective transmit covariance matrix\ndesign aimed at maximizing the symmetric rate, solved iteratively via\nsuccessive convex approximation. Third, we propose a new class of MIMO-CC\nschemes using a novel scheduling mechanism leveraging maximal multicasting\nopportunities to maximize delivery rates at given SNR levels while adhering to\nlinear processing constraints. Lastly, we devise linear multicast beamforming\nstrategies tailored for the flexible scheduling schemes in MIMO-CC systems and\npresent an iterative solution for the efficient design of beamformers.\nExtensive numerical simulations are used to verify the results of the paper.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cache-aided MIMO communications aims to jointly exploit both coded\ncaching~(CC) and spatial multiplexing gains to enhance communication\nefficiency. In this paper, we first analyze the achievable degrees of\nfreedom~(DoF) in a MIMO-CC system with CC gain \\(t\\), where a server with \\(L\\)\ntransmit antennas communicates with \\(K\\) users, each equipped with \\(G\\)\nreceive antennas. We demonstrate that the enhanced achievable DoF is\n\\(\\max_{\\beta, \\Omega} \\Omega \\beta\\), where the number of users \\(\\Omega\\)\nserved in each transmission is fine-tuned to maximize DoF, and \\(\\beta \\le\n\\min\\big(G, \\nicefrac{L \\binom{\\Omega-1}{t}}{1 + (\\Omega - t -\n1)\\binom{\\Omega-1}{t}}\\big)\\) represents the number of parallel streams decoded\nby each user. Second, we introduce an effective transmit covariance matrix\ndesign aimed at maximizing the symmetric rate, solved iteratively via\nsuccessive convex approximation. Third, we propose a new class of MIMO-CC\nschemes using a novel scheduling mechanism leveraging maximal multicasting\nopportunities to maximize delivery rates at given SNR levels while adhering to\nlinear processing constraints. Lastly, we devise linear multicast beamforming\nstrategies tailored for the flexible scheduling schemes in MIMO-CC systems and\npresent an iterative solution for the efficient design of beamformers.\nExtensive numerical simulations are used to verify the results of the paper."
                },
                "authors": [
                    {
                        "name": "Mohammad NaseriTehrani"
                    },
                    {
                        "name": "MohammadJavad Salehi"
                    },
                    {
                        "name": "Antti Tölli"
                    }
                ],
                "author_detail": {
                    "name": "Antti Tölli"
                },
                "author": "Antti Tölli",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.15743v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.15743v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IT",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.04043v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.04043v3",
                "updated": "2024-08-13T13:31:34Z",
                "updated_parsed": [
                    2024,
                    8,
                    13,
                    13,
                    31,
                    34,
                    1,
                    226,
                    0
                ],
                "published": "2024-08-07T18:51:07Z",
                "published_parsed": [
                    2024,
                    8,
                    7,
                    18,
                    51,
                    7,
                    2,
                    220,
                    0
                ],
                "title": "Ownership in low-level intermediate representation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Ownership in low-level intermediate representation"
                },
                "summary": "The concept of ownership in high level languages can aid both the programmer\nand the compiler to reason about the validity of memory operations. Previously,\nownership semantics has been used successfully in high level automatic program\nverification to model a reference to data by a first order logic (FOL)\nrepresentation of data instead of maintaining an address map. However,\nownership semantics is not used in low level program verification. We have\nidentified two challenges. First, ownership information is lost when a program\nis compiled to a low level intermediate representation (e.g., in LLVM IR).\nSecond, pointers in low level programs point to bytes using an address map\n(e.g., in unsafe Rust) and thus the verification condition (VC) cannot always\nreplace a pointer by its FOL abstraction. To remedy the situation, we develop\nownership semantics for an LLVM like low level intermediate representation.\nUsing these semantics, the VC can opportunistically model some memory accesses\nby a direct access of a pointer cache that stores byte representation of data.\nThis scheme reduces instances where an address map must be maintained,\nespecially for mostly safe programs that follow ownership semantics. For unsafe\nfunctionality, memory accesses are modelled by operations on an address map and\nwe provide mechanisms to keep the address map and pointer cache in sync. We\nimplement these semantics in SEABMC, a bit precise bounded model checker for\nLLVM. For evaluation, the source programs are assumed to be written in C. Since\nC does not have ownership built in, suitable macros are added that introduce\nand preserve ownership during translation to LLVM like IR for verification.\nThis approach is evaluated on mature open source C code. For both handcrafted\nbenchmarks and practical programs, we observe a speedup of $1.3x-5x$ during SMT\nsolving.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The concept of ownership in high level languages can aid both the programmer\nand the compiler to reason about the validity of memory operations. Previously,\nownership semantics has been used successfully in high level automatic program\nverification to model a reference to data by a first order logic (FOL)\nrepresentation of data instead of maintaining an address map. However,\nownership semantics is not used in low level program verification. We have\nidentified two challenges. First, ownership information is lost when a program\nis compiled to a low level intermediate representation (e.g., in LLVM IR).\nSecond, pointers in low level programs point to bytes using an address map\n(e.g., in unsafe Rust) and thus the verification condition (VC) cannot always\nreplace a pointer by its FOL abstraction. To remedy the situation, we develop\nownership semantics for an LLVM like low level intermediate representation.\nUsing these semantics, the VC can opportunistically model some memory accesses\nby a direct access of a pointer cache that stores byte representation of data.\nThis scheme reduces instances where an address map must be maintained,\nespecially for mostly safe programs that follow ownership semantics. For unsafe\nfunctionality, memory accesses are modelled by operations on an address map and\nwe provide mechanisms to keep the address map and pointer cache in sync. We\nimplement these semantics in SEABMC, a bit precise bounded model checker for\nLLVM. For evaluation, the source programs are assumed to be written in C. Since\nC does not have ownership built in, suitable macros are added that introduce\nand preserve ownership during translation to LLVM like IR for verification.\nThis approach is evaluated on mature open source C code. For both handcrafted\nbenchmarks and practical programs, we observe a speedup of $1.3x-5x$ during SMT\nsolving."
                },
                "authors": [
                    {
                        "name": "Siddharth Priya"
                    },
                    {
                        "name": "Arie Gurfinkel"
                    }
                ],
                "author_detail": {
                    "name": "Arie Gurfinkel"
                },
                "author": "Arie Gurfinkel",
                "arxiv_comment": "FMCAD 2024 conference",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.04043v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.04043v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.PL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.PL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "D.2.4",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.18003v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.18003v3",
                "updated": "2024-08-13T09:55:43Z",
                "updated_parsed": [
                    2024,
                    8,
                    13,
                    9,
                    55,
                    43,
                    1,
                    226,
                    0
                ],
                "published": "2024-07-25T12:56:22Z",
                "published_parsed": [
                    2024,
                    7,
                    25,
                    12,
                    56,
                    22,
                    3,
                    207,
                    0
                ],
                "title": "Keep the Cost Down: A Review on Methods to Optimize LLM' s KV-Cache\n  Consumption",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Keep the Cost Down: A Review on Methods to Optimize LLM' s KV-Cache\n  Consumption"
                },
                "summary": "Large Language Models (LLMs), epitomized by ChatGPT' s release in late 2022,\nhave revolutionized various industries with their advanced language\ncomprehension. However, their efficiency is challenged by the Transformer\narchitecture' s struggle with handling long texts. KV-Cache has emerged as a\npivotal solution to this issue, converting the time complexity of token\ngeneration from quadratic to linear, albeit with increased GPU memory overhead\nproportional to conversation length. With the development of the LLM community\nand academia, various KV-Cache compression methods have been proposed. In this\nreview, we dissect the various properties of KV-Cache and elaborate on various\nmethods currently used to optimize the KV-Cache space usage of LLMs. These\nmethods span the pre-training phase, deployment phase, and inference phase, and\nwe summarize the commonalities and differences among these methods.\nAdditionally, we list some metrics for evaluating the long-text capabilities of\nlarge language models, from both efficiency and capability perspectives. Our\nreview thus sheds light on the evolving landscape of LLM optimization, offering\ninsights into future advancements in this dynamic field.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs), epitomized by ChatGPT' s release in late 2022,\nhave revolutionized various industries with their advanced language\ncomprehension. However, their efficiency is challenged by the Transformer\narchitecture' s struggle with handling long texts. KV-Cache has emerged as a\npivotal solution to this issue, converting the time complexity of token\ngeneration from quadratic to linear, albeit with increased GPU memory overhead\nproportional to conversation length. With the development of the LLM community\nand academia, various KV-Cache compression methods have been proposed. In this\nreview, we dissect the various properties of KV-Cache and elaborate on various\nmethods currently used to optimize the KV-Cache space usage of LLMs. These\nmethods span the pre-training phase, deployment phase, and inference phase, and\nwe summarize the commonalities and differences among these methods.\nAdditionally, we list some metrics for evaluating the long-text capabilities of\nlarge language models, from both efficiency and capability perspectives. Our\nreview thus sheds light on the evolving landscape of LLM optimization, offering\ninsights into future advancements in this dynamic field."
                },
                "authors": [
                    {
                        "name": "Luohe Shi"
                    },
                    {
                        "name": "Hongyi Zhang"
                    },
                    {
                        "name": "Yao Yao"
                    },
                    {
                        "name": "Zuchao Li"
                    },
                    {
                        "name": "Hai Zhao"
                    }
                ],
                "author_detail": {
                    "name": "Hai Zhao"
                },
                "author": "Hai Zhao",
                "arxiv_comment": "to be published in CoLM 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.18003v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.18003v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            }
        ]
    },
    {
        "keyword": "LLM Inference",
        "arxiv_results": [
            {
                "id": "http://arxiv.org/abs/2409.12194v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.12194v3",
                "updated": "2024-09-20T04:38:21Z",
                "updated_parsed": [
                    2024,
                    9,
                    20,
                    4,
                    38,
                    21,
                    4,
                    264,
                    0
                ],
                "published": "2024-09-18T17:59:52Z",
                "published_parsed": [
                    2024,
                    9,
                    18,
                    17,
                    59,
                    52,
                    2,
                    262,
                    0
                ],
                "title": "Gender Representation and Bias in Indian Civil Service Mock Interviews",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Gender Representation and Bias in Indian Civil Service Mock Interviews"
                },
                "summary": "This paper makes three key contributions. First, via a substantial corpus of\n51,278 interview questions sourced from 888 YouTube videos of mock interviews\nof Indian civil service candidates, we demonstrate stark gender bias in the\nbroad nature of questions asked to male and female candidates. Second, our\nexperiments with large language models show a strong presence of gender bias in\nexplanations provided by the LLMs on the gender inference task. Finally, we\npresent a novel dataset of 51,278 interview questions that can inform future\nsocial science studies.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper makes three key contributions. First, via a substantial corpus of\n51,278 interview questions sourced from 888 YouTube videos of mock interviews\nof Indian civil service candidates, we demonstrate stark gender bias in the\nbroad nature of questions asked to male and female candidates. Second, our\nexperiments with large language models show a strong presence of gender bias in\nexplanations provided by the LLMs on the gender inference task. Finally, we\npresent a novel dataset of 51,278 interview questions that can inform future\nsocial science studies."
                },
                "authors": [
                    {
                        "name": "Somonnoy Banerjee"
                    },
                    {
                        "name": "Sujan Dutta"
                    },
                    {
                        "name": "Soumyajit Datta"
                    },
                    {
                        "name": "Ashiqur R. KhudaBukhsh"
                    }
                ],
                "author_detail": {
                    "name": "Ashiqur R. KhudaBukhsh"
                },
                "author": "Ashiqur R. KhudaBukhsh",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.12194v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.12194v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.12183v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.12183v1",
                "updated": "2024-09-18T17:55:00Z",
                "updated_parsed": [
                    2024,
                    9,
                    18,
                    17,
                    55,
                    0,
                    2,
                    262,
                    0
                ],
                "published": "2024-09-18T17:55:00Z",
                "published_parsed": [
                    2024,
                    9,
                    18,
                    17,
                    55,
                    0,
                    2,
                    262,
                    0
                ],
                "title": "To CoT or not to CoT? Chain-of-thought helps mainly on math and symbolic\n  reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "To CoT or not to CoT? Chain-of-thought helps mainly on math and symbolic\n  reasoning"
                },
                "summary": "Chain-of-thought (CoT) via prompting is the de facto method for eliciting\nreasoning capabilities from large language models (LLMs). But for what kinds of\ntasks is this extra ``thinking'' really helpful? To analyze this, we conducted\na quantitative meta-analysis covering over 100 papers using CoT and ran our own\nevaluations of 20 datasets across 14 models. Our results show that CoT gives\nstrong performance benefits primarily on tasks involving math or logic, with\nmuch smaller gains on other types of tasks. On MMLU, directly generating the\nanswer without CoT leads to almost identical accuracy as CoT unless the\nquestion or model's response contains an equals sign, indicating symbolic\noperations and reasoning. Following this finding, we analyze the behavior of\nCoT on these problems by separating planning and execution and comparing\nagainst tool-augmented LLMs. Much of CoT's gain comes from improving symbolic\nexecution, but it underperforms relative to using a symbolic solver. Our\nresults indicate that CoT can be applied selectively, maintaining performance\nwhile saving inference costs. Furthermore, they suggest a need to move beyond\nprompt-based CoT to new paradigms that better leverage intermediate computation\nacross the whole range of LLM applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Chain-of-thought (CoT) via prompting is the de facto method for eliciting\nreasoning capabilities from large language models (LLMs). But for what kinds of\ntasks is this extra ``thinking'' really helpful? To analyze this, we conducted\na quantitative meta-analysis covering over 100 papers using CoT and ran our own\nevaluations of 20 datasets across 14 models. Our results show that CoT gives\nstrong performance benefits primarily on tasks involving math or logic, with\nmuch smaller gains on other types of tasks. On MMLU, directly generating the\nanswer without CoT leads to almost identical accuracy as CoT unless the\nquestion or model's response contains an equals sign, indicating symbolic\noperations and reasoning. Following this finding, we analyze the behavior of\nCoT on these problems by separating planning and execution and comparing\nagainst tool-augmented LLMs. Much of CoT's gain comes from improving symbolic\nexecution, but it underperforms relative to using a symbolic solver. Our\nresults indicate that CoT can be applied selectively, maintaining performance\nwhile saving inference costs. Furthermore, they suggest a need to move beyond\nprompt-based CoT to new paradigms that better leverage intermediate computation\nacross the whole range of LLM applications."
                },
                "authors": [
                    {
                        "name": "Zayne Sprague"
                    },
                    {
                        "name": "Fangcong Yin"
                    },
                    {
                        "name": "Juan Diego Rodriguez"
                    },
                    {
                        "name": "Dongwei Jiang"
                    },
                    {
                        "name": "Manya Wadhwa"
                    },
                    {
                        "name": "Prasann Singhal"
                    },
                    {
                        "name": "Xinyu Zhao"
                    },
                    {
                        "name": "Xi Ye"
                    },
                    {
                        "name": "Kyle Mahowald"
                    },
                    {
                        "name": "Greg Durrett"
                    }
                ],
                "author_detail": {
                    "name": "Greg Durrett"
                },
                "author": "Greg Durrett",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.12183v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.12183v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.12181v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.12181v2",
                "updated": "2024-09-23T14:39:07Z",
                "updated_parsed": [
                    2024,
                    9,
                    23,
                    14,
                    39,
                    7,
                    0,
                    267,
                    0
                ],
                "published": "2024-09-18T17:53:17Z",
                "published_parsed": [
                    2024,
                    9,
                    18,
                    17,
                    53,
                    17,
                    2,
                    262,
                    0
                ],
                "title": "A Controlled Study on Long Context Extension and Generalization in LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Controlled Study on Long Context Extension and Generalization in LLMs"
                },
                "summary": "Broad textual understanding and in-context learning require language models\nthat utilize full document contexts. Due to the implementation challenges\nassociated with directly training long-context models, many methods have been\nproposed for extending models to handle long contexts. However, owing to\ndifferences in data and model classes, it has been challenging to compare these\napproaches, leading to uncertainty as to how to evaluate long-context\nperformance and whether it differs from standard evaluation. We implement a\ncontrolled protocol for extension methods with a standardized evaluation,\nutilizing consistent base models and extension data. Our study yields several\ninsights into long-context behavior. First, we reaffirm the critical role of\nperplexity as a general-purpose performance indicator even in longer-context\ntasks. Second, we find that current approximate attention methods\nsystematically underperform across long-context tasks. Finally, we confirm that\nexact fine-tuning based methods are generally effective within the range of\ntheir extension, whereas extrapolation remains challenging. All codebases,\nmodels, and checkpoints will be made available open-source, promoting\ntransparency and facilitating further research in this critical area of AI\ndevelopment.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Broad textual understanding and in-context learning require language models\nthat utilize full document contexts. Due to the implementation challenges\nassociated with directly training long-context models, many methods have been\nproposed for extending models to handle long contexts. However, owing to\ndifferences in data and model classes, it has been challenging to compare these\napproaches, leading to uncertainty as to how to evaluate long-context\nperformance and whether it differs from standard evaluation. We implement a\ncontrolled protocol for extension methods with a standardized evaluation,\nutilizing consistent base models and extension data. Our study yields several\ninsights into long-context behavior. First, we reaffirm the critical role of\nperplexity as a general-purpose performance indicator even in longer-context\ntasks. Second, we find that current approximate attention methods\nsystematically underperform across long-context tasks. Finally, we confirm that\nexact fine-tuning based methods are generally effective within the range of\ntheir extension, whereas extrapolation remains challenging. All codebases,\nmodels, and checkpoints will be made available open-source, promoting\ntransparency and facilitating further research in this critical area of AI\ndevelopment."
                },
                "authors": [
                    {
                        "name": "Yi Lu"
                    },
                    {
                        "name": "Jing Nathan Yan"
                    },
                    {
                        "name": "Songlin Yang"
                    },
                    {
                        "name": "Justin T. Chiu"
                    },
                    {
                        "name": "Siyu Ren"
                    },
                    {
                        "name": "Fei Yuan"
                    },
                    {
                        "name": "Wenting Zhao"
                    },
                    {
                        "name": "Zhiyong Wu"
                    },
                    {
                        "name": "Alexander M. Rush"
                    }
                ],
                "author_detail": {
                    "name": "Alexander M. Rush"
                },
                "author": "Alexander M. Rush",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.12181v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.12181v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.12180v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.12180v1",
                "updated": "2024-09-18T17:52:53Z",
                "updated_parsed": [
                    2024,
                    9,
                    18,
                    17,
                    52,
                    53,
                    2,
                    262,
                    0
                ],
                "published": "2024-09-18T17:52:53Z",
                "published_parsed": [
                    2024,
                    9,
                    18,
                    17,
                    52,
                    53,
                    2,
                    262,
                    0
                ],
                "title": "Finetuning Language Models to Emit Linguistic Expressions of Uncertainty",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Finetuning Language Models to Emit Linguistic Expressions of Uncertainty"
                },
                "summary": "Large language models (LLMs) are increasingly employed in information-seeking\nand decision-making tasks. Despite their broad utility, LLMs tend to generate\ninformation that conflicts with real-world facts, and their persuasive style\ncan make these inaccuracies appear confident and convincing. As a result,\nend-users struggle to consistently align the confidence expressed by LLMs with\nthe accuracy of their predictions, often leading to either blind trust in all\noutputs or a complete disregard for their reliability. In this work, we explore\nsupervised finetuning on uncertainty-augmented predictions as a method to\ndevelop models that produce linguistic expressions of uncertainty.\nSpecifically, we measure the calibration of pre-trained models and then\nfine-tune language models to generate calibrated linguistic expressions of\nuncertainty. Through experiments on various question-answering datasets, we\ndemonstrate that LLMs are well-calibrated in assessing their predictions, and\nsupervised finetuning based on the model's own confidence leads to\nwell-calibrated expressions of uncertainty, particularly for single-claim\nanswers.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) are increasingly employed in information-seeking\nand decision-making tasks. Despite their broad utility, LLMs tend to generate\ninformation that conflicts with real-world facts, and their persuasive style\ncan make these inaccuracies appear confident and convincing. As a result,\nend-users struggle to consistently align the confidence expressed by LLMs with\nthe accuracy of their predictions, often leading to either blind trust in all\noutputs or a complete disregard for their reliability. In this work, we explore\nsupervised finetuning on uncertainty-augmented predictions as a method to\ndevelop models that produce linguistic expressions of uncertainty.\nSpecifically, we measure the calibration of pre-trained models and then\nfine-tune language models to generate calibrated linguistic expressions of\nuncertainty. Through experiments on various question-answering datasets, we\ndemonstrate that LLMs are well-calibrated in assessing their predictions, and\nsupervised finetuning based on the model's own confidence leads to\nwell-calibrated expressions of uncertainty, particularly for single-claim\nanswers."
                },
                "authors": [
                    {
                        "name": "Arslan Chaudhry"
                    },
                    {
                        "name": "Sridhar Thiagarajan"
                    },
                    {
                        "name": "Dilan Gorur"
                    }
                ],
                "author_detail": {
                    "name": "Dilan Gorur"
                },
                "author": "Dilan Gorur",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.12180v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.12180v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.09249v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.09249v2",
                "updated": "2024-09-18T17:44:08Z",
                "updated_parsed": [
                    2024,
                    9,
                    18,
                    17,
                    44,
                    8,
                    2,
                    262,
                    0
                ],
                "published": "2024-09-14T01:21:56Z",
                "published_parsed": [
                    2024,
                    9,
                    14,
                    1,
                    21,
                    56,
                    5,
                    258,
                    0
                ],
                "title": "NovAScore: A New Automated Metric for Evaluating Document Level Novelty",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "NovAScore: A New Automated Metric for Evaluating Document Level Novelty"
                },
                "summary": "The rapid expansion of online content has intensified the issue of\ninformation redundancy, underscoring the need for solutions that can identify\ngenuinely new information. Despite this challenge, the research community has\nseen a decline in focus on novelty detection, particularly with the rise of\nlarge language models (LLMs). Additionally, previous approaches have relied\nheavily on human annotation, which is time-consuming, costly, and particularly\nchallenging when annotators must compare a target document against a vast\nnumber of historical documents. In this work, we introduce NovAScore (Novelty\nEvaluation in Atomicity Score), an automated metric for evaluating\ndocument-level novelty. NovAScore aggregates the novelty and salience scores of\natomic information, providing high interpretability and a detailed analysis of\na document's novelty. With its dynamic weight adjustment scheme, NovAScore\noffers enhanced flexibility and an additional dimension to assess both the\nnovelty level and the importance of information within a document. Our\nexperiments show that NovAScore strongly correlates with human judgments of\nnovelty, achieving a 0.626 Point-Biserial correlation on the TAP-DLND 1.0\ndataset and a 0.920 Pearson correlation on an internal human-annotated dataset.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rapid expansion of online content has intensified the issue of\ninformation redundancy, underscoring the need for solutions that can identify\ngenuinely new information. Despite this challenge, the research community has\nseen a decline in focus on novelty detection, particularly with the rise of\nlarge language models (LLMs). Additionally, previous approaches have relied\nheavily on human annotation, which is time-consuming, costly, and particularly\nchallenging when annotators must compare a target document against a vast\nnumber of historical documents. In this work, we introduce NovAScore (Novelty\nEvaluation in Atomicity Score), an automated metric for evaluating\ndocument-level novelty. NovAScore aggregates the novelty and salience scores of\natomic information, providing high interpretability and a detailed analysis of\na document's novelty. With its dynamic weight adjustment scheme, NovAScore\noffers enhanced flexibility and an additional dimension to assess both the\nnovelty level and the importance of information within a document. Our\nexperiments show that NovAScore strongly correlates with human judgments of\nnovelty, achieving a 0.626 Point-Biserial correlation on the TAP-DLND 1.0\ndataset and a 0.920 Pearson correlation on an internal human-annotated dataset."
                },
                "authors": [
                    {
                        "name": "Lin Ai"
                    },
                    {
                        "name": "Ziwei Gong"
                    },
                    {
                        "name": "Harshsaiprasad Deshpande"
                    },
                    {
                        "name": "Alexander Johnson"
                    },
                    {
                        "name": "Emmy Phung"
                    },
                    {
                        "name": "Ahmad Emami"
                    },
                    {
                        "name": "Julia Hirschberg"
                    }
                ],
                "author_detail": {
                    "name": "Julia Hirschberg"
                },
                "author": "Julia Hirschberg",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.09249v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.09249v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.12173v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.12173v1",
                "updated": "2024-09-18T17:41:11Z",
                "updated_parsed": [
                    2024,
                    9,
                    18,
                    17,
                    41,
                    11,
                    2,
                    262,
                    0
                ],
                "published": "2024-09-18T17:41:11Z",
                "published_parsed": [
                    2024,
                    9,
                    18,
                    17,
                    41,
                    11,
                    2,
                    262,
                    0
                ],
                "title": "Poisson approximate likelihood compared to the particle filter",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Poisson approximate likelihood compared to the particle filter"
                },
                "summary": "Filtering algorithms are fundamental for inference on partially observed\nstochastic dynamic systems, since they provide access to the likelihood\nfunction and hence enable likelihood-based or Bayesian inference. A novel\nPoisson approximate likelihood (PAL) filter was introduced by Whitehouse et al.\n(2023). PAL employs a Poisson approximation to conditional densities, offering\na fast approximation to the likelihood function for a certain subset of\npartially observed Markov process models. A central piece of evidence for PAL\nis the comparison in Table 1 of Whitehouse et al. (2023), which claims a large\nimprovement for PAL over a standard particle filter algorithm. This evidence,\nbased on a model and data from a previous scientific study by Stocks et al.\n(2020), might suggest that researchers confronted with similar models should\nuse PAL rather than particle filter methods. Taken at face value, this evidence\nalso reduces the credibility of Stocks et al. (2020) by indicating a\nshortcoming with the numerical methods that they used. However, we show that\nthe comparison of log-likelihood values made by Whitehouse et al. (2023) is\nflawed because their PAL calculations were carried out using a dataset scaled\ndifferently from the previous study. If PAL and the particle filter are applied\nto the same data, the advantage claimed for PAL disappears. On simulations\nwhere the model is correctly specified, the particle filter outperforms PAL.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Filtering algorithms are fundamental for inference on partially observed\nstochastic dynamic systems, since they provide access to the likelihood\nfunction and hence enable likelihood-based or Bayesian inference. A novel\nPoisson approximate likelihood (PAL) filter was introduced by Whitehouse et al.\n(2023). PAL employs a Poisson approximation to conditional densities, offering\na fast approximation to the likelihood function for a certain subset of\npartially observed Markov process models. A central piece of evidence for PAL\nis the comparison in Table 1 of Whitehouse et al. (2023), which claims a large\nimprovement for PAL over a standard particle filter algorithm. This evidence,\nbased on a model and data from a previous scientific study by Stocks et al.\n(2020), might suggest that researchers confronted with similar models should\nuse PAL rather than particle filter methods. Taken at face value, this evidence\nalso reduces the credibility of Stocks et al. (2020) by indicating a\nshortcoming with the numerical methods that they used. However, we show that\nthe comparison of log-likelihood values made by Whitehouse et al. (2023) is\nflawed because their PAL calculations were carried out using a dataset scaled\ndifferently from the previous study. If PAL and the particle filter are applied\nto the same data, the advantage claimed for PAL disappears. On simulations\nwhere the model is correctly specified, the particle filter outperforms PAL."
                },
                "authors": [
                    {
                        "name": "Yize Hao"
                    },
                    {
                        "name": "Aaron A. Abkemeier"
                    },
                    {
                        "name": "Edward L. Ionides"
                    }
                ],
                "author_detail": {
                    "name": "Edward L. Ionides"
                },
                "author": "Edward L. Ionides",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.12173v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.12173v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ME",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.12172v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.12172v1",
                "updated": "2024-09-18T17:38:25Z",
                "updated_parsed": [
                    2024,
                    9,
                    18,
                    17,
                    38,
                    25,
                    2,
                    262,
                    0
                ],
                "published": "2024-09-18T17:38:25Z",
                "published_parsed": [
                    2024,
                    9,
                    18,
                    17,
                    38,
                    25,
                    2,
                    262,
                    0
                ],
                "title": "You Only Read Once (YORO): Learning to Internalize Database Knowledge\n  for Text-to-SQL",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "You Only Read Once (YORO): Learning to Internalize Database Knowledge\n  for Text-to-SQL"
                },
                "summary": "While significant progress has been made on the text-to-SQL task, recent\nsolutions repeatedly encode the same database schema for every question,\nresulting in unnecessary high inference cost and often overlooking crucial\ndatabase knowledge. To address these issues, we propose You Only Read Once\n(YORO), a novel paradigm that directly internalizes database knowledge into the\nparametric knowledge of a text-to-SQL model during training and eliminates the\nneed for schema encoding during inference. YORO significantly reduces the input\ntoken length by 66%-98%. Despite its shorter inputs, our empirical results\ndemonstrate YORO's competitive performances with traditional systems on three\nbenchmarks as well as its significant outperformance on large databases.\nFurthermore, YORO excels in handling questions with challenging value\nretrievals such as abbreviation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While significant progress has been made on the text-to-SQL task, recent\nsolutions repeatedly encode the same database schema for every question,\nresulting in unnecessary high inference cost and often overlooking crucial\ndatabase knowledge. To address these issues, we propose You Only Read Once\n(YORO), a novel paradigm that directly internalizes database knowledge into the\nparametric knowledge of a text-to-SQL model during training and eliminates the\nneed for schema encoding during inference. YORO significantly reduces the input\ntoken length by 66%-98%. Despite its shorter inputs, our empirical results\ndemonstrate YORO's competitive performances with traditional systems on three\nbenchmarks as well as its significant outperformance on large databases.\nFurthermore, YORO excels in handling questions with challenging value\nretrievals such as abbreviation."
                },
                "authors": [
                    {
                        "name": "Hideo Kobayashi"
                    },
                    {
                        "name": "Wuwei Lan"
                    },
                    {
                        "name": "Peng Shi"
                    },
                    {
                        "name": "Shuaichen Chang"
                    },
                    {
                        "name": "Jiang Guo"
                    },
                    {
                        "name": "Henghui Zhu"
                    },
                    {
                        "name": "Zhiguo Wang"
                    },
                    {
                        "name": "Patrick Ng"
                    }
                ],
                "author_detail": {
                    "name": "Patrick Ng"
                },
                "author": "Patrick Ng",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.12172v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.12172v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.10289v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.10289v2",
                "updated": "2024-09-18T17:30:50Z",
                "updated_parsed": [
                    2024,
                    9,
                    18,
                    17,
                    30,
                    50,
                    2,
                    262,
                    0
                ],
                "published": "2024-09-16T13:56:17Z",
                "published_parsed": [
                    2024,
                    9,
                    16,
                    13,
                    56,
                    17,
                    0,
                    260,
                    0
                ],
                "title": "ReflectDiffu:Reflect between Emotion-intent Contagion and Mimicry for\n  Empathetic Response Generation via a RL-Diffusion Framework",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ReflectDiffu:Reflect between Emotion-intent Contagion and Mimicry for\n  Empathetic Response Generation via a RL-Diffusion Framework"
                },
                "summary": "Empathetic response generation necessitates the integration of emotional and\nintentional dynamics to foster meaningful interactions. Existing research\neither neglects the intricate interplay between emotion and intent, leading to\nsuboptimal controllability of empathy, or resorts to large language models\n(LLMs), which incur significant computational overhead. In this paper, we\nintroduce ReflectDiffu, a lightweight and comprehensive framework for\nempathetic response generation. This framework incorporates emotion contagion\nto augment emotional expressiveness and employs an emotion-reasoning mask to\npinpoint critical emotional elements. Additionally, it integrates intent\nmimicry within reinforcement learning for refinement during diffusion. By\nharnessing an intent twice reflect the mechanism of\nExploring-Sampling-Correcting, ReflectDiffu adeptly translates emotional\ndecision-making into precise intent actions, thereby addressing empathetic\nresponse misalignments stemming from emotional misrecognition. Through\nreflection, the framework maps emotional states to intents, markedly enhancing\nboth response empathy and flexibility. Comprehensive experiments reveal that\nReflectDiffu outperforms existing models regarding relevance, controllability,\nand informativeness, achieving state-of-the-art results in both automatic and\nhuman evaluations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Empathetic response generation necessitates the integration of emotional and\nintentional dynamics to foster meaningful interactions. Existing research\neither neglects the intricate interplay between emotion and intent, leading to\nsuboptimal controllability of empathy, or resorts to large language models\n(LLMs), which incur significant computational overhead. In this paper, we\nintroduce ReflectDiffu, a lightweight and comprehensive framework for\nempathetic response generation. This framework incorporates emotion contagion\nto augment emotional expressiveness and employs an emotion-reasoning mask to\npinpoint critical emotional elements. Additionally, it integrates intent\nmimicry within reinforcement learning for refinement during diffusion. By\nharnessing an intent twice reflect the mechanism of\nExploring-Sampling-Correcting, ReflectDiffu adeptly translates emotional\ndecision-making into precise intent actions, thereby addressing empathetic\nresponse misalignments stemming from emotional misrecognition. Through\nreflection, the framework maps emotional states to intents, markedly enhancing\nboth response empathy and flexibility. Comprehensive experiments reveal that\nReflectDiffu outperforms existing models regarding relevance, controllability,\nand informativeness, achieving state-of-the-art results in both automatic and\nhuman evaluations."
                },
                "authors": [
                    {
                        "name": "Jiahao Yuan"
                    },
                    {
                        "name": "Zixiang Di"
                    },
                    {
                        "name": "Zhiqing Cui"
                    },
                    {
                        "name": "Guisong Yang"
                    },
                    {
                        "name": "Usman Naseem"
                    }
                ],
                "author_detail": {
                    "name": "Usman Naseem"
                },
                "author": "Usman Naseem",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.10289v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.10289v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.12150v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.12150v1",
                "updated": "2024-09-18T17:15:06Z",
                "updated_parsed": [
                    2024,
                    9,
                    18,
                    17,
                    15,
                    6,
                    2,
                    262,
                    0
                ],
                "published": "2024-09-18T17:15:06Z",
                "published_parsed": [
                    2024,
                    9,
                    18,
                    17,
                    15,
                    6,
                    2,
                    262,
                    0
                ],
                "title": "Decoding Style: Efficient Fine-Tuning of LLMs for Image-Guided Outfit\n  Recommendation with Preference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Decoding Style: Efficient Fine-Tuning of LLMs for Image-Guided Outfit\n  Recommendation with Preference"
                },
                "summary": "Personalized outfit recommendation remains a complex challenge, demanding\nboth fashion compatibility understanding and trend awareness. This paper\npresents a novel framework that harnesses the expressive power of large\nlanguage models (LLMs) for this task, mitigating their \"black box\" and static\nnature through fine-tuning and direct feedback integration. We bridge the item\nvisual-textual gap in items descriptions by employing image captioning with a\nMultimodal Large Language Model (MLLM). This enables the LLM to extract style\nand color characteristics from human-curated fashion images, forming the basis\nfor personalized recommendations. The LLM is efficiently fine-tuned on the\nopen-source Polyvore dataset of curated fashion images, optimizing its ability\nto recommend stylish outfits. A direct preference mechanism using negative\nexamples is employed to enhance the LLM's decision-making process. This creates\na self-enhancing AI feedback loop that continuously refines recommendations in\nline with seasonal fashion trends. Our framework is evaluated on the Polyvore\ndataset, demonstrating its effectiveness in two key tasks: fill-in-the-blank,\nand complementary item retrieval. These evaluations underline the framework's\nability to generate stylish, trend-aligned outfit suggestions, continuously\nimproving through direct feedback. The evaluation results demonstrated that our\nproposed framework significantly outperforms the base LLM, creating more\ncohesive outfits. The improved performance in these tasks underscores the\nproposed framework's potential to enhance the shopping experience with accurate\nsuggestions, proving its effectiveness over the vanilla LLM based outfit\ngeneration.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Personalized outfit recommendation remains a complex challenge, demanding\nboth fashion compatibility understanding and trend awareness. This paper\npresents a novel framework that harnesses the expressive power of large\nlanguage models (LLMs) for this task, mitigating their \"black box\" and static\nnature through fine-tuning and direct feedback integration. We bridge the item\nvisual-textual gap in items descriptions by employing image captioning with a\nMultimodal Large Language Model (MLLM). This enables the LLM to extract style\nand color characteristics from human-curated fashion images, forming the basis\nfor personalized recommendations. The LLM is efficiently fine-tuned on the\nopen-source Polyvore dataset of curated fashion images, optimizing its ability\nto recommend stylish outfits. A direct preference mechanism using negative\nexamples is employed to enhance the LLM's decision-making process. This creates\na self-enhancing AI feedback loop that continuously refines recommendations in\nline with seasonal fashion trends. Our framework is evaluated on the Polyvore\ndataset, demonstrating its effectiveness in two key tasks: fill-in-the-blank,\nand complementary item retrieval. These evaluations underline the framework's\nability to generate stylish, trend-aligned outfit suggestions, continuously\nimproving through direct feedback. The evaluation results demonstrated that our\nproposed framework significantly outperforms the base LLM, creating more\ncohesive outfits. The improved performance in these tasks underscores the\nproposed framework's potential to enhance the shopping experience with accurate\nsuggestions, proving its effectiveness over the vanilla LLM based outfit\ngeneration."
                },
                "authors": [
                    {
                        "name": "Najmeh Forouzandehmehr"
                    },
                    {
                        "name": "Nima Farrokhsiar"
                    },
                    {
                        "name": "Ramin Giahi"
                    },
                    {
                        "name": "Evren Korpeoglu"
                    },
                    {
                        "name": "Kannan Achan"
                    }
                ],
                "author_detail": {
                    "name": "Kannan Achan"
                },
                "author": "Kannan Achan",
                "arxiv_comment": "CIKM 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.12150v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.12150v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.12147v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.12147v1",
                "updated": "2024-09-18T17:12:41Z",
                "updated_parsed": [
                    2024,
                    9,
                    18,
                    17,
                    12,
                    41,
                    2,
                    262,
                    0
                ],
                "published": "2024-09-18T17:12:41Z",
                "published_parsed": [
                    2024,
                    9,
                    18,
                    17,
                    12,
                    41,
                    2,
                    262,
                    0
                ],
                "title": "MAgICoRe: Multi-Agent, Iterative, Coarse-to-Fine Refinement for\n  Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MAgICoRe: Multi-Agent, Iterative, Coarse-to-Fine Refinement for\n  Reasoning"
                },
                "summary": "Large Language Models' (LLM) reasoning can be improved using test-time\naggregation strategies, i.e., generating multiple samples and voting among\ngenerated samples. While these improve performance, they often reach a\nsaturation point. Refinement offers an alternative by using LLM-generated\nfeedback to improve solution quality. However, refinement introduces 3 key\nchallenges: (1) Excessive refinement: Uniformly refining all instances can\nover-correct and reduce the overall performance. (2) Inability to localize and\naddress errors: LLMs have a limited ability to self-correct and struggle to\nidentify and correct their own mistakes. (3) Insufficient refinement: Deciding\nhow many iterations of refinement are needed is non-trivial, and stopping too\nsoon could leave errors unaddressed. To tackle these issues, we propose\nMAgICoRe, which avoids excessive refinement by categorizing problem difficulty\nas easy or hard, solving easy problems with coarse-grained aggregation and hard\nones with fine-grained and iterative multi-agent refinement. To improve error\nlocalization, we incorporate external step-wise reward model (RM) scores.\nMoreover, to ensure effective refinement, we employ a multi-agent loop with\nthree agents: Solver, Reviewer (which generates targeted feedback based on\nstep-wise RM scores), and the Refiner (which incorporates feedback). To ensure\nsufficient refinement, we re-evaluate updated solutions, iteratively initiating\nfurther rounds of refinement. We evaluate MAgICoRe on Llama-3-8B and GPT-3.5\nand show its effectiveness across 5 math datasets. Even one iteration of\nMAgICoRe beats Self-Consistency by 3.4%, Best-of-k by 3.2%, and Self-Refine by\n4.0% while using less than half the samples. Unlike iterative refinement with\nbaselines, MAgICoRe continues to improve with more iterations. Finally, our\nablations highlight the importance of MAgICoRe's RMs and multi-agent\ncommunication.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models' (LLM) reasoning can be improved using test-time\naggregation strategies, i.e., generating multiple samples and voting among\ngenerated samples. While these improve performance, they often reach a\nsaturation point. Refinement offers an alternative by using LLM-generated\nfeedback to improve solution quality. However, refinement introduces 3 key\nchallenges: (1) Excessive refinement: Uniformly refining all instances can\nover-correct and reduce the overall performance. (2) Inability to localize and\naddress errors: LLMs have a limited ability to self-correct and struggle to\nidentify and correct their own mistakes. (3) Insufficient refinement: Deciding\nhow many iterations of refinement are needed is non-trivial, and stopping too\nsoon could leave errors unaddressed. To tackle these issues, we propose\nMAgICoRe, which avoids excessive refinement by categorizing problem difficulty\nas easy or hard, solving easy problems with coarse-grained aggregation and hard\nones with fine-grained and iterative multi-agent refinement. To improve error\nlocalization, we incorporate external step-wise reward model (RM) scores.\nMoreover, to ensure effective refinement, we employ a multi-agent loop with\nthree agents: Solver, Reviewer (which generates targeted feedback based on\nstep-wise RM scores), and the Refiner (which incorporates feedback). To ensure\nsufficient refinement, we re-evaluate updated solutions, iteratively initiating\nfurther rounds of refinement. We evaluate MAgICoRe on Llama-3-8B and GPT-3.5\nand show its effectiveness across 5 math datasets. Even one iteration of\nMAgICoRe beats Self-Consistency by 3.4%, Best-of-k by 3.2%, and Self-Refine by\n4.0% while using less than half the samples. Unlike iterative refinement with\nbaselines, MAgICoRe continues to improve with more iterations. Finally, our\nablations highlight the importance of MAgICoRe's RMs and multi-agent\ncommunication."
                },
                "authors": [
                    {
                        "name": "Justin Chih-Yao Chen"
                    },
                    {
                        "name": "Archiki Prasad"
                    },
                    {
                        "name": "Swarnadeep Saha"
                    },
                    {
                        "name": "Elias Stengel-Eskin"
                    },
                    {
                        "name": "Mohit Bansal"
                    }
                ],
                "author_detail": {
                    "name": "Mohit Bansal"
                },
                "author": "Mohit Bansal",
                "arxiv_comment": "22 pages, code: https://github.com/dinobby/MAgICoRe",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.12147v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.12147v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.12145v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.12145v1",
                "updated": "2024-09-18T17:06:04Z",
                "updated_parsed": [
                    2024,
                    9,
                    18,
                    17,
                    6,
                    4,
                    2,
                    262,
                    0
                ],
                "published": "2024-09-18T17:06:04Z",
                "published_parsed": [
                    2024,
                    9,
                    18,
                    17,
                    6,
                    4,
                    2,
                    262,
                    0
                ],
                "title": "Probing the cosmic sterile-neutrino background with IceCube",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Probing the cosmic sterile-neutrino background with IceCube"
                },
                "summary": "In this paper, we take a close look at the interaction between the TeV--PeV\nenergy astrophysical neutrinos and a hypothetical cosmic sterile-neutrino\nbackground. These interactions yield absorption features, also called ``dips\",\nin the astrophysical neutrino spectrum, which are studied using the deposited\nenergy distribution of high-energy starting events (HESE) in the IceCube\ndetector. We improve upon the previous analysis by including the effects of\nregeneration and a realistic source distribution on the propagation of\nastrophysical neutrinos. We use the latest 7.5-year HESE dataset and include\nthe observation of Glashow resonance in our analysis. We evaluate the impact of\nthese dips on the inferred spectral index and overall normalization of the\nastrophysical neutrinos. We find a mild preference for dips in the 300--800 TeV\nrange, and the best-fit parameters for the mass of sterile-neutrino and the\nmediator are 0.5 eV and 23 MeV, respectively. We find that the inclusion of\nthese absorption features lowers the spectral index of astrophysical neutrinos\nto $2.60^{+0.19}_{-0.16}$. The lower spectral index can reduce the disagreement\nwith the Northern Tracks sample but requires dedicated analysis. We also\nforecast the event spectrum for IceCube-Gen2 for the two different fits.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this paper, we take a close look at the interaction between the TeV--PeV\nenergy astrophysical neutrinos and a hypothetical cosmic sterile-neutrino\nbackground. These interactions yield absorption features, also called ``dips\",\nin the astrophysical neutrino spectrum, which are studied using the deposited\nenergy distribution of high-energy starting events (HESE) in the IceCube\ndetector. We improve upon the previous analysis by including the effects of\nregeneration and a realistic source distribution on the propagation of\nastrophysical neutrinos. We use the latest 7.5-year HESE dataset and include\nthe observation of Glashow resonance in our analysis. We evaluate the impact of\nthese dips on the inferred spectral index and overall normalization of the\nastrophysical neutrinos. We find a mild preference for dips in the 300--800 TeV\nrange, and the best-fit parameters for the mass of sterile-neutrino and the\nmediator are 0.5 eV and 23 MeV, respectively. We find that the inclusion of\nthese absorption features lowers the spectral index of astrophysical neutrinos\nto $2.60^{+0.19}_{-0.16}$. The lower spectral index can reduce the disagreement\nwith the Northern Tracks sample but requires dedicated analysis. We also\nforecast the event spectrum for IceCube-Gen2 for the two different fits."
                },
                "authors": [
                    {
                        "name": "Bhavesh Chauhan"
                    },
                    {
                        "name": "Priyank Parashari"
                    }
                ],
                "author_detail": {
                    "name": "Priyank Parashari"
                },
                "author": "Priyank Parashari",
                "arxiv_comment": "21 pages, 5 Figures. Comments are welcome",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.12145v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.12145v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "hep-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "hep-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.12140v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.12140v1",
                "updated": "2024-09-18T17:03:30Z",
                "updated_parsed": [
                    2024,
                    9,
                    18,
                    17,
                    3,
                    30,
                    2,
                    262,
                    0
                ],
                "published": "2024-09-18T17:03:30Z",
                "published_parsed": [
                    2024,
                    9,
                    18,
                    17,
                    3,
                    30,
                    2,
                    262,
                    0
                ],
                "title": "MoRAG -- Multi-Fusion Retrieval Augmented Generation for Human Motion",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MoRAG -- Multi-Fusion Retrieval Augmented Generation for Human Motion"
                },
                "summary": "We introduce MoRAG, a novel multi-part fusion based retrieval-augmented\ngeneration strategy for text-based human motion generation. The method enhances\nmotion diffusion models by leveraging additional knowledge obtained through an\nimproved motion retrieval process. By effectively prompting large language\nmodels (LLMs), we address spelling errors and rephrasing issues in motion\nretrieval. Our approach utilizes a multi-part retrieval strategy to improve the\ngeneralizability of motion retrieval across the language space. We create\ndiverse samples through the spatial composition of the retrieved motions.\nFurthermore, by utilizing low-level, part-specific motion information, we can\nconstruct motion samples for unseen text descriptions. Our experiments\ndemonstrate that our framework can serve as a plug-and-play module, improving\nthe performance of motion diffusion models. Code, pretrained models and sample\nvideos will be made available at: https://motion-rag.github.io/",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce MoRAG, a novel multi-part fusion based retrieval-augmented\ngeneration strategy for text-based human motion generation. The method enhances\nmotion diffusion models by leveraging additional knowledge obtained through an\nimproved motion retrieval process. By effectively prompting large language\nmodels (LLMs), we address spelling errors and rephrasing issues in motion\nretrieval. Our approach utilizes a multi-part retrieval strategy to improve the\ngeneralizability of motion retrieval across the language space. We create\ndiverse samples through the spatial composition of the retrieved motions.\nFurthermore, by utilizing low-level, part-specific motion information, we can\nconstruct motion samples for unseen text descriptions. Our experiments\ndemonstrate that our framework can serve as a plug-and-play module, improving\nthe performance of motion diffusion models. Code, pretrained models and sample\nvideos will be made available at: https://motion-rag.github.io/"
                },
                "authors": [
                    {
                        "name": "Kalakonda Sai Shashank"
                    },
                    {
                        "name": "Shubh Maheshwari"
                    },
                    {
                        "name": "Ravi Kiran Sarvadevabhatla"
                    }
                ],
                "author_detail": {
                    "name": "Ravi Kiran Sarvadevabhatla"
                },
                "author": "Ravi Kiran Sarvadevabhatla",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.12140v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.12140v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.04176v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.04176v2",
                "updated": "2024-09-18T16:55:40Z",
                "updated_parsed": [
                    2024,
                    9,
                    18,
                    16,
                    55,
                    40,
                    2,
                    262,
                    0
                ],
                "published": "2024-06-06T15:34:19Z",
                "published_parsed": [
                    2024,
                    6,
                    6,
                    15,
                    34,
                    19,
                    3,
                    158,
                    0
                ],
                "title": "Explanation for the absence of secondary peaks in black hole light curve\n  autocorrelations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Explanation for the absence of secondary peaks in black hole light curve\n  autocorrelations"
                },
                "summary": "The observed radiation from hot gas accreting onto a black hole depends on\nboth the details of the flow and the spacetime geometry. The lensing behavior\nof a black hole produces a distinctive pattern of autocorrelations within its\nphoton ring that encodes its mass, spin, and inclination. In particular, the\ntime autocorrelation of the light curve is expected to display a series of\npeaks produced by light echoes of the source, with each peak delayed by the\ncharacteristic time lapse $\\tau$ between light echoes. However, such peaks are\nabsent from the light curves of observed black holes. Here, we develop an\nanalytical model for such light curves that demonstrates how, even though light\nechoes always exist in the signal, they do not produce autocorrelation peaks if\nthe characteristic correlation timescale $\\lambda_0$ of the source is greater\nthan $\\tau$. We validate our model against simulated light curves of a\nstochastic accretion model ray traced with a general-relativistic code, and\nthen fit the model to an observed light curve for Sgr A*. We infer that\n$\\lambda_0>\\tau$, providing an explanation for the absence of light echoes in\nthe time autocorrelations of Sgr A* light curves. Our results highlight the\nimportance for black hole parameter inference of spatially resolving the photon\nring via future space-based interferometry.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The observed radiation from hot gas accreting onto a black hole depends on\nboth the details of the flow and the spacetime geometry. The lensing behavior\nof a black hole produces a distinctive pattern of autocorrelations within its\nphoton ring that encodes its mass, spin, and inclination. In particular, the\ntime autocorrelation of the light curve is expected to display a series of\npeaks produced by light echoes of the source, with each peak delayed by the\ncharacteristic time lapse $\\tau$ between light echoes. However, such peaks are\nabsent from the light curves of observed black holes. Here, we develop an\nanalytical model for such light curves that demonstrates how, even though light\nechoes always exist in the signal, they do not produce autocorrelation peaks if\nthe characteristic correlation timescale $\\lambda_0$ of the source is greater\nthan $\\tau$. We validate our model against simulated light curves of a\nstochastic accretion model ray traced with a general-relativistic code, and\nthen fit the model to an observed light curve for Sgr A*. We infer that\n$\\lambda_0>\\tau$, providing an explanation for the absence of light echoes in\nthe time autocorrelations of Sgr A* light curves. Our results highlight the\nimportance for black hole parameter inference of spatially resolving the photon\nring via future space-based interferometry."
                },
                "authors": [
                    {
                        "name": "Alejandro Cárdenas-Avendaño"
                    },
                    {
                        "name": "Charles Gammie"
                    },
                    {
                        "name": "Alexandru Lupsasca"
                    }
                ],
                "author_detail": {
                    "name": "Alexandru Lupsasca"
                },
                "author": "Alexandru Lupsasca",
                "arxiv_comment": "5+8 pages, 1+5 figures. V2: The title was slightly modified to match\n  the published version, and a new section was added to the appendix",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.04176v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.04176v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.HE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.HE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "astro-ph.GA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "gr-qc",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "hep-th",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.12122v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.12122v1",
                "updated": "2024-09-18T16:45:37Z",
                "updated_parsed": [
                    2024,
                    9,
                    18,
                    16,
                    45,
                    37,
                    2,
                    262,
                    0
                ],
                "published": "2024-09-18T16:45:37Z",
                "published_parsed": [
                    2024,
                    9,
                    18,
                    16,
                    45,
                    37,
                    2,
                    262,
                    0
                ],
                "title": "Qwen2.5-Math Technical Report: Toward Mathematical Expert Model via\n  Self-Improvement",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Qwen2.5-Math Technical Report: Toward Mathematical Expert Model via\n  Self-Improvement"
                },
                "summary": "In this report, we present a series of math-specific large language models:\nQwen2.5-Math and Qwen2.5-Math-Instruct-1.5B/7B/72B. The core innovation of the\nQwen2.5 series lies in integrating the philosophy of self-improvement\nthroughout the entire pipeline, from pre-training and post-training to\ninference: (1) During the pre-training phase, Qwen2-Math-Instruct is utilized\nto generate large-scale, high-quality mathematical data. (2) In the\npost-training phase, we develop a reward model (RM) by conducting massive\nsampling from Qwen2-Math-Instruct. This RM is then applied to the iterative\nevolution of data in supervised fine-tuning (SFT). With a stronger SFT model,\nit's possible to iteratively train and update the RM, which in turn guides the\nnext round of SFT data iteration. On the final SFT model, we employ the\nultimate RM for reinforcement learning, resulting in the Qwen2.5-Math-Instruct.\n(3) Furthermore, during the inference stage, the RM is used to guide sampling,\noptimizing the model's performance.\n  Qwen2.5-Math-Instruct supports both Chinese and English, and possess advanced\nmathematical reasoning capabilities, including Chain-of-Thought (CoT) and\nTool-Integrated Reasoning (TIR). We evaluate our models on 10 mathematics\ndatasets in both English and Chinese, such as GSM8K, MATH, GaoKao, AMC23, and\nAIME24, covering a range of difficulties from grade school level to math\ncompetition problems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this report, we present a series of math-specific large language models:\nQwen2.5-Math and Qwen2.5-Math-Instruct-1.5B/7B/72B. The core innovation of the\nQwen2.5 series lies in integrating the philosophy of self-improvement\nthroughout the entire pipeline, from pre-training and post-training to\ninference: (1) During the pre-training phase, Qwen2-Math-Instruct is utilized\nto generate large-scale, high-quality mathematical data. (2) In the\npost-training phase, we develop a reward model (RM) by conducting massive\nsampling from Qwen2-Math-Instruct. This RM is then applied to the iterative\nevolution of data in supervised fine-tuning (SFT). With a stronger SFT model,\nit's possible to iteratively train and update the RM, which in turn guides the\nnext round of SFT data iteration. On the final SFT model, we employ the\nultimate RM for reinforcement learning, resulting in the Qwen2.5-Math-Instruct.\n(3) Furthermore, during the inference stage, the RM is used to guide sampling,\noptimizing the model's performance.\n  Qwen2.5-Math-Instruct supports both Chinese and English, and possess advanced\nmathematical reasoning capabilities, including Chain-of-Thought (CoT) and\nTool-Integrated Reasoning (TIR). We evaluate our models on 10 mathematics\ndatasets in both English and Chinese, such as GSM8K, MATH, GaoKao, AMC23, and\nAIME24, covering a range of difficulties from grade school level to math\ncompetition problems."
                },
                "authors": [
                    {
                        "name": "An Yang"
                    },
                    {
                        "name": "Beichen Zhang"
                    },
                    {
                        "name": "Binyuan Hui"
                    },
                    {
                        "name": "Bofei Gao"
                    },
                    {
                        "name": "Bowen Yu"
                    },
                    {
                        "name": "Chengpeng Li"
                    },
                    {
                        "name": "Dayiheng Liu"
                    },
                    {
                        "name": "Jianhong Tu"
                    },
                    {
                        "name": "Jingren Zhou"
                    },
                    {
                        "name": "Junyang Lin"
                    },
                    {
                        "name": "Keming Lu"
                    },
                    {
                        "name": "Mingfeng Xue"
                    },
                    {
                        "name": "Runji Lin"
                    },
                    {
                        "name": "Tianyu Liu"
                    },
                    {
                        "name": "Xingzhang Ren"
                    },
                    {
                        "name": "Zhenru Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Zhenru Zhang"
                },
                "author": "Zhenru Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.12122v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.12122v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.12117v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.12117v1",
                "updated": "2024-09-18T16:39:10Z",
                "updated_parsed": [
                    2024,
                    9,
                    18,
                    16,
                    39,
                    10,
                    2,
                    262,
                    0
                ],
                "published": "2024-09-18T16:39:10Z",
                "published_parsed": [
                    2024,
                    9,
                    18,
                    16,
                    39,
                    10,
                    2,
                    262,
                    0
                ],
                "title": "Low Frame-rate Speech Codec: a Codec Designed for Fast High-quality\n  Speech LLM Training and Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Low Frame-rate Speech Codec: a Codec Designed for Fast High-quality\n  Speech LLM Training and Inference"
                },
                "summary": "Large language models (LLMs) have significantly advanced audio processing\nthrough audio codecs that convert audio into discrete tokens, enabling the\napplication of language modeling techniques to audio data. However, audio\ncodecs often operate at high frame rates, resulting in slow training and\ninference, especially for autoregressive models. To address this challenge, we\npresent the Low Frame-rate Speech Codec (LFSC): a neural audio codec that\nleverages finite scalar quantization and adversarial training with large speech\nlanguage models to achieve high-quality audio compression with a 1.89 kbps\nbitrate and 21.5 frames per second. We demonstrate that our novel codec can\nmake the inference of LLM-based text-to-speech models around three times faster\nwhile improving intelligibility and producing quality comparable to previous\nmodels.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have significantly advanced audio processing\nthrough audio codecs that convert audio into discrete tokens, enabling the\napplication of language modeling techniques to audio data. However, audio\ncodecs often operate at high frame rates, resulting in slow training and\ninference, especially for autoregressive models. To address this challenge, we\npresent the Low Frame-rate Speech Codec (LFSC): a neural audio codec that\nleverages finite scalar quantization and adversarial training with large speech\nlanguage models to achieve high-quality audio compression with a 1.89 kbps\nbitrate and 21.5 frames per second. We demonstrate that our novel codec can\nmake the inference of LLM-based text-to-speech models around three times faster\nwhile improving intelligibility and producing quality comparable to previous\nmodels."
                },
                "authors": [
                    {
                        "name": "Edresson Casanova"
                    },
                    {
                        "name": "Ryan Langman"
                    },
                    {
                        "name": "Paarth Neekhara"
                    },
                    {
                        "name": "Shehzeen Hussain"
                    },
                    {
                        "name": "Jason Li"
                    },
                    {
                        "name": "Subhankar Ghosh"
                    },
                    {
                        "name": "Ante Jukić"
                    },
                    {
                        "name": "Sang-gil Lee"
                    }
                ],
                "author_detail": {
                    "name": "Sang-gil Lee"
                },
                "author": "Sang-gil Lee",
                "arxiv_comment": "Submitted to ICASSP 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.12117v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.12117v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.AS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.12116v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.12116v1",
                "updated": "2024-09-18T16:38:37Z",
                "updated_parsed": [
                    2024,
                    9,
                    18,
                    16,
                    38,
                    37,
                    2,
                    262,
                    0
                ],
                "published": "2024-09-18T16:38:37Z",
                "published_parsed": [
                    2024,
                    9,
                    18,
                    16,
                    38,
                    37,
                    2,
                    262,
                    0
                ],
                "title": "Stronger Baseline Models -- A Key Requirement for Aligning Machine\n  Learning Research with Clinical Utility",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Stronger Baseline Models -- A Key Requirement for Aligning Machine\n  Learning Research with Clinical Utility"
                },
                "summary": "Machine Learning (ML) research has increased substantially in recent years,\ndue to the success of predictive modeling across diverse application domains.\nHowever, well-known barriers exist when attempting to deploy ML models in\nhigh-stakes, clinical settings, including lack of model transparency (or the\ninability to audit the inference process), large training data requirements\nwith siloed data sources, and complicated metrics for measuring model utility.\nIn this work, we show empirically that including stronger baseline models in\nhealthcare ML evaluations has important downstream effects that aid\npractitioners in addressing these challenges. Through a series of case studies,\nwe find that the common practice of omitting baselines or comparing against a\nweak baseline model (e.g. a linear model with no optimization) obscures the\nvalue of ML methods proposed in the research literature. Using these insights,\nwe propose some best practices that will enable practitioners to more\neffectively study and deploy ML models in clinical settings.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Machine Learning (ML) research has increased substantially in recent years,\ndue to the success of predictive modeling across diverse application domains.\nHowever, well-known barriers exist when attempting to deploy ML models in\nhigh-stakes, clinical settings, including lack of model transparency (or the\ninability to audit the inference process), large training data requirements\nwith siloed data sources, and complicated metrics for measuring model utility.\nIn this work, we show empirically that including stronger baseline models in\nhealthcare ML evaluations has important downstream effects that aid\npractitioners in addressing these challenges. Through a series of case studies,\nwe find that the common practice of omitting baselines or comparing against a\nweak baseline model (e.g. a linear model with no optimization) obscures the\nvalue of ML methods proposed in the research literature. Using these insights,\nwe propose some best practices that will enable practitioners to more\neffectively study and deploy ML models in clinical settings."
                },
                "authors": [
                    {
                        "name": "Nathan Wolfrath"
                    },
                    {
                        "name": "Joel Wolfrath"
                    },
                    {
                        "name": "Hengrui Hu"
                    },
                    {
                        "name": "Anjishnu Banerjee"
                    },
                    {
                        "name": "Anai N. Kothari"
                    }
                ],
                "author_detail": {
                    "name": "Anai N. Kothari"
                },
                "author": "Anai N. Kothari",
                "arxiv_comment": "18 pages, 6 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.12116v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.12116v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.12106v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.12106v1",
                "updated": "2024-09-18T16:26:22Z",
                "updated_parsed": [
                    2024,
                    9,
                    18,
                    16,
                    26,
                    22,
                    2,
                    262,
                    0
                ],
                "published": "2024-09-18T16:26:22Z",
                "published_parsed": [
                    2024,
                    9,
                    18,
                    16,
                    26,
                    22,
                    2,
                    262,
                    0
                ],
                "title": "Measuring Human and AI Values based on Generative Psychometrics with\n  Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Measuring Human and AI Values based on Generative Psychometrics with\n  Large Language Models"
                },
                "summary": "Human values and their measurement are long-standing interdisciplinary\ninquiry. Recent advances in AI have sparked renewed interest in this area, with\nlarge language models (LLMs) emerging as both tools and subjects of value\nmeasurement. This work introduces Generative Psychometrics for Values (GPV), an\nLLM-based, data-driven value measurement paradigm, theoretically grounded in\ntext-revealed selective perceptions. We begin by fine-tuning an LLM for\naccurate perception-level value measurement and verifying the capability of\nLLMs to parse texts into perceptions, forming the core of the GPV pipeline.\nApplying GPV to human-authored blogs, we demonstrate its stability, validity,\nand superiority over prior psychological tools. Then, extending GPV to LLM\nvalue measurement, we advance the current art with 1) a psychometric\nmethodology that measures LLM values based on their scalable and free-form\noutputs, enabling context-specific measurement; 2) a comparative analysis of\nmeasurement paradigms, indicating response biases of prior methods; and 3) an\nattempt to bridge LLM values and their safety, revealing the predictive power\nof different value systems and the impacts of various values on LLM safety.\nThrough interdisciplinary efforts, we aim to leverage AI for next-generation\npsychometrics and psychometrics for value-aligned AI.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Human values and their measurement are long-standing interdisciplinary\ninquiry. Recent advances in AI have sparked renewed interest in this area, with\nlarge language models (LLMs) emerging as both tools and subjects of value\nmeasurement. This work introduces Generative Psychometrics for Values (GPV), an\nLLM-based, data-driven value measurement paradigm, theoretically grounded in\ntext-revealed selective perceptions. We begin by fine-tuning an LLM for\naccurate perception-level value measurement and verifying the capability of\nLLMs to parse texts into perceptions, forming the core of the GPV pipeline.\nApplying GPV to human-authored blogs, we demonstrate its stability, validity,\nand superiority over prior psychological tools. Then, extending GPV to LLM\nvalue measurement, we advance the current art with 1) a psychometric\nmethodology that measures LLM values based on their scalable and free-form\noutputs, enabling context-specific measurement; 2) a comparative analysis of\nmeasurement paradigms, indicating response biases of prior methods; and 3) an\nattempt to bridge LLM values and their safety, revealing the predictive power\nof different value systems and the impacts of various values on LLM safety.\nThrough interdisciplinary efforts, we aim to leverage AI for next-generation\npsychometrics and psychometrics for value-aligned AI."
                },
                "authors": [
                    {
                        "name": "Haoran Ye"
                    },
                    {
                        "name": "Yuhang Xie"
                    },
                    {
                        "name": "Yuanyi Ren"
                    },
                    {
                        "name": "Hanjun Fang"
                    },
                    {
                        "name": "Xin Zhang"
                    },
                    {
                        "name": "Guojie Song"
                    }
                ],
                "author_detail": {
                    "name": "Guojie Song"
                },
                "author": "Guojie Song",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.12106v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.12106v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.12096v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.12096v1",
                "updated": "2024-09-18T16:14:35Z",
                "updated_parsed": [
                    2024,
                    9,
                    18,
                    16,
                    14,
                    35,
                    2,
                    262,
                    0
                ],
                "published": "2024-09-18T16:14:35Z",
                "published_parsed": [
                    2024,
                    9,
                    18,
                    16,
                    14,
                    35,
                    2,
                    262,
                    0
                ],
                "title": "An Efficient Projection-Based Next-best-view Planning Framework for\n  Reconstruction of Unknown Objects",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "An Efficient Projection-Based Next-best-view Planning Framework for\n  Reconstruction of Unknown Objects"
                },
                "summary": "Efficiently and completely capturing the three-dimensional data of an object\nis a fundamental problem in industrial and robotic applications. The task of\nnext-best-view (NBV) planning is to infer the pose of the next viewpoint based\non the current data, and gradually realize the complete three-dimensional\nreconstruction. Many existing algorithms, however, suffer a large computational\nburden due to the use of ray-casting. To address this, this paper proposes a\nprojection-based NBV planning framework. It can select the next best view at an\nextremely fast speed while ensuring the complete scanning of the object.\nSpecifically, this framework refits different types of voxel clusters into\nellipsoids based on the voxel structure.Then, the next best view is selected\nfrom the candidate views using a projection-based viewpoint quality evaluation\nfunction in conjunction with a global partitioning strategy. This process\nreplaces the ray-casting in voxel structures, significantly improving the\ncomputational efficiency. Comparative experiments with other algorithms in a\nsimulation environment show that the framework proposed in this paper can\nachieve 10 times efficiency improvement on the basis of capturing roughly the\nsame coverage. The real-world experimental results also prove the efficiency\nand feasibility of the framework.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficiently and completely capturing the three-dimensional data of an object\nis a fundamental problem in industrial and robotic applications. The task of\nnext-best-view (NBV) planning is to infer the pose of the next viewpoint based\non the current data, and gradually realize the complete three-dimensional\nreconstruction. Many existing algorithms, however, suffer a large computational\nburden due to the use of ray-casting. To address this, this paper proposes a\nprojection-based NBV planning framework. It can select the next best view at an\nextremely fast speed while ensuring the complete scanning of the object.\nSpecifically, this framework refits different types of voxel clusters into\nellipsoids based on the voxel structure.Then, the next best view is selected\nfrom the candidate views using a projection-based viewpoint quality evaluation\nfunction in conjunction with a global partitioning strategy. This process\nreplaces the ray-casting in voxel structures, significantly improving the\ncomputational efficiency. Comparative experiments with other algorithms in a\nsimulation environment show that the framework proposed in this paper can\nachieve 10 times efficiency improvement on the basis of capturing roughly the\nsame coverage. The real-world experimental results also prove the efficiency\nand feasibility of the framework."
                },
                "authors": [
                    {
                        "name": "Zhizhou Jia"
                    },
                    {
                        "name": "Shaohui Zhang"
                    },
                    {
                        "name": "Qun Hao"
                    }
                ],
                "author_detail": {
                    "name": "Qun Hao"
                },
                "author": "Qun Hao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.12096v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.12096v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.15256v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.15256v3",
                "updated": "2024-09-18T16:09:40Z",
                "updated_parsed": [
                    2024,
                    9,
                    18,
                    16,
                    9,
                    40,
                    2,
                    262,
                    0
                ],
                "published": "2024-08-09T19:21:14Z",
                "published_parsed": [
                    2024,
                    8,
                    9,
                    19,
                    21,
                    14,
                    4,
                    222,
                    0
                ],
                "title": "Improving Ontology Requirements Engineering with OntoChat and\n  Participatory Prompting",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Improving Ontology Requirements Engineering with OntoChat and\n  Participatory Prompting"
                },
                "summary": "Past ontology requirements engineering (ORE) has primarily relied on manual\nmethods, such as interviews and collaborative forums, to gather user\nrequirements from domain experts, especially in large projects. Current\nOntoChat offers a framework for ORE that utilises large language models (LLMs)\nto streamline the process through four key functions: user story creation,\ncompetency question (CQ) extraction, CQ filtration and analysis, and ontology\ntesting support. In OntoChat, users are expected to prompt the chatbot to\ngenerate user stories. However, preliminary evaluations revealed that they\nstruggle to do this effectively. To address this issue, we experimented with a\nresearch method called participatory prompting, which involves\nresearcher-mediated interactions to help users without deep knowledge of LLMs\nuse the chatbot more effectively. This participatory prompting user study\nproduces pre-defined prompt templates based on user queries, focusing on\ncreating and refining personas, goals, scenarios, sample data, and data\nresources for user stories. These refined user stories will subsequently be\nconverted into CQs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Past ontology requirements engineering (ORE) has primarily relied on manual\nmethods, such as interviews and collaborative forums, to gather user\nrequirements from domain experts, especially in large projects. Current\nOntoChat offers a framework for ORE that utilises large language models (LLMs)\nto streamline the process through four key functions: user story creation,\ncompetency question (CQ) extraction, CQ filtration and analysis, and ontology\ntesting support. In OntoChat, users are expected to prompt the chatbot to\ngenerate user stories. However, preliminary evaluations revealed that they\nstruggle to do this effectively. To address this issue, we experimented with a\nresearch method called participatory prompting, which involves\nresearcher-mediated interactions to help users without deep knowledge of LLMs\nuse the chatbot more effectively. This participatory prompting user study\nproduces pre-defined prompt templates based on user queries, focusing on\ncreating and refining personas, goals, scenarios, sample data, and data\nresources for user stories. These refined user stories will subsequently be\nconverted into CQs."
                },
                "authors": [
                    {
                        "name": "Yihang Zhao"
                    },
                    {
                        "name": "Bohui Zhang"
                    },
                    {
                        "name": "Xi Hu"
                    },
                    {
                        "name": "Shuyin Ouyang"
                    },
                    {
                        "name": "Jongmo Kim"
                    },
                    {
                        "name": "Nitisha Jain"
                    },
                    {
                        "name": "Jacopo de Berardinis"
                    },
                    {
                        "name": "Albert Meroño-Peñuela"
                    },
                    {
                        "name": "Elena Simperl"
                    }
                ],
                "author_detail": {
                    "name": "Elena Simperl"
                },
                "author": "Elena Simperl",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.15256v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.15256v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.20361v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.20361v2",
                "updated": "2024-09-18T16:07:40Z",
                "updated_parsed": [
                    2024,
                    9,
                    18,
                    16,
                    7,
                    40,
                    2,
                    262,
                    0
                ],
                "published": "2024-07-29T18:21:34Z",
                "published_parsed": [
                    2024,
                    7,
                    29,
                    18,
                    21,
                    34,
                    0,
                    211,
                    0
                ],
                "title": "From ML to LLM: Evaluating the Robustness of Phishing Webpage Detection\n  Models against Adversarial Attacks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "From ML to LLM: Evaluating the Robustness of Phishing Webpage Detection\n  Models against Adversarial Attacks"
                },
                "summary": "Phishing attacks attempt to deceive users into stealing sensitive\ninformation, posing a significant cybersecurity threat. Advances in machine\nlearning (ML) and deep learning (DL) have led to the development of numerous\nphishing webpage detection solutions, but these models remain vulnerable to\nadversarial attacks. Evaluating their robustness against adversarial phishing\nwebpages is essential. Existing tools contain datasets of pre-designed phishing\nwebpages for a limited number of brands, and lack diversity in phishing\nfeatures.\n  To address these challenges, we develop PhishOracle, a tool that generates\nadversarial phishing webpages by embedding diverse phishing features into\nlegitimate webpages. We evaluate the robustness of two existing models, Stack\nmodel and Phishpedia, in classifying PhishOracle-generated adversarial phishing\nwebpages. Additionally, we study a commercial large language model, Gemini Pro\nVision, in the context of adversarial attacks. We conduct a user study to\ndetermine whether PhishOracle-generated adversarial phishing webpages deceive\nusers. Our findings reveal that many PhishOracle-generated phishing webpages\nevade current phishing webpage detection models and deceive users, but Gemini\nPro Vision is robust to the attack. We also develop the PhishOracle web app,\nallowing users to input a legitimate URL, select relevant phishing features and\ngenerate a corresponding phishing webpage. All resources are publicly available\non GitHub.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Phishing attacks attempt to deceive users into stealing sensitive\ninformation, posing a significant cybersecurity threat. Advances in machine\nlearning (ML) and deep learning (DL) have led to the development of numerous\nphishing webpage detection solutions, but these models remain vulnerable to\nadversarial attacks. Evaluating their robustness against adversarial phishing\nwebpages is essential. Existing tools contain datasets of pre-designed phishing\nwebpages for a limited number of brands, and lack diversity in phishing\nfeatures.\n  To address these challenges, we develop PhishOracle, a tool that generates\nadversarial phishing webpages by embedding diverse phishing features into\nlegitimate webpages. We evaluate the robustness of two existing models, Stack\nmodel and Phishpedia, in classifying PhishOracle-generated adversarial phishing\nwebpages. Additionally, we study a commercial large language model, Gemini Pro\nVision, in the context of adversarial attacks. We conduct a user study to\ndetermine whether PhishOracle-generated adversarial phishing webpages deceive\nusers. Our findings reveal that many PhishOracle-generated phishing webpages\nevade current phishing webpage detection models and deceive users, but Gemini\nPro Vision is robust to the attack. We also develop the PhishOracle web app,\nallowing users to input a legitimate URL, select relevant phishing features and\ngenerate a corresponding phishing webpage. All resources are publicly available\non GitHub."
                },
                "authors": [
                    {
                        "name": "Aditya Kulkarni"
                    },
                    {
                        "name": "Vivek Balachandran"
                    },
                    {
                        "name": "Dinil Mon Divakaran"
                    },
                    {
                        "name": "Tamal Das"
                    }
                ],
                "author_detail": {
                    "name": "Tamal Das"
                },
                "author": "Tamal Das",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.20361v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.20361v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.10979v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.10979v2",
                "updated": "2024-09-18T15:53:32Z",
                "updated_parsed": [
                    2024,
                    9,
                    18,
                    15,
                    53,
                    32,
                    2,
                    262,
                    0
                ],
                "published": "2024-04-17T01:24:08Z",
                "published_parsed": [
                    2024,
                    4,
                    17,
                    1,
                    24,
                    8,
                    2,
                    108,
                    0
                ],
                "title": "Scales of Stability and Turbulence in the Molecular ISM",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Scales of Stability and Turbulence in the Molecular ISM"
                },
                "summary": "We re-analyze the data of the BU-FCRAO $^{13}{\\rm CO}$ Galactic Ring Survey\n(GRS) to understand the dynamics of the turbulent molecular interstellar\nmedium. We define molecular clouds by their spatial half-power contours of\n$^{13}{\\rm CO}$ integrated intensity, independent of a boundary based on\nthresholding or tiling. We find properties of hydrostatic equilibrium (HE) and\nvirial equilibrium (VE), the former independent and the latter dependent on\ntime and spatial scales. We suggest that HE is a stationary property of the\nturbulence and that molecular clouds are high-density regions of a fluctuating\ncomponent. The gravitational and turbulent kinetic energies within clouds are\ncontinuously evolving toward a time-dependent VE with the fluctuating,\nexternal, turbulent pressure energy (PE) that can be treated parametrically\nowing to the shorter time scale for virialization. The average PE is comparable\nto the pressure of the multiphase ISM at the Galactic mid-plane. Larson's\nscaling relations analyzed by different statistical methods are not\nsignificant. The non-dimensional variances of size, line width, and column\ndensity are of comparable magnitude, ruling out the inference of constant\ncolumn density. Previously unrecognized autocorrelations may have contributed\nto the apparent validity of the inference.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We re-analyze the data of the BU-FCRAO $^{13}{\\rm CO}$ Galactic Ring Survey\n(GRS) to understand the dynamics of the turbulent molecular interstellar\nmedium. We define molecular clouds by their spatial half-power contours of\n$^{13}{\\rm CO}$ integrated intensity, independent of a boundary based on\nthresholding or tiling. We find properties of hydrostatic equilibrium (HE) and\nvirial equilibrium (VE), the former independent and the latter dependent on\ntime and spatial scales. We suggest that HE is a stationary property of the\nturbulence and that molecular clouds are high-density regions of a fluctuating\ncomponent. The gravitational and turbulent kinetic energies within clouds are\ncontinuously evolving toward a time-dependent VE with the fluctuating,\nexternal, turbulent pressure energy (PE) that can be treated parametrically\nowing to the shorter time scale for virialization. The average PE is comparable\nto the pressure of the multiphase ISM at the Galactic mid-plane. Larson's\nscaling relations analyzed by different statistical methods are not\nsignificant. The non-dimensional variances of size, line width, and column\ndensity are of comparable magnitude, ruling out the inference of constant\ncolumn density. Previously unrecognized autocorrelations may have contributed\nto the apparent validity of the inference."
                },
                "authors": [
                    {
                        "name": "Eric Keto"
                    }
                ],
                "author_detail": {
                    "name": "Eric Keto"
                },
                "author": "Eric Keto",
                "arxiv_comment": "Submitted to Astronomische Nachrichten",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.10979v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.10979v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.GA",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.GA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2305.19472v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2305.19472v3",
                "updated": "2024-09-18T15:30:33Z",
                "updated_parsed": [
                    2024,
                    9,
                    18,
                    15,
                    30,
                    33,
                    2,
                    262,
                    0
                ],
                "published": "2023-05-31T00:55:40Z",
                "published_parsed": [
                    2023,
                    5,
                    31,
                    0,
                    55,
                    40,
                    2,
                    151,
                    0
                ],
                "title": "PlaSma: Making Small Language Models Better Procedural Knowledge Models\n  for (Counterfactual) Planning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PlaSma: Making Small Language Models Better Procedural Knowledge Models\n  for (Counterfactual) Planning"
                },
                "summary": "Procedural planning, which entails decomposing a high-level goal into a\nsequence of temporally ordered steps, is an important yet intricate task for\nmachines. It involves integrating common-sense knowledge to reason about\ncomplex and often contextualized situations, e.g. ``scheduling a doctor's\nappointment without a phone''. While current approaches show encouraging\nresults using large language models (LLMs), they are hindered by drawbacks such\nas costly API calls and reproducibility issues. In this paper, we advocate\nplanning using smaller language models. We present PlaSma, a novel two-pronged\napproach to endow small language models with procedural knowledge and\n(constrained) language planning capabilities. More concretely, we develop\nsymbolic procedural knowledge distillation to enhance the commonsense knowledge\nin small language models and an inference-time algorithm to facilitate more\nstructured and accurate reasoning. In addition, we introduce a new related\ntask, Replanning, that requires a revision of a plan to cope with a constrained\nsituation. In both the planning and replanning settings, we show that\norders-of-magnitude smaller models (770M-11B parameters) can compete and often\nsurpass their larger teacher models' capabilities. Finally, we showcase\nsuccessful application of PlaSma in an embodied environment, VirtualHome.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Procedural planning, which entails decomposing a high-level goal into a\nsequence of temporally ordered steps, is an important yet intricate task for\nmachines. It involves integrating common-sense knowledge to reason about\ncomplex and often contextualized situations, e.g. ``scheduling a doctor's\nappointment without a phone''. While current approaches show encouraging\nresults using large language models (LLMs), they are hindered by drawbacks such\nas costly API calls and reproducibility issues. In this paper, we advocate\nplanning using smaller language models. We present PlaSma, a novel two-pronged\napproach to endow small language models with procedural knowledge and\n(constrained) language planning capabilities. More concretely, we develop\nsymbolic procedural knowledge distillation to enhance the commonsense knowledge\nin small language models and an inference-time algorithm to facilitate more\nstructured and accurate reasoning. In addition, we introduce a new related\ntask, Replanning, that requires a revision of a plan to cope with a constrained\nsituation. In both the planning and replanning settings, we show that\norders-of-magnitude smaller models (770M-11B parameters) can compete and often\nsurpass their larger teacher models' capabilities. Finally, we showcase\nsuccessful application of PlaSma in an embodied environment, VirtualHome."
                },
                "authors": [
                    {
                        "name": "Faeze Brahman"
                    },
                    {
                        "name": "Chandra Bhagavatula"
                    },
                    {
                        "name": "Valentina Pyatkin"
                    },
                    {
                        "name": "Jena D. Hwang"
                    },
                    {
                        "name": "Xiang Lorraine Li"
                    },
                    {
                        "name": "Hirona J. Arai"
                    },
                    {
                        "name": "Soumya Sanyal"
                    },
                    {
                        "name": "Keisuke Sakaguchi"
                    },
                    {
                        "name": "Xiang Ren"
                    },
                    {
                        "name": "Yejin Choi"
                    }
                ],
                "author_detail": {
                    "name": "Yejin Choi"
                },
                "author": "Yejin Choi",
                "arxiv_comment": "ICLR 2024 version , 31 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2305.19472v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2305.19472v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.12046v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.12046v2",
                "updated": "2024-09-19T02:48:43Z",
                "updated_parsed": [
                    2024,
                    9,
                    19,
                    2,
                    48,
                    43,
                    3,
                    263,
                    0
                ],
                "published": "2024-09-18T15:16:37Z",
                "published_parsed": [
                    2024,
                    9,
                    18,
                    15,
                    16,
                    37,
                    2,
                    262,
                    0
                ],
                "title": "Using Large Language Models to Generate Clinical Trial Tables and\n  Figures",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Using Large Language Models to Generate Clinical Trial Tables and\n  Figures"
                },
                "summary": "Tables, figures, and listings (TFLs) are essential tools for summarizing\nclinical trial data. Creation of TFLs for reporting activities is often a\ntime-consuming task encountered routinely during the execution of clinical\ntrials. This study explored the use of large language models (LLMs) to automate\nthe generation of TFLs through prompt engineering and few-shot transfer\nlearning. Using public clinical trial data in ADaM format, our results\ndemonstrated that LLMs can efficiently generate TFLs with prompt instructions,\nshowcasing their potential in this domain. Furthermore, we developed a\nconservational agent named Clinical Trial TFL Generation Agent: An app that\nmatches user queries to predefined prompts that produce customized programs to\ngenerate specific predefined TFLs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Tables, figures, and listings (TFLs) are essential tools for summarizing\nclinical trial data. Creation of TFLs for reporting activities is often a\ntime-consuming task encountered routinely during the execution of clinical\ntrials. This study explored the use of large language models (LLMs) to automate\nthe generation of TFLs through prompt engineering and few-shot transfer\nlearning. Using public clinical trial data in ADaM format, our results\ndemonstrated that LLMs can efficiently generate TFLs with prompt instructions,\nshowcasing their potential in this domain. Furthermore, we developed a\nconservational agent named Clinical Trial TFL Generation Agent: An app that\nmatches user queries to predefined prompts that produce customized programs to\ngenerate specific predefined TFLs."
                },
                "authors": [
                    {
                        "name": "Yumeng Yang"
                    },
                    {
                        "name": "Peter Krusche"
                    },
                    {
                        "name": "Kristyn Pantoja"
                    },
                    {
                        "name": "Cheng Shi"
                    },
                    {
                        "name": "Ethan Ludmir"
                    },
                    {
                        "name": "Kirk Roberts"
                    },
                    {
                        "name": "Gen Zhu"
                    }
                ],
                "author_detail": {
                    "name": "Gen Zhu"
                },
                "author": "Gen Zhu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.12046v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.12046v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.12020v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.12020v1",
                "updated": "2024-09-18T14:30:48Z",
                "updated_parsed": [
                    2024,
                    9,
                    18,
                    14,
                    30,
                    48,
                    2,
                    262,
                    0
                ],
                "published": "2024-09-18T14:30:48Z",
                "published_parsed": [
                    2024,
                    9,
                    18,
                    14,
                    30,
                    48,
                    2,
                    262,
                    0
                ],
                "title": "Promise and Peril of Collaborative Code Generation Models: Balancing\n  Effectiveness and Memorization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Promise and Peril of Collaborative Code Generation Models: Balancing\n  Effectiveness and Memorization"
                },
                "summary": "In the rapidly evolving field of machine learning, training models with\ndatasets from various locations and organizations presents significant\nchallenges due to privacy and legal concerns. The exploration of effective\ncollaborative training settings capable of leveraging valuable knowledge from\ndistributed and isolated datasets is increasingly crucial. This study\ninvestigates key factors that impact the effectiveness of collaborative\ntraining methods in code next-token prediction, as well as the correctness and\nutility of the generated code, demonstrating the promise of such methods.\nAdditionally, we evaluate the memorization of different participant training\ndata across various collaborative training settings, including centralized,\nfederated, and incremental training, highlighting their potential risks in\nleaking data. Our findings indicate that the size and diversity of code\ndatasets are pivotal factors influencing the success of collaboratively trained\ncode models. We show that federated learning achieves competitive performance\ncompared to centralized training while offering better data protection, as\nevidenced by lower memorization ratios in the generated code. However,\nfederated learning can still produce verbatim code snippets from hidden\ntraining data, potentially violating privacy or copyright. Our study further\nexplores effectiveness and memorization patterns in incremental learning,\nemphasizing the sequence in which individual participant datasets are\nintroduced. We also identify cross-organizational clones as a prevalent\nchallenge in both centralized and federated learning scenarios. Our findings\nhighlight the persistent risk of data leakage during inference, even when\ntraining data remains unseen. We conclude with recommendations for\npractitioners and researchers to optimize multisource datasets, propelling\ncross-organizational collaboration forward.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In the rapidly evolving field of machine learning, training models with\ndatasets from various locations and organizations presents significant\nchallenges due to privacy and legal concerns. The exploration of effective\ncollaborative training settings capable of leveraging valuable knowledge from\ndistributed and isolated datasets is increasingly crucial. This study\ninvestigates key factors that impact the effectiveness of collaborative\ntraining methods in code next-token prediction, as well as the correctness and\nutility of the generated code, demonstrating the promise of such methods.\nAdditionally, we evaluate the memorization of different participant training\ndata across various collaborative training settings, including centralized,\nfederated, and incremental training, highlighting their potential risks in\nleaking data. Our findings indicate that the size and diversity of code\ndatasets are pivotal factors influencing the success of collaboratively trained\ncode models. We show that federated learning achieves competitive performance\ncompared to centralized training while offering better data protection, as\nevidenced by lower memorization ratios in the generated code. However,\nfederated learning can still produce verbatim code snippets from hidden\ntraining data, potentially violating privacy or copyright. Our study further\nexplores effectiveness and memorization patterns in incremental learning,\nemphasizing the sequence in which individual participant datasets are\nintroduced. We also identify cross-organizational clones as a prevalent\nchallenge in both centralized and federated learning scenarios. Our findings\nhighlight the persistent risk of data leakage during inference, even when\ntraining data remains unseen. We conclude with recommendations for\npractitioners and researchers to optimize multisource datasets, propelling\ncross-organizational collaboration forward."
                },
                "authors": [
                    {
                        "name": "Zhi Chen"
                    },
                    {
                        "name": "Lingxiao Jiang"
                    }
                ],
                "author_detail": {
                    "name": "Lingxiao Jiang"
                },
                "author": "Lingxiao Jiang",
                "arxiv_comment": "Paper accepted to the ASE 2024 Conference Research Track",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.12020v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.12020v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.12019v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.12019v1",
                "updated": "2024-09-18T14:30:06Z",
                "updated_parsed": [
                    2024,
                    9,
                    18,
                    14,
                    30,
                    6,
                    2,
                    262,
                    0
                ],
                "published": "2024-09-18T14:30:06Z",
                "published_parsed": [
                    2024,
                    9,
                    18,
                    14,
                    30,
                    6,
                    2,
                    262,
                    0
                ],
                "title": "Asymptotics for conformal inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Asymptotics for conformal inference"
                },
                "summary": "Conformal inference is a versatile tool for building prediction sets in\nregression or classification. In this paper, we consider the false coverage\nproportion (FCP) in a transductive setting with a calibration sample of n\npoints and a test sample of m points. We identify the exact, distribution-free,\nasymptotic distribution of the FCP when both n and m tend to infinity. This\nshows in particular that FCP control can be achieved by using the well-known\nKolmogorov distribution, and puts forward that the asymptotic variance is\ndecreasing in the ratio n/m. We then provide a number of extensions by\nconsidering the novelty detection problem, weighted conformal inference and\ndistribution shift between the calibration sample and the test sample. In\nparticular, our asymptotical results allow to accurately quantify the\nasymptotical behavior of the errors when weighted conformal inference is used.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Conformal inference is a versatile tool for building prediction sets in\nregression or classification. In this paper, we consider the false coverage\nproportion (FCP) in a transductive setting with a calibration sample of n\npoints and a test sample of m points. We identify the exact, distribution-free,\nasymptotic distribution of the FCP when both n and m tend to infinity. This\nshows in particular that FCP control can be achieved by using the well-known\nKolmogorov distribution, and puts forward that the asymptotic variance is\ndecreasing in the ratio n/m. We then provide a number of extensions by\nconsidering the novelty detection problem, weighted conformal inference and\ndistribution shift between the calibration sample and the test sample. In\nparticular, our asymptotical results allow to accurately quantify the\nasymptotical behavior of the errors when weighted conformal inference is used."
                },
                "authors": [
                    {
                        "name": "Ulysse Gazin"
                    }
                ],
                "author_detail": {
                    "name": "Ulysse Gazin"
                },
                "author": "Ulysse Gazin",
                "arxiv_comment": "29 pages, 3 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.12019v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.12019v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "math.ST",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "math.ST",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.TH",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.12010v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.12010v1",
                "updated": "2024-09-18T14:24:29Z",
                "updated_parsed": [
                    2024,
                    9,
                    18,
                    14,
                    24,
                    29,
                    2,
                    262,
                    0
                ],
                "published": "2024-09-18T14:24:29Z",
                "published_parsed": [
                    2024,
                    9,
                    18,
                    14,
                    24,
                    29,
                    2,
                    262,
                    0
                ],
                "title": "ChefFusion: Multimodal Foundation Model Integrating Recipe and Food\n  Image Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ChefFusion: Multimodal Foundation Model Integrating Recipe and Food\n  Image Generation"
                },
                "summary": "Significant work has been conducted in the domain of food computing, yet\nthese studies typically focus on single tasks such as t2t (instruction\ngeneration from food titles and ingredients), i2t (recipe generation from food\nimages), or t2i (food image generation from recipes). None of these approaches\nintegrate all modalities simultaneously. To address this gap, we introduce a\nnovel food computing foundation model that achieves true multimodality,\nencompassing tasks such as t2t, t2i, i2t, it2t, and t2ti. By leveraging large\nlanguage models (LLMs) and pre-trained image encoder and decoder models, our\nmodel can perform a diverse array of food computing-related tasks, including\nfood understanding, food recognition, recipe generation, and food image\ngeneration. Compared to previous models, our foundation model demonstrates a\nsignificantly broader range of capabilities and exhibits superior performance,\nparticularly in food image generation and recipe generation tasks. We\nopen-sourced ChefFusion at GitHub.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Significant work has been conducted in the domain of food computing, yet\nthese studies typically focus on single tasks such as t2t (instruction\ngeneration from food titles and ingredients), i2t (recipe generation from food\nimages), or t2i (food image generation from recipes). None of these approaches\nintegrate all modalities simultaneously. To address this gap, we introduce a\nnovel food computing foundation model that achieves true multimodality,\nencompassing tasks such as t2t, t2i, i2t, it2t, and t2ti. By leveraging large\nlanguage models (LLMs) and pre-trained image encoder and decoder models, our\nmodel can perform a diverse array of food computing-related tasks, including\nfood understanding, food recognition, recipe generation, and food image\ngeneration. Compared to previous models, our foundation model demonstrates a\nsignificantly broader range of capabilities and exhibits superior performance,\nparticularly in food image generation and recipe generation tasks. We\nopen-sourced ChefFusion at GitHub."
                },
                "authors": [
                    {
                        "name": "Peiyu Li"
                    },
                    {
                        "name": "Xiaobao Huang"
                    },
                    {
                        "name": "Yijun Tian"
                    },
                    {
                        "name": "Nitesh V. Chawla"
                    }
                ],
                "author_detail": {
                    "name": "Nitesh V. Chawla"
                },
                "author": "Nitesh V. Chawla",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.12010v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.12010v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.12009v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.12009v1",
                "updated": "2024-09-18T14:21:07Z",
                "updated_parsed": [
                    2024,
                    9,
                    18,
                    14,
                    21,
                    7,
                    2,
                    262,
                    0
                ],
                "published": "2024-09-18T14:21:07Z",
                "published_parsed": [
                    2024,
                    9,
                    18,
                    14,
                    21,
                    7,
                    2,
                    262,
                    0
                ],
                "title": "Optimizing Redshift Distribution Inference through Joint\n  Self-Calibration and Clustering-Redshift Synergy",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Optimizing Redshift Distribution Inference through Joint\n  Self-Calibration and Clustering-Redshift Synergy"
                },
                "summary": "Accurately characterizing the true redshift (true-$z$) distribution of a\nphotometric redshift (photo-$z$) sample is critical for cosmological analyses\nin imaging surveys. Clustering-based techniques, which include\nclustering-redshift (CZ) and self-calibration (SC) methods--depending on\nwhether external spectroscopic data are used--offer powerful tools for this\npurpose. In this study, we explore the joint inference of the true-$z$\ndistribution by combining SC and CZ (denoted as SC+CZ). We derive simple\nmultiplicative update rules to perform the joint inference. By incorporating\nappropriate error weighting and an additional weighting function, our method\nshows significant improvement over previous algorithms. We validate our\napproach using a DES Y3 mock catalog. The true-$z$ distribution estimated\nthrough the combined SC+CZ method is generally more accurate than using SC or\nCZ alone. To account for the different constraining powers of these methods, we\nassign distinct weights to the SC and CZ contributions. The optimal weights,\nwhich minimize the distribution error, depend on the relative constraining\nstrength of the SC and CZ data. Specifically, for a spectroscopic redshift\nsample that represents 1% of the photo-$z$ sample, the optimal combination\nreduces the total error by 20% (40%) compared to using CZ (SC) alone, and it\nkeeps the bias in mean redshift [$\\Delta \\bar{z} / (1 + z) $] at the level of\n0.3%. Furthermore, when CZ data is only available in the low-$z$ range and the\nhigh-$z$ range relies solely on SC data, SC+CZ enables consistent estimation of\nthe true-$z$ distribution across the entire redshift range. Our findings\ndemonstrate that SC+CZ is an effective tool for constraining the true-$z$\ndistribution, paving the way for clustering-based methods to be applied at\n$z\\gtrsim 1$.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Accurately characterizing the true redshift (true-$z$) distribution of a\nphotometric redshift (photo-$z$) sample is critical for cosmological analyses\nin imaging surveys. Clustering-based techniques, which include\nclustering-redshift (CZ) and self-calibration (SC) methods--depending on\nwhether external spectroscopic data are used--offer powerful tools for this\npurpose. In this study, we explore the joint inference of the true-$z$\ndistribution by combining SC and CZ (denoted as SC+CZ). We derive simple\nmultiplicative update rules to perform the joint inference. By incorporating\nappropriate error weighting and an additional weighting function, our method\nshows significant improvement over previous algorithms. We validate our\napproach using a DES Y3 mock catalog. The true-$z$ distribution estimated\nthrough the combined SC+CZ method is generally more accurate than using SC or\nCZ alone. To account for the different constraining powers of these methods, we\nassign distinct weights to the SC and CZ contributions. The optimal weights,\nwhich minimize the distribution error, depend on the relative constraining\nstrength of the SC and CZ data. Specifically, for a spectroscopic redshift\nsample that represents 1% of the photo-$z$ sample, the optimal combination\nreduces the total error by 20% (40%) compared to using CZ (SC) alone, and it\nkeeps the bias in mean redshift [$\\Delta \\bar{z} / (1 + z) $] at the level of\n0.3%. Furthermore, when CZ data is only available in the low-$z$ range and the\nhigh-$z$ range relies solely on SC data, SC+CZ enables consistent estimation of\nthe true-$z$ distribution across the entire redshift range. Our findings\ndemonstrate that SC+CZ is an effective tool for constraining the true-$z$\ndistribution, paving the way for clustering-based methods to be applied at\n$z\\gtrsim 1$."
                },
                "authors": [
                    {
                        "name": "Weilun Zheng"
                    },
                    {
                        "name": "Kwan Chuen Chan"
                    },
                    {
                        "name": "Haojie Xu"
                    },
                    {
                        "name": "Le Zhang"
                    },
                    {
                        "name": "Ruiyu Song"
                    }
                ],
                "author_detail": {
                    "name": "Ruiyu Song"
                },
                "author": "Ruiyu Song",
                "arxiv_comment": "17 pages, 13 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.12009v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.12009v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.CO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.CO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "astro-ph.GA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2403.01535v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2403.01535v3",
                "updated": "2024-09-18T14:20:32Z",
                "updated_parsed": [
                    2024,
                    9,
                    18,
                    14,
                    20,
                    32,
                    2,
                    262,
                    0
                ],
                "published": "2024-03-03T15:28:47Z",
                "published_parsed": [
                    2024,
                    3,
                    3,
                    15,
                    28,
                    47,
                    6,
                    63,
                    0
                ],
                "title": "Neural Graph Generator: Feature-Conditioned Graph Generation using\n  Latent Diffusion Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Neural Graph Generator: Feature-Conditioned Graph Generation using\n  Latent Diffusion Models"
                },
                "summary": "Graph generation has emerged as a crucial task in machine learning, with\nsignificant challenges in generating graphs that accurately reflect specific\nproperties. Existing methods often fall short in efficiently addressing this\nneed as they struggle with the high-dimensional complexity and varied nature of\ngraph properties. In this paper, we introduce the Neural Graph Generator (NGG),\na novel approach which utilizes conditioned latent diffusion models for graph\ngeneration. NGG demonstrates a remarkable capacity to model complex graph\npatterns, offering control over the graph generation process. NGG employs a\nvariational graph autoencoder for graph compression and a diffusion process in\nthe latent vector space, guided by vectors summarizing graph statistics. We\ndemonstrate NGG's versatility across various graph generation tasks, showing\nits capability to capture desired graph properties and generalize to unseen\ngraphs. We also compare our generator to the graph generation capabilities of\ndifferent LLMs. This work signifies a shift in graph generation methodologies,\noffering a more practical and efficient solution for generating diverse graphs\nwith specific characteristics.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Graph generation has emerged as a crucial task in machine learning, with\nsignificant challenges in generating graphs that accurately reflect specific\nproperties. Existing methods often fall short in efficiently addressing this\nneed as they struggle with the high-dimensional complexity and varied nature of\ngraph properties. In this paper, we introduce the Neural Graph Generator (NGG),\na novel approach which utilizes conditioned latent diffusion models for graph\ngeneration. NGG demonstrates a remarkable capacity to model complex graph\npatterns, offering control over the graph generation process. NGG employs a\nvariational graph autoencoder for graph compression and a diffusion process in\nthe latent vector space, guided by vectors summarizing graph statistics. We\ndemonstrate NGG's versatility across various graph generation tasks, showing\nits capability to capture desired graph properties and generalize to unseen\ngraphs. We also compare our generator to the graph generation capabilities of\ndifferent LLMs. This work signifies a shift in graph generation methodologies,\noffering a more practical and efficient solution for generating diverse graphs\nwith specific characteristics."
                },
                "authors": [
                    {
                        "name": "Iakovos Evdaimon"
                    },
                    {
                        "name": "Giannis Nikolentzos"
                    },
                    {
                        "name": "Christos Xypolopoulos"
                    },
                    {
                        "name": "Ahmed Kammoun"
                    },
                    {
                        "name": "Michail Chatzianastasis"
                    },
                    {
                        "name": "Hadi Abdine"
                    },
                    {
                        "name": "Michalis Vazirgiannis"
                    }
                ],
                "author_detail": {
                    "name": "Michalis Vazirgiannis"
                },
                "author": "Michalis Vazirgiannis",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2403.01535v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2403.01535v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.14706v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.14706v2",
                "updated": "2024-09-18T13:52:54Z",
                "updated_parsed": [
                    2024,
                    9,
                    18,
                    13,
                    52,
                    54,
                    2,
                    262,
                    0
                ],
                "published": "2024-06-20T19:58:28Z",
                "published_parsed": [
                    2024,
                    6,
                    20,
                    19,
                    58,
                    28,
                    3,
                    172,
                    0
                ],
                "title": "WAGONN: Weight Bit Agglomeration in Crossbar Arrays for Reduced Impact\n  of Interconnect Resistance on DNN Inference Accuracy",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "WAGONN: Weight Bit Agglomeration in Crossbar Arrays for Reduced Impact\n  of Interconnect Resistance on DNN Inference Accuracy"
                },
                "summary": "Deep neural network (DNN) accelerators employing crossbar arrays capable of\nin-memory computing (IMC) are highly promising for neural computing platforms.\nHowever, in deeply scaled technologies, interconnect resistance severely\nimpairs IMC robustness, leading to a drop in the system accuracy. To address\nthis problem, we propose SWANN - a technique based on shuffling weights in\ncrossbar arrays which alleviates the detrimental effect of wire resistance on\nIMC. For 8T-SRAM-based 128x128 crossbar arrays in 7nm technology, SWANN\nenhances the accuracy from 47.78% to 83.5% for ResNet-20/CIFAR-10. We also show\nthat SWANN can be used synergistically with Partial-Word-LineActivation,\nfurther boosting the accuracy. Moreover, we evaluate the implications of SWANN\nfor compact ferroelectric-transistorbased crossbar arrays. SWANN incurs minimal\nhardware overhead, with less than a 1% increase in energy consumption.\nAdditionally, the latency and area overheads of SWANN are ~1% and ~16%,\nrespectively when 1 ADC is utilized per crossbar array.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Deep neural network (DNN) accelerators employing crossbar arrays capable of\nin-memory computing (IMC) are highly promising for neural computing platforms.\nHowever, in deeply scaled technologies, interconnect resistance severely\nimpairs IMC robustness, leading to a drop in the system accuracy. To address\nthis problem, we propose SWANN - a technique based on shuffling weights in\ncrossbar arrays which alleviates the detrimental effect of wire resistance on\nIMC. For 8T-SRAM-based 128x128 crossbar arrays in 7nm technology, SWANN\nenhances the accuracy from 47.78% to 83.5% for ResNet-20/CIFAR-10. We also show\nthat SWANN can be used synergistically with Partial-Word-LineActivation,\nfurther boosting the accuracy. Moreover, we evaluate the implications of SWANN\nfor compact ferroelectric-transistorbased crossbar arrays. SWANN incurs minimal\nhardware overhead, with less than a 1% increase in energy consumption.\nAdditionally, the latency and area overheads of SWANN are ~1% and ~16%,\nrespectively when 1 ADC is utilized per crossbar array."
                },
                "authors": [
                    {
                        "name": "Jeffry Victor"
                    },
                    {
                        "name": "Dong Eun Kim"
                    },
                    {
                        "name": "Chunguang Wang"
                    },
                    {
                        "name": "Kaushik Roy"
                    },
                    {
                        "name": "Sumeet Gupta"
                    }
                ],
                "author_detail": {
                    "name": "Sumeet Gupta"
                },
                "author": "Sumeet Gupta",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.14706v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.14706v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.ET",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.ET",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.00099v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.00099v3",
                "updated": "2024-09-18T13:45:08Z",
                "updated_parsed": [
                    2024,
                    9,
                    18,
                    13,
                    45,
                    8,
                    2,
                    262,
                    0
                ],
                "published": "2024-04-30T18:00:02Z",
                "published_parsed": [
                    2024,
                    4,
                    30,
                    18,
                    0,
                    2,
                    1,
                    121,
                    0
                ],
                "title": "Creative Beam Search: LLM-as-a-Judge For Improving Response Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Creative Beam Search: LLM-as-a-Judge For Improving Response Generation"
                },
                "summary": "Large language models are revolutionizing several areas, including artificial\ncreativity. However, the process of generation in machines profoundly diverges\nfrom that observed in humans. In particular, machine generation is\ncharacterized by a lack of intentionality and an underlying creative process.\nWe propose a method called Creative Beam Search that uses Diverse Beam Search\nand LLM-as-a-Judge to perform response generation and response validation. The\nresults of a qualitative experiment show how our approach can provide better\noutput than standard sampling techniques. We also show that the response\nvalidation step is a necessary complement to the response generation step.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models are revolutionizing several areas, including artificial\ncreativity. However, the process of generation in machines profoundly diverges\nfrom that observed in humans. In particular, machine generation is\ncharacterized by a lack of intentionality and an underlying creative process.\nWe propose a method called Creative Beam Search that uses Diverse Beam Search\nand LLM-as-a-Judge to perform response generation and response validation. The\nresults of a qualitative experiment show how our approach can provide better\noutput than standard sampling techniques. We also show that the response\nvalidation step is a necessary complement to the response generation step."
                },
                "authors": [
                    {
                        "name": "Giorgio Franceschelli"
                    },
                    {
                        "name": "Mirco Musolesi"
                    }
                ],
                "author_detail": {
                    "name": "Mirco Musolesi"
                },
                "author": "Mirco Musolesi",
                "arxiv_comment": "Presented as a short paper at the 15th International Conference on\n  Computational Creativity (ICCC'24)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.00099v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.00099v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.09810v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.09810v2",
                "updated": "2024-09-18T13:41:52Z",
                "updated_parsed": [
                    2024,
                    9,
                    18,
                    13,
                    41,
                    52,
                    2,
                    262,
                    0
                ],
                "published": "2024-09-15T18:04:03Z",
                "published_parsed": [
                    2024,
                    9,
                    15,
                    18,
                    4,
                    3,
                    6,
                    259,
                    0
                ],
                "title": "Local MALA-within-Gibbs for Bayesian image deblurring with total\n  variation prior",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Local MALA-within-Gibbs for Bayesian image deblurring with total\n  variation prior"
                },
                "summary": "We consider Bayesian inference for image deblurring with total variation (TV)\nprior. Since the posterior is analytically intractable, we resort to Markov\nchain Monte Carlo (MCMC) methods. However, since most MCMC methods\nsignificantly deteriorate in high dimensions, they are not suitable to handle\nhigh resolution imaging problems. In this paper, we show how low-dimensional\nsampling can still be facilitated by exploiting the sparse conditional\nstructure of the posterior. To this end, we make use of the local structures of\nthe blurring operator and the TV prior by partitioning the image into\nrectangular blocks and employing a blocked Gibbs sampler with proposals\nstemming from the Metropolis-Hastings adjusted Langevin Algorithm (MALA). We\nprove that this MALA-within-Gibbs (MLwG) sampling algorithm has\ndimension-independent block acceptance rates and dimension-independent\nconvergence rate. In order to apply the MALA proposals, we approximate the TV\nby a smoothed version, and show that the introduced approximation error is\nevenly distributed and dimension-independent. Since the posterior is a Gibbs\ndensity, we can use the Hammersley-Clifford Theorem to identify the posterior\nconditionals which only depend locally on the neighboring blocks. We outline\ncomputational strategies to evaluate the conditionals, which are the target\ndensities in the Gibbs updates, locally and in parallel. In two numerical\nexperiments, we validate the dimension-independent properties of the MLwG\nalgorithm and demonstrate its superior performance over MALA.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We consider Bayesian inference for image deblurring with total variation (TV)\nprior. Since the posterior is analytically intractable, we resort to Markov\nchain Monte Carlo (MCMC) methods. However, since most MCMC methods\nsignificantly deteriorate in high dimensions, they are not suitable to handle\nhigh resolution imaging problems. In this paper, we show how low-dimensional\nsampling can still be facilitated by exploiting the sparse conditional\nstructure of the posterior. To this end, we make use of the local structures of\nthe blurring operator and the TV prior by partitioning the image into\nrectangular blocks and employing a blocked Gibbs sampler with proposals\nstemming from the Metropolis-Hastings adjusted Langevin Algorithm (MALA). We\nprove that this MALA-within-Gibbs (MLwG) sampling algorithm has\ndimension-independent block acceptance rates and dimension-independent\nconvergence rate. In order to apply the MALA proposals, we approximate the TV\nby a smoothed version, and show that the introduced approximation error is\nevenly distributed and dimension-independent. Since the posterior is a Gibbs\ndensity, we can use the Hammersley-Clifford Theorem to identify the posterior\nconditionals which only depend locally on the neighboring blocks. We outline\ncomputational strategies to evaluate the conditionals, which are the target\ndensities in the Gibbs updates, locally and in parallel. In two numerical\nexperiments, we validate the dimension-independent properties of the MLwG\nalgorithm and demonstrate its superior performance over MALA."
                },
                "authors": [
                    {
                        "name": "Rafael Flock"
                    },
                    {
                        "name": "Shuigen Liu"
                    },
                    {
                        "name": "Yiqiu Dong"
                    },
                    {
                        "name": "Xin T. Tong"
                    }
                ],
                "author_detail": {
                    "name": "Xin T. Tong"
                },
                "author": "Xin T. Tong",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.09810v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.09810v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "math.NA",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "math.NA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "62F15, 68U10, 60J22",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.10576v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.10576v2",
                "updated": "2024-09-18T13:27:43Z",
                "updated_parsed": [
                    2024,
                    9,
                    18,
                    13,
                    27,
                    43,
                    2,
                    262,
                    0
                ],
                "published": "2024-09-15T15:21:45Z",
                "published_parsed": [
                    2024,
                    9,
                    15,
                    15,
                    21,
                    45,
                    6,
                    259,
                    0
                ],
                "title": "Language Models and Retrieval Augmented Generation for Automated\n  Structured Data Extraction from Diagnostic Reports",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Language Models and Retrieval Augmented Generation for Automated\n  Structured Data Extraction from Diagnostic Reports"
                },
                "summary": "Purpose: To develop and evaluate an automated system for extracting\nstructured clinical information from unstructured radiology and pathology\nreports using open-weights large language models (LMs) and retrieval augmented\ngeneration (RAG), and to assess the effects of model configuration variables on\nextraction performance. Methods and Materials: The study utilized two datasets:\n7,294 radiology reports annotated for Brain Tumor Reporting and Data System\n(BT-RADS) scores and 2,154 pathology reports annotated for isocitrate\ndehydrogenase (IDH) mutation status. An automated pipeline was developed to\nbenchmark the performance of various LMs and RAG configurations. The impact of\nmodel size, quantization, prompting strategies, output formatting, and\ninference parameters was systematically evaluated. Results: The best performing\nmodels achieved over 98% accuracy in extracting BT-RADS scores from radiology\nreports and over 90% for IDH mutation status extraction from pathology reports.\nThe top model being medical fine-tuned llama3. Larger, newer, and domain\nfine-tuned models consistently outperformed older and smaller models. Model\nquantization had minimal impact on performance. Few-shot prompting\nsignificantly improved accuracy. RAG improved performance for complex pathology\nreports but not for shorter radiology reports. Conclusions: Open LMs\ndemonstrate significant potential for automated extraction of structured\nclinical data from unstructured clinical reports with local privacy-preserving\napplication. Careful model selection, prompt engineering, and semi-automated\noptimization using annotated data are critical for optimal performance. These\napproaches could be reliable enough for practical use in research workflows,\nhighlighting the potential for human-machine collaboration in healthcare data\nextraction.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Purpose: To develop and evaluate an automated system for extracting\nstructured clinical information from unstructured radiology and pathology\nreports using open-weights large language models (LMs) and retrieval augmented\ngeneration (RAG), and to assess the effects of model configuration variables on\nextraction performance. Methods and Materials: The study utilized two datasets:\n7,294 radiology reports annotated for Brain Tumor Reporting and Data System\n(BT-RADS) scores and 2,154 pathology reports annotated for isocitrate\ndehydrogenase (IDH) mutation status. An automated pipeline was developed to\nbenchmark the performance of various LMs and RAG configurations. The impact of\nmodel size, quantization, prompting strategies, output formatting, and\ninference parameters was systematically evaluated. Results: The best performing\nmodels achieved over 98% accuracy in extracting BT-RADS scores from radiology\nreports and over 90% for IDH mutation status extraction from pathology reports.\nThe top model being medical fine-tuned llama3. Larger, newer, and domain\nfine-tuned models consistently outperformed older and smaller models. Model\nquantization had minimal impact on performance. Few-shot prompting\nsignificantly improved accuracy. RAG improved performance for complex pathology\nreports but not for shorter radiology reports. Conclusions: Open LMs\ndemonstrate significant potential for automated extraction of structured\nclinical data from unstructured clinical reports with local privacy-preserving\napplication. Careful model selection, prompt engineering, and semi-automated\noptimization using annotated data are critical for optimal performance. These\napproaches could be reliable enough for practical use in research workflows,\nhighlighting the potential for human-machine collaboration in healthcare data\nextraction."
                },
                "authors": [
                    {
                        "name": "Mohamed Sobhi Jabal"
                    },
                    {
                        "name": "Pranav Warman"
                    },
                    {
                        "name": "Jikai Zhang"
                    },
                    {
                        "name": "Kartikeye Gupta"
                    },
                    {
                        "name": "Ayush Jain"
                    },
                    {
                        "name": "Maciej Mazurowski"
                    },
                    {
                        "name": "Walter Wiggins"
                    },
                    {
                        "name": "Kirti Magudia"
                    },
                    {
                        "name": "Evan Calabrese"
                    }
                ],
                "author_detail": {
                    "name": "Evan Calabrese"
                },
                "author": "Evan Calabrese",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.10576v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.10576v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "J.3; I.2; I.2.7",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2304.00008v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2304.00008v4",
                "updated": "2024-09-18T13:25:52Z",
                "updated_parsed": [
                    2024,
                    9,
                    18,
                    13,
                    25,
                    52,
                    2,
                    262,
                    0
                ],
                "published": "2023-03-27T18:00:01Z",
                "published_parsed": [
                    2023,
                    3,
                    27,
                    18,
                    0,
                    1,
                    0,
                    86,
                    0
                ],
                "title": "On the Creativity of Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "On the Creativity of Large Language Models"
                },
                "summary": "Large Language Models (LLMs) are revolutionizing several areas of Artificial\nIntelligence. One of the most remarkable applications is creative writing,\ne.g., poetry or storytelling: the generated outputs are often of astonishing\nquality. However, a natural question arises: can LLMs be really considered\ncreative? In this article, we first analyze the development of LLMs under the\nlens of creativity theories, investigating the key open questions and\nchallenges. In particular, we focus our discussion on the dimensions of value,\nnovelty, and surprise as proposed by Margaret Boden in her work. Then, we\nconsider different classic perspectives, namely product, process, press, and\nperson. We discuss a set of ``easy'' and ``hard'' problems in machine\ncreativity, presenting them in relation to LLMs. Finally, we examine the\nsocietal impact of these technologies with a particular focus on the creative\nindustries, analyzing the opportunities offered, the challenges arising from\nthem, and the potential associated risks, from both legal and ethical points of\nview.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) are revolutionizing several areas of Artificial\nIntelligence. One of the most remarkable applications is creative writing,\ne.g., poetry or storytelling: the generated outputs are often of astonishing\nquality. However, a natural question arises: can LLMs be really considered\ncreative? In this article, we first analyze the development of LLMs under the\nlens of creativity theories, investigating the key open questions and\nchallenges. In particular, we focus our discussion on the dimensions of value,\nnovelty, and surprise as proposed by Margaret Boden in her work. Then, we\nconsider different classic perspectives, namely product, process, press, and\nperson. We discuss a set of ``easy'' and ``hard'' problems in machine\ncreativity, presenting them in relation to LLMs. Finally, we examine the\nsocietal impact of these technologies with a particular focus on the creative\nindustries, analyzing the opportunities offered, the challenges arising from\nthem, and the potential associated risks, from both legal and ethical points of\nview."
                },
                "authors": [
                    {
                        "name": "Giorgio Franceschelli"
                    },
                    {
                        "name": "Mirco Musolesi"
                    }
                ],
                "author_detail": {
                    "name": "Mirco Musolesi"
                },
                "author": "Mirco Musolesi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2304.00008v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2304.00008v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.11972v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.11972v1",
                "updated": "2024-09-18T13:24:44Z",
                "updated_parsed": [
                    2024,
                    9,
                    18,
                    13,
                    24,
                    44,
                    2,
                    262,
                    0
                ],
                "published": "2024-09-18T13:24:44Z",
                "published_parsed": [
                    2024,
                    9,
                    18,
                    13,
                    24,
                    44,
                    2,
                    262,
                    0
                ],
                "title": "Metric-Semantic Factor Graph Generation based on Graph Neural Networks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Metric-Semantic Factor Graph Generation based on Graph Neural Networks"
                },
                "summary": "Understanding the relationships between geometric structures and semantic\nconcepts is crucial for building accurate models of complex environments. In\nindoors, certain spatial constraints, such as the relative positioning of\nplanes, remain consistent despite variations in layout. This paper explores how\nthese invariant relationships can be captured in a graph SLAM framework by\nrepresenting high-level concepts like rooms and walls, linking them to\ngeometric elements like planes through an optimizable factor graph. Several\nefforts have tackled this issue with add-hoc solutions for each concept\ngeneration and with manually-defined factors.\n  This paper proposes a novel method for metric-semantic factor graph\ngeneration which includes defining a semantic scene graph, integrating\ngeometric information, and learning the interconnecting factors, all based on\nGraph Neural Networks (GNNs). An edge classification network (G-GNN) sorts the\nedges between planes into same room, same wall or none types. The resulting\nrelations are clustered, generating a room or wall for each cluster. A second\nfamily of networks (F-GNN) infers the geometrical origin of the new nodes. The\ndefinition of the factors employs the same F-GNN used for the metric attribute\nof the generated nodes. Furthermore, share the new factor graph with the\nS-Graphs+ algorithm, extending its graph expressiveness and scene\nrepresentation with the ultimate goal of improving the SLAM performance. The\ncomplexity of the environments is increased to N-plane rooms by training the\nnetworks on L-shaped rooms. The framework is evaluated in synthetic and\nsimulated scenarios as no real datasets of the required complex layouts are\navailable.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Understanding the relationships between geometric structures and semantic\nconcepts is crucial for building accurate models of complex environments. In\nindoors, certain spatial constraints, such as the relative positioning of\nplanes, remain consistent despite variations in layout. This paper explores how\nthese invariant relationships can be captured in a graph SLAM framework by\nrepresenting high-level concepts like rooms and walls, linking them to\ngeometric elements like planes through an optimizable factor graph. Several\nefforts have tackled this issue with add-hoc solutions for each concept\ngeneration and with manually-defined factors.\n  This paper proposes a novel method for metric-semantic factor graph\ngeneration which includes defining a semantic scene graph, integrating\ngeometric information, and learning the interconnecting factors, all based on\nGraph Neural Networks (GNNs). An edge classification network (G-GNN) sorts the\nedges between planes into same room, same wall or none types. The resulting\nrelations are clustered, generating a room or wall for each cluster. A second\nfamily of networks (F-GNN) infers the geometrical origin of the new nodes. The\ndefinition of the factors employs the same F-GNN used for the metric attribute\nof the generated nodes. Furthermore, share the new factor graph with the\nS-Graphs+ algorithm, extending its graph expressiveness and scene\nrepresentation with the ultimate goal of improving the SLAM performance. The\ncomplexity of the environments is increased to N-plane rooms by training the\nnetworks on L-shaped rooms. The framework is evaluated in synthetic and\nsimulated scenarios as no real datasets of the required complex layouts are\navailable."
                },
                "authors": [
                    {
                        "name": "Jose Andres Millan-Romera"
                    },
                    {
                        "name": "Hriday Bavle"
                    },
                    {
                        "name": "Muhammad Shaheer"
                    },
                    {
                        "name": "Holger Voos"
                    },
                    {
                        "name": "Jose Luis Sanchez-Lopez"
                    }
                ],
                "author_detail": {
                    "name": "Jose Luis Sanchez-Lopez"
                },
                "author": "Jose Luis Sanchez-Lopez",
                "arxiv_comment": "Submitted to ICRA 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.11972v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.11972v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.11971v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.11971v1",
                "updated": "2024-09-18T13:22:04Z",
                "updated_parsed": [
                    2024,
                    9,
                    18,
                    13,
                    22,
                    4,
                    2,
                    262,
                    0
                ],
                "published": "2024-09-18T13:22:04Z",
                "published_parsed": [
                    2024,
                    9,
                    18,
                    13,
                    22,
                    4,
                    2,
                    262,
                    0
                ],
                "title": "Sampling Latent Material-Property Information From LLM-Derived Embedding\n  Representations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Sampling Latent Material-Property Information From LLM-Derived Embedding\n  Representations"
                },
                "summary": "Vector embeddings derived from large language models (LLMs) show promise in\ncapturing latent information from the literature. Interestingly, these can be\nintegrated into material embeddings, potentially useful for data-driven\npredictions of materials properties. We investigate the extent to which\nLLM-derived vectors capture the desired information and their potential to\nprovide insights into material properties without additional training. Our\nfindings indicate that, although LLMs can be used to generate representations\nreflecting certain property information, extracting the embeddings requires\nidentifying the optimal contextual clues and appropriate comparators. Despite\nthis restriction, it appears that LLMs still have the potential to be useful in\ngenerating meaningful materials-science representations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Vector embeddings derived from large language models (LLMs) show promise in\ncapturing latent information from the literature. Interestingly, these can be\nintegrated into material embeddings, potentially useful for data-driven\npredictions of materials properties. We investigate the extent to which\nLLM-derived vectors capture the desired information and their potential to\nprovide insights into material properties without additional training. Our\nfindings indicate that, although LLMs can be used to generate representations\nreflecting certain property information, extracting the embeddings requires\nidentifying the optimal contextual clues and appropriate comparators. Despite\nthis restriction, it appears that LLMs still have the potential to be useful in\ngenerating meaningful materials-science representations."
                },
                "authors": [
                    {
                        "name": "Luke P. J. Gilligan"
                    },
                    {
                        "name": "Matteo Cobelli"
                    },
                    {
                        "name": "Hasan M. Sayeed"
                    },
                    {
                        "name": "Taylor D. Sparks"
                    },
                    {
                        "name": "Stefano Sanvito"
                    }
                ],
                "author_detail": {
                    "name": "Stefano Sanvito"
                },
                "author": "Stefano Sanvito",
                "arxiv_comment": "10 pages, 7 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.11971v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.11971v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cond-mat.mtrl-sci",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.11968v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.11968v1",
                "updated": "2024-09-18T13:20:23Z",
                "updated_parsed": [
                    2024,
                    9,
                    18,
                    13,
                    20,
                    23,
                    2,
                    262,
                    0
                ],
                "published": "2024-09-18T13:20:23Z",
                "published_parsed": [
                    2024,
                    9,
                    18,
                    13,
                    20,
                    23,
                    2,
                    262,
                    0
                ],
                "title": "Efficacy of Synthetic Data as a Benchmark",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficacy of Synthetic Data as a Benchmark"
                },
                "summary": "Large language models (LLMs) have enabled a range of applications in\nzero-shot and few-shot learning settings, including the generation of synthetic\ndatasets for training and testing. However, to reliably use these synthetic\ndatasets, it is essential to understand how representative they are of\nreal-world data. We investigate this by assessing the effectiveness of\ngenerating synthetic data through LLM and using it as a benchmark for various\nNLP tasks. Our experiments across six datasets, and three different tasks, show\nthat while synthetic data can effectively capture performance of various\nmethods for simpler tasks, such as intent classification, it falls short for\nmore complex tasks like named entity recognition. Additionally, we propose a\nnew metric called the bias factor, which evaluates the biases introduced when\nthe same LLM is used to both generate benchmarking data and to perform the\ntasks. We find that smaller LLMs exhibit biases towards their own generated\ndata, whereas larger models do not. Overall, our findings suggest that the\neffectiveness of synthetic data as a benchmark varies depending on the task,\nand that practitioners should rely on data generated from multiple larger\nmodels whenever possible.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have enabled a range of applications in\nzero-shot and few-shot learning settings, including the generation of synthetic\ndatasets for training and testing. However, to reliably use these synthetic\ndatasets, it is essential to understand how representative they are of\nreal-world data. We investigate this by assessing the effectiveness of\ngenerating synthetic data through LLM and using it as a benchmark for various\nNLP tasks. Our experiments across six datasets, and three different tasks, show\nthat while synthetic data can effectively capture performance of various\nmethods for simpler tasks, such as intent classification, it falls short for\nmore complex tasks like named entity recognition. Additionally, we propose a\nnew metric called the bias factor, which evaluates the biases introduced when\nthe same LLM is used to both generate benchmarking data and to perform the\ntasks. We find that smaller LLMs exhibit biases towards their own generated\ndata, whereas larger models do not. Overall, our findings suggest that the\neffectiveness of synthetic data as a benchmark varies depending on the task,\nand that practitioners should rely on data generated from multiple larger\nmodels whenever possible."
                },
                "authors": [
                    {
                        "name": "Gaurav Maheshwari"
                    },
                    {
                        "name": "Dmitry Ivanov"
                    },
                    {
                        "name": "Kevin El Haddad"
                    }
                ],
                "author_detail": {
                    "name": "Kevin El Haddad"
                },
                "author": "Kevin El Haddad",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.11968v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.11968v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.11967v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.11967v1",
                "updated": "2024-09-18T13:19:16Z",
                "updated_parsed": [
                    2024,
                    9,
                    18,
                    13,
                    19,
                    16,
                    2,
                    262,
                    0
                ],
                "published": "2024-09-18T13:19:16Z",
                "published_parsed": [
                    2024,
                    9,
                    18,
                    13,
                    19,
                    16,
                    2,
                    262,
                    0
                ],
                "title": "Incremental effects for continuous exposures",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Incremental effects for continuous exposures"
                },
                "summary": "Causal inference problems often involve continuous treatments, such as dose,\nduration, or frequency. However, continuous exposures bring many challenges,\nboth with identification and estimation. For example, identifying standard\ndose-response estimands requires that everyone has some chance of receiving any\nparticular level of the exposure (i.e., positivity). In this work, we explore\nan alternative approach: rather than estimating dose-response curves, we\nconsider stochastic interventions based on exponentially tilting the treatment\ndistribution by some parameter $\\delta$, which we term an incremental effect.\nThis increases or decreases the likelihood a unit receives a given treatment\nlevel, and crucially, does not require positivity for identification. We begin\nby deriving the efficient influence function and semiparametric efficiency\nbound for these incremental effects under continuous exposures. We then show\nthat estimation of the incremental effect is dependent on the size of the\nexponential tilt, as measured by $\\delta$. In particular, we derive new minimax\nlower bounds illustrating how the best possible root mean squared error scales\nwith an effective sample size of $n/\\delta$, instead of usual sample size $n$.\nFurther, we establish new convergence rates and bounds on the bias of double\nmachine learning-style estimators. Our novel analysis gives a better dependence\non $\\delta$ compared to standard analyses, by using mixed supremum and $L_2$\nnorms, instead of just $L_2$ norms from Cauchy-Schwarz bounds. Finally, we show\nthat taking $\\delta \\to \\infty$ gives a new estimator of the dose-response\ncurve at the edge of the support, and we give a detailed study of convergence\nrates in this regime.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Causal inference problems often involve continuous treatments, such as dose,\nduration, or frequency. However, continuous exposures bring many challenges,\nboth with identification and estimation. For example, identifying standard\ndose-response estimands requires that everyone has some chance of receiving any\nparticular level of the exposure (i.e., positivity). In this work, we explore\nan alternative approach: rather than estimating dose-response curves, we\nconsider stochastic interventions based on exponentially tilting the treatment\ndistribution by some parameter $\\delta$, which we term an incremental effect.\nThis increases or decreases the likelihood a unit receives a given treatment\nlevel, and crucially, does not require positivity for identification. We begin\nby deriving the efficient influence function and semiparametric efficiency\nbound for these incremental effects under continuous exposures. We then show\nthat estimation of the incremental effect is dependent on the size of the\nexponential tilt, as measured by $\\delta$. In particular, we derive new minimax\nlower bounds illustrating how the best possible root mean squared error scales\nwith an effective sample size of $n/\\delta$, instead of usual sample size $n$.\nFurther, we establish new convergence rates and bounds on the bias of double\nmachine learning-style estimators. Our novel analysis gives a better dependence\non $\\delta$ compared to standard analyses, by using mixed supremum and $L_2$\nnorms, instead of just $L_2$ norms from Cauchy-Schwarz bounds. Finally, we show\nthat taking $\\delta \\to \\infty$ gives a new estimator of the dose-response\ncurve at the edge of the support, and we give a detailed study of convergence\nrates in this regime."
                },
                "authors": [
                    {
                        "name": "Kyle Schindl"
                    },
                    {
                        "name": "Shuying Shen"
                    },
                    {
                        "name": "Edward H. Kennedy"
                    }
                ],
                "author_detail": {
                    "name": "Edward H. Kennedy"
                },
                "author": "Edward H. Kennedy",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.11967v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.11967v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ME",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.ST",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.TH",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.10516v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.10516v2",
                "updated": "2024-09-18T13:11:13Z",
                "updated_parsed": [
                    2024,
                    9,
                    18,
                    13,
                    11,
                    13,
                    2,
                    262,
                    0
                ],
                "published": "2024-09-16T17:59:52Z",
                "published_parsed": [
                    2024,
                    9,
                    16,
                    17,
                    59,
                    52,
                    0,
                    260,
                    0
                ],
                "title": "RetrievalAttention: Accelerating Long-Context LLM Inference via Vector\n  Retrieval",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RetrievalAttention: Accelerating Long-Context LLM Inference via Vector\n  Retrieval"
                },
                "summary": "Transformer-based Large Language Models (LLMs) have become increasingly\nimportant. However, due to the quadratic time complexity of attention\ncomputation, scaling LLMs to longer contexts incurs extremely slow inference\nlatency and high GPU memory consumption for caching key-value (KV) vectors.\nThis paper proposes RetrievalAttention, a training-free approach to both\naccelerate attention computation and reduce GPU memory consumption. By\nleveraging the dynamic sparsity of attention mechanism, RetrievalAttention\nproposes to use approximate nearest neighbor search (ANNS) indexes for KV\nvectors in CPU memory and retrieves the most relevant ones with vector search\nduring generation. Unfortunately, we observe that the off-the-shelf ANNS\nindexes are often ineffective for such retrieval tasks due to the\nout-of-distribution (OOD) between query vectors and key vectors in attention\nmechanism. RetrievalAttention addresses the OOD challenge by designing an\nattention-aware vector search algorithm that can adapt to the distribution of\nquery vectors. Our evaluation shows that RetrievalAttention only needs to\naccess 1--3% of data while maintaining high model accuracy. This leads to\nsignificant reduction in the inference cost of long-context LLMs with much\nlower GPU memory footprint. In particular, RetrievalAttention only needs a\nsingle NVIDIA RTX4090 (24GB) for serving 128K tokens in LLMs with 8B\nparameters, which is capable of generating one token in 0.188 seconds.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Transformer-based Large Language Models (LLMs) have become increasingly\nimportant. However, due to the quadratic time complexity of attention\ncomputation, scaling LLMs to longer contexts incurs extremely slow inference\nlatency and high GPU memory consumption for caching key-value (KV) vectors.\nThis paper proposes RetrievalAttention, a training-free approach to both\naccelerate attention computation and reduce GPU memory consumption. By\nleveraging the dynamic sparsity of attention mechanism, RetrievalAttention\nproposes to use approximate nearest neighbor search (ANNS) indexes for KV\nvectors in CPU memory and retrieves the most relevant ones with vector search\nduring generation. Unfortunately, we observe that the off-the-shelf ANNS\nindexes are often ineffective for such retrieval tasks due to the\nout-of-distribution (OOD) between query vectors and key vectors in attention\nmechanism. RetrievalAttention addresses the OOD challenge by designing an\nattention-aware vector search algorithm that can adapt to the distribution of\nquery vectors. Our evaluation shows that RetrievalAttention only needs to\naccess 1--3% of data while maintaining high model accuracy. This leads to\nsignificant reduction in the inference cost of long-context LLMs with much\nlower GPU memory footprint. In particular, RetrievalAttention only needs a\nsingle NVIDIA RTX4090 (24GB) for serving 128K tokens in LLMs with 8B\nparameters, which is capable of generating one token in 0.188 seconds."
                },
                "authors": [
                    {
                        "name": "Di Liu"
                    },
                    {
                        "name": "Meng Chen"
                    },
                    {
                        "name": "Baotong Lu"
                    },
                    {
                        "name": "Huiqiang Jiang"
                    },
                    {
                        "name": "Zhenhua Han"
                    },
                    {
                        "name": "Qianxi Zhang"
                    },
                    {
                        "name": "Qi Chen"
                    },
                    {
                        "name": "Chengruidong Zhang"
                    },
                    {
                        "name": "Bailu Ding"
                    },
                    {
                        "name": "Kai Zhang"
                    },
                    {
                        "name": "Chen Chen"
                    },
                    {
                        "name": "Fan Yang"
                    },
                    {
                        "name": "Yuqing Yang"
                    },
                    {
                        "name": "Lili Qiu"
                    }
                ],
                "author_detail": {
                    "name": "Lili Qiu"
                },
                "author": "Lili Qiu",
                "arxiv_comment": "16 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.10516v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.10516v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2402.12548v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2402.12548v2",
                "updated": "2024-09-18T13:10:56Z",
                "updated_parsed": [
                    2024,
                    9,
                    18,
                    13,
                    10,
                    56,
                    2,
                    262,
                    0
                ],
                "published": "2024-02-19T21:19:46Z",
                "published_parsed": [
                    2024,
                    2,
                    19,
                    21,
                    19,
                    46,
                    0,
                    50,
                    0
                ],
                "title": "Composite likelihood inference for space-time point processes",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Composite likelihood inference for space-time point processes"
                },
                "summary": "The dynamics of a rain forest is extremely complex involving births, deaths\nand growth of trees with complex interactions between trees, animals, climate,\nand environment. We consider the patterns of recruits (new trees) and dead\ntrees between rain forest censuses. For a current census we specify regression\nmodels for the conditional intensity of recruits and the conditional\nprobabilities of death given the current trees and spatial covariates. We\nestimate regression parameters using conditional composite likelihood functions\nthat only involve the conditional first order properties of the data. When\nconstructing assumption lean estimators of covariance matrices of parameter\nestimates we only need mild assumptions of decaying conditional correlations in\nspace while assumptions regarding correlations over time are avoided by\nexploiting conditional centering of composite likelihood score functions. Time\nseries of point patterns from rain forest censuses are quite short while each\npoint pattern covers a fairly big spatial region. To obtain asymptotic results\nwe therefore use a central limit theorem for the fixed timespan - increasing\nspatial domain asymptotic setting. This also allows us to handle the challenge\nof using stochastic covariates constructed from past point patterns.\nConveniently, it suffices to impose weak dependence assumptions on the\ninnovations of the space-time process. We investigate the proposed methodology\nby simulation studies and applications to rain forest data.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The dynamics of a rain forest is extremely complex involving births, deaths\nand growth of trees with complex interactions between trees, animals, climate,\nand environment. We consider the patterns of recruits (new trees) and dead\ntrees between rain forest censuses. For a current census we specify regression\nmodels for the conditional intensity of recruits and the conditional\nprobabilities of death given the current trees and spatial covariates. We\nestimate regression parameters using conditional composite likelihood functions\nthat only involve the conditional first order properties of the data. When\nconstructing assumption lean estimators of covariance matrices of parameter\nestimates we only need mild assumptions of decaying conditional correlations in\nspace while assumptions regarding correlations over time are avoided by\nexploiting conditional centering of composite likelihood score functions. Time\nseries of point patterns from rain forest censuses are quite short while each\npoint pattern covers a fairly big spatial region. To obtain asymptotic results\nwe therefore use a central limit theorem for the fixed timespan - increasing\nspatial domain asymptotic setting. This also allows us to handle the challenge\nof using stochastic covariates constructed from past point patterns.\nConveniently, it suffices to impose weak dependence assumptions on the\ninnovations of the space-time process. We investigate the proposed methodology\nby simulation studies and applications to rain forest data."
                },
                "authors": [
                    {
                        "name": "Abdollah Jalilian"
                    },
                    {
                        "name": "Francisco Cuevas-Pacheco"
                    },
                    {
                        "name": "Ganggang Xu"
                    },
                    {
                        "name": "Rasmus Waagepetersen"
                    }
                ],
                "author_detail": {
                    "name": "Rasmus Waagepetersen"
                },
                "author": "Rasmus Waagepetersen",
                "arxiv_comment": "This paper is still under revision",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2402.12548v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2402.12548v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ME",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2312.02515v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2312.02515v2",
                "updated": "2024-09-18T13:07:28Z",
                "updated_parsed": [
                    2024,
                    9,
                    18,
                    13,
                    7,
                    28,
                    2,
                    262,
                    0
                ],
                "published": "2023-12-05T05:38:38Z",
                "published_parsed": [
                    2023,
                    12,
                    5,
                    5,
                    38,
                    38,
                    1,
                    339,
                    0
                ],
                "title": "mLoRA: Fine-Tuning LoRA Adapters via Highly-Efficient Pipeline\n  Parallelism in Multiple GPUs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "mLoRA: Fine-Tuning LoRA Adapters via Highly-Efficient Pipeline\n  Parallelism in Multiple GPUs"
                },
                "summary": "Transformer-based, pre-trained large language models (LLMs) have demonstrated\noutstanding performance across diverse domains, particularly in the emerging\n{\\em pretrain-then-finetune} paradigm. Low-Rank Adaptation (LoRA), a\nparameter-efficient fine-tuning method, is commonly used to adapt a base LLM to\nmultiple downstream tasks. Further, LLM platforms enable developers to\nfine-tune multiple models and develop various domain-specific applications\nsimultaneously. However, existing model parallelism schemes suffer from high\ncommunication overhead and inefficient GPU utilization when training multiple\nLoRA tasks across GPUs and machines.\n  In this paper, we present mLoRA, a parallelism-efficient fine-tuning system\ndesigned for training multiple LoRA across GPUs and machines. mLoRA introduces\na novel LoRA-aware pipeline parallelism scheme that efficiently pipelines\nindependent LoRA adapters and their distinct fine-tuning stages across GPUs and\nmachines, along with a new LoRA-efficient operator to enhance GPU utilization\nduring pipelined LoRA training. Our extensive evaluation shows that mLoRA can\nsignificantly reduce average fine-tuning task completion time, e.g., by 30\\%,\ncompared to state-of-the-art methods like FSDP. More importantly, mLoRA enables\nsimultaneous fine-tuning of larger models, e.g., two Llama-2-13B models on four\nNVIDIA RTX A6000 48GB GPUs, which is not feasible for FSDP due to high memory\nrequirements. Hence, mLoRA not only increases fine-tuning efficiency but also\nmakes it more accessible on cost-effective GPUs. mLoRA has been deployed in\nAntGroup's production environment.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Transformer-based, pre-trained large language models (LLMs) have demonstrated\noutstanding performance across diverse domains, particularly in the emerging\n{\\em pretrain-then-finetune} paradigm. Low-Rank Adaptation (LoRA), a\nparameter-efficient fine-tuning method, is commonly used to adapt a base LLM to\nmultiple downstream tasks. Further, LLM platforms enable developers to\nfine-tune multiple models and develop various domain-specific applications\nsimultaneously. However, existing model parallelism schemes suffer from high\ncommunication overhead and inefficient GPU utilization when training multiple\nLoRA tasks across GPUs and machines.\n  In this paper, we present mLoRA, a parallelism-efficient fine-tuning system\ndesigned for training multiple LoRA across GPUs and machines. mLoRA introduces\na novel LoRA-aware pipeline parallelism scheme that efficiently pipelines\nindependent LoRA adapters and their distinct fine-tuning stages across GPUs and\nmachines, along with a new LoRA-efficient operator to enhance GPU utilization\nduring pipelined LoRA training. Our extensive evaluation shows that mLoRA can\nsignificantly reduce average fine-tuning task completion time, e.g., by 30\\%,\ncompared to state-of-the-art methods like FSDP. More importantly, mLoRA enables\nsimultaneous fine-tuning of larger models, e.g., two Llama-2-13B models on four\nNVIDIA RTX A6000 48GB GPUs, which is not feasible for FSDP due to high memory\nrequirements. Hence, mLoRA not only increases fine-tuning efficiency but also\nmakes it more accessible on cost-effective GPUs. mLoRA has been deployed in\nAntGroup's production environment."
                },
                "authors": [
                    {
                        "name": "Zhengmao Ye"
                    },
                    {
                        "name": "Dengchun Li"
                    },
                    {
                        "name": "Zetao Hu"
                    },
                    {
                        "name": "Tingfeng Lan"
                    },
                    {
                        "name": "Jian Sha"
                    },
                    {
                        "name": "Sicong Zhang"
                    },
                    {
                        "name": "Lei Duan"
                    },
                    {
                        "name": "Jie Zuo"
                    },
                    {
                        "name": "Hui Lu"
                    },
                    {
                        "name": "Yuanchun Zhou"
                    },
                    {
                        "name": "Mingjie Tang"
                    }
                ],
                "author_detail": {
                    "name": "Mingjie Tang"
                },
                "author": "Mingjie Tang",
                "arxiv_comment": "14 pages, 16 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2312.02515v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2312.02515v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.11951v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.11951v1",
                "updated": "2024-09-18T13:05:43Z",
                "updated_parsed": [
                    2024,
                    9,
                    18,
                    13,
                    5,
                    43,
                    2,
                    262,
                    0
                ],
                "published": "2024-09-18T13:05:43Z",
                "published_parsed": [
                    2024,
                    9,
                    18,
                    13,
                    5,
                    43,
                    2,
                    262,
                    0
                ],
                "title": "GaussianHeads: End-to-End Learning of Drivable Gaussian Head Avatars\n  from Coarse-to-fine Representations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GaussianHeads: End-to-End Learning of Drivable Gaussian Head Avatars\n  from Coarse-to-fine Representations"
                },
                "summary": "Real-time rendering of human head avatars is a cornerstone of many computer\ngraphics applications, such as augmented reality, video games, and films, to\nname a few. Recent approaches address this challenge with computationally\nefficient geometry primitives in a carefully calibrated multi-view setup.\nAlbeit producing photorealistic head renderings, it often fails to represent\ncomplex motion changes such as the mouth interior and strongly varying head\nposes. We propose a new method to generate highly dynamic and deformable human\nhead avatars from multi-view imagery in real-time. At the core of our method is\na hierarchical representation of head models that allows to capture the complex\ndynamics of facial expressions and head movements. First, with rich facial\nfeatures extracted from raw input frames, we learn to deform the coarse facial\ngeometry of the template mesh. We then initialize 3D Gaussians on the deformed\nsurface and refine their positions in a fine step. We train this coarse-to-fine\nfacial avatar model along with the head pose as a learnable parameter in an\nend-to-end framework. This enables not only controllable facial animation via\nvideo inputs, but also high-fidelity novel view synthesis of challenging facial\nexpressions, such as tongue deformations and fine-grained teeth structure under\nlarge motion changes. Moreover, it encourages the learned head avatar to\ngeneralize towards new facial expressions and head poses at inference time. We\ndemonstrate the performance of our method with comparisons against the related\nmethods on different datasets, spanning challenging facial expression sequences\nacross multiple identities. We also show the potential application of our\napproach by demonstrating a cross-identity facial performance transfer\napplication.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Real-time rendering of human head avatars is a cornerstone of many computer\ngraphics applications, such as augmented reality, video games, and films, to\nname a few. Recent approaches address this challenge with computationally\nefficient geometry primitives in a carefully calibrated multi-view setup.\nAlbeit producing photorealistic head renderings, it often fails to represent\ncomplex motion changes such as the mouth interior and strongly varying head\nposes. We propose a new method to generate highly dynamic and deformable human\nhead avatars from multi-view imagery in real-time. At the core of our method is\na hierarchical representation of head models that allows to capture the complex\ndynamics of facial expressions and head movements. First, with rich facial\nfeatures extracted from raw input frames, we learn to deform the coarse facial\ngeometry of the template mesh. We then initialize 3D Gaussians on the deformed\nsurface and refine their positions in a fine step. We train this coarse-to-fine\nfacial avatar model along with the head pose as a learnable parameter in an\nend-to-end framework. This enables not only controllable facial animation via\nvideo inputs, but also high-fidelity novel view synthesis of challenging facial\nexpressions, such as tongue deformations and fine-grained teeth structure under\nlarge motion changes. Moreover, it encourages the learned head avatar to\ngeneralize towards new facial expressions and head poses at inference time. We\ndemonstrate the performance of our method with comparisons against the related\nmethods on different datasets, spanning challenging facial expression sequences\nacross multiple identities. We also show the potential application of our\napproach by demonstrating a cross-identity facial performance transfer\napplication."
                },
                "authors": [
                    {
                        "name": "Kartik Teotia"
                    },
                    {
                        "name": "Hyeongwoo Kim"
                    },
                    {
                        "name": "Pablo Garrido"
                    },
                    {
                        "name": "Marc Habermann"
                    },
                    {
                        "name": "Mohamed Elgharib"
                    },
                    {
                        "name": "Christian Theobalt"
                    }
                ],
                "author_detail": {
                    "name": "Christian Theobalt"
                },
                "author": "Christian Theobalt",
                "arxiv_comment": "ACM Transaction on Graphics (SIGGRAPH Asia 2024); Project page:\n  https://vcai.mpi-inf.mpg.de/projects/GaussianHeads/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.11951v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.11951v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.GR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2311.14922v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2311.14922v2",
                "updated": "2024-09-18T12:39:06Z",
                "updated_parsed": [
                    2024,
                    9,
                    18,
                    12,
                    39,
                    6,
                    2,
                    262,
                    0
                ],
                "published": "2023-11-25T03:55:06Z",
                "published_parsed": [
                    2023,
                    11,
                    25,
                    3,
                    55,
                    6,
                    5,
                    329,
                    0
                ],
                "title": "GDTS: Goal-Guided Diffusion Model with Tree Sampling for Multi-Modal\n  Pedestrian Trajectory Prediction",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GDTS: Goal-Guided Diffusion Model with Tree Sampling for Multi-Modal\n  Pedestrian Trajectory Prediction"
                },
                "summary": "Accurate prediction of pedestrian trajectories is crucial for improving the\nsafety of autonomous driving. However, this task is generally nontrivial due to\nthe inherent stochasticity of human motion, which naturally requires the\npredictor to generate multi-modal prediction. Previous works leverage various\ngenerative methods, such as GAN and VAE, for pedestrian trajectory prediction.\nNevertheless, these methods may suffer from mode collapse and relatively\nlow-quality results. The denoising diffusion probabilistic model (DDPM) has\nrecently been applied to trajectory prediction due to its simple training\nprocess and powerful reconstruction ability. However, current diffusion-based\nmethods do not fully utilize input information and usually require many\ndenoising iterations that lead to a long inference time or an additional\nnetwork for initialization. To address these challenges and facilitate the use\nof diffusion models in multi-modal trajectory prediction, we propose GDTS, a\nnovel Goal-Guided Diffusion Model with Tree Sampling for multi-modal trajectory\nprediction. Considering the \"goal-driven\" characteristics of human motion, GDTS\nleverages goal estimation to guide the generation of the diffusion network. A\ntwo-stage tree sampling algorithm is presented, which leverages common features\nto reduce the inference time and improve accuracy for multi-modal prediction.\nExperimental results demonstrate that our proposed framework achieves\ncomparable state-of-the-art performance with real-time inference speed in\npublic datasets.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Accurate prediction of pedestrian trajectories is crucial for improving the\nsafety of autonomous driving. However, this task is generally nontrivial due to\nthe inherent stochasticity of human motion, which naturally requires the\npredictor to generate multi-modal prediction. Previous works leverage various\ngenerative methods, such as GAN and VAE, for pedestrian trajectory prediction.\nNevertheless, these methods may suffer from mode collapse and relatively\nlow-quality results. The denoising diffusion probabilistic model (DDPM) has\nrecently been applied to trajectory prediction due to its simple training\nprocess and powerful reconstruction ability. However, current diffusion-based\nmethods do not fully utilize input information and usually require many\ndenoising iterations that lead to a long inference time or an additional\nnetwork for initialization. To address these challenges and facilitate the use\nof diffusion models in multi-modal trajectory prediction, we propose GDTS, a\nnovel Goal-Guided Diffusion Model with Tree Sampling for multi-modal trajectory\nprediction. Considering the \"goal-driven\" characteristics of human motion, GDTS\nleverages goal estimation to guide the generation of the diffusion network. A\ntwo-stage tree sampling algorithm is presented, which leverages common features\nto reduce the inference time and improve accuracy for multi-modal prediction.\nExperimental results demonstrate that our proposed framework achieves\ncomparable state-of-the-art performance with real-time inference speed in\npublic datasets."
                },
                "authors": [
                    {
                        "name": "Ge Sun"
                    },
                    {
                        "name": "Sheng Wang"
                    },
                    {
                        "name": "Lei Zhu"
                    },
                    {
                        "name": "Ming Liu"
                    },
                    {
                        "name": "Jun Ma"
                    }
                ],
                "author_detail": {
                    "name": "Jun Ma"
                },
                "author": "Jun Ma",
                "arxiv_comment": "Submitted to ICRA 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2311.14922v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2311.14922v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.11920v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.11920v1",
                "updated": "2024-09-18T12:32:39Z",
                "updated_parsed": [
                    2024,
                    9,
                    18,
                    12,
                    32,
                    39,
                    2,
                    262,
                    0
                ],
                "published": "2024-09-18T12:32:39Z",
                "published_parsed": [
                    2024,
                    9,
                    18,
                    12,
                    32,
                    39,
                    2,
                    262,
                    0
                ],
                "title": "Generation of Complex 3D Human Motion by Temporal and Spatial\n  Composition of Diffusion Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Generation of Complex 3D Human Motion by Temporal and Spatial\n  Composition of Diffusion Models"
                },
                "summary": "In this paper, we address the challenge of generating realistic 3D human\nmotions for action classes that were never seen during the training phase. Our\napproach involves decomposing complex actions into simpler movements,\nspecifically those observed during training, by leveraging the knowledge of\nhuman motion contained in GPTs models. These simpler movements are then\ncombined into a single, realistic animation using the properties of diffusion\nmodels. Our claim is that this decomposition and subsequent recombination of\nsimple movements can synthesize an animation that accurately represents the\ncomplex input action. This method operates during the inference phase and can\nbe integrated with any pre-trained diffusion model, enabling the synthesis of\nmotion classes not present in the training data. We evaluate our method by\ndividing two benchmark human motion datasets into basic and complex actions,\nand then compare its performance against the state-of-the-art.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this paper, we address the challenge of generating realistic 3D human\nmotions for action classes that were never seen during the training phase. Our\napproach involves decomposing complex actions into simpler movements,\nspecifically those observed during training, by leveraging the knowledge of\nhuman motion contained in GPTs models. These simpler movements are then\ncombined into a single, realistic animation using the properties of diffusion\nmodels. Our claim is that this decomposition and subsequent recombination of\nsimple movements can synthesize an animation that accurately represents the\ncomplex input action. This method operates during the inference phase and can\nbe integrated with any pre-trained diffusion model, enabling the synthesis of\nmotion classes not present in the training data. We evaluate our method by\ndividing two benchmark human motion datasets into basic and complex actions,\nand then compare its performance against the state-of-the-art."
                },
                "authors": [
                    {
                        "name": "Lorenzo Mandelli"
                    },
                    {
                        "name": "Stefano Berretti"
                    }
                ],
                "author_detail": {
                    "name": "Stefano Berretti"
                },
                "author": "Stefano Berretti",
                "arxiv_comment": "13 pages, 6 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.11920v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.11920v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.0",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.11919v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.11919v1",
                "updated": "2024-09-18T12:32:25Z",
                "updated_parsed": [
                    2024,
                    9,
                    18,
                    12,
                    32,
                    25,
                    2,
                    262,
                    0
                ],
                "published": "2024-09-18T12:32:25Z",
                "published_parsed": [
                    2024,
                    9,
                    18,
                    12,
                    32,
                    25,
                    2,
                    262,
                    0
                ],
                "title": "LLM-wrapper: Black-Box Semantic-Aware Adaptation of Vision-Language\n  Foundation Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM-wrapper: Black-Box Semantic-Aware Adaptation of Vision-Language\n  Foundation Models"
                },
                "summary": "Vision Language Models (VLMs) have shown impressive performances on numerous\ntasks but their zero-shot capabilities can be limited compared to dedicated or\nfine-tuned models. Yet, fine-tuning VLMs comes with limitations as it requires\n`white-box' access to the model's architecture and weights as well as expertise\nto design the fine-tuning objectives and optimize the hyper-parameters, which\nare specific to each VLM and downstream task. In this work, we propose\nLLM-wrapper, a novel approach to adapt VLMs in a `black-box' manner by\nleveraging large language models (LLMs) so as to reason on their outputs. We\ndemonstrate the effectiveness of LLM-wrapper on Referring Expression\nComprehension (REC), a challenging open-vocabulary task that requires spatial\nand semantic reasoning. Our approach significantly boosts the performance of\noff-the-shelf models, resulting in competitive results when compared with\nclassic fine-tuning.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Vision Language Models (VLMs) have shown impressive performances on numerous\ntasks but their zero-shot capabilities can be limited compared to dedicated or\nfine-tuned models. Yet, fine-tuning VLMs comes with limitations as it requires\n`white-box' access to the model's architecture and weights as well as expertise\nto design the fine-tuning objectives and optimize the hyper-parameters, which\nare specific to each VLM and downstream task. In this work, we propose\nLLM-wrapper, a novel approach to adapt VLMs in a `black-box' manner by\nleveraging large language models (LLMs) so as to reason on their outputs. We\ndemonstrate the effectiveness of LLM-wrapper on Referring Expression\nComprehension (REC), a challenging open-vocabulary task that requires spatial\nand semantic reasoning. Our approach significantly boosts the performance of\noff-the-shelf models, resulting in competitive results when compared with\nclassic fine-tuning."
                },
                "authors": [
                    {
                        "name": "Amaia Cardiel"
                    },
                    {
                        "name": "Eloi Zablocki"
                    },
                    {
                        "name": "Oriane Siméoni"
                    },
                    {
                        "name": "Elias Ramzi"
                    },
                    {
                        "name": "Matthieu Cord"
                    }
                ],
                "author_detail": {
                    "name": "Matthieu Cord"
                },
                "author": "Matthieu Cord",
                "arxiv_comment": "EVAL-FoMo workshop, ECCV 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.11919v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.11919v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.11917v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.11917v1",
                "updated": "2024-09-18T12:29:22Z",
                "updated_parsed": [
                    2024,
                    9,
                    18,
                    12,
                    29,
                    22,
                    2,
                    262,
                    0
                ],
                "published": "2024-09-18T12:29:22Z",
                "published_parsed": [
                    2024,
                    9,
                    18,
                    12,
                    29,
                    22,
                    2,
                    262,
                    0
                ],
                "title": "LLMs in Education: Novel Perspectives, Challenges, and Opportunities",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLMs in Education: Novel Perspectives, Challenges, and Opportunities"
                },
                "summary": "The role of large language models (LLMs) in education is an increasing area\nof interest today, considering the new opportunities they offer for teaching,\nlearning, and assessment. This cutting-edge tutorial provides an overview of\nthe educational applications of NLP and the impact that the recent advances in\nLLMs have had on this field. We will discuss the key challenges and\nopportunities presented by LLMs, grounding them in the context of four major\neducational applications: reading, writing, and speaking skills, and\nintelligent tutoring systems (ITS). This COLING 2025 tutorial is designed for\nresearchers and practitioners interested in the educational applications of NLP\nand the role LLMs have to play in this area. It is the first of its kind to\naddress this timely topic.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The role of large language models (LLMs) in education is an increasing area\nof interest today, considering the new opportunities they offer for teaching,\nlearning, and assessment. This cutting-edge tutorial provides an overview of\nthe educational applications of NLP and the impact that the recent advances in\nLLMs have had on this field. We will discuss the key challenges and\nopportunities presented by LLMs, grounding them in the context of four major\neducational applications: reading, writing, and speaking skills, and\nintelligent tutoring systems (ITS). This COLING 2025 tutorial is designed for\nresearchers and practitioners interested in the educational applications of NLP\nand the role LLMs have to play in this area. It is the first of its kind to\naddress this timely topic."
                },
                "authors": [
                    {
                        "name": "Bashar Alhafni"
                    },
                    {
                        "name": "Sowmya Vajjala"
                    },
                    {
                        "name": "Stefano Bannò"
                    },
                    {
                        "name": "Kaushal Kumar Maurya"
                    },
                    {
                        "name": "Ekaterina Kochmar"
                    }
                ],
                "author_detail": {
                    "name": "Ekaterina Kochmar"
                },
                "author": "Ekaterina Kochmar",
                "arxiv_comment": "COLING 2025 Tutorial",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.11917v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.11917v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2310.12537v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2310.12537v5",
                "updated": "2024-09-20T08:49:37Z",
                "updated_parsed": [
                    2024,
                    9,
                    20,
                    8,
                    49,
                    37,
                    4,
                    264,
                    0
                ],
                "published": "2023-10-19T07:39:00Z",
                "published_parsed": [
                    2023,
                    10,
                    19,
                    7,
                    39,
                    0,
                    3,
                    292,
                    0
                ],
                "title": "ExtractGPT: Exploring the Potential of Large Language Models for Product\n  Attribute Value Extraction",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ExtractGPT: Exploring the Potential of Large Language Models for Product\n  Attribute Value Extraction"
                },
                "summary": "E-commerce platforms require structured product data in the form of\nattribute-value pairs to offer features such as faceted product search or\nattribute-based product comparison. However, vendors often provide unstructured\nproduct descriptions, necessitating the extraction of attribute-value pairs\nfrom these texts. BERT-based extraction methods require large amounts of\ntask-specific training data and struggle with unseen attribute values. This\npaper explores using large language models (LLMs) as a more training-data\nefficient and robust alternative. We propose prompt templates for zero-shot and\nfew-shot scenarios, comparing textual and JSON-based target schema\nrepresentations. Our experiments show that GPT-4 achieves the highest average\nF1-score of 85% using detailed attribute descriptions and demonstrations.\nLlama-3-70B performs nearly as well, offering a competitive open-source\nalternative. GPT-4 surpasses the best PLM baseline by 5% in F1-score.\nFine-tuning GPT-3.5 increases the performance to the level of GPT-4 but reduces\nthe model's ability to generalize to unseen attribute values.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "E-commerce platforms require structured product data in the form of\nattribute-value pairs to offer features such as faceted product search or\nattribute-based product comparison. However, vendors often provide unstructured\nproduct descriptions, necessitating the extraction of attribute-value pairs\nfrom these texts. BERT-based extraction methods require large amounts of\ntask-specific training data and struggle with unseen attribute values. This\npaper explores using large language models (LLMs) as a more training-data\nefficient and robust alternative. We propose prompt templates for zero-shot and\nfew-shot scenarios, comparing textual and JSON-based target schema\nrepresentations. Our experiments show that GPT-4 achieves the highest average\nF1-score of 85% using detailed attribute descriptions and demonstrations.\nLlama-3-70B performs nearly as well, offering a competitive open-source\nalternative. GPT-4 surpasses the best PLM baseline by 5% in F1-score.\nFine-tuning GPT-3.5 increases the performance to the level of GPT-4 but reduces\nthe model's ability to generalize to unseen attribute values."
                },
                "authors": [
                    {
                        "name": "Alexander Brinkmann"
                    },
                    {
                        "name": "Roee Shraga"
                    },
                    {
                        "name": "Christian Bizer"
                    }
                ],
                "author_detail": {
                    "name": "Christian Bizer"
                },
                "author": "Christian Bizer",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2310.12537v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2310.12537v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.12895v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.12895v4",
                "updated": "2024-09-22T13:38:17Z",
                "updated_parsed": [
                    2024,
                    9,
                    22,
                    13,
                    38,
                    17,
                    6,
                    266,
                    0
                ],
                "published": "2024-04-19T13:58:34Z",
                "published_parsed": [
                    2024,
                    4,
                    19,
                    13,
                    58,
                    34,
                    4,
                    110,
                    0
                ],
                "title": "The emergence of subjective temporality: the self-simulational theory of\n  temporal extension from the perspective of the free energy principle",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The emergence of subjective temporality: the self-simulational theory of\n  temporal extension from the perspective of the free energy principle"
                },
                "summary": "The self-simulational theory of temporal extension describes an\ninformation-theoretically formalized mechanism by which the width of subjective\ntemporality emerges from the architecture of self-modelling. In this paper, the\nperspective of the free energy principle will be assumed, to cast the emergence\nof subjective temporality, along with a Bayesian mechanism for hierarchical\nduration estimation, from first principles of the physics of self-organization.\nUsing active inference, a deep parametric generative model of temporal\ninference is simulated, which realizes the described dynamics on a\ncomputational level. Two biases (i.e. variations) of time-perception naturally\nemerge from the simulated computational model. This concerns the intentional\nbinding effect (i.e. the compression of the temporal interval between\nvoluntarily initiated actions and subsequent sensory consequences) and\nempirically documented alterations of subjective time experience in deep states\nof meditative absorption (i.e. in minimal phenomenal experience). Generally,\nnumerous systematic and domain-specific alterations of subjective temporal\nexperience are computationally explained in a unified manner, as enabled by\nintegration with current active inference accounts mapping onto the respective\ndomains. This concerns - next to more general scale-invariant effects of\nexplicit timing and central tendency effects - the temporality-modulating role\nof valence, impulsivity, boredom, flow-states, near death-experiences, and\nvarious psychopathologies, amongst others. The self-simulational theory of\ntemporal extension, from the perspective of the free energy principle, explains\nhow the subjective temporal Now emerges and varies from first principles,\naccounting for why sometimes, subjective time seems to fly, and sometimes,\nmoments feel like eternities; with the computational mechanism being readily\ndeployable synthetically.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The self-simulational theory of temporal extension describes an\ninformation-theoretically formalized mechanism by which the width of subjective\ntemporality emerges from the architecture of self-modelling. In this paper, the\nperspective of the free energy principle will be assumed, to cast the emergence\nof subjective temporality, along with a Bayesian mechanism for hierarchical\nduration estimation, from first principles of the physics of self-organization.\nUsing active inference, a deep parametric generative model of temporal\ninference is simulated, which realizes the described dynamics on a\ncomputational level. Two biases (i.e. variations) of time-perception naturally\nemerge from the simulated computational model. This concerns the intentional\nbinding effect (i.e. the compression of the temporal interval between\nvoluntarily initiated actions and subsequent sensory consequences) and\nempirically documented alterations of subjective time experience in deep states\nof meditative absorption (i.e. in minimal phenomenal experience). Generally,\nnumerous systematic and domain-specific alterations of subjective temporal\nexperience are computationally explained in a unified manner, as enabled by\nintegration with current active inference accounts mapping onto the respective\ndomains. This concerns - next to more general scale-invariant effects of\nexplicit timing and central tendency effects - the temporality-modulating role\nof valence, impulsivity, boredom, flow-states, near death-experiences, and\nvarious psychopathologies, amongst others. The self-simulational theory of\ntemporal extension, from the perspective of the free energy principle, explains\nhow the subjective temporal Now emerges and varies from first principles,\naccounting for why sometimes, subjective time seems to fly, and sometimes,\nmoments feel like eternities; with the computational mechanism being readily\ndeployable synthetically."
                },
                "authors": [
                    {
                        "name": "Jan Erik Bellingrath"
                    }
                ],
                "author_detail": {
                    "name": "Jan Erik Bellingrath"
                },
                "author": "Jan Erik Bellingrath",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.12895v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.12895v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "q-bio.NC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "q-bio.NC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.11901v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.11901v1",
                "updated": "2024-09-18T11:54:45Z",
                "updated_parsed": [
                    2024,
                    9,
                    18,
                    11,
                    54,
                    45,
                    2,
                    262,
                    0
                ],
                "published": "2024-09-18T11:54:45Z",
                "published_parsed": [
                    2024,
                    9,
                    18,
                    11,
                    54,
                    45,
                    2,
                    262,
                    0
                ],
                "title": "LLMs + Persona-Plug = Personalized LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLMs + Persona-Plug = Personalized LLMs"
                },
                "summary": "Personalization plays a critical role in numerous language tasks and\napplications, since users with the same requirements may prefer diverse outputs\nbased on their individual interests. This has led to the development of various\npersonalized approaches aimed at adapting large language models (LLMs) to\ngenerate customized outputs aligned with user preferences. Some of them involve\nfine-tuning a unique personalized LLM for each user, which is too expensive for\nwidespread application. Alternative approaches introduce personalization\ninformation in a plug-and-play manner by retrieving the user's relevant\nhistorical texts as demonstrations. However, this retrieval-based strategy may\nbreak the continuity of the user history and fail to capture the user's overall\nstyles and patterns, hence leading to sub-optimal performance. To address these\nchallenges, we propose a novel personalized LLM model, \\ours{}. It constructs a\nuser-specific embedding for each individual by modeling all her historical\ncontexts through a lightweight plug-in user embedder module. By attaching this\nembedding to the task input, LLMs can better understand and capture user habits\nand preferences, thereby producing more personalized outputs without tuning\ntheir own parameters. Extensive experiments on various tasks in the language\nmodel personalization (LaMP) benchmark demonstrate that the proposed model\nsignificantly outperforms existing personalized LLM approaches.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Personalization plays a critical role in numerous language tasks and\napplications, since users with the same requirements may prefer diverse outputs\nbased on their individual interests. This has led to the development of various\npersonalized approaches aimed at adapting large language models (LLMs) to\ngenerate customized outputs aligned with user preferences. Some of them involve\nfine-tuning a unique personalized LLM for each user, which is too expensive for\nwidespread application. Alternative approaches introduce personalization\ninformation in a plug-and-play manner by retrieving the user's relevant\nhistorical texts as demonstrations. However, this retrieval-based strategy may\nbreak the continuity of the user history and fail to capture the user's overall\nstyles and patterns, hence leading to sub-optimal performance. To address these\nchallenges, we propose a novel personalized LLM model, \\ours{}. It constructs a\nuser-specific embedding for each individual by modeling all her historical\ncontexts through a lightweight plug-in user embedder module. By attaching this\nembedding to the task input, LLMs can better understand and capture user habits\nand preferences, thereby producing more personalized outputs without tuning\ntheir own parameters. Extensive experiments on various tasks in the language\nmodel personalization (LaMP) benchmark demonstrate that the proposed model\nsignificantly outperforms existing personalized LLM approaches."
                },
                "authors": [
                    {
                        "name": "Jiongnan Liu"
                    },
                    {
                        "name": "Yutao Zhu"
                    },
                    {
                        "name": "Shuting Wang"
                    },
                    {
                        "name": "Xiaochi Wei"
                    },
                    {
                        "name": "Erxue Min"
                    },
                    {
                        "name": "Yu Lu"
                    },
                    {
                        "name": "Shuaiqiang Wang"
                    },
                    {
                        "name": "Dawei Yin"
                    },
                    {
                        "name": "Zhicheng Dou"
                    }
                ],
                "author_detail": {
                    "name": "Zhicheng Dou"
                },
                "author": "Zhicheng Dou",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.11901v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.11901v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.11863v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.11863v1",
                "updated": "2024-09-18T10:36:47Z",
                "updated_parsed": [
                    2024,
                    9,
                    18,
                    10,
                    36,
                    47,
                    2,
                    262,
                    0
                ],
                "published": "2024-09-18T10:36:47Z",
                "published_parsed": [
                    2024,
                    9,
                    18,
                    10,
                    36,
                    47,
                    2,
                    262,
                    0
                ],
                "title": "Learning Task Planning from Multi-Modal Demonstration for Multi-Stage\n  Contact-Rich Manipulation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Learning Task Planning from Multi-Modal Demonstration for Multi-Stage\n  Contact-Rich Manipulation"
                },
                "summary": "Large Language Models (LLMs) have gained popularity in task planning for\nlong-horizon manipulation tasks. To enhance the validity of LLM-generated\nplans, visual demonstrations and online videos have been widely employed to\nguide the planning process. However, for manipulation tasks involving subtle\nmovements but rich contact interactions, visual perception alone may be\ninsufficient for the LLM to fully interpret the demonstration. Additionally,\nvisual data provides limited information on force-related parameters and\nconditions, which are crucial for effective execution on real robots.\n  In this paper, we introduce an in-context learning framework that\nincorporates tactile and force-torque information from human demonstrations to\nenhance LLMs' ability to generate plans for new task scenarios. We propose a\nbootstrapped reasoning pipeline that sequentially integrates each modality into\na comprehensive task plan. This task plan is then used as a reference for\nplanning in new task configurations. Real-world experiments on two different\nsequential manipulation tasks demonstrate the effectiveness of our framework in\nimproving LLMs' understanding of multi-modal demonstrations and enhancing the\noverall planning performance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have gained popularity in task planning for\nlong-horizon manipulation tasks. To enhance the validity of LLM-generated\nplans, visual demonstrations and online videos have been widely employed to\nguide the planning process. However, for manipulation tasks involving subtle\nmovements but rich contact interactions, visual perception alone may be\ninsufficient for the LLM to fully interpret the demonstration. Additionally,\nvisual data provides limited information on force-related parameters and\nconditions, which are crucial for effective execution on real robots.\n  In this paper, we introduce an in-context learning framework that\nincorporates tactile and force-torque information from human demonstrations to\nenhance LLMs' ability to generate plans for new task scenarios. We propose a\nbootstrapped reasoning pipeline that sequentially integrates each modality into\na comprehensive task plan. This task plan is then used as a reference for\nplanning in new task configurations. Real-world experiments on two different\nsequential manipulation tasks demonstrate the effectiveness of our framework in\nimproving LLMs' understanding of multi-modal demonstrations and enhancing the\noverall planning performance."
                },
                "authors": [
                    {
                        "name": "Kejia Chen"
                    },
                    {
                        "name": "Zheng Shen"
                    },
                    {
                        "name": "Yue Zhang"
                    },
                    {
                        "name": "Lingyun Chen"
                    },
                    {
                        "name": "Fan Wu"
                    },
                    {
                        "name": "Zhenshan Bing"
                    },
                    {
                        "name": "Sami Haddadin"
                    },
                    {
                        "name": "Alois Knoll"
                    }
                ],
                "author_detail": {
                    "name": "Alois Knoll"
                },
                "author": "Alois Knoll",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.11863v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.11863v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.11860v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.11860v1",
                "updated": "2024-09-18T10:30:50Z",
                "updated_parsed": [
                    2024,
                    9,
                    18,
                    10,
                    30,
                    50,
                    2,
                    262,
                    0
                ],
                "published": "2024-09-18T10:30:50Z",
                "published_parsed": [
                    2024,
                    9,
                    18,
                    10,
                    30,
                    50,
                    2,
                    262,
                    0
                ],
                "title": "Retrieve, Annotate, Evaluate, Repeat: Leveraging Multimodal LLMs for\n  Large-Scale Product Retrieval Evaluation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Retrieve, Annotate, Evaluate, Repeat: Leveraging Multimodal LLMs for\n  Large-Scale Product Retrieval Evaluation"
                },
                "summary": "Evaluating production-level retrieval systems at scale is a crucial yet\nchallenging task due to the limited availability of a large pool of\nwell-trained human annotators. Large Language Models (LLMs) have the potential\nto address this scaling issue and offer a viable alternative to humans for the\nbulk of annotation tasks. In this paper, we propose a framework for assessing\nthe product search engines in a large-scale e-commerce setting, leveraging\nMultimodal LLMs for (i) generating tailored annotation guidelines for\nindividual queries, and (ii) conducting the subsequent annotation task. Our\nmethod, validated through deployment on a large e-commerce platform,\ndemonstrates comparable quality to human annotations, significantly reduces\ntime and cost, facilitates rapid problem discovery, and provides an effective\nsolution for production-level quality control at scale.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Evaluating production-level retrieval systems at scale is a crucial yet\nchallenging task due to the limited availability of a large pool of\nwell-trained human annotators. Large Language Models (LLMs) have the potential\nto address this scaling issue and offer a viable alternative to humans for the\nbulk of annotation tasks. In this paper, we propose a framework for assessing\nthe product search engines in a large-scale e-commerce setting, leveraging\nMultimodal LLMs for (i) generating tailored annotation guidelines for\nindividual queries, and (ii) conducting the subsequent annotation task. Our\nmethod, validated through deployment on a large e-commerce platform,\ndemonstrates comparable quality to human annotations, significantly reduces\ntime and cost, facilitates rapid problem discovery, and provides an effective\nsolution for production-level quality control at scale."
                },
                "authors": [
                    {
                        "name": "Kasra Hosseini"
                    },
                    {
                        "name": "Thomas Kober"
                    },
                    {
                        "name": "Josip Krapac"
                    },
                    {
                        "name": "Roland Vollgraf"
                    },
                    {
                        "name": "Weiwei Cheng"
                    },
                    {
                        "name": "Ana Peleteiro Ramallo"
                    }
                ],
                "author_detail": {
                    "name": "Ana Peleteiro Ramallo"
                },
                "author": "Ana Peleteiro Ramallo",
                "arxiv_comment": "13 pages, 5 figures, 4 Tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.11860v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.11860v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.ET",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.11022v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.11022v2",
                "updated": "2024-09-18T10:05:02Z",
                "updated_parsed": [
                    2024,
                    9,
                    18,
                    10,
                    5,
                    2,
                    2,
                    262,
                    0
                ],
                "published": "2024-09-17T09:32:12Z",
                "published_parsed": [
                    2024,
                    9,
                    17,
                    9,
                    32,
                    12,
                    1,
                    261,
                    0
                ],
                "title": "GEIC: Universal and Multilingual Named Entity Recognition with Large\n  Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GEIC: Universal and Multilingual Named Entity Recognition with Large\n  Language Models"
                },
                "summary": "Large Language Models (LLMs) have supplanted traditional methods in numerous\nnatural language processing tasks. Nonetheless, in Named Entity Recognition\n(NER), existing LLM-based methods underperform compared to baselines and\nrequire significantly more computational resources, limiting their application.\nIn this paper, we introduce the task of generation-based extraction and\nin-context classification (GEIC), designed to leverage LLMs' prior knowledge\nand self-attention mechanisms for NER tasks. We then propose CascadeNER, a\nuniversal and multilingual GEIC framework for few-shot and zero-shot NER.\nCascadeNER employs model cascading to utilize two small-parameter LLMs to\nextract and classify independently, reducing resource consumption while\nenhancing accuracy. We also introduce AnythingNER, the first NER dataset\nspecifically designed for LLMs, including 8 languages, 155 entity types and a\nnovel dynamic categorization system. Experiments show that CascadeNER achieves\nstate-of-the-art performance on low-resource and fine-grained scenarios,\nincluding CrossNER and FewNERD. Our work is openly accessible.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have supplanted traditional methods in numerous\nnatural language processing tasks. Nonetheless, in Named Entity Recognition\n(NER), existing LLM-based methods underperform compared to baselines and\nrequire significantly more computational resources, limiting their application.\nIn this paper, we introduce the task of generation-based extraction and\nin-context classification (GEIC), designed to leverage LLMs' prior knowledge\nand self-attention mechanisms for NER tasks. We then propose CascadeNER, a\nuniversal and multilingual GEIC framework for few-shot and zero-shot NER.\nCascadeNER employs model cascading to utilize two small-parameter LLMs to\nextract and classify independently, reducing resource consumption while\nenhancing accuracy. We also introduce AnythingNER, the first NER dataset\nspecifically designed for LLMs, including 8 languages, 155 entity types and a\nnovel dynamic categorization system. Experiments show that CascadeNER achieves\nstate-of-the-art performance on low-resource and fine-grained scenarios,\nincluding CrossNER and FewNERD. Our work is openly accessible."
                },
                "authors": [
                    {
                        "name": "Hanjun Luo"
                    },
                    {
                        "name": "Yingbin Jin"
                    },
                    {
                        "name": "Xuecheng Liu"
                    },
                    {
                        "name": "Tong Shang"
                    },
                    {
                        "name": "Ruizhe Chen"
                    },
                    {
                        "name": "Zuozhu Liu"
                    }
                ],
                "author_detail": {
                    "name": "Zuozhu Liu"
                },
                "author": "Zuozhu Liu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.11022v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.11022v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.11844v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.11844v1",
                "updated": "2024-09-18T09:55:48Z",
                "updated_parsed": [
                    2024,
                    9,
                    18,
                    9,
                    55,
                    48,
                    2,
                    262,
                    0
                ],
                "published": "2024-09-18T09:55:48Z",
                "published_parsed": [
                    2024,
                    9,
                    18,
                    9,
                    55,
                    48,
                    2,
                    262,
                    0
                ],
                "title": "MEOW: MEMOry Supervised LLM Unlearning Via Inverted Facts",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MEOW: MEMOry Supervised LLM Unlearning Via Inverted Facts"
                },
                "summary": "Large Language Models (LLMs) can memorize sensitive information, raising\nconcerns about potential misuse. LLM Unlearning, a post-hoc approach to remove\nthis information from trained LLMs, offers a promising solution to mitigate\nthese risks. However, previous practices face three key challenges: 1. Utility:\nsuccessful unlearning often causes catastrophic collapse on unrelated tasks. 2.\nEfficiency: many methods either involve adding similarly sized models, which\nslows down unlearning or inference, or require retain data that are difficult\nto obtain. 3. Robustness: even effective methods may still leak data via\nextraction techniques. To address these challenges, we propose MEOW, a simple\nyet effective gradient descent-based unlearning method. Specifically, we use an\noffline LLM to generate a set of inverted facts. Then, we design a new metric,\nMEMO, to quantify memorization in LLMs. Finally, based on the signals provided\nby MEMO, we select the most appropriate set of inverted facts and finetune the\nmodel based on them. We evaluate MEOW on the commonly used unlearn benchmark,\nToFU, with Llama2-7B-Chat and Phi-1.5B, and test it on both NLU and NLG tasks.\nResults demonstrate significant improvement of MEOW in forget quality without\nsubstantial loss in model utility. Meanwhile, MEOW does not exhibit significant\ndegradation in NLU or NLG capabilities, and there is even a slight improvement\nin NLU performance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) can memorize sensitive information, raising\nconcerns about potential misuse. LLM Unlearning, a post-hoc approach to remove\nthis information from trained LLMs, offers a promising solution to mitigate\nthese risks. However, previous practices face three key challenges: 1. Utility:\nsuccessful unlearning often causes catastrophic collapse on unrelated tasks. 2.\nEfficiency: many methods either involve adding similarly sized models, which\nslows down unlearning or inference, or require retain data that are difficult\nto obtain. 3. Robustness: even effective methods may still leak data via\nextraction techniques. To address these challenges, we propose MEOW, a simple\nyet effective gradient descent-based unlearning method. Specifically, we use an\noffline LLM to generate a set of inverted facts. Then, we design a new metric,\nMEMO, to quantify memorization in LLMs. Finally, based on the signals provided\nby MEMO, we select the most appropriate set of inverted facts and finetune the\nmodel based on them. We evaluate MEOW on the commonly used unlearn benchmark,\nToFU, with Llama2-7B-Chat and Phi-1.5B, and test it on both NLU and NLG tasks.\nResults demonstrate significant improvement of MEOW in forget quality without\nsubstantial loss in model utility. Meanwhile, MEOW does not exhibit significant\ndegradation in NLU or NLG capabilities, and there is even a slight improvement\nin NLU performance."
                },
                "authors": [
                    {
                        "name": "Tianle Gu"
                    },
                    {
                        "name": "Kexin Huang"
                    },
                    {
                        "name": "Ruilin Luo"
                    },
                    {
                        "name": "Yuanqi Yao"
                    },
                    {
                        "name": "Yujiu Yang"
                    },
                    {
                        "name": "Yan Teng"
                    },
                    {
                        "name": "Yingchun Wang"
                    }
                ],
                "author_detail": {
                    "name": "Yingchun Wang"
                },
                "author": "Yingchun Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.11844v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.11844v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.11835v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.11835v1",
                "updated": "2024-09-18T09:36:55Z",
                "updated_parsed": [
                    2024,
                    9,
                    18,
                    9,
                    36,
                    55,
                    2,
                    262,
                    0
                ],
                "published": "2024-09-18T09:36:55Z",
                "published_parsed": [
                    2024,
                    9,
                    18,
                    9,
                    36,
                    55,
                    2,
                    262,
                    0
                ],
                "title": "DPI-TTS: Directional Patch Interaction for Fast-Converging and Style\n  Temporal Modeling in Text-to-Speech",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DPI-TTS: Directional Patch Interaction for Fast-Converging and Style\n  Temporal Modeling in Text-to-Speech"
                },
                "summary": "In recent years, speech diffusion models have advanced rapidly. Alongside the\nwidely used U-Net architecture, transformer-based models such as the Diffusion\nTransformer (DiT) have also gained attention. However, current DiT speech\nmodels treat Mel spectrograms as general images, which overlooks the specific\nacoustic properties of speech. To address these limitations, we propose a\nmethod called Directional Patch Interaction for Text-to-Speech (DPI-TTS), which\nbuilds on DiT and achieves fast training without compromising accuracy.\nNotably, DPI-TTS employs a low-to-high frequency, frame-by-frame progressive\ninference approach that aligns more closely with acoustic properties, enhancing\nthe naturalness of the generated speech. Additionally, we introduce a\nfine-grained style temporal modeling method that further improves speaker style\nsimilarity. Experimental results demonstrate that our method increases the\ntraining speed by nearly 2 times and significantly outperforms the baseline\nmodels.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In recent years, speech diffusion models have advanced rapidly. Alongside the\nwidely used U-Net architecture, transformer-based models such as the Diffusion\nTransformer (DiT) have also gained attention. However, current DiT speech\nmodels treat Mel spectrograms as general images, which overlooks the specific\nacoustic properties of speech. To address these limitations, we propose a\nmethod called Directional Patch Interaction for Text-to-Speech (DPI-TTS), which\nbuilds on DiT and achieves fast training without compromising accuracy.\nNotably, DPI-TTS employs a low-to-high frequency, frame-by-frame progressive\ninference approach that aligns more closely with acoustic properties, enhancing\nthe naturalness of the generated speech. Additionally, we introduce a\nfine-grained style temporal modeling method that further improves speaker style\nsimilarity. Experimental results demonstrate that our method increases the\ntraining speed by nearly 2 times and significantly outperforms the baseline\nmodels."
                },
                "authors": [
                    {
                        "name": "Xin Qi"
                    },
                    {
                        "name": "Ruibo Fu"
                    },
                    {
                        "name": "Zhengqi Wen"
                    },
                    {
                        "name": "Tao Wang"
                    },
                    {
                        "name": "Chunyu Qiang"
                    },
                    {
                        "name": "Jianhua Tao"
                    },
                    {
                        "name": "Chenxing Li"
                    },
                    {
                        "name": "Yi Lu"
                    },
                    {
                        "name": "Shuchen Shi"
                    },
                    {
                        "name": "Zhiyong Wang"
                    },
                    {
                        "name": "Xiaopeng Wang"
                    },
                    {
                        "name": "Yuankun Xie"
                    },
                    {
                        "name": "Yukun Liu"
                    },
                    {
                        "name": "Xuefei Liu"
                    },
                    {
                        "name": "Guanjun Li"
                    }
                ],
                "author_detail": {
                    "name": "Guanjun Li"
                },
                "author": "Guanjun Li",
                "arxiv_comment": "Submitted to ICASSP2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.11835v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.11835v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SD",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.11172v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.11172v2",
                "updated": "2024-09-18T09:35:15Z",
                "updated_parsed": [
                    2024,
                    9,
                    18,
                    9,
                    35,
                    15,
                    2,
                    262,
                    0
                ],
                "published": "2024-09-17T13:26:17Z",
                "published_parsed": [
                    2024,
                    9,
                    17,
                    13,
                    26,
                    17,
                    1,
                    261,
                    0
                ],
                "title": "Annealed Winner-Takes-All for Motion Forecasting",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Annealed Winner-Takes-All for Motion Forecasting"
                },
                "summary": "In autonomous driving, motion prediction aims at forecasting the future\ntrajectories of nearby agents, helping the ego vehicle to anticipate behaviors\nand drive safely. A key challenge is generating a diverse set of future\npredictions, commonly addressed using data-driven models with Multiple Choice\nLearning (MCL) architectures and Winner-Takes-All (WTA) training objectives.\nHowever, these methods face initialization sensitivity and training\ninstabilities. Additionally, to compensate for limited performance, some\napproaches rely on training with a large set of hypotheses, requiring a\npost-selection step during inference to significantly reduce the number of\npredictions. To tackle these issues, we take inspiration from annealed MCL, a\nrecently introduced technique that improves the convergence properties of MCL\nmethods through an annealed Winner-Takes-All loss (aWTA). In this paper, we\ndemonstrate how the aWTA loss can be integrated with state-of-the-art motion\nforecasting models to enhance their performance using only a minimal set of\nhypotheses, eliminating the need for the cumbersome post-selection step. Our\napproach can be easily incorporated into any trajectory prediction model\nnormally trained using WTA and yields significant improvements. To facilitate\nthe application of our approach to future motion forecasting models, the code\nwill be made publicly available upon acceptance:\nhttps://github.com/valeoai/MF_aWTA.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In autonomous driving, motion prediction aims at forecasting the future\ntrajectories of nearby agents, helping the ego vehicle to anticipate behaviors\nand drive safely. A key challenge is generating a diverse set of future\npredictions, commonly addressed using data-driven models with Multiple Choice\nLearning (MCL) architectures and Winner-Takes-All (WTA) training objectives.\nHowever, these methods face initialization sensitivity and training\ninstabilities. Additionally, to compensate for limited performance, some\napproaches rely on training with a large set of hypotheses, requiring a\npost-selection step during inference to significantly reduce the number of\npredictions. To tackle these issues, we take inspiration from annealed MCL, a\nrecently introduced technique that improves the convergence properties of MCL\nmethods through an annealed Winner-Takes-All loss (aWTA). In this paper, we\ndemonstrate how the aWTA loss can be integrated with state-of-the-art motion\nforecasting models to enhance their performance using only a minimal set of\nhypotheses, eliminating the need for the cumbersome post-selection step. Our\napproach can be easily incorporated into any trajectory prediction model\nnormally trained using WTA and yields significant improvements. To facilitate\nthe application of our approach to future motion forecasting models, the code\nwill be made publicly available upon acceptance:\nhttps://github.com/valeoai/MF_aWTA."
                },
                "authors": [
                    {
                        "name": "Yihong Xu"
                    },
                    {
                        "name": "Victor Letzelter"
                    },
                    {
                        "name": "Mickaël Chen"
                    },
                    {
                        "name": "Éloi Zablocki"
                    },
                    {
                        "name": "Matthieu Cord"
                    }
                ],
                "author_detail": {
                    "name": "Matthieu Cord"
                },
                "author": "Matthieu Cord",
                "arxiv_comment": "7 pages, 6 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.11172v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.11172v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.14507v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.14507v3",
                "updated": "2024-09-18T09:25:20Z",
                "updated_parsed": [
                    2024,
                    9,
                    18,
                    9,
                    25,
                    20,
                    2,
                    262,
                    0
                ],
                "published": "2024-07-19T17:59:03Z",
                "published_parsed": [
                    2024,
                    7,
                    19,
                    17,
                    59,
                    3,
                    4,
                    201,
                    0
                ],
                "title": "Internal Consistency and Self-Feedback in Large Language Models: A\n  Survey",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Internal Consistency and Self-Feedback in Large Language Models: A\n  Survey"
                },
                "summary": "Large language models (LLMs) often exhibit deficient reasoning or generate\nhallucinations. To address these, studies prefixed with \"Self-\" such as\nSelf-Consistency, Self-Improve, and Self-Refine have been initiated. They share\na commonality: involving LLMs evaluating and updating themselves. Nonetheless,\nthese efforts lack a unified perspective on summarization, as existing surveys\npredominantly focus on categorization.\n  In this paper, we use a unified perspective of internal consistency, offering\nexplanations for reasoning deficiencies and hallucinations. Internal\nconsistency refers to the consistency in expressions among LLMs' latent,\ndecoding, or response layers based on sampling methodologies. Then, we\nintroduce an effective theoretical framework capable of mining internal\nconsistency, named Self-Feedback. This framework consists of two modules:\nSelf-Evaluation and Self-Update. The former captures internal consistency\nsignals, while the latter leverages the signals to enhance either the model's\nresponse or the model itself. This framework has been employed in numerous\nstudies.\n  We systematically classify these studies by tasks and lines of work;\nsummarize relevant evaluation methods and benchmarks; and delve into the\nconcern, \"Does Self-Feedback Really Work?\" We also propose several critical\nviewpoints, including the \"Hourglass Evolution of Internal Consistency\",\n\"Consistency Is (Almost) Correctness\" hypothesis, and \"The Paradox of Latent\nand Explicit Reasoning\". The relevant resources are open-sourced at\nhttps://github.com/IAAR-Shanghai/ICSFSurvey.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) often exhibit deficient reasoning or generate\nhallucinations. To address these, studies prefixed with \"Self-\" such as\nSelf-Consistency, Self-Improve, and Self-Refine have been initiated. They share\na commonality: involving LLMs evaluating and updating themselves. Nonetheless,\nthese efforts lack a unified perspective on summarization, as existing surveys\npredominantly focus on categorization.\n  In this paper, we use a unified perspective of internal consistency, offering\nexplanations for reasoning deficiencies and hallucinations. Internal\nconsistency refers to the consistency in expressions among LLMs' latent,\ndecoding, or response layers based on sampling methodologies. Then, we\nintroduce an effective theoretical framework capable of mining internal\nconsistency, named Self-Feedback. This framework consists of two modules:\nSelf-Evaluation and Self-Update. The former captures internal consistency\nsignals, while the latter leverages the signals to enhance either the model's\nresponse or the model itself. This framework has been employed in numerous\nstudies.\n  We systematically classify these studies by tasks and lines of work;\nsummarize relevant evaluation methods and benchmarks; and delve into the\nconcern, \"Does Self-Feedback Really Work?\" We also propose several critical\nviewpoints, including the \"Hourglass Evolution of Internal Consistency\",\n\"Consistency Is (Almost) Correctness\" hypothesis, and \"The Paradox of Latent\nand Explicit Reasoning\". The relevant resources are open-sourced at\nhttps://github.com/IAAR-Shanghai/ICSFSurvey."
                },
                "authors": [
                    {
                        "name": "Xun Liang"
                    },
                    {
                        "name": "Shichao Song"
                    },
                    {
                        "name": "Zifan Zheng"
                    },
                    {
                        "name": "Hanyu Wang"
                    },
                    {
                        "name": "Qingchen Yu"
                    },
                    {
                        "name": "Xunkai Li"
                    },
                    {
                        "name": "Rong-Hua Li"
                    },
                    {
                        "name": "Yi Wang"
                    },
                    {
                        "name": "Zhonghao Wang"
                    },
                    {
                        "name": "Feiyu Xiong"
                    },
                    {
                        "name": "Zhiyu Li"
                    }
                ],
                "author_detail": {
                    "name": "Zhiyu Li"
                },
                "author": "Zhiyu Li",
                "arxiv_comment": "20 pages, 10 figures, 6 tables, 13 equations",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.14507v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.14507v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.11817v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.11817v1",
                "updated": "2024-09-18T09:08:16Z",
                "updated_parsed": [
                    2024,
                    9,
                    18,
                    9,
                    8,
                    16,
                    2,
                    262,
                    0
                ],
                "published": "2024-09-18T09:08:16Z",
                "published_parsed": [
                    2024,
                    9,
                    18,
                    9,
                    8,
                    16,
                    2,
                    262,
                    0
                ],
                "title": "EFCM: Efficient Fine-tuning on Compressed Models for deployment of large\n  models in medical image analysis",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "EFCM: Efficient Fine-tuning on Compressed Models for deployment of large\n  models in medical image analysis"
                },
                "summary": "The recent development of deep learning large models in medicine shows\nremarkable performance in medical image analysis and diagnosis, but their large\nnumber of parameters causes memory and inference latency challenges. Knowledge\ndistillation offers a solution, but the slide-level gradients cannot be\nbackpropagated for student model updates due to high-resolution pathological\nimages and slide-level labels. This study presents an Efficient Fine-tuning on\nCompressed Models (EFCM) framework with two stages: unsupervised feature\ndistillation and fine-tuning. In the distillation stage, Feature Projection\nDistillation (FPD) is proposed with a TransScan module for adaptive receptive\nfield adjustment to enhance the knowledge absorption capability of the student\nmodel. In the slide-level fine-tuning stage, three strategies (Reuse CLAM,\nRetrain CLAM, and End2end Train CLAM (ETC)) are compared. Experiments are\nconducted on 11 downstream datasets related to three large medical models:\nRETFound for retina, MRM for chest X-ray, and BROW for histopathology. The\nexperimental results demonstrate that the EFCM framework significantly improves\naccuracy and efficiency in handling slide-level pathological image problems,\neffectively addressing the challenges of deploying large medical models.\nSpecifically, it achieves a 4.33% increase in ACC and a 5.2% increase in AUC\ncompared to the large model BROW on the TCGA-NSCLC and TCGA-BRCA datasets. The\nanalysis of model inference efficiency highlights the high efficiency of the\ndistillation fine-tuning method.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The recent development of deep learning large models in medicine shows\nremarkable performance in medical image analysis and diagnosis, but their large\nnumber of parameters causes memory and inference latency challenges. Knowledge\ndistillation offers a solution, but the slide-level gradients cannot be\nbackpropagated for student model updates due to high-resolution pathological\nimages and slide-level labels. This study presents an Efficient Fine-tuning on\nCompressed Models (EFCM) framework with two stages: unsupervised feature\ndistillation and fine-tuning. In the distillation stage, Feature Projection\nDistillation (FPD) is proposed with a TransScan module for adaptive receptive\nfield adjustment to enhance the knowledge absorption capability of the student\nmodel. In the slide-level fine-tuning stage, three strategies (Reuse CLAM,\nRetrain CLAM, and End2end Train CLAM (ETC)) are compared. Experiments are\nconducted on 11 downstream datasets related to three large medical models:\nRETFound for retina, MRM for chest X-ray, and BROW for histopathology. The\nexperimental results demonstrate that the EFCM framework significantly improves\naccuracy and efficiency in handling slide-level pathological image problems,\neffectively addressing the challenges of deploying large medical models.\nSpecifically, it achieves a 4.33% increase in ACC and a 5.2% increase in AUC\ncompared to the large model BROW on the TCGA-NSCLC and TCGA-BRCA datasets. The\nanalysis of model inference efficiency highlights the high efficiency of the\ndistillation fine-tuning method."
                },
                "authors": [
                    {
                        "name": "Shaojie Li"
                    },
                    {
                        "name": "Zhaoshuo Diao"
                    }
                ],
                "author_detail": {
                    "name": "Zhaoshuo Diao"
                },
                "author": "Zhaoshuo Diao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.11817v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.11817v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.10697v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.10697v2",
                "updated": "2024-09-18T08:57:00Z",
                "updated_parsed": [
                    2024,
                    9,
                    18,
                    8,
                    57,
                    0,
                    2,
                    262,
                    0
                ],
                "published": "2024-09-16T19:54:42Z",
                "published_parsed": [
                    2024,
                    9,
                    16,
                    19,
                    54,
                    42,
                    0,
                    260,
                    0
                ],
                "title": "LLMs as information warriors? Auditing how LLM-powered chatbots tackle\n  disinformation about Russia's war in Ukraine",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLMs as information warriors? Auditing how LLM-powered chatbots tackle\n  disinformation about Russia's war in Ukraine"
                },
                "summary": "The rise of large language models (LLMs) has a significant impact on\ninformation warfare. By facilitating the production of content related to\ndisinformation and propaganda campaigns, LLMs can amplify different types of\ninformation operations and mislead online users. In our study, we empirically\ninvestigate how LLM-powered chatbots, developed by Google, Microsoft, and\nPerplexity, handle disinformation about Russia's war in Ukraine and whether the\nchatbots' ability to provide accurate information on the topic varies across\nlanguages and over time. Our findings indicate that while for some chatbots\n(Perplexity), there is a significant improvement in performance over time in\nseveral languages, for others (Gemini), the performance improves only in\nEnglish but deteriorates in low-resource languages.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rise of large language models (LLMs) has a significant impact on\ninformation warfare. By facilitating the production of content related to\ndisinformation and propaganda campaigns, LLMs can amplify different types of\ninformation operations and mislead online users. In our study, we empirically\ninvestigate how LLM-powered chatbots, developed by Google, Microsoft, and\nPerplexity, handle disinformation about Russia's war in Ukraine and whether the\nchatbots' ability to provide accurate information on the topic varies across\nlanguages and over time. Our findings indicate that while for some chatbots\n(Perplexity), there is a significant improvement in performance over time in\nseveral languages, for others (Gemini), the performance improves only in\nEnglish but deteriorates in low-resource languages."
                },
                "authors": [
                    {
                        "name": "Mykola Makhortykh"
                    },
                    {
                        "name": "Ani Baghumyan"
                    },
                    {
                        "name": "Victoria Vziatysheva"
                    },
                    {
                        "name": "Maryna Sydorova"
                    },
                    {
                        "name": "Elizaveta Kuznetsova"
                    }
                ],
                "author_detail": {
                    "name": "Elizaveta Kuznetsova"
                },
                "author": "Elizaveta Kuznetsova",
                "arxiv_comment": "25 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.10697v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.10697v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.07976v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.07976v2",
                "updated": "2024-09-18T08:31:57Z",
                "updated_parsed": [
                    2024,
                    9,
                    18,
                    8,
                    31,
                    57,
                    2,
                    262,
                    0
                ],
                "published": "2024-09-12T12:16:00Z",
                "published_parsed": [
                    2024,
                    9,
                    12,
                    12,
                    16,
                    0,
                    3,
                    256,
                    0
                ],
                "title": "Constraints on the Primordial Black Hole Abundance through\n  Scalar-Induced Gravitational Waves from Advanced LIGO and Virgo's First Three\n  Observing Runs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Constraints on the Primordial Black Hole Abundance through\n  Scalar-Induced Gravitational Waves from Advanced LIGO and Virgo's First Three\n  Observing Runs"
                },
                "summary": "As a promising dark matter candidate, primordial black holes (PBHs) lighter\nthan $\\sim10^{-18}M_{\\odot}$ are supposed to have evaporated by today through\nHawking radiation. This scenario is challenged by the memory burden effect,\nwhich suggests that the evaporation of black holes may slow down significantly\nafter they have emitted about half of their initial mass. We explore the\nastrophysical implications of the memory burden effect on the PBH abundance by\ntoday and the possibility for PBHs lighter than $\\sim10^{-18}M_{\\odot}$ to\npersist as dark matter. Our analysis utilizes current LIGO-Virgo-KAGRA data to\nconstrain the primordial power spectrum and infer the PBH abundance. We find a\nnull detection of scalar-induced gravitational waves that accompanied the\nformation of the PBHs. Then we find that PBHs are ruled out within the mass\nrange $\\sim[10^{-24},10^{-19}]M_{\\odot}$. Furthermore, we expect that\nnext-generation gravitational wave detectors, such as the Einstein Telescope\nand the Cosmic Explorer, will provide even more stringent constraints. Our\nresults indicate that future detectors can reach sensitivities that could rule\nout PBH as dark matter within $\\sim[10^{-29}M_{\\odot},10^{-16}M_{\\odot}]$ in\nthe null detection of scalar-induced gravitational waves.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As a promising dark matter candidate, primordial black holes (PBHs) lighter\nthan $\\sim10^{-18}M_{\\odot}$ are supposed to have evaporated by today through\nHawking radiation. This scenario is challenged by the memory burden effect,\nwhich suggests that the evaporation of black holes may slow down significantly\nafter they have emitted about half of their initial mass. We explore the\nastrophysical implications of the memory burden effect on the PBH abundance by\ntoday and the possibility for PBHs lighter than $\\sim10^{-18}M_{\\odot}$ to\npersist as dark matter. Our analysis utilizes current LIGO-Virgo-KAGRA data to\nconstrain the primordial power spectrum and infer the PBH abundance. We find a\nnull detection of scalar-induced gravitational waves that accompanied the\nformation of the PBHs. Then we find that PBHs are ruled out within the mass\nrange $\\sim[10^{-24},10^{-19}]M_{\\odot}$. Furthermore, we expect that\nnext-generation gravitational wave detectors, such as the Einstein Telescope\nand the Cosmic Explorer, will provide even more stringent constraints. Our\nresults indicate that future detectors can reach sensitivities that could rule\nout PBH as dark matter within $\\sim[10^{-29}M_{\\odot},10^{-16}M_{\\odot}]$ in\nthe null detection of scalar-induced gravitational waves."
                },
                "authors": [
                    {
                        "name": "Yang Jiang"
                    },
                    {
                        "name": "Chen Yuan"
                    },
                    {
                        "name": "Chong-Zhi Li"
                    },
                    {
                        "name": "Qing-Guo Huang"
                    }
                ],
                "author_detail": {
                    "name": "Qing-Guo Huang"
                },
                "author": "Qing-Guo Huang",
                "arxiv_comment": "9 pages, 2 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.07976v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.07976v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.CO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.CO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "gr-qc",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "hep-th",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.11798v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.11798v1",
                "updated": "2024-09-18T08:30:20Z",
                "updated_parsed": [
                    2024,
                    9,
                    18,
                    8,
                    30,
                    20,
                    2,
                    262,
                    0
                ],
                "published": "2024-09-18T08:30:20Z",
                "published_parsed": [
                    2024,
                    9,
                    18,
                    8,
                    30,
                    20,
                    2,
                    262,
                    0
                ],
                "title": "The Factuality of Large Language Models in the Legal Domain",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Factuality of Large Language Models in the Legal Domain"
                },
                "summary": "This paper investigates the factuality of large language models (LLMs) as\nknowledge bases in the legal domain, in a realistic usage scenario: we allow\nfor acceptable variations in the answer, and let the model abstain from\nanswering when uncertain. First, we design a dataset of diverse factual\nquestions about case law and legislation. We then use the dataset to evaluate\nseveral LLMs under different evaluation methods, including exact, alias, and\nfuzzy matching. Our results show that the performance improves significantly\nunder the alias and fuzzy matching methods. Further, we explore the impact of\nabstaining and in-context examples, finding that both strategies enhance\nprecision. Finally, we demonstrate that additional pre-training on legal\ndocuments, as seen with SaulLM, further improves factual precision from 63% to\n81%.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper investigates the factuality of large language models (LLMs) as\nknowledge bases in the legal domain, in a realistic usage scenario: we allow\nfor acceptable variations in the answer, and let the model abstain from\nanswering when uncertain. First, we design a dataset of diverse factual\nquestions about case law and legislation. We then use the dataset to evaluate\nseveral LLMs under different evaluation methods, including exact, alias, and\nfuzzy matching. Our results show that the performance improves significantly\nunder the alias and fuzzy matching methods. Further, we explore the impact of\nabstaining and in-context examples, finding that both strategies enhance\nprecision. Finally, we demonstrate that additional pre-training on legal\ndocuments, as seen with SaulLM, further improves factual precision from 63% to\n81%."
                },
                "authors": [
                    {
                        "name": "Rajaa El Hamdani"
                    },
                    {
                        "name": "Thomas Bonald"
                    },
                    {
                        "name": "Fragkiskos Malliaros"
                    },
                    {
                        "name": "Nils Holzenberger"
                    },
                    {
                        "name": "Fabian Suchanek"
                    }
                ],
                "author_detail": {
                    "name": "Fabian Suchanek"
                },
                "author": "Fabian Suchanek",
                "arxiv_comment": "CIKM 2024, short paper",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.11798v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.11798v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2302.13965v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2302.13965v4",
                "updated": "2024-09-18T08:26:28Z",
                "updated_parsed": [
                    2024,
                    9,
                    18,
                    8,
                    26,
                    28,
                    2,
                    262,
                    0
                ],
                "published": "2023-02-27T17:06:22Z",
                "published_parsed": [
                    2023,
                    2,
                    27,
                    17,
                    6,
                    22,
                    0,
                    58,
                    0
                ],
                "title": "An Approximation Theory Framework for Measure-Transport Sampling\n  Algorithms",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "An Approximation Theory Framework for Measure-Transport Sampling\n  Algorithms"
                },
                "summary": "This article presents a general approximation-theoretic framework to analyze\nmeasure transport algorithms for probabilistic modeling. A primary motivating\napplication for such algorithms is sampling -- a central task in statistical\ninference and generative modeling. We provide a priori error estimates in the\ncontinuum limit, i.e., when the measures (or their densities) are given, but\nwhen the transport map is discretized or approximated using a\nfinite-dimensional function space. Our analysis relies on the regularity theory\nof transport maps and on classical approximation theory for high-dimensional\nfunctions. A third element of our analysis, which is of independent interest,\nis the development of new stability estimates that relate the distance between\ntwo maps to the distance~(or divergence) between the pushforward measures they\ndefine. We present a series of applications of our framework, where\nquantitative convergence rates are obtained for practical problems using\nWasserstein metrics, maximum mean discrepancy, and Kullback--Leibler\ndivergence. Specialized rates for approximations of the popular triangular\nKn{\\\"o}the-Rosenblatt maps are obtained, followed by numerical experiments that\ndemonstrate and extend our theory.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This article presents a general approximation-theoretic framework to analyze\nmeasure transport algorithms for probabilistic modeling. A primary motivating\napplication for such algorithms is sampling -- a central task in statistical\ninference and generative modeling. We provide a priori error estimates in the\ncontinuum limit, i.e., when the measures (or their densities) are given, but\nwhen the transport map is discretized or approximated using a\nfinite-dimensional function space. Our analysis relies on the regularity theory\nof transport maps and on classical approximation theory for high-dimensional\nfunctions. A third element of our analysis, which is of independent interest,\nis the development of new stability estimates that relate the distance between\ntwo maps to the distance~(or divergence) between the pushforward measures they\ndefine. We present a series of applications of our framework, where\nquantitative convergence rates are obtained for practical problems using\nWasserstein metrics, maximum mean discrepancy, and Kullback--Leibler\ndivergence. Specialized rates for approximations of the popular triangular\nKn{\\\"o}the-Rosenblatt maps are obtained, followed by numerical experiments that\ndemonstrate and extend our theory."
                },
                "authors": [
                    {
                        "name": "Ricardo Baptista"
                    },
                    {
                        "name": "Bamdad Hosseini"
                    },
                    {
                        "name": "Nikola B. Kovachki"
                    },
                    {
                        "name": "Youssef M. Marzouk"
                    },
                    {
                        "name": "Amir Sagiv"
                    }
                ],
                "author_detail": {
                    "name": "Amir Sagiv"
                },
                "author": "Amir Sagiv",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2302.13965v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2302.13965v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "math.NA",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "math.NA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.ST",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.TH",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.15540v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.15540v2",
                "updated": "2024-09-18T08:21:29Z",
                "updated_parsed": [
                    2024,
                    9,
                    18,
                    8,
                    21,
                    29,
                    2,
                    262,
                    0
                ],
                "published": "2024-06-21T17:39:57Z",
                "published_parsed": [
                    2024,
                    6,
                    21,
                    17,
                    39,
                    57,
                    4,
                    173,
                    0
                ],
                "title": "Specify What? Enhancing Neural Specification Synthesis by Symbolic\n  Methods",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Specify What? Enhancing Neural Specification Synthesis by Symbolic\n  Methods"
                },
                "summary": "We investigate how combinations of Large Language Models (LLMs) and symbolic\nanalyses can be used to synthesise specifications of C programs. The LLM\nprompts are augmented with outputs from two formal methods tools in the Frama-C\necosystem, Pathcrawler and EVA, to produce C program annotations in the\nspecification language ACSL. We demonstrate how the addition of symbolic\nanalysis to the workflow impacts the quality of annotations: information about\ninput/output examples from Pathcrawler produce more context-aware annotations,\nwhile the inclusion of EVA reports yields annotations more attuned to runtime\nerrors. In addition, we show that the method infers rather the programs intent\nthan its behaviour, by generating specifications for buggy programs and\nobserving robustness of the result against bugs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We investigate how combinations of Large Language Models (LLMs) and symbolic\nanalyses can be used to synthesise specifications of C programs. The LLM\nprompts are augmented with outputs from two formal methods tools in the Frama-C\necosystem, Pathcrawler and EVA, to produce C program annotations in the\nspecification language ACSL. We demonstrate how the addition of symbolic\nanalysis to the workflow impacts the quality of annotations: information about\ninput/output examples from Pathcrawler produce more context-aware annotations,\nwhile the inclusion of EVA reports yields annotations more attuned to runtime\nerrors. In addition, we show that the method infers rather the programs intent\nthan its behaviour, by generating specifications for buggy programs and\nobserving robustness of the result against bugs."
                },
                "authors": [
                    {
                        "name": "George Granberry"
                    },
                    {
                        "name": "Wolfgang Ahrendt"
                    },
                    {
                        "name": "Moa Johansson"
                    }
                ],
                "author_detail": {
                    "name": "Moa Johansson"
                },
                "author": "Moa Johansson",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.15540v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.15540v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.FL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.11786v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.11786v1",
                "updated": "2024-09-18T08:10:35Z",
                "updated_parsed": [
                    2024,
                    9,
                    18,
                    8,
                    10,
                    35,
                    2,
                    262,
                    0
                ],
                "published": "2024-09-18T08:10:35Z",
                "published_parsed": [
                    2024,
                    9,
                    18,
                    8,
                    10,
                    35,
                    2,
                    262,
                    0
                ],
                "title": "Efficient Low-Resolution Face Recognition via Bridge Distillation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient Low-Resolution Face Recognition via Bridge Distillation"
                },
                "summary": "Face recognition in the wild is now advancing towards light-weight models,\nfast inference speed and resolution-adapted capability. In this paper, we\npropose a bridge distillation approach to turn a complex face model pretrained\non private high-resolution faces into a light-weight one for low-resolution\nface recognition. In our approach, such a cross-dataset resolution-adapted\nknowledge transfer problem is solved via two-step distillation. In the first\nstep, we conduct cross-dataset distillation to transfer the prior knowledge\nfrom private high-resolution faces to public high-resolution faces and generate\ncompact and discriminative features. In the second step, the resolution-adapted\ndistillation is conducted to further transfer the prior knowledge to synthetic\nlow-resolution faces via multi-task learning. By learning low-resolution face\nrepresentations and mimicking the adapted high-resolution knowledge, a\nlight-weight student model can be constructed with high efficiency and\npromising accuracy in recognizing low-resolution faces. Experimental results\nshow that the student model performs impressively in recognizing low-resolution\nfaces with only 0.21M parameters and 0.057MB memory. Meanwhile, its speed\nreaches up to 14,705, ~934 and 763 faces per second on GPU, CPU and mobile\nphone, respectively.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Face recognition in the wild is now advancing towards light-weight models,\nfast inference speed and resolution-adapted capability. In this paper, we\npropose a bridge distillation approach to turn a complex face model pretrained\non private high-resolution faces into a light-weight one for low-resolution\nface recognition. In our approach, such a cross-dataset resolution-adapted\nknowledge transfer problem is solved via two-step distillation. In the first\nstep, we conduct cross-dataset distillation to transfer the prior knowledge\nfrom private high-resolution faces to public high-resolution faces and generate\ncompact and discriminative features. In the second step, the resolution-adapted\ndistillation is conducted to further transfer the prior knowledge to synthetic\nlow-resolution faces via multi-task learning. By learning low-resolution face\nrepresentations and mimicking the adapted high-resolution knowledge, a\nlight-weight student model can be constructed with high efficiency and\npromising accuracy in recognizing low-resolution faces. Experimental results\nshow that the student model performs impressively in recognizing low-resolution\nfaces with only 0.21M parameters and 0.057MB memory. Meanwhile, its speed\nreaches up to 14,705, ~934 and 763 faces per second on GPU, CPU and mobile\nphone, respectively."
                },
                "authors": [
                    {
                        "name": "Shiming Ge"
                    },
                    {
                        "name": "Shengwei Zhao"
                    },
                    {
                        "name": "Chenyu Li"
                    },
                    {
                        "name": "Yu Zhang"
                    },
                    {
                        "name": "Jia Li"
                    }
                ],
                "author_detail": {
                    "name": "Jia Li"
                },
                "author": "Jia Li",
                "arxiv_comment": "This paper is published in IEEE TIP 2020",
                "arxiv_journal_ref": "IEEE TIP 2020",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.11786v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.11786v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.11783v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.11783v2",
                "updated": "2024-09-20T04:13:33Z",
                "updated_parsed": [
                    2024,
                    9,
                    20,
                    4,
                    13,
                    33,
                    4,
                    264,
                    0
                ],
                "published": "2024-09-18T08:07:37Z",
                "published_parsed": [
                    2024,
                    9,
                    18,
                    8,
                    7,
                    37,
                    2,
                    262,
                    0
                ],
                "title": "Development and bilingual evaluation of Japanese medical large language\n  model within reasonably low computational resources",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Development and bilingual evaluation of Japanese medical large language\n  model within reasonably low computational resources"
                },
                "summary": "The recent success of large language models (LLMs) and the scaling law has\nled to a widespread adoption of larger models. Particularly in the healthcare\nindustry, there is an increasing demand for locally operated LLMs due to\nsecurity concerns. However, the majority of high quality open-source LLMs have\na size of 70B parameters, imposing significant financial burdens on users for\nGPU preparation and operation. To overcome these issues, we present a medical\nadaptation based on the recent 7B models, which enables the operation in low\ncomputational resources. We compare the performance on medical\nquestion-answering benchmarks in two languages (Japanese and English),\ndemonstrating that its scores reach parity with or surpass those of currently\nexisting medical LLMs that are ten times larger. We find that fine-tuning an\nEnglish-centric base model on Japanese medical dataset improves the score in\nboth language, supporting the effect of cross-lingual knowledge transfer. We\nhope that this study will alleviate financial challenges, serving as a stepping\nstone for clinical institutions to practically utilize LLMs locally. Our\nevaluation code is available at\nhttps://github.com/stardust-coder/japanese-lm-med-harness.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The recent success of large language models (LLMs) and the scaling law has\nled to a widespread adoption of larger models. Particularly in the healthcare\nindustry, there is an increasing demand for locally operated LLMs due to\nsecurity concerns. However, the majority of high quality open-source LLMs have\na size of 70B parameters, imposing significant financial burdens on users for\nGPU preparation and operation. To overcome these issues, we present a medical\nadaptation based on the recent 7B models, which enables the operation in low\ncomputational resources. We compare the performance on medical\nquestion-answering benchmarks in two languages (Japanese and English),\ndemonstrating that its scores reach parity with or surpass those of currently\nexisting medical LLMs that are ten times larger. We find that fine-tuning an\nEnglish-centric base model on Japanese medical dataset improves the score in\nboth language, supporting the effect of cross-lingual knowledge transfer. We\nhope that this study will alleviate financial challenges, serving as a stepping\nstone for clinical institutions to practically utilize LLMs locally. Our\nevaluation code is available at\nhttps://github.com/stardust-coder/japanese-lm-med-harness."
                },
                "authors": [
                    {
                        "name": "Issey Sukeda"
                    }
                ],
                "author_detail": {
                    "name": "Issey Sukeda"
                },
                "author": "Issey Sukeda",
                "arxiv_comment": "18 pages, 9 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.11783v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.11783v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.01272v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.01272v3",
                "updated": "2024-09-18T07:58:55Z",
                "updated_parsed": [
                    2024,
                    9,
                    18,
                    7,
                    58,
                    55,
                    2,
                    262,
                    0
                ],
                "published": "2024-07-01T13:25:33Z",
                "published_parsed": [
                    2024,
                    7,
                    1,
                    13,
                    25,
                    33,
                    0,
                    183,
                    0
                ],
                "title": "Show Less, Instruct More: Enriching Prompts with Definitions and\n  Guidelines for Zero-Shot NER",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Show Less, Instruct More: Enriching Prompts with Definitions and\n  Guidelines for Zero-Shot NER"
                },
                "summary": "Recently, several specialized instruction-tuned Large Language Models (LLMs)\nfor Named Entity Recognition (NER) have emerged. Compared to traditional NER\napproaches, these models have demonstrated strong generalization capabilities.\nExisting LLMs primarily focus on addressing zero-shot NER on Out-of-Domain\ninputs, while fine-tuning on an extensive number of entity classes that often\nhighly or completely overlap with test sets. In this work instead, we propose\nSLIMER, an approach designed to tackle never-seen-before entity tags by\ninstructing the model on fewer examples, and by leveraging a prompt enriched\nwith definition and guidelines. Experiments demonstrate that definition and\nguidelines yield better performance, faster and more robust learning,\nparticularly when labelling unseen named entities. Furthermore, SLIMER performs\ncomparably to state-of-the-art approaches in out-of-domain zero-shot NER, while\nbeing trained in a more fair, though certainly more challenging, setting.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recently, several specialized instruction-tuned Large Language Models (LLMs)\nfor Named Entity Recognition (NER) have emerged. Compared to traditional NER\napproaches, these models have demonstrated strong generalization capabilities.\nExisting LLMs primarily focus on addressing zero-shot NER on Out-of-Domain\ninputs, while fine-tuning on an extensive number of entity classes that often\nhighly or completely overlap with test sets. In this work instead, we propose\nSLIMER, an approach designed to tackle never-seen-before entity tags by\ninstructing the model on fewer examples, and by leveraging a prompt enriched\nwith definition and guidelines. Experiments demonstrate that definition and\nguidelines yield better performance, faster and more robust learning,\nparticularly when labelling unseen named entities. Furthermore, SLIMER performs\ncomparably to state-of-the-art approaches in out-of-domain zero-shot NER, while\nbeing trained in a more fair, though certainly more challenging, setting."
                },
                "authors": [
                    {
                        "name": "Andrew Zamai"
                    },
                    {
                        "name": "Andrea Zugarini"
                    },
                    {
                        "name": "Leonardo Rigutini"
                    },
                    {
                        "name": "Marco Ernandes"
                    },
                    {
                        "name": "Marco Maggini"
                    }
                ],
                "author_detail": {
                    "name": "Marco Maggini"
                },
                "author": "Marco Maggini",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.01272v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.01272v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.04464v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.04464v2",
                "updated": "2024-09-18T07:43:12Z",
                "updated_parsed": [
                    2024,
                    9,
                    18,
                    7,
                    43,
                    12,
                    2,
                    262,
                    0
                ],
                "published": "2024-09-03T07:25:01Z",
                "published_parsed": [
                    2024,
                    9,
                    3,
                    7,
                    25,
                    1,
                    1,
                    247,
                    0
                ],
                "title": "Leveraging Large Language Models for Solving Rare MIP Challenges",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Leveraging Large Language Models for Solving Rare MIP Challenges"
                },
                "summary": "Mixed Integer Programming (MIP) has been extensively applied in areas\nrequiring mathematical solvers to address complex instances within tight time\nconstraints. However, as the problem scale increases, the complexity of model\nformulation and finding feasible solutions escalates significantly. In\ncontrast, the model-building cost for end-to-end models, such as large language\nmodels (LLMs), remains largely unaffected by problem scale due to their pattern\nrecognition capabilities. While LLMs, like GPT-4, without fine-tuning, can\nhandle some traditional medium-scale MIP problems, they struggle with uncommon\nor highly specialized MIP scenarios. Fine-tuning LLMs can yield some feasible\nsolutions for medium-scale MIP instances, but these models typically fail to\nexplore diverse solutions when constrained by a low and constant temperature,\nlimiting their performance. In this paper, we propose and evaluate a\nrecursively dynamic temperature method integrated with a chain-of-thought\napproach. Our findings show that starting with a high temperature and gradually\nlowering it leads to better feasible solutions compared to other dynamic\ntemperature strategies. Additionally, by comparing results generated by the LLM\nwith those from Gurobi, we demonstrate that the LLM can produce solutions that\ncomplement traditional solvers by accelerating the pruning process and\nimproving overall efficiency.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mixed Integer Programming (MIP) has been extensively applied in areas\nrequiring mathematical solvers to address complex instances within tight time\nconstraints. However, as the problem scale increases, the complexity of model\nformulation and finding feasible solutions escalates significantly. In\ncontrast, the model-building cost for end-to-end models, such as large language\nmodels (LLMs), remains largely unaffected by problem scale due to their pattern\nrecognition capabilities. While LLMs, like GPT-4, without fine-tuning, can\nhandle some traditional medium-scale MIP problems, they struggle with uncommon\nor highly specialized MIP scenarios. Fine-tuning LLMs can yield some feasible\nsolutions for medium-scale MIP instances, but these models typically fail to\nexplore diverse solutions when constrained by a low and constant temperature,\nlimiting their performance. In this paper, we propose and evaluate a\nrecursively dynamic temperature method integrated with a chain-of-thought\napproach. Our findings show that starting with a high temperature and gradually\nlowering it leads to better feasible solutions compared to other dynamic\ntemperature strategies. Additionally, by comparing results generated by the LLM\nwith those from Gurobi, we demonstrate that the LLM can produce solutions that\ncomplement traditional solvers by accelerating the pruning process and\nimproving overall efficiency."
                },
                "authors": [
                    {
                        "name": "Teng Wang"
                    },
                    {
                        "name": "Wing-Yin Yu"
                    },
                    {
                        "name": "Ruifeng She"
                    },
                    {
                        "name": "Wenhan Yang"
                    },
                    {
                        "name": "Taijie Chen"
                    },
                    {
                        "name": "Jianping Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Jianping Zhang"
                },
                "author": "Jianping Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.04464v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.04464v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.OC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.10688v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.10688v2",
                "updated": "2024-09-18T07:27:51Z",
                "updated_parsed": [
                    2024,
                    9,
                    18,
                    7,
                    27,
                    51,
                    2,
                    262,
                    0
                ],
                "published": "2024-07-15T13:01:47Z",
                "published_parsed": [
                    2024,
                    7,
                    15,
                    13,
                    1,
                    47,
                    0,
                    197,
                    0
                ],
                "title": "Probability Passing for Graph Neural Networks: Graph Structure and\n  Representations Joint Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Probability Passing for Graph Neural Networks: Graph Structure and\n  Representations Joint Learning"
                },
                "summary": "Graph Neural Networks (GNNs) have achieved notable success in the analysis of\nnon-Euclidean data across a wide range of domains. However, their applicability\nis constrained by the dependence on the observed graph structure. To solve this\nproblem, Latent Graph Inference (LGI) is proposed to infer a task-specific\nlatent structure by computing similarity or edge probability of node features\nand then apply a GNN to produce predictions. Even so, existing approaches\nneglect the noise from node features, which affects generated graph structure\nand performance. In this work, we introduce a novel method called Probability\nPassing to refine the generated graph structure by aggregating edge\nprobabilities of neighboring nodes based on observed graph. Furthermore, we\ncontinue to utilize the LGI framework, inputting the refined graph structure\nand node features into GNNs to obtain predictions. We name the proposed scheme\nas Probability Passing-based Graph Neural Network (PPGNN). Moreover, the\nanchor-based technique is employed to reduce complexity and improve efficiency.\nExperimental results demonstrate the effectiveness of the proposed method.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Graph Neural Networks (GNNs) have achieved notable success in the analysis of\nnon-Euclidean data across a wide range of domains. However, their applicability\nis constrained by the dependence on the observed graph structure. To solve this\nproblem, Latent Graph Inference (LGI) is proposed to infer a task-specific\nlatent structure by computing similarity or edge probability of node features\nand then apply a GNN to produce predictions. Even so, existing approaches\nneglect the noise from node features, which affects generated graph structure\nand performance. In this work, we introduce a novel method called Probability\nPassing to refine the generated graph structure by aggregating edge\nprobabilities of neighboring nodes based on observed graph. Furthermore, we\ncontinue to utilize the LGI framework, inputting the refined graph structure\nand node features into GNNs to obtain predictions. We name the proposed scheme\nas Probability Passing-based Graph Neural Network (PPGNN). Moreover, the\nanchor-based technique is employed to reduce complexity and improve efficiency.\nExperimental results demonstrate the effectiveness of the proposed method."
                },
                "authors": [
                    {
                        "name": "Ziyan Wang"
                    },
                    {
                        "name": "Yaxuan He"
                    },
                    {
                        "name": "Bin Liu"
                    }
                ],
                "author_detail": {
                    "name": "Bin Liu"
                },
                "author": "Bin Liu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.10688v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.10688v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.11265v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.11265v2",
                "updated": "2024-09-18T07:26:40Z",
                "updated_parsed": [
                    2024,
                    9,
                    18,
                    7,
                    26,
                    40,
                    2,
                    262,
                    0
                ],
                "published": "2024-09-17T15:15:03Z",
                "published_parsed": [
                    2024,
                    9,
                    17,
                    15,
                    15,
                    3,
                    1,
                    261,
                    0
                ],
                "title": "Performance of Cross-Validated Targeted Maximum Likelihood Estimation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Performance of Cross-Validated Targeted Maximum Likelihood Estimation"
                },
                "summary": "Background: Advanced methods for causal inference, such as targeted maximum\nlikelihood estimation (TMLE), require certain conditions for statistical\ninference. However, in situations where there is not differentiability due to\ndata sparsity or near-positivity violations, the Donsker class condition is\nviolated. In such situations, TMLE variance can suffer from inflation of the\ntype I error and poor coverage, leading to conservative confidence intervals.\nCross-validation of the TMLE algorithm (CVTMLE) has been suggested to improve\non performance compared to TMLE in settings of positivity or Donsker class\nviolations. We aim to investigate the performance of CVTMLE compared to TMLE in\nvarious settings.\n  Methods: We utilised the data-generating mechanism as described in Leger et\nal. (2022) to run a Monte Carlo experiment under different Donsker class\nviolations. Then, we evaluated the respective statistical performances of TMLE\nand CVTMLE with different super learner libraries, with and without regression\ntree methods.\n  Results: We found that CVTMLE vastly improves confidence interval coverage\nwithout adversely affecting bias, particularly in settings with small sample\nsizes and near-positivity violations. Furthermore, incorporating regression\ntrees using standard TMLE with ensemble super learner-based initial estimates\nincreases bias and variance leading to invalid statistical inference.\n  Conclusions: It has been shown that when using CVTMLE the Donsker class\ncondition is no longer necessary to obtain valid statistical inference when\nusing regression trees and under either data sparsity or near-positivity\nviolations. We show through simulations that CVTMLE is much less sensitive to\nthe choice of the super learner library and thereby provides better estimation\nand inference in cases where the super learner library uses more flexible\ncandidates and is prone to overfitting.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Background: Advanced methods for causal inference, such as targeted maximum\nlikelihood estimation (TMLE), require certain conditions for statistical\ninference. However, in situations where there is not differentiability due to\ndata sparsity or near-positivity violations, the Donsker class condition is\nviolated. In such situations, TMLE variance can suffer from inflation of the\ntype I error and poor coverage, leading to conservative confidence intervals.\nCross-validation of the TMLE algorithm (CVTMLE) has been suggested to improve\non performance compared to TMLE in settings of positivity or Donsker class\nviolations. We aim to investigate the performance of CVTMLE compared to TMLE in\nvarious settings.\n  Methods: We utilised the data-generating mechanism as described in Leger et\nal. (2022) to run a Monte Carlo experiment under different Donsker class\nviolations. Then, we evaluated the respective statistical performances of TMLE\nand CVTMLE with different super learner libraries, with and without regression\ntree methods.\n  Results: We found that CVTMLE vastly improves confidence interval coverage\nwithout adversely affecting bias, particularly in settings with small sample\nsizes and near-positivity violations. Furthermore, incorporating regression\ntrees using standard TMLE with ensemble super learner-based initial estimates\nincreases bias and variance leading to invalid statistical inference.\n  Conclusions: It has been shown that when using CVTMLE the Donsker class\ncondition is no longer necessary to obtain valid statistical inference when\nusing regression trees and under either data sparsity or near-positivity\nviolations. We show through simulations that CVTMLE is much less sensitive to\nthe choice of the super learner library and thereby provides better estimation\nand inference in cases where the super learner library uses more flexible\ncandidates and is prone to overfitting."
                },
                "authors": [
                    {
                        "name": "Matthew J. Smith"
                    },
                    {
                        "name": "Rachael V. Phillips"
                    },
                    {
                        "name": "Camille Maringe"
                    },
                    {
                        "name": "Miguel Angel Luque-Fernandez"
                    }
                ],
                "author_detail": {
                    "name": "Miguel Angel Luque-Fernandez"
                },
                "author": "Miguel Angel Luque-Fernandez",
                "arxiv_comment": "20 pages, 3 figures, 1 table",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.11265v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.11265v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ME",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.AP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.10927v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.10927v2",
                "updated": "2024-09-18T07:23:50Z",
                "updated_parsed": [
                    2024,
                    9,
                    18,
                    7,
                    23,
                    50,
                    2,
                    262,
                    0
                ],
                "published": "2024-09-17T06:51:59Z",
                "published_parsed": [
                    2024,
                    9,
                    17,
                    6,
                    51,
                    59,
                    1,
                    261,
                    0
                ],
                "title": "Propulsion: Steering LLM with Tiny Fine-Tuning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Propulsion: Steering LLM with Tiny Fine-Tuning"
                },
                "summary": "The rapid advancements in Large Language Models (LLMs) have revolutionized\nnatural language processing (NLP) and related fields. However, fine-tuning\nthese models for specific tasks remains computationally expensive and risks\ndegrading pre-learned features. To address these challenges, we propose\nPropulsion, a novel parameter efficient fine-tuning (PEFT) method designed to\noptimize task-specific performance while drastically reducing computational\noverhead. Inspired by the concept of controlled adjustments in physical motion,\nPropulsion selectively re-scales specific dimensions of a pre-trained model,\nguiding output predictions toward task objectives without modifying the model's\nparameters. By introducing lightweight, trainable Propulsion parameters at the\npre-trained layer, we minimize the number of parameters updated during\nfine-tuning, preventing overfitting or overwriting of existing knowledge. Our\ntheoretical analysis, supported by Neural Tangent Kernel (NTK) theory, shows\nthat Propulsion approximates the performance of full fine-tuning with far fewer\ntrainable parameters. Empirically, Propulsion reduces the parameter count from\n355.3 million to just 0.086 million, achieving over a 10x reduction compared to\nstandard approaches like LoRA while maintaining competitive performance across\nbenchmarks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rapid advancements in Large Language Models (LLMs) have revolutionized\nnatural language processing (NLP) and related fields. However, fine-tuning\nthese models for specific tasks remains computationally expensive and risks\ndegrading pre-learned features. To address these challenges, we propose\nPropulsion, a novel parameter efficient fine-tuning (PEFT) method designed to\noptimize task-specific performance while drastically reducing computational\noverhead. Inspired by the concept of controlled adjustments in physical motion,\nPropulsion selectively re-scales specific dimensions of a pre-trained model,\nguiding output predictions toward task objectives without modifying the model's\nparameters. By introducing lightweight, trainable Propulsion parameters at the\npre-trained layer, we minimize the number of parameters updated during\nfine-tuning, preventing overfitting or overwriting of existing knowledge. Our\ntheoretical analysis, supported by Neural Tangent Kernel (NTK) theory, shows\nthat Propulsion approximates the performance of full fine-tuning with far fewer\ntrainable parameters. Empirically, Propulsion reduces the parameter count from\n355.3 million to just 0.086 million, achieving over a 10x reduction compared to\nstandard approaches like LoRA while maintaining competitive performance across\nbenchmarks."
                },
                "authors": [
                    {
                        "name": "Md Kowsher"
                    },
                    {
                        "name": "Nusrat Jahan Prottasha"
                    },
                    {
                        "name": "Prakash Bhat"
                    }
                ],
                "author_detail": {
                    "name": "Prakash Bhat"
                },
                "author": "Prakash Bhat",
                "arxiv_comment": "26 pages, 11 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.10927v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.10927v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.11041v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.11041v2",
                "updated": "2024-09-18T07:17:17Z",
                "updated_parsed": [
                    2024,
                    9,
                    18,
                    7,
                    17,
                    17,
                    2,
                    262,
                    0
                ],
                "published": "2024-09-17T10:04:50Z",
                "published_parsed": [
                    2024,
                    9,
                    17,
                    10,
                    4,
                    50,
                    1,
                    261,
                    0
                ],
                "title": "Towards No-Code Programming of Cobots: Experiments with Code Synthesis\n  by Large Code Models for Conversational Programming",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Towards No-Code Programming of Cobots: Experiments with Code Synthesis\n  by Large Code Models for Conversational Programming"
                },
                "summary": "While there has been a lot of research recently on robots in household\nenvironments, at the present time, most robots in existence can be found on\nshop floors, and most interactions between humans and robots happen there.\n``Collaborative robots'' (cobots) designed to work alongside humans on assembly\nlines traditionally require expert programming, limiting ability to make\nchanges, or manual guidance, limiting expressivity of the resulting programs.\nTo address these limitations, we explore using Large Language Models (LLMs),\nand in particular, their abilities of doing in-context learning, for\nconversational code generation. As a first step, we define RATS, the\n``Repetitive Assembly Task'', a 2D building task designed to lay the foundation\nfor simulating industry assembly scenarios. In this task, a `programmer'\ninstructs a cobot, using natural language, on how a certain assembly is to be\nbuilt; that is, the programmer induces a program, through natural language. We\ncreate a dataset that pairs target structures with various example instructions\n(human-authored, template-based, and model-generated) and example code. With\nthis, we systematically evaluate the capabilities of state-of-the-art LLMs for\nsynthesising this kind of code, given in-context examples. Evaluating in a\nsimulated environment, we find that LLMs are capable of generating accurate\n`first order code' (instruction sequences), but have problems producing\n`higher-order code' (abstractions such as functions, or use of loops).",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While there has been a lot of research recently on robots in household\nenvironments, at the present time, most robots in existence can be found on\nshop floors, and most interactions between humans and robots happen there.\n``Collaborative robots'' (cobots) designed to work alongside humans on assembly\nlines traditionally require expert programming, limiting ability to make\nchanges, or manual guidance, limiting expressivity of the resulting programs.\nTo address these limitations, we explore using Large Language Models (LLMs),\nand in particular, their abilities of doing in-context learning, for\nconversational code generation. As a first step, we define RATS, the\n``Repetitive Assembly Task'', a 2D building task designed to lay the foundation\nfor simulating industry assembly scenarios. In this task, a `programmer'\ninstructs a cobot, using natural language, on how a certain assembly is to be\nbuilt; that is, the programmer induces a program, through natural language. We\ncreate a dataset that pairs target structures with various example instructions\n(human-authored, template-based, and model-generated) and example code. With\nthis, we systematically evaluate the capabilities of state-of-the-art LLMs for\nsynthesising this kind of code, given in-context examples. Evaluating in a\nsimulated environment, we find that LLMs are capable of generating accurate\n`first order code' (instruction sequences), but have problems producing\n`higher-order code' (abstractions such as functions, or use of loops)."
                },
                "authors": [
                    {
                        "name": "Chalamalasetti Kranti"
                    },
                    {
                        "name": "Sherzod Hakimov"
                    },
                    {
                        "name": "David Schlangen"
                    }
                ],
                "author_detail": {
                    "name": "David Schlangen"
                },
                "author": "David Schlangen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.11041v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.11041v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.06787v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.06787v2",
                "updated": "2024-09-18T07:12:28Z",
                "updated_parsed": [
                    2024,
                    9,
                    18,
                    7,
                    12,
                    28,
                    2,
                    262,
                    0
                ],
                "published": "2024-08-13T10:15:55Z",
                "published_parsed": [
                    2024,
                    8,
                    13,
                    10,
                    15,
                    55,
                    1,
                    226,
                    0
                ],
                "title": "Unlock the Power of Frozen LLMs in Knowledge Graph Completion",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Unlock the Power of Frozen LLMs in Knowledge Graph Completion"
                },
                "summary": "Traditional knowledge graph completion (KGC) methods rely solely on\nstructural information, struggling with the inherent sparsity of knowledge\ngraphs (KGs). Large Language Models (LLMs) learn extensive knowledge from large\ncorpora with powerful context modeling, making them promising for mitigating\nthe limitations of previous methods. Directly fine-tuning LLMs offers great\ncapability but comes at the cost of huge time and memory consumption, while\nutilizing frozen LLMs yields suboptimal results.In this work, we aim to\nleverage LLMs for KGC effectively and efficiently. We capture the context-aware\nhidden states of knowledge triples by employing prompts to stimulate the\nintermediate layers of LLMs. We then train a data-efficient classifier on these\nhidden states to harness the inherent capabilities of frozen LLMs in KGC.\nAdditionally, to reduce ambiguity and enrich knowledge representation, we\ngenerate detailed entity descriptions through subgraph sampling on KGs.\nExtensive experiments on standard benchmarks demonstrate the efficiency and\neffectiveness of our approach. We outperform traditional KGC methods across\nmost datasets and, notably, achieve classification performance comparable to\nfine-tuned LLMs while enhancing GPU memory efficiency by $188\\times$ and\naccelerating training and inference by $13.48\\times$.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Traditional knowledge graph completion (KGC) methods rely solely on\nstructural information, struggling with the inherent sparsity of knowledge\ngraphs (KGs). Large Language Models (LLMs) learn extensive knowledge from large\ncorpora with powerful context modeling, making them promising for mitigating\nthe limitations of previous methods. Directly fine-tuning LLMs offers great\ncapability but comes at the cost of huge time and memory consumption, while\nutilizing frozen LLMs yields suboptimal results.In this work, we aim to\nleverage LLMs for KGC effectively and efficiently. We capture the context-aware\nhidden states of knowledge triples by employing prompts to stimulate the\nintermediate layers of LLMs. We then train a data-efficient classifier on these\nhidden states to harness the inherent capabilities of frozen LLMs in KGC.\nAdditionally, to reduce ambiguity and enrich knowledge representation, we\ngenerate detailed entity descriptions through subgraph sampling on KGs.\nExtensive experiments on standard benchmarks demonstrate the efficiency and\neffectiveness of our approach. We outperform traditional KGC methods across\nmost datasets and, notably, achieve classification performance comparable to\nfine-tuned LLMs while enhancing GPU memory efficiency by $188\\times$ and\naccelerating training and inference by $13.48\\times$."
                },
                "authors": [
                    {
                        "name": "Bo Xue"
                    },
                    {
                        "name": "Yi Xu"
                    },
                    {
                        "name": "Yunchong Song"
                    },
                    {
                        "name": "Yiming Pang"
                    },
                    {
                        "name": "Yuyang Ren"
                    },
                    {
                        "name": "Jiaxin Ding"
                    },
                    {
                        "name": "Luoyi Fu"
                    },
                    {
                        "name": "Xinbing Wang"
                    }
                ],
                "author_detail": {
                    "name": "Xinbing Wang"
                },
                "author": "Xinbing Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.06787v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.06787v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.07970v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.07970v3",
                "updated": "2024-09-18T07:06:02Z",
                "updated_parsed": [
                    2024,
                    9,
                    18,
                    7,
                    6,
                    2,
                    2,
                    262,
                    0
                ],
                "published": "2024-06-12T07:49:36Z",
                "published_parsed": [
                    2024,
                    6,
                    12,
                    7,
                    49,
                    36,
                    2,
                    164,
                    0
                ],
                "title": "Guiding In-Context Learning of LLMs through Quality Estimation for\n  Machine Translation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Guiding In-Context Learning of LLMs through Quality Estimation for\n  Machine Translation"
                },
                "summary": "The quality of output from large language models (LLMs), particularly in\nmachine translation (MT), is closely tied to the quality of in-context examples\n(ICEs) provided along with the query, i.e., the text to translate. The\neffectiveness of these ICEs is influenced by various factors, such as the\ndomain of the source text, the order in which the ICEs are presented, the\nnumber of these examples, and the prompt templates used. Naturally, selecting\nthe most impactful ICEs depends on understanding how these affect the resulting\ntranslation quality, which ultimately relies on translation references or human\njudgment. This paper presents a novel methodology for in-context learning (ICL)\nthat relies on a search algorithm guided by domain-specific quality estimation\n(QE). Leveraging the XGLM model, our methodology estimates the resulting\ntranslation quality without the need for translation references, selecting\neffective ICEs for MT to maximize translation quality. Our results demonstrate\nsignificant improvements over existing ICL methods and higher translation\nperformance compared to fine-tuning a pre-trained language model (PLM),\nspecifically mBART-50.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The quality of output from large language models (LLMs), particularly in\nmachine translation (MT), is closely tied to the quality of in-context examples\n(ICEs) provided along with the query, i.e., the text to translate. The\neffectiveness of these ICEs is influenced by various factors, such as the\ndomain of the source text, the order in which the ICEs are presented, the\nnumber of these examples, and the prompt templates used. Naturally, selecting\nthe most impactful ICEs depends on understanding how these affect the resulting\ntranslation quality, which ultimately relies on translation references or human\njudgment. This paper presents a novel methodology for in-context learning (ICL)\nthat relies on a search algorithm guided by domain-specific quality estimation\n(QE). Leveraging the XGLM model, our methodology estimates the resulting\ntranslation quality without the need for translation references, selecting\neffective ICEs for MT to maximize translation quality. Our results demonstrate\nsignificant improvements over existing ICL methods and higher translation\nperformance compared to fine-tuning a pre-trained language model (PLM),\nspecifically mBART-50."
                },
                "authors": [
                    {
                        "name": "Javad Pourmostafa Roshan Sharami"
                    },
                    {
                        "name": "Dimitar Shterionov"
                    },
                    {
                        "name": "Pieter Spronck"
                    }
                ],
                "author_detail": {
                    "name": "Pieter Spronck"
                },
                "author": "Pieter Spronck",
                "arxiv_comment": "Camera-ready version of the paper for the Association for Machine\n  Translation in the Americas (AMTA), including the link to the paper's\n  repository",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.07970v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.07970v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.11745v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.11745v1",
                "updated": "2024-09-18T06:57:33Z",
                "updated_parsed": [
                    2024,
                    9,
                    18,
                    6,
                    57,
                    33,
                    2,
                    262,
                    0
                ],
                "published": "2024-09-18T06:57:33Z",
                "published_parsed": [
                    2024,
                    9,
                    18,
                    6,
                    57,
                    33,
                    2,
                    262,
                    0
                ],
                "title": "Model-Embedded Gaussian Process Regression for Parameter Estimation in\n  Dynamical System",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Model-Embedded Gaussian Process Regression for Parameter Estimation in\n  Dynamical System"
                },
                "summary": "Identifying dynamical system (DS) is a vital task in science and engineering.\nTraditional methods require numerous calls to the DS solver, rendering\nlikelihood-based or least-squares inference frameworks impractical. For\nefficient parameter inference, two state-of-the-art techniques are the kernel\nmethod for modeling and the \"one-step framework\" for jointly inferring unknown\nparameters and hyperparameters. The kernel method is a quick and\nstraightforward technique, but it cannot estimate solutions and their\nderivatives, which must strictly adhere to physical laws. We propose a\nmodel-embedded \"one-step\" Bayesian framework for joint inference of unknown\nparameters and hyperparameters by maximizing the marginal likelihood. This\napproach models the solution and its derivatives using Gaussian process\nregression (GPR), taking into account smoothness and continuity properties, and\ntreats differential equations as constraints that can be naturally integrated\ninto the Bayesian framework in the linear case. Additionally, we prove the\nconvergence of the model-embedded Gaussian process regression (ME-GPR) for\ntheoretical development. Motivated by Taylor expansion, we introduce a\npiecewise first-order linearization strategy to handle nonlinear dynamic\nsystems. We derive estimates and confidence intervals, demonstrating that they\nexhibit low bias and good coverage properties for both simulated models and\nreal data.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Identifying dynamical system (DS) is a vital task in science and engineering.\nTraditional methods require numerous calls to the DS solver, rendering\nlikelihood-based or least-squares inference frameworks impractical. For\nefficient parameter inference, two state-of-the-art techniques are the kernel\nmethod for modeling and the \"one-step framework\" for jointly inferring unknown\nparameters and hyperparameters. The kernel method is a quick and\nstraightforward technique, but it cannot estimate solutions and their\nderivatives, which must strictly adhere to physical laws. We propose a\nmodel-embedded \"one-step\" Bayesian framework for joint inference of unknown\nparameters and hyperparameters by maximizing the marginal likelihood. This\napproach models the solution and its derivatives using Gaussian process\nregression (GPR), taking into account smoothness and continuity properties, and\ntreats differential equations as constraints that can be naturally integrated\ninto the Bayesian framework in the linear case. Additionally, we prove the\nconvergence of the model-embedded Gaussian process regression (ME-GPR) for\ntheoretical development. Motivated by Taylor expansion, we introduce a\npiecewise first-order linearization strategy to handle nonlinear dynamic\nsystems. We derive estimates and confidence intervals, demonstrating that they\nexhibit low bias and good coverage properties for both simulated models and\nreal data."
                },
                "authors": [
                    {
                        "name": "Ying Zhou"
                    },
                    {
                        "name": "Jinglai Li"
                    },
                    {
                        "name": "Xiang Zhou"
                    },
                    {
                        "name": "Hongqiao Wang"
                    }
                ],
                "author_detail": {
                    "name": "Hongqiao Wang"
                },
                "author": "Hongqiao Wang",
                "arxiv_comment": "24 pages, 3 figures, 5 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.11745v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.11745v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.CO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.CO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.DS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "62F15",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.11451v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.11451v3",
                "updated": "2024-09-18T06:53:40Z",
                "updated_parsed": [
                    2024,
                    9,
                    18,
                    6,
                    53,
                    40,
                    2,
                    262,
                    0
                ],
                "published": "2024-06-17T12:03:32Z",
                "published_parsed": [
                    2024,
                    6,
                    17,
                    12,
                    3,
                    32,
                    0,
                    169,
                    0
                ],
                "title": "CoMT: Chain-of-Medical-Thought Reduces Hallucination in Medical Report\n  Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CoMT: Chain-of-Medical-Thought Reduces Hallucination in Medical Report\n  Generation"
                },
                "summary": "Automatic medical report generation (MRG), which possesses significant\nresearch value as it can aid radiologists in clinical diagnosis and report\ncomposition, has garnered increasing attention. Despite recent progress,\ngenerating accurate reports remains arduous due to the requirement for precise\nclinical comprehension and disease diagnosis inference. Furthermore, owing to\nthe limited accessibility of medical data and the imbalanced distribution of\ndiseases, the underrepresentation of rare diseases in training data makes\nlarge-scale medical visual language models (LVLMs) prone to hallucinations,\nsuch as omissions or fabrications, severely undermining diagnostic performance\nand further intensifying the challenges for MRG in practice. In this study, to\neffectively mitigate hallucinations in medical report generation, we propose a\nchain-of-medical-thought approach (CoMT), which intends to imitate the\ncognitive process of human doctors by decomposing diagnostic procedures. The\nradiological features with different importance are structured into\nfine-grained medical thought chains to enhance the inferential ability during\ndiagnosis, thereby alleviating hallucination problems and enhancing the\ndiagnostic accuracy of MRG. All resources of this work will be released soon.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Automatic medical report generation (MRG), which possesses significant\nresearch value as it can aid radiologists in clinical diagnosis and report\ncomposition, has garnered increasing attention. Despite recent progress,\ngenerating accurate reports remains arduous due to the requirement for precise\nclinical comprehension and disease diagnosis inference. Furthermore, owing to\nthe limited accessibility of medical data and the imbalanced distribution of\ndiseases, the underrepresentation of rare diseases in training data makes\nlarge-scale medical visual language models (LVLMs) prone to hallucinations,\nsuch as omissions or fabrications, severely undermining diagnostic performance\nand further intensifying the challenges for MRG in practice. In this study, to\neffectively mitigate hallucinations in medical report generation, we propose a\nchain-of-medical-thought approach (CoMT), which intends to imitate the\ncognitive process of human doctors by decomposing diagnostic procedures. The\nradiological features with different importance are structured into\nfine-grained medical thought chains to enhance the inferential ability during\ndiagnosis, thereby alleviating hallucination problems and enhancing the\ndiagnostic accuracy of MRG. All resources of this work will be released soon."
                },
                "authors": [
                    {
                        "name": "Yue Jiang"
                    },
                    {
                        "name": "Jiawei Chen"
                    },
                    {
                        "name": "Dingkang Yang"
                    },
                    {
                        "name": "Mingcheng Li"
                    },
                    {
                        "name": "Shunli Wang"
                    },
                    {
                        "name": "Tong Wu"
                    },
                    {
                        "name": "Ke Li"
                    },
                    {
                        "name": "Lihua Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Lihua Zhang"
                },
                "author": "Lihua Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.11451v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.11451v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.11733v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.11733v2",
                "updated": "2024-09-19T02:33:54Z",
                "updated_parsed": [
                    2024,
                    9,
                    19,
                    2,
                    33,
                    54,
                    3,
                    263,
                    0
                ],
                "published": "2024-09-18T06:42:13Z",
                "published_parsed": [
                    2024,
                    9,
                    18,
                    6,
                    42,
                    13,
                    2,
                    262,
                    0
                ],
                "title": "Human-like Affective Cognition in Foundation Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Human-like Affective Cognition in Foundation Models"
                },
                "summary": "Understanding emotions is fundamental to human interaction and experience.\nHumans easily infer emotions from situations or facial expressions, situations\nfrom emotions, and do a variety of other affective cognition. How adept is\nmodern AI at these inferences? We introduce an evaluation framework for testing\naffective cognition in foundation models. Starting from psychological theory,\nwe generate 1,280 diverse scenarios exploring relationships between appraisals,\nemotions, expressions, and outcomes. We evaluate the abilities of foundation\nmodels (GPT-4, Claude-3, Gemini-1.5-Pro) and humans (N = 567) across carefully\nselected conditions. Our results show foundation models tend to agree with\nhuman intuitions, matching or exceeding interparticipant agreement. In some\nconditions, models are ``superhuman'' -- they better predict modal human\njudgements than the average human. All models benefit from chain-of-thought\nreasoning. This suggests foundation models have acquired a human-like\nunderstanding of emotions and their influence on beliefs and behavior.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Understanding emotions is fundamental to human interaction and experience.\nHumans easily infer emotions from situations or facial expressions, situations\nfrom emotions, and do a variety of other affective cognition. How adept is\nmodern AI at these inferences? We introduce an evaluation framework for testing\naffective cognition in foundation models. Starting from psychological theory,\nwe generate 1,280 diverse scenarios exploring relationships between appraisals,\nemotions, expressions, and outcomes. We evaluate the abilities of foundation\nmodels (GPT-4, Claude-3, Gemini-1.5-Pro) and humans (N = 567) across carefully\nselected conditions. Our results show foundation models tend to agree with\nhuman intuitions, matching or exceeding interparticipant agreement. In some\nconditions, models are ``superhuman'' -- they better predict modal human\njudgements than the average human. All models benefit from chain-of-thought\nreasoning. This suggests foundation models have acquired a human-like\nunderstanding of emotions and their influence on beliefs and behavior."
                },
                "authors": [
                    {
                        "name": "Kanishk Gandhi"
                    },
                    {
                        "name": "Zoe Lynch"
                    },
                    {
                        "name": "Jan-Philipp Fränken"
                    },
                    {
                        "name": "Kayla Patterson"
                    },
                    {
                        "name": "Sharon Wambu"
                    },
                    {
                        "name": "Tobias Gerstenberg"
                    },
                    {
                        "name": "Desmond C. Ong"
                    },
                    {
                        "name": "Noah D. Goodman"
                    }
                ],
                "author_detail": {
                    "name": "Noah D. Goodman"
                },
                "author": "Noah D. Goodman",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.11733v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.11733v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.02517v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.02517v3",
                "updated": "2024-09-18T06:33:32Z",
                "updated_parsed": [
                    2024,
                    9,
                    18,
                    6,
                    33,
                    32,
                    2,
                    262,
                    0
                ],
                "published": "2024-04-03T07:10:18Z",
                "published_parsed": [
                    2024,
                    4,
                    3,
                    7,
                    10,
                    18,
                    2,
                    94,
                    0
                ],
                "title": "HENet: Hybrid Encoding for End-to-end Multi-task 3D Perception from\n  Multi-view Cameras",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "HENet: Hybrid Encoding for End-to-end Multi-task 3D Perception from\n  Multi-view Cameras"
                },
                "summary": "Three-dimensional perception from multi-view cameras is a crucial component\nin autonomous driving systems, which involves multiple tasks like 3D object\ndetection and bird's-eye-view (BEV) semantic segmentation. To improve\nperception precision, large image encoders, high-resolution images, and\nlong-term temporal inputs have been adopted in recent 3D perception models,\nbringing remarkable performance gains. However, these techniques are often\nincompatible in training and inference scenarios due to computational resource\nconstraints. Besides, modern autonomous driving systems prefer to adopt an\nend-to-end framework for multi-task 3D perception, which can simplify the\noverall system architecture and reduce the implementation complexity. However,\nconflict between tasks often arises when optimizing multiple tasks jointly\nwithin an end-to-end 3D perception model. To alleviate these issues, we present\nan end-to-end framework named HENet for multi-task 3D perception in this paper.\nSpecifically, we propose a hybrid image encoding network, using a large image\nencoder for short-term frames and a small image encoder for long-term temporal\nframes. Then, we introduce a temporal feature integration module based on the\nattention mechanism to fuse the features of different frames extracted by the\ntwo aforementioned hybrid image encoders. Finally, according to the\ncharacteristics of each perception task, we utilize BEV features of different\ngrid sizes, independent BEV encoders, and task decoders for different tasks.\nExperimental results show that HENet achieves state-of-the-art end-to-end\nmulti-task 3D perception results on the nuScenes benchmark, including 3D object\ndetection and BEV semantic segmentation. The source code and models will be\nreleased at https://github.com/VDIGPKU/HENet.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Three-dimensional perception from multi-view cameras is a crucial component\nin autonomous driving systems, which involves multiple tasks like 3D object\ndetection and bird's-eye-view (BEV) semantic segmentation. To improve\nperception precision, large image encoders, high-resolution images, and\nlong-term temporal inputs have been adopted in recent 3D perception models,\nbringing remarkable performance gains. However, these techniques are often\nincompatible in training and inference scenarios due to computational resource\nconstraints. Besides, modern autonomous driving systems prefer to adopt an\nend-to-end framework for multi-task 3D perception, which can simplify the\noverall system architecture and reduce the implementation complexity. However,\nconflict between tasks often arises when optimizing multiple tasks jointly\nwithin an end-to-end 3D perception model. To alleviate these issues, we present\nan end-to-end framework named HENet for multi-task 3D perception in this paper.\nSpecifically, we propose a hybrid image encoding network, using a large image\nencoder for short-term frames and a small image encoder for long-term temporal\nframes. Then, we introduce a temporal feature integration module based on the\nattention mechanism to fuse the features of different frames extracted by the\ntwo aforementioned hybrid image encoders. Finally, according to the\ncharacteristics of each perception task, we utilize BEV features of different\ngrid sizes, independent BEV encoders, and task decoders for different tasks.\nExperimental results show that HENet achieves state-of-the-art end-to-end\nmulti-task 3D perception results on the nuScenes benchmark, including 3D object\ndetection and BEV semantic segmentation. The source code and models will be\nreleased at https://github.com/VDIGPKU/HENet."
                },
                "authors": [
                    {
                        "name": "Zhongyu Xia"
                    },
                    {
                        "name": "ZhiWei Lin"
                    },
                    {
                        "name": "Xinhao Wang"
                    },
                    {
                        "name": "Yongtao Wang"
                    },
                    {
                        "name": "Yun Xing"
                    },
                    {
                        "name": "Shengxiang Qi"
                    },
                    {
                        "name": "Nan Dong"
                    },
                    {
                        "name": "Ming-Hsuan Yang"
                    }
                ],
                "author_detail": {
                    "name": "Ming-Hsuan Yang"
                },
                "author": "Ming-Hsuan Yang",
                "arxiv_comment": "ECCV 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.02517v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.02517v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.11727v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.11727v1",
                "updated": "2024-09-18T06:27:26Z",
                "updated_parsed": [
                    2024,
                    9,
                    18,
                    6,
                    27,
                    26,
                    2,
                    262,
                    0
                ],
                "published": "2024-09-18T06:27:26Z",
                "published_parsed": [
                    2024,
                    9,
                    18,
                    6,
                    27,
                    26,
                    2,
                    262,
                    0
                ],
                "title": "Enabling Real-Time Conversations with Minimal Training Costs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Enabling Real-Time Conversations with Minimal Training Costs"
                },
                "summary": "Large language models (LLMs) have demonstrated the ability to improve human\nefficiency through conversational interactions. Conventional LLM-powered\ndialogue systems, operating on a turn-based paradigm, preclude real-time\ninteraction during response generation. To address this limitation, researchers\nhave proposed duplex models. These models can dynamically adapt to user input,\nfacilitating real-time interactive feedback. However, these methods typically\nrequire substantial computational resources to acquire the ability. To reduce\noverhead, this paper presents a new duplex decoding approach that enhances LLMs\nwith duplex ability, requiring minimal additional training. Specifically, our\nmethod employs parallel decoding of queries and responses in conversations,\neffectively implementing a channel-division-multiplexing decoding strategy.\nExperimental results indicate that our proposed method significantly enhances\nthe naturalness and human-likeness of user-AI interactions with minimal\ntraining costs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have demonstrated the ability to improve human\nefficiency through conversational interactions. Conventional LLM-powered\ndialogue systems, operating on a turn-based paradigm, preclude real-time\ninteraction during response generation. To address this limitation, researchers\nhave proposed duplex models. These models can dynamically adapt to user input,\nfacilitating real-time interactive feedback. However, these methods typically\nrequire substantial computational resources to acquire the ability. To reduce\noverhead, this paper presents a new duplex decoding approach that enhances LLMs\nwith duplex ability, requiring minimal additional training. Specifically, our\nmethod employs parallel decoding of queries and responses in conversations,\neffectively implementing a channel-division-multiplexing decoding strategy.\nExperimental results indicate that our proposed method significantly enhances\nthe naturalness and human-likeness of user-AI interactions with minimal\ntraining costs."
                },
                "authors": [
                    {
                        "name": "Wang Xu"
                    },
                    {
                        "name": "Shuo Wang"
                    },
                    {
                        "name": "Weilin Zhao"
                    },
                    {
                        "name": "Xu Han"
                    },
                    {
                        "name": "Yukun Yan"
                    },
                    {
                        "name": "Yudi Zhang"
                    },
                    {
                        "name": "Zhe Tao"
                    },
                    {
                        "name": "Zhiyuan Liu"
                    },
                    {
                        "name": "Wanxiang Che"
                    }
                ],
                "author_detail": {
                    "name": "Wanxiang Che"
                },
                "author": "Wanxiang Che",
                "arxiv_comment": "7pages, 6 figures, 1 table",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.11727v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.11727v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.11726v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.11726v1",
                "updated": "2024-09-18T06:21:44Z",
                "updated_parsed": [
                    2024,
                    9,
                    18,
                    6,
                    21,
                    44,
                    2,
                    262,
                    0
                ],
                "published": "2024-09-18T06:21:44Z",
                "published_parsed": [
                    2024,
                    9,
                    18,
                    6,
                    21,
                    44,
                    2,
                    262,
                    0
                ],
                "title": "Revealing the Challenge of Detecting Character Knowledge Errors in LLM\n  Role-Playing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Revealing the Challenge of Detecting Character Knowledge Errors in LLM\n  Role-Playing"
                },
                "summary": "Large language model (LLM) role-playing has gained widespread attention,\nwhere the authentic character knowledge is crucial for constructing realistic\nLLM role-playing agents. However, existing works usually overlook the\nexploration of LLMs' ability to detect characters' known knowledge errors (KKE)\nand unknown knowledge errors (UKE) while playing roles, which would lead to\nlow-quality automatic construction of character trainable corpus. In this\npaper, we propose a probing dataset to evaluate LLMs' ability to detect errors\nin KKE and UKE. The results indicate that even the latest LLMs struggle to\neffectively detect these two types of errors, especially when it comes to\nfamiliar knowledge. We experimented with various reasoning strategies and\npropose an agent-based reasoning method, Self-Recollection and Self-Doubt\n(S2RD), to further explore the potential for improving error detection\ncapabilities. Experiments show that our method effectively improves the LLMs'\nability to detect error character knowledge, but it remains an issue that\nrequires ongoing attention.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language model (LLM) role-playing has gained widespread attention,\nwhere the authentic character knowledge is crucial for constructing realistic\nLLM role-playing agents. However, existing works usually overlook the\nexploration of LLMs' ability to detect characters' known knowledge errors (KKE)\nand unknown knowledge errors (UKE) while playing roles, which would lead to\nlow-quality automatic construction of character trainable corpus. In this\npaper, we propose a probing dataset to evaluate LLMs' ability to detect errors\nin KKE and UKE. The results indicate that even the latest LLMs struggle to\neffectively detect these two types of errors, especially when it comes to\nfamiliar knowledge. We experimented with various reasoning strategies and\npropose an agent-based reasoning method, Self-Recollection and Self-Doubt\n(S2RD), to further explore the potential for improving error detection\ncapabilities. Experiments show that our method effectively improves the LLMs'\nability to detect error character knowledge, but it remains an issue that\nrequires ongoing attention."
                },
                "authors": [
                    {
                        "name": "Wenyuan Zhang"
                    },
                    {
                        "name": "Jiawei Sheng"
                    },
                    {
                        "name": "Shuaiyi Nie"
                    },
                    {
                        "name": "Zefeng Zhang"
                    },
                    {
                        "name": "Xinghua Zhang"
                    },
                    {
                        "name": "Yongquan He"
                    },
                    {
                        "name": "Tingwen Liu"
                    }
                ],
                "author_detail": {
                    "name": "Tingwen Liu"
                },
                "author": "Tingwen Liu",
                "arxiv_comment": "22 pages, 14 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.11726v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.11726v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.11724v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.11724v1",
                "updated": "2024-09-18T06:19:59Z",
                "updated_parsed": [
                    2024,
                    9,
                    18,
                    6,
                    19,
                    59,
                    2,
                    262,
                    0
                ],
                "published": "2024-09-18T06:19:59Z",
                "published_parsed": [
                    2024,
                    9,
                    18,
                    6,
                    19,
                    59,
                    2,
                    262,
                    0
                ],
                "title": "TART: An Open-Source Tool-Augmented Framework for Explainable\n  Table-based Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TART: An Open-Source Tool-Augmented Framework for Explainable\n  Table-based Reasoning"
                },
                "summary": "Current Large Language Models (LLMs) exhibit limited ability to understand\ntable structures and to apply precise numerical reasoning, which is crucial for\ntasks such as table question answering (TQA) and table-based fact verification\n(TFV). To address these challenges, we introduce our Tool-Augmented Reasoning\nframework for Tables (TART), which integrates LLMs with specialized tools. TART\ncontains three key components: a table formatter to ensure accurate data\nrepresentation, a tool maker to develop specific computational tools, and an\nexplanation generator to maintain explainability. We also present the TOOLTAB\ndataset, a new benchmark designed specifically for training LLMs in table-tool\nintegration. Our experiments indicate that TART achieves substantial\nimprovements over existing methods (e.g., Chain-of-Thought) by improving both\nthe precision of data processing and the clarity of the reasoning process.\nNotably, TART paired with CodeLlama achieves 90.0% of the accuracy of the\nclosed-sourced LLM GPT-3.5-turbo, highlighting its robustness in diverse\nreal-world scenarios. All the code and data are available at\nhttps://github.com/XinyuanLu00/TART.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Current Large Language Models (LLMs) exhibit limited ability to understand\ntable structures and to apply precise numerical reasoning, which is crucial for\ntasks such as table question answering (TQA) and table-based fact verification\n(TFV). To address these challenges, we introduce our Tool-Augmented Reasoning\nframework for Tables (TART), which integrates LLMs with specialized tools. TART\ncontains three key components: a table formatter to ensure accurate data\nrepresentation, a tool maker to develop specific computational tools, and an\nexplanation generator to maintain explainability. We also present the TOOLTAB\ndataset, a new benchmark designed specifically for training LLMs in table-tool\nintegration. Our experiments indicate that TART achieves substantial\nimprovements over existing methods (e.g., Chain-of-Thought) by improving both\nthe precision of data processing and the clarity of the reasoning process.\nNotably, TART paired with CodeLlama achieves 90.0% of the accuracy of the\nclosed-sourced LLM GPT-3.5-turbo, highlighting its robustness in diverse\nreal-world scenarios. All the code and data are available at\nhttps://github.com/XinyuanLu00/TART."
                },
                "authors": [
                    {
                        "name": "Xinyuan Lu"
                    },
                    {
                        "name": "Liangming Pan"
                    },
                    {
                        "name": "Yubo Ma"
                    },
                    {
                        "name": "Preslav Nakov"
                    },
                    {
                        "name": "Min-Yen Kan"
                    }
                ],
                "author_detail": {
                    "name": "Min-Yen Kan"
                },
                "author": "Min-Yen Kan",
                "arxiv_comment": "technical report",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.11724v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.11724v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.11719v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.11719v1",
                "updated": "2024-09-18T06:00:05Z",
                "updated_parsed": [
                    2024,
                    9,
                    18,
                    6,
                    0,
                    5,
                    2,
                    262,
                    0
                ],
                "published": "2024-09-18T06:00:05Z",
                "published_parsed": [
                    2024,
                    9,
                    18,
                    6,
                    0,
                    5,
                    2,
                    262,
                    0
                ],
                "title": "Short-term variability of the transitional pulsar candidate CXOU\n  J110926.4-650224 from X-rays to infrared",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Short-term variability of the transitional pulsar candidate CXOU\n  J110926.4-650224 from X-rays to infrared"
                },
                "summary": "CXOU J110926.4-650224 is a candidate transitional millisecond pulsar (tMSP)\nwith X-ray and radio emission properties reminiscent of those observed in\nconfirmed tMSPs in their X-ray 'subluminous' disc state. We present the results\nof observing campaigns that, for the first time, characterise the optical and\nnear-infrared variability of this source and establish a connection with the\nmode-switching phenomenon observed in X-rays. The optical emission exhibited\nflickering activity, frequent dipping episodes where it appeared redder, and a\nmulti-peaked flare where it was bluer. The variability pattern was strongly\ncorrelated with that of the X-ray emission. Each dip matched an X-ray low-mode\nepisode, indicating that a significant portion of the optical emission\noriginates from nearly the same region as the X-ray emission. The near-infrared\nemission also displayed remarkable variability, including a dip of 20 min in\nlength during which it nearly vanished. Time-resolved optical spectroscopic\nobservations reveal significant changes in the properties of emission lines\nfrom the disc and help infer the spectral type of the companion star to be\nbetween K0 and K5. We compare the properties of CXOU J110926.4-650224 with\nthose of other tMSPs in the X-ray subluminous disc state and discuss our\nfindings within the context of a recently proposed scenario that explains the\nphenomenology exhibited by the prototypical tMSP PSR J1023+0038.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CXOU J110926.4-650224 is a candidate transitional millisecond pulsar (tMSP)\nwith X-ray and radio emission properties reminiscent of those observed in\nconfirmed tMSPs in their X-ray 'subluminous' disc state. We present the results\nof observing campaigns that, for the first time, characterise the optical and\nnear-infrared variability of this source and establish a connection with the\nmode-switching phenomenon observed in X-rays. The optical emission exhibited\nflickering activity, frequent dipping episodes where it appeared redder, and a\nmulti-peaked flare where it was bluer. The variability pattern was strongly\ncorrelated with that of the X-ray emission. Each dip matched an X-ray low-mode\nepisode, indicating that a significant portion of the optical emission\noriginates from nearly the same region as the X-ray emission. The near-infrared\nemission also displayed remarkable variability, including a dip of 20 min in\nlength during which it nearly vanished. Time-resolved optical spectroscopic\nobservations reveal significant changes in the properties of emission lines\nfrom the disc and help infer the spectral type of the companion star to be\nbetween K0 and K5. We compare the properties of CXOU J110926.4-650224 with\nthose of other tMSPs in the X-ray subluminous disc state and discuss our\nfindings within the context of a recently proposed scenario that explains the\nphenomenology exhibited by the prototypical tMSP PSR J1023+0038."
                },
                "authors": [
                    {
                        "name": "F. Coti Zelati"
                    },
                    {
                        "name": "D. de Martino"
                    },
                    {
                        "name": "V. S. Dhillon"
                    },
                    {
                        "name": "T. R. Marsh"
                    },
                    {
                        "name": "F. Vincentelli"
                    },
                    {
                        "name": "S. Campana"
                    },
                    {
                        "name": "D. F. Torres"
                    },
                    {
                        "name": "A. Papitto"
                    },
                    {
                        "name": "M. C. Baglio"
                    },
                    {
                        "name": "A. Miraval Zanon"
                    },
                    {
                        "name": "N. Rea"
                    },
                    {
                        "name": "J. Brink"
                    },
                    {
                        "name": "D. A. H. Buckley"
                    },
                    {
                        "name": "P. D'Avanzo"
                    },
                    {
                        "name": "G. Illiano"
                    },
                    {
                        "name": "A. Manca"
                    },
                    {
                        "name": "A. Marino"
                    }
                ],
                "author_detail": {
                    "name": "A. Marino"
                },
                "author": "A. Marino",
                "arxiv_comment": "12 pages, 7 figures, 2 tables. Accepted for publication on Astronomy\n  & Astrophysics",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.11719v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.11719v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.HE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.HE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.11283v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.11283v2",
                "updated": "2024-09-18T05:42:01Z",
                "updated_parsed": [
                    2024,
                    9,
                    18,
                    5,
                    42,
                    1,
                    2,
                    262,
                    0
                ],
                "published": "2024-09-17T15:38:36Z",
                "published_parsed": [
                    2024,
                    9,
                    17,
                    15,
                    38,
                    36,
                    1,
                    261,
                    0
                ],
                "title": "Zero-resource Hallucination Detection for Text Generation via\n  Graph-based Contextual Knowledge Triples Modeling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Zero-resource Hallucination Detection for Text Generation via\n  Graph-based Contextual Knowledge Triples Modeling"
                },
                "summary": "LLMs obtain remarkable performance but suffer from hallucinations. Most\nresearch on detecting hallucination focuses on the questions with short and\nconcrete correct answers that are easy to check the faithfulness. Hallucination\ndetections for text generation with open-ended answers are more challenging.\nSome researchers use external knowledge to detect hallucinations in generated\ntexts, but external resources for specific scenarios are hard to access. Recent\nstudies on detecting hallucinations in long text without external resources\nconduct consistency comparison among multiple sampled outputs. To handle long\ntexts, researchers split long texts into multiple facts and individually\ncompare the consistency of each pairs of facts. However, these methods (1)\nhardly achieve alignment among multiple facts; (2) overlook dependencies\nbetween multiple contextual facts. In this paper, we propose a graph-based\ncontext-aware (GCA) hallucination detection for text generations, which aligns\nknowledge facts and considers the dependencies between contextual knowledge\ntriples in consistency comparison. Particularly, to align multiple facts, we\nconduct a triple-oriented response segmentation to extract multiple knowledge\ntriples. To model dependencies among contextual knowledge triple (facts), we\nconstruct contextual triple into a graph and enhance triples' interactions via\nmessage passing and aggregating via RGCN. To avoid the omission of knowledge\ntriples in long text, we conduct a LLM-based reverse verification via\nreconstructing the knowledge triples. Experiments show that our model enhances\nhallucination detection and excels all baselines.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLMs obtain remarkable performance but suffer from hallucinations. Most\nresearch on detecting hallucination focuses on the questions with short and\nconcrete correct answers that are easy to check the faithfulness. Hallucination\ndetections for text generation with open-ended answers are more challenging.\nSome researchers use external knowledge to detect hallucinations in generated\ntexts, but external resources for specific scenarios are hard to access. Recent\nstudies on detecting hallucinations in long text without external resources\nconduct consistency comparison among multiple sampled outputs. To handle long\ntexts, researchers split long texts into multiple facts and individually\ncompare the consistency of each pairs of facts. However, these methods (1)\nhardly achieve alignment among multiple facts; (2) overlook dependencies\nbetween multiple contextual facts. In this paper, we propose a graph-based\ncontext-aware (GCA) hallucination detection for text generations, which aligns\nknowledge facts and considers the dependencies between contextual knowledge\ntriples in consistency comparison. Particularly, to align multiple facts, we\nconduct a triple-oriented response segmentation to extract multiple knowledge\ntriples. To model dependencies among contextual knowledge triple (facts), we\nconstruct contextual triple into a graph and enhance triples' interactions via\nmessage passing and aggregating via RGCN. To avoid the omission of knowledge\ntriples in long text, we conduct a LLM-based reverse verification via\nreconstructing the knowledge triples. Experiments show that our model enhances\nhallucination detection and excels all baselines."
                },
                "authors": [
                    {
                        "name": "Xinyue Fang"
                    },
                    {
                        "name": "Zhen Huang"
                    },
                    {
                        "name": "Zhiliang Tian"
                    },
                    {
                        "name": "Minghui Fang"
                    },
                    {
                        "name": "Ziyi Pan"
                    },
                    {
                        "name": "Quntian Fang"
                    },
                    {
                        "name": "Zhihua Wen"
                    },
                    {
                        "name": "Hengyue Pan"
                    },
                    {
                        "name": "Dongsheng Li"
                    }
                ],
                "author_detail": {
                    "name": "Dongsheng Li"
                },
                "author": "Dongsheng Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.11283v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.11283v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.00978v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.00978v2",
                "updated": "2024-09-18T05:28:12Z",
                "updated_parsed": [
                    2024,
                    9,
                    18,
                    5,
                    28,
                    12,
                    2,
                    262,
                    0
                ],
                "published": "2024-04-01T07:49:11Z",
                "published_parsed": [
                    2024,
                    4,
                    1,
                    7,
                    49,
                    11,
                    0,
                    92,
                    0
                ],
                "title": "Prior Constraints-based Reward Model Training for Aligning Large\n  Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Prior Constraints-based Reward Model Training for Aligning Large\n  Language Models"
                },
                "summary": "Reinforcement learning with human feedback for aligning large language models\n(LLMs) trains a reward model typically using ranking loss with comparison\npairs.However, the training procedure suffers from an inherent problem: the\nuncontrolled scaling of reward scores during reinforcement learning due to the\nlack of constraints while training the reward model.This paper proposes a Prior\nConstraints-based Reward Model (namely PCRM) training method to mitigate this\nproblem. PCRM incorporates prior constraints, specifically, length ratio and\ncosine similarity between outputs of each comparison pair, during reward model\ntraining to regulate optimization magnitude and control score margins. We\ncomprehensively evaluate PCRM by examining its rank correlation with human\npreferences and its effectiveness in aligning LLMs via RL. Experimental results\ndemonstrate that PCRM significantly improves alignment performance by\neffectively constraining reward score scaling. As another bonus, our method is\neasily integrated into arbitrary rank-based alignment methods, such as direct\npreference optimization, and can yield consistent improvement.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reinforcement learning with human feedback for aligning large language models\n(LLMs) trains a reward model typically using ranking loss with comparison\npairs.However, the training procedure suffers from an inherent problem: the\nuncontrolled scaling of reward scores during reinforcement learning due to the\nlack of constraints while training the reward model.This paper proposes a Prior\nConstraints-based Reward Model (namely PCRM) training method to mitigate this\nproblem. PCRM incorporates prior constraints, specifically, length ratio and\ncosine similarity between outputs of each comparison pair, during reward model\ntraining to regulate optimization magnitude and control score margins. We\ncomprehensively evaluate PCRM by examining its rank correlation with human\npreferences and its effectiveness in aligning LLMs via RL. Experimental results\ndemonstrate that PCRM significantly improves alignment performance by\neffectively constraining reward score scaling. As another bonus, our method is\neasily integrated into arbitrary rank-based alignment methods, such as direct\npreference optimization, and can yield consistent improvement."
                },
                "authors": [
                    {
                        "name": "Hang Zhou"
                    },
                    {
                        "name": "Chenglong Wang"
                    },
                    {
                        "name": "Yimin Hu"
                    },
                    {
                        "name": "Tong Xiao"
                    },
                    {
                        "name": "Chunliang Zhang"
                    },
                    {
                        "name": "Jingbo Zhu"
                    }
                ],
                "author_detail": {
                    "name": "Jingbo Zhu"
                },
                "author": "Jingbo Zhu",
                "arxiv_comment": "Accepted by CCL 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.00978v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.00978v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.11704v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.11704v1",
                "updated": "2024-09-18T05:13:18Z",
                "updated_parsed": [
                    2024,
                    9,
                    18,
                    5,
                    13,
                    18,
                    2,
                    262,
                    0
                ],
                "published": "2024-09-18T05:13:18Z",
                "published_parsed": [
                    2024,
                    9,
                    18,
                    5,
                    13,
                    18,
                    2,
                    262,
                    0
                ],
                "title": "From Lists to Emojis: How Format Bias Affects Model Alignment",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "From Lists to Emojis: How Format Bias Affects Model Alignment"
                },
                "summary": "In this paper, we study format biases in reinforcement learning from human\nfeedback (RLHF). We observe that many widely-used preference models, including\nhuman evaluators, GPT-4, and top-ranking models on the RewardBench benchmark,\nexhibit strong biases towards specific format patterns, such as lists, links,\nbold text, and emojis. Furthermore, large language models (LLMs) can exploit\nthese biases to achieve higher rankings on popular benchmarks like AlpacaEval\nand LMSYS Chatbot Arena. One notable example of this is verbosity bias, where\ncurrent preference models favor longer responses that appear more\ncomprehensive, even when their quality is equal to or lower than shorter,\ncompeting responses. However, format biases beyond verbosity remain largely\nunderexplored in the literature. In this work, we extend the study of biases in\npreference learning beyond the commonly recognized length bias, offering a\ncomprehensive analysis of a wider range of format biases. Additionally, we show\nthat with a small amount of biased data (less than 1%), we can inject\nsignificant bias into the reward model. Moreover, these format biases can also\nbe easily exploited by downstream alignment algorithms, such as best-of-n\nsampling and online iterative DPO, as it is usually easier to manipulate the\nformat than to improve the quality of responses. Our findings emphasize the\nneed to disentangle format and content both for designing alignment algorithms\nand evaluating models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this paper, we study format biases in reinforcement learning from human\nfeedback (RLHF). We observe that many widely-used preference models, including\nhuman evaluators, GPT-4, and top-ranking models on the RewardBench benchmark,\nexhibit strong biases towards specific format patterns, such as lists, links,\nbold text, and emojis. Furthermore, large language models (LLMs) can exploit\nthese biases to achieve higher rankings on popular benchmarks like AlpacaEval\nand LMSYS Chatbot Arena. One notable example of this is verbosity bias, where\ncurrent preference models favor longer responses that appear more\ncomprehensive, even when their quality is equal to or lower than shorter,\ncompeting responses. However, format biases beyond verbosity remain largely\nunderexplored in the literature. In this work, we extend the study of biases in\npreference learning beyond the commonly recognized length bias, offering a\ncomprehensive analysis of a wider range of format biases. Additionally, we show\nthat with a small amount of biased data (less than 1%), we can inject\nsignificant bias into the reward model. Moreover, these format biases can also\nbe easily exploited by downstream alignment algorithms, such as best-of-n\nsampling and online iterative DPO, as it is usually easier to manipulate the\nformat than to improve the quality of responses. Our findings emphasize the\nneed to disentangle format and content both for designing alignment algorithms\nand evaluating models."
                },
                "authors": [
                    {
                        "name": "Xuanchang Zhang"
                    },
                    {
                        "name": "Wei Xiong"
                    },
                    {
                        "name": "Lichang Chen"
                    },
                    {
                        "name": "Tianyi Zhou"
                    },
                    {
                        "name": "Heng Huang"
                    },
                    {
                        "name": "Tong Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Tong Zhang"
                },
                "author": "Tong Zhang",
                "arxiv_comment": "Working in progress",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.11704v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.11704v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.11703v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.11703v1",
                "updated": "2024-09-18T04:56:52Z",
                "updated_parsed": [
                    2024,
                    9,
                    18,
                    4,
                    56,
                    52,
                    2,
                    262,
                    0
                ],
                "published": "2024-09-18T04:56:52Z",
                "published_parsed": [
                    2024,
                    9,
                    18,
                    4,
                    56,
                    52,
                    2,
                    262,
                    0
                ],
                "title": "Harnessing LLMs for API Interactions: A Framework for Classification and\n  Synthetic Data Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Harnessing LLMs for API Interactions: A Framework for Classification and\n  Synthetic Data Generation"
                },
                "summary": "As Large Language Models (LLMs) advance in natural language processing, there\nis growing interest in leveraging their capabilities to simplify software\ninteractions. In this paper, we propose a novel system that integrates LLMs for\nboth classifying natural language inputs into corresponding API calls and\nautomating the creation of sample datasets tailored to specific API functions.\nBy classifying natural language commands, our system allows users to invoke\ncomplex software functionalities through simple inputs, improving interaction\nefficiency and lowering the barrier to software utilization. Our dataset\ngeneration approach also enables the efficient and systematic evaluation of\ndifferent LLMs in classifying API calls, offering a practical tool for\ndevelopers or business owners to assess the suitability of LLMs for customized\nAPI management. We conduct experiments on several prominent LLMs using\ngenerated sample datasets for various API functions. The results show that\nGPT-4 achieves a high classification accuracy of 0.996, while LLaMA-3-8B\nperforms much worse at 0.759. These findings highlight the potential of LLMs to\ntransform API management and validate the effectiveness of our system in\nguiding model testing and selection across diverse applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As Large Language Models (LLMs) advance in natural language processing, there\nis growing interest in leveraging their capabilities to simplify software\ninteractions. In this paper, we propose a novel system that integrates LLMs for\nboth classifying natural language inputs into corresponding API calls and\nautomating the creation of sample datasets tailored to specific API functions.\nBy classifying natural language commands, our system allows users to invoke\ncomplex software functionalities through simple inputs, improving interaction\nefficiency and lowering the barrier to software utilization. Our dataset\ngeneration approach also enables the efficient and systematic evaluation of\ndifferent LLMs in classifying API calls, offering a practical tool for\ndevelopers or business owners to assess the suitability of LLMs for customized\nAPI management. We conduct experiments on several prominent LLMs using\ngenerated sample datasets for various API functions. The results show that\nGPT-4 achieves a high classification accuracy of 0.996, while LLaMA-3-8B\nperforms much worse at 0.759. These findings highlight the potential of LLMs to\ntransform API management and validate the effectiveness of our system in\nguiding model testing and selection across diverse applications."
                },
                "authors": [
                    {
                        "name": "Chunliang Tao"
                    },
                    {
                        "name": "Xiaojing Fan"
                    },
                    {
                        "name": "Yahe Yang"
                    }
                ],
                "author_detail": {
                    "name": "Yahe Yang"
                },
                "author": "Yahe Yang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.11703v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.11703v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2312.05821v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2312.05821v3",
                "updated": "2024-09-18T04:53:46Z",
                "updated_parsed": [
                    2024,
                    9,
                    18,
                    4,
                    53,
                    46,
                    2,
                    262,
                    0
                ],
                "published": "2023-12-10T08:41:24Z",
                "published_parsed": [
                    2023,
                    12,
                    10,
                    8,
                    41,
                    24,
                    6,
                    344,
                    0
                ],
                "title": "ASVD: Activation-aware Singular Value Decomposition for Compressing\n  Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ASVD: Activation-aware Singular Value Decomposition for Compressing\n  Large Language Models"
                },
                "summary": "In this paper, we introduce a new post-training compression paradigm for\nLarge Language Models (LLMs) to facilitate their wider adoption. We delve into\nLLM weight low-rank factorization, and find that the challenges of this task\nstem from the outlier phenomenon in the LLM activations and the sensitivity\ndifference among various kinds of layers. To address these issues, we propose a\ntraining-free approach called Activation-aware Singular Value Decomposition\n(ASVD). Specifically, ASVD manages activation outliers by scaling the weight\nmatrix based on the activation distribution, thereby enhancing decomposition\naccuracy. Additionally, we propose an efficient iterative calibration process\nto optimize layer-specific decomposition by addressing the varying sensitivity\nof different LLM layers. ASVD can compress a network by 10-20%, without\ncompromising the performance of LLMs. Based on the success of the low-rank\ndecomposition of projection matrices in the self-attention module, we further\nintroduce ASVD to compress the KV cache. By reducing the channel dimension of\nKV activations, memory requirements for KV cache can be largely reduced. Thanks\nto the 50-75% reduction in the rank of the KV projection matrices, ASVD can\nfurther achieve 50% KV cache reductions without performance drop in a\ntraining-free manner.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this paper, we introduce a new post-training compression paradigm for\nLarge Language Models (LLMs) to facilitate their wider adoption. We delve into\nLLM weight low-rank factorization, and find that the challenges of this task\nstem from the outlier phenomenon in the LLM activations and the sensitivity\ndifference among various kinds of layers. To address these issues, we propose a\ntraining-free approach called Activation-aware Singular Value Decomposition\n(ASVD). Specifically, ASVD manages activation outliers by scaling the weight\nmatrix based on the activation distribution, thereby enhancing decomposition\naccuracy. Additionally, we propose an efficient iterative calibration process\nto optimize layer-specific decomposition by addressing the varying sensitivity\nof different LLM layers. ASVD can compress a network by 10-20%, without\ncompromising the performance of LLMs. Based on the success of the low-rank\ndecomposition of projection matrices in the self-attention module, we further\nintroduce ASVD to compress the KV cache. By reducing the channel dimension of\nKV activations, memory requirements for KV cache can be largely reduced. Thanks\nto the 50-75% reduction in the rank of the KV projection matrices, ASVD can\nfurther achieve 50% KV cache reductions without performance drop in a\ntraining-free manner."
                },
                "authors": [
                    {
                        "name": "Zhihang Yuan"
                    },
                    {
                        "name": "Yuzhang Shang"
                    },
                    {
                        "name": "Yue Song"
                    },
                    {
                        "name": "Qiang Wu"
                    },
                    {
                        "name": "Yan Yan"
                    },
                    {
                        "name": "Guangyu Sun"
                    }
                ],
                "author_detail": {
                    "name": "Guangyu Sun"
                },
                "author": "Guangyu Sun",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2312.05821v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2312.05821v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.11701v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.11701v1",
                "updated": "2024-09-18T04:51:19Z",
                "updated_parsed": [
                    2024,
                    9,
                    18,
                    4,
                    51,
                    19,
                    2,
                    262,
                    0
                ],
                "published": "2024-09-18T04:51:19Z",
                "published_parsed": [
                    2024,
                    9,
                    18,
                    4,
                    51,
                    19,
                    2,
                    262,
                    0
                ],
                "title": "Bias Reduction in Matched Observational Studies with Continuous\n  Treatments: Calipered Non-Bipartite Matching and Bias-Corrected Estimation\n  and Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Bias Reduction in Matched Observational Studies with Continuous\n  Treatments: Calipered Non-Bipartite Matching and Bias-Corrected Estimation\n  and Inference"
                },
                "summary": "Matching is a commonly used causal inference framework in observational\nstudies. By pairing individuals with different treatment values but with the\nsame values of covariates (i.e., exact matching), the sample average treatment\neffect (SATE) can be consistently estimated and inferred using the classic\nNeyman-type (difference-in-means) estimator and confidence interval. However,\ninexact matching typically exists in practice and may cause substantial bias\nfor the downstream treatment effect estimation and inference. Many methods have\nbeen proposed to reduce bias due to inexact matching in the binary treatment\ncase. However, to our knowledge, no existing work has systematically\ninvestigated bias due to inexact matching in the continuous treatment case. To\nfill this blank, we propose a general framework for reducing bias in inexactly\nmatched observational studies with continuous treatments. In the matching\nstage, we propose a carefully formulated caliper that incorporates the\ninformation of both the paired covariates and treatment doses to better tailor\nmatching for the downstream SATE estimation and inference. In the estimation\nand inference stage, we propose a bias-corrected Neyman estimator paired with\nthe corresponding bias-corrected variance estimator to leverage the information\non propensity density discrepancies after inexact matching to further reduce\nthe bias due to inexact matching. We apply our proposed framework to COVID-19\nsocial mobility data to showcase differences between classic and bias-corrected\nSATE estimation and inference.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Matching is a commonly used causal inference framework in observational\nstudies. By pairing individuals with different treatment values but with the\nsame values of covariates (i.e., exact matching), the sample average treatment\neffect (SATE) can be consistently estimated and inferred using the classic\nNeyman-type (difference-in-means) estimator and confidence interval. However,\ninexact matching typically exists in practice and may cause substantial bias\nfor the downstream treatment effect estimation and inference. Many methods have\nbeen proposed to reduce bias due to inexact matching in the binary treatment\ncase. However, to our knowledge, no existing work has systematically\ninvestigated bias due to inexact matching in the continuous treatment case. To\nfill this blank, we propose a general framework for reducing bias in inexactly\nmatched observational studies with continuous treatments. In the matching\nstage, we propose a carefully formulated caliper that incorporates the\ninformation of both the paired covariates and treatment doses to better tailor\nmatching for the downstream SATE estimation and inference. In the estimation\nand inference stage, we propose a bias-corrected Neyman estimator paired with\nthe corresponding bias-corrected variance estimator to leverage the information\non propensity density discrepancies after inexact matching to further reduce\nthe bias due to inexact matching. We apply our proposed framework to COVID-19\nsocial mobility data to showcase differences between classic and bias-corrected\nSATE estimation and inference."
                },
                "authors": [
                    {
                        "name": "Anthony Frazier"
                    },
                    {
                        "name": "Siyu Heng"
                    },
                    {
                        "name": "Wen Zhou"
                    }
                ],
                "author_detail": {
                    "name": "Wen Zhou"
                },
                "author": "Wen Zhou",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.11701v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.11701v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ME",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.AP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.11700v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.11700v1",
                "updated": "2024-09-18T04:46:17Z",
                "updated_parsed": [
                    2024,
                    9,
                    18,
                    4,
                    46,
                    17,
                    2,
                    262,
                    0
                ],
                "published": "2024-09-18T04:46:17Z",
                "published_parsed": [
                    2024,
                    9,
                    18,
                    4,
                    46,
                    17,
                    2,
                    262,
                    0
                ],
                "title": "Real-Time Sound Event Localization and Detection: Deployment Challenges\n  on Edge Devices",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Real-Time Sound Event Localization and Detection: Deployment Challenges\n  on Edge Devices"
                },
                "summary": "Sound event localization and detection (SELD) is critical for various\nreal-world applications, including smart monitoring and Internet of Things\n(IoT) systems. Although deep neural networks (DNNs) represent the\nstate-of-the-art approach for SELD, their significant computational complexity\nand model sizes present challenges for deployment on resource-constrained edge\ndevices, especially under real-time conditions. Despite the growing need for\nreal-time SELD, research in this area remains limited. In this paper, we\ninvestigate the unique challenges of deploying SELD systems for real-world,\nreal-time applications by performing extensive experiments on a commercially\navailable Raspberry Pi 3 edge device. Our findings reveal two critical, often\noverlooked considerations: the high computational cost of feature extraction\nand the performance degradation associated with low-latency, real-time\ninference. This paper provides valuable insights and considerations for future\nwork toward developing more efficient and robust real-time SELD systems",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Sound event localization and detection (SELD) is critical for various\nreal-world applications, including smart monitoring and Internet of Things\n(IoT) systems. Although deep neural networks (DNNs) represent the\nstate-of-the-art approach for SELD, their significant computational complexity\nand model sizes present challenges for deployment on resource-constrained edge\ndevices, especially under real-time conditions. Despite the growing need for\nreal-time SELD, research in this area remains limited. In this paper, we\ninvestigate the unique challenges of deploying SELD systems for real-world,\nreal-time applications by performing extensive experiments on a commercially\navailable Raspberry Pi 3 edge device. Our findings reveal two critical, often\noverlooked considerations: the high computational cost of feature extraction\nand the performance degradation associated with low-latency, real-time\ninference. This paper provides valuable insights and considerations for future\nwork toward developing more efficient and robust real-time SELD systems"
                },
                "authors": [
                    {
                        "name": "Jun Wei Yeow"
                    },
                    {
                        "name": "Ee-Leng Tan"
                    },
                    {
                        "name": "Jisheng Bai"
                    },
                    {
                        "name": "Santi Peksi"
                    },
                    {
                        "name": "Woon-Seng Gan"
                    }
                ],
                "author_detail": {
                    "name": "Woon-Seng Gan"
                },
                "author": "Woon-Seng Gan",
                "arxiv_comment": "Submitted to ICASSP'25. Code is available at this link :\n  https://github.com/itsjunwei/Realtime-SELD-Edge",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.11700v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.11700v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SP",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.11691v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.11691v1",
                "updated": "2024-09-18T04:13:33Z",
                "updated_parsed": [
                    2024,
                    9,
                    18,
                    4,
                    13,
                    33,
                    2,
                    262,
                    0
                ],
                "published": "2024-09-18T04:13:33Z",
                "published_parsed": [
                    2024,
                    9,
                    18,
                    4,
                    13,
                    33,
                    2,
                    262,
                    0
                ],
                "title": "An evaluation of source-blending impact on the calibration of SKA EoR\n  experiments",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "An evaluation of source-blending impact on the calibration of SKA EoR\n  experiments"
                },
                "summary": "Twenty-one-centimetre signals from the Epoch of Reionization (EoR) are\nexpected to be detected in the low-frequency radio window by the\nnext-generation interferometers, particularly the Square Kilometre Array (SKA).\nHowever, precision data analysis pipelines are required to minimize the\nsystematics within an infinitesimal error budget. Consequently, there is a\ngrowing need to characterize the sources of errors in EoR analysis. In this\nstudy, we identify one such error origin, namely source blending, which is\nintroduced by the overlap of objects in the densely populated observing sky\nunder SKA1-Low's unprecedented sensitivity and resolution, and evaluate its\ntwo-fold impact in both the spatial and frequency domains using a novel hybrid\nevaluation (HEVAL) pipeline combining end-to-end simulation with an analytic\nmethod to mimic EoR analysis pipelines. Sky models corrupted by source blending\ninduce small but severe frequency-dependent calibration errors when coupled\nwith astronomical foregrounds, impeding EoR parameter inference with strong\nadditive residuals in the two-dimensional power spectrum space. We report that\nadditive residuals from poor calibration against sky models with blending\nratios of 5 and 0.5 per cent significantly contaminate the EoR window. In\ncontrast, the sky model with a 0.05 per cent blending ratio leaves little\nresidual imprint within the EoR window, therefore identifying a blending\ntolerance at approximately 0.05 per cent. Given that the SKA observing sky is\nestimated to suffer from an extended level of blending, strategies involving\nde-blending, frequency-dependent error mitigation, or a combination of both,\nare required to effectively attenuate the calibration impact of source-blending\ndefects.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Twenty-one-centimetre signals from the Epoch of Reionization (EoR) are\nexpected to be detected in the low-frequency radio window by the\nnext-generation interferometers, particularly the Square Kilometre Array (SKA).\nHowever, precision data analysis pipelines are required to minimize the\nsystematics within an infinitesimal error budget. Consequently, there is a\ngrowing need to characterize the sources of errors in EoR analysis. In this\nstudy, we identify one such error origin, namely source blending, which is\nintroduced by the overlap of objects in the densely populated observing sky\nunder SKA1-Low's unprecedented sensitivity and resolution, and evaluate its\ntwo-fold impact in both the spatial and frequency domains using a novel hybrid\nevaluation (HEVAL) pipeline combining end-to-end simulation with an analytic\nmethod to mimic EoR analysis pipelines. Sky models corrupted by source blending\ninduce small but severe frequency-dependent calibration errors when coupled\nwith astronomical foregrounds, impeding EoR parameter inference with strong\nadditive residuals in the two-dimensional power spectrum space. We report that\nadditive residuals from poor calibration against sky models with blending\nratios of 5 and 0.5 per cent significantly contaminate the EoR window. In\ncontrast, the sky model with a 0.05 per cent blending ratio leaves little\nresidual imprint within the EoR window, therefore identifying a blending\ntolerance at approximately 0.05 per cent. Given that the SKA observing sky is\nestimated to suffer from an extended level of blending, strategies involving\nde-blending, frequency-dependent error mitigation, or a combination of both,\nare required to effectively attenuate the calibration impact of source-blending\ndefects."
                },
                "authors": [
                    {
                        "name": "Chenxi Shan"
                    },
                    {
                        "name": "Haiguang Xu"
                    },
                    {
                        "name": "Yongkai Zhu"
                    },
                    {
                        "name": "Yuanyuan Zhao"
                    },
                    {
                        "name": "Sarah V. White"
                    },
                    {
                        "name": "Jack L. B. Line"
                    },
                    {
                        "name": "Dongchao Zheng"
                    },
                    {
                        "name": "Zhenghao Zhu"
                    },
                    {
                        "name": "Dan Hu"
                    },
                    {
                        "name": "Zhongli Zhang"
                    },
                    {
                        "name": "Xiangping Wu"
                    }
                ],
                "author_detail": {
                    "name": "Xiangping Wu"
                },
                "arxiv_affiliation": "National Astronomical Observatories, Chinese Academy of Sciences",
                "author": "Xiangping Wu",
                "arxiv_comment": "25 pages, 15 figures, 5 tables, accepted for publication in Monthly\n  Notices of the Royal Astronomical Society (MNRAS), Fg21Sim+ github\n  repository: https://github.com/Fg21Sim/Fg21SimPlus",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.11691v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.11691v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.IM",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.IM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "astro-ph.CO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "astro-ph.GA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "85-04, 85-08, 85-10",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "D.2.0; J.2; I.1.2; I.6.5; I.6.6",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.11690v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.11690v2",
                "updated": "2024-09-19T03:28:21Z",
                "updated_parsed": [
                    2024,
                    9,
                    19,
                    3,
                    28,
                    21,
                    3,
                    263,
                    0
                ],
                "published": "2024-09-18T04:10:44Z",
                "published_parsed": [
                    2024,
                    9,
                    18,
                    4,
                    10,
                    44,
                    2,
                    262,
                    0
                ],
                "title": "LLM-Powered Text Simulation Attack Against ID-Free Recommender Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM-Powered Text Simulation Attack Against ID-Free Recommender Systems"
                },
                "summary": "The ID-free recommendation paradigm has been proposed to address the\nlimitation that traditional recommender systems struggle to model cold-start\nusers or items with new IDs. Despite its effectiveness, this study uncovers\nthat ID-free recommender systems are vulnerable to the proposed Text Simulation\nattack (TextSimu) which aims to promote specific target items. As a novel type\nof text poisoning attack, TextSimu exploits large language models (LLM) to\nalter the textual information of target items by simulating the characteristics\nof popular items. It operates effectively in both black-box and white-box\nsettings, utilizing two key components: a unified popularity extraction module,\nwhich captures the essential characteristics of popular items, and an N-persona\nconsistency simulation strategy, which creates multiple personas to\ncollaboratively synthesize refined promotional textual descriptions for target\nitems by simulating the popular items. To withstand TextSimu-like attacks, we\nfurther explore the detection approach for identifying LLM-generated\npromotional text. Extensive experiments conducted on three datasets demonstrate\nthat TextSimu poses a more significant threat than existing poisoning attacks,\nwhile our defense method can detect malicious text of target items generated by\nTextSimu. By identifying the vulnerability, we aim to advance the development\nof more robust ID-free recommender systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The ID-free recommendation paradigm has been proposed to address the\nlimitation that traditional recommender systems struggle to model cold-start\nusers or items with new IDs. Despite its effectiveness, this study uncovers\nthat ID-free recommender systems are vulnerable to the proposed Text Simulation\nattack (TextSimu) which aims to promote specific target items. As a novel type\nof text poisoning attack, TextSimu exploits large language models (LLM) to\nalter the textual information of target items by simulating the characteristics\nof popular items. It operates effectively in both black-box and white-box\nsettings, utilizing two key components: a unified popularity extraction module,\nwhich captures the essential characteristics of popular items, and an N-persona\nconsistency simulation strategy, which creates multiple personas to\ncollaboratively synthesize refined promotional textual descriptions for target\nitems by simulating the popular items. To withstand TextSimu-like attacks, we\nfurther explore the detection approach for identifying LLM-generated\npromotional text. Extensive experiments conducted on three datasets demonstrate\nthat TextSimu poses a more significant threat than existing poisoning attacks,\nwhile our defense method can detect malicious text of target items generated by\nTextSimu. By identifying the vulnerability, we aim to advance the development\nof more robust ID-free recommender systems."
                },
                "authors": [
                    {
                        "name": "Zongwei Wang"
                    },
                    {
                        "name": "Min Gao"
                    },
                    {
                        "name": "Junliang Yu"
                    },
                    {
                        "name": "Xinyi Gao"
                    },
                    {
                        "name": "Quoc Viet Hung Nguyen"
                    },
                    {
                        "name": "Shazia Sadiq"
                    },
                    {
                        "name": "Hongzhi Yin"
                    }
                ],
                "author_detail": {
                    "name": "Hongzhi Yin"
                },
                "author": "Hongzhi Yin",
                "arxiv_comment": "12 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.11690v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.11690v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.11681v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.11681v1",
                "updated": "2024-09-18T03:45:44Z",
                "updated_parsed": [
                    2024,
                    9,
                    18,
                    3,
                    45,
                    44,
                    2,
                    262,
                    0
                ],
                "published": "2024-09-18T03:45:44Z",
                "published_parsed": [
                    2024,
                    9,
                    18,
                    3,
                    45,
                    44,
                    2,
                    262,
                    0
                ],
                "title": "Gradient-Driven 3D Segmentation and Affordance Transfer in Gaussian\n  Splatting Using 2D Masks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Gradient-Driven 3D Segmentation and Affordance Transfer in Gaussian\n  Splatting Using 2D Masks"
                },
                "summary": "3D Gaussian Splatting has emerged as a powerful 3D scene representation\ntechnique, capturing fine details with high efficiency. In this paper, we\nintroduce a novel voting-based method that extends 2D segmentation models to 3D\nGaussian splats. Our approach leverages masked gradients, where gradients are\nfiltered by input 2D masks, and these gradients are used as votes to achieve\naccurate segmentation. As a byproduct, we discovered that inference-time\ngradients can also be used to prune Gaussians, resulting in up to 21%\ncompression. Additionally, we explore few-shot affordance transfer, allowing\nannotations from 2D images to be effectively transferred onto 3D Gaussian\nsplats. The robust yet straightforward mathematical formulation underlying this\napproach makes it a highly effective tool for numerous downstream applications,\nsuch as augmented reality (AR), object editing, and robotics. The project code\nand additional resources are available at\nhttps://jojijoseph.github.io/3dgs-segmentation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "3D Gaussian Splatting has emerged as a powerful 3D scene representation\ntechnique, capturing fine details with high efficiency. In this paper, we\nintroduce a novel voting-based method that extends 2D segmentation models to 3D\nGaussian splats. Our approach leverages masked gradients, where gradients are\nfiltered by input 2D masks, and these gradients are used as votes to achieve\naccurate segmentation. As a byproduct, we discovered that inference-time\ngradients can also be used to prune Gaussians, resulting in up to 21%\ncompression. Additionally, we explore few-shot affordance transfer, allowing\nannotations from 2D images to be effectively transferred onto 3D Gaussian\nsplats. The robust yet straightforward mathematical formulation underlying this\napproach makes it a highly effective tool for numerous downstream applications,\nsuch as augmented reality (AR), object editing, and robotics. The project code\nand additional resources are available at\nhttps://jojijoseph.github.io/3dgs-segmentation."
                },
                "authors": [
                    {
                        "name": "Joji Joseph"
                    },
                    {
                        "name": "Bharadwaj Amrutur"
                    },
                    {
                        "name": "Shalabh Bhatnagar"
                    }
                ],
                "author_detail": {
                    "name": "Shalabh Bhatnagar"
                },
                "author": "Shalabh Bhatnagar",
                "arxiv_comment": "Preprint, Under review for ICRA 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.11681v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.11681v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.11675v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.11675v1",
                "updated": "2024-09-18T03:30:01Z",
                "updated_parsed": [
                    2024,
                    9,
                    18,
                    3,
                    30,
                    1,
                    2,
                    262,
                    0
                ],
                "published": "2024-09-18T03:30:01Z",
                "published_parsed": [
                    2024,
                    9,
                    18,
                    3,
                    30,
                    1,
                    2,
                    262,
                    0
                ],
                "title": "Towards Explainable Goal Recognition Using Weight of Evidence (WoE): A\n  Human-Centered Approach",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Towards Explainable Goal Recognition Using Weight of Evidence (WoE): A\n  Human-Centered Approach"
                },
                "summary": "Goal recognition (GR) involves inferring an agent's unobserved goal from a\nsequence of observations. This is a critical problem in AI with diverse\napplications. Traditionally, GR has been addressed using 'inference to the best\nexplanation' or abduction, where hypotheses about the agent's goals are\ngenerated as the most plausible explanations for observed behavior.\nAlternatively, some approaches enhance interpretability by ensuring that an\nagent's behavior aligns with an observer's expectations or by making the\nreasoning behind decisions more transparent. In this work, we tackle a\ndifferent challenge: explaining the GR process in a way that is comprehensible\nto humans. We introduce and evaluate an explainable model for goal recognition\n(GR) agents, grounded in the theoretical framework and cognitive processes\nunderlying human behavior explanation. Drawing on insights from two human-agent\nstudies, we propose a conceptual framework for human-centered explanations of\nGR. Using this framework, we develop the eXplainable Goal Recognition (XGR)\nmodel, which generates explanations for both why and why not questions. We\nevaluate the model computationally across eight GR benchmarks and through three\nuser studies. The first study assesses the efficiency of generating human-like\nexplanations within the Sokoban game domain, the second examines perceived\nexplainability in the same domain, and the third evaluates the model's\neffectiveness in aiding decision-making in illegal fishing detection. Results\ndemonstrate that the XGR model significantly enhances user understanding,\ntrust, and decision-making compared to baseline models, underscoring its\npotential to improve human-agent collaboration.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Goal recognition (GR) involves inferring an agent's unobserved goal from a\nsequence of observations. This is a critical problem in AI with diverse\napplications. Traditionally, GR has been addressed using 'inference to the best\nexplanation' or abduction, where hypotheses about the agent's goals are\ngenerated as the most plausible explanations for observed behavior.\nAlternatively, some approaches enhance interpretability by ensuring that an\nagent's behavior aligns with an observer's expectations or by making the\nreasoning behind decisions more transparent. In this work, we tackle a\ndifferent challenge: explaining the GR process in a way that is comprehensible\nto humans. We introduce and evaluate an explainable model for goal recognition\n(GR) agents, grounded in the theoretical framework and cognitive processes\nunderlying human behavior explanation. Drawing on insights from two human-agent\nstudies, we propose a conceptual framework for human-centered explanations of\nGR. Using this framework, we develop the eXplainable Goal Recognition (XGR)\nmodel, which generates explanations for both why and why not questions. We\nevaluate the model computationally across eight GR benchmarks and through three\nuser studies. The first study assesses the efficiency of generating human-like\nexplanations within the Sokoban game domain, the second examines perceived\nexplainability in the same domain, and the third evaluates the model's\neffectiveness in aiding decision-making in illegal fishing detection. Results\ndemonstrate that the XGR model significantly enhances user understanding,\ntrust, and decision-making compared to baseline models, underscoring its\npotential to improve human-agent collaboration."
                },
                "authors": [
                    {
                        "name": "Abeer Alshehri"
                    },
                    {
                        "name": "Amal Abdulrahman"
                    },
                    {
                        "name": "Hajar Alamri"
                    },
                    {
                        "name": "Tim Miller"
                    },
                    {
                        "name": "Mor Vered"
                    }
                ],
                "author_detail": {
                    "name": "Mor Vered"
                },
                "author": "Mor Vered",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.11675v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.11675v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.11673v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.11673v1",
                "updated": "2024-09-18T03:20:04Z",
                "updated_parsed": [
                    2024,
                    9,
                    18,
                    3,
                    20,
                    4,
                    2,
                    262,
                    0
                ],
                "published": "2024-09-18T03:20:04Z",
                "published_parsed": [
                    2024,
                    9,
                    18,
                    3,
                    20,
                    4,
                    2,
                    262,
                    0
                ],
                "title": "RUIE: Retrieval-based Unified Information Extraction using Large\n  Language Model",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RUIE: Retrieval-based Unified Information Extraction using Large\n  Language Model"
                },
                "summary": "Unified information extraction (UIE) aims to complete all information\nextraction tasks using a single model or framework. While previous work has\nprimarily focused on instruction-tuning large language models (LLMs) with\nconstructed datasets, these methods require significant computational resources\nand struggle to generalize to unseen tasks. To address these limitations, we\npropose RUIE (Retrieval-based Unified Information Extraction), a framework that\nleverages in-context learning to enable rapid generalization while reducing\ncomputational costs. The key challenge in RUIE is selecting the most beneficial\ndemonstrations for LLMs to effectively handle diverse IE tasks. To achieve\nthis, we integrate LLM preferences for ranking candidate demonstrations and\ndesign a keyword-enhanced reward model to capture fine-grained relationships\nbetween queries and demonstrations. We then train a bi-encoder retriever for\nUIE through contrastive learning and knowledge distillation. To the best of our\nknowledge, RUIE is the first trainable retrieval framework for UIE.\nExperimental results on 8 held-out datasets demonstrate RUIE's effectiveness in\ngeneralizing to unseen tasks, with average F1-score improvements of 19.22 and\n3.13 compared to instruction-tuning methods and other retrievers, respectively.\nFurther analysis confirms RUIE's adaptability to LLMs of varying sizes and the\nimportance of its key components.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Unified information extraction (UIE) aims to complete all information\nextraction tasks using a single model or framework. While previous work has\nprimarily focused on instruction-tuning large language models (LLMs) with\nconstructed datasets, these methods require significant computational resources\nand struggle to generalize to unseen tasks. To address these limitations, we\npropose RUIE (Retrieval-based Unified Information Extraction), a framework that\nleverages in-context learning to enable rapid generalization while reducing\ncomputational costs. The key challenge in RUIE is selecting the most beneficial\ndemonstrations for LLMs to effectively handle diverse IE tasks. To achieve\nthis, we integrate LLM preferences for ranking candidate demonstrations and\ndesign a keyword-enhanced reward model to capture fine-grained relationships\nbetween queries and demonstrations. We then train a bi-encoder retriever for\nUIE through contrastive learning and knowledge distillation. To the best of our\nknowledge, RUIE is the first trainable retrieval framework for UIE.\nExperimental results on 8 held-out datasets demonstrate RUIE's effectiveness in\ngeneralizing to unseen tasks, with average F1-score improvements of 19.22 and\n3.13 compared to instruction-tuning methods and other retrievers, respectively.\nFurther analysis confirms RUIE's adaptability to LLMs of varying sizes and the\nimportance of its key components."
                },
                "authors": [
                    {
                        "name": "Xincheng Liao"
                    },
                    {
                        "name": "Junwen Duan"
                    },
                    {
                        "name": "Yixi Huang"
                    },
                    {
                        "name": "Jianxin Wang"
                    }
                ],
                "author_detail": {
                    "name": "Jianxin Wang"
                },
                "author": "Jianxin Wang",
                "arxiv_comment": "14 pages, 3 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.11673v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.11673v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2302.10163v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2302.10163v4",
                "updated": "2024-09-22T03:24:50Z",
                "updated_parsed": [
                    2024,
                    9,
                    22,
                    3,
                    24,
                    50,
                    6,
                    266,
                    0
                ],
                "published": "2023-02-20T18:49:34Z",
                "published_parsed": [
                    2023,
                    2,
                    20,
                    18,
                    49,
                    34,
                    0,
                    51,
                    0
                ],
                "title": "Learning temporal relationships between symbols with Laplace Neural\n  Manifolds",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Learning temporal relationships between symbols with Laplace Neural\n  Manifolds"
                },
                "summary": "Firing across populations of neurons in many regions of the mammalian brain\nmaintains a temporal memory, a neural timeline of the recent past. Behavioral\nresults demonstrate that people can both remember the past and anticipate the\nfuture over an analogous internal timeline. This paper presents a mathematical\nframework for building this timeline of the future. We assume that the input to\nthe system is a time series of symbols--sparse tokenized representations of the\npresent--in continuous time. The goal is to record pairwise temporal\nrelationships between symbols over a wide range of time scales. We assume that\nthe brain has access to a temporal memory in the form of the real Laplace\ntransform. Hebbian associations with a diversity of synaptic time scales are\nformed between the past timeline and the present symbol. The associative memory\nstores the convolution between the past and the present. Knowing the temporal\nrelationship between the past and the present allows one to infer relationships\nbetween the present and the future. With appropriate normalization, this\nHebbian associative matrix can store a Laplace successor representation and a\nLaplace predecessor representation from which measures of temporal contingency\ncan be evaluated. The diversity of synaptic time constants allows for learning\nof non-stationary statistics as well as joint statistics between triplets of\nsymbols. This framework synthesizes a number of recent neuroscientific findings\nincluding results from dopamine neurons in the mesolimbic forebrain.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Firing across populations of neurons in many regions of the mammalian brain\nmaintains a temporal memory, a neural timeline of the recent past. Behavioral\nresults demonstrate that people can both remember the past and anticipate the\nfuture over an analogous internal timeline. This paper presents a mathematical\nframework for building this timeline of the future. We assume that the input to\nthe system is a time series of symbols--sparse tokenized representations of the\npresent--in continuous time. The goal is to record pairwise temporal\nrelationships between symbols over a wide range of time scales. We assume that\nthe brain has access to a temporal memory in the form of the real Laplace\ntransform. Hebbian associations with a diversity of synaptic time scales are\nformed between the past timeline and the present symbol. The associative memory\nstores the convolution between the past and the present. Knowing the temporal\nrelationship between the past and the present allows one to infer relationships\nbetween the present and the future. With appropriate normalization, this\nHebbian associative matrix can store a Laplace successor representation and a\nLaplace predecessor representation from which measures of temporal contingency\ncan be evaluated. The diversity of synaptic time constants allows for learning\nof non-stationary statistics as well as joint statistics between triplets of\nsymbols. This framework synthesizes a number of recent neuroscientific findings\nincluding results from dopamine neurons in the mesolimbic forebrain."
                },
                "authors": [
                    {
                        "name": "Marc W. Howard"
                    },
                    {
                        "name": "Zahra G. Esfahani"
                    },
                    {
                        "name": "Bao Le"
                    },
                    {
                        "name": "Per B. Sederberg"
                    }
                ],
                "author_detail": {
                    "name": "Per B. Sederberg"
                },
                "author": "Per B. Sederberg",
                "arxiv_comment": "Fixed a bunch of typos. This is the final version of this paper on\n  arXiv",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2302.10163v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2302.10163v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "q-bio.NC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "q-bio.NC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.09044v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.09044v2",
                "updated": "2024-09-18T02:57:12Z",
                "updated_parsed": [
                    2024,
                    9,
                    18,
                    2,
                    57,
                    12,
                    2,
                    262,
                    0
                ],
                "published": "2024-06-13T12:30:02Z",
                "published_parsed": [
                    2024,
                    6,
                    13,
                    12,
                    30,
                    2,
                    3,
                    165,
                    0
                ],
                "title": "MiLoRA: Harnessing Minor Singular Components for Parameter-Efficient LLM\n  Finetuning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MiLoRA: Harnessing Minor Singular Components for Parameter-Efficient LLM\n  Finetuning"
                },
                "summary": "Efficient finetuning of large language models (LLMs) aims to adapt the LLMs\nwith reduced computational and memory cost. Previous LoRA-based approaches\ninitialize the low-rank matrices with Gaussian distribution and zero values\nwhile keeping the original weight matrices frozen. However, the trainable model\nparameters optimized in an unguided subspace might interfere with the\nwell-learned subspace of the pretrained weight matrices. In this paper, we\npropose MiLoRA, a simple yet effective LLM finetuning approach that only\nupdates the minor singular components of the weight matrix while keeping the\nprincipal singular components frozen. It is observed that the minor matrix\ncorresponds to the noisy or long-tail information, while the principal matrix\ncontains important knowledge. The MiLoRA initializes the low-rank matrices\nwithin a subspace that is orthogonal to the principal matrix, thus the\npretrained knowledge is expected to be well preserved. During finetuning,\nMiLoRA makes the most use of the less-optimized subspace for learning the\nlabeled dataset. Extensive experiments on commonsense reasoning, math\nreasoning, instruction following and visual instruction following benchmarks\npresent the superior performance of our method.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient finetuning of large language models (LLMs) aims to adapt the LLMs\nwith reduced computational and memory cost. Previous LoRA-based approaches\ninitialize the low-rank matrices with Gaussian distribution and zero values\nwhile keeping the original weight matrices frozen. However, the trainable model\nparameters optimized in an unguided subspace might interfere with the\nwell-learned subspace of the pretrained weight matrices. In this paper, we\npropose MiLoRA, a simple yet effective LLM finetuning approach that only\nupdates the minor singular components of the weight matrix while keeping the\nprincipal singular components frozen. It is observed that the minor matrix\ncorresponds to the noisy or long-tail information, while the principal matrix\ncontains important knowledge. The MiLoRA initializes the low-rank matrices\nwithin a subspace that is orthogonal to the principal matrix, thus the\npretrained knowledge is expected to be well preserved. During finetuning,\nMiLoRA makes the most use of the less-optimized subspace for learning the\nlabeled dataset. Extensive experiments on commonsense reasoning, math\nreasoning, instruction following and visual instruction following benchmarks\npresent the superior performance of our method."
                },
                "authors": [
                    {
                        "name": "Hanqing Wang"
                    },
                    {
                        "name": "Yixia Li"
                    },
                    {
                        "name": "Shuo Wang"
                    },
                    {
                        "name": "Guanhua Chen"
                    },
                    {
                        "name": "Yun Chen"
                    }
                ],
                "author_detail": {
                    "name": "Yun Chen"
                },
                "author": "Yun Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.09044v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.09044v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.11650v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.11650v1",
                "updated": "2024-09-18T02:35:00Z",
                "updated_parsed": [
                    2024,
                    9,
                    18,
                    2,
                    35,
                    0,
                    2,
                    262,
                    0
                ],
                "published": "2024-09-18T02:35:00Z",
                "published_parsed": [
                    2024,
                    9,
                    18,
                    2,
                    35,
                    0,
                    2,
                    262,
                    0
                ],
                "title": "Art and Science of Quantizing Large-Scale Models: A Comprehensive\n  Overview",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Art and Science of Quantizing Large-Scale Models: A Comprehensive\n  Overview"
                },
                "summary": "This paper provides a comprehensive overview of the principles, challenges,\nand methodologies associated with quantizing large-scale neural network models.\nAs neural networks have evolved towards larger and more complex architectures\nto address increasingly sophisticated tasks, the computational and energy costs\nhave escalated significantly. We explore the necessity and impact of model size\ngrowth, highlighting the performance benefits as well as the computational\nchallenges and environmental considerations. The core focus is on model\nquantization as a fundamental approach to mitigate these challenges by reducing\nmodel size and improving efficiency without substantially compromising\naccuracy. We delve into various quantization techniques, including both\npost-training quantization (PTQ) and quantization-aware training (QAT), and\nanalyze several state-of-the-art algorithms such as LLM-QAT, PEQA(L4Q),\nZeroQuant, SmoothQuant, and others. Through comparative analysis, we examine\nhow these methods address issues like outliers, importance weighting, and\nactivation quantization, ultimately contributing to more sustainable and\naccessible deployment of large-scale models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper provides a comprehensive overview of the principles, challenges,\nand methodologies associated with quantizing large-scale neural network models.\nAs neural networks have evolved towards larger and more complex architectures\nto address increasingly sophisticated tasks, the computational and energy costs\nhave escalated significantly. We explore the necessity and impact of model size\ngrowth, highlighting the performance benefits as well as the computational\nchallenges and environmental considerations. The core focus is on model\nquantization as a fundamental approach to mitigate these challenges by reducing\nmodel size and improving efficiency without substantially compromising\naccuracy. We delve into various quantization techniques, including both\npost-training quantization (PTQ) and quantization-aware training (QAT), and\nanalyze several state-of-the-art algorithms such as LLM-QAT, PEQA(L4Q),\nZeroQuant, SmoothQuant, and others. Through comparative analysis, we examine\nhow these methods address issues like outliers, importance weighting, and\nactivation quantization, ultimately contributing to more sustainable and\naccessible deployment of large-scale models."
                },
                "authors": [
                    {
                        "name": "Yanshu Wang"
                    },
                    {
                        "name": "Tong Yang"
                    },
                    {
                        "name": "Xiyan Liang"
                    },
                    {
                        "name": "Guoan Wang"
                    },
                    {
                        "name": "Hanning Lu"
                    },
                    {
                        "name": "Xu Zhe"
                    },
                    {
                        "name": "Yaoming Li"
                    },
                    {
                        "name": "Li Weitao"
                    }
                ],
                "author_detail": {
                    "name": "Li Weitao"
                },
                "author": "Li Weitao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.11650v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.11650v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.02657v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.02657v3",
                "updated": "2024-09-18T02:31:12Z",
                "updated_parsed": [
                    2024,
                    9,
                    18,
                    2,
                    31,
                    12,
                    2,
                    262,
                    0
                ],
                "published": "2024-04-03T11:40:17Z",
                "published_parsed": [
                    2024,
                    4,
                    3,
                    11,
                    40,
                    17,
                    2,
                    94,
                    0
                ],
                "title": "Rethinking Kullback-Leibler Divergence in Knowledge Distillation for\n  Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Rethinking Kullback-Leibler Divergence in Knowledge Distillation for\n  Large Language Models"
                },
                "summary": "Kullback-Leiber divergence has been widely used in Knowledge Distillation\n(KD) to compress Large Language Models (LLMs). Contrary to prior assertions\nthat reverse Kullback-Leibler (RKL) divergence is mode-seeking and thus\npreferable over the mean-seeking forward Kullback-Leibler (FKL) divergence,\nthis study empirically and theoretically demonstrates that neither mode-seeking\nnor mean-seeking properties manifest in KD for LLMs. Instead, RKL and FKL are\nfound to share the same optimization objective and both converge after a\nsufficient number of epochs. However, due to practical constraints, LLMs are\nseldom trained for such an extensive number of epochs. Meanwhile, we further\nfind that RKL focuses on the tail part of the distributions, while FKL focuses\non the head part at the beginning epochs. Consequently, we propose a simple yet\neffective Adaptive Kullback-Leiber (AKL) divergence method, which adaptively\nallocates weights to combine FKL and RKL. Metric-based and GPT-4-based\nevaluations demonstrate that the proposed AKL outperforms the baselines across\nvarious tasks and improves the diversity and quality of generated responses.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Kullback-Leiber divergence has been widely used in Knowledge Distillation\n(KD) to compress Large Language Models (LLMs). Contrary to prior assertions\nthat reverse Kullback-Leibler (RKL) divergence is mode-seeking and thus\npreferable over the mean-seeking forward Kullback-Leibler (FKL) divergence,\nthis study empirically and theoretically demonstrates that neither mode-seeking\nnor mean-seeking properties manifest in KD for LLMs. Instead, RKL and FKL are\nfound to share the same optimization objective and both converge after a\nsufficient number of epochs. However, due to practical constraints, LLMs are\nseldom trained for such an extensive number of epochs. Meanwhile, we further\nfind that RKL focuses on the tail part of the distributions, while FKL focuses\non the head part at the beginning epochs. Consequently, we propose a simple yet\neffective Adaptive Kullback-Leiber (AKL) divergence method, which adaptively\nallocates weights to combine FKL and RKL. Metric-based and GPT-4-based\nevaluations demonstrate that the proposed AKL outperforms the baselines across\nvarious tasks and improves the diversity and quality of generated responses."
                },
                "authors": [
                    {
                        "name": "Taiqiang Wu"
                    },
                    {
                        "name": "Chaofan Tao"
                    },
                    {
                        "name": "Jiahao Wang"
                    },
                    {
                        "name": "Runming Yang"
                    },
                    {
                        "name": "Zhe Zhao"
                    },
                    {
                        "name": "Ngai Wong"
                    }
                ],
                "author_detail": {
                    "name": "Ngai Wong"
                },
                "author": "Ngai Wong",
                "arxiv_comment": "working in progress, code available at\n  https://github.com/wutaiqiang/LLM_KD_AKL",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.02657v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.02657v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.11643v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.11643v1",
                "updated": "2024-09-18T02:14:30Z",
                "updated_parsed": [
                    2024,
                    9,
                    18,
                    2,
                    14,
                    30,
                    2,
                    262,
                    0
                ],
                "published": "2024-09-18T02:14:30Z",
                "published_parsed": [
                    2024,
                    9,
                    18,
                    2,
                    14,
                    30,
                    2,
                    262,
                    0
                ],
                "title": "Combating Phone Scams with LLM-based Detection: Where Do We Stand?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Combating Phone Scams with LLM-based Detection: Where Do We Stand?"
                },
                "summary": "Phone scams pose a significant threat to individuals and communities, causing\nsubstantial financial losses and emotional distress. Despite ongoing efforts to\ncombat these scams, scammers continue to adapt and refine their tactics, making\nit imperative to explore innovative countermeasures. This research explores the\npotential of large language models (LLMs) to provide detection of fraudulent\nphone calls. By analyzing the conversational dynamics between scammers and\nvictims, LLM-based detectors can identify potential scams as they occur,\noffering immediate protection to users. While such approaches demonstrate\npromising results, we also acknowledge the challenges of biased datasets,\nrelatively low recall, and hallucinations that must be addressed for further\nadvancement in this field",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Phone scams pose a significant threat to individuals and communities, causing\nsubstantial financial losses and emotional distress. Despite ongoing efforts to\ncombat these scams, scammers continue to adapt and refine their tactics, making\nit imperative to explore innovative countermeasures. This research explores the\npotential of large language models (LLMs) to provide detection of fraudulent\nphone calls. By analyzing the conversational dynamics between scammers and\nvictims, LLM-based detectors can identify potential scams as they occur,\noffering immediate protection to users. While such approaches demonstrate\npromising results, we also acknowledge the challenges of biased datasets,\nrelatively low recall, and hallucinations that must be addressed for further\nadvancement in this field"
                },
                "authors": [
                    {
                        "name": "Zitong Shen"
                    },
                    {
                        "name": "Kangzhong Wang"
                    },
                    {
                        "name": "Youqian Zhang"
                    },
                    {
                        "name": "Grace Ngai"
                    },
                    {
                        "name": "Eugene Y. Fu"
                    }
                ],
                "author_detail": {
                    "name": "Eugene Y. Fu"
                },
                "author": "Eugene Y. Fu",
                "arxiv_comment": "2 pages, 1 figure",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.11643v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.11643v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.0",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2401.15284v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2401.15284v4",
                "updated": "2024-09-18T02:03:35Z",
                "updated_parsed": [
                    2024,
                    9,
                    18,
                    2,
                    3,
                    35,
                    2,
                    262,
                    0
                ],
                "published": "2024-01-27T03:53:25Z",
                "published_parsed": [
                    2024,
                    1,
                    27,
                    3,
                    53,
                    25,
                    5,
                    27,
                    0
                ],
                "title": "Beyond principlism: Practical strategies for ethical AI use in research\n  practices",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Beyond principlism: Practical strategies for ethical AI use in research\n  practices"
                },
                "summary": "The rapid adoption of generative artificial intelligence (AI) in scientific\nresearch, particularly large language models (LLMs), has outpaced the\ndevelopment of ethical guidelines, leading to a Triple-Too problem: too many\nhigh-level ethical initiatives, too abstract principles lacking contextual and\npractical relevance, and too much focus on restrictions and risks over benefits\nand utilities. Existing approaches, including principlism (reliance on abstract\nethical principles), formalism (rigid application of rules), and technical\nsolutionism (overemphasis on technological fixes), offer little practical\nguidance for addressing ethical challenges of AI in scientific research\npractices. To bridge the gap between abstract principles and day-to-day\nresearch practices, a user-centered, realism-inspired approach is proposed\nhere. It outlines five specific goals for ethical AI use: 1) understanding\nmodel training and output, including bias mitigation strategies; 2) respecting\nprivacy, confidentiality, and copyright; 3) avoiding plagiarism and policy\nviolations; 4) applying AI beneficially compared to alternatives; and 5) using\nAI transparently and reproducibly. Each goal is accompanied by actionable\nstrategies and realistic cases of misuse and corrective measures. I argue that\nethical AI application requires evaluating its utility against existing\nalternatives rather than isolated performance metrics. Additionally, I propose\ndocumentation guidelines to enhance transparency and reproducibility in\nAI-assisted research. Moving forward, we need targeted professional\ndevelopment, training programs, and balanced enforcement mechanisms to promote\nresponsible AI use while fostering innovation. By refining these ethical\nguidelines and adapting them to emerging AI capabilities, we can accelerate\nscientific progress without compromising research integrity.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rapid adoption of generative artificial intelligence (AI) in scientific\nresearch, particularly large language models (LLMs), has outpaced the\ndevelopment of ethical guidelines, leading to a Triple-Too problem: too many\nhigh-level ethical initiatives, too abstract principles lacking contextual and\npractical relevance, and too much focus on restrictions and risks over benefits\nand utilities. Existing approaches, including principlism (reliance on abstract\nethical principles), formalism (rigid application of rules), and technical\nsolutionism (overemphasis on technological fixes), offer little practical\nguidance for addressing ethical challenges of AI in scientific research\npractices. To bridge the gap between abstract principles and day-to-day\nresearch practices, a user-centered, realism-inspired approach is proposed\nhere. It outlines five specific goals for ethical AI use: 1) understanding\nmodel training and output, including bias mitigation strategies; 2) respecting\nprivacy, confidentiality, and copyright; 3) avoiding plagiarism and policy\nviolations; 4) applying AI beneficially compared to alternatives; and 5) using\nAI transparently and reproducibly. Each goal is accompanied by actionable\nstrategies and realistic cases of misuse and corrective measures. I argue that\nethical AI application requires evaluating its utility against existing\nalternatives rather than isolated performance metrics. Additionally, I propose\ndocumentation guidelines to enhance transparency and reproducibility in\nAI-assisted research. Moving forward, we need targeted professional\ndevelopment, training programs, and balanced enforcement mechanisms to promote\nresponsible AI use while fostering innovation. By refining these ethical\nguidelines and adapting them to emerging AI capabilities, we can accelerate\nscientific progress without compromising research integrity."
                },
                "authors": [
                    {
                        "name": "Zhicheng Lin"
                    }
                ],
                "author_detail": {
                    "name": "Zhicheng Lin"
                },
                "author": "Zhicheng Lin",
                "arxiv_comment": "Accepted in: AI and Ethics. 20 pages, 1 figure, 3 tables, 2 boxes",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2401.15284v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2401.15284v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.11638v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.11638v1",
                "updated": "2024-09-18T02:02:30Z",
                "updated_parsed": [
                    2024,
                    9,
                    18,
                    2,
                    2,
                    30,
                    2,
                    262,
                    0
                ],
                "published": "2024-09-18T02:02:30Z",
                "published_parsed": [
                    2024,
                    9,
                    18,
                    2,
                    2,
                    30,
                    2,
                    262,
                    0
                ],
                "title": "BanStereoSet: A Dataset to Measure Stereotypical Social Biases in LLMs\n  for Bangla",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "BanStereoSet: A Dataset to Measure Stereotypical Social Biases in LLMs\n  for Bangla"
                },
                "summary": "This study presents BanStereoSet, a dataset designed to evaluate\nstereotypical social biases in multilingual LLMs for the Bangla language. In an\neffort to extend the focus of bias research beyond English-centric datasets, we\nhave localized the content from the StereoSet, IndiBias, and Kamruzzaman et.\nal.'s datasets, producing a resource tailored to capture biases prevalent\nwithin the Bangla-speaking community. Our BanStereoSet dataset consists of\n1,194 sentences spanning 9 categories of bias: race, profession, gender,\nageism, beauty, beauty in profession, region, caste, and religion. This dataset\nnot only serves as a crucial tool for measuring bias in multilingual LLMs but\nalso facilitates the exploration of stereotypical bias across different social\ncategories, potentially guiding the development of more equitable language\ntechnologies in Bangladeshi contexts. Our analysis of several language models\nusing this dataset indicates significant biases, reinforcing the necessity for\nculturally and linguistically adapted datasets to develop more equitable\nlanguage technologies.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This study presents BanStereoSet, a dataset designed to evaluate\nstereotypical social biases in multilingual LLMs for the Bangla language. In an\neffort to extend the focus of bias research beyond English-centric datasets, we\nhave localized the content from the StereoSet, IndiBias, and Kamruzzaman et.\nal.'s datasets, producing a resource tailored to capture biases prevalent\nwithin the Bangla-speaking community. Our BanStereoSet dataset consists of\n1,194 sentences spanning 9 categories of bias: race, profession, gender,\nageism, beauty, beauty in profession, region, caste, and religion. This dataset\nnot only serves as a crucial tool for measuring bias in multilingual LLMs but\nalso facilitates the exploration of stereotypical bias across different social\ncategories, potentially guiding the development of more equitable language\ntechnologies in Bangladeshi contexts. Our analysis of several language models\nusing this dataset indicates significant biases, reinforcing the necessity for\nculturally and linguistically adapted datasets to develop more equitable\nlanguage technologies."
                },
                "authors": [
                    {
                        "name": "Mahammed Kamruzzaman"
                    },
                    {
                        "name": "Abdullah Al Monsur"
                    },
                    {
                        "name": "Shrabon Das"
                    },
                    {
                        "name": "Enamul Hassan"
                    },
                    {
                        "name": "Gene Louis Kim"
                    }
                ],
                "author_detail": {
                    "name": "Gene Louis Kim"
                },
                "author": "Gene Louis Kim",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.11638v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.11638v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.11636v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.11636v1",
                "updated": "2024-09-18T01:56:34Z",
                "updated_parsed": [
                    2024,
                    9,
                    18,
                    1,
                    56,
                    34,
                    2,
                    262,
                    0
                ],
                "published": "2024-09-18T01:56:34Z",
                "published_parsed": [
                    2024,
                    9,
                    18,
                    1,
                    56,
                    34,
                    2,
                    262,
                    0
                ],
                "title": "\"A Woman is More Culturally Knowledgeable than A Man?\": The Effect of\n  Personas on Cultural Norm Interpretation in LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "\"A Woman is More Culturally Knowledgeable than A Man?\": The Effect of\n  Personas on Cultural Norm Interpretation in LLMs"
                },
                "summary": "As the deployment of large language models (LLMs) expands, there is an\nincreasing demand for personalized LLMs. One method to personalize and guide\nthe outputs of these models is by assigning a persona -- a role that describes\nthe expected behavior of the LLM (e.g., a man, a woman, an engineer). This\nstudy investigates whether an LLM's understanding of social norms varies across\nassigned personas. Ideally, the perception of a social norm should remain\nconsistent regardless of the persona, since acceptability of a social norm\nshould be determined by the region the norm originates from, rather than by\nindividual characteristics such as gender, body size, or race. A norm is\nuniversal within its cultural context. In our research, we tested 36 distinct\npersonas from 12 sociodemographic categories (e.g., age, gender, beauty) across\nfour different LLMs. We find that LLMs' cultural norm interpretation varies\nbased on the persona used and the norm interpretation also varies within a\nsociodemographic category (e.g., a fat person and a thin person as in physical\nappearance group) where an LLM with the more socially desirable persona (e.g.,\na thin person) interprets social norms more accurately than with the less\nsocially desirable persona (e.g., a fat person). We also discuss how different\ntypes of social biases may contribute to the results that we observe.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As the deployment of large language models (LLMs) expands, there is an\nincreasing demand for personalized LLMs. One method to personalize and guide\nthe outputs of these models is by assigning a persona -- a role that describes\nthe expected behavior of the LLM (e.g., a man, a woman, an engineer). This\nstudy investigates whether an LLM's understanding of social norms varies across\nassigned personas. Ideally, the perception of a social norm should remain\nconsistent regardless of the persona, since acceptability of a social norm\nshould be determined by the region the norm originates from, rather than by\nindividual characteristics such as gender, body size, or race. A norm is\nuniversal within its cultural context. In our research, we tested 36 distinct\npersonas from 12 sociodemographic categories (e.g., age, gender, beauty) across\nfour different LLMs. We find that LLMs' cultural norm interpretation varies\nbased on the persona used and the norm interpretation also varies within a\nsociodemographic category (e.g., a fat person and a thin person as in physical\nappearance group) where an LLM with the more socially desirable persona (e.g.,\na thin person) interprets social norms more accurately than with the less\nsocially desirable persona (e.g., a fat person). We also discuss how different\ntypes of social biases may contribute to the results that we observe."
                },
                "authors": [
                    {
                        "name": "Mahammed Kamruzzaman"
                    },
                    {
                        "name": "Hieu Nguyen"
                    },
                    {
                        "name": "Nazmul Hassan"
                    },
                    {
                        "name": "Gene Louis Kim"
                    }
                ],
                "author_detail": {
                    "name": "Gene Louis Kim"
                },
                "author": "Gene Louis Kim",
                "arxiv_comment": "Preprint, Under Review",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.11636v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.11636v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.01521v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.01521v2",
                "updated": "2024-09-18T01:54:39Z",
                "updated_parsed": [
                    2024,
                    9,
                    18,
                    1,
                    54,
                    39,
                    2,
                    262,
                    0
                ],
                "published": "2024-09-03T01:30:20Z",
                "published_parsed": [
                    2024,
                    9,
                    3,
                    1,
                    30,
                    20,
                    1,
                    247,
                    0
                ],
                "title": "Modelling Volatility of Spatio-temporal Integer-valued Data with Network\n  Structure and Asymmetry",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Modelling Volatility of Spatio-temporal Integer-valued Data with Network\n  Structure and Asymmetry"
                },
                "summary": "This paper proposes a spatial threshold GARCH-type model for dynamic\nspatio-temporal integer-valued data with network structure. The proposed model\ncan simplify the parameterization by using network structure in data, and can\ncapture the asymmetric property in dynamic volatility by adopting a threshold\nstructure. The proposed model assumes the conditional distribution is Poisson\ndistribution. Asymptotic theory of maximum likelihood estimation (MLE) for the\nspatial model is derived when both sample size and network dimension are large.\nWe obtain asymptotic statistical inferences via investigation of the weak\ndependence of components of the model and application of limit theorems for\nweakly dependent random fields. Simulation studies and a real data example are\npresented to support our methodology.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper proposes a spatial threshold GARCH-type model for dynamic\nspatio-temporal integer-valued data with network structure. The proposed model\ncan simplify the parameterization by using network structure in data, and can\ncapture the asymmetric property in dynamic volatility by adopting a threshold\nstructure. The proposed model assumes the conditional distribution is Poisson\ndistribution. Asymptotic theory of maximum likelihood estimation (MLE) for the\nspatial model is derived when both sample size and network dimension are large.\nWe obtain asymptotic statistical inferences via investigation of the weak\ndependence of components of the model and application of limit theorems for\nweakly dependent random fields. Simulation studies and a real data example are\npresented to support our methodology."
                },
                "authors": [
                    {
                        "name": "Yue Pan"
                    },
                    {
                        "name": "Jiazhu Pan"
                    }
                ],
                "author_detail": {
                    "name": "Jiazhu Pan"
                },
                "author": "Jiazhu Pan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.01521v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.01521v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ME",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "62M10, 91B05 (Primary) 60G60, 60F05 (Secondary)",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            }
        ]
    },
    {
        "keyword": "LLM Deployment",
        "arxiv_results": [
            {
                "id": "http://arxiv.org/abs/2409.12194v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.12194v3",
                "updated": "2024-09-20T04:38:21Z",
                "updated_parsed": [
                    2024,
                    9,
                    20,
                    4,
                    38,
                    21,
                    4,
                    264,
                    0
                ],
                "published": "2024-09-18T17:59:52Z",
                "published_parsed": [
                    2024,
                    9,
                    18,
                    17,
                    59,
                    52,
                    2,
                    262,
                    0
                ],
                "title": "Gender Representation and Bias in Indian Civil Service Mock Interviews",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Gender Representation and Bias in Indian Civil Service Mock Interviews"
                },
                "summary": "This paper makes three key contributions. First, via a substantial corpus of\n51,278 interview questions sourced from 888 YouTube videos of mock interviews\nof Indian civil service candidates, we demonstrate stark gender bias in the\nbroad nature of questions asked to male and female candidates. Second, our\nexperiments with large language models show a strong presence of gender bias in\nexplanations provided by the LLMs on the gender inference task. Finally, we\npresent a novel dataset of 51,278 interview questions that can inform future\nsocial science studies.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper makes three key contributions. First, via a substantial corpus of\n51,278 interview questions sourced from 888 YouTube videos of mock interviews\nof Indian civil service candidates, we demonstrate stark gender bias in the\nbroad nature of questions asked to male and female candidates. Second, our\nexperiments with large language models show a strong presence of gender bias in\nexplanations provided by the LLMs on the gender inference task. Finally, we\npresent a novel dataset of 51,278 interview questions that can inform future\nsocial science studies."
                },
                "authors": [
                    {
                        "name": "Somonnoy Banerjee"
                    },
                    {
                        "name": "Sujan Dutta"
                    },
                    {
                        "name": "Soumyajit Datta"
                    },
                    {
                        "name": "Ashiqur R. KhudaBukhsh"
                    }
                ],
                "author_detail": {
                    "name": "Ashiqur R. KhudaBukhsh"
                },
                "author": "Ashiqur R. KhudaBukhsh",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.12194v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.12194v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.12183v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.12183v1",
                "updated": "2024-09-18T17:55:00Z",
                "updated_parsed": [
                    2024,
                    9,
                    18,
                    17,
                    55,
                    0,
                    2,
                    262,
                    0
                ],
                "published": "2024-09-18T17:55:00Z",
                "published_parsed": [
                    2024,
                    9,
                    18,
                    17,
                    55,
                    0,
                    2,
                    262,
                    0
                ],
                "title": "To CoT or not to CoT? Chain-of-thought helps mainly on math and symbolic\n  reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "To CoT or not to CoT? Chain-of-thought helps mainly on math and symbolic\n  reasoning"
                },
                "summary": "Chain-of-thought (CoT) via prompting is the de facto method for eliciting\nreasoning capabilities from large language models (LLMs). But for what kinds of\ntasks is this extra ``thinking'' really helpful? To analyze this, we conducted\na quantitative meta-analysis covering over 100 papers using CoT and ran our own\nevaluations of 20 datasets across 14 models. Our results show that CoT gives\nstrong performance benefits primarily on tasks involving math or logic, with\nmuch smaller gains on other types of tasks. On MMLU, directly generating the\nanswer without CoT leads to almost identical accuracy as CoT unless the\nquestion or model's response contains an equals sign, indicating symbolic\noperations and reasoning. Following this finding, we analyze the behavior of\nCoT on these problems by separating planning and execution and comparing\nagainst tool-augmented LLMs. Much of CoT's gain comes from improving symbolic\nexecution, but it underperforms relative to using a symbolic solver. Our\nresults indicate that CoT can be applied selectively, maintaining performance\nwhile saving inference costs. Furthermore, they suggest a need to move beyond\nprompt-based CoT to new paradigms that better leverage intermediate computation\nacross the whole range of LLM applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Chain-of-thought (CoT) via prompting is the de facto method for eliciting\nreasoning capabilities from large language models (LLMs). But for what kinds of\ntasks is this extra ``thinking'' really helpful? To analyze this, we conducted\na quantitative meta-analysis covering over 100 papers using CoT and ran our own\nevaluations of 20 datasets across 14 models. Our results show that CoT gives\nstrong performance benefits primarily on tasks involving math or logic, with\nmuch smaller gains on other types of tasks. On MMLU, directly generating the\nanswer without CoT leads to almost identical accuracy as CoT unless the\nquestion or model's response contains an equals sign, indicating symbolic\noperations and reasoning. Following this finding, we analyze the behavior of\nCoT on these problems by separating planning and execution and comparing\nagainst tool-augmented LLMs. Much of CoT's gain comes from improving symbolic\nexecution, but it underperforms relative to using a symbolic solver. Our\nresults indicate that CoT can be applied selectively, maintaining performance\nwhile saving inference costs. Furthermore, they suggest a need to move beyond\nprompt-based CoT to new paradigms that better leverage intermediate computation\nacross the whole range of LLM applications."
                },
                "authors": [
                    {
                        "name": "Zayne Sprague"
                    },
                    {
                        "name": "Fangcong Yin"
                    },
                    {
                        "name": "Juan Diego Rodriguez"
                    },
                    {
                        "name": "Dongwei Jiang"
                    },
                    {
                        "name": "Manya Wadhwa"
                    },
                    {
                        "name": "Prasann Singhal"
                    },
                    {
                        "name": "Xinyu Zhao"
                    },
                    {
                        "name": "Xi Ye"
                    },
                    {
                        "name": "Kyle Mahowald"
                    },
                    {
                        "name": "Greg Durrett"
                    }
                ],
                "author_detail": {
                    "name": "Greg Durrett"
                },
                "author": "Greg Durrett",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.12183v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.12183v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.12181v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.12181v2",
                "updated": "2024-09-23T14:39:07Z",
                "updated_parsed": [
                    2024,
                    9,
                    23,
                    14,
                    39,
                    7,
                    0,
                    267,
                    0
                ],
                "published": "2024-09-18T17:53:17Z",
                "published_parsed": [
                    2024,
                    9,
                    18,
                    17,
                    53,
                    17,
                    2,
                    262,
                    0
                ],
                "title": "A Controlled Study on Long Context Extension and Generalization in LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Controlled Study on Long Context Extension and Generalization in LLMs"
                },
                "summary": "Broad textual understanding and in-context learning require language models\nthat utilize full document contexts. Due to the implementation challenges\nassociated with directly training long-context models, many methods have been\nproposed for extending models to handle long contexts. However, owing to\ndifferences in data and model classes, it has been challenging to compare these\napproaches, leading to uncertainty as to how to evaluate long-context\nperformance and whether it differs from standard evaluation. We implement a\ncontrolled protocol for extension methods with a standardized evaluation,\nutilizing consistent base models and extension data. Our study yields several\ninsights into long-context behavior. First, we reaffirm the critical role of\nperplexity as a general-purpose performance indicator even in longer-context\ntasks. Second, we find that current approximate attention methods\nsystematically underperform across long-context tasks. Finally, we confirm that\nexact fine-tuning based methods are generally effective within the range of\ntheir extension, whereas extrapolation remains challenging. All codebases,\nmodels, and checkpoints will be made available open-source, promoting\ntransparency and facilitating further research in this critical area of AI\ndevelopment.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Broad textual understanding and in-context learning require language models\nthat utilize full document contexts. Due to the implementation challenges\nassociated with directly training long-context models, many methods have been\nproposed for extending models to handle long contexts. However, owing to\ndifferences in data and model classes, it has been challenging to compare these\napproaches, leading to uncertainty as to how to evaluate long-context\nperformance and whether it differs from standard evaluation. We implement a\ncontrolled protocol for extension methods with a standardized evaluation,\nutilizing consistent base models and extension data. Our study yields several\ninsights into long-context behavior. First, we reaffirm the critical role of\nperplexity as a general-purpose performance indicator even in longer-context\ntasks. Second, we find that current approximate attention methods\nsystematically underperform across long-context tasks. Finally, we confirm that\nexact fine-tuning based methods are generally effective within the range of\ntheir extension, whereas extrapolation remains challenging. All codebases,\nmodels, and checkpoints will be made available open-source, promoting\ntransparency and facilitating further research in this critical area of AI\ndevelopment."
                },
                "authors": [
                    {
                        "name": "Yi Lu"
                    },
                    {
                        "name": "Jing Nathan Yan"
                    },
                    {
                        "name": "Songlin Yang"
                    },
                    {
                        "name": "Justin T. Chiu"
                    },
                    {
                        "name": "Siyu Ren"
                    },
                    {
                        "name": "Fei Yuan"
                    },
                    {
                        "name": "Wenting Zhao"
                    },
                    {
                        "name": "Zhiyong Wu"
                    },
                    {
                        "name": "Alexander M. Rush"
                    }
                ],
                "author_detail": {
                    "name": "Alexander M. Rush"
                },
                "author": "Alexander M. Rush",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.12181v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.12181v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.12180v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.12180v1",
                "updated": "2024-09-18T17:52:53Z",
                "updated_parsed": [
                    2024,
                    9,
                    18,
                    17,
                    52,
                    53,
                    2,
                    262,
                    0
                ],
                "published": "2024-09-18T17:52:53Z",
                "published_parsed": [
                    2024,
                    9,
                    18,
                    17,
                    52,
                    53,
                    2,
                    262,
                    0
                ],
                "title": "Finetuning Language Models to Emit Linguistic Expressions of Uncertainty",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Finetuning Language Models to Emit Linguistic Expressions of Uncertainty"
                },
                "summary": "Large language models (LLMs) are increasingly employed in information-seeking\nand decision-making tasks. Despite their broad utility, LLMs tend to generate\ninformation that conflicts with real-world facts, and their persuasive style\ncan make these inaccuracies appear confident and convincing. As a result,\nend-users struggle to consistently align the confidence expressed by LLMs with\nthe accuracy of their predictions, often leading to either blind trust in all\noutputs or a complete disregard for their reliability. In this work, we explore\nsupervised finetuning on uncertainty-augmented predictions as a method to\ndevelop models that produce linguistic expressions of uncertainty.\nSpecifically, we measure the calibration of pre-trained models and then\nfine-tune language models to generate calibrated linguistic expressions of\nuncertainty. Through experiments on various question-answering datasets, we\ndemonstrate that LLMs are well-calibrated in assessing their predictions, and\nsupervised finetuning based on the model's own confidence leads to\nwell-calibrated expressions of uncertainty, particularly for single-claim\nanswers.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) are increasingly employed in information-seeking\nand decision-making tasks. Despite their broad utility, LLMs tend to generate\ninformation that conflicts with real-world facts, and their persuasive style\ncan make these inaccuracies appear confident and convincing. As a result,\nend-users struggle to consistently align the confidence expressed by LLMs with\nthe accuracy of their predictions, often leading to either blind trust in all\noutputs or a complete disregard for their reliability. In this work, we explore\nsupervised finetuning on uncertainty-augmented predictions as a method to\ndevelop models that produce linguistic expressions of uncertainty.\nSpecifically, we measure the calibration of pre-trained models and then\nfine-tune language models to generate calibrated linguistic expressions of\nuncertainty. Through experiments on various question-answering datasets, we\ndemonstrate that LLMs are well-calibrated in assessing their predictions, and\nsupervised finetuning based on the model's own confidence leads to\nwell-calibrated expressions of uncertainty, particularly for single-claim\nanswers."
                },
                "authors": [
                    {
                        "name": "Arslan Chaudhry"
                    },
                    {
                        "name": "Sridhar Thiagarajan"
                    },
                    {
                        "name": "Dilan Gorur"
                    }
                ],
                "author_detail": {
                    "name": "Dilan Gorur"
                },
                "author": "Dilan Gorur",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.12180v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.12180v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.09249v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.09249v2",
                "updated": "2024-09-18T17:44:08Z",
                "updated_parsed": [
                    2024,
                    9,
                    18,
                    17,
                    44,
                    8,
                    2,
                    262,
                    0
                ],
                "published": "2024-09-14T01:21:56Z",
                "published_parsed": [
                    2024,
                    9,
                    14,
                    1,
                    21,
                    56,
                    5,
                    258,
                    0
                ],
                "title": "NovAScore: A New Automated Metric for Evaluating Document Level Novelty",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "NovAScore: A New Automated Metric for Evaluating Document Level Novelty"
                },
                "summary": "The rapid expansion of online content has intensified the issue of\ninformation redundancy, underscoring the need for solutions that can identify\ngenuinely new information. Despite this challenge, the research community has\nseen a decline in focus on novelty detection, particularly with the rise of\nlarge language models (LLMs). Additionally, previous approaches have relied\nheavily on human annotation, which is time-consuming, costly, and particularly\nchallenging when annotators must compare a target document against a vast\nnumber of historical documents. In this work, we introduce NovAScore (Novelty\nEvaluation in Atomicity Score), an automated metric for evaluating\ndocument-level novelty. NovAScore aggregates the novelty and salience scores of\natomic information, providing high interpretability and a detailed analysis of\na document's novelty. With its dynamic weight adjustment scheme, NovAScore\noffers enhanced flexibility and an additional dimension to assess both the\nnovelty level and the importance of information within a document. Our\nexperiments show that NovAScore strongly correlates with human judgments of\nnovelty, achieving a 0.626 Point-Biserial correlation on the TAP-DLND 1.0\ndataset and a 0.920 Pearson correlation on an internal human-annotated dataset.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rapid expansion of online content has intensified the issue of\ninformation redundancy, underscoring the need for solutions that can identify\ngenuinely new information. Despite this challenge, the research community has\nseen a decline in focus on novelty detection, particularly with the rise of\nlarge language models (LLMs). Additionally, previous approaches have relied\nheavily on human annotation, which is time-consuming, costly, and particularly\nchallenging when annotators must compare a target document against a vast\nnumber of historical documents. In this work, we introduce NovAScore (Novelty\nEvaluation in Atomicity Score), an automated metric for evaluating\ndocument-level novelty. NovAScore aggregates the novelty and salience scores of\natomic information, providing high interpretability and a detailed analysis of\na document's novelty. With its dynamic weight adjustment scheme, NovAScore\noffers enhanced flexibility and an additional dimension to assess both the\nnovelty level and the importance of information within a document. Our\nexperiments show that NovAScore strongly correlates with human judgments of\nnovelty, achieving a 0.626 Point-Biserial correlation on the TAP-DLND 1.0\ndataset and a 0.920 Pearson correlation on an internal human-annotated dataset."
                },
                "authors": [
                    {
                        "name": "Lin Ai"
                    },
                    {
                        "name": "Ziwei Gong"
                    },
                    {
                        "name": "Harshsaiprasad Deshpande"
                    },
                    {
                        "name": "Alexander Johnson"
                    },
                    {
                        "name": "Emmy Phung"
                    },
                    {
                        "name": "Ahmad Emami"
                    },
                    {
                        "name": "Julia Hirschberg"
                    }
                ],
                "author_detail": {
                    "name": "Julia Hirschberg"
                },
                "author": "Julia Hirschberg",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.09249v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.09249v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.10289v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.10289v2",
                "updated": "2024-09-18T17:30:50Z",
                "updated_parsed": [
                    2024,
                    9,
                    18,
                    17,
                    30,
                    50,
                    2,
                    262,
                    0
                ],
                "published": "2024-09-16T13:56:17Z",
                "published_parsed": [
                    2024,
                    9,
                    16,
                    13,
                    56,
                    17,
                    0,
                    260,
                    0
                ],
                "title": "ReflectDiffu:Reflect between Emotion-intent Contagion and Mimicry for\n  Empathetic Response Generation via a RL-Diffusion Framework",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ReflectDiffu:Reflect between Emotion-intent Contagion and Mimicry for\n  Empathetic Response Generation via a RL-Diffusion Framework"
                },
                "summary": "Empathetic response generation necessitates the integration of emotional and\nintentional dynamics to foster meaningful interactions. Existing research\neither neglects the intricate interplay between emotion and intent, leading to\nsuboptimal controllability of empathy, or resorts to large language models\n(LLMs), which incur significant computational overhead. In this paper, we\nintroduce ReflectDiffu, a lightweight and comprehensive framework for\nempathetic response generation. This framework incorporates emotion contagion\nto augment emotional expressiveness and employs an emotion-reasoning mask to\npinpoint critical emotional elements. Additionally, it integrates intent\nmimicry within reinforcement learning for refinement during diffusion. By\nharnessing an intent twice reflect the mechanism of\nExploring-Sampling-Correcting, ReflectDiffu adeptly translates emotional\ndecision-making into precise intent actions, thereby addressing empathetic\nresponse misalignments stemming from emotional misrecognition. Through\nreflection, the framework maps emotional states to intents, markedly enhancing\nboth response empathy and flexibility. Comprehensive experiments reveal that\nReflectDiffu outperforms existing models regarding relevance, controllability,\nand informativeness, achieving state-of-the-art results in both automatic and\nhuman evaluations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Empathetic response generation necessitates the integration of emotional and\nintentional dynamics to foster meaningful interactions. Existing research\neither neglects the intricate interplay between emotion and intent, leading to\nsuboptimal controllability of empathy, or resorts to large language models\n(LLMs), which incur significant computational overhead. In this paper, we\nintroduce ReflectDiffu, a lightweight and comprehensive framework for\nempathetic response generation. This framework incorporates emotion contagion\nto augment emotional expressiveness and employs an emotion-reasoning mask to\npinpoint critical emotional elements. Additionally, it integrates intent\nmimicry within reinforcement learning for refinement during diffusion. By\nharnessing an intent twice reflect the mechanism of\nExploring-Sampling-Correcting, ReflectDiffu adeptly translates emotional\ndecision-making into precise intent actions, thereby addressing empathetic\nresponse misalignments stemming from emotional misrecognition. Through\nreflection, the framework maps emotional states to intents, markedly enhancing\nboth response empathy and flexibility. Comprehensive experiments reveal that\nReflectDiffu outperforms existing models regarding relevance, controllability,\nand informativeness, achieving state-of-the-art results in both automatic and\nhuman evaluations."
                },
                "authors": [
                    {
                        "name": "Jiahao Yuan"
                    },
                    {
                        "name": "Zixiang Di"
                    },
                    {
                        "name": "Zhiqing Cui"
                    },
                    {
                        "name": "Guisong Yang"
                    },
                    {
                        "name": "Usman Naseem"
                    }
                ],
                "author_detail": {
                    "name": "Usman Naseem"
                },
                "author": "Usman Naseem",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.10289v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.10289v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.12150v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.12150v1",
                "updated": "2024-09-18T17:15:06Z",
                "updated_parsed": [
                    2024,
                    9,
                    18,
                    17,
                    15,
                    6,
                    2,
                    262,
                    0
                ],
                "published": "2024-09-18T17:15:06Z",
                "published_parsed": [
                    2024,
                    9,
                    18,
                    17,
                    15,
                    6,
                    2,
                    262,
                    0
                ],
                "title": "Decoding Style: Efficient Fine-Tuning of LLMs for Image-Guided Outfit\n  Recommendation with Preference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Decoding Style: Efficient Fine-Tuning of LLMs for Image-Guided Outfit\n  Recommendation with Preference"
                },
                "summary": "Personalized outfit recommendation remains a complex challenge, demanding\nboth fashion compatibility understanding and trend awareness. This paper\npresents a novel framework that harnesses the expressive power of large\nlanguage models (LLMs) for this task, mitigating their \"black box\" and static\nnature through fine-tuning and direct feedback integration. We bridge the item\nvisual-textual gap in items descriptions by employing image captioning with a\nMultimodal Large Language Model (MLLM). This enables the LLM to extract style\nand color characteristics from human-curated fashion images, forming the basis\nfor personalized recommendations. The LLM is efficiently fine-tuned on the\nopen-source Polyvore dataset of curated fashion images, optimizing its ability\nto recommend stylish outfits. A direct preference mechanism using negative\nexamples is employed to enhance the LLM's decision-making process. This creates\na self-enhancing AI feedback loop that continuously refines recommendations in\nline with seasonal fashion trends. Our framework is evaluated on the Polyvore\ndataset, demonstrating its effectiveness in two key tasks: fill-in-the-blank,\nand complementary item retrieval. These evaluations underline the framework's\nability to generate stylish, trend-aligned outfit suggestions, continuously\nimproving through direct feedback. The evaluation results demonstrated that our\nproposed framework significantly outperforms the base LLM, creating more\ncohesive outfits. The improved performance in these tasks underscores the\nproposed framework's potential to enhance the shopping experience with accurate\nsuggestions, proving its effectiveness over the vanilla LLM based outfit\ngeneration.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Personalized outfit recommendation remains a complex challenge, demanding\nboth fashion compatibility understanding and trend awareness. This paper\npresents a novel framework that harnesses the expressive power of large\nlanguage models (LLMs) for this task, mitigating their \"black box\" and static\nnature through fine-tuning and direct feedback integration. We bridge the item\nvisual-textual gap in items descriptions by employing image captioning with a\nMultimodal Large Language Model (MLLM). This enables the LLM to extract style\nand color characteristics from human-curated fashion images, forming the basis\nfor personalized recommendations. The LLM is efficiently fine-tuned on the\nopen-source Polyvore dataset of curated fashion images, optimizing its ability\nto recommend stylish outfits. A direct preference mechanism using negative\nexamples is employed to enhance the LLM's decision-making process. This creates\na self-enhancing AI feedback loop that continuously refines recommendations in\nline with seasonal fashion trends. Our framework is evaluated on the Polyvore\ndataset, demonstrating its effectiveness in two key tasks: fill-in-the-blank,\nand complementary item retrieval. These evaluations underline the framework's\nability to generate stylish, trend-aligned outfit suggestions, continuously\nimproving through direct feedback. The evaluation results demonstrated that our\nproposed framework significantly outperforms the base LLM, creating more\ncohesive outfits. The improved performance in these tasks underscores the\nproposed framework's potential to enhance the shopping experience with accurate\nsuggestions, proving its effectiveness over the vanilla LLM based outfit\ngeneration."
                },
                "authors": [
                    {
                        "name": "Najmeh Forouzandehmehr"
                    },
                    {
                        "name": "Nima Farrokhsiar"
                    },
                    {
                        "name": "Ramin Giahi"
                    },
                    {
                        "name": "Evren Korpeoglu"
                    },
                    {
                        "name": "Kannan Achan"
                    }
                ],
                "author_detail": {
                    "name": "Kannan Achan"
                },
                "author": "Kannan Achan",
                "arxiv_comment": "CIKM 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.12150v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.12150v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.12147v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.12147v1",
                "updated": "2024-09-18T17:12:41Z",
                "updated_parsed": [
                    2024,
                    9,
                    18,
                    17,
                    12,
                    41,
                    2,
                    262,
                    0
                ],
                "published": "2024-09-18T17:12:41Z",
                "published_parsed": [
                    2024,
                    9,
                    18,
                    17,
                    12,
                    41,
                    2,
                    262,
                    0
                ],
                "title": "MAgICoRe: Multi-Agent, Iterative, Coarse-to-Fine Refinement for\n  Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MAgICoRe: Multi-Agent, Iterative, Coarse-to-Fine Refinement for\n  Reasoning"
                },
                "summary": "Large Language Models' (LLM) reasoning can be improved using test-time\naggregation strategies, i.e., generating multiple samples and voting among\ngenerated samples. While these improve performance, they often reach a\nsaturation point. Refinement offers an alternative by using LLM-generated\nfeedback to improve solution quality. However, refinement introduces 3 key\nchallenges: (1) Excessive refinement: Uniformly refining all instances can\nover-correct and reduce the overall performance. (2) Inability to localize and\naddress errors: LLMs have a limited ability to self-correct and struggle to\nidentify and correct their own mistakes. (3) Insufficient refinement: Deciding\nhow many iterations of refinement are needed is non-trivial, and stopping too\nsoon could leave errors unaddressed. To tackle these issues, we propose\nMAgICoRe, which avoids excessive refinement by categorizing problem difficulty\nas easy or hard, solving easy problems with coarse-grained aggregation and hard\nones with fine-grained and iterative multi-agent refinement. To improve error\nlocalization, we incorporate external step-wise reward model (RM) scores.\nMoreover, to ensure effective refinement, we employ a multi-agent loop with\nthree agents: Solver, Reviewer (which generates targeted feedback based on\nstep-wise RM scores), and the Refiner (which incorporates feedback). To ensure\nsufficient refinement, we re-evaluate updated solutions, iteratively initiating\nfurther rounds of refinement. We evaluate MAgICoRe on Llama-3-8B and GPT-3.5\nand show its effectiveness across 5 math datasets. Even one iteration of\nMAgICoRe beats Self-Consistency by 3.4%, Best-of-k by 3.2%, and Self-Refine by\n4.0% while using less than half the samples. Unlike iterative refinement with\nbaselines, MAgICoRe continues to improve with more iterations. Finally, our\nablations highlight the importance of MAgICoRe's RMs and multi-agent\ncommunication.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models' (LLM) reasoning can be improved using test-time\naggregation strategies, i.e., generating multiple samples and voting among\ngenerated samples. While these improve performance, they often reach a\nsaturation point. Refinement offers an alternative by using LLM-generated\nfeedback to improve solution quality. However, refinement introduces 3 key\nchallenges: (1) Excessive refinement: Uniformly refining all instances can\nover-correct and reduce the overall performance. (2) Inability to localize and\naddress errors: LLMs have a limited ability to self-correct and struggle to\nidentify and correct their own mistakes. (3) Insufficient refinement: Deciding\nhow many iterations of refinement are needed is non-trivial, and stopping too\nsoon could leave errors unaddressed. To tackle these issues, we propose\nMAgICoRe, which avoids excessive refinement by categorizing problem difficulty\nas easy or hard, solving easy problems with coarse-grained aggregation and hard\nones with fine-grained and iterative multi-agent refinement. To improve error\nlocalization, we incorporate external step-wise reward model (RM) scores.\nMoreover, to ensure effective refinement, we employ a multi-agent loop with\nthree agents: Solver, Reviewer (which generates targeted feedback based on\nstep-wise RM scores), and the Refiner (which incorporates feedback). To ensure\nsufficient refinement, we re-evaluate updated solutions, iteratively initiating\nfurther rounds of refinement. We evaluate MAgICoRe on Llama-3-8B and GPT-3.5\nand show its effectiveness across 5 math datasets. Even one iteration of\nMAgICoRe beats Self-Consistency by 3.4%, Best-of-k by 3.2%, and Self-Refine by\n4.0% while using less than half the samples. Unlike iterative refinement with\nbaselines, MAgICoRe continues to improve with more iterations. Finally, our\nablations highlight the importance of MAgICoRe's RMs and multi-agent\ncommunication."
                },
                "authors": [
                    {
                        "name": "Justin Chih-Yao Chen"
                    },
                    {
                        "name": "Archiki Prasad"
                    },
                    {
                        "name": "Swarnadeep Saha"
                    },
                    {
                        "name": "Elias Stengel-Eskin"
                    },
                    {
                        "name": "Mohit Bansal"
                    }
                ],
                "author_detail": {
                    "name": "Mohit Bansal"
                },
                "author": "Mohit Bansal",
                "arxiv_comment": "22 pages, code: https://github.com/dinobby/MAgICoRe",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.12147v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.12147v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2312.09548v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2312.09548v2",
                "updated": "2024-09-18T17:05:56Z",
                "updated_parsed": [
                    2024,
                    9,
                    18,
                    17,
                    5,
                    56,
                    2,
                    262,
                    0
                ],
                "published": "2023-12-15T06:00:26Z",
                "published_parsed": [
                    2023,
                    12,
                    15,
                    6,
                    0,
                    26,
                    4,
                    349,
                    0
                ],
                "title": "Integrating AI and Learning Analytics for Data-Driven Pedagogical\n  Decisions and Personalized Interventions in Education",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Integrating AI and Learning Analytics for Data-Driven Pedagogical\n  Decisions and Personalized Interventions in Education"
                },
                "summary": "This research study explores the conceptualization, development, and\ndeployment of an innovative learning analytics tool, leveraging OpenAI's GPT-4\nmodel to quantify student engagement, map learning progression, and evaluate\ndiverse instructional strategies within an educational context. By analyzing\ncritical data points such as students' stress levels, curiosity, confusion,\nagitation, topic preferences, and study methods, the tool provides a\ncomprehensive view of the learning environment. It also employs Bloom's\ntaxonomy to assess cognitive development based on student inquiries. In\naddition to technical evaluation through synthetic data, feedback from a survey\nof teaching faculty at the University of Iowa was collected to gauge perceived\nbenefits and challenges. Faculty recognized the tool's potential to enhance\ninstructional decision-making through real-time insights but expressed concerns\nabout data security and the accuracy of AI-generated insights. The study\noutlines the design, implementation, and evaluation of the tool, highlighting\nits contributions to educational outcomes, practical integration within\nlearning management systems, and future refinements needed to address privacy\nand accuracy concerns. This research underscores AI's role in shaping\npersonalized, data-driven education.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This research study explores the conceptualization, development, and\ndeployment of an innovative learning analytics tool, leveraging OpenAI's GPT-4\nmodel to quantify student engagement, map learning progression, and evaluate\ndiverse instructional strategies within an educational context. By analyzing\ncritical data points such as students' stress levels, curiosity, confusion,\nagitation, topic preferences, and study methods, the tool provides a\ncomprehensive view of the learning environment. It also employs Bloom's\ntaxonomy to assess cognitive development based on student inquiries. In\naddition to technical evaluation through synthetic data, feedback from a survey\nof teaching faculty at the University of Iowa was collected to gauge perceived\nbenefits and challenges. Faculty recognized the tool's potential to enhance\ninstructional decision-making through real-time insights but expressed concerns\nabout data security and the accuracy of AI-generated insights. The study\noutlines the design, implementation, and evaluation of the tool, highlighting\nits contributions to educational outcomes, practical integration within\nlearning management systems, and future refinements needed to address privacy\nand accuracy concerns. This research underscores AI's role in shaping\npersonalized, data-driven education."
                },
                "authors": [
                    {
                        "name": "Ramteja Sajja"
                    },
                    {
                        "name": "Yusuf Sermet"
                    },
                    {
                        "name": "David Cwiertny"
                    },
                    {
                        "name": "Ibrahim Demir"
                    }
                ],
                "author_detail": {
                    "name": "Ibrahim Demir"
                },
                "author": "Ibrahim Demir",
                "arxiv_comment": "26 pages, 11 figures, 8784 words",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2312.09548v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2312.09548v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.12140v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.12140v1",
                "updated": "2024-09-18T17:03:30Z",
                "updated_parsed": [
                    2024,
                    9,
                    18,
                    17,
                    3,
                    30,
                    2,
                    262,
                    0
                ],
                "published": "2024-09-18T17:03:30Z",
                "published_parsed": [
                    2024,
                    9,
                    18,
                    17,
                    3,
                    30,
                    2,
                    262,
                    0
                ],
                "title": "MoRAG -- Multi-Fusion Retrieval Augmented Generation for Human Motion",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MoRAG -- Multi-Fusion Retrieval Augmented Generation for Human Motion"
                },
                "summary": "We introduce MoRAG, a novel multi-part fusion based retrieval-augmented\ngeneration strategy for text-based human motion generation. The method enhances\nmotion diffusion models by leveraging additional knowledge obtained through an\nimproved motion retrieval process. By effectively prompting large language\nmodels (LLMs), we address spelling errors and rephrasing issues in motion\nretrieval. Our approach utilizes a multi-part retrieval strategy to improve the\ngeneralizability of motion retrieval across the language space. We create\ndiverse samples through the spatial composition of the retrieved motions.\nFurthermore, by utilizing low-level, part-specific motion information, we can\nconstruct motion samples for unseen text descriptions. Our experiments\ndemonstrate that our framework can serve as a plug-and-play module, improving\nthe performance of motion diffusion models. Code, pretrained models and sample\nvideos will be made available at: https://motion-rag.github.io/",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce MoRAG, a novel multi-part fusion based retrieval-augmented\ngeneration strategy for text-based human motion generation. The method enhances\nmotion diffusion models by leveraging additional knowledge obtained through an\nimproved motion retrieval process. By effectively prompting large language\nmodels (LLMs), we address spelling errors and rephrasing issues in motion\nretrieval. Our approach utilizes a multi-part retrieval strategy to improve the\ngeneralizability of motion retrieval across the language space. We create\ndiverse samples through the spatial composition of the retrieved motions.\nFurthermore, by utilizing low-level, part-specific motion information, we can\nconstruct motion samples for unseen text descriptions. Our experiments\ndemonstrate that our framework can serve as a plug-and-play module, improving\nthe performance of motion diffusion models. Code, pretrained models and sample\nvideos will be made available at: https://motion-rag.github.io/"
                },
                "authors": [
                    {
                        "name": "Kalakonda Sai Shashank"
                    },
                    {
                        "name": "Shubh Maheshwari"
                    },
                    {
                        "name": "Ravi Kiran Sarvadevabhatla"
                    }
                ],
                "author_detail": {
                    "name": "Ravi Kiran Sarvadevabhatla"
                },
                "author": "Ravi Kiran Sarvadevabhatla",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.12140v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.12140v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.12117v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.12117v1",
                "updated": "2024-09-18T16:39:10Z",
                "updated_parsed": [
                    2024,
                    9,
                    18,
                    16,
                    39,
                    10,
                    2,
                    262,
                    0
                ],
                "published": "2024-09-18T16:39:10Z",
                "published_parsed": [
                    2024,
                    9,
                    18,
                    16,
                    39,
                    10,
                    2,
                    262,
                    0
                ],
                "title": "Low Frame-rate Speech Codec: a Codec Designed for Fast High-quality\n  Speech LLM Training and Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Low Frame-rate Speech Codec: a Codec Designed for Fast High-quality\n  Speech LLM Training and Inference"
                },
                "summary": "Large language models (LLMs) have significantly advanced audio processing\nthrough audio codecs that convert audio into discrete tokens, enabling the\napplication of language modeling techniques to audio data. However, audio\ncodecs often operate at high frame rates, resulting in slow training and\ninference, especially for autoregressive models. To address this challenge, we\npresent the Low Frame-rate Speech Codec (LFSC): a neural audio codec that\nleverages finite scalar quantization and adversarial training with large speech\nlanguage models to achieve high-quality audio compression with a 1.89 kbps\nbitrate and 21.5 frames per second. We demonstrate that our novel codec can\nmake the inference of LLM-based text-to-speech models around three times faster\nwhile improving intelligibility and producing quality comparable to previous\nmodels.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have significantly advanced audio processing\nthrough audio codecs that convert audio into discrete tokens, enabling the\napplication of language modeling techniques to audio data. However, audio\ncodecs often operate at high frame rates, resulting in slow training and\ninference, especially for autoregressive models. To address this challenge, we\npresent the Low Frame-rate Speech Codec (LFSC): a neural audio codec that\nleverages finite scalar quantization and adversarial training with large speech\nlanguage models to achieve high-quality audio compression with a 1.89 kbps\nbitrate and 21.5 frames per second. We demonstrate that our novel codec can\nmake the inference of LLM-based text-to-speech models around three times faster\nwhile improving intelligibility and producing quality comparable to previous\nmodels."
                },
                "authors": [
                    {
                        "name": "Edresson Casanova"
                    },
                    {
                        "name": "Ryan Langman"
                    },
                    {
                        "name": "Paarth Neekhara"
                    },
                    {
                        "name": "Shehzeen Hussain"
                    },
                    {
                        "name": "Jason Li"
                    },
                    {
                        "name": "Subhankar Ghosh"
                    },
                    {
                        "name": "Ante Jukić"
                    },
                    {
                        "name": "Sang-gil Lee"
                    }
                ],
                "author_detail": {
                    "name": "Sang-gil Lee"
                },
                "author": "Sang-gil Lee",
                "arxiv_comment": "Submitted to ICASSP 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.12117v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.12117v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.AS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.12111v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.12111v1",
                "updated": "2024-09-18T16:30:49Z",
                "updated_parsed": [
                    2024,
                    9,
                    18,
                    16,
                    30,
                    49,
                    2,
                    262,
                    0
                ],
                "published": "2024-09-18T16:30:49Z",
                "published_parsed": [
                    2024,
                    9,
                    18,
                    16,
                    30,
                    49,
                    2,
                    262,
                    0
                ],
                "title": "Applications of Knowledge Distillation in Remote Sensing: A Survey",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Applications of Knowledge Distillation in Remote Sensing: A Survey"
                },
                "summary": "With the ever-growing complexity of models in the field of remote sensing\n(RS), there is an increasing demand for solutions that balance model accuracy\nwith computational efficiency. Knowledge distillation (KD) has emerged as a\npowerful tool to meet this need, enabling the transfer of knowledge from large,\ncomplex models to smaller, more efficient ones without significant loss in\nperformance. This review article provides an extensive examination of KD and\nits innovative applications in RS. KD, a technique developed to transfer\nknowledge from a complex, often cumbersome model (teacher) to a more compact\nand efficient model (student), has seen significant evolution and application\nacross various domains. Initially, we introduce the fundamental concepts and\nhistorical progression of KD methods. The advantages of employing KD are\nhighlighted, particularly in terms of model compression, enhanced computational\nefficiency, and improved performance, which are pivotal for practical\ndeployments in RS scenarios. The article provides a comprehensive taxonomy of\nKD techniques, where each category is critically analyzed to demonstrate the\nbreadth and depth of the alternative options, and illustrates specific case\nstudies that showcase the practical implementation of KD methods in RS tasks,\nsuch as instance segmentation and object detection. Further, the review\ndiscusses the challenges and limitations of KD in RS, including practical\nconstraints and prospective future directions, providing a comprehensive\noverview for researchers and practitioners in the field of RS. Through this\norganization, the paper not only elucidates the current state of research in KD\nbut also sets the stage for future research opportunities, thereby contributing\nsignificantly to both academic research and real-world applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the ever-growing complexity of models in the field of remote sensing\n(RS), there is an increasing demand for solutions that balance model accuracy\nwith computational efficiency. Knowledge distillation (KD) has emerged as a\npowerful tool to meet this need, enabling the transfer of knowledge from large,\ncomplex models to smaller, more efficient ones without significant loss in\nperformance. This review article provides an extensive examination of KD and\nits innovative applications in RS. KD, a technique developed to transfer\nknowledge from a complex, often cumbersome model (teacher) to a more compact\nand efficient model (student), has seen significant evolution and application\nacross various domains. Initially, we introduce the fundamental concepts and\nhistorical progression of KD methods. The advantages of employing KD are\nhighlighted, particularly in terms of model compression, enhanced computational\nefficiency, and improved performance, which are pivotal for practical\ndeployments in RS scenarios. The article provides a comprehensive taxonomy of\nKD techniques, where each category is critically analyzed to demonstrate the\nbreadth and depth of the alternative options, and illustrates specific case\nstudies that showcase the practical implementation of KD methods in RS tasks,\nsuch as instance segmentation and object detection. Further, the review\ndiscusses the challenges and limitations of KD in RS, including practical\nconstraints and prospective future directions, providing a comprehensive\noverview for researchers and practitioners in the field of RS. Through this\norganization, the paper not only elucidates the current state of research in KD\nbut also sets the stage for future research opportunities, thereby contributing\nsignificantly to both academic research and real-world applications."
                },
                "authors": [
                    {
                        "name": "Yassine Himeur"
                    },
                    {
                        "name": "Nour Aburaed"
                    },
                    {
                        "name": "Omar Elharrouss"
                    },
                    {
                        "name": "Iraklis Varlamis"
                    },
                    {
                        "name": "Shadi Atalla"
                    },
                    {
                        "name": "Wathiq Mansoor"
                    },
                    {
                        "name": "Hussain Al Ahmad"
                    }
                ],
                "author_detail": {
                    "name": "Hussain Al Ahmad"
                },
                "author": "Hussain Al Ahmad",
                "arxiv_comment": "50 pages, 11 figures and 9 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.12111v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.12111v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.12106v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.12106v1",
                "updated": "2024-09-18T16:26:22Z",
                "updated_parsed": [
                    2024,
                    9,
                    18,
                    16,
                    26,
                    22,
                    2,
                    262,
                    0
                ],
                "published": "2024-09-18T16:26:22Z",
                "published_parsed": [
                    2024,
                    9,
                    18,
                    16,
                    26,
                    22,
                    2,
                    262,
                    0
                ],
                "title": "Measuring Human and AI Values based on Generative Psychometrics with\n  Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Measuring Human and AI Values based on Generative Psychometrics with\n  Large Language Models"
                },
                "summary": "Human values and their measurement are long-standing interdisciplinary\ninquiry. Recent advances in AI have sparked renewed interest in this area, with\nlarge language models (LLMs) emerging as both tools and subjects of value\nmeasurement. This work introduces Generative Psychometrics for Values (GPV), an\nLLM-based, data-driven value measurement paradigm, theoretically grounded in\ntext-revealed selective perceptions. We begin by fine-tuning an LLM for\naccurate perception-level value measurement and verifying the capability of\nLLMs to parse texts into perceptions, forming the core of the GPV pipeline.\nApplying GPV to human-authored blogs, we demonstrate its stability, validity,\nand superiority over prior psychological tools. Then, extending GPV to LLM\nvalue measurement, we advance the current art with 1) a psychometric\nmethodology that measures LLM values based on their scalable and free-form\noutputs, enabling context-specific measurement; 2) a comparative analysis of\nmeasurement paradigms, indicating response biases of prior methods; and 3) an\nattempt to bridge LLM values and their safety, revealing the predictive power\nof different value systems and the impacts of various values on LLM safety.\nThrough interdisciplinary efforts, we aim to leverage AI for next-generation\npsychometrics and psychometrics for value-aligned AI.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Human values and their measurement are long-standing interdisciplinary\ninquiry. Recent advances in AI have sparked renewed interest in this area, with\nlarge language models (LLMs) emerging as both tools and subjects of value\nmeasurement. This work introduces Generative Psychometrics for Values (GPV), an\nLLM-based, data-driven value measurement paradigm, theoretically grounded in\ntext-revealed selective perceptions. We begin by fine-tuning an LLM for\naccurate perception-level value measurement and verifying the capability of\nLLMs to parse texts into perceptions, forming the core of the GPV pipeline.\nApplying GPV to human-authored blogs, we demonstrate its stability, validity,\nand superiority over prior psychological tools. Then, extending GPV to LLM\nvalue measurement, we advance the current art with 1) a psychometric\nmethodology that measures LLM values based on their scalable and free-form\noutputs, enabling context-specific measurement; 2) a comparative analysis of\nmeasurement paradigms, indicating response biases of prior methods; and 3) an\nattempt to bridge LLM values and their safety, revealing the predictive power\nof different value systems and the impacts of various values on LLM safety.\nThrough interdisciplinary efforts, we aim to leverage AI for next-generation\npsychometrics and psychometrics for value-aligned AI."
                },
                "authors": [
                    {
                        "name": "Haoran Ye"
                    },
                    {
                        "name": "Yuhang Xie"
                    },
                    {
                        "name": "Yuanyi Ren"
                    },
                    {
                        "name": "Hanjun Fang"
                    },
                    {
                        "name": "Xin Zhang"
                    },
                    {
                        "name": "Guojie Song"
                    }
                ],
                "author_detail": {
                    "name": "Guojie Song"
                },
                "author": "Guojie Song",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.12106v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.12106v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.15256v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.15256v3",
                "updated": "2024-09-18T16:09:40Z",
                "updated_parsed": [
                    2024,
                    9,
                    18,
                    16,
                    9,
                    40,
                    2,
                    262,
                    0
                ],
                "published": "2024-08-09T19:21:14Z",
                "published_parsed": [
                    2024,
                    8,
                    9,
                    19,
                    21,
                    14,
                    4,
                    222,
                    0
                ],
                "title": "Improving Ontology Requirements Engineering with OntoChat and\n  Participatory Prompting",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Improving Ontology Requirements Engineering with OntoChat and\n  Participatory Prompting"
                },
                "summary": "Past ontology requirements engineering (ORE) has primarily relied on manual\nmethods, such as interviews and collaborative forums, to gather user\nrequirements from domain experts, especially in large projects. Current\nOntoChat offers a framework for ORE that utilises large language models (LLMs)\nto streamline the process through four key functions: user story creation,\ncompetency question (CQ) extraction, CQ filtration and analysis, and ontology\ntesting support. In OntoChat, users are expected to prompt the chatbot to\ngenerate user stories. However, preliminary evaluations revealed that they\nstruggle to do this effectively. To address this issue, we experimented with a\nresearch method called participatory prompting, which involves\nresearcher-mediated interactions to help users without deep knowledge of LLMs\nuse the chatbot more effectively. This participatory prompting user study\nproduces pre-defined prompt templates based on user queries, focusing on\ncreating and refining personas, goals, scenarios, sample data, and data\nresources for user stories. These refined user stories will subsequently be\nconverted into CQs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Past ontology requirements engineering (ORE) has primarily relied on manual\nmethods, such as interviews and collaborative forums, to gather user\nrequirements from domain experts, especially in large projects. Current\nOntoChat offers a framework for ORE that utilises large language models (LLMs)\nto streamline the process through four key functions: user story creation,\ncompetency question (CQ) extraction, CQ filtration and analysis, and ontology\ntesting support. In OntoChat, users are expected to prompt the chatbot to\ngenerate user stories. However, preliminary evaluations revealed that they\nstruggle to do this effectively. To address this issue, we experimented with a\nresearch method called participatory prompting, which involves\nresearcher-mediated interactions to help users without deep knowledge of LLMs\nuse the chatbot more effectively. This participatory prompting user study\nproduces pre-defined prompt templates based on user queries, focusing on\ncreating and refining personas, goals, scenarios, sample data, and data\nresources for user stories. These refined user stories will subsequently be\nconverted into CQs."
                },
                "authors": [
                    {
                        "name": "Yihang Zhao"
                    },
                    {
                        "name": "Bohui Zhang"
                    },
                    {
                        "name": "Xi Hu"
                    },
                    {
                        "name": "Shuyin Ouyang"
                    },
                    {
                        "name": "Jongmo Kim"
                    },
                    {
                        "name": "Nitisha Jain"
                    },
                    {
                        "name": "Jacopo de Berardinis"
                    },
                    {
                        "name": "Albert Meroño-Peñuela"
                    },
                    {
                        "name": "Elena Simperl"
                    }
                ],
                "author_detail": {
                    "name": "Elena Simperl"
                },
                "author": "Elena Simperl",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.15256v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.15256v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.20361v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.20361v2",
                "updated": "2024-09-18T16:07:40Z",
                "updated_parsed": [
                    2024,
                    9,
                    18,
                    16,
                    7,
                    40,
                    2,
                    262,
                    0
                ],
                "published": "2024-07-29T18:21:34Z",
                "published_parsed": [
                    2024,
                    7,
                    29,
                    18,
                    21,
                    34,
                    0,
                    211,
                    0
                ],
                "title": "From ML to LLM: Evaluating the Robustness of Phishing Webpage Detection\n  Models against Adversarial Attacks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "From ML to LLM: Evaluating the Robustness of Phishing Webpage Detection\n  Models against Adversarial Attacks"
                },
                "summary": "Phishing attacks attempt to deceive users into stealing sensitive\ninformation, posing a significant cybersecurity threat. Advances in machine\nlearning (ML) and deep learning (DL) have led to the development of numerous\nphishing webpage detection solutions, but these models remain vulnerable to\nadversarial attacks. Evaluating their robustness against adversarial phishing\nwebpages is essential. Existing tools contain datasets of pre-designed phishing\nwebpages for a limited number of brands, and lack diversity in phishing\nfeatures.\n  To address these challenges, we develop PhishOracle, a tool that generates\nadversarial phishing webpages by embedding diverse phishing features into\nlegitimate webpages. We evaluate the robustness of two existing models, Stack\nmodel and Phishpedia, in classifying PhishOracle-generated adversarial phishing\nwebpages. Additionally, we study a commercial large language model, Gemini Pro\nVision, in the context of adversarial attacks. We conduct a user study to\ndetermine whether PhishOracle-generated adversarial phishing webpages deceive\nusers. Our findings reveal that many PhishOracle-generated phishing webpages\nevade current phishing webpage detection models and deceive users, but Gemini\nPro Vision is robust to the attack. We also develop the PhishOracle web app,\nallowing users to input a legitimate URL, select relevant phishing features and\ngenerate a corresponding phishing webpage. All resources are publicly available\non GitHub.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Phishing attacks attempt to deceive users into stealing sensitive\ninformation, posing a significant cybersecurity threat. Advances in machine\nlearning (ML) and deep learning (DL) have led to the development of numerous\nphishing webpage detection solutions, but these models remain vulnerable to\nadversarial attacks. Evaluating their robustness against adversarial phishing\nwebpages is essential. Existing tools contain datasets of pre-designed phishing\nwebpages for a limited number of brands, and lack diversity in phishing\nfeatures.\n  To address these challenges, we develop PhishOracle, a tool that generates\nadversarial phishing webpages by embedding diverse phishing features into\nlegitimate webpages. We evaluate the robustness of two existing models, Stack\nmodel and Phishpedia, in classifying PhishOracle-generated adversarial phishing\nwebpages. Additionally, we study a commercial large language model, Gemini Pro\nVision, in the context of adversarial attacks. We conduct a user study to\ndetermine whether PhishOracle-generated adversarial phishing webpages deceive\nusers. Our findings reveal that many PhishOracle-generated phishing webpages\nevade current phishing webpage detection models and deceive users, but Gemini\nPro Vision is robust to the attack. We also develop the PhishOracle web app,\nallowing users to input a legitimate URL, select relevant phishing features and\ngenerate a corresponding phishing webpage. All resources are publicly available\non GitHub."
                },
                "authors": [
                    {
                        "name": "Aditya Kulkarni"
                    },
                    {
                        "name": "Vivek Balachandran"
                    },
                    {
                        "name": "Dinil Mon Divakaran"
                    },
                    {
                        "name": "Tamal Das"
                    }
                ],
                "author_detail": {
                    "name": "Tamal Das"
                },
                "author": "Tamal Das",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.20361v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.20361v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.12061v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.12061v1",
                "updated": "2024-09-18T15:34:31Z",
                "updated_parsed": [
                    2024,
                    9,
                    18,
                    15,
                    34,
                    31,
                    2,
                    262,
                    0
                ],
                "published": "2024-09-18T15:34:31Z",
                "published_parsed": [
                    2024,
                    9,
                    18,
                    15,
                    34,
                    31,
                    2,
                    262,
                    0
                ],
                "title": "Generalized Robot Learning Framework",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Generalized Robot Learning Framework"
                },
                "summary": "Imitation based robot learning has recently gained significant attention in\nthe robotics field due to its theoretical potential for transferability and\ngeneralizability. However, it remains notoriously costly, both in terms of\nhardware and data collection, and deploying it in real-world environments\ndemands meticulous setup of robots and precise experimental conditions. In this\npaper, we present a low-cost robot learning framework that is both easily\nreproducible and transferable to various robots and environments. We\ndemonstrate that deployable imitation learning can be successfully applied even\nto industrial-grade robots, not just expensive collaborative robotic arms.\nFurthermore, our results show that multi-task robot learning is achievable with\nsimple network architectures and fewer demonstrations than previously thought\nnecessary. As the current evaluating method is almost subjective when it comes\nto real-world manipulation tasks, we propose Voting Positive Rate (VPR) - a\nnovel evaluation strategy that provides a more objective assessment of\nperformance. We conduct an extensive comparison of success rates across various\nself-designed tasks to validate our approach. To foster collaboration and\nsupport the robot learning community, we have open-sourced all relevant\ndatasets and model checkpoints, available at huggingface.co/ZhiChengAI.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Imitation based robot learning has recently gained significant attention in\nthe robotics field due to its theoretical potential for transferability and\ngeneralizability. However, it remains notoriously costly, both in terms of\nhardware and data collection, and deploying it in real-world environments\ndemands meticulous setup of robots and precise experimental conditions. In this\npaper, we present a low-cost robot learning framework that is both easily\nreproducible and transferable to various robots and environments. We\ndemonstrate that deployable imitation learning can be successfully applied even\nto industrial-grade robots, not just expensive collaborative robotic arms.\nFurthermore, our results show that multi-task robot learning is achievable with\nsimple network architectures and fewer demonstrations than previously thought\nnecessary. As the current evaluating method is almost subjective when it comes\nto real-world manipulation tasks, we propose Voting Positive Rate (VPR) - a\nnovel evaluation strategy that provides a more objective assessment of\nperformance. We conduct an extensive comparison of success rates across various\nself-designed tasks to validate our approach. To foster collaboration and\nsupport the robot learning community, we have open-sourced all relevant\ndatasets and model checkpoints, available at huggingface.co/ZhiChengAI."
                },
                "authors": [
                    {
                        "name": "Jiahuan Yan"
                    },
                    {
                        "name": "Zhouyang Hong"
                    },
                    {
                        "name": "Yu Zhao"
                    },
                    {
                        "name": "Yu Tian"
                    },
                    {
                        "name": "Yunxin Liu"
                    },
                    {
                        "name": "Travis Davies"
                    },
                    {
                        "name": "Luhui Hu"
                    }
                ],
                "author_detail": {
                    "name": "Luhui Hu"
                },
                "author": "Luhui Hu",
                "arxiv_comment": "6 pages, 2 figures. cs.RO",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.12061v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.12061v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2305.19472v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2305.19472v3",
                "updated": "2024-09-18T15:30:33Z",
                "updated_parsed": [
                    2024,
                    9,
                    18,
                    15,
                    30,
                    33,
                    2,
                    262,
                    0
                ],
                "published": "2023-05-31T00:55:40Z",
                "published_parsed": [
                    2023,
                    5,
                    31,
                    0,
                    55,
                    40,
                    2,
                    151,
                    0
                ],
                "title": "PlaSma: Making Small Language Models Better Procedural Knowledge Models\n  for (Counterfactual) Planning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PlaSma: Making Small Language Models Better Procedural Knowledge Models\n  for (Counterfactual) Planning"
                },
                "summary": "Procedural planning, which entails decomposing a high-level goal into a\nsequence of temporally ordered steps, is an important yet intricate task for\nmachines. It involves integrating common-sense knowledge to reason about\ncomplex and often contextualized situations, e.g. ``scheduling a doctor's\nappointment without a phone''. While current approaches show encouraging\nresults using large language models (LLMs), they are hindered by drawbacks such\nas costly API calls and reproducibility issues. In this paper, we advocate\nplanning using smaller language models. We present PlaSma, a novel two-pronged\napproach to endow small language models with procedural knowledge and\n(constrained) language planning capabilities. More concretely, we develop\nsymbolic procedural knowledge distillation to enhance the commonsense knowledge\nin small language models and an inference-time algorithm to facilitate more\nstructured and accurate reasoning. In addition, we introduce a new related\ntask, Replanning, that requires a revision of a plan to cope with a constrained\nsituation. In both the planning and replanning settings, we show that\norders-of-magnitude smaller models (770M-11B parameters) can compete and often\nsurpass their larger teacher models' capabilities. Finally, we showcase\nsuccessful application of PlaSma in an embodied environment, VirtualHome.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Procedural planning, which entails decomposing a high-level goal into a\nsequence of temporally ordered steps, is an important yet intricate task for\nmachines. It involves integrating common-sense knowledge to reason about\ncomplex and often contextualized situations, e.g. ``scheduling a doctor's\nappointment without a phone''. While current approaches show encouraging\nresults using large language models (LLMs), they are hindered by drawbacks such\nas costly API calls and reproducibility issues. In this paper, we advocate\nplanning using smaller language models. We present PlaSma, a novel two-pronged\napproach to endow small language models with procedural knowledge and\n(constrained) language planning capabilities. More concretely, we develop\nsymbolic procedural knowledge distillation to enhance the commonsense knowledge\nin small language models and an inference-time algorithm to facilitate more\nstructured and accurate reasoning. In addition, we introduce a new related\ntask, Replanning, that requires a revision of a plan to cope with a constrained\nsituation. In both the planning and replanning settings, we show that\norders-of-magnitude smaller models (770M-11B parameters) can compete and often\nsurpass their larger teacher models' capabilities. Finally, we showcase\nsuccessful application of PlaSma in an embodied environment, VirtualHome."
                },
                "authors": [
                    {
                        "name": "Faeze Brahman"
                    },
                    {
                        "name": "Chandra Bhagavatula"
                    },
                    {
                        "name": "Valentina Pyatkin"
                    },
                    {
                        "name": "Jena D. Hwang"
                    },
                    {
                        "name": "Xiang Lorraine Li"
                    },
                    {
                        "name": "Hirona J. Arai"
                    },
                    {
                        "name": "Soumya Sanyal"
                    },
                    {
                        "name": "Keisuke Sakaguchi"
                    },
                    {
                        "name": "Xiang Ren"
                    },
                    {
                        "name": "Yejin Choi"
                    }
                ],
                "author_detail": {
                    "name": "Yejin Choi"
                },
                "author": "Yejin Choi",
                "arxiv_comment": "ICLR 2024 version , 31 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2305.19472v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2305.19472v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.12055v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.12055v1",
                "updated": "2024-09-18T15:30:29Z",
                "updated_parsed": [
                    2024,
                    9,
                    18,
                    15,
                    30,
                    29,
                    2,
                    262,
                    0
                ],
                "published": "2024-09-18T15:30:29Z",
                "published_parsed": [
                    2024,
                    9,
                    18,
                    15,
                    30,
                    29,
                    2,
                    262,
                    0
                ],
                "title": "Artemis: Efficient Commit-and-Prove SNARKs for zkML",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Artemis: Efficient Commit-and-Prove SNARKs for zkML"
                },
                "summary": "The widespread adoption of machine learning (ML) in various critical\napplications, from healthcare to autonomous systems, has raised significant\nconcerns about privacy, accountability, and trustworthiness. To address these\nconcerns, recent research has focused on developing zero-knowledge machine\nlearning (zkML) techniques that enable the verification of various aspects of\nML models without revealing sensitive information. Recent advances in zkML have\nsubstantially improved efficiency; however, these efforts have primarily\noptimized the process of proving ML computations correct, often overlooking the\nsubstantial overhead associated with verifying the necessary commitments to the\nmodel and data. To address this gap, this paper introduces two new\nCommit-and-Prove SNARK (CP-SNARK) constructions (Apollo and Artemis) that\neffectively address the emerging challenge of commitment verification in zkML\npipelines. Apollo operates on KZG commitments and requires white-box use of the\nunderlying proof system, whereas Artemis is compatible with any homomorphic\npolynomial commitment and only makes black-box use of the proof system. As a\nresult, Artemis is compatible with state-of-the-art proof systems without\ntrusted setup. We present the first implementation of these CP-SNARKs, evaluate\ntheir performance on a diverse set of ML models, and show substantial\nimprovements over existing methods, achieving significant reductions in prover\ncosts and maintaining efficiency even for large-scale models. For example, for\nthe VGG model, we reduce the overhead associated with commitment checks from\n11.5x to 1.2x. Our results suggest that these contributions can move zkML\ntowards practical deployment, particularly in scenarios involving large and\ncomplex ML models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The widespread adoption of machine learning (ML) in various critical\napplications, from healthcare to autonomous systems, has raised significant\nconcerns about privacy, accountability, and trustworthiness. To address these\nconcerns, recent research has focused on developing zero-knowledge machine\nlearning (zkML) techniques that enable the verification of various aspects of\nML models without revealing sensitive information. Recent advances in zkML have\nsubstantially improved efficiency; however, these efforts have primarily\noptimized the process of proving ML computations correct, often overlooking the\nsubstantial overhead associated with verifying the necessary commitments to the\nmodel and data. To address this gap, this paper introduces two new\nCommit-and-Prove SNARK (CP-SNARK) constructions (Apollo and Artemis) that\neffectively address the emerging challenge of commitment verification in zkML\npipelines. Apollo operates on KZG commitments and requires white-box use of the\nunderlying proof system, whereas Artemis is compatible with any homomorphic\npolynomial commitment and only makes black-box use of the proof system. As a\nresult, Artemis is compatible with state-of-the-art proof systems without\ntrusted setup. We present the first implementation of these CP-SNARKs, evaluate\ntheir performance on a diverse set of ML models, and show substantial\nimprovements over existing methods, achieving significant reductions in prover\ncosts and maintaining efficiency even for large-scale models. For example, for\nthe VGG model, we reduce the overhead associated with commitment checks from\n11.5x to 1.2x. Our results suggest that these contributions can move zkML\ntowards practical deployment, particularly in scenarios involving large and\ncomplex ML models."
                },
                "authors": [
                    {
                        "name": "Hidde Lycklama"
                    },
                    {
                        "name": "Alexander Viand"
                    },
                    {
                        "name": "Nikolay Avramov"
                    },
                    {
                        "name": "Nicolas Küchler"
                    },
                    {
                        "name": "Anwar Hithnawi"
                    }
                ],
                "author_detail": {
                    "name": "Anwar Hithnawi"
                },
                "author": "Anwar Hithnawi",
                "arxiv_comment": "17 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.12055v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.12055v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.12046v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.12046v2",
                "updated": "2024-09-19T02:48:43Z",
                "updated_parsed": [
                    2024,
                    9,
                    19,
                    2,
                    48,
                    43,
                    3,
                    263,
                    0
                ],
                "published": "2024-09-18T15:16:37Z",
                "published_parsed": [
                    2024,
                    9,
                    18,
                    15,
                    16,
                    37,
                    2,
                    262,
                    0
                ],
                "title": "Using Large Language Models to Generate Clinical Trial Tables and\n  Figures",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Using Large Language Models to Generate Clinical Trial Tables and\n  Figures"
                },
                "summary": "Tables, figures, and listings (TFLs) are essential tools for summarizing\nclinical trial data. Creation of TFLs for reporting activities is often a\ntime-consuming task encountered routinely during the execution of clinical\ntrials. This study explored the use of large language models (LLMs) to automate\nthe generation of TFLs through prompt engineering and few-shot transfer\nlearning. Using public clinical trial data in ADaM format, our results\ndemonstrated that LLMs can efficiently generate TFLs with prompt instructions,\nshowcasing their potential in this domain. Furthermore, we developed a\nconservational agent named Clinical Trial TFL Generation Agent: An app that\nmatches user queries to predefined prompts that produce customized programs to\ngenerate specific predefined TFLs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Tables, figures, and listings (TFLs) are essential tools for summarizing\nclinical trial data. Creation of TFLs for reporting activities is often a\ntime-consuming task encountered routinely during the execution of clinical\ntrials. This study explored the use of large language models (LLMs) to automate\nthe generation of TFLs through prompt engineering and few-shot transfer\nlearning. Using public clinical trial data in ADaM format, our results\ndemonstrated that LLMs can efficiently generate TFLs with prompt instructions,\nshowcasing their potential in this domain. Furthermore, we developed a\nconservational agent named Clinical Trial TFL Generation Agent: An app that\nmatches user queries to predefined prompts that produce customized programs to\ngenerate specific predefined TFLs."
                },
                "authors": [
                    {
                        "name": "Yumeng Yang"
                    },
                    {
                        "name": "Peter Krusche"
                    },
                    {
                        "name": "Kristyn Pantoja"
                    },
                    {
                        "name": "Cheng Shi"
                    },
                    {
                        "name": "Ethan Ludmir"
                    },
                    {
                        "name": "Kirk Roberts"
                    },
                    {
                        "name": "Gen Zhu"
                    }
                ],
                "author_detail": {
                    "name": "Gen Zhu"
                },
                "author": "Gen Zhu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.12046v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.12046v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.12045v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.12045v2",
                "updated": "2024-09-23T12:42:32Z",
                "updated_parsed": [
                    2024,
                    9,
                    23,
                    12,
                    42,
                    32,
                    0,
                    267,
                    0
                ],
                "published": "2024-09-18T15:08:41Z",
                "published_parsed": [
                    2024,
                    9,
                    18,
                    15,
                    8,
                    41,
                    2,
                    262,
                    0
                ],
                "title": "Handling Long-Term Safety and Uncertainty in Safe Reinforcement Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Handling Long-Term Safety and Uncertainty in Safe Reinforcement Learning"
                },
                "summary": "Safety is one of the key issues preventing the deployment of reinforcement\nlearning techniques in real-world robots. While most approaches in the Safe\nReinforcement Learning area do not require prior knowledge of constraints and\nrobot kinematics and rely solely on data, it is often difficult to deploy them\nin complex real-world settings. Instead, model-based approaches that\nincorporate prior knowledge of the constraints and dynamics into the learning\nframework have proven capable of deploying the learning algorithm directly on\nthe real robot. Unfortunately, while an approximated model of the robot\ndynamics is often available, the safety constraints are task-specific and hard\nto obtain: they may be too complicated to encode analytically, too expensive to\ncompute, or it may be difficult to envision a priori the long-term safety\nrequirements. In this paper, we bridge this gap by extending the safe\nexploration method, ATACOM, with learnable constraints, with a particular focus\non ensuring long-term safety and handling of uncertainty. Our approach is\ncompetitive or superior to state-of-the-art methods in final performance while\nmaintaining safer behavior during training.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Safety is one of the key issues preventing the deployment of reinforcement\nlearning techniques in real-world robots. While most approaches in the Safe\nReinforcement Learning area do not require prior knowledge of constraints and\nrobot kinematics and rely solely on data, it is often difficult to deploy them\nin complex real-world settings. Instead, model-based approaches that\nincorporate prior knowledge of the constraints and dynamics into the learning\nframework have proven capable of deploying the learning algorithm directly on\nthe real robot. Unfortunately, while an approximated model of the robot\ndynamics is often available, the safety constraints are task-specific and hard\nto obtain: they may be too complicated to encode analytically, too expensive to\ncompute, or it may be difficult to envision a priori the long-term safety\nrequirements. In this paper, we bridge this gap by extending the safe\nexploration method, ATACOM, with learnable constraints, with a particular focus\non ensuring long-term safety and handling of uncertainty. Our approach is\ncompetitive or superior to state-of-the-art methods in final performance while\nmaintaining safer behavior during training."
                },
                "authors": [
                    {
                        "name": "Jonas Günster"
                    },
                    {
                        "name": "Puze Liu"
                    },
                    {
                        "name": "Jan Peters"
                    },
                    {
                        "name": "Davide Tateo"
                    }
                ],
                "author_detail": {
                    "name": "Davide Tateo"
                },
                "author": "Davide Tateo",
                "arxiv_comment": "Preprint version of a paper accepted to the Conference on Robot\n  Learning",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.12045v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.12045v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.12026v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.12026v1",
                "updated": "2024-09-18T14:36:50Z",
                "updated_parsed": [
                    2024,
                    9,
                    18,
                    14,
                    36,
                    50,
                    2,
                    262,
                    0
                ],
                "published": "2024-09-18T14:36:50Z",
                "published_parsed": [
                    2024,
                    9,
                    18,
                    14,
                    36,
                    50,
                    2,
                    262,
                    0
                ],
                "title": "On Vision Transformers for Classification Tasks in Side-Scan Sonar\n  Imagery",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "On Vision Transformers for Classification Tasks in Side-Scan Sonar\n  Imagery"
                },
                "summary": "Side-scan sonar (SSS) imagery presents unique challenges in the\nclassification of man-made objects on the seafloor due to the complex and\nvaried underwater environments. Historically, experts have manually interpreted\nSSS images, relying on conventional machine learning techniques with\nhand-crafted features. While Convolutional Neural Networks (CNNs) significantly\nadvanced automated classification in this domain, they often fall short when\ndealing with diverse seafloor textures, such as rocky or ripple sand bottoms,\nwhere false positive rates may increase. Recently, Vision Transformers (ViTs)\nhave shown potential in addressing these limitations by utilizing a\nself-attention mechanism to capture global information in image patches,\noffering more flexibility in processing spatial hierarchies. This paper\nrigorously compares the performance of ViT models alongside commonly used CNN\narchitectures, such as ResNet and ConvNext, for binary classification tasks in\nSSS imagery. The dataset encompasses diverse geographical seafloor types and is\nbalanced between the presence and absence of man-made objects. ViT-based models\nexhibit superior classification performance across f1-score, precision, recall,\nand accuracy metrics, although at the cost of greater computational resources.\nCNNs, with their inductive biases, demonstrate better computational efficiency,\nmaking them suitable for deployment in resource-constrained environments like\nunderwater vehicles. Future research directions include exploring\nself-supervised learning for ViTs and multi-modal fusion to further enhance\nperformance in challenging underwater environments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Side-scan sonar (SSS) imagery presents unique challenges in the\nclassification of man-made objects on the seafloor due to the complex and\nvaried underwater environments. Historically, experts have manually interpreted\nSSS images, relying on conventional machine learning techniques with\nhand-crafted features. While Convolutional Neural Networks (CNNs) significantly\nadvanced automated classification in this domain, they often fall short when\ndealing with diverse seafloor textures, such as rocky or ripple sand bottoms,\nwhere false positive rates may increase. Recently, Vision Transformers (ViTs)\nhave shown potential in addressing these limitations by utilizing a\nself-attention mechanism to capture global information in image patches,\noffering more flexibility in processing spatial hierarchies. This paper\nrigorously compares the performance of ViT models alongside commonly used CNN\narchitectures, such as ResNet and ConvNext, for binary classification tasks in\nSSS imagery. The dataset encompasses diverse geographical seafloor types and is\nbalanced between the presence and absence of man-made objects. ViT-based models\nexhibit superior classification performance across f1-score, precision, recall,\nand accuracy metrics, although at the cost of greater computational resources.\nCNNs, with their inductive biases, demonstrate better computational efficiency,\nmaking them suitable for deployment in resource-constrained environments like\nunderwater vehicles. Future research directions include exploring\nself-supervised learning for ViTs and multi-modal fusion to further enhance\nperformance in challenging underwater environments."
                },
                "authors": [
                    {
                        "name": "BW Sheffield"
                    },
                    {
                        "name": "Jeffrey Ellen"
                    },
                    {
                        "name": "Ben Whitmore"
                    }
                ],
                "author_detail": {
                    "name": "Ben Whitmore"
                },
                "author": "Ben Whitmore",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.12026v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.12026v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2308.07106v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2308.07106v2",
                "updated": "2024-09-18T14:27:41Z",
                "updated_parsed": [
                    2024,
                    9,
                    18,
                    14,
                    27,
                    41,
                    2,
                    262,
                    0
                ],
                "published": "2023-08-14T12:38:43Z",
                "published_parsed": [
                    2023,
                    8,
                    14,
                    12,
                    38,
                    43,
                    0,
                    226,
                    0
                ],
                "title": "Checklist to Define the Identification of TP, FP, and FN Object\n  Detections in Automated Driving",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Checklist to Define the Identification of TP, FP, and FN Object\n  Detections in Automated Driving"
                },
                "summary": "The object perception of automated driving systems must pass quality and\nrobustness tests before a safe deployment. Such tests typically identify true\npositive (TP), false-positive (FP), and false-negative (FN) detections and\naggregate them to metrics. Since the literature seems to be lacking a\ncomprehensive way to define the identification of TPs/FPs/FNs, this paper\nprovides a checklist of relevant functional aspects and implementation details.\nBesides labeling policies of the test set, we cover areas of vision, occlusion\nhandling, safety-relevant areas, matching criteria, temporal and probabilistic\nissues, and further aspects. Even though the checklist cannot be fully\nformalized, it can help practitioners minimize the ambiguity of their tests,\nwhich, in turn, makes statements on object perception more reliable and\ncomparable.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The object perception of automated driving systems must pass quality and\nrobustness tests before a safe deployment. Such tests typically identify true\npositive (TP), false-positive (FP), and false-negative (FN) detections and\naggregate them to metrics. Since the literature seems to be lacking a\ncomprehensive way to define the identification of TPs/FPs/FNs, this paper\nprovides a checklist of relevant functional aspects and implementation details.\nBesides labeling policies of the test set, we cover areas of vision, occlusion\nhandling, safety-relevant areas, matching criteria, temporal and probabilistic\nissues, and further aspects. Even though the checklist cannot be fully\nformalized, it can help practitioners minimize the ambiguity of their tests,\nwhich, in turn, makes statements on object perception more reliable and\ncomparable."
                },
                "authors": [
                    {
                        "name": "Michael Hoss"
                    }
                ],
                "author_detail": {
                    "name": "Michael Hoss"
                },
                "author": "Michael Hoss",
                "arxiv_comment": "This version improves the checklist's usability by providing bullet\n  points to follow. It also condenses the contributions to safety assurance\n  down to the \"Related Work\" section. 11 pages, 3 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2308.07106v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2308.07106v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.12010v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.12010v1",
                "updated": "2024-09-18T14:24:29Z",
                "updated_parsed": [
                    2024,
                    9,
                    18,
                    14,
                    24,
                    29,
                    2,
                    262,
                    0
                ],
                "published": "2024-09-18T14:24:29Z",
                "published_parsed": [
                    2024,
                    9,
                    18,
                    14,
                    24,
                    29,
                    2,
                    262,
                    0
                ],
                "title": "ChefFusion: Multimodal Foundation Model Integrating Recipe and Food\n  Image Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ChefFusion: Multimodal Foundation Model Integrating Recipe and Food\n  Image Generation"
                },
                "summary": "Significant work has been conducted in the domain of food computing, yet\nthese studies typically focus on single tasks such as t2t (instruction\ngeneration from food titles and ingredients), i2t (recipe generation from food\nimages), or t2i (food image generation from recipes). None of these approaches\nintegrate all modalities simultaneously. To address this gap, we introduce a\nnovel food computing foundation model that achieves true multimodality,\nencompassing tasks such as t2t, t2i, i2t, it2t, and t2ti. By leveraging large\nlanguage models (LLMs) and pre-trained image encoder and decoder models, our\nmodel can perform a diverse array of food computing-related tasks, including\nfood understanding, food recognition, recipe generation, and food image\ngeneration. Compared to previous models, our foundation model demonstrates a\nsignificantly broader range of capabilities and exhibits superior performance,\nparticularly in food image generation and recipe generation tasks. We\nopen-sourced ChefFusion at GitHub.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Significant work has been conducted in the domain of food computing, yet\nthese studies typically focus on single tasks such as t2t (instruction\ngeneration from food titles and ingredients), i2t (recipe generation from food\nimages), or t2i (food image generation from recipes). None of these approaches\nintegrate all modalities simultaneously. To address this gap, we introduce a\nnovel food computing foundation model that achieves true multimodality,\nencompassing tasks such as t2t, t2i, i2t, it2t, and t2ti. By leveraging large\nlanguage models (LLMs) and pre-trained image encoder and decoder models, our\nmodel can perform a diverse array of food computing-related tasks, including\nfood understanding, food recognition, recipe generation, and food image\ngeneration. Compared to previous models, our foundation model demonstrates a\nsignificantly broader range of capabilities and exhibits superior performance,\nparticularly in food image generation and recipe generation tasks. We\nopen-sourced ChefFusion at GitHub."
                },
                "authors": [
                    {
                        "name": "Peiyu Li"
                    },
                    {
                        "name": "Xiaobao Huang"
                    },
                    {
                        "name": "Yijun Tian"
                    },
                    {
                        "name": "Nitesh V. Chawla"
                    }
                ],
                "author_detail": {
                    "name": "Nitesh V. Chawla"
                },
                "author": "Nitesh V. Chawla",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.12010v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.12010v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2403.01535v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2403.01535v3",
                "updated": "2024-09-18T14:20:32Z",
                "updated_parsed": [
                    2024,
                    9,
                    18,
                    14,
                    20,
                    32,
                    2,
                    262,
                    0
                ],
                "published": "2024-03-03T15:28:47Z",
                "published_parsed": [
                    2024,
                    3,
                    3,
                    15,
                    28,
                    47,
                    6,
                    63,
                    0
                ],
                "title": "Neural Graph Generator: Feature-Conditioned Graph Generation using\n  Latent Diffusion Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Neural Graph Generator: Feature-Conditioned Graph Generation using\n  Latent Diffusion Models"
                },
                "summary": "Graph generation has emerged as a crucial task in machine learning, with\nsignificant challenges in generating graphs that accurately reflect specific\nproperties. Existing methods often fall short in efficiently addressing this\nneed as they struggle with the high-dimensional complexity and varied nature of\ngraph properties. In this paper, we introduce the Neural Graph Generator (NGG),\na novel approach which utilizes conditioned latent diffusion models for graph\ngeneration. NGG demonstrates a remarkable capacity to model complex graph\npatterns, offering control over the graph generation process. NGG employs a\nvariational graph autoencoder for graph compression and a diffusion process in\nthe latent vector space, guided by vectors summarizing graph statistics. We\ndemonstrate NGG's versatility across various graph generation tasks, showing\nits capability to capture desired graph properties and generalize to unseen\ngraphs. We also compare our generator to the graph generation capabilities of\ndifferent LLMs. This work signifies a shift in graph generation methodologies,\noffering a more practical and efficient solution for generating diverse graphs\nwith specific characteristics.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Graph generation has emerged as a crucial task in machine learning, with\nsignificant challenges in generating graphs that accurately reflect specific\nproperties. Existing methods often fall short in efficiently addressing this\nneed as they struggle with the high-dimensional complexity and varied nature of\ngraph properties. In this paper, we introduce the Neural Graph Generator (NGG),\na novel approach which utilizes conditioned latent diffusion models for graph\ngeneration. NGG demonstrates a remarkable capacity to model complex graph\npatterns, offering control over the graph generation process. NGG employs a\nvariational graph autoencoder for graph compression and a diffusion process in\nthe latent vector space, guided by vectors summarizing graph statistics. We\ndemonstrate NGG's versatility across various graph generation tasks, showing\nits capability to capture desired graph properties and generalize to unseen\ngraphs. We also compare our generator to the graph generation capabilities of\ndifferent LLMs. This work signifies a shift in graph generation methodologies,\noffering a more practical and efficient solution for generating diverse graphs\nwith specific characteristics."
                },
                "authors": [
                    {
                        "name": "Iakovos Evdaimon"
                    },
                    {
                        "name": "Giannis Nikolentzos"
                    },
                    {
                        "name": "Christos Xypolopoulos"
                    },
                    {
                        "name": "Ahmed Kammoun"
                    },
                    {
                        "name": "Michail Chatzianastasis"
                    },
                    {
                        "name": "Hadi Abdine"
                    },
                    {
                        "name": "Michalis Vazirgiannis"
                    }
                ],
                "author_detail": {
                    "name": "Michalis Vazirgiannis"
                },
                "author": "Michalis Vazirgiannis",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2403.01535v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2403.01535v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.00099v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.00099v3",
                "updated": "2024-09-18T13:45:08Z",
                "updated_parsed": [
                    2024,
                    9,
                    18,
                    13,
                    45,
                    8,
                    2,
                    262,
                    0
                ],
                "published": "2024-04-30T18:00:02Z",
                "published_parsed": [
                    2024,
                    4,
                    30,
                    18,
                    0,
                    2,
                    1,
                    121,
                    0
                ],
                "title": "Creative Beam Search: LLM-as-a-Judge For Improving Response Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Creative Beam Search: LLM-as-a-Judge For Improving Response Generation"
                },
                "summary": "Large language models are revolutionizing several areas, including artificial\ncreativity. However, the process of generation in machines profoundly diverges\nfrom that observed in humans. In particular, machine generation is\ncharacterized by a lack of intentionality and an underlying creative process.\nWe propose a method called Creative Beam Search that uses Diverse Beam Search\nand LLM-as-a-Judge to perform response generation and response validation. The\nresults of a qualitative experiment show how our approach can provide better\noutput than standard sampling techniques. We also show that the response\nvalidation step is a necessary complement to the response generation step.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models are revolutionizing several areas, including artificial\ncreativity. However, the process of generation in machines profoundly diverges\nfrom that observed in humans. In particular, machine generation is\ncharacterized by a lack of intentionality and an underlying creative process.\nWe propose a method called Creative Beam Search that uses Diverse Beam Search\nand LLM-as-a-Judge to perform response generation and response validation. The\nresults of a qualitative experiment show how our approach can provide better\noutput than standard sampling techniques. We also show that the response\nvalidation step is a necessary complement to the response generation step."
                },
                "authors": [
                    {
                        "name": "Giorgio Franceschelli"
                    },
                    {
                        "name": "Mirco Musolesi"
                    }
                ],
                "author_detail": {
                    "name": "Mirco Musolesi"
                },
                "author": "Mirco Musolesi",
                "arxiv_comment": "Presented as a short paper at the 15th International Conference on\n  Computational Creativity (ICCC'24)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.00099v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.00099v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2304.00008v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2304.00008v4",
                "updated": "2024-09-18T13:25:52Z",
                "updated_parsed": [
                    2024,
                    9,
                    18,
                    13,
                    25,
                    52,
                    2,
                    262,
                    0
                ],
                "published": "2023-03-27T18:00:01Z",
                "published_parsed": [
                    2023,
                    3,
                    27,
                    18,
                    0,
                    1,
                    0,
                    86,
                    0
                ],
                "title": "On the Creativity of Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "On the Creativity of Large Language Models"
                },
                "summary": "Large Language Models (LLMs) are revolutionizing several areas of Artificial\nIntelligence. One of the most remarkable applications is creative writing,\ne.g., poetry or storytelling: the generated outputs are often of astonishing\nquality. However, a natural question arises: can LLMs be really considered\ncreative? In this article, we first analyze the development of LLMs under the\nlens of creativity theories, investigating the key open questions and\nchallenges. In particular, we focus our discussion on the dimensions of value,\nnovelty, and surprise as proposed by Margaret Boden in her work. Then, we\nconsider different classic perspectives, namely product, process, press, and\nperson. We discuss a set of ``easy'' and ``hard'' problems in machine\ncreativity, presenting them in relation to LLMs. Finally, we examine the\nsocietal impact of these technologies with a particular focus on the creative\nindustries, analyzing the opportunities offered, the challenges arising from\nthem, and the potential associated risks, from both legal and ethical points of\nview.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) are revolutionizing several areas of Artificial\nIntelligence. One of the most remarkable applications is creative writing,\ne.g., poetry or storytelling: the generated outputs are often of astonishing\nquality. However, a natural question arises: can LLMs be really considered\ncreative? In this article, we first analyze the development of LLMs under the\nlens of creativity theories, investigating the key open questions and\nchallenges. In particular, we focus our discussion on the dimensions of value,\nnovelty, and surprise as proposed by Margaret Boden in her work. Then, we\nconsider different classic perspectives, namely product, process, press, and\nperson. We discuss a set of ``easy'' and ``hard'' problems in machine\ncreativity, presenting them in relation to LLMs. Finally, we examine the\nsocietal impact of these technologies with a particular focus on the creative\nindustries, analyzing the opportunities offered, the challenges arising from\nthem, and the potential associated risks, from both legal and ethical points of\nview."
                },
                "authors": [
                    {
                        "name": "Giorgio Franceschelli"
                    },
                    {
                        "name": "Mirco Musolesi"
                    }
                ],
                "author_detail": {
                    "name": "Mirco Musolesi"
                },
                "author": "Mirco Musolesi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2304.00008v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2304.00008v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.11971v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.11971v1",
                "updated": "2024-09-18T13:22:04Z",
                "updated_parsed": [
                    2024,
                    9,
                    18,
                    13,
                    22,
                    4,
                    2,
                    262,
                    0
                ],
                "published": "2024-09-18T13:22:04Z",
                "published_parsed": [
                    2024,
                    9,
                    18,
                    13,
                    22,
                    4,
                    2,
                    262,
                    0
                ],
                "title": "Sampling Latent Material-Property Information From LLM-Derived Embedding\n  Representations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Sampling Latent Material-Property Information From LLM-Derived Embedding\n  Representations"
                },
                "summary": "Vector embeddings derived from large language models (LLMs) show promise in\ncapturing latent information from the literature. Interestingly, these can be\nintegrated into material embeddings, potentially useful for data-driven\npredictions of materials properties. We investigate the extent to which\nLLM-derived vectors capture the desired information and their potential to\nprovide insights into material properties without additional training. Our\nfindings indicate that, although LLMs can be used to generate representations\nreflecting certain property information, extracting the embeddings requires\nidentifying the optimal contextual clues and appropriate comparators. Despite\nthis restriction, it appears that LLMs still have the potential to be useful in\ngenerating meaningful materials-science representations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Vector embeddings derived from large language models (LLMs) show promise in\ncapturing latent information from the literature. Interestingly, these can be\nintegrated into material embeddings, potentially useful for data-driven\npredictions of materials properties. We investigate the extent to which\nLLM-derived vectors capture the desired information and their potential to\nprovide insights into material properties without additional training. Our\nfindings indicate that, although LLMs can be used to generate representations\nreflecting certain property information, extracting the embeddings requires\nidentifying the optimal contextual clues and appropriate comparators. Despite\nthis restriction, it appears that LLMs still have the potential to be useful in\ngenerating meaningful materials-science representations."
                },
                "authors": [
                    {
                        "name": "Luke P. J. Gilligan"
                    },
                    {
                        "name": "Matteo Cobelli"
                    },
                    {
                        "name": "Hasan M. Sayeed"
                    },
                    {
                        "name": "Taylor D. Sparks"
                    },
                    {
                        "name": "Stefano Sanvito"
                    }
                ],
                "author_detail": {
                    "name": "Stefano Sanvito"
                },
                "author": "Stefano Sanvito",
                "arxiv_comment": "10 pages, 7 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.11971v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.11971v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cond-mat.mtrl-sci",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.11968v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.11968v1",
                "updated": "2024-09-18T13:20:23Z",
                "updated_parsed": [
                    2024,
                    9,
                    18,
                    13,
                    20,
                    23,
                    2,
                    262,
                    0
                ],
                "published": "2024-09-18T13:20:23Z",
                "published_parsed": [
                    2024,
                    9,
                    18,
                    13,
                    20,
                    23,
                    2,
                    262,
                    0
                ],
                "title": "Efficacy of Synthetic Data as a Benchmark",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficacy of Synthetic Data as a Benchmark"
                },
                "summary": "Large language models (LLMs) have enabled a range of applications in\nzero-shot and few-shot learning settings, including the generation of synthetic\ndatasets for training and testing. However, to reliably use these synthetic\ndatasets, it is essential to understand how representative they are of\nreal-world data. We investigate this by assessing the effectiveness of\ngenerating synthetic data through LLM and using it as a benchmark for various\nNLP tasks. Our experiments across six datasets, and three different tasks, show\nthat while synthetic data can effectively capture performance of various\nmethods for simpler tasks, such as intent classification, it falls short for\nmore complex tasks like named entity recognition. Additionally, we propose a\nnew metric called the bias factor, which evaluates the biases introduced when\nthe same LLM is used to both generate benchmarking data and to perform the\ntasks. We find that smaller LLMs exhibit biases towards their own generated\ndata, whereas larger models do not. Overall, our findings suggest that the\neffectiveness of synthetic data as a benchmark varies depending on the task,\nand that practitioners should rely on data generated from multiple larger\nmodels whenever possible.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have enabled a range of applications in\nzero-shot and few-shot learning settings, including the generation of synthetic\ndatasets for training and testing. However, to reliably use these synthetic\ndatasets, it is essential to understand how representative they are of\nreal-world data. We investigate this by assessing the effectiveness of\ngenerating synthetic data through LLM and using it as a benchmark for various\nNLP tasks. Our experiments across six datasets, and three different tasks, show\nthat while synthetic data can effectively capture performance of various\nmethods for simpler tasks, such as intent classification, it falls short for\nmore complex tasks like named entity recognition. Additionally, we propose a\nnew metric called the bias factor, which evaluates the biases introduced when\nthe same LLM is used to both generate benchmarking data and to perform the\ntasks. We find that smaller LLMs exhibit biases towards their own generated\ndata, whereas larger models do not. Overall, our findings suggest that the\neffectiveness of synthetic data as a benchmark varies depending on the task,\nand that practitioners should rely on data generated from multiple larger\nmodels whenever possible."
                },
                "authors": [
                    {
                        "name": "Gaurav Maheshwari"
                    },
                    {
                        "name": "Dmitry Ivanov"
                    },
                    {
                        "name": "Kevin El Haddad"
                    }
                ],
                "author_detail": {
                    "name": "Kevin El Haddad"
                },
                "author": "Kevin El Haddad",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.11968v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.11968v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.10516v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.10516v2",
                "updated": "2024-09-18T13:11:13Z",
                "updated_parsed": [
                    2024,
                    9,
                    18,
                    13,
                    11,
                    13,
                    2,
                    262,
                    0
                ],
                "published": "2024-09-16T17:59:52Z",
                "published_parsed": [
                    2024,
                    9,
                    16,
                    17,
                    59,
                    52,
                    0,
                    260,
                    0
                ],
                "title": "RetrievalAttention: Accelerating Long-Context LLM Inference via Vector\n  Retrieval",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RetrievalAttention: Accelerating Long-Context LLM Inference via Vector\n  Retrieval"
                },
                "summary": "Transformer-based Large Language Models (LLMs) have become increasingly\nimportant. However, due to the quadratic time complexity of attention\ncomputation, scaling LLMs to longer contexts incurs extremely slow inference\nlatency and high GPU memory consumption for caching key-value (KV) vectors.\nThis paper proposes RetrievalAttention, a training-free approach to both\naccelerate attention computation and reduce GPU memory consumption. By\nleveraging the dynamic sparsity of attention mechanism, RetrievalAttention\nproposes to use approximate nearest neighbor search (ANNS) indexes for KV\nvectors in CPU memory and retrieves the most relevant ones with vector search\nduring generation. Unfortunately, we observe that the off-the-shelf ANNS\nindexes are often ineffective for such retrieval tasks due to the\nout-of-distribution (OOD) between query vectors and key vectors in attention\nmechanism. RetrievalAttention addresses the OOD challenge by designing an\nattention-aware vector search algorithm that can adapt to the distribution of\nquery vectors. Our evaluation shows that RetrievalAttention only needs to\naccess 1--3% of data while maintaining high model accuracy. This leads to\nsignificant reduction in the inference cost of long-context LLMs with much\nlower GPU memory footprint. In particular, RetrievalAttention only needs a\nsingle NVIDIA RTX4090 (24GB) for serving 128K tokens in LLMs with 8B\nparameters, which is capable of generating one token in 0.188 seconds.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Transformer-based Large Language Models (LLMs) have become increasingly\nimportant. However, due to the quadratic time complexity of attention\ncomputation, scaling LLMs to longer contexts incurs extremely slow inference\nlatency and high GPU memory consumption for caching key-value (KV) vectors.\nThis paper proposes RetrievalAttention, a training-free approach to both\naccelerate attention computation and reduce GPU memory consumption. By\nleveraging the dynamic sparsity of attention mechanism, RetrievalAttention\nproposes to use approximate nearest neighbor search (ANNS) indexes for KV\nvectors in CPU memory and retrieves the most relevant ones with vector search\nduring generation. Unfortunately, we observe that the off-the-shelf ANNS\nindexes are often ineffective for such retrieval tasks due to the\nout-of-distribution (OOD) between query vectors and key vectors in attention\nmechanism. RetrievalAttention addresses the OOD challenge by designing an\nattention-aware vector search algorithm that can adapt to the distribution of\nquery vectors. Our evaluation shows that RetrievalAttention only needs to\naccess 1--3% of data while maintaining high model accuracy. This leads to\nsignificant reduction in the inference cost of long-context LLMs with much\nlower GPU memory footprint. In particular, RetrievalAttention only needs a\nsingle NVIDIA RTX4090 (24GB) for serving 128K tokens in LLMs with 8B\nparameters, which is capable of generating one token in 0.188 seconds."
                },
                "authors": [
                    {
                        "name": "Di Liu"
                    },
                    {
                        "name": "Meng Chen"
                    },
                    {
                        "name": "Baotong Lu"
                    },
                    {
                        "name": "Huiqiang Jiang"
                    },
                    {
                        "name": "Zhenhua Han"
                    },
                    {
                        "name": "Qianxi Zhang"
                    },
                    {
                        "name": "Qi Chen"
                    },
                    {
                        "name": "Chengruidong Zhang"
                    },
                    {
                        "name": "Bailu Ding"
                    },
                    {
                        "name": "Kai Zhang"
                    },
                    {
                        "name": "Chen Chen"
                    },
                    {
                        "name": "Fan Yang"
                    },
                    {
                        "name": "Yuqing Yang"
                    },
                    {
                        "name": "Lili Qiu"
                    }
                ],
                "author_detail": {
                    "name": "Lili Qiu"
                },
                "author": "Lili Qiu",
                "arxiv_comment": "16 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.10516v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.10516v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2312.02515v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2312.02515v2",
                "updated": "2024-09-18T13:07:28Z",
                "updated_parsed": [
                    2024,
                    9,
                    18,
                    13,
                    7,
                    28,
                    2,
                    262,
                    0
                ],
                "published": "2023-12-05T05:38:38Z",
                "published_parsed": [
                    2023,
                    12,
                    5,
                    5,
                    38,
                    38,
                    1,
                    339,
                    0
                ],
                "title": "mLoRA: Fine-Tuning LoRA Adapters via Highly-Efficient Pipeline\n  Parallelism in Multiple GPUs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "mLoRA: Fine-Tuning LoRA Adapters via Highly-Efficient Pipeline\n  Parallelism in Multiple GPUs"
                },
                "summary": "Transformer-based, pre-trained large language models (LLMs) have demonstrated\noutstanding performance across diverse domains, particularly in the emerging\n{\\em pretrain-then-finetune} paradigm. Low-Rank Adaptation (LoRA), a\nparameter-efficient fine-tuning method, is commonly used to adapt a base LLM to\nmultiple downstream tasks. Further, LLM platforms enable developers to\nfine-tune multiple models and develop various domain-specific applications\nsimultaneously. However, existing model parallelism schemes suffer from high\ncommunication overhead and inefficient GPU utilization when training multiple\nLoRA tasks across GPUs and machines.\n  In this paper, we present mLoRA, a parallelism-efficient fine-tuning system\ndesigned for training multiple LoRA across GPUs and machines. mLoRA introduces\na novel LoRA-aware pipeline parallelism scheme that efficiently pipelines\nindependent LoRA adapters and their distinct fine-tuning stages across GPUs and\nmachines, along with a new LoRA-efficient operator to enhance GPU utilization\nduring pipelined LoRA training. Our extensive evaluation shows that mLoRA can\nsignificantly reduce average fine-tuning task completion time, e.g., by 30\\%,\ncompared to state-of-the-art methods like FSDP. More importantly, mLoRA enables\nsimultaneous fine-tuning of larger models, e.g., two Llama-2-13B models on four\nNVIDIA RTX A6000 48GB GPUs, which is not feasible for FSDP due to high memory\nrequirements. Hence, mLoRA not only increases fine-tuning efficiency but also\nmakes it more accessible on cost-effective GPUs. mLoRA has been deployed in\nAntGroup's production environment.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Transformer-based, pre-trained large language models (LLMs) have demonstrated\noutstanding performance across diverse domains, particularly in the emerging\n{\\em pretrain-then-finetune} paradigm. Low-Rank Adaptation (LoRA), a\nparameter-efficient fine-tuning method, is commonly used to adapt a base LLM to\nmultiple downstream tasks. Further, LLM platforms enable developers to\nfine-tune multiple models and develop various domain-specific applications\nsimultaneously. However, existing model parallelism schemes suffer from high\ncommunication overhead and inefficient GPU utilization when training multiple\nLoRA tasks across GPUs and machines.\n  In this paper, we present mLoRA, a parallelism-efficient fine-tuning system\ndesigned for training multiple LoRA across GPUs and machines. mLoRA introduces\na novel LoRA-aware pipeline parallelism scheme that efficiently pipelines\nindependent LoRA adapters and their distinct fine-tuning stages across GPUs and\nmachines, along with a new LoRA-efficient operator to enhance GPU utilization\nduring pipelined LoRA training. Our extensive evaluation shows that mLoRA can\nsignificantly reduce average fine-tuning task completion time, e.g., by 30\\%,\ncompared to state-of-the-art methods like FSDP. More importantly, mLoRA enables\nsimultaneous fine-tuning of larger models, e.g., two Llama-2-13B models on four\nNVIDIA RTX A6000 48GB GPUs, which is not feasible for FSDP due to high memory\nrequirements. Hence, mLoRA not only increases fine-tuning efficiency but also\nmakes it more accessible on cost-effective GPUs. mLoRA has been deployed in\nAntGroup's production environment."
                },
                "authors": [
                    {
                        "name": "Zhengmao Ye"
                    },
                    {
                        "name": "Dengchun Li"
                    },
                    {
                        "name": "Zetao Hu"
                    },
                    {
                        "name": "Tingfeng Lan"
                    },
                    {
                        "name": "Jian Sha"
                    },
                    {
                        "name": "Sicong Zhang"
                    },
                    {
                        "name": "Lei Duan"
                    },
                    {
                        "name": "Jie Zuo"
                    },
                    {
                        "name": "Hui Lu"
                    },
                    {
                        "name": "Yuanchun Zhou"
                    },
                    {
                        "name": "Mingjie Tang"
                    }
                ],
                "author_detail": {
                    "name": "Mingjie Tang"
                },
                "author": "Mingjie Tang",
                "arxiv_comment": "14 pages, 16 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2312.02515v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2312.02515v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.11919v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.11919v1",
                "updated": "2024-09-18T12:32:25Z",
                "updated_parsed": [
                    2024,
                    9,
                    18,
                    12,
                    32,
                    25,
                    2,
                    262,
                    0
                ],
                "published": "2024-09-18T12:32:25Z",
                "published_parsed": [
                    2024,
                    9,
                    18,
                    12,
                    32,
                    25,
                    2,
                    262,
                    0
                ],
                "title": "LLM-wrapper: Black-Box Semantic-Aware Adaptation of Vision-Language\n  Foundation Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM-wrapper: Black-Box Semantic-Aware Adaptation of Vision-Language\n  Foundation Models"
                },
                "summary": "Vision Language Models (VLMs) have shown impressive performances on numerous\ntasks but their zero-shot capabilities can be limited compared to dedicated or\nfine-tuned models. Yet, fine-tuning VLMs comes with limitations as it requires\n`white-box' access to the model's architecture and weights as well as expertise\nto design the fine-tuning objectives and optimize the hyper-parameters, which\nare specific to each VLM and downstream task. In this work, we propose\nLLM-wrapper, a novel approach to adapt VLMs in a `black-box' manner by\nleveraging large language models (LLMs) so as to reason on their outputs. We\ndemonstrate the effectiveness of LLM-wrapper on Referring Expression\nComprehension (REC), a challenging open-vocabulary task that requires spatial\nand semantic reasoning. Our approach significantly boosts the performance of\noff-the-shelf models, resulting in competitive results when compared with\nclassic fine-tuning.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Vision Language Models (VLMs) have shown impressive performances on numerous\ntasks but their zero-shot capabilities can be limited compared to dedicated or\nfine-tuned models. Yet, fine-tuning VLMs comes with limitations as it requires\n`white-box' access to the model's architecture and weights as well as expertise\nto design the fine-tuning objectives and optimize the hyper-parameters, which\nare specific to each VLM and downstream task. In this work, we propose\nLLM-wrapper, a novel approach to adapt VLMs in a `black-box' manner by\nleveraging large language models (LLMs) so as to reason on their outputs. We\ndemonstrate the effectiveness of LLM-wrapper on Referring Expression\nComprehension (REC), a challenging open-vocabulary task that requires spatial\nand semantic reasoning. Our approach significantly boosts the performance of\noff-the-shelf models, resulting in competitive results when compared with\nclassic fine-tuning."
                },
                "authors": [
                    {
                        "name": "Amaia Cardiel"
                    },
                    {
                        "name": "Eloi Zablocki"
                    },
                    {
                        "name": "Oriane Siméoni"
                    },
                    {
                        "name": "Elias Ramzi"
                    },
                    {
                        "name": "Matthieu Cord"
                    }
                ],
                "author_detail": {
                    "name": "Matthieu Cord"
                },
                "author": "Matthieu Cord",
                "arxiv_comment": "EVAL-FoMo workshop, ECCV 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.11919v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.11919v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.11917v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.11917v1",
                "updated": "2024-09-18T12:29:22Z",
                "updated_parsed": [
                    2024,
                    9,
                    18,
                    12,
                    29,
                    22,
                    2,
                    262,
                    0
                ],
                "published": "2024-09-18T12:29:22Z",
                "published_parsed": [
                    2024,
                    9,
                    18,
                    12,
                    29,
                    22,
                    2,
                    262,
                    0
                ],
                "title": "LLMs in Education: Novel Perspectives, Challenges, and Opportunities",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLMs in Education: Novel Perspectives, Challenges, and Opportunities"
                },
                "summary": "The role of large language models (LLMs) in education is an increasing area\nof interest today, considering the new opportunities they offer for teaching,\nlearning, and assessment. This cutting-edge tutorial provides an overview of\nthe educational applications of NLP and the impact that the recent advances in\nLLMs have had on this field. We will discuss the key challenges and\nopportunities presented by LLMs, grounding them in the context of four major\neducational applications: reading, writing, and speaking skills, and\nintelligent tutoring systems (ITS). This COLING 2025 tutorial is designed for\nresearchers and practitioners interested in the educational applications of NLP\nand the role LLMs have to play in this area. It is the first of its kind to\naddress this timely topic.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The role of large language models (LLMs) in education is an increasing area\nof interest today, considering the new opportunities they offer for teaching,\nlearning, and assessment. This cutting-edge tutorial provides an overview of\nthe educational applications of NLP and the impact that the recent advances in\nLLMs have had on this field. We will discuss the key challenges and\nopportunities presented by LLMs, grounding them in the context of four major\neducational applications: reading, writing, and speaking skills, and\nintelligent tutoring systems (ITS). This COLING 2025 tutorial is designed for\nresearchers and practitioners interested in the educational applications of NLP\nand the role LLMs have to play in this area. It is the first of its kind to\naddress this timely topic."
                },
                "authors": [
                    {
                        "name": "Bashar Alhafni"
                    },
                    {
                        "name": "Sowmya Vajjala"
                    },
                    {
                        "name": "Stefano Bannò"
                    },
                    {
                        "name": "Kaushal Kumar Maurya"
                    },
                    {
                        "name": "Ekaterina Kochmar"
                    }
                ],
                "author_detail": {
                    "name": "Ekaterina Kochmar"
                },
                "author": "Ekaterina Kochmar",
                "arxiv_comment": "COLING 2025 Tutorial",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.11917v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.11917v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2310.12537v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2310.12537v5",
                "updated": "2024-09-20T08:49:37Z",
                "updated_parsed": [
                    2024,
                    9,
                    20,
                    8,
                    49,
                    37,
                    4,
                    264,
                    0
                ],
                "published": "2023-10-19T07:39:00Z",
                "published_parsed": [
                    2023,
                    10,
                    19,
                    7,
                    39,
                    0,
                    3,
                    292,
                    0
                ],
                "title": "ExtractGPT: Exploring the Potential of Large Language Models for Product\n  Attribute Value Extraction",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ExtractGPT: Exploring the Potential of Large Language Models for Product\n  Attribute Value Extraction"
                },
                "summary": "E-commerce platforms require structured product data in the form of\nattribute-value pairs to offer features such as faceted product search or\nattribute-based product comparison. However, vendors often provide unstructured\nproduct descriptions, necessitating the extraction of attribute-value pairs\nfrom these texts. BERT-based extraction methods require large amounts of\ntask-specific training data and struggle with unseen attribute values. This\npaper explores using large language models (LLMs) as a more training-data\nefficient and robust alternative. We propose prompt templates for zero-shot and\nfew-shot scenarios, comparing textual and JSON-based target schema\nrepresentations. Our experiments show that GPT-4 achieves the highest average\nF1-score of 85% using detailed attribute descriptions and demonstrations.\nLlama-3-70B performs nearly as well, offering a competitive open-source\nalternative. GPT-4 surpasses the best PLM baseline by 5% in F1-score.\nFine-tuning GPT-3.5 increases the performance to the level of GPT-4 but reduces\nthe model's ability to generalize to unseen attribute values.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "E-commerce platforms require structured product data in the form of\nattribute-value pairs to offer features such as faceted product search or\nattribute-based product comparison. However, vendors often provide unstructured\nproduct descriptions, necessitating the extraction of attribute-value pairs\nfrom these texts. BERT-based extraction methods require large amounts of\ntask-specific training data and struggle with unseen attribute values. This\npaper explores using large language models (LLMs) as a more training-data\nefficient and robust alternative. We propose prompt templates for zero-shot and\nfew-shot scenarios, comparing textual and JSON-based target schema\nrepresentations. Our experiments show that GPT-4 achieves the highest average\nF1-score of 85% using detailed attribute descriptions and demonstrations.\nLlama-3-70B performs nearly as well, offering a competitive open-source\nalternative. GPT-4 surpasses the best PLM baseline by 5% in F1-score.\nFine-tuning GPT-3.5 increases the performance to the level of GPT-4 but reduces\nthe model's ability to generalize to unseen attribute values."
                },
                "authors": [
                    {
                        "name": "Alexander Brinkmann"
                    },
                    {
                        "name": "Roee Shraga"
                    },
                    {
                        "name": "Christian Bizer"
                    }
                ],
                "author_detail": {
                    "name": "Christian Bizer"
                },
                "author": "Christian Bizer",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2310.12537v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2310.12537v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.12895v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.12895v4",
                "updated": "2024-09-22T13:38:17Z",
                "updated_parsed": [
                    2024,
                    9,
                    22,
                    13,
                    38,
                    17,
                    6,
                    266,
                    0
                ],
                "published": "2024-04-19T13:58:34Z",
                "published_parsed": [
                    2024,
                    4,
                    19,
                    13,
                    58,
                    34,
                    4,
                    110,
                    0
                ],
                "title": "The emergence of subjective temporality: the self-simulational theory of\n  temporal extension from the perspective of the free energy principle",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The emergence of subjective temporality: the self-simulational theory of\n  temporal extension from the perspective of the free energy principle"
                },
                "summary": "The self-simulational theory of temporal extension describes an\ninformation-theoretically formalized mechanism by which the width of subjective\ntemporality emerges from the architecture of self-modelling. In this paper, the\nperspective of the free energy principle will be assumed, to cast the emergence\nof subjective temporality, along with a Bayesian mechanism for hierarchical\nduration estimation, from first principles of the physics of self-organization.\nUsing active inference, a deep parametric generative model of temporal\ninference is simulated, which realizes the described dynamics on a\ncomputational level. Two biases (i.e. variations) of time-perception naturally\nemerge from the simulated computational model. This concerns the intentional\nbinding effect (i.e. the compression of the temporal interval between\nvoluntarily initiated actions and subsequent sensory consequences) and\nempirically documented alterations of subjective time experience in deep states\nof meditative absorption (i.e. in minimal phenomenal experience). Generally,\nnumerous systematic and domain-specific alterations of subjective temporal\nexperience are computationally explained in a unified manner, as enabled by\nintegration with current active inference accounts mapping onto the respective\ndomains. This concerns - next to more general scale-invariant effects of\nexplicit timing and central tendency effects - the temporality-modulating role\nof valence, impulsivity, boredom, flow-states, near death-experiences, and\nvarious psychopathologies, amongst others. The self-simulational theory of\ntemporal extension, from the perspective of the free energy principle, explains\nhow the subjective temporal Now emerges and varies from first principles,\naccounting for why sometimes, subjective time seems to fly, and sometimes,\nmoments feel like eternities; with the computational mechanism being readily\ndeployable synthetically.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The self-simulational theory of temporal extension describes an\ninformation-theoretically formalized mechanism by which the width of subjective\ntemporality emerges from the architecture of self-modelling. In this paper, the\nperspective of the free energy principle will be assumed, to cast the emergence\nof subjective temporality, along with a Bayesian mechanism for hierarchical\nduration estimation, from first principles of the physics of self-organization.\nUsing active inference, a deep parametric generative model of temporal\ninference is simulated, which realizes the described dynamics on a\ncomputational level. Two biases (i.e. variations) of time-perception naturally\nemerge from the simulated computational model. This concerns the intentional\nbinding effect (i.e. the compression of the temporal interval between\nvoluntarily initiated actions and subsequent sensory consequences) and\nempirically documented alterations of subjective time experience in deep states\nof meditative absorption (i.e. in minimal phenomenal experience). Generally,\nnumerous systematic and domain-specific alterations of subjective temporal\nexperience are computationally explained in a unified manner, as enabled by\nintegration with current active inference accounts mapping onto the respective\ndomains. This concerns - next to more general scale-invariant effects of\nexplicit timing and central tendency effects - the temporality-modulating role\nof valence, impulsivity, boredom, flow-states, near death-experiences, and\nvarious psychopathologies, amongst others. The self-simulational theory of\ntemporal extension, from the perspective of the free energy principle, explains\nhow the subjective temporal Now emerges and varies from first principles,\naccounting for why sometimes, subjective time seems to fly, and sometimes,\nmoments feel like eternities; with the computational mechanism being readily\ndeployable synthetically."
                },
                "authors": [
                    {
                        "name": "Jan Erik Bellingrath"
                    }
                ],
                "author_detail": {
                    "name": "Jan Erik Bellingrath"
                },
                "author": "Jan Erik Bellingrath",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.12895v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.12895v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "q-bio.NC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "q-bio.NC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.11901v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.11901v1",
                "updated": "2024-09-18T11:54:45Z",
                "updated_parsed": [
                    2024,
                    9,
                    18,
                    11,
                    54,
                    45,
                    2,
                    262,
                    0
                ],
                "published": "2024-09-18T11:54:45Z",
                "published_parsed": [
                    2024,
                    9,
                    18,
                    11,
                    54,
                    45,
                    2,
                    262,
                    0
                ],
                "title": "LLMs + Persona-Plug = Personalized LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLMs + Persona-Plug = Personalized LLMs"
                },
                "summary": "Personalization plays a critical role in numerous language tasks and\napplications, since users with the same requirements may prefer diverse outputs\nbased on their individual interests. This has led to the development of various\npersonalized approaches aimed at adapting large language models (LLMs) to\ngenerate customized outputs aligned with user preferences. Some of them involve\nfine-tuning a unique personalized LLM for each user, which is too expensive for\nwidespread application. Alternative approaches introduce personalization\ninformation in a plug-and-play manner by retrieving the user's relevant\nhistorical texts as demonstrations. However, this retrieval-based strategy may\nbreak the continuity of the user history and fail to capture the user's overall\nstyles and patterns, hence leading to sub-optimal performance. To address these\nchallenges, we propose a novel personalized LLM model, \\ours{}. It constructs a\nuser-specific embedding for each individual by modeling all her historical\ncontexts through a lightweight plug-in user embedder module. By attaching this\nembedding to the task input, LLMs can better understand and capture user habits\nand preferences, thereby producing more personalized outputs without tuning\ntheir own parameters. Extensive experiments on various tasks in the language\nmodel personalization (LaMP) benchmark demonstrate that the proposed model\nsignificantly outperforms existing personalized LLM approaches.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Personalization plays a critical role in numerous language tasks and\napplications, since users with the same requirements may prefer diverse outputs\nbased on their individual interests. This has led to the development of various\npersonalized approaches aimed at adapting large language models (LLMs) to\ngenerate customized outputs aligned with user preferences. Some of them involve\nfine-tuning a unique personalized LLM for each user, which is too expensive for\nwidespread application. Alternative approaches introduce personalization\ninformation in a plug-and-play manner by retrieving the user's relevant\nhistorical texts as demonstrations. However, this retrieval-based strategy may\nbreak the continuity of the user history and fail to capture the user's overall\nstyles and patterns, hence leading to sub-optimal performance. To address these\nchallenges, we propose a novel personalized LLM model, \\ours{}. It constructs a\nuser-specific embedding for each individual by modeling all her historical\ncontexts through a lightweight plug-in user embedder module. By attaching this\nembedding to the task input, LLMs can better understand and capture user habits\nand preferences, thereby producing more personalized outputs without tuning\ntheir own parameters. Extensive experiments on various tasks in the language\nmodel personalization (LaMP) benchmark demonstrate that the proposed model\nsignificantly outperforms existing personalized LLM approaches."
                },
                "authors": [
                    {
                        "name": "Jiongnan Liu"
                    },
                    {
                        "name": "Yutao Zhu"
                    },
                    {
                        "name": "Shuting Wang"
                    },
                    {
                        "name": "Xiaochi Wei"
                    },
                    {
                        "name": "Erxue Min"
                    },
                    {
                        "name": "Yu Lu"
                    },
                    {
                        "name": "Shuaiqiang Wang"
                    },
                    {
                        "name": "Dawei Yin"
                    },
                    {
                        "name": "Zhicheng Dou"
                    }
                ],
                "author_detail": {
                    "name": "Zhicheng Dou"
                },
                "author": "Zhicheng Dou",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.11901v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.11901v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2309.15221v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2309.15221v2",
                "updated": "2024-09-18T11:38:53Z",
                "updated_parsed": [
                    2024,
                    9,
                    18,
                    11,
                    38,
                    53,
                    2,
                    262,
                    0
                ],
                "published": "2023-09-26T19:39:11Z",
                "published_parsed": [
                    2023,
                    9,
                    26,
                    19,
                    39,
                    11,
                    1,
                    269,
                    0
                ],
                "title": "Silver Telluride Colloidal Quantum Dot Infrared Photodetectors and Image\n  Sensors",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Silver Telluride Colloidal Quantum Dot Infrared Photodetectors and Image\n  Sensors"
                },
                "summary": "Photodetectors that are sensitive in shortwave infrared (SWIR) range (1 um -\n2 um) are of significant interest for applications in 3D, night and adverse\nweather imaging, machine vision and autonomous driving, among others. Currently\navailable technologies in the SWIR rely on costly epitaxial semiconductors that\nare not monolithically integrated with CMOS electronics. Solution-processed\nquantum dots can address this challenge by enabling low-cost manufacturing and\nsimple monolithic integration on silicon in a back-end-of-line (BEOL) process.\nTo date, colloidal quantum dot (CQD) materials to access the SWIR are mostly\nbased on lead sulfide (PbS) and mercury telluride (HgTe) compounds, imposing\nmajor regulatory concerns and impeding their deployment in consumer electronics\ndue to toxicity concerns. Here we report a new synthesis method for\nenvironmentally-friendly silver telluride (Ag2Te) quantum dots and their\napplication in high-performance SWIR photodetectors. The CQD photodetector\nstack employs materials compliant with the Restriction of Hazardous Substance\n(RoHS) directives and is sensitive in the spectral range from 350 nm - 1600 nm.\nThe room-temperature detectivity is of the order 1012 Jones, the 3dB bandwidth\nis in excess of 0.1 MHz and the linear dynamic range is over 118 dB. We also\nrealize a monolithically integrated SWIR imager based on solution processed,\nheavy-metal-free materials, thus paving the way of this technology to consumer\nelectronics market.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Photodetectors that are sensitive in shortwave infrared (SWIR) range (1 um -\n2 um) are of significant interest for applications in 3D, night and adverse\nweather imaging, machine vision and autonomous driving, among others. Currently\navailable technologies in the SWIR rely on costly epitaxial semiconductors that\nare not monolithically integrated with CMOS electronics. Solution-processed\nquantum dots can address this challenge by enabling low-cost manufacturing and\nsimple monolithic integration on silicon in a back-end-of-line (BEOL) process.\nTo date, colloidal quantum dot (CQD) materials to access the SWIR are mostly\nbased on lead sulfide (PbS) and mercury telluride (HgTe) compounds, imposing\nmajor regulatory concerns and impeding their deployment in consumer electronics\ndue to toxicity concerns. Here we report a new synthesis method for\nenvironmentally-friendly silver telluride (Ag2Te) quantum dots and their\napplication in high-performance SWIR photodetectors. The CQD photodetector\nstack employs materials compliant with the Restriction of Hazardous Substance\n(RoHS) directives and is sensitive in the spectral range from 350 nm - 1600 nm.\nThe room-temperature detectivity is of the order 1012 Jones, the 3dB bandwidth\nis in excess of 0.1 MHz and the linear dynamic range is over 118 dB. We also\nrealize a monolithically integrated SWIR imager based on solution processed,\nheavy-metal-free materials, thus paving the way of this technology to consumer\nelectronics market."
                },
                "authors": [
                    {
                        "name": "Yongjie Wang"
                    },
                    {
                        "name": "Lucheng Peng"
                    },
                    {
                        "name": "Julien Schreier"
                    },
                    {
                        "name": "Yu Bi"
                    },
                    {
                        "name": "Andres Black"
                    },
                    {
                        "name": "Aditya Malla"
                    },
                    {
                        "name": "Stijn Goosens"
                    },
                    {
                        "name": "Gerasimos Konstantatos"
                    }
                ],
                "author_detail": {
                    "name": "Gerasimos Konstantatos"
                },
                "author": "Gerasimos Konstantatos",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2309.15221v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2309.15221v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.app-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.app-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.11870v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.11870v1",
                "updated": "2024-09-18T10:52:16Z",
                "updated_parsed": [
                    2024,
                    9,
                    18,
                    10,
                    52,
                    16,
                    2,
                    262,
                    0
                ],
                "published": "2024-09-18T10:52:16Z",
                "published_parsed": [
                    2024,
                    9,
                    18,
                    10,
                    52,
                    16,
                    2,
                    262,
                    0
                ],
                "title": "SpotLight: Robotic Scene Understanding through Interaction and\n  Affordance Detection",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SpotLight: Robotic Scene Understanding through Interaction and\n  Affordance Detection"
                },
                "summary": "Despite increasing research efforts on household robotics, robots intended\nfor deployment in domestic settings still struggle with more complex tasks such\nas interacting with functional elements like drawers or light switches, largely\ndue to limited task-specific understanding and interaction capabilities. These\ntasks require not only detection and pose estimation but also an understanding\nof the affordances these elements provide. To address these challenges and\nenhance robotic scene understanding, we introduce SpotLight: A comprehensive\nframework for robotic interaction with functional elements, specifically light\nswitches. Furthermore, this framework enables robots to improve their\nenvironmental understanding through interaction. Leveraging VLM-based\naffordance prediction to estimate motion primitives for light switch\ninteraction, we achieve up to 84% operation success in real world experiments.\nWe further introduce a specialized dataset containing 715 images as well as a\ncustom detection model for light switch detection. We demonstrate how the\nframework can facilitate robot learning through physical interaction by having\nthe robot explore the environment and discover previously unknown relationships\nin a scene graph representation. Lastly, we propose an extension to the\nframework to accommodate other functional interactions such as swing doors,\nshowcasing its flexibility. Videos and Code:\ntimengelbracht.github.io/SpotLight/",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Despite increasing research efforts on household robotics, robots intended\nfor deployment in domestic settings still struggle with more complex tasks such\nas interacting with functional elements like drawers or light switches, largely\ndue to limited task-specific understanding and interaction capabilities. These\ntasks require not only detection and pose estimation but also an understanding\nof the affordances these elements provide. To address these challenges and\nenhance robotic scene understanding, we introduce SpotLight: A comprehensive\nframework for robotic interaction with functional elements, specifically light\nswitches. Furthermore, this framework enables robots to improve their\nenvironmental understanding through interaction. Leveraging VLM-based\naffordance prediction to estimate motion primitives for light switch\ninteraction, we achieve up to 84% operation success in real world experiments.\nWe further introduce a specialized dataset containing 715 images as well as a\ncustom detection model for light switch detection. We demonstrate how the\nframework can facilitate robot learning through physical interaction by having\nthe robot explore the environment and discover previously unknown relationships\nin a scene graph representation. Lastly, we propose an extension to the\nframework to accommodate other functional interactions such as swing doors,\nshowcasing its flexibility. Videos and Code:\ntimengelbracht.github.io/SpotLight/"
                },
                "authors": [
                    {
                        "name": "Tim Engelbracht"
                    },
                    {
                        "name": "René Zurbrügg"
                    },
                    {
                        "name": "Marc Pollefeys"
                    },
                    {
                        "name": "Hermann Blum"
                    },
                    {
                        "name": "Zuria Bauer"
                    }
                ],
                "author_detail": {
                    "name": "Zuria Bauer"
                },
                "author": "Zuria Bauer",
                "arxiv_comment": "timengelbracht.github.io/SpotLight/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.11870v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.11870v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.11863v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.11863v1",
                "updated": "2024-09-18T10:36:47Z",
                "updated_parsed": [
                    2024,
                    9,
                    18,
                    10,
                    36,
                    47,
                    2,
                    262,
                    0
                ],
                "published": "2024-09-18T10:36:47Z",
                "published_parsed": [
                    2024,
                    9,
                    18,
                    10,
                    36,
                    47,
                    2,
                    262,
                    0
                ],
                "title": "Learning Task Planning from Multi-Modal Demonstration for Multi-Stage\n  Contact-Rich Manipulation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Learning Task Planning from Multi-Modal Demonstration for Multi-Stage\n  Contact-Rich Manipulation"
                },
                "summary": "Large Language Models (LLMs) have gained popularity in task planning for\nlong-horizon manipulation tasks. To enhance the validity of LLM-generated\nplans, visual demonstrations and online videos have been widely employed to\nguide the planning process. However, for manipulation tasks involving subtle\nmovements but rich contact interactions, visual perception alone may be\ninsufficient for the LLM to fully interpret the demonstration. Additionally,\nvisual data provides limited information on force-related parameters and\nconditions, which are crucial for effective execution on real robots.\n  In this paper, we introduce an in-context learning framework that\nincorporates tactile and force-torque information from human demonstrations to\nenhance LLMs' ability to generate plans for new task scenarios. We propose a\nbootstrapped reasoning pipeline that sequentially integrates each modality into\na comprehensive task plan. This task plan is then used as a reference for\nplanning in new task configurations. Real-world experiments on two different\nsequential manipulation tasks demonstrate the effectiveness of our framework in\nimproving LLMs' understanding of multi-modal demonstrations and enhancing the\noverall planning performance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have gained popularity in task planning for\nlong-horizon manipulation tasks. To enhance the validity of LLM-generated\nplans, visual demonstrations and online videos have been widely employed to\nguide the planning process. However, for manipulation tasks involving subtle\nmovements but rich contact interactions, visual perception alone may be\ninsufficient for the LLM to fully interpret the demonstration. Additionally,\nvisual data provides limited information on force-related parameters and\nconditions, which are crucial for effective execution on real robots.\n  In this paper, we introduce an in-context learning framework that\nincorporates tactile and force-torque information from human demonstrations to\nenhance LLMs' ability to generate plans for new task scenarios. We propose a\nbootstrapped reasoning pipeline that sequentially integrates each modality into\na comprehensive task plan. This task plan is then used as a reference for\nplanning in new task configurations. Real-world experiments on two different\nsequential manipulation tasks demonstrate the effectiveness of our framework in\nimproving LLMs' understanding of multi-modal demonstrations and enhancing the\noverall planning performance."
                },
                "authors": [
                    {
                        "name": "Kejia Chen"
                    },
                    {
                        "name": "Zheng Shen"
                    },
                    {
                        "name": "Yue Zhang"
                    },
                    {
                        "name": "Lingyun Chen"
                    },
                    {
                        "name": "Fan Wu"
                    },
                    {
                        "name": "Zhenshan Bing"
                    },
                    {
                        "name": "Sami Haddadin"
                    },
                    {
                        "name": "Alois Knoll"
                    }
                ],
                "author_detail": {
                    "name": "Alois Knoll"
                },
                "author": "Alois Knoll",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.11863v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.11863v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.11860v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.11860v1",
                "updated": "2024-09-18T10:30:50Z",
                "updated_parsed": [
                    2024,
                    9,
                    18,
                    10,
                    30,
                    50,
                    2,
                    262,
                    0
                ],
                "published": "2024-09-18T10:30:50Z",
                "published_parsed": [
                    2024,
                    9,
                    18,
                    10,
                    30,
                    50,
                    2,
                    262,
                    0
                ],
                "title": "Retrieve, Annotate, Evaluate, Repeat: Leveraging Multimodal LLMs for\n  Large-Scale Product Retrieval Evaluation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Retrieve, Annotate, Evaluate, Repeat: Leveraging Multimodal LLMs for\n  Large-Scale Product Retrieval Evaluation"
                },
                "summary": "Evaluating production-level retrieval systems at scale is a crucial yet\nchallenging task due to the limited availability of a large pool of\nwell-trained human annotators. Large Language Models (LLMs) have the potential\nto address this scaling issue and offer a viable alternative to humans for the\nbulk of annotation tasks. In this paper, we propose a framework for assessing\nthe product search engines in a large-scale e-commerce setting, leveraging\nMultimodal LLMs for (i) generating tailored annotation guidelines for\nindividual queries, and (ii) conducting the subsequent annotation task. Our\nmethod, validated through deployment on a large e-commerce platform,\ndemonstrates comparable quality to human annotations, significantly reduces\ntime and cost, facilitates rapid problem discovery, and provides an effective\nsolution for production-level quality control at scale.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Evaluating production-level retrieval systems at scale is a crucial yet\nchallenging task due to the limited availability of a large pool of\nwell-trained human annotators. Large Language Models (LLMs) have the potential\nto address this scaling issue and offer a viable alternative to humans for the\nbulk of annotation tasks. In this paper, we propose a framework for assessing\nthe product search engines in a large-scale e-commerce setting, leveraging\nMultimodal LLMs for (i) generating tailored annotation guidelines for\nindividual queries, and (ii) conducting the subsequent annotation task. Our\nmethod, validated through deployment on a large e-commerce platform,\ndemonstrates comparable quality to human annotations, significantly reduces\ntime and cost, facilitates rapid problem discovery, and provides an effective\nsolution for production-level quality control at scale."
                },
                "authors": [
                    {
                        "name": "Kasra Hosseini"
                    },
                    {
                        "name": "Thomas Kober"
                    },
                    {
                        "name": "Josip Krapac"
                    },
                    {
                        "name": "Roland Vollgraf"
                    },
                    {
                        "name": "Weiwei Cheng"
                    },
                    {
                        "name": "Ana Peleteiro Ramallo"
                    }
                ],
                "author_detail": {
                    "name": "Ana Peleteiro Ramallo"
                },
                "author": "Ana Peleteiro Ramallo",
                "arxiv_comment": "13 pages, 5 figures, 4 Tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.11860v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.11860v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.ET",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.11022v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.11022v2",
                "updated": "2024-09-18T10:05:02Z",
                "updated_parsed": [
                    2024,
                    9,
                    18,
                    10,
                    5,
                    2,
                    2,
                    262,
                    0
                ],
                "published": "2024-09-17T09:32:12Z",
                "published_parsed": [
                    2024,
                    9,
                    17,
                    9,
                    32,
                    12,
                    1,
                    261,
                    0
                ],
                "title": "GEIC: Universal and Multilingual Named Entity Recognition with Large\n  Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GEIC: Universal and Multilingual Named Entity Recognition with Large\n  Language Models"
                },
                "summary": "Large Language Models (LLMs) have supplanted traditional methods in numerous\nnatural language processing tasks. Nonetheless, in Named Entity Recognition\n(NER), existing LLM-based methods underperform compared to baselines and\nrequire significantly more computational resources, limiting their application.\nIn this paper, we introduce the task of generation-based extraction and\nin-context classification (GEIC), designed to leverage LLMs' prior knowledge\nand self-attention mechanisms for NER tasks. We then propose CascadeNER, a\nuniversal and multilingual GEIC framework for few-shot and zero-shot NER.\nCascadeNER employs model cascading to utilize two small-parameter LLMs to\nextract and classify independently, reducing resource consumption while\nenhancing accuracy. We also introduce AnythingNER, the first NER dataset\nspecifically designed for LLMs, including 8 languages, 155 entity types and a\nnovel dynamic categorization system. Experiments show that CascadeNER achieves\nstate-of-the-art performance on low-resource and fine-grained scenarios,\nincluding CrossNER and FewNERD. Our work is openly accessible.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have supplanted traditional methods in numerous\nnatural language processing tasks. Nonetheless, in Named Entity Recognition\n(NER), existing LLM-based methods underperform compared to baselines and\nrequire significantly more computational resources, limiting their application.\nIn this paper, we introduce the task of generation-based extraction and\nin-context classification (GEIC), designed to leverage LLMs' prior knowledge\nand self-attention mechanisms for NER tasks. We then propose CascadeNER, a\nuniversal and multilingual GEIC framework for few-shot and zero-shot NER.\nCascadeNER employs model cascading to utilize two small-parameter LLMs to\nextract and classify independently, reducing resource consumption while\nenhancing accuracy. We also introduce AnythingNER, the first NER dataset\nspecifically designed for LLMs, including 8 languages, 155 entity types and a\nnovel dynamic categorization system. Experiments show that CascadeNER achieves\nstate-of-the-art performance on low-resource and fine-grained scenarios,\nincluding CrossNER and FewNERD. Our work is openly accessible."
                },
                "authors": [
                    {
                        "name": "Hanjun Luo"
                    },
                    {
                        "name": "Yingbin Jin"
                    },
                    {
                        "name": "Xuecheng Liu"
                    },
                    {
                        "name": "Tong Shang"
                    },
                    {
                        "name": "Ruizhe Chen"
                    },
                    {
                        "name": "Zuozhu Liu"
                    }
                ],
                "author_detail": {
                    "name": "Zuozhu Liu"
                },
                "author": "Zuozhu Liu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.11022v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.11022v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.11844v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.11844v1",
                "updated": "2024-09-18T09:55:48Z",
                "updated_parsed": [
                    2024,
                    9,
                    18,
                    9,
                    55,
                    48,
                    2,
                    262,
                    0
                ],
                "published": "2024-09-18T09:55:48Z",
                "published_parsed": [
                    2024,
                    9,
                    18,
                    9,
                    55,
                    48,
                    2,
                    262,
                    0
                ],
                "title": "MEOW: MEMOry Supervised LLM Unlearning Via Inverted Facts",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MEOW: MEMOry Supervised LLM Unlearning Via Inverted Facts"
                },
                "summary": "Large Language Models (LLMs) can memorize sensitive information, raising\nconcerns about potential misuse. LLM Unlearning, a post-hoc approach to remove\nthis information from trained LLMs, offers a promising solution to mitigate\nthese risks. However, previous practices face three key challenges: 1. Utility:\nsuccessful unlearning often causes catastrophic collapse on unrelated tasks. 2.\nEfficiency: many methods either involve adding similarly sized models, which\nslows down unlearning or inference, or require retain data that are difficult\nto obtain. 3. Robustness: even effective methods may still leak data via\nextraction techniques. To address these challenges, we propose MEOW, a simple\nyet effective gradient descent-based unlearning method. Specifically, we use an\noffline LLM to generate a set of inverted facts. Then, we design a new metric,\nMEMO, to quantify memorization in LLMs. Finally, based on the signals provided\nby MEMO, we select the most appropriate set of inverted facts and finetune the\nmodel based on them. We evaluate MEOW on the commonly used unlearn benchmark,\nToFU, with Llama2-7B-Chat and Phi-1.5B, and test it on both NLU and NLG tasks.\nResults demonstrate significant improvement of MEOW in forget quality without\nsubstantial loss in model utility. Meanwhile, MEOW does not exhibit significant\ndegradation in NLU or NLG capabilities, and there is even a slight improvement\nin NLU performance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) can memorize sensitive information, raising\nconcerns about potential misuse. LLM Unlearning, a post-hoc approach to remove\nthis information from trained LLMs, offers a promising solution to mitigate\nthese risks. However, previous practices face three key challenges: 1. Utility:\nsuccessful unlearning often causes catastrophic collapse on unrelated tasks. 2.\nEfficiency: many methods either involve adding similarly sized models, which\nslows down unlearning or inference, or require retain data that are difficult\nto obtain. 3. Robustness: even effective methods may still leak data via\nextraction techniques. To address these challenges, we propose MEOW, a simple\nyet effective gradient descent-based unlearning method. Specifically, we use an\noffline LLM to generate a set of inverted facts. Then, we design a new metric,\nMEMO, to quantify memorization in LLMs. Finally, based on the signals provided\nby MEMO, we select the most appropriate set of inverted facts and finetune the\nmodel based on them. We evaluate MEOW on the commonly used unlearn benchmark,\nToFU, with Llama2-7B-Chat and Phi-1.5B, and test it on both NLU and NLG tasks.\nResults demonstrate significant improvement of MEOW in forget quality without\nsubstantial loss in model utility. Meanwhile, MEOW does not exhibit significant\ndegradation in NLU or NLG capabilities, and there is even a slight improvement\nin NLU performance."
                },
                "authors": [
                    {
                        "name": "Tianle Gu"
                    },
                    {
                        "name": "Kexin Huang"
                    },
                    {
                        "name": "Ruilin Luo"
                    },
                    {
                        "name": "Yuanqi Yao"
                    },
                    {
                        "name": "Yujiu Yang"
                    },
                    {
                        "name": "Yan Teng"
                    },
                    {
                        "name": "Yingchun Wang"
                    }
                ],
                "author_detail": {
                    "name": "Yingchun Wang"
                },
                "author": "Yingchun Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.11844v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.11844v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.14507v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.14507v3",
                "updated": "2024-09-18T09:25:20Z",
                "updated_parsed": [
                    2024,
                    9,
                    18,
                    9,
                    25,
                    20,
                    2,
                    262,
                    0
                ],
                "published": "2024-07-19T17:59:03Z",
                "published_parsed": [
                    2024,
                    7,
                    19,
                    17,
                    59,
                    3,
                    4,
                    201,
                    0
                ],
                "title": "Internal Consistency and Self-Feedback in Large Language Models: A\n  Survey",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Internal Consistency and Self-Feedback in Large Language Models: A\n  Survey"
                },
                "summary": "Large language models (LLMs) often exhibit deficient reasoning or generate\nhallucinations. To address these, studies prefixed with \"Self-\" such as\nSelf-Consistency, Self-Improve, and Self-Refine have been initiated. They share\na commonality: involving LLMs evaluating and updating themselves. Nonetheless,\nthese efforts lack a unified perspective on summarization, as existing surveys\npredominantly focus on categorization.\n  In this paper, we use a unified perspective of internal consistency, offering\nexplanations for reasoning deficiencies and hallucinations. Internal\nconsistency refers to the consistency in expressions among LLMs' latent,\ndecoding, or response layers based on sampling methodologies. Then, we\nintroduce an effective theoretical framework capable of mining internal\nconsistency, named Self-Feedback. This framework consists of two modules:\nSelf-Evaluation and Self-Update. The former captures internal consistency\nsignals, while the latter leverages the signals to enhance either the model's\nresponse or the model itself. This framework has been employed in numerous\nstudies.\n  We systematically classify these studies by tasks and lines of work;\nsummarize relevant evaluation methods and benchmarks; and delve into the\nconcern, \"Does Self-Feedback Really Work?\" We also propose several critical\nviewpoints, including the \"Hourglass Evolution of Internal Consistency\",\n\"Consistency Is (Almost) Correctness\" hypothesis, and \"The Paradox of Latent\nand Explicit Reasoning\". The relevant resources are open-sourced at\nhttps://github.com/IAAR-Shanghai/ICSFSurvey.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) often exhibit deficient reasoning or generate\nhallucinations. To address these, studies prefixed with \"Self-\" such as\nSelf-Consistency, Self-Improve, and Self-Refine have been initiated. They share\na commonality: involving LLMs evaluating and updating themselves. Nonetheless,\nthese efforts lack a unified perspective on summarization, as existing surveys\npredominantly focus on categorization.\n  In this paper, we use a unified perspective of internal consistency, offering\nexplanations for reasoning deficiencies and hallucinations. Internal\nconsistency refers to the consistency in expressions among LLMs' latent,\ndecoding, or response layers based on sampling methodologies. Then, we\nintroduce an effective theoretical framework capable of mining internal\nconsistency, named Self-Feedback. This framework consists of two modules:\nSelf-Evaluation and Self-Update. The former captures internal consistency\nsignals, while the latter leverages the signals to enhance either the model's\nresponse or the model itself. This framework has been employed in numerous\nstudies.\n  We systematically classify these studies by tasks and lines of work;\nsummarize relevant evaluation methods and benchmarks; and delve into the\nconcern, \"Does Self-Feedback Really Work?\" We also propose several critical\nviewpoints, including the \"Hourglass Evolution of Internal Consistency\",\n\"Consistency Is (Almost) Correctness\" hypothesis, and \"The Paradox of Latent\nand Explicit Reasoning\". The relevant resources are open-sourced at\nhttps://github.com/IAAR-Shanghai/ICSFSurvey."
                },
                "authors": [
                    {
                        "name": "Xun Liang"
                    },
                    {
                        "name": "Shichao Song"
                    },
                    {
                        "name": "Zifan Zheng"
                    },
                    {
                        "name": "Hanyu Wang"
                    },
                    {
                        "name": "Qingchen Yu"
                    },
                    {
                        "name": "Xunkai Li"
                    },
                    {
                        "name": "Rong-Hua Li"
                    },
                    {
                        "name": "Yi Wang"
                    },
                    {
                        "name": "Zhonghao Wang"
                    },
                    {
                        "name": "Feiyu Xiong"
                    },
                    {
                        "name": "Zhiyu Li"
                    }
                ],
                "author_detail": {
                    "name": "Zhiyu Li"
                },
                "author": "Zhiyu Li",
                "arxiv_comment": "20 pages, 10 figures, 6 tables, 13 equations",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.14507v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.14507v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.11817v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.11817v1",
                "updated": "2024-09-18T09:08:16Z",
                "updated_parsed": [
                    2024,
                    9,
                    18,
                    9,
                    8,
                    16,
                    2,
                    262,
                    0
                ],
                "published": "2024-09-18T09:08:16Z",
                "published_parsed": [
                    2024,
                    9,
                    18,
                    9,
                    8,
                    16,
                    2,
                    262,
                    0
                ],
                "title": "EFCM: Efficient Fine-tuning on Compressed Models for deployment of large\n  models in medical image analysis",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "EFCM: Efficient Fine-tuning on Compressed Models for deployment of large\n  models in medical image analysis"
                },
                "summary": "The recent development of deep learning large models in medicine shows\nremarkable performance in medical image analysis and diagnosis, but their large\nnumber of parameters causes memory and inference latency challenges. Knowledge\ndistillation offers a solution, but the slide-level gradients cannot be\nbackpropagated for student model updates due to high-resolution pathological\nimages and slide-level labels. This study presents an Efficient Fine-tuning on\nCompressed Models (EFCM) framework with two stages: unsupervised feature\ndistillation and fine-tuning. In the distillation stage, Feature Projection\nDistillation (FPD) is proposed with a TransScan module for adaptive receptive\nfield adjustment to enhance the knowledge absorption capability of the student\nmodel. In the slide-level fine-tuning stage, three strategies (Reuse CLAM,\nRetrain CLAM, and End2end Train CLAM (ETC)) are compared. Experiments are\nconducted on 11 downstream datasets related to three large medical models:\nRETFound for retina, MRM for chest X-ray, and BROW for histopathology. The\nexperimental results demonstrate that the EFCM framework significantly improves\naccuracy and efficiency in handling slide-level pathological image problems,\neffectively addressing the challenges of deploying large medical models.\nSpecifically, it achieves a 4.33% increase in ACC and a 5.2% increase in AUC\ncompared to the large model BROW on the TCGA-NSCLC and TCGA-BRCA datasets. The\nanalysis of model inference efficiency highlights the high efficiency of the\ndistillation fine-tuning method.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The recent development of deep learning large models in medicine shows\nremarkable performance in medical image analysis and diagnosis, but their large\nnumber of parameters causes memory and inference latency challenges. Knowledge\ndistillation offers a solution, but the slide-level gradients cannot be\nbackpropagated for student model updates due to high-resolution pathological\nimages and slide-level labels. This study presents an Efficient Fine-tuning on\nCompressed Models (EFCM) framework with two stages: unsupervised feature\ndistillation and fine-tuning. In the distillation stage, Feature Projection\nDistillation (FPD) is proposed with a TransScan module for adaptive receptive\nfield adjustment to enhance the knowledge absorption capability of the student\nmodel. In the slide-level fine-tuning stage, three strategies (Reuse CLAM,\nRetrain CLAM, and End2end Train CLAM (ETC)) are compared. Experiments are\nconducted on 11 downstream datasets related to three large medical models:\nRETFound for retina, MRM for chest X-ray, and BROW for histopathology. The\nexperimental results demonstrate that the EFCM framework significantly improves\naccuracy and efficiency in handling slide-level pathological image problems,\neffectively addressing the challenges of deploying large medical models.\nSpecifically, it achieves a 4.33% increase in ACC and a 5.2% increase in AUC\ncompared to the large model BROW on the TCGA-NSCLC and TCGA-BRCA datasets. The\nanalysis of model inference efficiency highlights the high efficiency of the\ndistillation fine-tuning method."
                },
                "authors": [
                    {
                        "name": "Shaojie Li"
                    },
                    {
                        "name": "Zhaoshuo Diao"
                    }
                ],
                "author_detail": {
                    "name": "Zhaoshuo Diao"
                },
                "author": "Zhaoshuo Diao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.11817v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.11817v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.10697v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.10697v2",
                "updated": "2024-09-18T08:57:00Z",
                "updated_parsed": [
                    2024,
                    9,
                    18,
                    8,
                    57,
                    0,
                    2,
                    262,
                    0
                ],
                "published": "2024-09-16T19:54:42Z",
                "published_parsed": [
                    2024,
                    9,
                    16,
                    19,
                    54,
                    42,
                    0,
                    260,
                    0
                ],
                "title": "LLMs as information warriors? Auditing how LLM-powered chatbots tackle\n  disinformation about Russia's war in Ukraine",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLMs as information warriors? Auditing how LLM-powered chatbots tackle\n  disinformation about Russia's war in Ukraine"
                },
                "summary": "The rise of large language models (LLMs) has a significant impact on\ninformation warfare. By facilitating the production of content related to\ndisinformation and propaganda campaigns, LLMs can amplify different types of\ninformation operations and mislead online users. In our study, we empirically\ninvestigate how LLM-powered chatbots, developed by Google, Microsoft, and\nPerplexity, handle disinformation about Russia's war in Ukraine and whether the\nchatbots' ability to provide accurate information on the topic varies across\nlanguages and over time. Our findings indicate that while for some chatbots\n(Perplexity), there is a significant improvement in performance over time in\nseveral languages, for others (Gemini), the performance improves only in\nEnglish but deteriorates in low-resource languages.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rise of large language models (LLMs) has a significant impact on\ninformation warfare. By facilitating the production of content related to\ndisinformation and propaganda campaigns, LLMs can amplify different types of\ninformation operations and mislead online users. In our study, we empirically\ninvestigate how LLM-powered chatbots, developed by Google, Microsoft, and\nPerplexity, handle disinformation about Russia's war in Ukraine and whether the\nchatbots' ability to provide accurate information on the topic varies across\nlanguages and over time. Our findings indicate that while for some chatbots\n(Perplexity), there is a significant improvement in performance over time in\nseveral languages, for others (Gemini), the performance improves only in\nEnglish but deteriorates in low-resource languages."
                },
                "authors": [
                    {
                        "name": "Mykola Makhortykh"
                    },
                    {
                        "name": "Ani Baghumyan"
                    },
                    {
                        "name": "Victoria Vziatysheva"
                    },
                    {
                        "name": "Maryna Sydorova"
                    },
                    {
                        "name": "Elizaveta Kuznetsova"
                    }
                ],
                "author_detail": {
                    "name": "Elizaveta Kuznetsova"
                },
                "author": "Elizaveta Kuznetsova",
                "arxiv_comment": "25 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.10697v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.10697v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.11799v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.11799v1",
                "updated": "2024-09-18T08:31:09Z",
                "updated_parsed": [
                    2024,
                    9,
                    18,
                    8,
                    31,
                    9,
                    2,
                    262,
                    0
                ],
                "published": "2024-09-18T08:31:09Z",
                "published_parsed": [
                    2024,
                    9,
                    18,
                    8,
                    31,
                    9,
                    2,
                    262,
                    0
                ],
                "title": "Age-of-Information and Energy Optimization in Digital Twin Edge Networks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Age-of-Information and Energy Optimization in Digital Twin Edge Networks"
                },
                "summary": "In this paper, we study the intricate realm of digital twin synchronization\nand deployment in multi-access edge computing (MEC) networks, with the aim of\noptimizing and balancing the two performance metrics Age of Information (AoI)\nand energy efficiency. We jointly consider the problems of edge association,\npower allocation, and digital twin deployment. However, the inherent randomness\nof the problem presents a significant challenge in identifying an optimal\nsolution. To address this, we first analyze the feasibility conditions of the\noptimization problem. We then examine a specific scenario involving a static\nchannel and propose a cyclic scheduling scheme. This enables us to derive the\nsum AoI in closed form. As a result, the joint optimization problem of edge\nassociation and power control is solved optimally by finding a minimum weight\nperfect matching. Moreover, we examine the one-shot optimization problem in the\ncontexts of both frequent digital twin migrations and fixed digital twin\ndeployments, and propose an efficient online algorithm to address the general\noptimization problem. This algorithm effectively reduces system costs by\nbalancing frequent migrations and fixed deployments. Numerical results\ndemonstrate the effectiveness of our proposed scheme in terms of low cost and\nhigh efficiency.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this paper, we study the intricate realm of digital twin synchronization\nand deployment in multi-access edge computing (MEC) networks, with the aim of\noptimizing and balancing the two performance metrics Age of Information (AoI)\nand energy efficiency. We jointly consider the problems of edge association,\npower allocation, and digital twin deployment. However, the inherent randomness\nof the problem presents a significant challenge in identifying an optimal\nsolution. To address this, we first analyze the feasibility conditions of the\noptimization problem. We then examine a specific scenario involving a static\nchannel and propose a cyclic scheduling scheme. This enables us to derive the\nsum AoI in closed form. As a result, the joint optimization problem of edge\nassociation and power control is solved optimally by finding a minimum weight\nperfect matching. Moreover, we examine the one-shot optimization problem in the\ncontexts of both frequent digital twin migrations and fixed digital twin\ndeployments, and propose an efficient online algorithm to address the general\noptimization problem. This algorithm effectively reduces system costs by\nbalancing frequent migrations and fixed deployments. Numerical results\ndemonstrate the effectiveness of our proposed scheme in terms of low cost and\nhigh efficiency."
                },
                "authors": [
                    {
                        "name": "Yongna Guo"
                    },
                    {
                        "name": "Yaru Fu"
                    },
                    {
                        "name": "Yan Zhang"
                    },
                    {
                        "name": "Tony Q. S. Quek"
                    }
                ],
                "author_detail": {
                    "name": "Tony Q. S. Quek"
                },
                "author": "Tony Q. S. Quek",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.11799v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.11799v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.11798v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.11798v1",
                "updated": "2024-09-18T08:30:20Z",
                "updated_parsed": [
                    2024,
                    9,
                    18,
                    8,
                    30,
                    20,
                    2,
                    262,
                    0
                ],
                "published": "2024-09-18T08:30:20Z",
                "published_parsed": [
                    2024,
                    9,
                    18,
                    8,
                    30,
                    20,
                    2,
                    262,
                    0
                ],
                "title": "The Factuality of Large Language Models in the Legal Domain",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Factuality of Large Language Models in the Legal Domain"
                },
                "summary": "This paper investigates the factuality of large language models (LLMs) as\nknowledge bases in the legal domain, in a realistic usage scenario: we allow\nfor acceptable variations in the answer, and let the model abstain from\nanswering when uncertain. First, we design a dataset of diverse factual\nquestions about case law and legislation. We then use the dataset to evaluate\nseveral LLMs under different evaluation methods, including exact, alias, and\nfuzzy matching. Our results show that the performance improves significantly\nunder the alias and fuzzy matching methods. Further, we explore the impact of\nabstaining and in-context examples, finding that both strategies enhance\nprecision. Finally, we demonstrate that additional pre-training on legal\ndocuments, as seen with SaulLM, further improves factual precision from 63% to\n81%.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper investigates the factuality of large language models (LLMs) as\nknowledge bases in the legal domain, in a realistic usage scenario: we allow\nfor acceptable variations in the answer, and let the model abstain from\nanswering when uncertain. First, we design a dataset of diverse factual\nquestions about case law and legislation. We then use the dataset to evaluate\nseveral LLMs under different evaluation methods, including exact, alias, and\nfuzzy matching. Our results show that the performance improves significantly\nunder the alias and fuzzy matching methods. Further, we explore the impact of\nabstaining and in-context examples, finding that both strategies enhance\nprecision. Finally, we demonstrate that additional pre-training on legal\ndocuments, as seen with SaulLM, further improves factual precision from 63% to\n81%."
                },
                "authors": [
                    {
                        "name": "Rajaa El Hamdani"
                    },
                    {
                        "name": "Thomas Bonald"
                    },
                    {
                        "name": "Fragkiskos Malliaros"
                    },
                    {
                        "name": "Nils Holzenberger"
                    },
                    {
                        "name": "Fabian Suchanek"
                    }
                ],
                "author_detail": {
                    "name": "Fabian Suchanek"
                },
                "author": "Fabian Suchanek",
                "arxiv_comment": "CIKM 2024, short paper",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.11798v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.11798v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.15540v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.15540v2",
                "updated": "2024-09-18T08:21:29Z",
                "updated_parsed": [
                    2024,
                    9,
                    18,
                    8,
                    21,
                    29,
                    2,
                    262,
                    0
                ],
                "published": "2024-06-21T17:39:57Z",
                "published_parsed": [
                    2024,
                    6,
                    21,
                    17,
                    39,
                    57,
                    4,
                    173,
                    0
                ],
                "title": "Specify What? Enhancing Neural Specification Synthesis by Symbolic\n  Methods",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Specify What? Enhancing Neural Specification Synthesis by Symbolic\n  Methods"
                },
                "summary": "We investigate how combinations of Large Language Models (LLMs) and symbolic\nanalyses can be used to synthesise specifications of C programs. The LLM\nprompts are augmented with outputs from two formal methods tools in the Frama-C\necosystem, Pathcrawler and EVA, to produce C program annotations in the\nspecification language ACSL. We demonstrate how the addition of symbolic\nanalysis to the workflow impacts the quality of annotations: information about\ninput/output examples from Pathcrawler produce more context-aware annotations,\nwhile the inclusion of EVA reports yields annotations more attuned to runtime\nerrors. In addition, we show that the method infers rather the programs intent\nthan its behaviour, by generating specifications for buggy programs and\nobserving robustness of the result against bugs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We investigate how combinations of Large Language Models (LLMs) and symbolic\nanalyses can be used to synthesise specifications of C programs. The LLM\nprompts are augmented with outputs from two formal methods tools in the Frama-C\necosystem, Pathcrawler and EVA, to produce C program annotations in the\nspecification language ACSL. We demonstrate how the addition of symbolic\nanalysis to the workflow impacts the quality of annotations: information about\ninput/output examples from Pathcrawler produce more context-aware annotations,\nwhile the inclusion of EVA reports yields annotations more attuned to runtime\nerrors. In addition, we show that the method infers rather the programs intent\nthan its behaviour, by generating specifications for buggy programs and\nobserving robustness of the result against bugs."
                },
                "authors": [
                    {
                        "name": "George Granberry"
                    },
                    {
                        "name": "Wolfgang Ahrendt"
                    },
                    {
                        "name": "Moa Johansson"
                    }
                ],
                "author_detail": {
                    "name": "Moa Johansson"
                },
                "author": "Moa Johansson",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.15540v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.15540v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.FL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.11783v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.11783v2",
                "updated": "2024-09-20T04:13:33Z",
                "updated_parsed": [
                    2024,
                    9,
                    20,
                    4,
                    13,
                    33,
                    4,
                    264,
                    0
                ],
                "published": "2024-09-18T08:07:37Z",
                "published_parsed": [
                    2024,
                    9,
                    18,
                    8,
                    7,
                    37,
                    2,
                    262,
                    0
                ],
                "title": "Development and bilingual evaluation of Japanese medical large language\n  model within reasonably low computational resources",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Development and bilingual evaluation of Japanese medical large language\n  model within reasonably low computational resources"
                },
                "summary": "The recent success of large language models (LLMs) and the scaling law has\nled to a widespread adoption of larger models. Particularly in the healthcare\nindustry, there is an increasing demand for locally operated LLMs due to\nsecurity concerns. However, the majority of high quality open-source LLMs have\na size of 70B parameters, imposing significant financial burdens on users for\nGPU preparation and operation. To overcome these issues, we present a medical\nadaptation based on the recent 7B models, which enables the operation in low\ncomputational resources. We compare the performance on medical\nquestion-answering benchmarks in two languages (Japanese and English),\ndemonstrating that its scores reach parity with or surpass those of currently\nexisting medical LLMs that are ten times larger. We find that fine-tuning an\nEnglish-centric base model on Japanese medical dataset improves the score in\nboth language, supporting the effect of cross-lingual knowledge transfer. We\nhope that this study will alleviate financial challenges, serving as a stepping\nstone for clinical institutions to practically utilize LLMs locally. Our\nevaluation code is available at\nhttps://github.com/stardust-coder/japanese-lm-med-harness.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The recent success of large language models (LLMs) and the scaling law has\nled to a widespread adoption of larger models. Particularly in the healthcare\nindustry, there is an increasing demand for locally operated LLMs due to\nsecurity concerns. However, the majority of high quality open-source LLMs have\na size of 70B parameters, imposing significant financial burdens on users for\nGPU preparation and operation. To overcome these issues, we present a medical\nadaptation based on the recent 7B models, which enables the operation in low\ncomputational resources. We compare the performance on medical\nquestion-answering benchmarks in two languages (Japanese and English),\ndemonstrating that its scores reach parity with or surpass those of currently\nexisting medical LLMs that are ten times larger. We find that fine-tuning an\nEnglish-centric base model on Japanese medical dataset improves the score in\nboth language, supporting the effect of cross-lingual knowledge transfer. We\nhope that this study will alleviate financial challenges, serving as a stepping\nstone for clinical institutions to practically utilize LLMs locally. Our\nevaluation code is available at\nhttps://github.com/stardust-coder/japanese-lm-med-harness."
                },
                "authors": [
                    {
                        "name": "Issey Sukeda"
                    }
                ],
                "author_detail": {
                    "name": "Issey Sukeda"
                },
                "author": "Issey Sukeda",
                "arxiv_comment": "18 pages, 9 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.11783v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.11783v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.01272v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.01272v3",
                "updated": "2024-09-18T07:58:55Z",
                "updated_parsed": [
                    2024,
                    9,
                    18,
                    7,
                    58,
                    55,
                    2,
                    262,
                    0
                ],
                "published": "2024-07-01T13:25:33Z",
                "published_parsed": [
                    2024,
                    7,
                    1,
                    13,
                    25,
                    33,
                    0,
                    183,
                    0
                ],
                "title": "Show Less, Instruct More: Enriching Prompts with Definitions and\n  Guidelines for Zero-Shot NER",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Show Less, Instruct More: Enriching Prompts with Definitions and\n  Guidelines for Zero-Shot NER"
                },
                "summary": "Recently, several specialized instruction-tuned Large Language Models (LLMs)\nfor Named Entity Recognition (NER) have emerged. Compared to traditional NER\napproaches, these models have demonstrated strong generalization capabilities.\nExisting LLMs primarily focus on addressing zero-shot NER on Out-of-Domain\ninputs, while fine-tuning on an extensive number of entity classes that often\nhighly or completely overlap with test sets. In this work instead, we propose\nSLIMER, an approach designed to tackle never-seen-before entity tags by\ninstructing the model on fewer examples, and by leveraging a prompt enriched\nwith definition and guidelines. Experiments demonstrate that definition and\nguidelines yield better performance, faster and more robust learning,\nparticularly when labelling unseen named entities. Furthermore, SLIMER performs\ncomparably to state-of-the-art approaches in out-of-domain zero-shot NER, while\nbeing trained in a more fair, though certainly more challenging, setting.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recently, several specialized instruction-tuned Large Language Models (LLMs)\nfor Named Entity Recognition (NER) have emerged. Compared to traditional NER\napproaches, these models have demonstrated strong generalization capabilities.\nExisting LLMs primarily focus on addressing zero-shot NER on Out-of-Domain\ninputs, while fine-tuning on an extensive number of entity classes that often\nhighly or completely overlap with test sets. In this work instead, we propose\nSLIMER, an approach designed to tackle never-seen-before entity tags by\ninstructing the model on fewer examples, and by leveraging a prompt enriched\nwith definition and guidelines. Experiments demonstrate that definition and\nguidelines yield better performance, faster and more robust learning,\nparticularly when labelling unseen named entities. Furthermore, SLIMER performs\ncomparably to state-of-the-art approaches in out-of-domain zero-shot NER, while\nbeing trained in a more fair, though certainly more challenging, setting."
                },
                "authors": [
                    {
                        "name": "Andrew Zamai"
                    },
                    {
                        "name": "Andrea Zugarini"
                    },
                    {
                        "name": "Leonardo Rigutini"
                    },
                    {
                        "name": "Marco Ernandes"
                    },
                    {
                        "name": "Marco Maggini"
                    }
                ],
                "author_detail": {
                    "name": "Marco Maggini"
                },
                "author": "Marco Maggini",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.01272v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.01272v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2401.12708v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2401.12708v2",
                "updated": "2024-09-18T07:48:33Z",
                "updated_parsed": [
                    2024,
                    9,
                    18,
                    7,
                    48,
                    33,
                    2,
                    262,
                    0
                ],
                "published": "2024-01-23T12:15:47Z",
                "published_parsed": [
                    2024,
                    1,
                    23,
                    12,
                    15,
                    47,
                    1,
                    23,
                    0
                ],
                "title": "Deep Neural Network Benchmarks for Selective Classification",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Deep Neural Network Benchmarks for Selective Classification"
                },
                "summary": "With the increasing deployment of machine learning models in many socially\nsensitive tasks, there is a growing demand for reliable and trustworthy\npredictions. One way to accomplish these requirements is to allow a model to\nabstain from making a prediction when there is a high risk of making an error.\nThis requires adding a selection mechanism to the model, which selects those\nexamples for which the model will provide a prediction. The selective\nclassification framework aims to design a mechanism that balances the fraction\nof rejected predictions (i.e., the proportion of examples for which the model\ndoes not make a prediction) versus the improvement in predictive performance on\nthe selected predictions. Multiple selective classification frameworks exist,\nmost of which rely on deep neural network architectures. However, the empirical\nevaluation of the existing approaches is still limited to partial comparisons\namong methods and settings, providing practitioners with little insight into\ntheir relative merits. We fill this gap by benchmarking 18 baselines on a\ndiverse set of 44 datasets that includes both image and tabular data. Moreover,\nthere is a mix of binary and multiclass tasks. We evaluate these approaches\nusing several criteria, including selective error rate, empirical coverage,\ndistribution of rejected instance's classes, and performance on\nout-of-distribution instances. The results indicate that there is not a single\nclear winner among the surveyed baselines, and the best method depends on the\nusers' objectives.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the increasing deployment of machine learning models in many socially\nsensitive tasks, there is a growing demand for reliable and trustworthy\npredictions. One way to accomplish these requirements is to allow a model to\nabstain from making a prediction when there is a high risk of making an error.\nThis requires adding a selection mechanism to the model, which selects those\nexamples for which the model will provide a prediction. The selective\nclassification framework aims to design a mechanism that balances the fraction\nof rejected predictions (i.e., the proportion of examples for which the model\ndoes not make a prediction) versus the improvement in predictive performance on\nthe selected predictions. Multiple selective classification frameworks exist,\nmost of which rely on deep neural network architectures. However, the empirical\nevaluation of the existing approaches is still limited to partial comparisons\namong methods and settings, providing practitioners with little insight into\ntheir relative merits. We fill this gap by benchmarking 18 baselines on a\ndiverse set of 44 datasets that includes both image and tabular data. Moreover,\nthere is a mix of binary and multiclass tasks. We evaluate these approaches\nusing several criteria, including selective error rate, empirical coverage,\ndistribution of rejected instance's classes, and performance on\nout-of-distribution instances. The results indicate that there is not a single\nclear winner among the surveyed baselines, and the best method depends on the\nusers' objectives."
                },
                "authors": [
                    {
                        "name": "Andrea Pugnana"
                    },
                    {
                        "name": "Lorenzo Perini"
                    },
                    {
                        "name": "Jesse Davis"
                    },
                    {
                        "name": "Salvatore Ruggieri"
                    }
                ],
                "author_detail": {
                    "name": "Salvatore Ruggieri"
                },
                "author": "Salvatore Ruggieri",
                "arxiv_comment": "Published in The Journal of Data centric Machine Learning Research\n  (DMLR), Vol 1, (17):1-58 (2024)",
                "arxiv_journal_ref": "Journal of Data-centric Machine Learning Research (DMLR), Vol 1,\n  (17):1-58, (2024)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2401.12708v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2401.12708v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.04464v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.04464v2",
                "updated": "2024-09-18T07:43:12Z",
                "updated_parsed": [
                    2024,
                    9,
                    18,
                    7,
                    43,
                    12,
                    2,
                    262,
                    0
                ],
                "published": "2024-09-03T07:25:01Z",
                "published_parsed": [
                    2024,
                    9,
                    3,
                    7,
                    25,
                    1,
                    1,
                    247,
                    0
                ],
                "title": "Leveraging Large Language Models for Solving Rare MIP Challenges",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Leveraging Large Language Models for Solving Rare MIP Challenges"
                },
                "summary": "Mixed Integer Programming (MIP) has been extensively applied in areas\nrequiring mathematical solvers to address complex instances within tight time\nconstraints. However, as the problem scale increases, the complexity of model\nformulation and finding feasible solutions escalates significantly. In\ncontrast, the model-building cost for end-to-end models, such as large language\nmodels (LLMs), remains largely unaffected by problem scale due to their pattern\nrecognition capabilities. While LLMs, like GPT-4, without fine-tuning, can\nhandle some traditional medium-scale MIP problems, they struggle with uncommon\nor highly specialized MIP scenarios. Fine-tuning LLMs can yield some feasible\nsolutions for medium-scale MIP instances, but these models typically fail to\nexplore diverse solutions when constrained by a low and constant temperature,\nlimiting their performance. In this paper, we propose and evaluate a\nrecursively dynamic temperature method integrated with a chain-of-thought\napproach. Our findings show that starting with a high temperature and gradually\nlowering it leads to better feasible solutions compared to other dynamic\ntemperature strategies. Additionally, by comparing results generated by the LLM\nwith those from Gurobi, we demonstrate that the LLM can produce solutions that\ncomplement traditional solvers by accelerating the pruning process and\nimproving overall efficiency.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mixed Integer Programming (MIP) has been extensively applied in areas\nrequiring mathematical solvers to address complex instances within tight time\nconstraints. However, as the problem scale increases, the complexity of model\nformulation and finding feasible solutions escalates significantly. In\ncontrast, the model-building cost for end-to-end models, such as large language\nmodels (LLMs), remains largely unaffected by problem scale due to their pattern\nrecognition capabilities. While LLMs, like GPT-4, without fine-tuning, can\nhandle some traditional medium-scale MIP problems, they struggle with uncommon\nor highly specialized MIP scenarios. Fine-tuning LLMs can yield some feasible\nsolutions for medium-scale MIP instances, but these models typically fail to\nexplore diverse solutions when constrained by a low and constant temperature,\nlimiting their performance. In this paper, we propose and evaluate a\nrecursively dynamic temperature method integrated with a chain-of-thought\napproach. Our findings show that starting with a high temperature and gradually\nlowering it leads to better feasible solutions compared to other dynamic\ntemperature strategies. Additionally, by comparing results generated by the LLM\nwith those from Gurobi, we demonstrate that the LLM can produce solutions that\ncomplement traditional solvers by accelerating the pruning process and\nimproving overall efficiency."
                },
                "authors": [
                    {
                        "name": "Teng Wang"
                    },
                    {
                        "name": "Wing-Yin Yu"
                    },
                    {
                        "name": "Ruifeng She"
                    },
                    {
                        "name": "Wenhan Yang"
                    },
                    {
                        "name": "Taijie Chen"
                    },
                    {
                        "name": "Jianping Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Jianping Zhang"
                },
                "author": "Jianping Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.04464v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.04464v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.OC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.10195v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.10195v2",
                "updated": "2024-09-18T07:26:55Z",
                "updated_parsed": [
                    2024,
                    9,
                    18,
                    7,
                    26,
                    55,
                    2,
                    262,
                    0
                ],
                "published": "2024-07-14T13:34:00Z",
                "published_parsed": [
                    2024,
                    7,
                    14,
                    13,
                    34,
                    0,
                    6,
                    196,
                    0
                ],
                "title": "V2I-Calib: A Novel Calibration Approach for Collaborative Vehicle and\n  Infrastructure LiDAR Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "V2I-Calib: A Novel Calibration Approach for Collaborative Vehicle and\n  Infrastructure LiDAR Systems"
                },
                "summary": "Cooperative LiDAR systems integrating vehicles and road infrastructure,\ntermed V2I calibration, exhibit substantial potential, yet their deployment\nencounters numerous challenges. A pivotal aspect of ensuring data accuracy and\nconsistency across such systems involves the calibration of LiDAR units across\nheterogeneous vehicular and infrastructural endpoints. This necessitates the\ndevelopment of calibration methods that are both real-time and robust,\nparticularly those that can ensure robust performance in urban canyon scenarios\nwithout relying on initial positioning values. Accordingly, this paper\nintroduces a novel approach to V2I calibration, leveraging spatial association\ninformation among perceived objects. Central to this method is the innovative\nOverall Intersection over Union (oIoU) metric, which quantifies the correlation\nbetween targets identified by vehicle and infrastructure systems, thereby\nfacilitating the real-time monitoring of calibration results. Our approach\ninvolves identifying common targets within the perception results of vehicle\nand infrastructure LiDAR systems through the construction of an affinity\nmatrix. These common targets then form the basis for the calculation and\noptimization of extrinsic parameters. Comparative and ablation studies\nconducted using the DAIR-V2X dataset substantiate the superiority of our\napproach. For further insights and resources, our project repository is\naccessible at https://github.com/MassimoQu/v2i-calib.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cooperative LiDAR systems integrating vehicles and road infrastructure,\ntermed V2I calibration, exhibit substantial potential, yet their deployment\nencounters numerous challenges. A pivotal aspect of ensuring data accuracy and\nconsistency across such systems involves the calibration of LiDAR units across\nheterogeneous vehicular and infrastructural endpoints. This necessitates the\ndevelopment of calibration methods that are both real-time and robust,\nparticularly those that can ensure robust performance in urban canyon scenarios\nwithout relying on initial positioning values. Accordingly, this paper\nintroduces a novel approach to V2I calibration, leveraging spatial association\ninformation among perceived objects. Central to this method is the innovative\nOverall Intersection over Union (oIoU) metric, which quantifies the correlation\nbetween targets identified by vehicle and infrastructure systems, thereby\nfacilitating the real-time monitoring of calibration results. Our approach\ninvolves identifying common targets within the perception results of vehicle\nand infrastructure LiDAR systems through the construction of an affinity\nmatrix. These common targets then form the basis for the calculation and\noptimization of extrinsic parameters. Comparative and ablation studies\nconducted using the DAIR-V2X dataset substantiate the superiority of our\napproach. For further insights and resources, our project repository is\naccessible at https://github.com/MassimoQu/v2i-calib."
                },
                "authors": [
                    {
                        "name": "Qianxin Qu"
                    },
                    {
                        "name": "Yijin Xiong"
                    },
                    {
                        "name": "Guipeng Zhang"
                    },
                    {
                        "name": "Xin Wu"
                    },
                    {
                        "name": "Xiaohan Gao"
                    },
                    {
                        "name": "Xin Gao"
                    },
                    {
                        "name": "Hanyu Li"
                    },
                    {
                        "name": "Shichun Guo"
                    },
                    {
                        "name": "Guoying Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Guoying Zhang"
                },
                "author": "Guoying Zhang",
                "arxiv_comment": "IROS2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.10195v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.10195v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.10927v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.10927v2",
                "updated": "2024-09-18T07:23:50Z",
                "updated_parsed": [
                    2024,
                    9,
                    18,
                    7,
                    23,
                    50,
                    2,
                    262,
                    0
                ],
                "published": "2024-09-17T06:51:59Z",
                "published_parsed": [
                    2024,
                    9,
                    17,
                    6,
                    51,
                    59,
                    1,
                    261,
                    0
                ],
                "title": "Propulsion: Steering LLM with Tiny Fine-Tuning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Propulsion: Steering LLM with Tiny Fine-Tuning"
                },
                "summary": "The rapid advancements in Large Language Models (LLMs) have revolutionized\nnatural language processing (NLP) and related fields. However, fine-tuning\nthese models for specific tasks remains computationally expensive and risks\ndegrading pre-learned features. To address these challenges, we propose\nPropulsion, a novel parameter efficient fine-tuning (PEFT) method designed to\noptimize task-specific performance while drastically reducing computational\noverhead. Inspired by the concept of controlled adjustments in physical motion,\nPropulsion selectively re-scales specific dimensions of a pre-trained model,\nguiding output predictions toward task objectives without modifying the model's\nparameters. By introducing lightweight, trainable Propulsion parameters at the\npre-trained layer, we minimize the number of parameters updated during\nfine-tuning, preventing overfitting or overwriting of existing knowledge. Our\ntheoretical analysis, supported by Neural Tangent Kernel (NTK) theory, shows\nthat Propulsion approximates the performance of full fine-tuning with far fewer\ntrainable parameters. Empirically, Propulsion reduces the parameter count from\n355.3 million to just 0.086 million, achieving over a 10x reduction compared to\nstandard approaches like LoRA while maintaining competitive performance across\nbenchmarks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rapid advancements in Large Language Models (LLMs) have revolutionized\nnatural language processing (NLP) and related fields. However, fine-tuning\nthese models for specific tasks remains computationally expensive and risks\ndegrading pre-learned features. To address these challenges, we propose\nPropulsion, a novel parameter efficient fine-tuning (PEFT) method designed to\noptimize task-specific performance while drastically reducing computational\noverhead. Inspired by the concept of controlled adjustments in physical motion,\nPropulsion selectively re-scales specific dimensions of a pre-trained model,\nguiding output predictions toward task objectives without modifying the model's\nparameters. By introducing lightweight, trainable Propulsion parameters at the\npre-trained layer, we minimize the number of parameters updated during\nfine-tuning, preventing overfitting or overwriting of existing knowledge. Our\ntheoretical analysis, supported by Neural Tangent Kernel (NTK) theory, shows\nthat Propulsion approximates the performance of full fine-tuning with far fewer\ntrainable parameters. Empirically, Propulsion reduces the parameter count from\n355.3 million to just 0.086 million, achieving over a 10x reduction compared to\nstandard approaches like LoRA while maintaining competitive performance across\nbenchmarks."
                },
                "authors": [
                    {
                        "name": "Md Kowsher"
                    },
                    {
                        "name": "Nusrat Jahan Prottasha"
                    },
                    {
                        "name": "Prakash Bhat"
                    }
                ],
                "author_detail": {
                    "name": "Prakash Bhat"
                },
                "author": "Prakash Bhat",
                "arxiv_comment": "26 pages, 11 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.10927v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.10927v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.11041v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.11041v2",
                "updated": "2024-09-18T07:17:17Z",
                "updated_parsed": [
                    2024,
                    9,
                    18,
                    7,
                    17,
                    17,
                    2,
                    262,
                    0
                ],
                "published": "2024-09-17T10:04:50Z",
                "published_parsed": [
                    2024,
                    9,
                    17,
                    10,
                    4,
                    50,
                    1,
                    261,
                    0
                ],
                "title": "Towards No-Code Programming of Cobots: Experiments with Code Synthesis\n  by Large Code Models for Conversational Programming",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Towards No-Code Programming of Cobots: Experiments with Code Synthesis\n  by Large Code Models for Conversational Programming"
                },
                "summary": "While there has been a lot of research recently on robots in household\nenvironments, at the present time, most robots in existence can be found on\nshop floors, and most interactions between humans and robots happen there.\n``Collaborative robots'' (cobots) designed to work alongside humans on assembly\nlines traditionally require expert programming, limiting ability to make\nchanges, or manual guidance, limiting expressivity of the resulting programs.\nTo address these limitations, we explore using Large Language Models (LLMs),\nand in particular, their abilities of doing in-context learning, for\nconversational code generation. As a first step, we define RATS, the\n``Repetitive Assembly Task'', a 2D building task designed to lay the foundation\nfor simulating industry assembly scenarios. In this task, a `programmer'\ninstructs a cobot, using natural language, on how a certain assembly is to be\nbuilt; that is, the programmer induces a program, through natural language. We\ncreate a dataset that pairs target structures with various example instructions\n(human-authored, template-based, and model-generated) and example code. With\nthis, we systematically evaluate the capabilities of state-of-the-art LLMs for\nsynthesising this kind of code, given in-context examples. Evaluating in a\nsimulated environment, we find that LLMs are capable of generating accurate\n`first order code' (instruction sequences), but have problems producing\n`higher-order code' (abstractions such as functions, or use of loops).",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While there has been a lot of research recently on robots in household\nenvironments, at the present time, most robots in existence can be found on\nshop floors, and most interactions between humans and robots happen there.\n``Collaborative robots'' (cobots) designed to work alongside humans on assembly\nlines traditionally require expert programming, limiting ability to make\nchanges, or manual guidance, limiting expressivity of the resulting programs.\nTo address these limitations, we explore using Large Language Models (LLMs),\nand in particular, their abilities of doing in-context learning, for\nconversational code generation. As a first step, we define RATS, the\n``Repetitive Assembly Task'', a 2D building task designed to lay the foundation\nfor simulating industry assembly scenarios. In this task, a `programmer'\ninstructs a cobot, using natural language, on how a certain assembly is to be\nbuilt; that is, the programmer induces a program, through natural language. We\ncreate a dataset that pairs target structures with various example instructions\n(human-authored, template-based, and model-generated) and example code. With\nthis, we systematically evaluate the capabilities of state-of-the-art LLMs for\nsynthesising this kind of code, given in-context examples. Evaluating in a\nsimulated environment, we find that LLMs are capable of generating accurate\n`first order code' (instruction sequences), but have problems producing\n`higher-order code' (abstractions such as functions, or use of loops)."
                },
                "authors": [
                    {
                        "name": "Chalamalasetti Kranti"
                    },
                    {
                        "name": "Sherzod Hakimov"
                    },
                    {
                        "name": "David Schlangen"
                    }
                ],
                "author_detail": {
                    "name": "David Schlangen"
                },
                "author": "David Schlangen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.11041v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.11041v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.06787v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.06787v2",
                "updated": "2024-09-18T07:12:28Z",
                "updated_parsed": [
                    2024,
                    9,
                    18,
                    7,
                    12,
                    28,
                    2,
                    262,
                    0
                ],
                "published": "2024-08-13T10:15:55Z",
                "published_parsed": [
                    2024,
                    8,
                    13,
                    10,
                    15,
                    55,
                    1,
                    226,
                    0
                ],
                "title": "Unlock the Power of Frozen LLMs in Knowledge Graph Completion",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Unlock the Power of Frozen LLMs in Knowledge Graph Completion"
                },
                "summary": "Traditional knowledge graph completion (KGC) methods rely solely on\nstructural information, struggling with the inherent sparsity of knowledge\ngraphs (KGs). Large Language Models (LLMs) learn extensive knowledge from large\ncorpora with powerful context modeling, making them promising for mitigating\nthe limitations of previous methods. Directly fine-tuning LLMs offers great\ncapability but comes at the cost of huge time and memory consumption, while\nutilizing frozen LLMs yields suboptimal results.In this work, we aim to\nleverage LLMs for KGC effectively and efficiently. We capture the context-aware\nhidden states of knowledge triples by employing prompts to stimulate the\nintermediate layers of LLMs. We then train a data-efficient classifier on these\nhidden states to harness the inherent capabilities of frozen LLMs in KGC.\nAdditionally, to reduce ambiguity and enrich knowledge representation, we\ngenerate detailed entity descriptions through subgraph sampling on KGs.\nExtensive experiments on standard benchmarks demonstrate the efficiency and\neffectiveness of our approach. We outperform traditional KGC methods across\nmost datasets and, notably, achieve classification performance comparable to\nfine-tuned LLMs while enhancing GPU memory efficiency by $188\\times$ and\naccelerating training and inference by $13.48\\times$.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Traditional knowledge graph completion (KGC) methods rely solely on\nstructural information, struggling with the inherent sparsity of knowledge\ngraphs (KGs). Large Language Models (LLMs) learn extensive knowledge from large\ncorpora with powerful context modeling, making them promising for mitigating\nthe limitations of previous methods. Directly fine-tuning LLMs offers great\ncapability but comes at the cost of huge time and memory consumption, while\nutilizing frozen LLMs yields suboptimal results.In this work, we aim to\nleverage LLMs for KGC effectively and efficiently. We capture the context-aware\nhidden states of knowledge triples by employing prompts to stimulate the\nintermediate layers of LLMs. We then train a data-efficient classifier on these\nhidden states to harness the inherent capabilities of frozen LLMs in KGC.\nAdditionally, to reduce ambiguity and enrich knowledge representation, we\ngenerate detailed entity descriptions through subgraph sampling on KGs.\nExtensive experiments on standard benchmarks demonstrate the efficiency and\neffectiveness of our approach. We outperform traditional KGC methods across\nmost datasets and, notably, achieve classification performance comparable to\nfine-tuned LLMs while enhancing GPU memory efficiency by $188\\times$ and\naccelerating training and inference by $13.48\\times$."
                },
                "authors": [
                    {
                        "name": "Bo Xue"
                    },
                    {
                        "name": "Yi Xu"
                    },
                    {
                        "name": "Yunchong Song"
                    },
                    {
                        "name": "Yiming Pang"
                    },
                    {
                        "name": "Yuyang Ren"
                    },
                    {
                        "name": "Jiaxin Ding"
                    },
                    {
                        "name": "Luoyi Fu"
                    },
                    {
                        "name": "Xinbing Wang"
                    }
                ],
                "author_detail": {
                    "name": "Xinbing Wang"
                },
                "author": "Xinbing Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.06787v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.06787v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.07970v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.07970v3",
                "updated": "2024-09-18T07:06:02Z",
                "updated_parsed": [
                    2024,
                    9,
                    18,
                    7,
                    6,
                    2,
                    2,
                    262,
                    0
                ],
                "published": "2024-06-12T07:49:36Z",
                "published_parsed": [
                    2024,
                    6,
                    12,
                    7,
                    49,
                    36,
                    2,
                    164,
                    0
                ],
                "title": "Guiding In-Context Learning of LLMs through Quality Estimation for\n  Machine Translation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Guiding In-Context Learning of LLMs through Quality Estimation for\n  Machine Translation"
                },
                "summary": "The quality of output from large language models (LLMs), particularly in\nmachine translation (MT), is closely tied to the quality of in-context examples\n(ICEs) provided along with the query, i.e., the text to translate. The\neffectiveness of these ICEs is influenced by various factors, such as the\ndomain of the source text, the order in which the ICEs are presented, the\nnumber of these examples, and the prompt templates used. Naturally, selecting\nthe most impactful ICEs depends on understanding how these affect the resulting\ntranslation quality, which ultimately relies on translation references or human\njudgment. This paper presents a novel methodology for in-context learning (ICL)\nthat relies on a search algorithm guided by domain-specific quality estimation\n(QE). Leveraging the XGLM model, our methodology estimates the resulting\ntranslation quality without the need for translation references, selecting\neffective ICEs for MT to maximize translation quality. Our results demonstrate\nsignificant improvements over existing ICL methods and higher translation\nperformance compared to fine-tuning a pre-trained language model (PLM),\nspecifically mBART-50.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The quality of output from large language models (LLMs), particularly in\nmachine translation (MT), is closely tied to the quality of in-context examples\n(ICEs) provided along with the query, i.e., the text to translate. The\neffectiveness of these ICEs is influenced by various factors, such as the\ndomain of the source text, the order in which the ICEs are presented, the\nnumber of these examples, and the prompt templates used. Naturally, selecting\nthe most impactful ICEs depends on understanding how these affect the resulting\ntranslation quality, which ultimately relies on translation references or human\njudgment. This paper presents a novel methodology for in-context learning (ICL)\nthat relies on a search algorithm guided by domain-specific quality estimation\n(QE). Leveraging the XGLM model, our methodology estimates the resulting\ntranslation quality without the need for translation references, selecting\neffective ICEs for MT to maximize translation quality. Our results demonstrate\nsignificant improvements over existing ICL methods and higher translation\nperformance compared to fine-tuning a pre-trained language model (PLM),\nspecifically mBART-50."
                },
                "authors": [
                    {
                        "name": "Javad Pourmostafa Roshan Sharami"
                    },
                    {
                        "name": "Dimitar Shterionov"
                    },
                    {
                        "name": "Pieter Spronck"
                    }
                ],
                "author_detail": {
                    "name": "Pieter Spronck"
                },
                "author": "Pieter Spronck",
                "arxiv_comment": "Camera-ready version of the paper for the Association for Machine\n  Translation in the Americas (AMTA), including the link to the paper's\n  repository",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.07970v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.07970v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.11741v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.11741v1",
                "updated": "2024-09-18T06:54:36Z",
                "updated_parsed": [
                    2024,
                    9,
                    18,
                    6,
                    54,
                    36,
                    2,
                    262,
                    0
                ],
                "published": "2024-09-18T06:54:36Z",
                "published_parsed": [
                    2024,
                    9,
                    18,
                    6,
                    54,
                    36,
                    2,
                    262,
                    0
                ],
                "title": "HARP: Human-Assisted Regrouping with Permutation Invariant Critic for\n  Multi-Agent Reinforcement Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "HARP: Human-Assisted Regrouping with Permutation Invariant Critic for\n  Multi-Agent Reinforcement Learning"
                },
                "summary": "Human-in-the-loop reinforcement learning integrates human expertise to\naccelerate agent learning and provide critical guidance and feedback in complex\nfields. However, many existing approaches focus on single-agent tasks and\nrequire continuous human involvement during the training process, significantly\nincreasing the human workload and limiting scalability. In this paper, we\npropose HARP (Human-Assisted Regrouping with Permutation Invariant Critic), a\nmulti-agent reinforcement learning framework designed for group-oriented tasks.\nHARP integrates automatic agent regrouping with strategic human assistance\nduring deployment, enabling and allowing non-experts to offer effective\nguidance with minimal intervention. During training, agents dynamically adjust\ntheir groupings to optimize collaborative task completion. When deployed, they\nactively seek human assistance and utilize the Permutation Invariant Group\nCritic to evaluate and refine human-proposed groupings, allowing non-expert\nusers to contribute valuable suggestions. In multiple collaboration scenarios,\nour approach is able to leverage limited guidance from non-experts and enhance\nperformance. The project can be found at https://github.com/huawen-hu/HARP.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Human-in-the-loop reinforcement learning integrates human expertise to\naccelerate agent learning and provide critical guidance and feedback in complex\nfields. However, many existing approaches focus on single-agent tasks and\nrequire continuous human involvement during the training process, significantly\nincreasing the human workload and limiting scalability. In this paper, we\npropose HARP (Human-Assisted Regrouping with Permutation Invariant Critic), a\nmulti-agent reinforcement learning framework designed for group-oriented tasks.\nHARP integrates automatic agent regrouping with strategic human assistance\nduring deployment, enabling and allowing non-experts to offer effective\nguidance with minimal intervention. During training, agents dynamically adjust\ntheir groupings to optimize collaborative task completion. When deployed, they\nactively seek human assistance and utilize the Permutation Invariant Group\nCritic to evaluate and refine human-proposed groupings, allowing non-expert\nusers to contribute valuable suggestions. In multiple collaboration scenarios,\nour approach is able to leverage limited guidance from non-experts and enhance\nperformance. The project can be found at https://github.com/huawen-hu/HARP."
                },
                "authors": [
                    {
                        "name": "Huawen Hu"
                    },
                    {
                        "name": "Enze Shi"
                    },
                    {
                        "name": "Chenxi Yue"
                    },
                    {
                        "name": "Shuocun Yang"
                    },
                    {
                        "name": "Zihao Wu"
                    },
                    {
                        "name": "Yiwei Li"
                    },
                    {
                        "name": "Tianyang Zhong"
                    },
                    {
                        "name": "Tuo Zhang"
                    },
                    {
                        "name": "Tianming Liu"
                    },
                    {
                        "name": "Shu Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Shu Zhang"
                },
                "author": "Shu Zhang",
                "arxiv_comment": "7 pages, 6 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.11741v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.11741v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.11727v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.11727v1",
                "updated": "2024-09-18T06:27:26Z",
                "updated_parsed": [
                    2024,
                    9,
                    18,
                    6,
                    27,
                    26,
                    2,
                    262,
                    0
                ],
                "published": "2024-09-18T06:27:26Z",
                "published_parsed": [
                    2024,
                    9,
                    18,
                    6,
                    27,
                    26,
                    2,
                    262,
                    0
                ],
                "title": "Enabling Real-Time Conversations with Minimal Training Costs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Enabling Real-Time Conversations with Minimal Training Costs"
                },
                "summary": "Large language models (LLMs) have demonstrated the ability to improve human\nefficiency through conversational interactions. Conventional LLM-powered\ndialogue systems, operating on a turn-based paradigm, preclude real-time\ninteraction during response generation. To address this limitation, researchers\nhave proposed duplex models. These models can dynamically adapt to user input,\nfacilitating real-time interactive feedback. However, these methods typically\nrequire substantial computational resources to acquire the ability. To reduce\noverhead, this paper presents a new duplex decoding approach that enhances LLMs\nwith duplex ability, requiring minimal additional training. Specifically, our\nmethod employs parallel decoding of queries and responses in conversations,\neffectively implementing a channel-division-multiplexing decoding strategy.\nExperimental results indicate that our proposed method significantly enhances\nthe naturalness and human-likeness of user-AI interactions with minimal\ntraining costs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have demonstrated the ability to improve human\nefficiency through conversational interactions. Conventional LLM-powered\ndialogue systems, operating on a turn-based paradigm, preclude real-time\ninteraction during response generation. To address this limitation, researchers\nhave proposed duplex models. These models can dynamically adapt to user input,\nfacilitating real-time interactive feedback. However, these methods typically\nrequire substantial computational resources to acquire the ability. To reduce\noverhead, this paper presents a new duplex decoding approach that enhances LLMs\nwith duplex ability, requiring minimal additional training. Specifically, our\nmethod employs parallel decoding of queries and responses in conversations,\neffectively implementing a channel-division-multiplexing decoding strategy.\nExperimental results indicate that our proposed method significantly enhances\nthe naturalness and human-likeness of user-AI interactions with minimal\ntraining costs."
                },
                "authors": [
                    {
                        "name": "Wang Xu"
                    },
                    {
                        "name": "Shuo Wang"
                    },
                    {
                        "name": "Weilin Zhao"
                    },
                    {
                        "name": "Xu Han"
                    },
                    {
                        "name": "Yukun Yan"
                    },
                    {
                        "name": "Yudi Zhang"
                    },
                    {
                        "name": "Zhe Tao"
                    },
                    {
                        "name": "Zhiyuan Liu"
                    },
                    {
                        "name": "Wanxiang Che"
                    }
                ],
                "author_detail": {
                    "name": "Wanxiang Che"
                },
                "author": "Wanxiang Che",
                "arxiv_comment": "7pages, 6 figures, 1 table",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.11727v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.11727v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.11726v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.11726v1",
                "updated": "2024-09-18T06:21:44Z",
                "updated_parsed": [
                    2024,
                    9,
                    18,
                    6,
                    21,
                    44,
                    2,
                    262,
                    0
                ],
                "published": "2024-09-18T06:21:44Z",
                "published_parsed": [
                    2024,
                    9,
                    18,
                    6,
                    21,
                    44,
                    2,
                    262,
                    0
                ],
                "title": "Revealing the Challenge of Detecting Character Knowledge Errors in LLM\n  Role-Playing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Revealing the Challenge of Detecting Character Knowledge Errors in LLM\n  Role-Playing"
                },
                "summary": "Large language model (LLM) role-playing has gained widespread attention,\nwhere the authentic character knowledge is crucial for constructing realistic\nLLM role-playing agents. However, existing works usually overlook the\nexploration of LLMs' ability to detect characters' known knowledge errors (KKE)\nand unknown knowledge errors (UKE) while playing roles, which would lead to\nlow-quality automatic construction of character trainable corpus. In this\npaper, we propose a probing dataset to evaluate LLMs' ability to detect errors\nin KKE and UKE. The results indicate that even the latest LLMs struggle to\neffectively detect these two types of errors, especially when it comes to\nfamiliar knowledge. We experimented with various reasoning strategies and\npropose an agent-based reasoning method, Self-Recollection and Self-Doubt\n(S2RD), to further explore the potential for improving error detection\ncapabilities. Experiments show that our method effectively improves the LLMs'\nability to detect error character knowledge, but it remains an issue that\nrequires ongoing attention.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language model (LLM) role-playing has gained widespread attention,\nwhere the authentic character knowledge is crucial for constructing realistic\nLLM role-playing agents. However, existing works usually overlook the\nexploration of LLMs' ability to detect characters' known knowledge errors (KKE)\nand unknown knowledge errors (UKE) while playing roles, which would lead to\nlow-quality automatic construction of character trainable corpus. In this\npaper, we propose a probing dataset to evaluate LLMs' ability to detect errors\nin KKE and UKE. The results indicate that even the latest LLMs struggle to\neffectively detect these two types of errors, especially when it comes to\nfamiliar knowledge. We experimented with various reasoning strategies and\npropose an agent-based reasoning method, Self-Recollection and Self-Doubt\n(S2RD), to further explore the potential for improving error detection\ncapabilities. Experiments show that our method effectively improves the LLMs'\nability to detect error character knowledge, but it remains an issue that\nrequires ongoing attention."
                },
                "authors": [
                    {
                        "name": "Wenyuan Zhang"
                    },
                    {
                        "name": "Jiawei Sheng"
                    },
                    {
                        "name": "Shuaiyi Nie"
                    },
                    {
                        "name": "Zefeng Zhang"
                    },
                    {
                        "name": "Xinghua Zhang"
                    },
                    {
                        "name": "Yongquan He"
                    },
                    {
                        "name": "Tingwen Liu"
                    }
                ],
                "author_detail": {
                    "name": "Tingwen Liu"
                },
                "author": "Tingwen Liu",
                "arxiv_comment": "22 pages, 14 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.11726v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.11726v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.11725v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.11725v1",
                "updated": "2024-09-18T06:21:36Z",
                "updated_parsed": [
                    2024,
                    9,
                    18,
                    6,
                    21,
                    36,
                    2,
                    262,
                    0
                ],
                "published": "2024-09-18T06:21:36Z",
                "published_parsed": [
                    2024,
                    9,
                    18,
                    6,
                    21,
                    36,
                    2,
                    262,
                    0
                ],
                "title": "Dense-TSNet: Dense Connected Two-Stage Structure for Ultra-Lightweight\n  Speech Enhancement",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Dense-TSNet: Dense Connected Two-Stage Structure for Ultra-Lightweight\n  Speech Enhancement"
                },
                "summary": "Speech enhancement aims to improve speech quality and intelligibility in\nnoisy environments. Recent advancements have concentrated on deep neural\nnetworks, particularly employing the Two-Stage (TS) architecture to enhance\nfeature extraction. However, the complexity and size of these models remain\nsignificant, which limits their applicability in resource-constrained\nscenarios. Designing models suitable for edge devices presents its own set of\nchallenges. Narrow lightweight models often encounter performance bottlenecks\ndue to uneven loss landscapes. Additionally, advanced operators such as\nTransformers or Mamba may lack the practical adaptability and efficiency that\nconvolutional neural networks (CNNs) offer in real-world deployments. To\naddress these challenges, we propose Dense-TSNet, an innovative\nultra-lightweight speech enhancement network. Our approach employs a novel\nDense Two-Stage (Dense-TS) architecture, which, compared to the classic\nTwo-Stage architecture, ensures more robust refinement of the objective\nfunction in the later training stages. This leads to improved final\nperformance, addressing the early convergence limitations of the baseline\nmodel. We also introduce the Multi-View Gaze Block (MVGB), which enhances\nfeature extraction by incorporating global, channel, and local perspectives\nthrough convolutional neural networks (CNNs). Furthermore, we discuss how the\nchoice of loss function impacts perceptual quality. Dense-TSNet demonstrates\npromising performance with a compact model size of around 14K parameters,\nmaking it particularly well-suited for deployment in resource-constrained\nenvironments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Speech enhancement aims to improve speech quality and intelligibility in\nnoisy environments. Recent advancements have concentrated on deep neural\nnetworks, particularly employing the Two-Stage (TS) architecture to enhance\nfeature extraction. However, the complexity and size of these models remain\nsignificant, which limits their applicability in resource-constrained\nscenarios. Designing models suitable for edge devices presents its own set of\nchallenges. Narrow lightweight models often encounter performance bottlenecks\ndue to uneven loss landscapes. Additionally, advanced operators such as\nTransformers or Mamba may lack the practical adaptability and efficiency that\nconvolutional neural networks (CNNs) offer in real-world deployments. To\naddress these challenges, we propose Dense-TSNet, an innovative\nultra-lightweight speech enhancement network. Our approach employs a novel\nDense Two-Stage (Dense-TS) architecture, which, compared to the classic\nTwo-Stage architecture, ensures more robust refinement of the objective\nfunction in the later training stages. This leads to improved final\nperformance, addressing the early convergence limitations of the baseline\nmodel. We also introduce the Multi-View Gaze Block (MVGB), which enhances\nfeature extraction by incorporating global, channel, and local perspectives\nthrough convolutional neural networks (CNNs). Furthermore, we discuss how the\nchoice of loss function impacts perceptual quality. Dense-TSNet demonstrates\npromising performance with a compact model size of around 14K parameters,\nmaking it particularly well-suited for deployment in resource-constrained\nenvironments."
                },
                "authors": [
                    {
                        "name": "Zizhen Lin"
                    },
                    {
                        "name": "Yuanle Li"
                    },
                    {
                        "name": "Junyu Wang"
                    },
                    {
                        "name": "Ruili Li"
                    }
                ],
                "author_detail": {
                    "name": "Ruili Li"
                },
                "author": "Ruili Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.11725v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.11725v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.AS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.11724v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.11724v1",
                "updated": "2024-09-18T06:19:59Z",
                "updated_parsed": [
                    2024,
                    9,
                    18,
                    6,
                    19,
                    59,
                    2,
                    262,
                    0
                ],
                "published": "2024-09-18T06:19:59Z",
                "published_parsed": [
                    2024,
                    9,
                    18,
                    6,
                    19,
                    59,
                    2,
                    262,
                    0
                ],
                "title": "TART: An Open-Source Tool-Augmented Framework for Explainable\n  Table-based Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TART: An Open-Source Tool-Augmented Framework for Explainable\n  Table-based Reasoning"
                },
                "summary": "Current Large Language Models (LLMs) exhibit limited ability to understand\ntable structures and to apply precise numerical reasoning, which is crucial for\ntasks such as table question answering (TQA) and table-based fact verification\n(TFV). To address these challenges, we introduce our Tool-Augmented Reasoning\nframework for Tables (TART), which integrates LLMs with specialized tools. TART\ncontains three key components: a table formatter to ensure accurate data\nrepresentation, a tool maker to develop specific computational tools, and an\nexplanation generator to maintain explainability. We also present the TOOLTAB\ndataset, a new benchmark designed specifically for training LLMs in table-tool\nintegration. Our experiments indicate that TART achieves substantial\nimprovements over existing methods (e.g., Chain-of-Thought) by improving both\nthe precision of data processing and the clarity of the reasoning process.\nNotably, TART paired with CodeLlama achieves 90.0% of the accuracy of the\nclosed-sourced LLM GPT-3.5-turbo, highlighting its robustness in diverse\nreal-world scenarios. All the code and data are available at\nhttps://github.com/XinyuanLu00/TART.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Current Large Language Models (LLMs) exhibit limited ability to understand\ntable structures and to apply precise numerical reasoning, which is crucial for\ntasks such as table question answering (TQA) and table-based fact verification\n(TFV). To address these challenges, we introduce our Tool-Augmented Reasoning\nframework for Tables (TART), which integrates LLMs with specialized tools. TART\ncontains three key components: a table formatter to ensure accurate data\nrepresentation, a tool maker to develop specific computational tools, and an\nexplanation generator to maintain explainability. We also present the TOOLTAB\ndataset, a new benchmark designed specifically for training LLMs in table-tool\nintegration. Our experiments indicate that TART achieves substantial\nimprovements over existing methods (e.g., Chain-of-Thought) by improving both\nthe precision of data processing and the clarity of the reasoning process.\nNotably, TART paired with CodeLlama achieves 90.0% of the accuracy of the\nclosed-sourced LLM GPT-3.5-turbo, highlighting its robustness in diverse\nreal-world scenarios. All the code and data are available at\nhttps://github.com/XinyuanLu00/TART."
                },
                "authors": [
                    {
                        "name": "Xinyuan Lu"
                    },
                    {
                        "name": "Liangming Pan"
                    },
                    {
                        "name": "Yubo Ma"
                    },
                    {
                        "name": "Preslav Nakov"
                    },
                    {
                        "name": "Min-Yen Kan"
                    }
                ],
                "author_detail": {
                    "name": "Min-Yen Kan"
                },
                "author": "Min-Yen Kan",
                "arxiv_comment": "technical report",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.11724v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.11724v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.11283v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.11283v2",
                "updated": "2024-09-18T05:42:01Z",
                "updated_parsed": [
                    2024,
                    9,
                    18,
                    5,
                    42,
                    1,
                    2,
                    262,
                    0
                ],
                "published": "2024-09-17T15:38:36Z",
                "published_parsed": [
                    2024,
                    9,
                    17,
                    15,
                    38,
                    36,
                    1,
                    261,
                    0
                ],
                "title": "Zero-resource Hallucination Detection for Text Generation via\n  Graph-based Contextual Knowledge Triples Modeling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Zero-resource Hallucination Detection for Text Generation via\n  Graph-based Contextual Knowledge Triples Modeling"
                },
                "summary": "LLMs obtain remarkable performance but suffer from hallucinations. Most\nresearch on detecting hallucination focuses on the questions with short and\nconcrete correct answers that are easy to check the faithfulness. Hallucination\ndetections for text generation with open-ended answers are more challenging.\nSome researchers use external knowledge to detect hallucinations in generated\ntexts, but external resources for specific scenarios are hard to access. Recent\nstudies on detecting hallucinations in long text without external resources\nconduct consistency comparison among multiple sampled outputs. To handle long\ntexts, researchers split long texts into multiple facts and individually\ncompare the consistency of each pairs of facts. However, these methods (1)\nhardly achieve alignment among multiple facts; (2) overlook dependencies\nbetween multiple contextual facts. In this paper, we propose a graph-based\ncontext-aware (GCA) hallucination detection for text generations, which aligns\nknowledge facts and considers the dependencies between contextual knowledge\ntriples in consistency comparison. Particularly, to align multiple facts, we\nconduct a triple-oriented response segmentation to extract multiple knowledge\ntriples. To model dependencies among contextual knowledge triple (facts), we\nconstruct contextual triple into a graph and enhance triples' interactions via\nmessage passing and aggregating via RGCN. To avoid the omission of knowledge\ntriples in long text, we conduct a LLM-based reverse verification via\nreconstructing the knowledge triples. Experiments show that our model enhances\nhallucination detection and excels all baselines.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLMs obtain remarkable performance but suffer from hallucinations. Most\nresearch on detecting hallucination focuses on the questions with short and\nconcrete correct answers that are easy to check the faithfulness. Hallucination\ndetections for text generation with open-ended answers are more challenging.\nSome researchers use external knowledge to detect hallucinations in generated\ntexts, but external resources for specific scenarios are hard to access. Recent\nstudies on detecting hallucinations in long text without external resources\nconduct consistency comparison among multiple sampled outputs. To handle long\ntexts, researchers split long texts into multiple facts and individually\ncompare the consistency of each pairs of facts. However, these methods (1)\nhardly achieve alignment among multiple facts; (2) overlook dependencies\nbetween multiple contextual facts. In this paper, we propose a graph-based\ncontext-aware (GCA) hallucination detection for text generations, which aligns\nknowledge facts and considers the dependencies between contextual knowledge\ntriples in consistency comparison. Particularly, to align multiple facts, we\nconduct a triple-oriented response segmentation to extract multiple knowledge\ntriples. To model dependencies among contextual knowledge triple (facts), we\nconstruct contextual triple into a graph and enhance triples' interactions via\nmessage passing and aggregating via RGCN. To avoid the omission of knowledge\ntriples in long text, we conduct a LLM-based reverse verification via\nreconstructing the knowledge triples. Experiments show that our model enhances\nhallucination detection and excels all baselines."
                },
                "authors": [
                    {
                        "name": "Xinyue Fang"
                    },
                    {
                        "name": "Zhen Huang"
                    },
                    {
                        "name": "Zhiliang Tian"
                    },
                    {
                        "name": "Minghui Fang"
                    },
                    {
                        "name": "Ziyi Pan"
                    },
                    {
                        "name": "Quntian Fang"
                    },
                    {
                        "name": "Zhihua Wen"
                    },
                    {
                        "name": "Hengyue Pan"
                    },
                    {
                        "name": "Dongsheng Li"
                    }
                ],
                "author_detail": {
                    "name": "Dongsheng Li"
                },
                "author": "Dongsheng Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.11283v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.11283v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.00978v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.00978v2",
                "updated": "2024-09-18T05:28:12Z",
                "updated_parsed": [
                    2024,
                    9,
                    18,
                    5,
                    28,
                    12,
                    2,
                    262,
                    0
                ],
                "published": "2024-04-01T07:49:11Z",
                "published_parsed": [
                    2024,
                    4,
                    1,
                    7,
                    49,
                    11,
                    0,
                    92,
                    0
                ],
                "title": "Prior Constraints-based Reward Model Training for Aligning Large\n  Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Prior Constraints-based Reward Model Training for Aligning Large\n  Language Models"
                },
                "summary": "Reinforcement learning with human feedback for aligning large language models\n(LLMs) trains a reward model typically using ranking loss with comparison\npairs.However, the training procedure suffers from an inherent problem: the\nuncontrolled scaling of reward scores during reinforcement learning due to the\nlack of constraints while training the reward model.This paper proposes a Prior\nConstraints-based Reward Model (namely PCRM) training method to mitigate this\nproblem. PCRM incorporates prior constraints, specifically, length ratio and\ncosine similarity between outputs of each comparison pair, during reward model\ntraining to regulate optimization magnitude and control score margins. We\ncomprehensively evaluate PCRM by examining its rank correlation with human\npreferences and its effectiveness in aligning LLMs via RL. Experimental results\ndemonstrate that PCRM significantly improves alignment performance by\neffectively constraining reward score scaling. As another bonus, our method is\neasily integrated into arbitrary rank-based alignment methods, such as direct\npreference optimization, and can yield consistent improvement.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reinforcement learning with human feedback for aligning large language models\n(LLMs) trains a reward model typically using ranking loss with comparison\npairs.However, the training procedure suffers from an inherent problem: the\nuncontrolled scaling of reward scores during reinforcement learning due to the\nlack of constraints while training the reward model.This paper proposes a Prior\nConstraints-based Reward Model (namely PCRM) training method to mitigate this\nproblem. PCRM incorporates prior constraints, specifically, length ratio and\ncosine similarity between outputs of each comparison pair, during reward model\ntraining to regulate optimization magnitude and control score margins. We\ncomprehensively evaluate PCRM by examining its rank correlation with human\npreferences and its effectiveness in aligning LLMs via RL. Experimental results\ndemonstrate that PCRM significantly improves alignment performance by\neffectively constraining reward score scaling. As another bonus, our method is\neasily integrated into arbitrary rank-based alignment methods, such as direct\npreference optimization, and can yield consistent improvement."
                },
                "authors": [
                    {
                        "name": "Hang Zhou"
                    },
                    {
                        "name": "Chenglong Wang"
                    },
                    {
                        "name": "Yimin Hu"
                    },
                    {
                        "name": "Tong Xiao"
                    },
                    {
                        "name": "Chunliang Zhang"
                    },
                    {
                        "name": "Jingbo Zhu"
                    }
                ],
                "author_detail": {
                    "name": "Jingbo Zhu"
                },
                "author": "Jingbo Zhu",
                "arxiv_comment": "Accepted by CCL 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.00978v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.00978v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.11704v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.11704v1",
                "updated": "2024-09-18T05:13:18Z",
                "updated_parsed": [
                    2024,
                    9,
                    18,
                    5,
                    13,
                    18,
                    2,
                    262,
                    0
                ],
                "published": "2024-09-18T05:13:18Z",
                "published_parsed": [
                    2024,
                    9,
                    18,
                    5,
                    13,
                    18,
                    2,
                    262,
                    0
                ],
                "title": "From Lists to Emojis: How Format Bias Affects Model Alignment",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "From Lists to Emojis: How Format Bias Affects Model Alignment"
                },
                "summary": "In this paper, we study format biases in reinforcement learning from human\nfeedback (RLHF). We observe that many widely-used preference models, including\nhuman evaluators, GPT-4, and top-ranking models on the RewardBench benchmark,\nexhibit strong biases towards specific format patterns, such as lists, links,\nbold text, and emojis. Furthermore, large language models (LLMs) can exploit\nthese biases to achieve higher rankings on popular benchmarks like AlpacaEval\nand LMSYS Chatbot Arena. One notable example of this is verbosity bias, where\ncurrent preference models favor longer responses that appear more\ncomprehensive, even when their quality is equal to or lower than shorter,\ncompeting responses. However, format biases beyond verbosity remain largely\nunderexplored in the literature. In this work, we extend the study of biases in\npreference learning beyond the commonly recognized length bias, offering a\ncomprehensive analysis of a wider range of format biases. Additionally, we show\nthat with a small amount of biased data (less than 1%), we can inject\nsignificant bias into the reward model. Moreover, these format biases can also\nbe easily exploited by downstream alignment algorithms, such as best-of-n\nsampling and online iterative DPO, as it is usually easier to manipulate the\nformat than to improve the quality of responses. Our findings emphasize the\nneed to disentangle format and content both for designing alignment algorithms\nand evaluating models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this paper, we study format biases in reinforcement learning from human\nfeedback (RLHF). We observe that many widely-used preference models, including\nhuman evaluators, GPT-4, and top-ranking models on the RewardBench benchmark,\nexhibit strong biases towards specific format patterns, such as lists, links,\nbold text, and emojis. Furthermore, large language models (LLMs) can exploit\nthese biases to achieve higher rankings on popular benchmarks like AlpacaEval\nand LMSYS Chatbot Arena. One notable example of this is verbosity bias, where\ncurrent preference models favor longer responses that appear more\ncomprehensive, even when their quality is equal to or lower than shorter,\ncompeting responses. However, format biases beyond verbosity remain largely\nunderexplored in the literature. In this work, we extend the study of biases in\npreference learning beyond the commonly recognized length bias, offering a\ncomprehensive analysis of a wider range of format biases. Additionally, we show\nthat with a small amount of biased data (less than 1%), we can inject\nsignificant bias into the reward model. Moreover, these format biases can also\nbe easily exploited by downstream alignment algorithms, such as best-of-n\nsampling and online iterative DPO, as it is usually easier to manipulate the\nformat than to improve the quality of responses. Our findings emphasize the\nneed to disentangle format and content both for designing alignment algorithms\nand evaluating models."
                },
                "authors": [
                    {
                        "name": "Xuanchang Zhang"
                    },
                    {
                        "name": "Wei Xiong"
                    },
                    {
                        "name": "Lichang Chen"
                    },
                    {
                        "name": "Tianyi Zhou"
                    },
                    {
                        "name": "Heng Huang"
                    },
                    {
                        "name": "Tong Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Tong Zhang"
                },
                "author": "Tong Zhang",
                "arxiv_comment": "Working in progress",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.11704v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.11704v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.11703v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.11703v1",
                "updated": "2024-09-18T04:56:52Z",
                "updated_parsed": [
                    2024,
                    9,
                    18,
                    4,
                    56,
                    52,
                    2,
                    262,
                    0
                ],
                "published": "2024-09-18T04:56:52Z",
                "published_parsed": [
                    2024,
                    9,
                    18,
                    4,
                    56,
                    52,
                    2,
                    262,
                    0
                ],
                "title": "Harnessing LLMs for API Interactions: A Framework for Classification and\n  Synthetic Data Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Harnessing LLMs for API Interactions: A Framework for Classification and\n  Synthetic Data Generation"
                },
                "summary": "As Large Language Models (LLMs) advance in natural language processing, there\nis growing interest in leveraging their capabilities to simplify software\ninteractions. In this paper, we propose a novel system that integrates LLMs for\nboth classifying natural language inputs into corresponding API calls and\nautomating the creation of sample datasets tailored to specific API functions.\nBy classifying natural language commands, our system allows users to invoke\ncomplex software functionalities through simple inputs, improving interaction\nefficiency and lowering the barrier to software utilization. Our dataset\ngeneration approach also enables the efficient and systematic evaluation of\ndifferent LLMs in classifying API calls, offering a practical tool for\ndevelopers or business owners to assess the suitability of LLMs for customized\nAPI management. We conduct experiments on several prominent LLMs using\ngenerated sample datasets for various API functions. The results show that\nGPT-4 achieves a high classification accuracy of 0.996, while LLaMA-3-8B\nperforms much worse at 0.759. These findings highlight the potential of LLMs to\ntransform API management and validate the effectiveness of our system in\nguiding model testing and selection across diverse applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As Large Language Models (LLMs) advance in natural language processing, there\nis growing interest in leveraging their capabilities to simplify software\ninteractions. In this paper, we propose a novel system that integrates LLMs for\nboth classifying natural language inputs into corresponding API calls and\nautomating the creation of sample datasets tailored to specific API functions.\nBy classifying natural language commands, our system allows users to invoke\ncomplex software functionalities through simple inputs, improving interaction\nefficiency and lowering the barrier to software utilization. Our dataset\ngeneration approach also enables the efficient and systematic evaluation of\ndifferent LLMs in classifying API calls, offering a practical tool for\ndevelopers or business owners to assess the suitability of LLMs for customized\nAPI management. We conduct experiments on several prominent LLMs using\ngenerated sample datasets for various API functions. The results show that\nGPT-4 achieves a high classification accuracy of 0.996, while LLaMA-3-8B\nperforms much worse at 0.759. These findings highlight the potential of LLMs to\ntransform API management and validate the effectiveness of our system in\nguiding model testing and selection across diverse applications."
                },
                "authors": [
                    {
                        "name": "Chunliang Tao"
                    },
                    {
                        "name": "Xiaojing Fan"
                    },
                    {
                        "name": "Yahe Yang"
                    }
                ],
                "author_detail": {
                    "name": "Yahe Yang"
                },
                "author": "Yahe Yang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.11703v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.11703v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2312.05821v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2312.05821v3",
                "updated": "2024-09-18T04:53:46Z",
                "updated_parsed": [
                    2024,
                    9,
                    18,
                    4,
                    53,
                    46,
                    2,
                    262,
                    0
                ],
                "published": "2023-12-10T08:41:24Z",
                "published_parsed": [
                    2023,
                    12,
                    10,
                    8,
                    41,
                    24,
                    6,
                    344,
                    0
                ],
                "title": "ASVD: Activation-aware Singular Value Decomposition for Compressing\n  Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ASVD: Activation-aware Singular Value Decomposition for Compressing\n  Large Language Models"
                },
                "summary": "In this paper, we introduce a new post-training compression paradigm for\nLarge Language Models (LLMs) to facilitate their wider adoption. We delve into\nLLM weight low-rank factorization, and find that the challenges of this task\nstem from the outlier phenomenon in the LLM activations and the sensitivity\ndifference among various kinds of layers. To address these issues, we propose a\ntraining-free approach called Activation-aware Singular Value Decomposition\n(ASVD). Specifically, ASVD manages activation outliers by scaling the weight\nmatrix based on the activation distribution, thereby enhancing decomposition\naccuracy. Additionally, we propose an efficient iterative calibration process\nto optimize layer-specific decomposition by addressing the varying sensitivity\nof different LLM layers. ASVD can compress a network by 10-20%, without\ncompromising the performance of LLMs. Based on the success of the low-rank\ndecomposition of projection matrices in the self-attention module, we further\nintroduce ASVD to compress the KV cache. By reducing the channel dimension of\nKV activations, memory requirements for KV cache can be largely reduced. Thanks\nto the 50-75% reduction in the rank of the KV projection matrices, ASVD can\nfurther achieve 50% KV cache reductions without performance drop in a\ntraining-free manner.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this paper, we introduce a new post-training compression paradigm for\nLarge Language Models (LLMs) to facilitate their wider adoption. We delve into\nLLM weight low-rank factorization, and find that the challenges of this task\nstem from the outlier phenomenon in the LLM activations and the sensitivity\ndifference among various kinds of layers. To address these issues, we propose a\ntraining-free approach called Activation-aware Singular Value Decomposition\n(ASVD). Specifically, ASVD manages activation outliers by scaling the weight\nmatrix based on the activation distribution, thereby enhancing decomposition\naccuracy. Additionally, we propose an efficient iterative calibration process\nto optimize layer-specific decomposition by addressing the varying sensitivity\nof different LLM layers. ASVD can compress a network by 10-20%, without\ncompromising the performance of LLMs. Based on the success of the low-rank\ndecomposition of projection matrices in the self-attention module, we further\nintroduce ASVD to compress the KV cache. By reducing the channel dimension of\nKV activations, memory requirements for KV cache can be largely reduced. Thanks\nto the 50-75% reduction in the rank of the KV projection matrices, ASVD can\nfurther achieve 50% KV cache reductions without performance drop in a\ntraining-free manner."
                },
                "authors": [
                    {
                        "name": "Zhihang Yuan"
                    },
                    {
                        "name": "Yuzhang Shang"
                    },
                    {
                        "name": "Yue Song"
                    },
                    {
                        "name": "Qiang Wu"
                    },
                    {
                        "name": "Yan Yan"
                    },
                    {
                        "name": "Guangyu Sun"
                    }
                ],
                "author_detail": {
                    "name": "Guangyu Sun"
                },
                "author": "Guangyu Sun",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2312.05821v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2312.05821v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.11700v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.11700v1",
                "updated": "2024-09-18T04:46:17Z",
                "updated_parsed": [
                    2024,
                    9,
                    18,
                    4,
                    46,
                    17,
                    2,
                    262,
                    0
                ],
                "published": "2024-09-18T04:46:17Z",
                "published_parsed": [
                    2024,
                    9,
                    18,
                    4,
                    46,
                    17,
                    2,
                    262,
                    0
                ],
                "title": "Real-Time Sound Event Localization and Detection: Deployment Challenges\n  on Edge Devices",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Real-Time Sound Event Localization and Detection: Deployment Challenges\n  on Edge Devices"
                },
                "summary": "Sound event localization and detection (SELD) is critical for various\nreal-world applications, including smart monitoring and Internet of Things\n(IoT) systems. Although deep neural networks (DNNs) represent the\nstate-of-the-art approach for SELD, their significant computational complexity\nand model sizes present challenges for deployment on resource-constrained edge\ndevices, especially under real-time conditions. Despite the growing need for\nreal-time SELD, research in this area remains limited. In this paper, we\ninvestigate the unique challenges of deploying SELD systems for real-world,\nreal-time applications by performing extensive experiments on a commercially\navailable Raspberry Pi 3 edge device. Our findings reveal two critical, often\noverlooked considerations: the high computational cost of feature extraction\nand the performance degradation associated with low-latency, real-time\ninference. This paper provides valuable insights and considerations for future\nwork toward developing more efficient and robust real-time SELD systems",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Sound event localization and detection (SELD) is critical for various\nreal-world applications, including smart monitoring and Internet of Things\n(IoT) systems. Although deep neural networks (DNNs) represent the\nstate-of-the-art approach for SELD, their significant computational complexity\nand model sizes present challenges for deployment on resource-constrained edge\ndevices, especially under real-time conditions. Despite the growing need for\nreal-time SELD, research in this area remains limited. In this paper, we\ninvestigate the unique challenges of deploying SELD systems for real-world,\nreal-time applications by performing extensive experiments on a commercially\navailable Raspberry Pi 3 edge device. Our findings reveal two critical, often\noverlooked considerations: the high computational cost of feature extraction\nand the performance degradation associated with low-latency, real-time\ninference. This paper provides valuable insights and considerations for future\nwork toward developing more efficient and robust real-time SELD systems"
                },
                "authors": [
                    {
                        "name": "Jun Wei Yeow"
                    },
                    {
                        "name": "Ee-Leng Tan"
                    },
                    {
                        "name": "Jisheng Bai"
                    },
                    {
                        "name": "Santi Peksi"
                    },
                    {
                        "name": "Woon-Seng Gan"
                    }
                ],
                "author_detail": {
                    "name": "Woon-Seng Gan"
                },
                "author": "Woon-Seng Gan",
                "arxiv_comment": "Submitted to ICASSP'25. Code is available at this link :\n  https://github.com/itsjunwei/Realtime-SELD-Edge",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.11700v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.11700v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SP",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.11696v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.11696v1",
                "updated": "2024-09-18T04:32:08Z",
                "updated_parsed": [
                    2024,
                    9,
                    18,
                    4,
                    32,
                    8,
                    2,
                    262,
                    0
                ],
                "published": "2024-09-18T04:32:08Z",
                "published_parsed": [
                    2024,
                    9,
                    18,
                    4,
                    32,
                    8,
                    2,
                    262,
                    0
                ],
                "title": "RMP-YOLO: A Robust Motion Predictor for Partially Observable Scenarios\n  even if You Only Look Once",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RMP-YOLO: A Robust Motion Predictor for Partially Observable Scenarios\n  even if You Only Look Once"
                },
                "summary": "We introduce RMP-YOLO, a unified framework designed to provide robust motion\npredictions even with incomplete input data. Our key insight stems from the\nobservation that complete and reliable historical trajectory data plays a\npivotal role in ensuring accurate motion prediction. Therefore, we propose a\nnew paradigm that prioritizes the reconstruction of intact historical\ntrajectories before feeding them into the prediction modules. Our approach\nintroduces a novel scene tokenization module to enhance the extraction and\nfusion of spatial and temporal features. Following this, our proposed recovery\nmodule reconstructs agents' incomplete historical trajectories by leveraging\nlocal map topology and interactions with nearby agents. The reconstructed,\nclean historical data is then integrated into the downstream prediction\nmodules. Our framework is able to effectively handle missing data of varying\nlengths and remains robust against observation noise, while maintaining high\nprediction accuracy. Furthermore, our recovery module is compatible with\nexisting prediction models, ensuring seamless integration. Extensive\nexperiments validate the effectiveness of our approach, and deployment in\nreal-world autonomous vehicles confirms its practical utility. In the 2024\nWaymo Motion Prediction Competition, our method, RMP-YOLO, achieves\nstate-of-the-art performance, securing third place.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce RMP-YOLO, a unified framework designed to provide robust motion\npredictions even with incomplete input data. Our key insight stems from the\nobservation that complete and reliable historical trajectory data plays a\npivotal role in ensuring accurate motion prediction. Therefore, we propose a\nnew paradigm that prioritizes the reconstruction of intact historical\ntrajectories before feeding them into the prediction modules. Our approach\nintroduces a novel scene tokenization module to enhance the extraction and\nfusion of spatial and temporal features. Following this, our proposed recovery\nmodule reconstructs agents' incomplete historical trajectories by leveraging\nlocal map topology and interactions with nearby agents. The reconstructed,\nclean historical data is then integrated into the downstream prediction\nmodules. Our framework is able to effectively handle missing data of varying\nlengths and remains robust against observation noise, while maintaining high\nprediction accuracy. Furthermore, our recovery module is compatible with\nexisting prediction models, ensuring seamless integration. Extensive\nexperiments validate the effectiveness of our approach, and deployment in\nreal-world autonomous vehicles confirms its practical utility. In the 2024\nWaymo Motion Prediction Competition, our method, RMP-YOLO, achieves\nstate-of-the-art performance, securing third place."
                },
                "authors": [
                    {
                        "name": "Jiawei Sun"
                    },
                    {
                        "name": "Jiahui Li"
                    },
                    {
                        "name": "Tingchen Liu"
                    },
                    {
                        "name": "Chengran Yuan"
                    },
                    {
                        "name": "Shuo Sun"
                    },
                    {
                        "name": "Zefan Huang"
                    },
                    {
                        "name": "Anthony Wong"
                    },
                    {
                        "name": "Keng Peng Tee"
                    },
                    {
                        "name": "Marcelo H. Ang Jr"
                    }
                ],
                "author_detail": {
                    "name": "Marcelo H. Ang Jr"
                },
                "author": "Marcelo H. Ang Jr",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.11696v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.11696v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.11690v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.11690v2",
                "updated": "2024-09-19T03:28:21Z",
                "updated_parsed": [
                    2024,
                    9,
                    19,
                    3,
                    28,
                    21,
                    3,
                    263,
                    0
                ],
                "published": "2024-09-18T04:10:44Z",
                "published_parsed": [
                    2024,
                    9,
                    18,
                    4,
                    10,
                    44,
                    2,
                    262,
                    0
                ],
                "title": "LLM-Powered Text Simulation Attack Against ID-Free Recommender Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM-Powered Text Simulation Attack Against ID-Free Recommender Systems"
                },
                "summary": "The ID-free recommendation paradigm has been proposed to address the\nlimitation that traditional recommender systems struggle to model cold-start\nusers or items with new IDs. Despite its effectiveness, this study uncovers\nthat ID-free recommender systems are vulnerable to the proposed Text Simulation\nattack (TextSimu) which aims to promote specific target items. As a novel type\nof text poisoning attack, TextSimu exploits large language models (LLM) to\nalter the textual information of target items by simulating the characteristics\nof popular items. It operates effectively in both black-box and white-box\nsettings, utilizing two key components: a unified popularity extraction module,\nwhich captures the essential characteristics of popular items, and an N-persona\nconsistency simulation strategy, which creates multiple personas to\ncollaboratively synthesize refined promotional textual descriptions for target\nitems by simulating the popular items. To withstand TextSimu-like attacks, we\nfurther explore the detection approach for identifying LLM-generated\npromotional text. Extensive experiments conducted on three datasets demonstrate\nthat TextSimu poses a more significant threat than existing poisoning attacks,\nwhile our defense method can detect malicious text of target items generated by\nTextSimu. By identifying the vulnerability, we aim to advance the development\nof more robust ID-free recommender systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The ID-free recommendation paradigm has been proposed to address the\nlimitation that traditional recommender systems struggle to model cold-start\nusers or items with new IDs. Despite its effectiveness, this study uncovers\nthat ID-free recommender systems are vulnerable to the proposed Text Simulation\nattack (TextSimu) which aims to promote specific target items. As a novel type\nof text poisoning attack, TextSimu exploits large language models (LLM) to\nalter the textual information of target items by simulating the characteristics\nof popular items. It operates effectively in both black-box and white-box\nsettings, utilizing two key components: a unified popularity extraction module,\nwhich captures the essential characteristics of popular items, and an N-persona\nconsistency simulation strategy, which creates multiple personas to\ncollaboratively synthesize refined promotional textual descriptions for target\nitems by simulating the popular items. To withstand TextSimu-like attacks, we\nfurther explore the detection approach for identifying LLM-generated\npromotional text. Extensive experiments conducted on three datasets demonstrate\nthat TextSimu poses a more significant threat than existing poisoning attacks,\nwhile our defense method can detect malicious text of target items generated by\nTextSimu. By identifying the vulnerability, we aim to advance the development\nof more robust ID-free recommender systems."
                },
                "authors": [
                    {
                        "name": "Zongwei Wang"
                    },
                    {
                        "name": "Min Gao"
                    },
                    {
                        "name": "Junliang Yu"
                    },
                    {
                        "name": "Xinyi Gao"
                    },
                    {
                        "name": "Quoc Viet Hung Nguyen"
                    },
                    {
                        "name": "Shazia Sadiq"
                    },
                    {
                        "name": "Hongzhi Yin"
                    }
                ],
                "author_detail": {
                    "name": "Hongzhi Yin"
                },
                "author": "Hongzhi Yin",
                "arxiv_comment": "12 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.11690v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.11690v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.11680v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.11680v1",
                "updated": "2024-09-18T03:41:15Z",
                "updated_parsed": [
                    2024,
                    9,
                    18,
                    3,
                    41,
                    15,
                    2,
                    262,
                    0
                ],
                "published": "2024-09-18T03:41:15Z",
                "published_parsed": [
                    2024,
                    9,
                    18,
                    3,
                    41,
                    15,
                    2,
                    262,
                    0
                ],
                "title": "What to Consider When Considering Differential Privacy for Policy",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "What to Consider When Considering Differential Privacy for Policy"
                },
                "summary": "Differential privacy (DP) is a mathematical definition of privacy that can be\nwidely applied when publishing data. DP has been recognized as a potential\nmeans of adhering to various privacy-related legal requirements. However, it\ncan be difficult to reason about whether DP may be appropriate for a given\ncontext due to tensions that arise when it is brought from theory into\npractice. To aid policymaking around privacy concerns, we identify three\ncategories of challenges to understanding DP along with associated questions\nthat policymakers can ask about the potential deployment context to anticipate\nits impacts.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Differential privacy (DP) is a mathematical definition of privacy that can be\nwidely applied when publishing data. DP has been recognized as a potential\nmeans of adhering to various privacy-related legal requirements. However, it\ncan be difficult to reason about whether DP may be appropriate for a given\ncontext due to tensions that arise when it is brought from theory into\npractice. To aid policymaking around privacy concerns, we identify three\ncategories of challenges to understanding DP along with associated questions\nthat policymakers can ask about the potential deployment context to anticipate\nits impacts."
                },
                "authors": [
                    {
                        "name": "Priyanka Nanayakkara"
                    },
                    {
                        "name": "Jessica Hullman"
                    }
                ],
                "author_detail": {
                    "name": "Jessica Hullman"
                },
                "author": "Jessica Hullman",
                "arxiv_doi": "10.1177/23727322241278687",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1177/23727322241278687",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2409.11680v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.11680v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "This paper is accepted for publication in Policy Insights from the\n  Behavioral and Brain Sciences (2024)",
                "arxiv_primary_category": {
                    "term": "cs.CY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.11673v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.11673v1",
                "updated": "2024-09-18T03:20:04Z",
                "updated_parsed": [
                    2024,
                    9,
                    18,
                    3,
                    20,
                    4,
                    2,
                    262,
                    0
                ],
                "published": "2024-09-18T03:20:04Z",
                "published_parsed": [
                    2024,
                    9,
                    18,
                    3,
                    20,
                    4,
                    2,
                    262,
                    0
                ],
                "title": "RUIE: Retrieval-based Unified Information Extraction using Large\n  Language Model",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RUIE: Retrieval-based Unified Information Extraction using Large\n  Language Model"
                },
                "summary": "Unified information extraction (UIE) aims to complete all information\nextraction tasks using a single model or framework. While previous work has\nprimarily focused on instruction-tuning large language models (LLMs) with\nconstructed datasets, these methods require significant computational resources\nand struggle to generalize to unseen tasks. To address these limitations, we\npropose RUIE (Retrieval-based Unified Information Extraction), a framework that\nleverages in-context learning to enable rapid generalization while reducing\ncomputational costs. The key challenge in RUIE is selecting the most beneficial\ndemonstrations for LLMs to effectively handle diverse IE tasks. To achieve\nthis, we integrate LLM preferences for ranking candidate demonstrations and\ndesign a keyword-enhanced reward model to capture fine-grained relationships\nbetween queries and demonstrations. We then train a bi-encoder retriever for\nUIE through contrastive learning and knowledge distillation. To the best of our\nknowledge, RUIE is the first trainable retrieval framework for UIE.\nExperimental results on 8 held-out datasets demonstrate RUIE's effectiveness in\ngeneralizing to unseen tasks, with average F1-score improvements of 19.22 and\n3.13 compared to instruction-tuning methods and other retrievers, respectively.\nFurther analysis confirms RUIE's adaptability to LLMs of varying sizes and the\nimportance of its key components.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Unified information extraction (UIE) aims to complete all information\nextraction tasks using a single model or framework. While previous work has\nprimarily focused on instruction-tuning large language models (LLMs) with\nconstructed datasets, these methods require significant computational resources\nand struggle to generalize to unseen tasks. To address these limitations, we\npropose RUIE (Retrieval-based Unified Information Extraction), a framework that\nleverages in-context learning to enable rapid generalization while reducing\ncomputational costs. The key challenge in RUIE is selecting the most beneficial\ndemonstrations for LLMs to effectively handle diverse IE tasks. To achieve\nthis, we integrate LLM preferences for ranking candidate demonstrations and\ndesign a keyword-enhanced reward model to capture fine-grained relationships\nbetween queries and demonstrations. We then train a bi-encoder retriever for\nUIE through contrastive learning and knowledge distillation. To the best of our\nknowledge, RUIE is the first trainable retrieval framework for UIE.\nExperimental results on 8 held-out datasets demonstrate RUIE's effectiveness in\ngeneralizing to unseen tasks, with average F1-score improvements of 19.22 and\n3.13 compared to instruction-tuning methods and other retrievers, respectively.\nFurther analysis confirms RUIE's adaptability to LLMs of varying sizes and the\nimportance of its key components."
                },
                "authors": [
                    {
                        "name": "Xincheng Liao"
                    },
                    {
                        "name": "Junwen Duan"
                    },
                    {
                        "name": "Yixi Huang"
                    },
                    {
                        "name": "Jianxin Wang"
                    }
                ],
                "author_detail": {
                    "name": "Jianxin Wang"
                },
                "author": "Jianxin Wang",
                "arxiv_comment": "14 pages, 3 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.11673v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.11673v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.09044v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.09044v2",
                "updated": "2024-09-18T02:57:12Z",
                "updated_parsed": [
                    2024,
                    9,
                    18,
                    2,
                    57,
                    12,
                    2,
                    262,
                    0
                ],
                "published": "2024-06-13T12:30:02Z",
                "published_parsed": [
                    2024,
                    6,
                    13,
                    12,
                    30,
                    2,
                    3,
                    165,
                    0
                ],
                "title": "MiLoRA: Harnessing Minor Singular Components for Parameter-Efficient LLM\n  Finetuning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MiLoRA: Harnessing Minor Singular Components for Parameter-Efficient LLM\n  Finetuning"
                },
                "summary": "Efficient finetuning of large language models (LLMs) aims to adapt the LLMs\nwith reduced computational and memory cost. Previous LoRA-based approaches\ninitialize the low-rank matrices with Gaussian distribution and zero values\nwhile keeping the original weight matrices frozen. However, the trainable model\nparameters optimized in an unguided subspace might interfere with the\nwell-learned subspace of the pretrained weight matrices. In this paper, we\npropose MiLoRA, a simple yet effective LLM finetuning approach that only\nupdates the minor singular components of the weight matrix while keeping the\nprincipal singular components frozen. It is observed that the minor matrix\ncorresponds to the noisy or long-tail information, while the principal matrix\ncontains important knowledge. The MiLoRA initializes the low-rank matrices\nwithin a subspace that is orthogonal to the principal matrix, thus the\npretrained knowledge is expected to be well preserved. During finetuning,\nMiLoRA makes the most use of the less-optimized subspace for learning the\nlabeled dataset. Extensive experiments on commonsense reasoning, math\nreasoning, instruction following and visual instruction following benchmarks\npresent the superior performance of our method.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient finetuning of large language models (LLMs) aims to adapt the LLMs\nwith reduced computational and memory cost. Previous LoRA-based approaches\ninitialize the low-rank matrices with Gaussian distribution and zero values\nwhile keeping the original weight matrices frozen. However, the trainable model\nparameters optimized in an unguided subspace might interfere with the\nwell-learned subspace of the pretrained weight matrices. In this paper, we\npropose MiLoRA, a simple yet effective LLM finetuning approach that only\nupdates the minor singular components of the weight matrix while keeping the\nprincipal singular components frozen. It is observed that the minor matrix\ncorresponds to the noisy or long-tail information, while the principal matrix\ncontains important knowledge. The MiLoRA initializes the low-rank matrices\nwithin a subspace that is orthogonal to the principal matrix, thus the\npretrained knowledge is expected to be well preserved. During finetuning,\nMiLoRA makes the most use of the less-optimized subspace for learning the\nlabeled dataset. Extensive experiments on commonsense reasoning, math\nreasoning, instruction following and visual instruction following benchmarks\npresent the superior performance of our method."
                },
                "authors": [
                    {
                        "name": "Hanqing Wang"
                    },
                    {
                        "name": "Yixia Li"
                    },
                    {
                        "name": "Shuo Wang"
                    },
                    {
                        "name": "Guanhua Chen"
                    },
                    {
                        "name": "Yun Chen"
                    }
                ],
                "author_detail": {
                    "name": "Yun Chen"
                },
                "author": "Yun Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.09044v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.09044v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.11650v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.11650v1",
                "updated": "2024-09-18T02:35:00Z",
                "updated_parsed": [
                    2024,
                    9,
                    18,
                    2,
                    35,
                    0,
                    2,
                    262,
                    0
                ],
                "published": "2024-09-18T02:35:00Z",
                "published_parsed": [
                    2024,
                    9,
                    18,
                    2,
                    35,
                    0,
                    2,
                    262,
                    0
                ],
                "title": "Art and Science of Quantizing Large-Scale Models: A Comprehensive\n  Overview",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Art and Science of Quantizing Large-Scale Models: A Comprehensive\n  Overview"
                },
                "summary": "This paper provides a comprehensive overview of the principles, challenges,\nand methodologies associated with quantizing large-scale neural network models.\nAs neural networks have evolved towards larger and more complex architectures\nto address increasingly sophisticated tasks, the computational and energy costs\nhave escalated significantly. We explore the necessity and impact of model size\ngrowth, highlighting the performance benefits as well as the computational\nchallenges and environmental considerations. The core focus is on model\nquantization as a fundamental approach to mitigate these challenges by reducing\nmodel size and improving efficiency without substantially compromising\naccuracy. We delve into various quantization techniques, including both\npost-training quantization (PTQ) and quantization-aware training (QAT), and\nanalyze several state-of-the-art algorithms such as LLM-QAT, PEQA(L4Q),\nZeroQuant, SmoothQuant, and others. Through comparative analysis, we examine\nhow these methods address issues like outliers, importance weighting, and\nactivation quantization, ultimately contributing to more sustainable and\naccessible deployment of large-scale models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper provides a comprehensive overview of the principles, challenges,\nand methodologies associated with quantizing large-scale neural network models.\nAs neural networks have evolved towards larger and more complex architectures\nto address increasingly sophisticated tasks, the computational and energy costs\nhave escalated significantly. We explore the necessity and impact of model size\ngrowth, highlighting the performance benefits as well as the computational\nchallenges and environmental considerations. The core focus is on model\nquantization as a fundamental approach to mitigate these challenges by reducing\nmodel size and improving efficiency without substantially compromising\naccuracy. We delve into various quantization techniques, including both\npost-training quantization (PTQ) and quantization-aware training (QAT), and\nanalyze several state-of-the-art algorithms such as LLM-QAT, PEQA(L4Q),\nZeroQuant, SmoothQuant, and others. Through comparative analysis, we examine\nhow these methods address issues like outliers, importance weighting, and\nactivation quantization, ultimately contributing to more sustainable and\naccessible deployment of large-scale models."
                },
                "authors": [
                    {
                        "name": "Yanshu Wang"
                    },
                    {
                        "name": "Tong Yang"
                    },
                    {
                        "name": "Xiyan Liang"
                    },
                    {
                        "name": "Guoan Wang"
                    },
                    {
                        "name": "Hanning Lu"
                    },
                    {
                        "name": "Xu Zhe"
                    },
                    {
                        "name": "Yaoming Li"
                    },
                    {
                        "name": "Li Weitao"
                    }
                ],
                "author_detail": {
                    "name": "Li Weitao"
                },
                "author": "Li Weitao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.11650v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.11650v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.02657v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.02657v3",
                "updated": "2024-09-18T02:31:12Z",
                "updated_parsed": [
                    2024,
                    9,
                    18,
                    2,
                    31,
                    12,
                    2,
                    262,
                    0
                ],
                "published": "2024-04-03T11:40:17Z",
                "published_parsed": [
                    2024,
                    4,
                    3,
                    11,
                    40,
                    17,
                    2,
                    94,
                    0
                ],
                "title": "Rethinking Kullback-Leibler Divergence in Knowledge Distillation for\n  Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Rethinking Kullback-Leibler Divergence in Knowledge Distillation for\n  Large Language Models"
                },
                "summary": "Kullback-Leiber divergence has been widely used in Knowledge Distillation\n(KD) to compress Large Language Models (LLMs). Contrary to prior assertions\nthat reverse Kullback-Leibler (RKL) divergence is mode-seeking and thus\npreferable over the mean-seeking forward Kullback-Leibler (FKL) divergence,\nthis study empirically and theoretically demonstrates that neither mode-seeking\nnor mean-seeking properties manifest in KD for LLMs. Instead, RKL and FKL are\nfound to share the same optimization objective and both converge after a\nsufficient number of epochs. However, due to practical constraints, LLMs are\nseldom trained for such an extensive number of epochs. Meanwhile, we further\nfind that RKL focuses on the tail part of the distributions, while FKL focuses\non the head part at the beginning epochs. Consequently, we propose a simple yet\neffective Adaptive Kullback-Leiber (AKL) divergence method, which adaptively\nallocates weights to combine FKL and RKL. Metric-based and GPT-4-based\nevaluations demonstrate that the proposed AKL outperforms the baselines across\nvarious tasks and improves the diversity and quality of generated responses.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Kullback-Leiber divergence has been widely used in Knowledge Distillation\n(KD) to compress Large Language Models (LLMs). Contrary to prior assertions\nthat reverse Kullback-Leibler (RKL) divergence is mode-seeking and thus\npreferable over the mean-seeking forward Kullback-Leibler (FKL) divergence,\nthis study empirically and theoretically demonstrates that neither mode-seeking\nnor mean-seeking properties manifest in KD for LLMs. Instead, RKL and FKL are\nfound to share the same optimization objective and both converge after a\nsufficient number of epochs. However, due to practical constraints, LLMs are\nseldom trained for such an extensive number of epochs. Meanwhile, we further\nfind that RKL focuses on the tail part of the distributions, while FKL focuses\non the head part at the beginning epochs. Consequently, we propose a simple yet\neffective Adaptive Kullback-Leiber (AKL) divergence method, which adaptively\nallocates weights to combine FKL and RKL. Metric-based and GPT-4-based\nevaluations demonstrate that the proposed AKL outperforms the baselines across\nvarious tasks and improves the diversity and quality of generated responses."
                },
                "authors": [
                    {
                        "name": "Taiqiang Wu"
                    },
                    {
                        "name": "Chaofan Tao"
                    },
                    {
                        "name": "Jiahao Wang"
                    },
                    {
                        "name": "Runming Yang"
                    },
                    {
                        "name": "Zhe Zhao"
                    },
                    {
                        "name": "Ngai Wong"
                    }
                ],
                "author_detail": {
                    "name": "Ngai Wong"
                },
                "author": "Ngai Wong",
                "arxiv_comment": "working in progress, code available at\n  https://github.com/wutaiqiang/LLM_KD_AKL",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.02657v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.02657v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.11643v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.11643v1",
                "updated": "2024-09-18T02:14:30Z",
                "updated_parsed": [
                    2024,
                    9,
                    18,
                    2,
                    14,
                    30,
                    2,
                    262,
                    0
                ],
                "published": "2024-09-18T02:14:30Z",
                "published_parsed": [
                    2024,
                    9,
                    18,
                    2,
                    14,
                    30,
                    2,
                    262,
                    0
                ],
                "title": "Combating Phone Scams with LLM-based Detection: Where Do We Stand?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Combating Phone Scams with LLM-based Detection: Where Do We Stand?"
                },
                "summary": "Phone scams pose a significant threat to individuals and communities, causing\nsubstantial financial losses and emotional distress. Despite ongoing efforts to\ncombat these scams, scammers continue to adapt and refine their tactics, making\nit imperative to explore innovative countermeasures. This research explores the\npotential of large language models (LLMs) to provide detection of fraudulent\nphone calls. By analyzing the conversational dynamics between scammers and\nvictims, LLM-based detectors can identify potential scams as they occur,\noffering immediate protection to users. While such approaches demonstrate\npromising results, we also acknowledge the challenges of biased datasets,\nrelatively low recall, and hallucinations that must be addressed for further\nadvancement in this field",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Phone scams pose a significant threat to individuals and communities, causing\nsubstantial financial losses and emotional distress. Despite ongoing efforts to\ncombat these scams, scammers continue to adapt and refine their tactics, making\nit imperative to explore innovative countermeasures. This research explores the\npotential of large language models (LLMs) to provide detection of fraudulent\nphone calls. By analyzing the conversational dynamics between scammers and\nvictims, LLM-based detectors can identify potential scams as they occur,\noffering immediate protection to users. While such approaches demonstrate\npromising results, we also acknowledge the challenges of biased datasets,\nrelatively low recall, and hallucinations that must be addressed for further\nadvancement in this field"
                },
                "authors": [
                    {
                        "name": "Zitong Shen"
                    },
                    {
                        "name": "Kangzhong Wang"
                    },
                    {
                        "name": "Youqian Zhang"
                    },
                    {
                        "name": "Grace Ngai"
                    },
                    {
                        "name": "Eugene Y. Fu"
                    }
                ],
                "author_detail": {
                    "name": "Eugene Y. Fu"
                },
                "author": "Eugene Y. Fu",
                "arxiv_comment": "2 pages, 1 figure",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.11643v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.11643v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.0",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2401.15284v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2401.15284v4",
                "updated": "2024-09-18T02:03:35Z",
                "updated_parsed": [
                    2024,
                    9,
                    18,
                    2,
                    3,
                    35,
                    2,
                    262,
                    0
                ],
                "published": "2024-01-27T03:53:25Z",
                "published_parsed": [
                    2024,
                    1,
                    27,
                    3,
                    53,
                    25,
                    5,
                    27,
                    0
                ],
                "title": "Beyond principlism: Practical strategies for ethical AI use in research\n  practices",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Beyond principlism: Practical strategies for ethical AI use in research\n  practices"
                },
                "summary": "The rapid adoption of generative artificial intelligence (AI) in scientific\nresearch, particularly large language models (LLMs), has outpaced the\ndevelopment of ethical guidelines, leading to a Triple-Too problem: too many\nhigh-level ethical initiatives, too abstract principles lacking contextual and\npractical relevance, and too much focus on restrictions and risks over benefits\nand utilities. Existing approaches, including principlism (reliance on abstract\nethical principles), formalism (rigid application of rules), and technical\nsolutionism (overemphasis on technological fixes), offer little practical\nguidance for addressing ethical challenges of AI in scientific research\npractices. To bridge the gap between abstract principles and day-to-day\nresearch practices, a user-centered, realism-inspired approach is proposed\nhere. It outlines five specific goals for ethical AI use: 1) understanding\nmodel training and output, including bias mitigation strategies; 2) respecting\nprivacy, confidentiality, and copyright; 3) avoiding plagiarism and policy\nviolations; 4) applying AI beneficially compared to alternatives; and 5) using\nAI transparently and reproducibly. Each goal is accompanied by actionable\nstrategies and realistic cases of misuse and corrective measures. I argue that\nethical AI application requires evaluating its utility against existing\nalternatives rather than isolated performance metrics. Additionally, I propose\ndocumentation guidelines to enhance transparency and reproducibility in\nAI-assisted research. Moving forward, we need targeted professional\ndevelopment, training programs, and balanced enforcement mechanisms to promote\nresponsible AI use while fostering innovation. By refining these ethical\nguidelines and adapting them to emerging AI capabilities, we can accelerate\nscientific progress without compromising research integrity.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rapid adoption of generative artificial intelligence (AI) in scientific\nresearch, particularly large language models (LLMs), has outpaced the\ndevelopment of ethical guidelines, leading to a Triple-Too problem: too many\nhigh-level ethical initiatives, too abstract principles lacking contextual and\npractical relevance, and too much focus on restrictions and risks over benefits\nand utilities. Existing approaches, including principlism (reliance on abstract\nethical principles), formalism (rigid application of rules), and technical\nsolutionism (overemphasis on technological fixes), offer little practical\nguidance for addressing ethical challenges of AI in scientific research\npractices. To bridge the gap between abstract principles and day-to-day\nresearch practices, a user-centered, realism-inspired approach is proposed\nhere. It outlines five specific goals for ethical AI use: 1) understanding\nmodel training and output, including bias mitigation strategies; 2) respecting\nprivacy, confidentiality, and copyright; 3) avoiding plagiarism and policy\nviolations; 4) applying AI beneficially compared to alternatives; and 5) using\nAI transparently and reproducibly. Each goal is accompanied by actionable\nstrategies and realistic cases of misuse and corrective measures. I argue that\nethical AI application requires evaluating its utility against existing\nalternatives rather than isolated performance metrics. Additionally, I propose\ndocumentation guidelines to enhance transparency and reproducibility in\nAI-assisted research. Moving forward, we need targeted professional\ndevelopment, training programs, and balanced enforcement mechanisms to promote\nresponsible AI use while fostering innovation. By refining these ethical\nguidelines and adapting them to emerging AI capabilities, we can accelerate\nscientific progress without compromising research integrity."
                },
                "authors": [
                    {
                        "name": "Zhicheng Lin"
                    }
                ],
                "author_detail": {
                    "name": "Zhicheng Lin"
                },
                "author": "Zhicheng Lin",
                "arxiv_comment": "Accepted in: AI and Ethics. 20 pages, 1 figure, 3 tables, 2 boxes",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2401.15284v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2401.15284v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.11638v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.11638v1",
                "updated": "2024-09-18T02:02:30Z",
                "updated_parsed": [
                    2024,
                    9,
                    18,
                    2,
                    2,
                    30,
                    2,
                    262,
                    0
                ],
                "published": "2024-09-18T02:02:30Z",
                "published_parsed": [
                    2024,
                    9,
                    18,
                    2,
                    2,
                    30,
                    2,
                    262,
                    0
                ],
                "title": "BanStereoSet: A Dataset to Measure Stereotypical Social Biases in LLMs\n  for Bangla",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "BanStereoSet: A Dataset to Measure Stereotypical Social Biases in LLMs\n  for Bangla"
                },
                "summary": "This study presents BanStereoSet, a dataset designed to evaluate\nstereotypical social biases in multilingual LLMs for the Bangla language. In an\neffort to extend the focus of bias research beyond English-centric datasets, we\nhave localized the content from the StereoSet, IndiBias, and Kamruzzaman et.\nal.'s datasets, producing a resource tailored to capture biases prevalent\nwithin the Bangla-speaking community. Our BanStereoSet dataset consists of\n1,194 sentences spanning 9 categories of bias: race, profession, gender,\nageism, beauty, beauty in profession, region, caste, and religion. This dataset\nnot only serves as a crucial tool for measuring bias in multilingual LLMs but\nalso facilitates the exploration of stereotypical bias across different social\ncategories, potentially guiding the development of more equitable language\ntechnologies in Bangladeshi contexts. Our analysis of several language models\nusing this dataset indicates significant biases, reinforcing the necessity for\nculturally and linguistically adapted datasets to develop more equitable\nlanguage technologies.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This study presents BanStereoSet, a dataset designed to evaluate\nstereotypical social biases in multilingual LLMs for the Bangla language. In an\neffort to extend the focus of bias research beyond English-centric datasets, we\nhave localized the content from the StereoSet, IndiBias, and Kamruzzaman et.\nal.'s datasets, producing a resource tailored to capture biases prevalent\nwithin the Bangla-speaking community. Our BanStereoSet dataset consists of\n1,194 sentences spanning 9 categories of bias: race, profession, gender,\nageism, beauty, beauty in profession, region, caste, and religion. This dataset\nnot only serves as a crucial tool for measuring bias in multilingual LLMs but\nalso facilitates the exploration of stereotypical bias across different social\ncategories, potentially guiding the development of more equitable language\ntechnologies in Bangladeshi contexts. Our analysis of several language models\nusing this dataset indicates significant biases, reinforcing the necessity for\nculturally and linguistically adapted datasets to develop more equitable\nlanguage technologies."
                },
                "authors": [
                    {
                        "name": "Mahammed Kamruzzaman"
                    },
                    {
                        "name": "Abdullah Al Monsur"
                    },
                    {
                        "name": "Shrabon Das"
                    },
                    {
                        "name": "Enamul Hassan"
                    },
                    {
                        "name": "Gene Louis Kim"
                    }
                ],
                "author_detail": {
                    "name": "Gene Louis Kim"
                },
                "author": "Gene Louis Kim",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.11638v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.11638v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.11636v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.11636v1",
                "updated": "2024-09-18T01:56:34Z",
                "updated_parsed": [
                    2024,
                    9,
                    18,
                    1,
                    56,
                    34,
                    2,
                    262,
                    0
                ],
                "published": "2024-09-18T01:56:34Z",
                "published_parsed": [
                    2024,
                    9,
                    18,
                    1,
                    56,
                    34,
                    2,
                    262,
                    0
                ],
                "title": "\"A Woman is More Culturally Knowledgeable than A Man?\": The Effect of\n  Personas on Cultural Norm Interpretation in LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "\"A Woman is More Culturally Knowledgeable than A Man?\": The Effect of\n  Personas on Cultural Norm Interpretation in LLMs"
                },
                "summary": "As the deployment of large language models (LLMs) expands, there is an\nincreasing demand for personalized LLMs. One method to personalize and guide\nthe outputs of these models is by assigning a persona -- a role that describes\nthe expected behavior of the LLM (e.g., a man, a woman, an engineer). This\nstudy investigates whether an LLM's understanding of social norms varies across\nassigned personas. Ideally, the perception of a social norm should remain\nconsistent regardless of the persona, since acceptability of a social norm\nshould be determined by the region the norm originates from, rather than by\nindividual characteristics such as gender, body size, or race. A norm is\nuniversal within its cultural context. In our research, we tested 36 distinct\npersonas from 12 sociodemographic categories (e.g., age, gender, beauty) across\nfour different LLMs. We find that LLMs' cultural norm interpretation varies\nbased on the persona used and the norm interpretation also varies within a\nsociodemographic category (e.g., a fat person and a thin person as in physical\nappearance group) where an LLM with the more socially desirable persona (e.g.,\na thin person) interprets social norms more accurately than with the less\nsocially desirable persona (e.g., a fat person). We also discuss how different\ntypes of social biases may contribute to the results that we observe.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As the deployment of large language models (LLMs) expands, there is an\nincreasing demand for personalized LLMs. One method to personalize and guide\nthe outputs of these models is by assigning a persona -- a role that describes\nthe expected behavior of the LLM (e.g., a man, a woman, an engineer). This\nstudy investigates whether an LLM's understanding of social norms varies across\nassigned personas. Ideally, the perception of a social norm should remain\nconsistent regardless of the persona, since acceptability of a social norm\nshould be determined by the region the norm originates from, rather than by\nindividual characteristics such as gender, body size, or race. A norm is\nuniversal within its cultural context. In our research, we tested 36 distinct\npersonas from 12 sociodemographic categories (e.g., age, gender, beauty) across\nfour different LLMs. We find that LLMs' cultural norm interpretation varies\nbased on the persona used and the norm interpretation also varies within a\nsociodemographic category (e.g., a fat person and a thin person as in physical\nappearance group) where an LLM with the more socially desirable persona (e.g.,\na thin person) interprets social norms more accurately than with the less\nsocially desirable persona (e.g., a fat person). We also discuss how different\ntypes of social biases may contribute to the results that we observe."
                },
                "authors": [
                    {
                        "name": "Mahammed Kamruzzaman"
                    },
                    {
                        "name": "Hieu Nguyen"
                    },
                    {
                        "name": "Nazmul Hassan"
                    },
                    {
                        "name": "Gene Louis Kim"
                    }
                ],
                "author_detail": {
                    "name": "Gene Louis Kim"
                },
                "author": "Gene Louis Kim",
                "arxiv_comment": "Preprint, Under Review",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.11636v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.11636v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.05385v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.05385v3",
                "updated": "2024-09-18T01:39:02Z",
                "updated_parsed": [
                    2024,
                    9,
                    18,
                    1,
                    39,
                    2,
                    2,
                    262,
                    0
                ],
                "published": "2024-09-09T07:32:30Z",
                "published_parsed": [
                    2024,
                    9,
                    9,
                    7,
                    32,
                    30,
                    0,
                    253,
                    0
                ],
                "title": "Towards Building a Robust Knowledge Intensive Question Answering Model\n  with Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Towards Building a Robust Knowledge Intensive Question Answering Model\n  with Large Language Models"
                },
                "summary": "The development of LLMs has greatly enhanced the intelligence and fluency of\nquestion answering, while the emergence of retrieval enhancement has enabled\nmodels to better utilize external information. However, the presence of noise\nand errors in retrieved information poses challenges to the robustness of LLMs.\nIn this work, to evaluate the model's performance under multiple interferences,\nwe first construct a dataset based on machine reading comprehension datasets\nsimulating various scenarios, including critical information absence, noise,\nand conflicts. To address the issue of model accuracy decline caused by noisy\nexternal information, we propose a data augmentation-based fine-tuning method\nto enhance LLM's robustness against noise. Additionally, contrastive learning\napproach is utilized to preserve the model's discrimination capability of\nexternal information. We have conducted experiments on both existing LLMs and\nour approach, the results are evaluated by GPT-4, which indicates that our\nproposed methods improve model robustness while strengthening the model's\ndiscrimination capability.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The development of LLMs has greatly enhanced the intelligence and fluency of\nquestion answering, while the emergence of retrieval enhancement has enabled\nmodels to better utilize external information. However, the presence of noise\nand errors in retrieved information poses challenges to the robustness of LLMs.\nIn this work, to evaluate the model's performance under multiple interferences,\nwe first construct a dataset based on machine reading comprehension datasets\nsimulating various scenarios, including critical information absence, noise,\nand conflicts. To address the issue of model accuracy decline caused by noisy\nexternal information, we propose a data augmentation-based fine-tuning method\nto enhance LLM's robustness against noise. Additionally, contrastive learning\napproach is utilized to preserve the model's discrimination capability of\nexternal information. We have conducted experiments on both existing LLMs and\nour approach, the results are evaluated by GPT-4, which indicates that our\nproposed methods improve model robustness while strengthening the model's\ndiscrimination capability."
                },
                "authors": [
                    {
                        "name": "Xingyun Hong"
                    },
                    {
                        "name": "Yan Shao"
                    },
                    {
                        "name": "Zhilin Wang"
                    },
                    {
                        "name": "Manni Duan"
                    },
                    {
                        "name": "Jin Xiongnan"
                    }
                ],
                "author_detail": {
                    "name": "Jin Xiongnan"
                },
                "author": "Jin Xiongnan",
                "arxiv_comment": "This paper has been accepted by NLPCC-2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.05385v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.05385v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.10561v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.10561v2",
                "updated": "2024-09-18T01:18:48Z",
                "updated_parsed": [
                    2024,
                    9,
                    18,
                    1,
                    18,
                    48,
                    2,
                    262,
                    0
                ],
                "published": "2024-09-11T14:41:44Z",
                "published_parsed": [
                    2024,
                    9,
                    11,
                    14,
                    41,
                    44,
                    2,
                    255,
                    0
                ],
                "title": "DrLLM: Prompt-Enhanced Distributed Denial-of-Service Resistance Method\n  with Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DrLLM: Prompt-Enhanced Distributed Denial-of-Service Resistance Method\n  with Large Language Models"
                },
                "summary": "The increasing number of Distributed Denial of Service (DDoS) attacks poses a\nmajor threat to the Internet, highlighting the importance of DDoS mitigation.\nMost existing approaches require complex training methods to learn data\nfeatures, which increases the complexity and generality of the application. In\nthis paper, we propose DrLLM, which aims to mine anomalous traffic information\nin zero-shot scenarios through Large Language Models (LLMs). To bridge the gap\nbetween DrLLM and existing approaches, we embed the global and local\ninformation of the traffic data into the reasoning paradigm and design three\nmodules, namely Knowledge Embedding, Token Embedding, and Progressive Role\nReasoning, for data representation and reasoning. In addition we explore the\ngeneralization of prompt engineering in the cybersecurity domain to improve the\nclassification capability of DrLLM. Our ablation experiments demonstrate the\napplicability of DrLLM in zero-shot scenarios and further demonstrate the\npotential of LLMs in the network domains. DrLLM implementation code has been\nopen-sourced at https://github.com/liuup/DrLLM.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The increasing number of Distributed Denial of Service (DDoS) attacks poses a\nmajor threat to the Internet, highlighting the importance of DDoS mitigation.\nMost existing approaches require complex training methods to learn data\nfeatures, which increases the complexity and generality of the application. In\nthis paper, we propose DrLLM, which aims to mine anomalous traffic information\nin zero-shot scenarios through Large Language Models (LLMs). To bridge the gap\nbetween DrLLM and existing approaches, we embed the global and local\ninformation of the traffic data into the reasoning paradigm and design three\nmodules, namely Knowledge Embedding, Token Embedding, and Progressive Role\nReasoning, for data representation and reasoning. In addition we explore the\ngeneralization of prompt engineering in the cybersecurity domain to improve the\nclassification capability of DrLLM. Our ablation experiments demonstrate the\napplicability of DrLLM in zero-shot scenarios and further demonstrate the\npotential of LLMs in the network domains. DrLLM implementation code has been\nopen-sourced at https://github.com/liuup/DrLLM."
                },
                "authors": [
                    {
                        "name": "Zhenyu Yin"
                    },
                    {
                        "name": "Shang Liu"
                    },
                    {
                        "name": "Guangyuan Xu"
                    }
                ],
                "author_detail": {
                    "name": "Guangyuan Xu"
                },
                "author": "Guangyuan Xu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.10561v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.10561v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.19493v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.19493v2",
                "updated": "2024-09-18T00:31:27Z",
                "updated_parsed": [
                    2024,
                    9,
                    18,
                    0,
                    31,
                    27,
                    2,
                    262,
                    0
                ],
                "published": "2024-07-28T13:23:43Z",
                "published_parsed": [
                    2024,
                    7,
                    28,
                    13,
                    23,
                    43,
                    6,
                    210,
                    0
                ],
                "title": "Official-NV: An LLM-Generated News Video Dataset for Multimodal Fake\n  News Detection",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Official-NV: An LLM-Generated News Video Dataset for Multimodal Fake\n  News Detection"
                },
                "summary": "News media, especially video news media, have penetrated into every aspect of\ndaily life, which also brings the risk of fake news. Therefore, multimodal fake\nnews detection has recently garnered increased attention. However, the existing\ndatasets are comprised of user-uploaded videos and contain an excess amounts of\nsuperfluous data, which introduces noise into the model training process. To\naddress this issue, we construct a dataset named Official-NV, comprising\nofficially published news videos. The crawl officially published videos are\naugmented through the use of LLMs-based generation and manual verification,\nthereby expanding the dataset. Furthermore, the proposed dataset is benchmarked\nagainst several baselines to demonstrate its effectiveness in multimodal news\ndetection.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "News media, especially video news media, have penetrated into every aspect of\ndaily life, which also brings the risk of fake news. Therefore, multimodal fake\nnews detection has recently garnered increased attention. However, the existing\ndatasets are comprised of user-uploaded videos and contain an excess amounts of\nsuperfluous data, which introduces noise into the model training process. To\naddress this issue, we construct a dataset named Official-NV, comprising\nofficially published news videos. The crawl officially published videos are\naugmented through the use of LLMs-based generation and manual verification,\nthereby expanding the dataset. Furthermore, the proposed dataset is benchmarked\nagainst several baselines to demonstrate its effectiveness in multimodal news\ndetection."
                },
                "authors": [
                    {
                        "name": "Yihao Wang"
                    },
                    {
                        "name": "Lizhi Chen"
                    },
                    {
                        "name": "Zhong Qian"
                    },
                    {
                        "name": "Peifeng Li"
                    }
                ],
                "author_detail": {
                    "name": "Peifeng Li"
                },
                "author": "Peifeng Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.19493v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.19493v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.08221v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.08221v2",
                "updated": "2024-09-18T00:30:42Z",
                "updated_parsed": [
                    2024,
                    9,
                    18,
                    0,
                    30,
                    42,
                    2,
                    262,
                    0
                ],
                "published": "2024-06-12T13:51:51Z",
                "published_parsed": [
                    2024,
                    6,
                    12,
                    13,
                    51,
                    51,
                    2,
                    164,
                    0
                ],
                "title": "FAIL: Analyzing Software Failures from the News Using LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FAIL: Analyzing Software Failures from the News Using LLMs"
                },
                "summary": "Software failures inform engineering work, standards, regulations. For\nexample, the Log4J vulnerability brought government and industry attention to\nevaluating and securing software supply chains. Accessing private engineering\nrecords is difficult, so failure analyses tend to use information reported by\nthe news media. However, prior works in this direction have relied on manual\nanalysis. That has limited the scale of their analyses. The community lacks\nautomated support to enable such analyses to consider a wide range of news\nsources and incidents. In this paper, we propose the Failure Analysis\nInvestigation with LLMs (FAIL) system to fill this gap. FAIL collects,\nanalyzes, and summarizes software failures as reported in the news. FAIL groups\narticles that describe the same incidents. It then analyzes incidents using\nexisting taxonomies for postmortems, faults, and system characteristics. To\ntune and evaluate FAIL, we followed the methods of prior works by manually\nanalyzing 31 software failures. FAIL achieved an F1 score of 90% for collecting\nnews about software failures, a V-measure of 0.98 for merging articles\nreporting on the same incident, and extracted 90% of the facts about failures.\nWe then applied FAIL to a total of 137,427 news articles from 11 providers\npublished between 2010 and 2022. FAIL identified and analyzed 2457 distinct\nfailures reported across 4,184 articles. Our findings include: (1) current\ngeneration of large language models are capable of identifying news articles\nthat describe failures, and analyzing them according to structured taxonomies;\n(2) high recurrences of similar failures within organizations and across\norganizations; and (3) severity of the consequences of software failures have\nincreased over the past decade. The full FAIL database is available so that\nresearchers, engineers, and policymakers can learn from a diversity of software\nfailures.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Software failures inform engineering work, standards, regulations. For\nexample, the Log4J vulnerability brought government and industry attention to\nevaluating and securing software supply chains. Accessing private engineering\nrecords is difficult, so failure analyses tend to use information reported by\nthe news media. However, prior works in this direction have relied on manual\nanalysis. That has limited the scale of their analyses. The community lacks\nautomated support to enable such analyses to consider a wide range of news\nsources and incidents. In this paper, we propose the Failure Analysis\nInvestigation with LLMs (FAIL) system to fill this gap. FAIL collects,\nanalyzes, and summarizes software failures as reported in the news. FAIL groups\narticles that describe the same incidents. It then analyzes incidents using\nexisting taxonomies for postmortems, faults, and system characteristics. To\ntune and evaluate FAIL, we followed the methods of prior works by manually\nanalyzing 31 software failures. FAIL achieved an F1 score of 90% for collecting\nnews about software failures, a V-measure of 0.98 for merging articles\nreporting on the same incident, and extracted 90% of the facts about failures.\nWe then applied FAIL to a total of 137,427 news articles from 11 providers\npublished between 2010 and 2022. FAIL identified and analyzed 2457 distinct\nfailures reported across 4,184 articles. Our findings include: (1) current\ngeneration of large language models are capable of identifying news articles\nthat describe failures, and analyzing them according to structured taxonomies;\n(2) high recurrences of similar failures within organizations and across\norganizations; and (3) severity of the consequences of software failures have\nincreased over the past decade. The full FAIL database is available so that\nresearchers, engineers, and policymakers can learn from a diversity of software\nfailures."
                },
                "authors": [
                    {
                        "name": "Dharun Anandayuvaraj"
                    },
                    {
                        "name": "Matthew Campbell"
                    },
                    {
                        "name": "Arav Tewari"
                    },
                    {
                        "name": "James C. Davis"
                    }
                ],
                "author_detail": {
                    "name": "James C. Davis"
                },
                "author": "James C. Davis",
                "arxiv_doi": "10.1145/3691620.3695022",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3691620.3695022",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2406.08221v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.08221v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "Accapted at the 9th IEEE/ACM International Conference on Automated\n  Software Engineering (ASE 2024)",
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2401.05632v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2401.05632v3",
                "updated": "2024-09-18T00:02:02Z",
                "updated_parsed": [
                    2024,
                    9,
                    18,
                    0,
                    2,
                    2,
                    2,
                    262,
                    0
                ],
                "published": "2024-01-11T03:04:38Z",
                "published_parsed": [
                    2024,
                    1,
                    11,
                    3,
                    4,
                    38,
                    3,
                    11,
                    0
                ],
                "title": "Natural Language Processing for Dialects of a Language: A Survey",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Natural Language Processing for Dialects of a Language: A Survey"
                },
                "summary": "State-of-the-art natural language processing (NLP) models are trained on\nmassive training corpora, and report a superlative performance on evaluation\ndatasets. This survey delves into an important attribute of these datasets: the\ndialect of a language. Motivated by the performance degradation of NLP models\nfor dialectic datasets and its implications for the equity of language\ntechnologies, we survey past research in NLP for dialects in terms of datasets,\nand approaches. We describe a wide range of NLP tasks in terms of two\ncategories: natural language understanding (NLU) (for tasks such as dialect\nclassification, sentiment analysis, parsing, and NLU benchmarks) and natural\nlanguage generation (NLG) (for summarisation, machine translation, and dialogue\nsystems). The survey is also broad in its coverage of languages which include\nEnglish, Arabic, German among others. We observe that past work in NLP\nconcerning dialects goes deeper than mere dialect classification, and . This\nincludes early approaches that used sentence transduction that lead to the\nrecent approaches that integrate hypernetworks into LoRA. We expect that this\nsurvey will be useful to NLP researchers interested in building equitable\nlanguage technologies by rethinking LLM benchmarks and model architectures.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "State-of-the-art natural language processing (NLP) models are trained on\nmassive training corpora, and report a superlative performance on evaluation\ndatasets. This survey delves into an important attribute of these datasets: the\ndialect of a language. Motivated by the performance degradation of NLP models\nfor dialectic datasets and its implications for the equity of language\ntechnologies, we survey past research in NLP for dialects in terms of datasets,\nand approaches. We describe a wide range of NLP tasks in terms of two\ncategories: natural language understanding (NLU) (for tasks such as dialect\nclassification, sentiment analysis, parsing, and NLU benchmarks) and natural\nlanguage generation (NLG) (for summarisation, machine translation, and dialogue\nsystems). The survey is also broad in its coverage of languages which include\nEnglish, Arabic, German among others. We observe that past work in NLP\nconcerning dialects goes deeper than mere dialect classification, and . This\nincludes early approaches that used sentence transduction that lead to the\nrecent approaches that integrate hypernetworks into LoRA. We expect that this\nsurvey will be useful to NLP researchers interested in building equitable\nlanguage technologies by rethinking LLM benchmarks and model architectures."
                },
                "authors": [
                    {
                        "name": "Aditya Joshi"
                    },
                    {
                        "name": "Raj Dabre"
                    },
                    {
                        "name": "Diptesh Kanojia"
                    },
                    {
                        "name": "Zhuang Li"
                    },
                    {
                        "name": "Haolan Zhan"
                    },
                    {
                        "name": "Gholamreza Haffari"
                    },
                    {
                        "name": "Doris Dippold"
                    }
                ],
                "author_detail": {
                    "name": "Doris Dippold"
                },
                "author": "Doris Dippold",
                "arxiv_comment": "The paper is under review at ACM Computing Surveys. Please reach out\n  to the authors in the case of feedback",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2401.05632v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2401.05632v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.11589v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.11589v1",
                "updated": "2024-09-17T22:34:33Z",
                "updated_parsed": [
                    2024,
                    9,
                    17,
                    22,
                    34,
                    33,
                    1,
                    261,
                    0
                ],
                "published": "2024-09-17T22:34:33Z",
                "published_parsed": [
                    2024,
                    9,
                    17,
                    22,
                    34,
                    33,
                    1,
                    261,
                    0
                ],
                "title": "ProSLM : A Prolog Synergized Language Model for explainable Domain\n  Specific Knowledge Based Question Answering",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ProSLM : A Prolog Synergized Language Model for explainable Domain\n  Specific Knowledge Based Question Answering"
                },
                "summary": "Neurosymbolic approaches can add robustness to opaque neural systems by\nincorporating explainable symbolic representations. However, previous\napproaches have not used formal logic to contextualize queries to and validate\noutputs of large language models (LLMs). We propose \\systemname{}, a novel\nneurosymbolic framework, to improve the robustness and reliability of LLMs in\nquestion-answering tasks. We provide \\systemname{} with a domain-specific\nknowledge base, a logical reasoning system, and an integration to an existing\nLLM. This framework has two capabilities (1) context gathering: generating\nexplainable and relevant context for a given query, and (2) validation:\nconfirming and validating the factual accuracy of a statement in accordance\nwith a knowledge base (KB). Our work opens a new area of neurosymbolic\ngenerative AI text validation and user personalization.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Neurosymbolic approaches can add robustness to opaque neural systems by\nincorporating explainable symbolic representations. However, previous\napproaches have not used formal logic to contextualize queries to and validate\noutputs of large language models (LLMs). We propose \\systemname{}, a novel\nneurosymbolic framework, to improve the robustness and reliability of LLMs in\nquestion-answering tasks. We provide \\systemname{} with a domain-specific\nknowledge base, a logical reasoning system, and an integration to an existing\nLLM. This framework has two capabilities (1) context gathering: generating\nexplainable and relevant context for a given query, and (2) validation:\nconfirming and validating the factual accuracy of a statement in accordance\nwith a knowledge base (KB). Our work opens a new area of neurosymbolic\ngenerative AI text validation and user personalization."
                },
                "authors": [
                    {
                        "name": "Priyesh Vakharia"
                    },
                    {
                        "name": "Abigail Kufeldt"
                    },
                    {
                        "name": "Max Meyers"
                    },
                    {
                        "name": "Ian Lane"
                    },
                    {
                        "name": "Leilani Gilpin"
                    }
                ],
                "author_detail": {
                    "name": "Leilani Gilpin"
                },
                "author": "Leilani Gilpin",
                "arxiv_doi": "10.1007/978-3-031-71170-1_23",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1007/978-3-031-71170-1_23",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2409.11589v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.11589v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "Accepted at NeSy 2024",
                "arxiv_journal_ref": "Springer, Lecture Notes on Computer Science (LNAI,volume 14980),\n  2024",
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.02257v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.02257v2",
                "updated": "2024-09-17T22:26:51Z",
                "updated_parsed": [
                    2024,
                    9,
                    17,
                    22,
                    26,
                    51,
                    1,
                    261,
                    0
                ],
                "published": "2024-09-03T19:31:03Z",
                "published_parsed": [
                    2024,
                    9,
                    3,
                    19,
                    31,
                    3,
                    1,
                    247,
                    0
                ],
                "title": "MMLU-Pro+: Evaluating Higher-Order Reasoning and Shortcut Learning in\n  LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MMLU-Pro+: Evaluating Higher-Order Reasoning and Shortcut Learning in\n  LLMs"
                },
                "summary": "Existing benchmarks for large language models (LLMs) increasingly struggle to\ndifferentiate between top-performing models, underscoring the need for more\nchallenging evaluation frameworks. We introduce MMLU-Pro+, an enhanced\nbenchmark building upon MMLU-Pro to assess shortcut learning and higher-order\nreasoning in LLMs. By incorporating questions with multiple correct answers\nacross diverse domains, MMLU-Pro+ tests LLMs' ability to engage in complex\nreasoning and resist simplistic problem-solving strategies. Our results show\nthat MMLU-Pro+ maintains MMLU-Pro's difficulty while providing a more rigorous\ntest of model discrimination, particularly in multi-correct answer scenarios.\nWe introduce novel metrics like shortcut selection ratio and correct pair\nidentification ratio, offering deeper insights into model behavior and\nanchoring bias. Evaluations of six state-of-the-art LLMs reveal significant\nperformance gaps, highlighting variations in reasoning abilities and bias\nsusceptibility. We release the dataset and evaluation codes at\n\\url{https://github.com/asgsaeid/mmlu-pro-plus}.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Existing benchmarks for large language models (LLMs) increasingly struggle to\ndifferentiate between top-performing models, underscoring the need for more\nchallenging evaluation frameworks. We introduce MMLU-Pro+, an enhanced\nbenchmark building upon MMLU-Pro to assess shortcut learning and higher-order\nreasoning in LLMs. By incorporating questions with multiple correct answers\nacross diverse domains, MMLU-Pro+ tests LLMs' ability to engage in complex\nreasoning and resist simplistic problem-solving strategies. Our results show\nthat MMLU-Pro+ maintains MMLU-Pro's difficulty while providing a more rigorous\ntest of model discrimination, particularly in multi-correct answer scenarios.\nWe introduce novel metrics like shortcut selection ratio and correct pair\nidentification ratio, offering deeper insights into model behavior and\nanchoring bias. Evaluations of six state-of-the-art LLMs reveal significant\nperformance gaps, highlighting variations in reasoning abilities and bias\nsusceptibility. We release the dataset and evaluation codes at\n\\url{https://github.com/asgsaeid/mmlu-pro-plus}."
                },
                "authors": [
                    {
                        "name": "Saeid Asgari Taghanaki"
                    },
                    {
                        "name": "Aliasgahr Khani"
                    },
                    {
                        "name": "Amir Khasahmadi"
                    }
                ],
                "author_detail": {
                    "name": "Amir Khasahmadi"
                },
                "author": "Amir Khasahmadi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.02257v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.02257v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.13940v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.13940v2",
                "updated": "2024-09-17T22:19:17Z",
                "updated_parsed": [
                    2024,
                    9,
                    17,
                    22,
                    19,
                    17,
                    1,
                    261,
                    0
                ],
                "published": "2024-08-25T21:20:17Z",
                "published_parsed": [
                    2024,
                    8,
                    25,
                    21,
                    20,
                    17,
                    6,
                    238,
                    0
                ],
                "title": "CoT Rerailer: Enhancing the Reliability of Large Language Models in\n  Complex Reasoning Tasks through Error Detection and Correction",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CoT Rerailer: Enhancing the Reliability of Large Language Models in\n  Complex Reasoning Tasks through Error Detection and Correction"
                },
                "summary": "Chain-of-Thought (CoT) prompting enhances Large Language Models (LLMs)\ncomplex reasoning abilities by generating intermediate steps. However, these\nsteps can introduce hallucinations and accumulate errors. We propose the CoT\nRerailer to address these challenges, employing self-consistency and\nmulti-agent debate systems to identify and rectify errors in the reasoning\nprocess. The CoT Rerailer first selects the most logically correct Reasoning\nPath (RP) using consistency checks and critical evaluation by automated agents.\nIt then engages a multi-agent debate system to propose and validate corrections\nto ensure the generation of an error-free intermediate logical path. The\ncorrected steps are then used to generate a revised reasoning chain to further\nreduce hallucinations and enhance answer quality. We demonstrate the\neffectiveness of our approach across diverse question-answering datasets in\nvarious knowledge domains. The CoT Rerailer enhances the reliability of\nLLM-generated reasoning, contributing to more trustworthy AI driven\ndecision-making processes.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Chain-of-Thought (CoT) prompting enhances Large Language Models (LLMs)\ncomplex reasoning abilities by generating intermediate steps. However, these\nsteps can introduce hallucinations and accumulate errors. We propose the CoT\nRerailer to address these challenges, employing self-consistency and\nmulti-agent debate systems to identify and rectify errors in the reasoning\nprocess. The CoT Rerailer first selects the most logically correct Reasoning\nPath (RP) using consistency checks and critical evaluation by automated agents.\nIt then engages a multi-agent debate system to propose and validate corrections\nto ensure the generation of an error-free intermediate logical path. The\ncorrected steps are then used to generate a revised reasoning chain to further\nreduce hallucinations and enhance answer quality. We demonstrate the\neffectiveness of our approach across diverse question-answering datasets in\nvarious knowledge domains. The CoT Rerailer enhances the reliability of\nLLM-generated reasoning, contributing to more trustworthy AI driven\ndecision-making processes."
                },
                "authors": [
                    {
                        "name": "Guangya Wan"
                    },
                    {
                        "name": "Yuqi Wu"
                    },
                    {
                        "name": "Jie Chen"
                    },
                    {
                        "name": "Sheng Li"
                    }
                ],
                "author_detail": {
                    "name": "Sheng Li"
                },
                "author": "Sheng Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.13940v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.13940v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.11580v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.11580v1",
                "updated": "2024-09-17T22:12:07Z",
                "updated_parsed": [
                    2024,
                    9,
                    17,
                    22,
                    12,
                    7,
                    1,
                    261,
                    0
                ],
                "published": "2024-09-17T22:12:07Z",
                "published_parsed": [
                    2024,
                    9,
                    17,
                    22,
                    12,
                    7,
                    1,
                    261,
                    0
                ],
                "title": "PLATO: Planning with LLMs and Affordances for Tool Manipulation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PLATO: Planning with LLMs and Affordances for Tool Manipulation"
                },
                "summary": "As robotic systems become increasingly integrated into complex real-world\nenvironments, there is a growing need for approaches that enable robots to\nunderstand and act upon natural language instructions without relying on\nextensive pre-programmed knowledge of their surroundings. This paper presents\nPLATO, an innovative system that addresses this challenge by leveraging\nspecialized large language model agents to process natural language inputs,\nunderstand the environment, predict tool affordances, and generate executable\nactions for robotic systems. Unlike traditional systems that depend on\nhard-coded environmental information, PLATO employs a modular architecture of\nspecialized agents to operate without any initial knowledge of the environment.\nThese agents identify objects and their locations within the scene, generate a\ncomprehensive high-level plan, translate this plan into a series of low-level\nactions, and verify the completion of each step. The system is particularly\ntested on challenging tool-use tasks, which involve handling diverse objects\nand require long-horizon planning. PLATO's design allows it to adapt to dynamic\nand unstructured settings, significantly enhancing its flexibility and\nrobustness. By evaluating the system across various complex scenarios, we\ndemonstrate its capability to tackle a diverse range of tasks and offer a novel\nsolution to integrate LLMs with robotic platforms, advancing the\nstate-of-the-art in autonomous robotic task execution. For videos and prompt\ndetails, please see our project website:\nhttps://sites.google.com/andrew.cmu.edu/plato",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As robotic systems become increasingly integrated into complex real-world\nenvironments, there is a growing need for approaches that enable robots to\nunderstand and act upon natural language instructions without relying on\nextensive pre-programmed knowledge of their surroundings. This paper presents\nPLATO, an innovative system that addresses this challenge by leveraging\nspecialized large language model agents to process natural language inputs,\nunderstand the environment, predict tool affordances, and generate executable\nactions for robotic systems. Unlike traditional systems that depend on\nhard-coded environmental information, PLATO employs a modular architecture of\nspecialized agents to operate without any initial knowledge of the environment.\nThese agents identify objects and their locations within the scene, generate a\ncomprehensive high-level plan, translate this plan into a series of low-level\nactions, and verify the completion of each step. The system is particularly\ntested on challenging tool-use tasks, which involve handling diverse objects\nand require long-horizon planning. PLATO's design allows it to adapt to dynamic\nand unstructured settings, significantly enhancing its flexibility and\nrobustness. By evaluating the system across various complex scenarios, we\ndemonstrate its capability to tackle a diverse range of tasks and offer a novel\nsolution to integrate LLMs with robotic platforms, advancing the\nstate-of-the-art in autonomous robotic task execution. For videos and prompt\ndetails, please see our project website:\nhttps://sites.google.com/andrew.cmu.edu/plato"
                },
                "authors": [
                    {
                        "name": "Arvind Car"
                    },
                    {
                        "name": "Sai Sravan Yarlagadda"
                    },
                    {
                        "name": "Alison Bartsch"
                    },
                    {
                        "name": "Abraham George"
                    },
                    {
                        "name": "Amir Barati Farimani"
                    }
                ],
                "author_detail": {
                    "name": "Amir Barati Farimani"
                },
                "author": "Amir Barati Farimani",
                "arxiv_comment": "7 pages, 4 figures, submitted to ICRA 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.11580v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.11580v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.11579v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.11579v1",
                "updated": "2024-09-17T22:06:46Z",
                "updated_parsed": [
                    2024,
                    9,
                    17,
                    22,
                    6,
                    46,
                    1,
                    261,
                    0
                ],
                "published": "2024-09-17T22:06:46Z",
                "published_parsed": [
                    2024,
                    9,
                    17,
                    22,
                    6,
                    46,
                    1,
                    261,
                    0
                ],
                "title": "HEARTS: A Holistic Framework for Explainable, Sustainable and Robust\n  Text Stereotype Detection",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "HEARTS: A Holistic Framework for Explainable, Sustainable and Robust\n  Text Stereotype Detection"
                },
                "summary": "Stereotypes are generalised assumptions about societal groups, and even\nstate-of-the-art LLMs using in-context learning struggle to identify them\naccurately. Due to the subjective nature of stereotypes, where what constitutes\na stereotype can vary widely depending on cultural, social, and individual\nperspectives, robust explainability is crucial. Explainable models ensure that\nthese nuanced judgments can be understood and validated by human users,\npromoting trust and accountability. We address these challenges by introducing\nHEARTS (Holistic Framework for Explainable, Sustainable, and Robust Text\nStereotype Detection), a framework that enhances model performance, minimises\ncarbon footprint, and provides transparent, interpretable explanations. We\nestablish the Expanded Multi-Grain Stereotype Dataset (EMGSD), comprising\n57,201 labeled texts across six groups, including under-represented\ndemographics like LGBTQ+ and regional stereotypes. Ablation studies confirm\nthat BERT models fine-tuned on EMGSD outperform those trained on individual\ncomponents. We then analyse a fine-tuned, carbon-efficient ALBERT-V2 model\nusing SHAP to generate token-level importance values, ensuring alignment with\nhuman understanding, and calculate explainability confidence scores by\ncomparing SHAP and LIME outputs. Finally, HEARTS is applied to assess\nstereotypical bias in 12 LLM outputs, revealing a gradual reduction in bias\nover time within model families.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Stereotypes are generalised assumptions about societal groups, and even\nstate-of-the-art LLMs using in-context learning struggle to identify them\naccurately. Due to the subjective nature of stereotypes, where what constitutes\na stereotype can vary widely depending on cultural, social, and individual\nperspectives, robust explainability is crucial. Explainable models ensure that\nthese nuanced judgments can be understood and validated by human users,\npromoting trust and accountability. We address these challenges by introducing\nHEARTS (Holistic Framework for Explainable, Sustainable, and Robust Text\nStereotype Detection), a framework that enhances model performance, minimises\ncarbon footprint, and provides transparent, interpretable explanations. We\nestablish the Expanded Multi-Grain Stereotype Dataset (EMGSD), comprising\n57,201 labeled texts across six groups, including under-represented\ndemographics like LGBTQ+ and regional stereotypes. Ablation studies confirm\nthat BERT models fine-tuned on EMGSD outperform those trained on individual\ncomponents. We then analyse a fine-tuned, carbon-efficient ALBERT-V2 model\nusing SHAP to generate token-level importance values, ensuring alignment with\nhuman understanding, and calculate explainability confidence scores by\ncomparing SHAP and LIME outputs. Finally, HEARTS is applied to assess\nstereotypical bias in 12 LLM outputs, revealing a gradual reduction in bias\nover time within model families."
                },
                "authors": [
                    {
                        "name": "Theo King"
                    },
                    {
                        "name": "Zekun Wu"
                    },
                    {
                        "name": "Adriano Koshiyama"
                    },
                    {
                        "name": "Emre Kazim"
                    },
                    {
                        "name": "Philip Treleaven"
                    }
                ],
                "author_detail": {
                    "name": "Philip Treleaven"
                },
                "author": "Philip Treleaven",
                "arxiv_comment": "Submitted to NeurIPS 2024 SoLaR Workshop",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.11579v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.11579v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.01862v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.01862v3",
                "updated": "2024-09-17T22:00:36Z",
                "updated_parsed": [
                    2024,
                    9,
                    17,
                    22,
                    0,
                    36,
                    1,
                    261,
                    0
                ],
                "published": "2024-06-04T00:26:12Z",
                "published_parsed": [
                    2024,
                    6,
                    4,
                    0,
                    26,
                    12,
                    1,
                    156,
                    0
                ],
                "title": "Charting the Landscape of Nefarious Uses of Generative Artificial\n  Intelligence for Online Election Interference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Charting the Landscape of Nefarious Uses of Generative Artificial\n  Intelligence for Online Election Interference"
                },
                "summary": "Generative Artificial Intelligence (GenAI) and Large Language Models (LLMs)\npose significant risks, particularly in the realm of online election\ninterference. This paper explores the nefarious applications of GenAI,\nhighlighting their potential to disrupt democratic processes through deepfakes,\nbotnets, targeted misinformation campaigns, and synthetic identities. By\nexamining recent case studies and public incidents, we illustrate how malicious\nactors exploit these technologies to try influencing voter behavior, spread\ndisinformation, and undermine public trust in electoral systems. The paper also\ndiscusses the societal implications of these threats, emphasizing the urgent\nneed for robust mitigation strategies and international cooperation to\nsafeguard democratic integrity.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Generative Artificial Intelligence (GenAI) and Large Language Models (LLMs)\npose significant risks, particularly in the realm of online election\ninterference. This paper explores the nefarious applications of GenAI,\nhighlighting their potential to disrupt democratic processes through deepfakes,\nbotnets, targeted misinformation campaigns, and synthetic identities. By\nexamining recent case studies and public incidents, we illustrate how malicious\nactors exploit these technologies to try influencing voter behavior, spread\ndisinformation, and undermine public trust in electoral systems. The paper also\ndiscusses the societal implications of these threats, emphasizing the urgent\nneed for robust mitigation strategies and international cooperation to\nsafeguard democratic integrity."
                },
                "authors": [
                    {
                        "name": "Emilio Ferrara"
                    }
                ],
                "author_detail": {
                    "name": "Emilio Ferrara"
                },
                "author": "Emilio Ferrara",
                "arxiv_comment": "The 2024 Election Integrity Initiative: HUMANS Lab - Working Paper\n  No. 2024.1",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.01862v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.01862v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.11570v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.11570v1",
                "updated": "2024-09-17T21:52:09Z",
                "updated_parsed": [
                    2024,
                    9,
                    17,
                    21,
                    52,
                    9,
                    1,
                    261,
                    0
                ],
                "published": "2024-09-17T21:52:09Z",
                "published_parsed": [
                    2024,
                    9,
                    17,
                    21,
                    52,
                    9,
                    1,
                    261,
                    0
                ],
                "title": "VertiEncoder: Self-Supervised Kinodynamic Representation Learning on\n  Vertically Challenging Terrain",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "VertiEncoder: Self-Supervised Kinodynamic Representation Learning on\n  Vertically Challenging Terrain"
                },
                "summary": "We present VertiEncoder, a self-supervised representation learning approach\nfor robot mobility on vertically challenging terrain. Using the same\npre-training process, VertiEncoder can handle four different downstream tasks,\nincluding forward kinodynamics learning, inverse kinodynamics learning,\nbehavior cloning, and patch reconstruction with a single representation.\nVertiEncoder uses a TransformerEncoder to learn the local context of its\nsurroundings by random masking and next patch reconstruction. We show that\nVertiEncoder achieves better performance across all four different tasks\ncompared to specialized End-to-End models with 77% fewer parameters. We also\nshow VertiEncoder's comparable performance against state-of-the-art kinodynamic\nmodeling and planning approaches in real-world robot deployment. These results\nunderscore the efficacy of VertiEncoder in mitigating overfitting and fostering\nmore robust generalization across diverse environmental contexts and downstream\nvehicle kinodynamic tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present VertiEncoder, a self-supervised representation learning approach\nfor robot mobility on vertically challenging terrain. Using the same\npre-training process, VertiEncoder can handle four different downstream tasks,\nincluding forward kinodynamics learning, inverse kinodynamics learning,\nbehavior cloning, and patch reconstruction with a single representation.\nVertiEncoder uses a TransformerEncoder to learn the local context of its\nsurroundings by random masking and next patch reconstruction. We show that\nVertiEncoder achieves better performance across all four different tasks\ncompared to specialized End-to-End models with 77% fewer parameters. We also\nshow VertiEncoder's comparable performance against state-of-the-art kinodynamic\nmodeling and planning approaches in real-world robot deployment. These results\nunderscore the efficacy of VertiEncoder in mitigating overfitting and fostering\nmore robust generalization across diverse environmental contexts and downstream\nvehicle kinodynamic tasks."
                },
                "authors": [
                    {
                        "name": "Mohammad Nazeri"
                    },
                    {
                        "name": "Aniket Datar"
                    },
                    {
                        "name": "Anuj Pokhrel"
                    },
                    {
                        "name": "Chenhui Pan"
                    },
                    {
                        "name": "Garrett Warnell"
                    },
                    {
                        "name": "Xuesu Xiao"
                    }
                ],
                "author_detail": {
                    "name": "Xuesu Xiao"
                },
                "author": "Xuesu Xiao",
                "arxiv_comment": "8 pages. Code: https://github.com/mhnazeri/VertiEncoder",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.11570v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.11570v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2311.09336v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2311.09336v4",
                "updated": "2024-09-17T21:33:49Z",
                "updated_parsed": [
                    2024,
                    9,
                    17,
                    21,
                    33,
                    49,
                    1,
                    261,
                    0
                ],
                "published": "2023-11-15T19:52:11Z",
                "published_parsed": [
                    2023,
                    11,
                    15,
                    19,
                    52,
                    11,
                    2,
                    319,
                    0
                ],
                "title": "Fine-grained LLM Agent: Pinpointing and Refining Large Language Models\n  via Fine-Grained Actionable Feedback",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Fine-grained LLM Agent: Pinpointing and Refining Large Language Models\n  via Fine-Grained Actionable Feedback"
                },
                "summary": "Recent large language models (LLM) are leveraging human feedback to improve\ntheir generation quality. However, human feedback is costly to obtain,\nespecially during inference. In this work, we propose Fine-grained LLM agent,\nan inference time optimization method to refine LLM's output. The core idea is\nto use a learned fine-grained feedback model to pinpoint defects and guide LLM\nto refine them iteratively. Using original LLM as a proposal of edits,\nFine-grained LLM agent searches for defect-less text via simulated annealing,\ntrading off the exploration and exploitation. We conduct experiments on three\ntext generation tasks, including machine translation, long-form question\nanswering (QA), and topical summarization. Fine-grained LLM agent consistently\noutperforms all baseline approaches, achieving improvements up to 1.7 MetricX\npoints on translation tasks, 8.1 ROUGE-L on ASQA, 2.2 ROUGE-L on topical\nsummarization.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent large language models (LLM) are leveraging human feedback to improve\ntheir generation quality. However, human feedback is costly to obtain,\nespecially during inference. In this work, we propose Fine-grained LLM agent,\nan inference time optimization method to refine LLM's output. The core idea is\nto use a learned fine-grained feedback model to pinpoint defects and guide LLM\nto refine them iteratively. Using original LLM as a proposal of edits,\nFine-grained LLM agent searches for defect-less text via simulated annealing,\ntrading off the exploration and exploitation. We conduct experiments on three\ntext generation tasks, including machine translation, long-form question\nanswering (QA), and topical summarization. Fine-grained LLM agent consistently\noutperforms all baseline approaches, achieving improvements up to 1.7 MetricX\npoints on translation tasks, 8.1 ROUGE-L on ASQA, 2.2 ROUGE-L on topical\nsummarization."
                },
                "authors": [
                    {
                        "name": "Wenda Xu"
                    },
                    {
                        "name": "Daniel Deutsch"
                    },
                    {
                        "name": "Mara Finkelstein"
                    },
                    {
                        "name": "Juraj Juraska"
                    },
                    {
                        "name": "Biao Zhang"
                    },
                    {
                        "name": "Zhongtao Liu"
                    },
                    {
                        "name": "William Yang Wang"
                    },
                    {
                        "name": "Lei Li"
                    },
                    {
                        "name": "Markus Freitag"
                    }
                ],
                "author_detail": {
                    "name": "Markus Freitag"
                },
                "author": "Markus Freitag",
                "arxiv_comment": "Accepted to NAACL 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2311.09336v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2311.09336v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.17967v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.17967v2",
                "updated": "2024-09-17T21:29:13Z",
                "updated_parsed": [
                    2024,
                    9,
                    17,
                    21,
                    29,
                    13,
                    1,
                    261,
                    0
                ],
                "published": "2024-06-25T22:49:17Z",
                "published_parsed": [
                    2024,
                    6,
                    25,
                    22,
                    49,
                    17,
                    1,
                    177,
                    0
                ],
                "title": "Unmasking the Imposters: How Censorship and Domain Adaptation Affect the\n  Detection of Machine-Generated Tweets",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Unmasking the Imposters: How Censorship and Domain Adaptation Affect the\n  Detection of Machine-Generated Tweets"
                },
                "summary": "The rapid development of large language models (LLMs) has significantly\nimproved the generation of fluent and convincing text, raising concerns about\ntheir potential misuse on social media platforms. We present a comprehensive\nmethodology for creating nine Twitter datasets to examine the generative\ncapabilities of four prominent LLMs: Llama 3, Mistral, Qwen2, and GPT4o. These\ndatasets encompass four censored and five uncensored model configurations,\nincluding 7B and 8B parameter base-instruction models of the three open-source\nLLMs. Additionally, we perform a data quality analysis to assess the\ncharacteristics of textual outputs from human, \"censored,\" and \"uncensored\"\nmodels, employing semantic meaning, lexical richness, structural patterns,\ncontent characteristics, and detector performance metrics to identify\ndifferences and similarities. Our evaluation demonstrates that \"uncensored\"\nmodels significantly undermine the effectiveness of automated detection\nmethods. This study addresses a critical gap by exploring smaller open-source\nmodels and the ramifications of \"uncensoring,\" providing valuable insights into\nhow domain adaptation and content moderation strategies influence both the\ndetectability and structural characteristics of machine-generated text.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rapid development of large language models (LLMs) has significantly\nimproved the generation of fluent and convincing text, raising concerns about\ntheir potential misuse on social media platforms. We present a comprehensive\nmethodology for creating nine Twitter datasets to examine the generative\ncapabilities of four prominent LLMs: Llama 3, Mistral, Qwen2, and GPT4o. These\ndatasets encompass four censored and five uncensored model configurations,\nincluding 7B and 8B parameter base-instruction models of the three open-source\nLLMs. Additionally, we perform a data quality analysis to assess the\ncharacteristics of textual outputs from human, \"censored,\" and \"uncensored\"\nmodels, employing semantic meaning, lexical richness, structural patterns,\ncontent characteristics, and detector performance metrics to identify\ndifferences and similarities. Our evaluation demonstrates that \"uncensored\"\nmodels significantly undermine the effectiveness of automated detection\nmethods. This study addresses a critical gap by exploring smaller open-source\nmodels and the ramifications of \"uncensoring,\" providing valuable insights into\nhow domain adaptation and content moderation strategies influence both the\ndetectability and structural characteristics of machine-generated text."
                },
                "authors": [
                    {
                        "name": "Bryan E. Tuck"
                    },
                    {
                        "name": "Rakesh M. Verma"
                    }
                ],
                "author_detail": {
                    "name": "Rakesh M. Verma"
                },
                "author": "Rakesh M. Verma",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.17967v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.17967v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.11547v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.11547v1",
                "updated": "2024-09-17T20:40:02Z",
                "updated_parsed": [
                    2024,
                    9,
                    17,
                    20,
                    40,
                    2,
                    1,
                    261,
                    0
                ],
                "published": "2024-09-17T20:40:02Z",
                "published_parsed": [
                    2024,
                    9,
                    17,
                    20,
                    40,
                    2,
                    1,
                    261,
                    0
                ],
                "title": "Small Language Models can Outperform Humans in Short Creative Writing: A\n  Study Comparing SLMs with Humans and LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Small Language Models can Outperform Humans in Short Creative Writing: A\n  Study Comparing SLMs with Humans and LLMs"
                },
                "summary": "In this paper, we evaluate the creative fiction writing abilities of a\nfine-tuned small language model (SLM), BART Large, and compare its performance\nto humans and two large language models (LLMs): GPT-3.5 and GPT-4o. Our\nevaluation consists of two experiments: (i) a human evaluation where readers\nassess the stories generated by the SLM compared to human-written stories, and\n(ii) a qualitative linguistic analysis comparing the textual characteristics of\nthe stories generated by the different models. In the first experiment, we\nasked 68 participants to rate short stories generated by the models and humans\nalong dimensions such as grammaticality, relevance, creativity, and\nattractiveness. BART Large outperformed human writers in most aspects, except\ncreativity, with an overall score of 2.11 compared to 1.85 for human-written\ntexts -- a 14% improvement. In the second experiment, the qualitative analysis\nrevealed that, while GPT-4o exhibited near-perfect internal and external\ncoherence, it tended to produce more predictable narratives, with only 3% of\nits stories seen as novel. In contrast, 15% of BART's stories were considered\nnovel, indicating a higher degree of creativity despite its smaller model size.\nThis study provides both quantitative and qualitative insights into how model\nsize and fine-tuning influence the balance between creativity, fluency, and\ncoherence in creative writing tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this paper, we evaluate the creative fiction writing abilities of a\nfine-tuned small language model (SLM), BART Large, and compare its performance\nto humans and two large language models (LLMs): GPT-3.5 and GPT-4o. Our\nevaluation consists of two experiments: (i) a human evaluation where readers\nassess the stories generated by the SLM compared to human-written stories, and\n(ii) a qualitative linguistic analysis comparing the textual characteristics of\nthe stories generated by the different models. In the first experiment, we\nasked 68 participants to rate short stories generated by the models and humans\nalong dimensions such as grammaticality, relevance, creativity, and\nattractiveness. BART Large outperformed human writers in most aspects, except\ncreativity, with an overall score of 2.11 compared to 1.85 for human-written\ntexts -- a 14% improvement. In the second experiment, the qualitative analysis\nrevealed that, while GPT-4o exhibited near-perfect internal and external\ncoherence, it tended to produce more predictable narratives, with only 3% of\nits stories seen as novel. In contrast, 15% of BART's stories were considered\nnovel, indicating a higher degree of creativity despite its smaller model size.\nThis study provides both quantitative and qualitative insights into how model\nsize and fine-tuning influence the balance between creativity, fluency, and\ncoherence in creative writing tasks."
                },
                "authors": [
                    {
                        "name": "Guillermo Marco"
                    },
                    {
                        "name": "Luz Rello"
                    },
                    {
                        "name": "Julio Gonzalo"
                    }
                ],
                "author_detail": {
                    "name": "Julio Gonzalo"
                },
                "author": "Julio Gonzalo",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.11547v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.11547v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.11540v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.11540v1",
                "updated": "2024-09-17T20:23:36Z",
                "updated_parsed": [
                    2024,
                    9,
                    17,
                    20,
                    23,
                    36,
                    1,
                    261,
                    0
                ],
                "published": "2024-09-17T20:23:36Z",
                "published_parsed": [
                    2024,
                    9,
                    17,
                    20,
                    23,
                    36,
                    1,
                    261,
                    0
                ],
                "title": "What Does ChatGPT Make of Historical Stock Returns? Extrapolation and\n  Miscalibration in LLM Stock Return Forecasts",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "What Does ChatGPT Make of Historical Stock Returns? Extrapolation and\n  Miscalibration in LLM Stock Return Forecasts"
                },
                "summary": "We examine how large language models (LLMs) interpret historical stock\nreturns and compare their forecasts with estimates from a crowd-sourced\nplatform for ranking stocks. While stock returns exhibit short-term reversals,\nLLM forecasts over-extrapolate, placing excessive weight on recent performance\nsimilar to humans. LLM forecasts appear optimistic relative to historical and\nfuture realized returns. When prompted for 80% confidence interval predictions,\nLLM responses are better calibrated than survey evidence but are pessimistic\nabout outliers, leading to skewed forecast distributions. The findings suggest\nLLMs manifest common behavioral biases when forecasting expected returns but\nare better at gauging risks than humans.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We examine how large language models (LLMs) interpret historical stock\nreturns and compare their forecasts with estimates from a crowd-sourced\nplatform for ranking stocks. While stock returns exhibit short-term reversals,\nLLM forecasts over-extrapolate, placing excessive weight on recent performance\nsimilar to humans. LLM forecasts appear optimistic relative to historical and\nfuture realized returns. When prompted for 80% confidence interval predictions,\nLLM responses are better calibrated than survey evidence but are pessimistic\nabout outliers, leading to skewed forecast distributions. The findings suggest\nLLMs manifest common behavioral biases when forecasting expected returns but\nare better at gauging risks than humans."
                },
                "authors": [
                    {
                        "name": "Shuaiyu Chen"
                    },
                    {
                        "name": "T. Clifton Green"
                    },
                    {
                        "name": "Huseyin Gulen"
                    },
                    {
                        "name": "Dexin Zhou"
                    }
                ],
                "author_detail": {
                    "name": "Dexin Zhou"
                },
                "author": "Dexin Zhou",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.11540v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.11540v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "q-fin.GN",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "q-fin.GN",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "econ.GN",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "q-fin.EC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.11538v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.11538v1",
                "updated": "2024-09-17T20:16:43Z",
                "updated_parsed": [
                    2024,
                    9,
                    17,
                    20,
                    16,
                    43,
                    1,
                    261,
                    0
                ],
                "published": "2024-09-17T20:16:43Z",
                "published_parsed": [
                    2024,
                    9,
                    17,
                    20,
                    16,
                    43,
                    1,
                    261,
                    0
                ],
                "title": "Chain-of-Thought Prompting for Speech Translation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Chain-of-Thought Prompting for Speech Translation"
                },
                "summary": "Large language models (LLMs) have demonstrated remarkable advancements in\nlanguage understanding and generation. Building on the success of text-based\nLLMs, recent research has adapted these models to use speech embeddings for\nprompting, resulting in Speech-LLM models that exhibit strong performance in\nautomatic speech recognition (ASR) and automatic speech translation (AST). In\nthis work, we propose a novel approach to leverage ASR transcripts as prompts\nfor AST in a Speech-LLM built on an encoder-decoder text LLM. The Speech-LLM\nmodel consists of a speech encoder and an encoder-decoder structure\nMegatron-T5. By first decoding speech to generate ASR transcripts and\nsubsequently using these transcripts along with encoded speech for prompting,\nwe guide the speech translation in a two-step process like chain-of-thought\n(CoT) prompting. Low-rank adaptation (LoRA) is used for the T5 LLM for model\nadaptation and shows superior performance to full model fine-tuning.\nExperimental results show that the proposed CoT prompting significantly\nimproves AST performance, achieving an average increase of 2.4 BLEU points\nacross 6 En->X or X->En AST tasks compared to speech prompting alone.\nAdditionally, compared to a related CoT prediction method that predicts a\nconcatenated sequence of ASR and AST transcripts, our method performs better by\nan average of 2 BLEU points.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have demonstrated remarkable advancements in\nlanguage understanding and generation. Building on the success of text-based\nLLMs, recent research has adapted these models to use speech embeddings for\nprompting, resulting in Speech-LLM models that exhibit strong performance in\nautomatic speech recognition (ASR) and automatic speech translation (AST). In\nthis work, we propose a novel approach to leverage ASR transcripts as prompts\nfor AST in a Speech-LLM built on an encoder-decoder text LLM. The Speech-LLM\nmodel consists of a speech encoder and an encoder-decoder structure\nMegatron-T5. By first decoding speech to generate ASR transcripts and\nsubsequently using these transcripts along with encoded speech for prompting,\nwe guide the speech translation in a two-step process like chain-of-thought\n(CoT) prompting. Low-rank adaptation (LoRA) is used for the T5 LLM for model\nadaptation and shows superior performance to full model fine-tuning.\nExperimental results show that the proposed CoT prompting significantly\nimproves AST performance, achieving an average increase of 2.4 BLEU points\nacross 6 En->X or X->En AST tasks compared to speech prompting alone.\nAdditionally, compared to a related CoT prediction method that predicts a\nconcatenated sequence of ASR and AST transcripts, our method performs better by\nan average of 2 BLEU points."
                },
                "authors": [
                    {
                        "name": "Ke Hu"
                    },
                    {
                        "name": "Zhehuai Chen"
                    },
                    {
                        "name": "Chao-Han Huck Yang"
                    },
                    {
                        "name": "Piotr Żelasko"
                    },
                    {
                        "name": "Oleksii Hrinchuk"
                    },
                    {
                        "name": "Vitaly Lavrukhin"
                    },
                    {
                        "name": "Jagadeesh Balam"
                    },
                    {
                        "name": "Boris Ginsburg"
                    }
                ],
                "author_detail": {
                    "name": "Boris Ginsburg"
                },
                "author": "Boris Ginsburg",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.11538v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.11538v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.11527v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.11527v1",
                "updated": "2024-09-17T19:54:37Z",
                "updated_parsed": [
                    2024,
                    9,
                    17,
                    19,
                    54,
                    37,
                    1,
                    261,
                    0
                ],
                "published": "2024-09-17T19:54:37Z",
                "published_parsed": [
                    2024,
                    9,
                    17,
                    19,
                    54,
                    37,
                    1,
                    261,
                    0
                ],
                "title": "Improving LLM Reasoning with Multi-Agent Tree-of-Thought Validator Agent",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Improving LLM Reasoning with Multi-Agent Tree-of-Thought Validator Agent"
                },
                "summary": "Multi-agent strategies have emerged as a promising approach to enhance the\nreasoning abilities of Large Language Models (LLMs) by assigning specialized\nroles in the problem-solving process. Concurrently, Tree of Thoughts (ToT)\nmethods have shown potential in improving reasoning for complex\nquestion-answering tasks by exploring diverse reasoning paths. A critical\nlimitation in multi-agent reasoning is the 'Reasoner' agent's shallow\nexploration of reasoning paths. While ToT strategies could help mitigate this\nproblem, they may generate flawed reasoning branches, which could harm the\ntrustworthiness of the final answer. To leverage the strengths of both\nmulti-agent reasoning and ToT strategies, we introduce a novel approach\ncombining ToT-based Reasoner agents with a Thought Validator agent. Multiple\nReasoner agents operate in parallel, employing ToT to explore diverse reasoning\npaths. The Thought Validator then scrutinizes these paths, considering a\nReasoner's conclusion only if its reasoning is valid. This method enables a\nmore robust voting strategy by discarding faulty reasoning paths, enhancing the\nsystem's ability to tackle tasks requiring systematic and trustworthy\nreasoning. Our method demonstrates superior performance compared to existing\ntechniques when evaluated on the GSM8K dataset, outperforming the standard ToT\nstrategy by an average 5.6\\% across four LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-agent strategies have emerged as a promising approach to enhance the\nreasoning abilities of Large Language Models (LLMs) by assigning specialized\nroles in the problem-solving process. Concurrently, Tree of Thoughts (ToT)\nmethods have shown potential in improving reasoning for complex\nquestion-answering tasks by exploring diverse reasoning paths. A critical\nlimitation in multi-agent reasoning is the 'Reasoner' agent's shallow\nexploration of reasoning paths. While ToT strategies could help mitigate this\nproblem, they may generate flawed reasoning branches, which could harm the\ntrustworthiness of the final answer. To leverage the strengths of both\nmulti-agent reasoning and ToT strategies, we introduce a novel approach\ncombining ToT-based Reasoner agents with a Thought Validator agent. Multiple\nReasoner agents operate in parallel, employing ToT to explore diverse reasoning\npaths. The Thought Validator then scrutinizes these paths, considering a\nReasoner's conclusion only if its reasoning is valid. This method enables a\nmore robust voting strategy by discarding faulty reasoning paths, enhancing the\nsystem's ability to tackle tasks requiring systematic and trustworthy\nreasoning. Our method demonstrates superior performance compared to existing\ntechniques when evaluated on the GSM8K dataset, outperforming the standard ToT\nstrategy by an average 5.6\\% across four LLMs."
                },
                "authors": [
                    {
                        "name": "Fatemeh Haji"
                    },
                    {
                        "name": "Mazal Bethany"
                    },
                    {
                        "name": "Maryam Tabar"
                    },
                    {
                        "name": "Jason Chiang"
                    },
                    {
                        "name": "Anthony Rios"
                    },
                    {
                        "name": "Peyman Najafirad"
                    }
                ],
                "author_detail": {
                    "name": "Peyman Najafirad"
                },
                "author": "Peyman Najafirad",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.11527v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.11527v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.11501v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.11501v1",
                "updated": "2024-09-17T19:05:37Z",
                "updated_parsed": [
                    2024,
                    9,
                    17,
                    19,
                    5,
                    37,
                    1,
                    261,
                    0
                ],
                "published": "2024-09-17T19:05:37Z",
                "published_parsed": [
                    2024,
                    9,
                    17,
                    19,
                    5,
                    37,
                    1,
                    261,
                    0
                ],
                "title": "Egalitarian Language Representation in Language Models: It All Begins\n  with Tokenizers",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Egalitarian Language Representation in Language Models: It All Begins\n  with Tokenizers"
                },
                "summary": "Tokenizers act as a bridge between human language and the latent space of\nlanguage models, influencing how language is represented in these models. Due\nto the immense popularity of English-Centric Large Language Models (LLMs),\nefforts are being made to adapt them for other languages. However, we\ndemonstrate that, from a tokenization standpoint, not all tokenizers offer fair\nrepresentation for complex script languages such as Tamil, Sinhala, and Hindi,\nprimarily due to the choice of pre-tokenization methods. We go further to show\nthat pre-tokenization plays a more critical role than the tokenization\nalgorithm itself in achieving an egalitarian representation of these complex\nscript languages. To address this, we introduce an improvement to the Byte Pair\nEncoding (BPE) algorithm by incorporating graphemes, which we term Grapheme\nPair Encoding (GPE). Our experiments show that grapheme-based character\nextraction outperforms byte-level tokenizers for complex scripts. We validate\nthis approach through experiments on Tamil, Sinhala, and Hindi.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Tokenizers act as a bridge between human language and the latent space of\nlanguage models, influencing how language is represented in these models. Due\nto the immense popularity of English-Centric Large Language Models (LLMs),\nefforts are being made to adapt them for other languages. However, we\ndemonstrate that, from a tokenization standpoint, not all tokenizers offer fair\nrepresentation for complex script languages such as Tamil, Sinhala, and Hindi,\nprimarily due to the choice of pre-tokenization methods. We go further to show\nthat pre-tokenization plays a more critical role than the tokenization\nalgorithm itself in achieving an egalitarian representation of these complex\nscript languages. To address this, we introduce an improvement to the Byte Pair\nEncoding (BPE) algorithm by incorporating graphemes, which we term Grapheme\nPair Encoding (GPE). Our experiments show that grapheme-based character\nextraction outperforms byte-level tokenizers for complex scripts. We validate\nthis approach through experiments on Tamil, Sinhala, and Hindi."
                },
                "authors": [
                    {
                        "name": "Menan Velayuthan"
                    },
                    {
                        "name": "Kengatharaiyer Sarveswaran"
                    }
                ],
                "author_detail": {
                    "name": "Kengatharaiyer Sarveswaran"
                },
                "author": "Kengatharaiyer Sarveswaran",
                "arxiv_comment": "Content - 8 pages, References - 3 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.11501v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.11501v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.7",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.11500v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.11500v1",
                "updated": "2024-09-17T19:02:39Z",
                "updated_parsed": [
                    2024,
                    9,
                    17,
                    19,
                    2,
                    39,
                    1,
                    261,
                    0
                ],
                "published": "2024-09-17T19:02:39Z",
                "published_parsed": [
                    2024,
                    9,
                    17,
                    19,
                    2,
                    39,
                    1,
                    261,
                    0
                ],
                "title": "Multi-Document Grounded Multi-Turn Synthetic Dialog Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-Document Grounded Multi-Turn Synthetic Dialog Generation"
                },
                "summary": "We introduce a technique for multi-document grounded multi-turn synthetic\ndialog generation that incorporates three main ideas. First, we control the\noverall dialog flow using taxonomy-driven user queries that are generated with\nChain-of-Thought (CoT) prompting. Second, we support the generation of\nmulti-document grounded dialogs by mimicking real-world use of retrievers to\nupdate the grounding documents after every user-turn in the dialog. Third, we\napply LLM-as-a-Judge to filter out queries with incorrect answers. Human\nevaluation of the synthetic dialog data suggests that the data is diverse,\ncoherent, and includes mostly correct answers. Both human and automatic\nevaluations of answerable queries indicate that models fine-tuned on synthetic\ndialogs consistently out-perform those fine-tuned on existing human generated\ntraining data across four publicly available multi-turn document grounded\nbenchmark test sets.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce a technique for multi-document grounded multi-turn synthetic\ndialog generation that incorporates three main ideas. First, we control the\noverall dialog flow using taxonomy-driven user queries that are generated with\nChain-of-Thought (CoT) prompting. Second, we support the generation of\nmulti-document grounded dialogs by mimicking real-world use of retrievers to\nupdate the grounding documents after every user-turn in the dialog. Third, we\napply LLM-as-a-Judge to filter out queries with incorrect answers. Human\nevaluation of the synthetic dialog data suggests that the data is diverse,\ncoherent, and includes mostly correct answers. Both human and automatic\nevaluations of answerable queries indicate that models fine-tuned on synthetic\ndialogs consistently out-perform those fine-tuned on existing human generated\ntraining data across four publicly available multi-turn document grounded\nbenchmark test sets."
                },
                "authors": [
                    {
                        "name": "Young-Suk Lee"
                    },
                    {
                        "name": "Chulaka Gunasekara"
                    },
                    {
                        "name": "Danish Contractor"
                    },
                    {
                        "name": "Ramón Fernandez Astudillo"
                    },
                    {
                        "name": "Radu Florian"
                    }
                ],
                "author_detail": {
                    "name": "Radu Florian"
                },
                "author": "Radu Florian",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.11500v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.11500v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.11498v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.11498v1",
                "updated": "2024-09-17T19:00:21Z",
                "updated_parsed": [
                    2024,
                    9,
                    17,
                    19,
                    0,
                    21,
                    1,
                    261,
                    0
                ],
                "published": "2024-09-17T19:00:21Z",
                "published_parsed": [
                    2024,
                    9,
                    17,
                    19,
                    0,
                    21,
                    1,
                    261,
                    0
                ],
                "title": "Augment, Drop & Swap: Improving Diversity in LLM Captions for Efficient\n  Music-Text Representation Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Augment, Drop & Swap: Improving Diversity in LLM Captions for Efficient\n  Music-Text Representation Learning"
                },
                "summary": "Audio-text contrastive models have become a powerful approach in music\nrepresentation learning. Despite their empirical success, however, little is\nknown about the influence of key design choices on the quality of music-text\nrepresentations learnt through this framework. In this work, we expose these\ndesign choices within the constraints of limited data and computation budgets,\nand establish a more solid understanding of their impact grounded in empirical\nobservations along three axes: the choice of base encoders, the level of\ncuration in training data, and the use of text augmentation. We find that data\ncuration is the single most important factor for music-text contrastive\ntraining in resource-constrained scenarios. Motivated by this insight, we\nintroduce two novel techniques, Augmented View Dropout and TextSwap, which\nincrease the diversity and descriptiveness of text inputs seen in training.\nThrough our experiments we demonstrate that these are effective at boosting\nperformance across different pre-training regimes, model architectures, and\ndownstream data distributions, without incurring higher computational costs or\nrequiring additional training data.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Audio-text contrastive models have become a powerful approach in music\nrepresentation learning. Despite their empirical success, however, little is\nknown about the influence of key design choices on the quality of music-text\nrepresentations learnt through this framework. In this work, we expose these\ndesign choices within the constraints of limited data and computation budgets,\nand establish a more solid understanding of their impact grounded in empirical\nobservations along three axes: the choice of base encoders, the level of\ncuration in training data, and the use of text augmentation. We find that data\ncuration is the single most important factor for music-text contrastive\ntraining in resource-constrained scenarios. Motivated by this insight, we\nintroduce two novel techniques, Augmented View Dropout and TextSwap, which\nincrease the diversity and descriptiveness of text inputs seen in training.\nThrough our experiments we demonstrate that these are effective at boosting\nperformance across different pre-training regimes, model architectures, and\ndownstream data distributions, without incurring higher computational costs or\nrequiring additional training data."
                },
                "authors": [
                    {
                        "name": "Ilaria Manco"
                    },
                    {
                        "name": "Justin Salamon"
                    },
                    {
                        "name": "Oriol Nieto"
                    }
                ],
                "author_detail": {
                    "name": "Oriol Nieto"
                },
                "author": "Oriol Nieto",
                "arxiv_comment": "To appear in the Proceedings of the 25th International Society for\n  Music Information Retrieval Conference (ISMIR 2024)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.11498v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.11498v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SD",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.15045v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.15045v2",
                "updated": "2024-09-17T18:57:49Z",
                "updated_parsed": [
                    2024,
                    9,
                    17,
                    18,
                    57,
                    49,
                    1,
                    261,
                    0
                ],
                "published": "2024-06-21T10:48:21Z",
                "published_parsed": [
                    2024,
                    6,
                    21,
                    10,
                    48,
                    21,
                    4,
                    173,
                    0
                ],
                "title": "Integrating Knowledge Retrieval and Large Language Models for Clinical\n  Report Correction",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Integrating Knowledge Retrieval and Large Language Models for Clinical\n  Report Correction"
                },
                "summary": "This study proposes an approach for error correction in radiology reports,\nleveraging large language models (LLMs) and retrieval-augmented generation\n(RAG) techniques. The proposed framework employs a novel internal+external\nretrieval mechanism to extract relevant medical entities and relations from the\nreport of interest and an external knowledge source. A three-stage inference\nprocess is introduced, decomposing the task into error detection, localization,\nand correction subtasks, which enhances the explainability and performance of\nthe system. The effectiveness of the approach is evaluated using a benchmark\ndataset created by corrupting real-world radiology reports with realistic\nerrors, guided by domain experts. Experimental results demonstrate the benefits\nof the proposed methods, with the combination of internal and external\nretrieval significantly improving the accuracy of error detection,\nlocalization, and correction across various state-of-the-art LLMs. The findings\ncontribute to the development of more robust and reliable error correction\nsystems for clinical documentation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This study proposes an approach for error correction in radiology reports,\nleveraging large language models (LLMs) and retrieval-augmented generation\n(RAG) techniques. The proposed framework employs a novel internal+external\nretrieval mechanism to extract relevant medical entities and relations from the\nreport of interest and an external knowledge source. A three-stage inference\nprocess is introduced, decomposing the task into error detection, localization,\nand correction subtasks, which enhances the explainability and performance of\nthe system. The effectiveness of the approach is evaluated using a benchmark\ndataset created by corrupting real-world radiology reports with realistic\nerrors, guided by domain experts. Experimental results demonstrate the benefits\nof the proposed methods, with the combination of internal and external\nretrieval significantly improving the accuracy of error detection,\nlocalization, and correction across various state-of-the-art LLMs. The findings\ncontribute to the development of more robust and reliable error correction\nsystems for clinical documentation."
                },
                "authors": [
                    {
                        "name": "Jinge Wu"
                    },
                    {
                        "name": "Zhaolong Wu"
                    },
                    {
                        "name": "Ruizhe Li"
                    },
                    {
                        "name": "Abul Hasan"
                    },
                    {
                        "name": "Yunsoo Kim"
                    },
                    {
                        "name": "Jason P. Y. Cheung"
                    },
                    {
                        "name": "Teng Zhang"
                    },
                    {
                        "name": "Honghan Wu"
                    }
                ],
                "author_detail": {
                    "name": "Honghan Wu"
                },
                "author": "Honghan Wu",
                "arxiv_comment": "v2",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.15045v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.15045v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            }
        ]
    }
]