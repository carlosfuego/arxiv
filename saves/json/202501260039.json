[
    {
        "keyword": "kv cache",
        "arxiv_results": [
            {
                "id": "http://arxiv.org/abs/2501.13629v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.13629v1",
                "updated": "2025-01-23T12:58:14Z",
                "updated_parsed": [
                    2025,
                    1,
                    23,
                    12,
                    58,
                    14,
                    3,
                    23,
                    0
                ],
                "published": "2025-01-23T12:58:14Z",
                "published_parsed": [
                    2025,
                    1,
                    23,
                    12,
                    58,
                    14,
                    3,
                    23,
                    0
                ],
                "title": "Sigma: Differential Rescaling of Query, Key and Value for Efficient\n  Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Sigma: Differential Rescaling of Query, Key and Value for Efficient\n  Language Models"
                },
                "summary": "We introduce Sigma, an efficient large language model specialized for the\nsystem domain, empowered by a novel architecture including DiffQKV attention,\nand pre-trained on our meticulously collected system domain data. DiffQKV\nattention significantly enhances the inference efficiency of Sigma by\noptimizing the Query (Q), Key (K), and Value (V) components in the attention\nmechanism differentially, based on their varying impacts on the model\nperformance and efficiency indicators. Specifically, we (1) conduct extensive\nexperiments that demonstrate the model's varying sensitivity to the compression\nof K and V components, leading to the development of differentially compressed\nKV, and (2) propose augmented Q to expand the Q head dimension, which enhances\nthe model's representation capacity with minimal impacts on the inference\nspeed. Rigorous theoretical and empirical analyses reveal that DiffQKV\nattention significantly enhances efficiency, achieving up to a 33.36%\nimprovement in inference speed over the conventional grouped-query attention\n(GQA) in long-context scenarios. We pre-train Sigma on 6T tokens from various\nsources, including 19.5B system domain data that we carefully collect and 1T\ntokens of synthesized and rewritten data. In general domains, Sigma achieves\ncomparable performance to other state-of-arts models. In the system domain, we\nintroduce the first comprehensive benchmark AIMicius, where Sigma demonstrates\nremarkable performance across all tasks, significantly outperforming GPT-4 with\nan absolute improvement up to 52.5%.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce Sigma, an efficient large language model specialized for the\nsystem domain, empowered by a novel architecture including DiffQKV attention,\nand pre-trained on our meticulously collected system domain data. DiffQKV\nattention significantly enhances the inference efficiency of Sigma by\noptimizing the Query (Q), Key (K), and Value (V) components in the attention\nmechanism differentially, based on their varying impacts on the model\nperformance and efficiency indicators. Specifically, we (1) conduct extensive\nexperiments that demonstrate the model's varying sensitivity to the compression\nof K and V components, leading to the development of differentially compressed\nKV, and (2) propose augmented Q to expand the Q head dimension, which enhances\nthe model's representation capacity with minimal impacts on the inference\nspeed. Rigorous theoretical and empirical analyses reveal that DiffQKV\nattention significantly enhances efficiency, achieving up to a 33.36%\nimprovement in inference speed over the conventional grouped-query attention\n(GQA) in long-context scenarios. We pre-train Sigma on 6T tokens from various\nsources, including 19.5B system domain data that we carefully collect and 1T\ntokens of synthesized and rewritten data. In general domains, Sigma achieves\ncomparable performance to other state-of-arts models. In the system domain, we\nintroduce the first comprehensive benchmark AIMicius, where Sigma demonstrates\nremarkable performance across all tasks, significantly outperforming GPT-4 with\nan absolute improvement up to 52.5%."
                },
                "authors": [
                    {
                        "name": "Zhenghao Lin"
                    },
                    {
                        "name": "Zihao Tang"
                    },
                    {
                        "name": "Xiao Liu"
                    },
                    {
                        "name": "Yeyun Gong"
                    },
                    {
                        "name": "Yi Cheng"
                    },
                    {
                        "name": "Qi Chen"
                    },
                    {
                        "name": "Hang Li"
                    },
                    {
                        "name": "Ying Xin"
                    },
                    {
                        "name": "Ziyue Yang"
                    },
                    {
                        "name": "Kailai Yang"
                    },
                    {
                        "name": "Yu Yan"
                    },
                    {
                        "name": "Xiao Liang"
                    },
                    {
                        "name": "Shuai Lu"
                    },
                    {
                        "name": "Yiming Huang"
                    },
                    {
                        "name": "Zheheng Luo"
                    },
                    {
                        "name": "Lei Qu"
                    },
                    {
                        "name": "Xuan Feng"
                    },
                    {
                        "name": "Yaoxiang Wang"
                    },
                    {
                        "name": "Yuqing Xia"
                    },
                    {
                        "name": "Feiyang Chen"
                    },
                    {
                        "name": "Yuting Jiang"
                    },
                    {
                        "name": "Yasen Hu"
                    },
                    {
                        "name": "Hao Ni"
                    },
                    {
                        "name": "Binyang Li"
                    },
                    {
                        "name": "Guoshuai Zhao"
                    },
                    {
                        "name": "Jui-Hao Chiang"
                    },
                    {
                        "name": "Zhongxin Guo"
                    },
                    {
                        "name": "Chen Lin"
                    },
                    {
                        "name": "Kun Kuang"
                    },
                    {
                        "name": "Wenjie Li"
                    },
                    {
                        "name": "Yelong Shen"
                    },
                    {
                        "name": "Jian Jiao"
                    },
                    {
                        "name": "Peng Cheng"
                    },
                    {
                        "name": "Mao Yang"
                    }
                ],
                "author_detail": {
                    "name": "Mao Yang"
                },
                "author": "Mao Yang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.13629v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.13629v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.13540v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.13540v1",
                "updated": "2025-01-23T10:40:09Z",
                "updated_parsed": [
                    2025,
                    1,
                    23,
                    10,
                    40,
                    9,
                    3,
                    23,
                    0
                ],
                "published": "2025-01-23T10:40:09Z",
                "published_parsed": [
                    2025,
                    1,
                    23,
                    10,
                    40,
                    9,
                    3,
                    23,
                    0
                ],
                "title": "POPS: From History to Mitigation of DNS Cache Poisoning Attacks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "POPS: From History to Mitigation of DNS Cache Poisoning Attacks"
                },
                "summary": "We present a novel yet simple and comprehensive DNS cache POisoning\nPrevention System (POPS), designed to integrate as a module in Intrusion\nPrevention Systems (IPS). POPS addresses statistical DNS poisoning attacks,\nincluding those documented from 2002 to the present, and offers robust\nprotection against similar future threats. It consists of two main components:\na detection module that employs three simple rules, and a mitigation module\nthat leverages the TC flag in the DNS header to enhance security. Once\nactivated, the mitigation module has zero false positives or negatives,\ncorrecting any such errors on the side of the detection module.\n  We first analyze POPS against historical DNS services and attacks, showing\nthat it would have mitigated all network-based statistical poisoning attacks,\nyielding a success rate of only 0.0076% for the adversary. We then simulate\nPOPS on traffic benchmarks (PCAPs) incorporating current potential\nnetwork-based statistical poisoning attacks, and benign PCAPs; the simulated\nattacks still succeed with a probability of 0.0076%. This occurs because five\nmalicious packets go through before POPS detects the attack and activates the\nmitigation module. In addition, POPS completes its task using only 20%-50% of\nthe time required by other tools (e.g., Suricata or Snort), and after examining\njust 5%-10% as many packets. Furthermore, it successfully identifies DNS cache\npoisoning attacks-such as fragmentation attacks-that both Suricata and Snort\nfail to detect, underscoring its superiority in providing comprehensive DNS\nprotection.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present a novel yet simple and comprehensive DNS cache POisoning\nPrevention System (POPS), designed to integrate as a module in Intrusion\nPrevention Systems (IPS). POPS addresses statistical DNS poisoning attacks,\nincluding those documented from 2002 to the present, and offers robust\nprotection against similar future threats. It consists of two main components:\na detection module that employs three simple rules, and a mitigation module\nthat leverages the TC flag in the DNS header to enhance security. Once\nactivated, the mitigation module has zero false positives or negatives,\ncorrecting any such errors on the side of the detection module.\n  We first analyze POPS against historical DNS services and attacks, showing\nthat it would have mitigated all network-based statistical poisoning attacks,\nyielding a success rate of only 0.0076% for the adversary. We then simulate\nPOPS on traffic benchmarks (PCAPs) incorporating current potential\nnetwork-based statistical poisoning attacks, and benign PCAPs; the simulated\nattacks still succeed with a probability of 0.0076%. This occurs because five\nmalicious packets go through before POPS detects the attack and activates the\nmitigation module. In addition, POPS completes its task using only 20%-50% of\nthe time required by other tools (e.g., Suricata or Snort), and after examining\njust 5%-10% as many packets. Furthermore, it successfully identifies DNS cache\npoisoning attacks-such as fragmentation attacks-that both Suricata and Snort\nfail to detect, underscoring its superiority in providing comprehensive DNS\nprotection."
                },
                "authors": [
                    {
                        "name": "Yehuda Afek"
                    },
                    {
                        "name": "Harel Berger"
                    },
                    {
                        "name": "Anat Bremler-Barr"
                    }
                ],
                "author_detail": {
                    "name": "Anat Bremler-Barr"
                },
                "author": "Anat Bremler-Barr",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.13540v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.13540v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.09827v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.09827v3",
                "updated": "2025-01-23T07:25:28Z",
                "updated_parsed": [
                    2025,
                    1,
                    23,
                    7,
                    25,
                    28,
                    3,
                    23,
                    0
                ],
                "published": "2024-06-14T08:32:45Z",
                "published_parsed": [
                    2024,
                    6,
                    14,
                    8,
                    32,
                    45,
                    4,
                    166,
                    0
                ],
                "title": "A Training-free Sub-quadratic Cost Transformer Model Serving Framework\n  With Hierarchically Pruned Attention",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Training-free Sub-quadratic Cost Transformer Model Serving Framework\n  With Hierarchically Pruned Attention"
                },
                "summary": "In modern large language models (LLMs), increasing the context length is\ncrucial for improving comprehension and coherence in long-context, multi-modal,\nand retrieval-augmented language generation. While many recent transformer\nmodels attempt to extend their context length over a million tokens, they\nremain impractical due to the quadratic time and space complexities. Although\nrecent works on linear and sparse attention mechanisms can achieve this goal,\ntheir real-world applicability is often limited by the need to re-train from\nscratch and significantly worse performance. In response, we propose a novel\napproach, Hierarchically Pruned Attention (HiP), which reduces the time\ncomplexity of the attention mechanism to $O(T \\log T)$ and the space complexity\nto $O(T)$, where $T$ is the sequence length. We notice a pattern in the\nattention scores of pretrained LLMs where tokens close together tend to have\nsimilar scores, which we call ``attention locality''. Based on this\nobservation, we utilize a novel tree-search-like algorithm that estimates the\ntop-$k$ key tokens for a given query on the fly, which is mathematically\nguaranteed to have better performance than random attention pruning. In\naddition to improving the time complexity of the attention mechanism, we\nfurther optimize GPU memory usage by implementing KV cache offloading, which\nstores only $O(\\log T)$ tokens on the GPU while maintaining similar decoding\nthroughput. Experiments on benchmarks show that HiP, with its training-free\nnature, significantly reduces both prefill and decoding latencies, as well as\nmemory usage, while maintaining high-quality generation with minimal\ndegradation. HiP enables pretrained LLMs to scale up to millions of tokens on\ncommodity GPUs, potentially unlocking long-context LLM applications previously\ndeemed infeasible.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In modern large language models (LLMs), increasing the context length is\ncrucial for improving comprehension and coherence in long-context, multi-modal,\nand retrieval-augmented language generation. While many recent transformer\nmodels attempt to extend their context length over a million tokens, they\nremain impractical due to the quadratic time and space complexities. Although\nrecent works on linear and sparse attention mechanisms can achieve this goal,\ntheir real-world applicability is often limited by the need to re-train from\nscratch and significantly worse performance. In response, we propose a novel\napproach, Hierarchically Pruned Attention (HiP), which reduces the time\ncomplexity of the attention mechanism to $O(T \\log T)$ and the space complexity\nto $O(T)$, where $T$ is the sequence length. We notice a pattern in the\nattention scores of pretrained LLMs where tokens close together tend to have\nsimilar scores, which we call ``attention locality''. Based on this\nobservation, we utilize a novel tree-search-like algorithm that estimates the\ntop-$k$ key tokens for a given query on the fly, which is mathematically\nguaranteed to have better performance than random attention pruning. In\naddition to improving the time complexity of the attention mechanism, we\nfurther optimize GPU memory usage by implementing KV cache offloading, which\nstores only $O(\\log T)$ tokens on the GPU while maintaining similar decoding\nthroughput. Experiments on benchmarks show that HiP, with its training-free\nnature, significantly reduces both prefill and decoding latencies, as well as\nmemory usage, while maintaining high-quality generation with minimal\ndegradation. HiP enables pretrained LLMs to scale up to millions of tokens on\ncommodity GPUs, potentially unlocking long-context LLM applications previously\ndeemed infeasible."
                },
                "authors": [
                    {
                        "name": "Heejun Lee"
                    },
                    {
                        "name": "Geon Park"
                    },
                    {
                        "name": "Youngwan Lee"
                    },
                    {
                        "name": "Jaduk Suh"
                    },
                    {
                        "name": "Jina Kim"
                    },
                    {
                        "name": "Wonyoung Jeong"
                    },
                    {
                        "name": "Bumsik Kim"
                    },
                    {
                        "name": "Hyemin Lee"
                    },
                    {
                        "name": "Myeongjae Jeon"
                    },
                    {
                        "name": "Sung Ju Hwang"
                    }
                ],
                "author_detail": {
                    "name": "Sung Ju Hwang"
                },
                "author": "Sung Ju Hwang",
                "arxiv_comment": "44 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.09827v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.09827v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.07523v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.07523v2",
                "updated": "2025-01-23T06:48:22Z",
                "updated_parsed": [
                    2025,
                    1,
                    23,
                    6,
                    48,
                    22,
                    3,
                    23,
                    0
                ],
                "published": "2025-01-13T17:50:30Z",
                "published_parsed": [
                    2025,
                    1,
                    13,
                    17,
                    50,
                    30,
                    0,
                    13,
                    0
                ],
                "title": "Parallel Key-Value Cache Fusion for Position Invariant RAG",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Parallel Key-Value Cache Fusion for Position Invariant RAG"
                },
                "summary": "Recent advancements in Large Language Models (LLMs) underscore the necessity\nof Retrieval Augmented Generation (RAG) to leverage external information.\nHowever, LLMs are sensitive to the position of relevant information within\ncontexts and tend to generate incorrect responses when such information is\nplaced in the middle, known as `Lost in the Middle' phenomenon. In this paper,\nwe introduce a framework that generates consistent outputs for decoder-only\nmodels, irrespective of the input context order. Experimental results for three\nopen domain question answering tasks demonstrate position invariance, where the\nmodel is not sensitive to input context order, and superior robustness to\nirrelevent passages compared to prevailing approaches for RAG pipelines.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advancements in Large Language Models (LLMs) underscore the necessity\nof Retrieval Augmented Generation (RAG) to leverage external information.\nHowever, LLMs are sensitive to the position of relevant information within\ncontexts and tend to generate incorrect responses when such information is\nplaced in the middle, known as `Lost in the Middle' phenomenon. In this paper,\nwe introduce a framework that generates consistent outputs for decoder-only\nmodels, irrespective of the input context order. Experimental results for three\nopen domain question answering tasks demonstrate position invariance, where the\nmodel is not sensitive to input context order, and superior robustness to\nirrelevent passages compared to prevailing approaches for RAG pipelines."
                },
                "authors": [
                    {
                        "name": "Philhoon Oh"
                    },
                    {
                        "name": "Jinwoo Shin"
                    },
                    {
                        "name": "James Thorne"
                    }
                ],
                "author_detail": {
                    "name": "James Thorne"
                },
                "author": "James Thorne",
                "arxiv_comment": "5 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.07523v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.07523v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.13331v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.13331v1",
                "updated": "2025-01-23T02:20:08Z",
                "updated_parsed": [
                    2025,
                    1,
                    23,
                    2,
                    20,
                    8,
                    3,
                    23,
                    0
                ],
                "published": "2025-01-23T02:20:08Z",
                "published_parsed": [
                    2025,
                    1,
                    23,
                    2,
                    20,
                    8,
                    3,
                    23,
                    0
                ],
                "title": "Qrazor: Reliable and effortless 4-bit llm quantization by significant\n  data razoring",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Qrazor: Reliable and effortless 4-bit llm quantization by significant\n  data razoring"
                },
                "summary": "Large-scale language models (LLMs) have demonstrated outstanding performance\nin language processing tasks, yet their deployment is often hindered by high\nmemory demands and computational complexity. Although low-bit quantization\ntechniques, such as 4-bit quantization, present a potential solution, they\nfrequently lead to significant accuracy degradation or require substantial\neffort for such aggressive quantization approaches. To overcome these\nchallenges, we introduce QRazor, a reliable and effortless quantization scheme\ndesigned to enable 4-bit quantization for weights, activations, and KV cache in\ntransformer-based LLMs. The scheme involves two main stages: quantization and\ncompression. During the quantization stage, weights, activations, and KV cache\nvalues are quantized with wider 8 or 16-bit integers as a basis to achieve\nnearly identical accuracy to the original full-precision LLM models, using the\nabsolute max scaling. Subsequently, all data are compressed to 4-bit using our\nproposed significant data razoring (SDR) technique, which retains only the four\nmost salient bits while discarding the others. Furthermore, we present an\ninteger-based arithmetic unit dedicated to QRazor, enabling direct\nlow-precision arithmetic operations without decompressing the SDR data. Despite\nthe reduced quantization effort, QRazor achieves LLM accuracies better or\ncomparable to state-of-the-art 4-bit methods. By also validating the hardware\nefficiency, our decompression-free arithmetic unit achieves 61.2% and 57.8%\nreduction in area and power consumption, respectively.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large-scale language models (LLMs) have demonstrated outstanding performance\nin language processing tasks, yet their deployment is often hindered by high\nmemory demands and computational complexity. Although low-bit quantization\ntechniques, such as 4-bit quantization, present a potential solution, they\nfrequently lead to significant accuracy degradation or require substantial\neffort for such aggressive quantization approaches. To overcome these\nchallenges, we introduce QRazor, a reliable and effortless quantization scheme\ndesigned to enable 4-bit quantization for weights, activations, and KV cache in\ntransformer-based LLMs. The scheme involves two main stages: quantization and\ncompression. During the quantization stage, weights, activations, and KV cache\nvalues are quantized with wider 8 or 16-bit integers as a basis to achieve\nnearly identical accuracy to the original full-precision LLM models, using the\nabsolute max scaling. Subsequently, all data are compressed to 4-bit using our\nproposed significant data razoring (SDR) technique, which retains only the four\nmost salient bits while discarding the others. Furthermore, we present an\ninteger-based arithmetic unit dedicated to QRazor, enabling direct\nlow-precision arithmetic operations without decompressing the SDR data. Despite\nthe reduced quantization effort, QRazor achieves LLM accuracies better or\ncomparable to state-of-the-art 4-bit methods. By also validating the hardware\nefficiency, our decompression-free arithmetic unit achieves 61.2% and 57.8%\nreduction in area and power consumption, respectively."
                },
                "authors": [
                    {
                        "name": "Dongyoung Lee"
                    },
                    {
                        "name": "Seungkyu Choi"
                    },
                    {
                        "name": "Ik Joon Chang"
                    }
                ],
                "author_detail": {
                    "name": "Ik Joon Chang"
                },
                "author": "Ik Joon Chang",
                "arxiv_comment": "19 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.13331v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.13331v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.13298v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.13298v1",
                "updated": "2025-01-23T00:57:01Z",
                "updated_parsed": [
                    2025,
                    1,
                    23,
                    0,
                    57,
                    1,
                    3,
                    23,
                    0
                ],
                "published": "2025-01-23T00:57:01Z",
                "published_parsed": [
                    2025,
                    1,
                    23,
                    0,
                    57,
                    1,
                    3,
                    23,
                    0
                ],
                "title": "Collaborative Coded Caching for Partially Connected Networks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Collaborative Coded Caching for Partially Connected Networks"
                },
                "summary": "Coded caching leverages the differences in user cache memories to achieve\ngains that scale with the total cache size, alleviating network congestion due\nto high-quality content requests. Additionally, distributing transmitters over\na wide area can mitigate the adverse effects of path loss. In this work, we\nconsider a partially connected network where the channel between distributed\ntransmitters (helpers) and users is modeled as a distributed MIMO Gaussian\nbroadcast channel. We propose a novel delivery scheme consisting of two phases:\n\\emph{partitioning} and \\emph{transmission}. In the partitioning phase, users\nwith identical cache profiles are partitioned into the minimum number of sets,\nsuch that users within each set can successfully decode their desired message\nfrom a joint transmission enabled by MIMO precoding. To optimally partition the\nusers, we employ the branch and bound method. In the transmission phase, each\npartition is treated as a single entity, and codewords are multicast to\npartitions with distinct cache profiles. The proposed delivery scheme is\napplicable to any partially connected network, and while the partitioning is\noptimal, the overall delivery scheme, including transmission, is heuristic.\nInterestingly, simulation results show that its performance closely\napproximates that of the fully connected optimal solution.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Coded caching leverages the differences in user cache memories to achieve\ngains that scale with the total cache size, alleviating network congestion due\nto high-quality content requests. Additionally, distributing transmitters over\na wide area can mitigate the adverse effects of path loss. In this work, we\nconsider a partially connected network where the channel between distributed\ntransmitters (helpers) and users is modeled as a distributed MIMO Gaussian\nbroadcast channel. We propose a novel delivery scheme consisting of two phases:\n\\emph{partitioning} and \\emph{transmission}. In the partitioning phase, users\nwith identical cache profiles are partitioned into the minimum number of sets,\nsuch that users within each set can successfully decode their desired message\nfrom a joint transmission enabled by MIMO precoding. To optimally partition the\nusers, we employ the branch and bound method. In the transmission phase, each\npartition is treated as a single entity, and codewords are multicast to\npartitions with distinct cache profiles. The proposed delivery scheme is\napplicable to any partially connected network, and while the partitioning is\noptimal, the overall delivery scheme, including transmission, is heuristic.\nInterestingly, simulation results show that its performance closely\napproximates that of the fully connected optimal solution."
                },
                "authors": [
                    {
                        "name": "Kagan Akcay"
                    },
                    {
                        "name": "Eleftherios Lampiris"
                    },
                    {
                        "name": "MohammadJavad Salehi"
                    },
                    {
                        "name": "Giuseppe Caire"
                    }
                ],
                "author_detail": {
                    "name": "Giuseppe Caire"
                },
                "author": "Giuseppe Caire",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.13298v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.13298v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IT",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.11745v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.11745v2",
                "updated": "2025-01-22T16:25:47Z",
                "updated_parsed": [
                    2025,
                    1,
                    22,
                    16,
                    25,
                    47,
                    2,
                    22,
                    0
                ],
                "published": "2025-01-20T21:07:44Z",
                "published_parsed": [
                    2025,
                    1,
                    20,
                    21,
                    7,
                    44,
                    0,
                    20,
                    0
                ],
                "title": "Personalized Federated Learning for Cellular VR: Online Learning and\n  Dynamic Caching",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Personalized Federated Learning for Cellular VR: Online Learning and\n  Dynamic Caching"
                },
                "summary": "Delivering an immersive experience to virtual reality (VR) users through\nwireless connectivity offers the freedom to engage from anywhere at any time.\nNevertheless, it is challenging to ensure seamless wireless connectivity that\ndelivers real-time and high-quality videos to the VR users. This paper proposes\na field of view (FoV) aware caching for mobile edge computing (MEC)-enabled\nwireless VR network. In particular, the FoV of each VR user is\ncached/prefetched at the base stations (BSs) based on the caching strategies\ntailored to each BS. Specifically, decentralized and personalized federated\nlearning (DP-FL) based caching strategies with guarantees are presented.\nConsidering VR systems composed of multiple VR devices and BSs, a DP-FL caching\nalgorithm is implemented at each BS to personalize content delivery for VR\nusers. The utilized DP-FL algorithm guarantees a probably approximately correct\n(PAC) bound on the conditional average cache hit. Further, to reduce the cost\nof communicating gradients, one-bit quantization of the stochastic gradient\ndescent (OBSGD) is proposed, and a convergence guarantee of\n$\\mathcal{O}(1/\\sqrt{T})$ is obtained for the proposed algorithm, where $T$ is\nthe number of iterations. Additionally, to better account for the wireless\nchannel dynamics, the FoVs are grouped into multicast or unicast groups based\non the number of requesting VR users. The performance of the proposed DP-FL\nalgorithm is validated through realistic VR head-tracking dataset, and the\nproposed algorithm is shown to have better performance in terms of average\ndelay and cache hit as compared to baseline algorithms.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Delivering an immersive experience to virtual reality (VR) users through\nwireless connectivity offers the freedom to engage from anywhere at any time.\nNevertheless, it is challenging to ensure seamless wireless connectivity that\ndelivers real-time and high-quality videos to the VR users. This paper proposes\na field of view (FoV) aware caching for mobile edge computing (MEC)-enabled\nwireless VR network. In particular, the FoV of each VR user is\ncached/prefetched at the base stations (BSs) based on the caching strategies\ntailored to each BS. Specifically, decentralized and personalized federated\nlearning (DP-FL) based caching strategies with guarantees are presented.\nConsidering VR systems composed of multiple VR devices and BSs, a DP-FL caching\nalgorithm is implemented at each BS to personalize content delivery for VR\nusers. The utilized DP-FL algorithm guarantees a probably approximately correct\n(PAC) bound on the conditional average cache hit. Further, to reduce the cost\nof communicating gradients, one-bit quantization of the stochastic gradient\ndescent (OBSGD) is proposed, and a convergence guarantee of\n$\\mathcal{O}(1/\\sqrt{T})$ is obtained for the proposed algorithm, where $T$ is\nthe number of iterations. Additionally, to better account for the wireless\nchannel dynamics, the FoVs are grouped into multicast or unicast groups based\non the number of requesting VR users. The performance of the proposed DP-FL\nalgorithm is validated through realistic VR head-tracking dataset, and the\nproposed algorithm is shown to have better performance in terms of average\ndelay and cache hit as compared to baseline algorithms."
                },
                "authors": [
                    {
                        "name": "Krishnendu S. Tharakan"
                    },
                    {
                        "name": "Hayssam Dahrouj"
                    },
                    {
                        "name": "Nour Kouzayha"
                    },
                    {
                        "name": "Hesham ElSawy"
                    },
                    {
                        "name": "Tareq Y. Al-Naffouri"
                    }
                ],
                "author_detail": {
                    "name": "Tareq Y. Al-Naffouri"
                },
                "author": "Tareq Y. Al-Naffouri",
                "arxiv_comment": "accepted for publication in IEEE Transactions on Communications",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.11745v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.11745v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IT",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.12959v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.12959v1",
                "updated": "2025-01-22T15:33:17Z",
                "updated_parsed": [
                    2025,
                    1,
                    22,
                    15,
                    33,
                    17,
                    2,
                    22,
                    0
                ],
                "published": "2025-01-22T15:33:17Z",
                "published_parsed": [
                    2025,
                    1,
                    22,
                    15,
                    33,
                    17,
                    2,
                    22,
                    0
                ],
                "title": "Efficient Prompt Compression with Evaluator Heads for Long-Context\n  Transformer Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient Prompt Compression with Evaluator Heads for Long-Context\n  Transformer Inference"
                },
                "summary": "Although applications involving long-context inputs are crucial for the\neffective utilization of large language models (LLMs), they also result in\nincreased computational costs and reduced performance. To address this\nchallenge, we propose an efficient, training-free prompt compression method\nthat retains key information within compressed prompts. We identify specific\nattention heads in transformer-based LLMs, which we designate as evaluator\nheads, that are capable of selecting tokens in long inputs that are most\nsignificant for inference. Building on this discovery, we develop EHPC, an\nEvaluator Head-based Prompt Compression method, which enables LLMs to rapidly\n\"skim through\" input prompts by leveraging only the first few layers with\nevaluator heads during the pre-filling stage, subsequently passing only the\nimportant tokens to the model for inference. EHPC achieves state-of-the-art\nresults across two mainstream benchmarks: prompt compression and long-context\ninference acceleration. Consequently, it effectively reduces the complexity and\ncosts associated with commercial API calls. We further demonstrate that EHPC\nattains competitive results compared to key-value cache-based acceleration\nmethods, thereby highlighting its potential to enhance the efficiency of LLMs\nfor long-context tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Although applications involving long-context inputs are crucial for the\neffective utilization of large language models (LLMs), they also result in\nincreased computational costs and reduced performance. To address this\nchallenge, we propose an efficient, training-free prompt compression method\nthat retains key information within compressed prompts. We identify specific\nattention heads in transformer-based LLMs, which we designate as evaluator\nheads, that are capable of selecting tokens in long inputs that are most\nsignificant for inference. Building on this discovery, we develop EHPC, an\nEvaluator Head-based Prompt Compression method, which enables LLMs to rapidly\n\"skim through\" input prompts by leveraging only the first few layers with\nevaluator heads during the pre-filling stage, subsequently passing only the\nimportant tokens to the model for inference. EHPC achieves state-of-the-art\nresults across two mainstream benchmarks: prompt compression and long-context\ninference acceleration. Consequently, it effectively reduces the complexity and\ncosts associated with commercial API calls. We further demonstrate that EHPC\nattains competitive results compared to key-value cache-based acceleration\nmethods, thereby highlighting its potential to enhance the efficiency of LLMs\nfor long-context tasks."
                },
                "authors": [
                    {
                        "name": "Weizhi Fei"
                    },
                    {
                        "name": "Xueyan Niu"
                    },
                    {
                        "name": "Guoqing Xie"
                    },
                    {
                        "name": "Yingqing Liu"
                    },
                    {
                        "name": "Bo Bai"
                    },
                    {
                        "name": "Wei Han"
                    }
                ],
                "author_detail": {
                    "name": "Wei Han"
                },
                "author": "Wei Han",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.12959v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.12959v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.01253v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.01253v5",
                "updated": "2025-01-22T15:09:58Z",
                "updated_parsed": [
                    2025,
                    1,
                    22,
                    15,
                    9,
                    58,
                    2,
                    22,
                    0
                ],
                "published": "2024-12-02T08:22:56Z",
                "published_parsed": [
                    2024,
                    12,
                    2,
                    8,
                    22,
                    56,
                    0,
                    337,
                    0
                ],
                "title": "Yi-Lightning Technical Report",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Yi-Lightning Technical Report"
                },
                "summary": "This technical report presents Yi-Lightning, our latest flagship large\nlanguage model (LLM). It achieves exceptional performance, ranking 6th overall\non Chatbot Arena, with particularly strong results (2nd to 4th place) in\nspecialized categories including Chinese, Math, Coding, and Hard Prompts.\nYi-Lightning leverages an enhanced Mixture-of-Experts (MoE) architecture,\nfeaturing advanced expert segmentation and routing mechanisms coupled with\noptimized KV-caching techniques. Our development process encompasses\ncomprehensive pre-training, supervised fine-tuning (SFT), and reinforcement\nlearning from human feedback (RLHF), where we devise deliberate strategies for\nmulti-stage training, synthetic data construction, and reward modeling.\nFurthermore, we implement RAISE (Responsible AI Safety Engine), a\nfour-component framework to address safety issues across pre-training,\npost-training, and serving phases. Empowered by our scalable super-computing\ninfrastructure, all these innovations substantially reduce training, deployment\nand inference costs while maintaining high-performance standards. With further\nevaluations on public academic benchmarks, Yi-Lightning demonstrates\ncompetitive performance against top-tier LLMs, while we observe a notable\ndisparity between traditional, static benchmark results and real-world, dynamic\nhuman preferences. This observation prompts a critical reassessment of\nconventional benchmarks' utility in guiding the development of more intelligent\nand powerful AI systems for practical applications. Yi-Lightning is now\navailable through our developer platform at https://platform.lingyiwanwu.com.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This technical report presents Yi-Lightning, our latest flagship large\nlanguage model (LLM). It achieves exceptional performance, ranking 6th overall\non Chatbot Arena, with particularly strong results (2nd to 4th place) in\nspecialized categories including Chinese, Math, Coding, and Hard Prompts.\nYi-Lightning leverages an enhanced Mixture-of-Experts (MoE) architecture,\nfeaturing advanced expert segmentation and routing mechanisms coupled with\noptimized KV-caching techniques. Our development process encompasses\ncomprehensive pre-training, supervised fine-tuning (SFT), and reinforcement\nlearning from human feedback (RLHF), where we devise deliberate strategies for\nmulti-stage training, synthetic data construction, and reward modeling.\nFurthermore, we implement RAISE (Responsible AI Safety Engine), a\nfour-component framework to address safety issues across pre-training,\npost-training, and serving phases. Empowered by our scalable super-computing\ninfrastructure, all these innovations substantially reduce training, deployment\nand inference costs while maintaining high-performance standards. With further\nevaluations on public academic benchmarks, Yi-Lightning demonstrates\ncompetitive performance against top-tier LLMs, while we observe a notable\ndisparity between traditional, static benchmark results and real-world, dynamic\nhuman preferences. This observation prompts a critical reassessment of\nconventional benchmarks' utility in guiding the development of more intelligent\nand powerful AI systems for practical applications. Yi-Lightning is now\navailable through our developer platform at https://platform.lingyiwanwu.com."
                },
                "authors": [
                    {
                        "name": "Alan Wake"
                    },
                    {
                        "name": "Bei Chen"
                    },
                    {
                        "name": "C. X. Lv"
                    },
                    {
                        "name": "Chao Li"
                    },
                    {
                        "name": "Chengen Huang"
                    },
                    {
                        "name": "Chenglin Cai"
                    },
                    {
                        "name": "Chujie Zheng"
                    },
                    {
                        "name": "Daniel Cooper"
                    },
                    {
                        "name": "Fan Zhou"
                    },
                    {
                        "name": "Feng Hu"
                    },
                    {
                        "name": "Ge Zhang"
                    },
                    {
                        "name": "Guoyin Wang"
                    },
                    {
                        "name": "Heng Ji"
                    },
                    {
                        "name": "Howard Qiu"
                    },
                    {
                        "name": "Jiangcheng Zhu"
                    },
                    {
                        "name": "Jun Tian"
                    },
                    {
                        "name": "Katherine Su"
                    },
                    {
                        "name": "Lihuan Zhang"
                    },
                    {
                        "name": "Liying Li"
                    },
                    {
                        "name": "Ming Song"
                    },
                    {
                        "name": "Mou Li"
                    },
                    {
                        "name": "Peng Liu"
                    },
                    {
                        "name": "Qicheng Hu"
                    },
                    {
                        "name": "Shawn Wang"
                    },
                    {
                        "name": "Shijun Zhou"
                    },
                    {
                        "name": "Shiming Yang"
                    },
                    {
                        "name": "Shiyong Li"
                    },
                    {
                        "name": "Tianhang Zhu"
                    },
                    {
                        "name": "Wen Xie"
                    },
                    {
                        "name": "Wenhao Huang"
                    },
                    {
                        "name": "Xiang He"
                    },
                    {
                        "name": "Xiaobo Chen"
                    },
                    {
                        "name": "Xiaohui Hu"
                    },
                    {
                        "name": "Xiaoyi Ren"
                    },
                    {
                        "name": "Xinyao Niu"
                    },
                    {
                        "name": "Yanpeng Li"
                    },
                    {
                        "name": "Yongke Zhao"
                    },
                    {
                        "name": "Yongzhen Luo"
                    },
                    {
                        "name": "Yuchi Xu"
                    },
                    {
                        "name": "Yuxuan Sha"
                    },
                    {
                        "name": "Zhaodong Yan"
                    },
                    {
                        "name": "Zhiyuan Liu"
                    },
                    {
                        "name": "Zirui Zhang"
                    },
                    {
                        "name": "Zonghong Dai"
                    }
                ],
                "author_detail": {
                    "name": "Zonghong Dai"
                },
                "author": "Zonghong Dai",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.01253v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.01253v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2310.08894v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2310.08894v3",
                "updated": "2025-01-22T15:05:08Z",
                "updated_parsed": [
                    2025,
                    1,
                    22,
                    15,
                    5,
                    8,
                    2,
                    22,
                    0
                ],
                "published": "2023-10-13T06:58:07Z",
                "published_parsed": [
                    2023,
                    10,
                    13,
                    6,
                    58,
                    7,
                    4,
                    286,
                    0
                ],
                "title": "Multi-Antenna Coded Caching for Multi-Access Networks with Cyclic\n  Wrap-Around",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-Antenna Coded Caching for Multi-Access Networks with Cyclic\n  Wrap-Around"
                },
                "summary": "This work explores a multiple transmit antenna setting in a multi-access\ncoded caching (MACC) network where each user accesses more than one cache. A\nMACC network has $K$ users and $K$ caches, and each user has access to $r < K$\nconsecutive caches in a cyclic wrap-around manner. There are $L$ antennas at\nthe server, and each cache has a normalized size of $M/N \\leq 1$. The cyclic\nwrap-around MACC network with a single antenna at the server has been a\nwell-investigated topic, and several coded caching schemes and improved lower\nbounds on the performance are known for the same. However, this MACC network\nhas not yet been studied under multi-antenna settings in the coded caching\nliterature. We study the multi-antenna MACC problem and propose a solution for\nthe same by constructing a pair of arrays called caching and delivery arrays.\nWe present three constructions of caching and delivery arrays for different\nscenarios and obtain corresponding multi-antenna MACC schemes for the same. Two\nschemes resulting from the above constructions achieve optimal performance\nunder uncoded placement and one-shot delivery. The optimality is shown by\nmatching the performance of the multi-antenna MACC scheme to that of an optimal\nmulti-antenna scheme for a dedicated cache network having an identical number\nof users, and each user has a normalized cache size of $rM/N$. Further, as a\nspecial case, one of the proposed schemes subsumes an existing optimal MACC\nscheme for the single-antenna setting.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This work explores a multiple transmit antenna setting in a multi-access\ncoded caching (MACC) network where each user accesses more than one cache. A\nMACC network has $K$ users and $K$ caches, and each user has access to $r < K$\nconsecutive caches in a cyclic wrap-around manner. There are $L$ antennas at\nthe server, and each cache has a normalized size of $M/N \\leq 1$. The cyclic\nwrap-around MACC network with a single antenna at the server has been a\nwell-investigated topic, and several coded caching schemes and improved lower\nbounds on the performance are known for the same. However, this MACC network\nhas not yet been studied under multi-antenna settings in the coded caching\nliterature. We study the multi-antenna MACC problem and propose a solution for\nthe same by constructing a pair of arrays called caching and delivery arrays.\nWe present three constructions of caching and delivery arrays for different\nscenarios and obtain corresponding multi-antenna MACC schemes for the same. Two\nschemes resulting from the above constructions achieve optimal performance\nunder uncoded placement and one-shot delivery. The optimality is shown by\nmatching the performance of the multi-antenna MACC scheme to that of an optimal\nmulti-antenna scheme for a dedicated cache network having an identical number\nof users, and each user has a normalized cache size of $rM/N$. Further, as a\nspecial case, one of the proposed schemes subsumes an existing optimal MACC\nscheme for the single-antenna setting."
                },
                "authors": [
                    {
                        "name": "Elizabath Peter"
                    },
                    {
                        "name": "K. K. Krishnan Namboodiri"
                    },
                    {
                        "name": "B. Sundar Rajan"
                    }
                ],
                "author_detail": {
                    "name": "B. Sundar Rajan"
                },
                "author": "B. Sundar Rajan",
                "arxiv_comment": "To appear in IEEE Transactions on Communications",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2310.08894v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2310.08894v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IT",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.03940v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.03940v2",
                "updated": "2025-01-22T10:39:50Z",
                "updated_parsed": [
                    2025,
                    1,
                    22,
                    10,
                    39,
                    50,
                    2,
                    22,
                    0
                ],
                "published": "2025-01-07T17:00:49Z",
                "published_parsed": [
                    2025,
                    1,
                    7,
                    17,
                    0,
                    49,
                    1,
                    7,
                    0
                ],
                "title": "Not all tokens are created equal: Perplexity Attention Weighted Networks\n  for AI generated text detection",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Not all tokens are created equal: Perplexity Attention Weighted Networks\n  for AI generated text detection"
                },
                "summary": "The rapid advancement in large language models (LLMs) has significantly\nenhanced their ability to generate coherent and contextually relevant text,\nraising concerns about the misuse of AI-generated content and making it\ncritical to detect it. However, the task remains challenging, particularly in\nunseen domains or with unfamiliar LLMs. Leveraging LLM next-token distribution\noutputs offers a theoretically appealing approach for detection, as they\nencapsulate insights from the models' extensive pre-training on diverse\ncorpora. Despite its promise, zero-shot methods that attempt to operationalize\nthese outputs have met with limited success. We hypothesize that one of the\nproblems is that they use the mean to aggregate next-token distribution metrics\nacross tokens, when some tokens are naturally easier or harder to predict and\nshould be weighted differently. Based on this idea, we propose the Perplexity\nAttention Weighted Network (PAWN), which uses the last hidden states of the LLM\nand positions to weight the sum of a series of features based on metrics from\nthe next-token distribution across the sequence length. Although not zero-shot,\nour method allows us to cache the last hidden states and next-token\ndistribution metrics on disk, greatly reducing the training resource\nrequirements. PAWN shows competitive and even better performance\nin-distribution than the strongest baselines (fine-tuned LMs) with a fraction\nof their trainable parameters. Our model also generalizes better to unseen\ndomains and source models, with smaller variability in the decision boundary\nacross distribution shifts. It is also more robust to adversarial attacks, and\nif the backbone has multilingual capabilities, it presents decent\ngeneralization to languages not seen during supervised training, with LLaMA3-1B\nreaching a mean macro-averaged F1 score of 81.46% in cross-validation with nine\nlanguages.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rapid advancement in large language models (LLMs) has significantly\nenhanced their ability to generate coherent and contextually relevant text,\nraising concerns about the misuse of AI-generated content and making it\ncritical to detect it. However, the task remains challenging, particularly in\nunseen domains or with unfamiliar LLMs. Leveraging LLM next-token distribution\noutputs offers a theoretically appealing approach for detection, as they\nencapsulate insights from the models' extensive pre-training on diverse\ncorpora. Despite its promise, zero-shot methods that attempt to operationalize\nthese outputs have met with limited success. We hypothesize that one of the\nproblems is that they use the mean to aggregate next-token distribution metrics\nacross tokens, when some tokens are naturally easier or harder to predict and\nshould be weighted differently. Based on this idea, we propose the Perplexity\nAttention Weighted Network (PAWN), which uses the last hidden states of the LLM\nand positions to weight the sum of a series of features based on metrics from\nthe next-token distribution across the sequence length. Although not zero-shot,\nour method allows us to cache the last hidden states and next-token\ndistribution metrics on disk, greatly reducing the training resource\nrequirements. PAWN shows competitive and even better performance\nin-distribution than the strongest baselines (fine-tuned LMs) with a fraction\nof their trainable parameters. Our model also generalizes better to unseen\ndomains and source models, with smaller variability in the decision boundary\nacross distribution shifts. It is also more robust to adversarial attacks, and\nif the backbone has multilingual capabilities, it presents decent\ngeneralization to languages not seen during supervised training, with LLaMA3-1B\nreaching a mean macro-averaged F1 score of 81.46% in cross-validation with nine\nlanguages."
                },
                "authors": [
                    {
                        "name": "Pablo Miralles-González"
                    },
                    {
                        "name": "Javier Huertas-Tato"
                    },
                    {
                        "name": "Alejandro Martín"
                    },
                    {
                        "name": "David Camacho"
                    }
                ],
                "author_detail": {
                    "name": "David Camacho"
                },
                "author": "David Camacho",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.03940v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.03940v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.12744v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.12744v1",
                "updated": "2025-01-22T09:25:29Z",
                "updated_parsed": [
                    2025,
                    1,
                    22,
                    9,
                    25,
                    29,
                    2,
                    22,
                    0
                ],
                "published": "2025-01-22T09:25:29Z",
                "published_parsed": [
                    2025,
                    1,
                    22,
                    9,
                    25,
                    29,
                    2,
                    22,
                    0
                ],
                "title": "Bright single-photon source in a silicon chip by nanoscale positioning\n  of a color center in a microcavity",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Bright single-photon source in a silicon chip by nanoscale positioning\n  of a color center in a microcavity"
                },
                "summary": "We present an all-silicon source of near-infrared linearly-polarized single\nphotons, fabricated by nanoscale positioning of a color center in a\nsilicon-on-insulator microcavity. The color center consists of a single W\ncenter, created at a well-defined position by Si$^{+}$ ion implantation through\na 150 nm-diameter nanohole in a mask. A circular Bragg grating cavity resonant\nwith the W's zero-phonon line at 1217 nm is fabricated at the same location as\nthe nanohole. Under above-gap continuous-wave excitation, a very clean photon\nantibunching behavior ($g{^2} \\leq 0.06$) is observed over the entire power\nrange, which highlights the absence of parasitic emitters. Purcell-enhancement\nof W's zero-phonon emission provides both a record-high photoluminescence count\nrate among Si color centers (ca $1.2 \\times 10^{6}$ counts/s) and apparent\nDebye-Waller factor around 99%. We also demonstrate the triggered emission of\nsingle photons with 93% purity under weak pulsed laser excitation. At high\npulsed laser power, we reveal a detrimental effect of repumping processes, that\ncould be mitigated using selective pumping schemes in the future. These results\nrepresent a major step towards on-demand sources of indistinguishable\nnear-infrared single photons within silicon photonics chips.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present an all-silicon source of near-infrared linearly-polarized single\nphotons, fabricated by nanoscale positioning of a color center in a\nsilicon-on-insulator microcavity. The color center consists of a single W\ncenter, created at a well-defined position by Si$^{+}$ ion implantation through\na 150 nm-diameter nanohole in a mask. A circular Bragg grating cavity resonant\nwith the W's zero-phonon line at 1217 nm is fabricated at the same location as\nthe nanohole. Under above-gap continuous-wave excitation, a very clean photon\nantibunching behavior ($g{^2} \\leq 0.06$) is observed over the entire power\nrange, which highlights the absence of parasitic emitters. Purcell-enhancement\nof W's zero-phonon emission provides both a record-high photoluminescence count\nrate among Si color centers (ca $1.2 \\times 10^{6}$ counts/s) and apparent\nDebye-Waller factor around 99%. We also demonstrate the triggered emission of\nsingle photons with 93% purity under weak pulsed laser excitation. At high\npulsed laser power, we reveal a detrimental effect of repumping processes, that\ncould be mitigated using selective pumping schemes in the future. These results\nrepresent a major step towards on-demand sources of indistinguishable\nnear-infrared single photons within silicon photonics chips."
                },
                "authors": [
                    {
                        "name": "Baptiste Lefaucher"
                    },
                    {
                        "name": "Yoann Baron"
                    },
                    {
                        "name": "Jean-Baptiste Jager"
                    },
                    {
                        "name": "Vincent Calvo"
                    },
                    {
                        "name": "Christian Elsässer"
                    },
                    {
                        "name": "Giuliano Coppola"
                    },
                    {
                        "name": "Frédéric Mazen"
                    },
                    {
                        "name": "Sébastien Kerdilès"
                    },
                    {
                        "name": "Félix Cache"
                    },
                    {
                        "name": "Anaïs Dréau"
                    },
                    {
                        "name": "Jean-Michel Gérard"
                    }
                ],
                "author_detail": {
                    "name": "Jean-Michel Gérard"
                },
                "arxiv_affiliation": "Univ. Grenoble Alpes, CEA, Grenoble INP, IRIG, PHELIQS",
                "author": "Jean-Michel Gérard",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.12744v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.12744v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.optics",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.optics",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "quant-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.12689v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.12689v1",
                "updated": "2025-01-22T07:52:38Z",
                "updated_parsed": [
                    2025,
                    1,
                    22,
                    7,
                    52,
                    38,
                    2,
                    22,
                    0
                ],
                "published": "2025-01-22T07:52:38Z",
                "published_parsed": [
                    2025,
                    1,
                    22,
                    7,
                    52,
                    38,
                    2,
                    22,
                    0
                ],
                "title": "EchoLM: Accelerating LLM Serving with Real-time Knowledge Distillation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "EchoLM: Accelerating LLM Serving with Real-time Knowledge Distillation"
                },
                "summary": "Large language models (LLMs) have excelled in various applications, yet\nserving them at scale is challenging due to their substantial resource demands\nand high latency. Our real-world studies reveal that over 60% of user requests\nto LLMs have semantically similar counterparts, suggesting the potential for\nknowledge sharing among requests. However, naively caching and reusing past\nresponses leads to large quality degradation. In this paper, we introduce\nEchoLM, an in-context caching system that leverages historical requests as\nexamples to guide response generation, enabling selective offloading of\nrequests to more efficient LLMs. However, enabling this real-time knowledge\ntransfer leads to intricate tradeoffs between response quality, latency, and\nsystem throughput at scale. For a new request, EchoLM identifies similar,\nhigh-utility examples and efficiently prepends them to the input for better\nresponse. At scale, EchoLM adaptively routes requests to LLMs of varying\ncapabilities, accounting for response quality and serving loads. EchoLM employs\na cost-aware cache replay mechanism to improve example quality and coverage\noffline, maximizing cache utility and runtime efficiency. Evaluations on\nmillions of open-source requests demonstrate that EchoLM has a throughput\nimprovement of 1.4-5.9x while reducing latency by 28-71% without hurting\nresponse quality on average.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have excelled in various applications, yet\nserving them at scale is challenging due to their substantial resource demands\nand high latency. Our real-world studies reveal that over 60% of user requests\nto LLMs have semantically similar counterparts, suggesting the potential for\nknowledge sharing among requests. However, naively caching and reusing past\nresponses leads to large quality degradation. In this paper, we introduce\nEchoLM, an in-context caching system that leverages historical requests as\nexamples to guide response generation, enabling selective offloading of\nrequests to more efficient LLMs. However, enabling this real-time knowledge\ntransfer leads to intricate tradeoffs between response quality, latency, and\nsystem throughput at scale. For a new request, EchoLM identifies similar,\nhigh-utility examples and efficiently prepends them to the input for better\nresponse. At scale, EchoLM adaptively routes requests to LLMs of varying\ncapabilities, accounting for response quality and serving loads. EchoLM employs\na cost-aware cache replay mechanism to improve example quality and coverage\noffline, maximizing cache utility and runtime efficiency. Evaluations on\nmillions of open-source requests demonstrate that EchoLM has a throughput\nimprovement of 1.4-5.9x while reducing latency by 28-71% without hurting\nresponse quality on average."
                },
                "authors": [
                    {
                        "name": "Yifan Yu"
                    },
                    {
                        "name": "Yu Gan"
                    },
                    {
                        "name": "Lily Tasi"
                    },
                    {
                        "name": "Nikhil Sarda"
                    },
                    {
                        "name": "Jiaming Shen"
                    },
                    {
                        "name": "Yanqi Zhou"
                    },
                    {
                        "name": "Arvind Krishnamurthy"
                    },
                    {
                        "name": "Fan Lai"
                    },
                    {
                        "name": "Henry M. Levy"
                    },
                    {
                        "name": "David Culler"
                    }
                ],
                "author_detail": {
                    "name": "David Culler"
                },
                "author": "David Culler",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.12689v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.12689v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.12528v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.12528v1",
                "updated": "2025-01-21T22:33:15Z",
                "updated_parsed": [
                    2025,
                    1,
                    21,
                    22,
                    33,
                    15,
                    1,
                    21,
                    0
                ],
                "published": "2025-01-21T22:33:15Z",
                "published_parsed": [
                    2025,
                    1,
                    21,
                    22,
                    33,
                    15,
                    1,
                    21,
                    0
                ],
                "title": "Improved Coded Caching Scheme for Multi-User Information Retrieval\n  System",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Improved Coded Caching Scheme for Multi-User Information Retrieval\n  System"
                },
                "summary": "In this paper, we study the coded caching scheme for the $(L, K, M, N)$\nmulti-user information retrieval (MIR) system, which consists of a content\nlibrary containing $N$ files, a base station (BS) with $L$ antennas that cannot\naccess the library, and $K$ single-antenna users, each of which can cache at\nmost $M$ files from the library. The users communicate with the others assisted\nby the BS to decode their required files. In this paper, we focus on designing\na coded caching scheme with low communication latency measured by normalized\ndelivery time (NDT), computational complexity, and subpacketizations. When\n$\\frac{KM}{N}\\geq L$ we first simply the precoding matrix in the downlink step\nto an identity matrix and use the multiple-antenna placement delivery array\n(MAPDA), which was originally proposed for the multiple-input single-output\nnetworks, to generate several new schemes for MIR system. Compared to the\nexisting schemes, both the theoretical and numerical analyses show that our new\nschemes achieve much lower computational complexity and smaller\nsubpacketizations with the same NDT.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this paper, we study the coded caching scheme for the $(L, K, M, N)$\nmulti-user information retrieval (MIR) system, which consists of a content\nlibrary containing $N$ files, a base station (BS) with $L$ antennas that cannot\naccess the library, and $K$ single-antenna users, each of which can cache at\nmost $M$ files from the library. The users communicate with the others assisted\nby the BS to decode their required files. In this paper, we focus on designing\na coded caching scheme with low communication latency measured by normalized\ndelivery time (NDT), computational complexity, and subpacketizations. When\n$\\frac{KM}{N}\\geq L$ we first simply the precoding matrix in the downlink step\nto an identity matrix and use the multiple-antenna placement delivery array\n(MAPDA), which was originally proposed for the multiple-input single-output\nnetworks, to generate several new schemes for MIR system. Compared to the\nexisting schemes, both the theoretical and numerical analyses show that our new\nschemes achieve much lower computational complexity and smaller\nsubpacketizations with the same NDT."
                },
                "authors": [
                    {
                        "name": "Junyi Wang"
                    },
                    {
                        "name": "Quan Zang"
                    },
                    {
                        "name": "Jinyu Wang"
                    },
                    {
                        "name": "Minquan Cheng"
                    }
                ],
                "author_detail": {
                    "name": "Minquan Cheng"
                },
                "author": "Minquan Cheng",
                "arxiv_comment": "14",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.12528v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.12528v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IT",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.12084v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.12084v1",
                "updated": "2025-01-21T12:19:02Z",
                "updated_parsed": [
                    2025,
                    1,
                    21,
                    12,
                    19,
                    2,
                    1,
                    21,
                    0
                ],
                "published": "2025-01-21T12:19:02Z",
                "published_parsed": [
                    2025,
                    1,
                    21,
                    12,
                    19,
                    2,
                    1,
                    21,
                    0
                ],
                "title": "Dissecting the NVIDIA Hopper Architecture through Microbenchmarking and\n  Multiple Level Analysis",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Dissecting the NVIDIA Hopper Architecture through Microbenchmarking and\n  Multiple Level Analysis"
                },
                "summary": "Modern GPUs, with their specialized hardware like tensor cores, are essential\nfor demanding AI and deep learning applications. This study presents a\ncomprehensive, multi-level microbenchmarking analysis of the NVIDIA Hopper GPU\narchitecture, delving into its performance characteristics and novel features.\nWe benchmark Hopper's memory subsystem latency and throughput, comparing its L2\npartitioned cache behavior and global memory access patterns against recent GPU\ngenerations, Ampere and Ada Lovelace. Our analysis reveals significant\nperformance differences and architectural improvements in Hopper. A core\ncontribution of this work is a detailed evaluation of Hopper's\nfourth-generation tensor cores, including their FP8 precision support and the\nnovel asynchronous wgmma instructions, assessing their impact on matrix\nmultiply-accumulate operations. We further investigate the performance\nimplications of other key Hopper innovations: DPX instructions for accelerating\ndynamic programming algorithms, distributed shared memory (DSM) for inter-SM\ncommunication, and the Tensor Memory Accelerator (TMA) for asynchronous data\nmovement. This multi-level approach encompasses instruction-level\nmicrobenchmarks, library-level analysis of the Transformer Engine, and\napplication-level benchmarks of tensor core performance within large language\nmodels. Our findings provide valuable, in-depth insights for software\ndevelopers seeking to optimize performance and develop accurate performance\nmodels for the Hopper architecture, ultimately contributing to a deeper\nunderstanding of its potential for accelerating AI and other computationally\nintensive workloads.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Modern GPUs, with their specialized hardware like tensor cores, are essential\nfor demanding AI and deep learning applications. This study presents a\ncomprehensive, multi-level microbenchmarking analysis of the NVIDIA Hopper GPU\narchitecture, delving into its performance characteristics and novel features.\nWe benchmark Hopper's memory subsystem latency and throughput, comparing its L2\npartitioned cache behavior and global memory access patterns against recent GPU\ngenerations, Ampere and Ada Lovelace. Our analysis reveals significant\nperformance differences and architectural improvements in Hopper. A core\ncontribution of this work is a detailed evaluation of Hopper's\nfourth-generation tensor cores, including their FP8 precision support and the\nnovel asynchronous wgmma instructions, assessing their impact on matrix\nmultiply-accumulate operations. We further investigate the performance\nimplications of other key Hopper innovations: DPX instructions for accelerating\ndynamic programming algorithms, distributed shared memory (DSM) for inter-SM\ncommunication, and the Tensor Memory Accelerator (TMA) for asynchronous data\nmovement. This multi-level approach encompasses instruction-level\nmicrobenchmarks, library-level analysis of the Transformer Engine, and\napplication-level benchmarks of tensor core performance within large language\nmodels. Our findings provide valuable, in-depth insights for software\ndevelopers seeking to optimize performance and develop accurate performance\nmodels for the Hopper architecture, ultimately contributing to a deeper\nunderstanding of its potential for accelerating AI and other computationally\nintensive workloads."
                },
                "authors": [
                    {
                        "name": "Weile Luo"
                    },
                    {
                        "name": "Ruibo Fan"
                    },
                    {
                        "name": "Zeyu Li"
                    },
                    {
                        "name": "Dayou Du"
                    },
                    {
                        "name": "Hongyuan Liu"
                    },
                    {
                        "name": "Qiang Wang"
                    },
                    {
                        "name": "Xiaowen Chu"
                    }
                ],
                "author_detail": {
                    "name": "Xiaowen Chu"
                },
                "author": "Xiaowen Chu",
                "arxiv_comment": "arXiv admin note: substantial text overlap with arXiv:2402.13499",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.12084v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.12084v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.11940v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.11940v1",
                "updated": "2025-01-21T07:32:06Z",
                "updated_parsed": [
                    2025,
                    1,
                    21,
                    7,
                    32,
                    6,
                    1,
                    21,
                    0
                ],
                "published": "2025-01-21T07:32:06Z",
                "published_parsed": [
                    2025,
                    1,
                    21,
                    7,
                    32,
                    6,
                    1,
                    21,
                    0
                ],
                "title": "Build Optimization: A Systematic Literature Review",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Build Optimization: A Systematic Literature Review"
                },
                "summary": "Continuous Integration (CI) consists of an automated build process involving\ncontinuous compilation, testing, and packaging of the software system. While CI\ncomes up with several advantages related to quality and time to delivery, CI\nalso presents several challenges addressed by a large body of research. To\nbetter understand the literature so as to help practitioners find solutions for\ntheir problems and guide future research, we conduct a systematic review of 97\nstudies on build optimization published between 2006 and 2024, which we\nsummarized according to their goals, methodologies, used datasets, and\nleveraged metrics. The identified build optimization studies focus on two main\nchallenges: (1) long build durations, and (2) build failures. To meet the first\nchallenge, existing studies have developed a range of techniques, including\npredicting build outcome and duration, selective build execution, and build\nacceleration using caching or repairing performance smells. The causes of build\nfailures have been the subject of several studies, leading to the development\nof techniques for predicting build script maintenance and automating repair.\nRecent studies have also focused on predicting flaky build failures caused by\nenvironmental issues. The majority of these techniques use machine learning\nalgorithms and leverage build metrics, which we classify into five categories.\nAdditionally, we identify eight publicly available build datasets for build\noptimization research.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Continuous Integration (CI) consists of an automated build process involving\ncontinuous compilation, testing, and packaging of the software system. While CI\ncomes up with several advantages related to quality and time to delivery, CI\nalso presents several challenges addressed by a large body of research. To\nbetter understand the literature so as to help practitioners find solutions for\ntheir problems and guide future research, we conduct a systematic review of 97\nstudies on build optimization published between 2006 and 2024, which we\nsummarized according to their goals, methodologies, used datasets, and\nleveraged metrics. The identified build optimization studies focus on two main\nchallenges: (1) long build durations, and (2) build failures. To meet the first\nchallenge, existing studies have developed a range of techniques, including\npredicting build outcome and duration, selective build execution, and build\nacceleration using caching or repairing performance smells. The causes of build\nfailures have been the subject of several studies, leading to the development\nof techniques for predicting build script maintenance and automating repair.\nRecent studies have also focused on predicting flaky build failures caused by\nenvironmental issues. The majority of these techniques use machine learning\nalgorithms and leverage build metrics, which we classify into five categories.\nAdditionally, we identify eight publicly available build datasets for build\noptimization research."
                },
                "authors": [
                    {
                        "name": "Henri Aïdasso"
                    },
                    {
                        "name": "Mohammed Sayagh"
                    },
                    {
                        "name": "Francis Bordeleau"
                    }
                ],
                "author_detail": {
                    "name": "Francis Bordeleau"
                },
                "author": "Francis Bordeleau",
                "arxiv_comment": "An earlier version of this work was submitted to ACM CSUR in November\n  2023",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.11940v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.11940v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.11855v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.11855v1",
                "updated": "2025-01-21T03:13:21Z",
                "updated_parsed": [
                    2025,
                    1,
                    21,
                    3,
                    13,
                    21,
                    1,
                    21,
                    0
                ],
                "published": "2025-01-21T03:13:21Z",
                "published_parsed": [
                    2025,
                    1,
                    21,
                    3,
                    13,
                    21,
                    1,
                    21,
                    0
                ],
                "title": "A New Construction Structure on Coded Caching with Linear\n  Subpacketization: Non-Half-Sum Disjoint Packing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A New Construction Structure on Coded Caching with Linear\n  Subpacketization: Non-Half-Sum Disjoint Packing"
                },
                "summary": "Coded caching is a promising technique to effectively reduce peak traffic by\nusing local caches and the multicast gains generated by these local caches. We\nprefer to design a coded caching scheme with the subpacketization $F$ and\ntransmission load $R$ as small as possible since these are the key metrics for\nevaluating the implementation complexity and transmission efficiency of the\nscheme, respectively. However, most of the existing coded caching schemes have\nlarge subpacketizations which grow exponentially with the number of users $K$,\nand there are a few schemes with linear subpacketizations which have large\ntransmission loads. In this paper, we focus on studying the linear\nsubpacketization, i.e., $K=F$, coded caching scheme with low transmission load.\nSpecifically, we first introduce a new combinatorial structure called\nnon-half-sum disjoint packing (NHSDP) which can be used to generate a coded\ncaching scheme with $K=F$. Then a class of new schemes is obtained by\nconstructing NHSDP. Theoretical and numerical comparisons show that (i)\ncompared to the existing schemes with linear subpacketization (to the number of\nusers), the proposed scheme achieves a lower load; (ii) compared to some\nexisting schemes with polynomial subpacketization, the proposed scheme can also\nachieve a lower load in some cases; (iii) compared to some existing schemes\nwith exponential subpacketization, the proposed scheme has loads close to those\nof these schemes in some cases. Moreover, the new concept of NHSDP is closely\nrelated to the classical combinatorial structures such as cyclic difference\npacking (CDP), non-three-term arithmetic progressions (NTAP), and perfect hash\nfamily (PHF). These connections indicate that NHSDP is an important\ncombinatorial structure in the field of combinatorial design.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Coded caching is a promising technique to effectively reduce peak traffic by\nusing local caches and the multicast gains generated by these local caches. We\nprefer to design a coded caching scheme with the subpacketization $F$ and\ntransmission load $R$ as small as possible since these are the key metrics for\nevaluating the implementation complexity and transmission efficiency of the\nscheme, respectively. However, most of the existing coded caching schemes have\nlarge subpacketizations which grow exponentially with the number of users $K$,\nand there are a few schemes with linear subpacketizations which have large\ntransmission loads. In this paper, we focus on studying the linear\nsubpacketization, i.e., $K=F$, coded caching scheme with low transmission load.\nSpecifically, we first introduce a new combinatorial structure called\nnon-half-sum disjoint packing (NHSDP) which can be used to generate a coded\ncaching scheme with $K=F$. Then a class of new schemes is obtained by\nconstructing NHSDP. Theoretical and numerical comparisons show that (i)\ncompared to the existing schemes with linear subpacketization (to the number of\nusers), the proposed scheme achieves a lower load; (ii) compared to some\nexisting schemes with polynomial subpacketization, the proposed scheme can also\nachieve a lower load in some cases; (iii) compared to some existing schemes\nwith exponential subpacketization, the proposed scheme has loads close to those\nof these schemes in some cases. Moreover, the new concept of NHSDP is closely\nrelated to the classical combinatorial structures such as cyclic difference\npacking (CDP), non-three-term arithmetic progressions (NTAP), and perfect hash\nfamily (PHF). These connections indicate that NHSDP is an important\ncombinatorial structure in the field of combinatorial design."
                },
                "authors": [
                    {
                        "name": "Minquan Cheng"
                    },
                    {
                        "name": "Huimei Wei"
                    },
                    {
                        "name": "Kai Wan"
                    },
                    {
                        "name": "Giuseppe Caire"
                    }
                ],
                "author_detail": {
                    "name": "Giuseppe Caire"
                },
                "author": "Giuseppe Caire",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.11855v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.11855v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IT",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.11834v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.11834v1",
                "updated": "2025-01-21T02:35:31Z",
                "updated_parsed": [
                    2025,
                    1,
                    21,
                    2,
                    35,
                    31,
                    1,
                    21,
                    0
                ],
                "published": "2025-01-21T02:35:31Z",
                "published_parsed": [
                    2025,
                    1,
                    21,
                    2,
                    35,
                    31,
                    1,
                    21,
                    0
                ],
                "title": "PDA Construction via Union of Cartesian Product Cache Configurations for\n  Coded Caching",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PDA Construction via Union of Cartesian Product Cache Configurations for\n  Coded Caching"
                },
                "summary": "Caching is an efficient technique to reduce peak traffic by storing popular\ncontent in local caches. Placement delivery array (PDA) proposed by Yan et al.\nis a combinatorial structure to design coded caching schemes with uncoded\nplacement and one-shot linear delivery. By taking the $m$-fold Cartesian\nproduct of a small base PDA, Wang et al. constructed a big PDA while\nmaintaining the memory ratio and transmission load unchanged, which achieves\nlinear growth in both the number of users and coded caching gain. In order to\nachieve exponential growth in both the number of users and coded caching gain,\nin this paper we propose a PDA construction by taking the union operation of\nthe cache configurations from the $m$-fold Cartesian product of a base PDA. The\nresulting PDA leads to a coded caching scheme with subpacketization increasing\nsub-exponentially with the number of users while keeping the load constant for\nfixed memory ratio. By applying the proposed construction to existing base\nPDAs, three new coded caching schemes are obtained, which cover some existing\nschemes as special cases and can achieve lower load with simultaneously lower\nsubpacketization for some memory ratios.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Caching is an efficient technique to reduce peak traffic by storing popular\ncontent in local caches. Placement delivery array (PDA) proposed by Yan et al.\nis a combinatorial structure to design coded caching schemes with uncoded\nplacement and one-shot linear delivery. By taking the $m$-fold Cartesian\nproduct of a small base PDA, Wang et al. constructed a big PDA while\nmaintaining the memory ratio and transmission load unchanged, which achieves\nlinear growth in both the number of users and coded caching gain. In order to\nachieve exponential growth in both the number of users and coded caching gain,\nin this paper we propose a PDA construction by taking the union operation of\nthe cache configurations from the $m$-fold Cartesian product of a base PDA. The\nresulting PDA leads to a coded caching scheme with subpacketization increasing\nsub-exponentially with the number of users while keeping the load constant for\nfixed memory ratio. By applying the proposed construction to existing base\nPDAs, three new coded caching schemes are obtained, which cover some existing\nschemes as special cases and can achieve lower load with simultaneously lower\nsubpacketization for some memory ratios."
                },
                "authors": [
                    {
                        "name": "Jinyu Wang"
                    },
                    {
                        "name": "Minquan Cheng"
                    },
                    {
                        "name": "Kai Wan"
                    },
                    {
                        "name": "Giuseppe Caire"
                    }
                ],
                "author_detail": {
                    "name": "Giuseppe Caire"
                },
                "author": "Giuseppe Caire",
                "arxiv_comment": "35 pages, 4 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.11834v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.11834v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IT",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.11779v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.11779v1",
                "updated": "2025-01-20T23:10:13Z",
                "updated_parsed": [
                    2025,
                    1,
                    20,
                    23,
                    10,
                    13,
                    0,
                    20,
                    0
                ],
                "published": "2025-01-20T23:10:13Z",
                "published_parsed": [
                    2025,
                    1,
                    20,
                    23,
                    10,
                    13,
                    0,
                    20,
                    0
                ],
                "title": "Glinthawk: A Two-Tiered Architecture for High-Throughput LLM Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Glinthawk: A Two-Tiered Architecture for High-Throughput LLM Inference"
                },
                "summary": "Large Language Models (LLM) have revolutionized natural language processing,\nbut their inference demands substantial resources, while under-utilizing\nhigh-end accelerators like GPUs. A major bottleneck arises from the attention\nmechanism, which requires storing large key-value caches, limiting the maximum\nachievable throughput way below the available computing resources. Current\napproaches attempt to mitigate this issue through memory-efficient attention\nand paging mechanisms, but remained constrained by the assumption that all\noperations must be performed on high-end accelerators.\n  In this work, we propose Glinthawk, a two-tiered architecture that decouples\nthe attention mechanism from the rest of the Transformer model. This approach\nallows the memory requirements for attention to scale independently, enabling\nlarger batch sizes and more efficient use of the high-end accelerators. We\nprototype Glinthawk with NVIDIA T4 GPUs as one tier and standard CPU VMs as the\nother. Compared to a traditional single-tier setup, it improves throughput by\n$5.9\\times$ and reduces cost of generation by $2.8\\times$. For longer sequence\nlengths, it achieves $16.3\\times$ throughput improvement at $2.4\\times$ less\ncost. Our evaluation shows that this architecture can tolerate moderate network\nlatency with minimal performance degradation, making it highly effective for\nlatency-tolerant, throughput-oriented applications such as batch processing. We\nshared our prototype publicly at \\url{https://github.com/microsoft/glinthawk}.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLM) have revolutionized natural language processing,\nbut their inference demands substantial resources, while under-utilizing\nhigh-end accelerators like GPUs. A major bottleneck arises from the attention\nmechanism, which requires storing large key-value caches, limiting the maximum\nachievable throughput way below the available computing resources. Current\napproaches attempt to mitigate this issue through memory-efficient attention\nand paging mechanisms, but remained constrained by the assumption that all\noperations must be performed on high-end accelerators.\n  In this work, we propose Glinthawk, a two-tiered architecture that decouples\nthe attention mechanism from the rest of the Transformer model. This approach\nallows the memory requirements for attention to scale independently, enabling\nlarger batch sizes and more efficient use of the high-end accelerators. We\nprototype Glinthawk with NVIDIA T4 GPUs as one tier and standard CPU VMs as the\nother. Compared to a traditional single-tier setup, it improves throughput by\n$5.9\\times$ and reduces cost of generation by $2.8\\times$. For longer sequence\nlengths, it achieves $16.3\\times$ throughput improvement at $2.4\\times$ less\ncost. Our evaluation shows that this architecture can tolerate moderate network\nlatency with minimal performance degradation, making it highly effective for\nlatency-tolerant, throughput-oriented applications such as batch processing. We\nshared our prototype publicly at \\url{https://github.com/microsoft/glinthawk}."
                },
                "authors": [
                    {
                        "name": "Pouya Hamadanian"
                    },
                    {
                        "name": "Sadjad Fouladi"
                    }
                ],
                "author_detail": {
                    "name": "Sadjad Fouladi"
                },
                "author": "Sadjad Fouladi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.11779v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.11779v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.11502v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.11502v1",
                "updated": "2025-01-20T14:19:48Z",
                "updated_parsed": [
                    2025,
                    1,
                    20,
                    14,
                    19,
                    48,
                    0,
                    20,
                    0
                ],
                "published": "2025-01-20T14:19:48Z",
                "published_parsed": [
                    2025,
                    1,
                    20,
                    14,
                    19,
                    48,
                    0,
                    20,
                    0
                ],
                "title": "Hierarchical Coded Caching in High Memory Regime with Coded Placement",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Hierarchical Coded Caching in High Memory Regime with Coded Placement"
                },
                "summary": "We consider a two-layer hierarchical coded caching network where a server\nwith a library of $N$ files is connected to $K_1$ mirrors, each having a cache\nmemory of size $M_1$. Each mirror is further connected to $K_2$ users, each\nequipped with a dedicated cache of size $M_2$. In this paper, we propose two\ndistinct coded caching schemes based on coded placement, corresponding to two\ndistinct memory pairs, \\( (M_1, M_2) \\). We show that the proposed schemes\noutperform the existing schemes at these memory points given by the proposed\nschemes for smaller values of $K_2$. In setups where mirrors are positioned\nnear each other, avoiding signal interference is crucial. This can be ensured\nby having all mirrors transmit using orthogonal carrier frequencies. To compare\nour schemes with existing ones, we used the composite rate metric, which\naccurately represents the total bandwidth utilized in such setups. The\ncomposite rate is given by $\\overline{R} = R_1 + K_1 R_2$, where $R_1$ is the\nrate from the server to the mirrors, and $R_2$ is the rate from the mirrors to\nthe users, with respect to $M_1$ and $M_2$.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We consider a two-layer hierarchical coded caching network where a server\nwith a library of $N$ files is connected to $K_1$ mirrors, each having a cache\nmemory of size $M_1$. Each mirror is further connected to $K_2$ users, each\nequipped with a dedicated cache of size $M_2$. In this paper, we propose two\ndistinct coded caching schemes based on coded placement, corresponding to two\ndistinct memory pairs, \\( (M_1, M_2) \\). We show that the proposed schemes\noutperform the existing schemes at these memory points given by the proposed\nschemes for smaller values of $K_2$. In setups where mirrors are positioned\nnear each other, avoiding signal interference is crucial. This can be ensured\nby having all mirrors transmit using orthogonal carrier frequencies. To compare\nour schemes with existing ones, we used the composite rate metric, which\naccurately represents the total bandwidth utilized in such setups. The\ncomposite rate is given by $\\overline{R} = R_1 + K_1 R_2$, where $R_1$ is the\nrate from the server to the mirrors, and $R_2$ is the rate from the mirrors to\nthe users, with respect to $M_1$ and $M_2$."
                },
                "authors": [
                    {
                        "name": "Rajlaxmi Pandey"
                    },
                    {
                        "name": "Charul Rajput"
                    },
                    {
                        "name": "B. Sundar Rajan"
                    }
                ],
                "author_detail": {
                    "name": "B. Sundar Rajan"
                },
                "author": "B. Sundar Rajan",
                "arxiv_comment": "7 pages, 3 figures and 2 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.11502v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.11502v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IT",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.10659v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.10659v3",
                "updated": "2025-01-20T08:44:01Z",
                "updated_parsed": [
                    2025,
                    1,
                    20,
                    8,
                    44,
                    1,
                    0,
                    20,
                    0
                ],
                "published": "2024-11-16T01:39:44Z",
                "published_parsed": [
                    2024,
                    11,
                    16,
                    1,
                    39,
                    44,
                    5,
                    321,
                    0
                ],
                "title": "Spineless Traversal for Layout Invalidation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Spineless Traversal for Layout Invalidation"
                },
                "summary": "Latency is a major concern for web rendering engines like those in Chrome,\nSafari, and Firefox. These engines reduce latency by using an incremental\nlayout algorithm to redraw the page when the user interacts with it. In such an\nalgorithm, elements that change frame-to-frame are marked dirty; only the dirty\nelements need be processed to draw the next frame, dramatically reducing\nlatency. However, the standard incremental layout algorithm must search the\npage for dirty elements, accessing a number of auxiliary elements in the\nprocess. These auxiliary elements add cache misses and stalled cycles, and are\nresponsible for a sizable fraction of all layout latency. We introduce a new,\nfaster incremental layout algorithm called Spineless Traversal. Spineless\nTraversal uses a more computationally demanding priority queue algorithm to\navoid the need to access auxiliary nodes and thus reduces cache traffic and\nstalls. This leads to dramatic speedups on the most latency-critical\ninteractions such as hovering, typing, or animations. Moreover, thanks to\nnumerous low-level optimizations, we are able to make Spineless Traversal\ncompetitive across the whole spectrum of incremental layout workloads. As a\nresult, across 2216 benchmarks, Spineless Traversal is faster on 78.2% of the\nbenchmark, with a mean speedup of 3.23x concentrated in the most\nlatency-critical interactions such as hovering, typing, and animations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Latency is a major concern for web rendering engines like those in Chrome,\nSafari, and Firefox. These engines reduce latency by using an incremental\nlayout algorithm to redraw the page when the user interacts with it. In such an\nalgorithm, elements that change frame-to-frame are marked dirty; only the dirty\nelements need be processed to draw the next frame, dramatically reducing\nlatency. However, the standard incremental layout algorithm must search the\npage for dirty elements, accessing a number of auxiliary elements in the\nprocess. These auxiliary elements add cache misses and stalled cycles, and are\nresponsible for a sizable fraction of all layout latency. We introduce a new,\nfaster incremental layout algorithm called Spineless Traversal. Spineless\nTraversal uses a more computationally demanding priority queue algorithm to\navoid the need to access auxiliary nodes and thus reduces cache traffic and\nstalls. This leads to dramatic speedups on the most latency-critical\ninteractions such as hovering, typing, or animations. Moreover, thanks to\nnumerous low-level optimizations, we are able to make Spineless Traversal\ncompetitive across the whole spectrum of incremental layout workloads. As a\nresult, across 2216 benchmarks, Spineless Traversal is faster on 78.2% of the\nbenchmark, with a mean speedup of 3.23x concentrated in the most\nlatency-critical interactions such as hovering, typing, and animations."
                },
                "authors": [
                    {
                        "name": "Marisa Kirisame"
                    },
                    {
                        "name": "Tiezhi Wang"
                    },
                    {
                        "name": "Pavel Panchekha"
                    }
                ],
                "author_detail": {
                    "name": "Pavel Panchekha"
                },
                "author": "Pavel Panchekha",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.10659v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.10659v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.PL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.PL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.11175v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.11175v1",
                "updated": "2025-01-19T21:25:53Z",
                "updated_parsed": [
                    2025,
                    1,
                    19,
                    21,
                    25,
                    53,
                    6,
                    19,
                    0
                ],
                "published": "2025-01-19T21:25:53Z",
                "published_parsed": [
                    2025,
                    1,
                    19,
                    21,
                    25,
                    53,
                    6,
                    19,
                    0
                ],
                "title": "ProKeR: A Kernel Perspective on Few-Shot Adaptation of Large\n  Vision-Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ProKeR: A Kernel Perspective on Few-Shot Adaptation of Large\n  Vision-Language Models"
                },
                "summary": "The growing popularity of Contrastive Language-Image Pretraining (CLIP) has\nled to its widespread application in various visual downstream tasks. To\nenhance CLIP's effectiveness and versatility, efficient few-shot adaptation\ntechniques have been widely adopted. Among these approaches, training-free\nmethods, particularly caching methods exemplified by Tip-Adapter, have gained\nattention for their lightweight adaptation without the need for additional\nfine-tuning. In this paper, we revisit Tip-Adapter from a kernel perspective,\nshowing that caching methods function as local adapters and are connected to a\nwell-established kernel literature. Drawing on this insight, we offer a\ntheoretical understanding of how these methods operate and suggest multiple\navenues for enhancing the Tip-Adapter baseline. Notably, our analysis shows the\nimportance of incorporating global information in local adapters. Therefore, we\nsubsequently propose a global method that learns a proximal regularizer in a\nreproducing kernel Hilbert space (RKHS) using CLIP as a base learner. Our\nmethod, which we call ProKeR (Proximal Kernel ridge Regression), has a closed\nform solution and achieves state-of-the-art performances across 11 datasets in\nthe standard few-shot adaptation benchmark.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The growing popularity of Contrastive Language-Image Pretraining (CLIP) has\nled to its widespread application in various visual downstream tasks. To\nenhance CLIP's effectiveness and versatility, efficient few-shot adaptation\ntechniques have been widely adopted. Among these approaches, training-free\nmethods, particularly caching methods exemplified by Tip-Adapter, have gained\nattention for their lightweight adaptation without the need for additional\nfine-tuning. In this paper, we revisit Tip-Adapter from a kernel perspective,\nshowing that caching methods function as local adapters and are connected to a\nwell-established kernel literature. Drawing on this insight, we offer a\ntheoretical understanding of how these methods operate and suggest multiple\navenues for enhancing the Tip-Adapter baseline. Notably, our analysis shows the\nimportance of incorporating global information in local adapters. Therefore, we\nsubsequently propose a global method that learns a proximal regularizer in a\nreproducing kernel Hilbert space (RKHS) using CLIP as a base learner. Our\nmethod, which we call ProKeR (Proximal Kernel ridge Regression), has a closed\nform solution and achieves state-of-the-art performances across 11 datasets in\nthe standard few-shot adaptation benchmark."
                },
                "authors": [
                    {
                        "name": "Yassir Bendou"
                    },
                    {
                        "name": "Amine Ouasfi"
                    },
                    {
                        "name": "Vincent Gripon"
                    },
                    {
                        "name": "Adnane Boukhayma"
                    }
                ],
                "author_detail": {
                    "name": "Adnane Boukhayma"
                },
                "author": "Adnane Boukhayma",
                "arxiv_comment": "Code available at https://ybendou.github.io/ProKeR",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.11175v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.11175v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.02088v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.02088v3",
                "updated": "2025-01-19T19:46:21Z",
                "updated_parsed": [
                    2025,
                    1,
                    19,
                    19,
                    46,
                    21,
                    6,
                    19,
                    0
                ],
                "published": "2024-09-03T17:40:24Z",
                "published_parsed": [
                    2024,
                    9,
                    3,
                    17,
                    40,
                    24,
                    1,
                    247,
                    0
                ],
                "title": "Cache Coherence Over Disaggregated Memory",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cache Coherence Over Disaggregated Memory"
                },
                "summary": "Disaggregating memory from compute offers the opportunity to better utilize\nstranded memory in cloud data centers. It is important to cache data in the\ncompute nodes and maintain cache coherence across multiple compute nodes.\nHowever, the limited computing power on disaggregated memory servers makes\ntraditional cache coherence protocols suboptimal, particularly in the case of\nstranded memory. This paper introduces SELCC; a Shared-Exclusive Latch Cache\nCoherence protocol that maintains cache coherence without imposing any\ncomputational burden on the remote memory side. It aligns the state machine of\nthe shared-exclusive latch protocol with the MSI protocol by introducing lazy\nlatch-release and invalidation messages, thereby ensuring both atomicity of\ndata access and cache coherence. SELCC embeds cache-ownership metadata directly\ninto the RDMA latch word, enabling efficient cache ownership management via\nRDMA atomic operations. SELCC can serve as an abstraction layer over\ndisaggregated memory with APIs that resemble main-memory accesses. A concurrent\nB-tree and three transaction concurrency control algorithms are realized using\nSELCC's abstraction layer. Experimental results show that SELCC significantly\noutperforms Remote-Procedure-Call-based protocols for cache coherence under\nlimited remote computing power. Applications on SELCC achieve comparable or\nsuperior performance over disaggregated memory compared to competitors.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Disaggregating memory from compute offers the opportunity to better utilize\nstranded memory in cloud data centers. It is important to cache data in the\ncompute nodes and maintain cache coherence across multiple compute nodes.\nHowever, the limited computing power on disaggregated memory servers makes\ntraditional cache coherence protocols suboptimal, particularly in the case of\nstranded memory. This paper introduces SELCC; a Shared-Exclusive Latch Cache\nCoherence protocol that maintains cache coherence without imposing any\ncomputational burden on the remote memory side. It aligns the state machine of\nthe shared-exclusive latch protocol with the MSI protocol by introducing lazy\nlatch-release and invalidation messages, thereby ensuring both atomicity of\ndata access and cache coherence. SELCC embeds cache-ownership metadata directly\ninto the RDMA latch word, enabling efficient cache ownership management via\nRDMA atomic operations. SELCC can serve as an abstraction layer over\ndisaggregated memory with APIs that resemble main-memory accesses. A concurrent\nB-tree and three transaction concurrency control algorithms are realized using\nSELCC's abstraction layer. Experimental results show that SELCC significantly\noutperforms Remote-Procedure-Call-based protocols for cache coherence under\nlimited remote computing power. Applications on SELCC achieve comparable or\nsuperior performance over disaggregated memory compared to competitors."
                },
                "authors": [
                    {
                        "name": "Ruihong Wang"
                    },
                    {
                        "name": "Jianguo Wang"
                    },
                    {
                        "name": "Walid G. Aref"
                    }
                ],
                "author_detail": {
                    "name": "Walid G. Aref"
                },
                "author": "Walid G. Aref",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.02088v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.02088v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.ET",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.11126v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.11126v1",
                "updated": "2025-01-19T17:33:28Z",
                "updated_parsed": [
                    2025,
                    1,
                    19,
                    17,
                    33,
                    28,
                    6,
                    19,
                    0
                ],
                "published": "2025-01-19T17:33:28Z",
                "published_parsed": [
                    2025,
                    1,
                    19,
                    17,
                    33,
                    28,
                    6,
                    19,
                    0
                ],
                "title": "SIC-free Multicast Scheduling for Multi-antenna Coded Caching",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SIC-free Multicast Scheduling for Multi-antenna Coded Caching"
                },
                "summary": "Multi-antenna coded caching (CC) with multicast beamforming often relies on\ncomplex successive interference cancellation (SIC) structures to decode a\nsuperposition of multiple streams received by each user. Traditional\nsignal-level schemes require the regeneration of interfering signals from the\ncache, adding significant computational complexity. To address this, we propose\na bit-level multicast scheduling scheme enabling linear, SIC-free decoding of\nparallel streams by repeatedly transmitting data terms with linearly\nindependent coefficients. Two reference strategies for constructing the\ncoefficients matrix are considered: a random strategy, which lacks control over\nmatrix construction, and an equal-distant strategy, which balances users'\ninterference and data terms equally. In contrast, the proposed sparse strategy\nminimizes the number of multicast streams transmitted in parallel during each\ninterval, simplifying the system while optimizing resource usage. To further\nenhance the symmetric rate, a successive projection algorithm is applied to\nexploit channel properties and optimize user ordering. With the coefficients\nmatrix and optimized user ordering in place, multicast beamformers are refined\nto aggregate desired data from relevant multicast streams. Numerical\nsimulations validate the effectiveness of the sparse strategy, demonstrating\nsignificant gains in symmetric rate.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-antenna coded caching (CC) with multicast beamforming often relies on\ncomplex successive interference cancellation (SIC) structures to decode a\nsuperposition of multiple streams received by each user. Traditional\nsignal-level schemes require the regeneration of interfering signals from the\ncache, adding significant computational complexity. To address this, we propose\na bit-level multicast scheduling scheme enabling linear, SIC-free decoding of\nparallel streams by repeatedly transmitting data terms with linearly\nindependent coefficients. Two reference strategies for constructing the\ncoefficients matrix are considered: a random strategy, which lacks control over\nmatrix construction, and an equal-distant strategy, which balances users'\ninterference and data terms equally. In contrast, the proposed sparse strategy\nminimizes the number of multicast streams transmitted in parallel during each\ninterval, simplifying the system while optimizing resource usage. To further\nenhance the symmetric rate, a successive projection algorithm is applied to\nexploit channel properties and optimize user ordering. With the coefficients\nmatrix and optimized user ordering in place, multicast beamformers are refined\nto aggregate desired data from relevant multicast streams. Numerical\nsimulations validate the effectiveness of the sparse strategy, demonstrating\nsignificant gains in symmetric rate."
                },
                "authors": [
                    {
                        "name": "MohammadJavad Sojdeh"
                    },
                    {
                        "name": "MohammadJavad Salehi"
                    },
                    {
                        "name": "Antti Tölli"
                    }
                ],
                "author_detail": {
                    "name": "Antti Tölli"
                },
                "author": "Antti Tölli",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.11126v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.11126v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IT",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2312.15024v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2312.15024v2",
                "updated": "2025-01-19T15:47:14Z",
                "updated_parsed": [
                    2025,
                    1,
                    19,
                    15,
                    47,
                    14,
                    6,
                    19,
                    0
                ],
                "published": "2023-12-22T19:15:23Z",
                "published_parsed": [
                    2023,
                    12,
                    22,
                    19,
                    15,
                    23,
                    4,
                    356,
                    0
                ],
                "title": "Coded Caching for Hierarchical Two-Layer Networks with Coded Placement",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Coded Caching for Hierarchical Two-Layer Networks with Coded Placement"
                },
                "summary": "We examine a two-layered hierarchical coded caching problem, a configuration\naddressed in existing research. This involves a server connected to $K_1$\nmirrors, each of which serves $K_2$ users. The mirrors and the users are\nequipped with caches of size $M_1$ and $M_2$, respectively. We propose a\nhierarchical coded caching scheme with coded placements that outperforms\nexisting schemes. To ensure a fair comparison, we introduce the notion of\ncomposite rate, defined as $\\overline{R} = R_1 + K_1 R_2$, where $R_1$ is the\nrate from the server to mirrors and $R_2$ is the rate from mirrors to users.\nThe composite rate has not been discussed before in the literature and is\npertinent when mirrors transmit with different carrier frequencies. For the\nproposed scheme, we show a trade-off between the global memory\n$\\overline{M}=K_1M_1+K_1K_2M_2$ of the system and the composite rate and\ncompare with the existing schemes. Additionally, we conduct this comparative\nanalysis by plotting $R_1$ + $R_2$ against global memory, which is particularly\nbeneficial for systems wherein each mirror can utilize the same carrier\nfrequency, given their significant spatial separation. Additionally, we propose\nan optimized scheme for the specific case of a single mirror, showing improved\nperformance in this scenario.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We examine a two-layered hierarchical coded caching problem, a configuration\naddressed in existing research. This involves a server connected to $K_1$\nmirrors, each of which serves $K_2$ users. The mirrors and the users are\nequipped with caches of size $M_1$ and $M_2$, respectively. We propose a\nhierarchical coded caching scheme with coded placements that outperforms\nexisting schemes. To ensure a fair comparison, we introduce the notion of\ncomposite rate, defined as $\\overline{R} = R_1 + K_1 R_2$, where $R_1$ is the\nrate from the server to mirrors and $R_2$ is the rate from mirrors to users.\nThe composite rate has not been discussed before in the literature and is\npertinent when mirrors transmit with different carrier frequencies. For the\nproposed scheme, we show a trade-off between the global memory\n$\\overline{M}=K_1M_1+K_1K_2M_2$ of the system and the composite rate and\ncompare with the existing schemes. Additionally, we conduct this comparative\nanalysis by plotting $R_1$ + $R_2$ against global memory, which is particularly\nbeneficial for systems wherein each mirror can utilize the same carrier\nfrequency, given their significant spatial separation. Additionally, we propose\nan optimized scheme for the specific case of a single mirror, showing improved\nperformance in this scenario."
                },
                "authors": [
                    {
                        "name": "Rajlaxmi Pandey"
                    },
                    {
                        "name": "Charul Rajput"
                    },
                    {
                        "name": "B. Sundar Rajan"
                    }
                ],
                "author_detail": {
                    "name": "B. Sundar Rajan"
                },
                "author": "B. Sundar Rajan",
                "arxiv_comment": "47 pages, 16 figures and 2 tables. More figures, explanations and\n  comparisons included",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2312.15024v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2312.15024v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IT",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.10854v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.10854v1",
                "updated": "2025-01-18T19:10:23Z",
                "updated_parsed": [
                    2025,
                    1,
                    18,
                    19,
                    10,
                    23,
                    5,
                    18,
                    0
                ],
                "published": "2025-01-18T19:10:23Z",
                "published_parsed": [
                    2025,
                    1,
                    18,
                    19,
                    10,
                    23,
                    5,
                    18,
                    0
                ],
                "title": "Achievable DoF Bounds for Cache-Aided Asymmetric MIMO Communications",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Achievable DoF Bounds for Cache-Aided Asymmetric MIMO Communications"
                },
                "summary": "Integrating coded caching (CC) into multiple-input multiple-output (MIMO)\ncommunications can significantly enhance the achievable degrees of freedom\n(DoF) in wireless networks. This paper investigates a practical cache-aided\nasymmetric MIMO configuration with cache ratio $\\gamma$, where a server\nequipped with $L$ transmit antennas communicates with $K$ users, each having\n$G_k$ receive antennas. We propose three content-aware MIMO-CC strategies: the\n\\emph{min-G} scheme, which treats the system as symmetric by assuming all users\nhave the same number of antennas, equal to the smallest among them; the\n\\emph{Grouping} scheme, which maximizes spatial multiplexing gain separately\nwithin each user subset at the cost of some global caching gain; and the\n\\emph{Phantom} scheme, which dynamically redistributes spatial resources using\nvirtual or \"phantom\" antenna users, bridging the performance gains of the min-G\nand Grouping schemes. These strategies jointly optimize the number of users,\n$\\Omega$, and the parallel streams decoded by each user, $\\beta_k$, ensuring\nlinear decodability for all target users. Analytical and numerical results\nconfirm that the proposed schemes achieve significant DoF improvements across\nvarious system configurations, demonstrating the potential of content-aware\nMIMO-CC strategies for enhancing wireless network performance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Integrating coded caching (CC) into multiple-input multiple-output (MIMO)\ncommunications can significantly enhance the achievable degrees of freedom\n(DoF) in wireless networks. This paper investigates a practical cache-aided\nasymmetric MIMO configuration with cache ratio $\\gamma$, where a server\nequipped with $L$ transmit antennas communicates with $K$ users, each having\n$G_k$ receive antennas. We propose three content-aware MIMO-CC strategies: the\n\\emph{min-G} scheme, which treats the system as symmetric by assuming all users\nhave the same number of antennas, equal to the smallest among them; the\n\\emph{Grouping} scheme, which maximizes spatial multiplexing gain separately\nwithin each user subset at the cost of some global caching gain; and the\n\\emph{Phantom} scheme, which dynamically redistributes spatial resources using\nvirtual or \"phantom\" antenna users, bridging the performance gains of the min-G\nand Grouping schemes. These strategies jointly optimize the number of users,\n$\\Omega$, and the parallel streams decoded by each user, $\\beta_k$, ensuring\nlinear decodability for all target users. Analytical and numerical results\nconfirm that the proposed schemes achieve significant DoF improvements across\nvarious system configurations, demonstrating the potential of content-aware\nMIMO-CC strategies for enhancing wireless network performance."
                },
                "authors": [
                    {
                        "name": "Mohammad NaseriTehrani"
                    },
                    {
                        "name": "MohammadJavad Salehi"
                    },
                    {
                        "name": "Antti Tölli"
                    }
                ],
                "author_detail": {
                    "name": "Antti Tölli"
                },
                "author": "Antti Tölli",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.10854v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.10854v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IT",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.10756v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.10756v1",
                "updated": "2025-01-18T13:04:23Z",
                "updated_parsed": [
                    2025,
                    1,
                    18,
                    13,
                    4,
                    23,
                    5,
                    18,
                    0
                ],
                "published": "2025-01-18T13:04:23Z",
                "published_parsed": [
                    2025,
                    1,
                    18,
                    13,
                    4,
                    23,
                    5,
                    18,
                    0
                ],
                "title": "D2D Coded Caching Schemes for Multiaccess Networks with Combinatorial\n  Access Topology",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "D2D Coded Caching Schemes for Multiaccess Networks with Combinatorial\n  Access Topology"
                },
                "summary": "This paper considers wireless device-to-device (D2D) coded caching in a\nmultiaccess network, where the users communicate with each other and each user\ncan access multiple cache nodes. Access topologies derived from two\ncombinatorial designs known as the $t$-design and $t$-group divisible design\n($t$-GDD), referred to as the $t$-design and $t$-GDD topologies respectively,\nwhich subsume a few other known topologies, have been studied for the\nmultiaccess coded caching (MACC) network by Cheng \\textit{et al.} in\n\\cite{MACC_des}. These access topologies are extended to a multiaccess D2D\ncoded caching (MADCC) network and novel MADCC schemes are proposed. MADCC\nnetwork has been studied so far only for the cyclic wrap-around topology. Apart\nfrom the proposed novel MADCC schemes, MADCC schemes are also derived from the\nexisting MACC schemes in \\cite{MACC_des}. To compare the performance of\ndifferent MADCC schemes, the metrics of load per user and subpacketization\nlevel are used while keeping the number of caches and cache memory size same.\nThe proposed MADCC scheme with $t$-design topology performs better in terms of\nsubpacketization level while achieving the same load per user compared to the\nMADCC scheme derived from the MACC scheme with $t$-design topology in\n\\cite{MACC_des}. The proposed MADCC scheme with $t$-GDD topology performs\nbetter in terms of load per user while achieving the same subpacketization\nlevel compared to the MADCC scheme derived from the MACC scheme with $t$-GDD\ntopology in \\cite{MACC_des} in some cases. Compared to the existing MADCC\nscheme with cyclic wrap-around topology, the proposed MADCC scheme with\n$t$-design topology performs better in terms of load per user, and the proposed\nMADCC scheme with $t$-GDD topology performs better in terms of subpacketization\nlevel at the expense of an increase in load per user.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper considers wireless device-to-device (D2D) coded caching in a\nmultiaccess network, where the users communicate with each other and each user\ncan access multiple cache nodes. Access topologies derived from two\ncombinatorial designs known as the $t$-design and $t$-group divisible design\n($t$-GDD), referred to as the $t$-design and $t$-GDD topologies respectively,\nwhich subsume a few other known topologies, have been studied for the\nmultiaccess coded caching (MACC) network by Cheng \\textit{et al.} in\n\\cite{MACC_des}. These access topologies are extended to a multiaccess D2D\ncoded caching (MADCC) network and novel MADCC schemes are proposed. MADCC\nnetwork has been studied so far only for the cyclic wrap-around topology. Apart\nfrom the proposed novel MADCC schemes, MADCC schemes are also derived from the\nexisting MACC schemes in \\cite{MACC_des}. To compare the performance of\ndifferent MADCC schemes, the metrics of load per user and subpacketization\nlevel are used while keeping the number of caches and cache memory size same.\nThe proposed MADCC scheme with $t$-design topology performs better in terms of\nsubpacketization level while achieving the same load per user compared to the\nMADCC scheme derived from the MACC scheme with $t$-design topology in\n\\cite{MACC_des}. The proposed MADCC scheme with $t$-GDD topology performs\nbetter in terms of load per user while achieving the same subpacketization\nlevel compared to the MADCC scheme derived from the MACC scheme with $t$-GDD\ntopology in \\cite{MACC_des} in some cases. Compared to the existing MADCC\nscheme with cyclic wrap-around topology, the proposed MADCC scheme with\n$t$-design topology performs better in terms of load per user, and the proposed\nMADCC scheme with $t$-GDD topology performs better in terms of subpacketization\nlevel at the expense of an increase in load per user."
                },
                "authors": [
                    {
                        "name": "Rashid Ummer N. T."
                    },
                    {
                        "name": "B. Sundar Rajan"
                    }
                ],
                "author_detail": {
                    "name": "B. Sundar Rajan"
                },
                "author": "B. Sundar Rajan",
                "arxiv_comment": "21 pages, 12 figures and 4 tables. Some overlap with 2409.14350v1\n  [cs.IT] 22 Sept. 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.10756v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.10756v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IT",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.10682v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.10682v1",
                "updated": "2025-01-18T07:29:20Z",
                "updated_parsed": [
                    2025,
                    1,
                    18,
                    7,
                    29,
                    20,
                    5,
                    18,
                    0
                ],
                "published": "2025-01-18T07:29:20Z",
                "published_parsed": [
                    2025,
                    1,
                    18,
                    7,
                    29,
                    20,
                    5,
                    18,
                    0
                ],
                "title": "SkyByte: Architecting an Efficient Memory-Semantic CXL-based SSD with OS\n  and Hardware Co-design",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SkyByte: Architecting an Efficient Memory-Semantic CXL-based SSD with OS\n  and Hardware Co-design"
                },
                "summary": "The CXL-based solid-state drive (CXL-SSD) provides a promising approach\ntowards scaling the main memory capacity at low cost. However, the CXL-SSD\nfaces performance challenges due to the long flash access latency and\nunpredictable events such as garbage collection in the SSD device, stalling the\nhost processor and wasting compute cycles. Although the CXL interface enables\nthe byte-granular data access to the SSD, accessing flash chips is still at\npage granularity due to physical limitations. The mismatch of access\ngranularity causes significant unnecessary I/O traffic to flash chips,\nworsening the suboptimal end-to-end data access performance. In this paper, we\npresent SkyByte, an efficient CXL-based SSD that employs a holistic approach to\naddress the aforementioned challenges by co-designing the host operating system\n(OS) and SSD controller. To alleviate the long memory stall when accessing the\nCXL-SSD, SkyByte revisits the OS context switch mechanism and enables\nopportunistic context switches upon the detection of long access delays. To\naccommodate byte-granular data accesses, SkyByte architects the internal DRAM\nof the SSD controller into a cacheline-level write log and a page-level data\ncache, and enables data coalescing upon log cleaning to reduce the I/O traffic\nto flash chips. SkyByte also employs optimization techniques that include\nadaptive page migration for exploring the performance benefits of fast host\nmemory by promoting hot pages in CXL-SSD to the host. We implement SkyByte with\na CXL-SSD simulator and evaluate its efficiency with various data-intensive\napplications. Our experiments show that SkyByte outperforms current CXL-based\nSSD by 6.11X, and reduces the I/O traffic to flash chips by 23.08X on average.\nSkyByte also reaches 75% of the performance of the ideal case that assumes\nunlimited DRAM capacity in the host, which offers an attractive cost-effective\nsolution.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The CXL-based solid-state drive (CXL-SSD) provides a promising approach\ntowards scaling the main memory capacity at low cost. However, the CXL-SSD\nfaces performance challenges due to the long flash access latency and\nunpredictable events such as garbage collection in the SSD device, stalling the\nhost processor and wasting compute cycles. Although the CXL interface enables\nthe byte-granular data access to the SSD, accessing flash chips is still at\npage granularity due to physical limitations. The mismatch of access\ngranularity causes significant unnecessary I/O traffic to flash chips,\nworsening the suboptimal end-to-end data access performance. In this paper, we\npresent SkyByte, an efficient CXL-based SSD that employs a holistic approach to\naddress the aforementioned challenges by co-designing the host operating system\n(OS) and SSD controller. To alleviate the long memory stall when accessing the\nCXL-SSD, SkyByte revisits the OS context switch mechanism and enables\nopportunistic context switches upon the detection of long access delays. To\naccommodate byte-granular data accesses, SkyByte architects the internal DRAM\nof the SSD controller into a cacheline-level write log and a page-level data\ncache, and enables data coalescing upon log cleaning to reduce the I/O traffic\nto flash chips. SkyByte also employs optimization techniques that include\nadaptive page migration for exploring the performance benefits of fast host\nmemory by promoting hot pages in CXL-SSD to the host. We implement SkyByte with\na CXL-SSD simulator and evaluate its efficiency with various data-intensive\napplications. Our experiments show that SkyByte outperforms current CXL-based\nSSD by 6.11X, and reduces the I/O traffic to flash chips by 23.08X on average.\nSkyByte also reaches 75% of the performance of the ideal case that assumes\nunlimited DRAM capacity in the host, which offers an attractive cost-effective\nsolution."
                },
                "authors": [
                    {
                        "name": "Haoyang Zhang"
                    },
                    {
                        "name": "Yuqi Xue"
                    },
                    {
                        "name": "Yirui Eric Zhou"
                    },
                    {
                        "name": "Shaobo Li"
                    },
                    {
                        "name": "Jian Huang"
                    }
                ],
                "author_detail": {
                    "name": "Jian Huang"
                },
                "author": "Jian Huang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.10682v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.10682v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.05221v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.05221v3",
                "updated": "2025-01-17T16:16:54Z",
                "updated_parsed": [
                    2025,
                    1,
                    17,
                    16,
                    16,
                    54,
                    4,
                    17,
                    0
                ],
                "published": "2024-09-08T20:47:44Z",
                "published_parsed": [
                    2024,
                    9,
                    8,
                    20,
                    47,
                    44,
                    6,
                    252,
                    0
                ],
                "title": "Geometric rigidity of simple modules for algebraic groups",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Geometric rigidity of simple modules for algebraic groups"
                },
                "summary": "Let k be a field, let G be an affine algebraic k-group and V a\nfinite-dimensional G-module. We say V is rigid if the socle series and radical\nseries coincide for the action of G on each indecomposable summand of V; say V\nis geometrically rigid (resp. absolutely rigid) if V is rigid after base change\nof G and V to k (resp. any field extension of k). We show that all simple\nG-modules are geometrically rigid, though not in general absolutely rigid. More\nprecisely, we show that if V is a simple G-module, then there is a finite\npurely inseparable extension kV /k naturally attached to V such that V is\nabsolutely rigid as a G-module after base change to kV. The proof turns on an\ninvestigation of algebras of the form K otimes E where K and E are field\nextensions of k; we give an example of such an algebra which is not rigid as a\nmodule over itself. We establish the existence of the purely inseparable field\nextension kV /k through an analogous version for artinian algebras.\n  In the second half of the paper we apply recent results on the structure and\nrepresentation theory of pseudo-reductive groups to give a concrete description\nof kV when G is smooth and connected. Namely, we combine the main structure\ntheorem of the Conrad-Prasad classification of pseudo-reductive G together with\nour previous high weight theory. For V a simple G-module, we calculate the\nminimal field of definition of the geometric Jacobson radical of EndG(V) in\nterms of the high weight of V and the Conrad-Prasad classification data; this\ngives a concrete construction of the field kV as a subextension of the minimal\nfield of definition of the geometric unipotent radical of G. We also observe\nthat the Conrad-Prasad classification can be used to hone the dimension formula\nfor V we had previously established; we also use it to give a description of\nEndG(V) which includes a dimension formula.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Let k be a field, let G be an affine algebraic k-group and V a\nfinite-dimensional G-module. We say V is rigid if the socle series and radical\nseries coincide for the action of G on each indecomposable summand of V; say V\nis geometrically rigid (resp. absolutely rigid) if V is rigid after base change\nof G and V to k (resp. any field extension of k). We show that all simple\nG-modules are geometrically rigid, though not in general absolutely rigid. More\nprecisely, we show that if V is a simple G-module, then there is a finite\npurely inseparable extension kV /k naturally attached to V such that V is\nabsolutely rigid as a G-module after base change to kV. The proof turns on an\ninvestigation of algebras of the form K otimes E where K and E are field\nextensions of k; we give an example of such an algebra which is not rigid as a\nmodule over itself. We establish the existence of the purely inseparable field\nextension kV /k through an analogous version for artinian algebras.\n  In the second half of the paper we apply recent results on the structure and\nrepresentation theory of pseudo-reductive groups to give a concrete description\nof kV when G is smooth and connected. Namely, we combine the main structure\ntheorem of the Conrad-Prasad classification of pseudo-reductive G together with\nour previous high weight theory. For V a simple G-module, we calculate the\nminimal field of definition of the geometric Jacobson radical of EndG(V) in\nterms of the high weight of V and the Conrad-Prasad classification data; this\ngives a concrete construction of the field kV as a subextension of the minimal\nfield of definition of the geometric unipotent radical of G. We also observe\nthat the Conrad-Prasad classification can be used to hone the dimension formula\nfor V we had previously established; we also use it to give a description of\nEndG(V) which includes a dimension formula."
                },
                "authors": [
                    {
                        "name": "Michael Bate"
                    },
                    {
                        "name": "David I. Stewart"
                    }
                ],
                "author_detail": {
                    "name": "David I. Stewart"
                },
                "author": "David I. Stewart",
                "arxiv_comment": "v3; 30 pages; Theorem 1 now holds for arbitrary affine algebraic\n  groups over fields",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.05221v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.05221v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "math.RT",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "math.RT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.GR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.RA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "20G05",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.10138v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.10138v1",
                "updated": "2025-01-17T12:01:28Z",
                "updated_parsed": [
                    2025,
                    1,
                    17,
                    12,
                    1,
                    28,
                    4,
                    17,
                    0
                ],
                "published": "2025-01-17T12:01:28Z",
                "published_parsed": [
                    2025,
                    1,
                    17,
                    12,
                    1,
                    28,
                    4,
                    17,
                    0
                ],
                "title": "The NIC should be part of the OS",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The NIC should be part of the OS"
                },
                "summary": "The network interface adapter (NIC) is a critical component of a modern cloud\nserver which occupies a unique position. Not only is network performance vital\nto the efficient operation of the machine, but unlike application-oriented\ncompute accelerators like GPUs, the network subsystem must react to\nunpredictable events like the arrival of a network packet and communicate with\nthe appropriate application end point with minimal latency. Current approaches\nto server stacks navigate a trade-off between flexibility, efficiency, and\nperformance: the fastest kernel-bypass approaches dedicate cores to\napplications, busy-wait on receive queues, etc. while more flexible approaches\nappropriate to more dynamic workload mixes incur much greater software overhead\non the data path. However, we reject this trade-off, which we ascribe to an\narbitrary (and sub-optimal) split in system state between the OS and the NIC.\nInstead, by exploiting the properties of cache-coherent interconnects and\nintegrating the NIC closely with the OS kernel, we can achieve something\nsurprising: performance for RPC workloads better than the fastest kernel-bypass\napproaches without sacrificing the robustness and dynamic adaptation of\nkernel-based network subsystems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The network interface adapter (NIC) is a critical component of a modern cloud\nserver which occupies a unique position. Not only is network performance vital\nto the efficient operation of the machine, but unlike application-oriented\ncompute accelerators like GPUs, the network subsystem must react to\nunpredictable events like the arrival of a network packet and communicate with\nthe appropriate application end point with minimal latency. Current approaches\nto server stacks navigate a trade-off between flexibility, efficiency, and\nperformance: the fastest kernel-bypass approaches dedicate cores to\napplications, busy-wait on receive queues, etc. while more flexible approaches\nappropriate to more dynamic workload mixes incur much greater software overhead\non the data path. However, we reject this trade-off, which we ascribe to an\narbitrary (and sub-optimal) split in system state between the OS and the NIC.\nInstead, by exploiting the properties of cache-coherent interconnects and\nintegrating the NIC closely with the OS kernel, we can achieve something\nsurprising: performance for RPC workloads better than the fastest kernel-bypass\napproaches without sacrificing the robustness and dynamic adaptation of\nkernel-based network subsystems."
                },
                "authors": [
                    {
                        "name": "Pengcheng Xu"
                    },
                    {
                        "name": "Timothy Roscoe"
                    }
                ],
                "author_detail": {
                    "name": "Timothy Roscoe"
                },
                "author": "Timothy Roscoe",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.10138v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.10138v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.OS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.OS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.03594v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.03594v2",
                "updated": "2025-01-17T09:37:36Z",
                "updated_parsed": [
                    2025,
                    1,
                    17,
                    9,
                    37,
                    36,
                    4,
                    17,
                    0
                ],
                "published": "2024-11-29T05:57:37Z",
                "published_parsed": [
                    2024,
                    11,
                    29,
                    5,
                    57,
                    37,
                    4,
                    334,
                    0
                ],
                "title": "BatchLLM: Optimizing Large Batched LLM Inference with Global Prefix\n  Sharing and Throughput-oriented Token Batching",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "BatchLLM: Optimizing Large Batched LLM Inference with Global Prefix\n  Sharing and Throughput-oriented Token Batching"
                },
                "summary": "Large language models (LLMs) increasingly play an important role in a wide\nrange of information processing and management tasks. Many of these tasks are\nperformed in large batches or even offline, and the performance indictor for\nwhich is throughput. These tasks usually show the characteristic of prefix\nsharing, where different prompt input can partially show the common prefix.\nHowever, the existing LLM inference engines tend to optimize the streaming\nrequests and show limitations of supporting the large batched tasks with the\nprefix sharing characteristic. The existing solutions use the LRU-based cache\nto reuse the KV context of common prefix between requests. The KV context that\nare about to be reused may prematurely evicted with the implicit cache\nmanagement. Besides, the streaming oriented systems do not leverage the\nrequest-batch information and can not mix the decoding tokens with the prefill\nchunks to the best for the batched scenarios, and thus fails to saturate the\nGPU. We propose BatchLLM to address the above problems. BatchLLM explicitly\nidentifies the common prefixes globally. The requests sharing the same prefix\nwill be scheduled together to reuse the KV context the best. BatchLLM reorders\nthe requests and schedules the requests with larger ratio of decoding first to\nbetter mix the decoding tokens with the latter prefill chunks, and applies\nmemory-centric token batching to enlarge the token-batch sizes, which helps to\nincrease the GPU utilization. Finally, BatchLLM optimizes the prefix-shared\nAttention kernel with horizontal fusion to reduce tail effect and kernel launch\noverhead. Extensive evaluation shows that BatchLLM outperforms vLLM and SGLang\nby 1.3$\\times$ to 10.8$\\times$ on a set of microbenchmarks and a typical\nindustry workload under different hardware environments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) increasingly play an important role in a wide\nrange of information processing and management tasks. Many of these tasks are\nperformed in large batches or even offline, and the performance indictor for\nwhich is throughput. These tasks usually show the characteristic of prefix\nsharing, where different prompt input can partially show the common prefix.\nHowever, the existing LLM inference engines tend to optimize the streaming\nrequests and show limitations of supporting the large batched tasks with the\nprefix sharing characteristic. The existing solutions use the LRU-based cache\nto reuse the KV context of common prefix between requests. The KV context that\nare about to be reused may prematurely evicted with the implicit cache\nmanagement. Besides, the streaming oriented systems do not leverage the\nrequest-batch information and can not mix the decoding tokens with the prefill\nchunks to the best for the batched scenarios, and thus fails to saturate the\nGPU. We propose BatchLLM to address the above problems. BatchLLM explicitly\nidentifies the common prefixes globally. The requests sharing the same prefix\nwill be scheduled together to reuse the KV context the best. BatchLLM reorders\nthe requests and schedules the requests with larger ratio of decoding first to\nbetter mix the decoding tokens with the latter prefill chunks, and applies\nmemory-centric token batching to enlarge the token-batch sizes, which helps to\nincrease the GPU utilization. Finally, BatchLLM optimizes the prefix-shared\nAttention kernel with horizontal fusion to reduce tail effect and kernel launch\noverhead. Extensive evaluation shows that BatchLLM outperforms vLLM and SGLang\nby 1.3$\\times$ to 10.8$\\times$ on a set of microbenchmarks and a typical\nindustry workload under different hardware environments."
                },
                "authors": [
                    {
                        "name": "Zhen Zheng"
                    },
                    {
                        "name": "Xin Ji"
                    },
                    {
                        "name": "Taosong Fang"
                    },
                    {
                        "name": "Fanghao Zhou"
                    },
                    {
                        "name": "Chuanjie Liu"
                    },
                    {
                        "name": "Gang Peng"
                    }
                ],
                "author_detail": {
                    "name": "Gang Peng"
                },
                "author": "Gang Peng",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.03594v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.03594v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.09902v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.09902v1",
                "updated": "2025-01-17T01:24:12Z",
                "updated_parsed": [
                    2025,
                    1,
                    17,
                    1,
                    24,
                    12,
                    4,
                    17,
                    0
                ],
                "published": "2025-01-17T01:24:12Z",
                "published_parsed": [
                    2025,
                    1,
                    17,
                    1,
                    24,
                    12,
                    4,
                    17,
                    0
                ],
                "title": "Multi-Dimensional Vector ISA Extension for Mobile In-Cache Computing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-Dimensional Vector ISA Extension for Mobile In-Cache Computing"
                },
                "summary": "In-cache computing technology transforms existing caches into long-vector\ncompute units and offers low-cost alternatives to building expensive vector\nengines for mobile CPUs. Unfortunately, existing long-vector Instruction Set\nArchitecture (ISA) extensions, such as RISC-V Vector Extension (RVV) and Arm\nScalable Vector Extension (SVE), provide only one-dimensional strided and\nrandom memory accesses. While this is sufficient for typical vector engines, it\nfails to effectively utilize the large Single Instruction, Multiple Data (SIMD)\nwidths of in-cache vector engines. This is because mobile data-parallel kernels\nexpose limited parallelism across a single dimension.\n  Based on our analysis of mobile vector kernels, we introduce a long-vector\nMulti-dimensional Vector ISA Extension (MVE) for mobile in-cache computing. MVE\nachieves high SIMD resource utilization and enables flexible programming by\nabstracting cache geometry and data layout. The proposed ISA features\nmulti-dimensional strided and random memory accesses and efficient\ndimension-level masked execution to encode parallelism across multiple\ndimensions. Using a wide range of data-parallel mobile workloads, we\ndemonstrate that MVE offers significant performance and energy reduction\nbenefits of 2.9x and 8.8x, on average, compared to the SIMD units of a\ncommercial mobile processor, at an area overhead of 3.6%.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In-cache computing technology transforms existing caches into long-vector\ncompute units and offers low-cost alternatives to building expensive vector\nengines for mobile CPUs. Unfortunately, existing long-vector Instruction Set\nArchitecture (ISA) extensions, such as RISC-V Vector Extension (RVV) and Arm\nScalable Vector Extension (SVE), provide only one-dimensional strided and\nrandom memory accesses. While this is sufficient for typical vector engines, it\nfails to effectively utilize the large Single Instruction, Multiple Data (SIMD)\nwidths of in-cache vector engines. This is because mobile data-parallel kernels\nexpose limited parallelism across a single dimension.\n  Based on our analysis of mobile vector kernels, we introduce a long-vector\nMulti-dimensional Vector ISA Extension (MVE) for mobile in-cache computing. MVE\nachieves high SIMD resource utilization and enables flexible programming by\nabstracting cache geometry and data layout. The proposed ISA features\nmulti-dimensional strided and random memory accesses and efficient\ndimension-level masked execution to encode parallelism across multiple\ndimensions. Using a wide range of data-parallel mobile workloads, we\ndemonstrate that MVE offers significant performance and energy reduction\nbenefits of 2.9x and 8.8x, on average, compared to the SIMD units of a\ncommercial mobile processor, at an area overhead of 3.6%."
                },
                "authors": [
                    {
                        "name": "Alireza Khadem"
                    },
                    {
                        "name": "Daichi Fujiki"
                    },
                    {
                        "name": "Hilbert Chen"
                    },
                    {
                        "name": "Yufeng Gu"
                    },
                    {
                        "name": "Nishil Talati"
                    },
                    {
                        "name": "Scott Mahlke"
                    },
                    {
                        "name": "Reetuparna Das"
                    }
                ],
                "author_detail": {
                    "name": "Reetuparna Das"
                },
                "author": "Reetuparna Das",
                "arxiv_comment": "2025 IEEE International Symposium on High-Performance Computer\n  Architecture (HPCA)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.09902v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.09902v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.04501v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.04501v2",
                "updated": "2025-01-16T15:11:42Z",
                "updated_parsed": [
                    2025,
                    1,
                    16,
                    15,
                    11,
                    42,
                    3,
                    16,
                    0
                ],
                "published": "2024-07-05T13:42:30Z",
                "published_parsed": [
                    2024,
                    7,
                    5,
                    13,
                    42,
                    30,
                    4,
                    187,
                    0
                ],
                "title": "Cryogenic Behavior of High-Permittivity Gate Dielectrics: The Impact of\n  the Atomic Layer Deposition Temperature and the Lithographic Patterning\n  Method",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cryogenic Behavior of High-Permittivity Gate Dielectrics: The Impact of\n  the Atomic Layer Deposition Temperature and the Lithographic Patterning\n  Method"
                },
                "summary": "Dielectrics featuring a high relative permittivity, i.e., high-k dielectrics,\nhave become the standard insulators in gate architectures, enhancing the\nelectrical performance of both room temperature and cryogenic electronics. This\nstudy delves into the cryogenic (3 K) performance of high-k dielectrics\ncommonly used as gate insulators. We fabricated Al2O3 and HfO2 layers via\nAtomic Layer Deposition (ALD) and we extrapolated relative permittivity (k) and\ndielectric strength (E_BD) from AC (100 Hz to 100 kHz) and DC measurements on\nmetal-insulator-metal capacitors. Our findings reveal a strong dependence of\nHfO2 cryogenic performance on the ALD growth temperature, while the latter\nshows a negligible impact on Al2O3. We estimated a ~9 % and ~14 % reduction of\nthe relative permittivity of HfO2 and Al2O3, respectively, from 300 K to 3 K.\nAdditionally, we designed and fabricated Al2O3/HfO2 bilayers and we checked\ntheir properties at cryogenic temperatures. The study also investigates the\nimpact of the patterning method, namely, UV or electron-beam lithography\n(acceleration voltage of 10, 20, or 30 kV), on the high-k dielectric\nproperties.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Dielectrics featuring a high relative permittivity, i.e., high-k dielectrics,\nhave become the standard insulators in gate architectures, enhancing the\nelectrical performance of both room temperature and cryogenic electronics. This\nstudy delves into the cryogenic (3 K) performance of high-k dielectrics\ncommonly used as gate insulators. We fabricated Al2O3 and HfO2 layers via\nAtomic Layer Deposition (ALD) and we extrapolated relative permittivity (k) and\ndielectric strength (E_BD) from AC (100 Hz to 100 kHz) and DC measurements on\nmetal-insulator-metal capacitors. Our findings reveal a strong dependence of\nHfO2 cryogenic performance on the ALD growth temperature, while the latter\nshows a negligible impact on Al2O3. We estimated a ~9 % and ~14 % reduction of\nthe relative permittivity of HfO2 and Al2O3, respectively, from 300 K to 3 K.\nAdditionally, we designed and fabricated Al2O3/HfO2 bilayers and we checked\ntheir properties at cryogenic temperatures. The study also investigates the\nimpact of the patterning method, namely, UV or electron-beam lithography\n(acceleration voltage of 10, 20, or 30 kV), on the high-k dielectric\nproperties."
                },
                "authors": [
                    {
                        "name": "Alessandro Paghi"
                    },
                    {
                        "name": "Sebastiano Battisti"
                    },
                    {
                        "name": "Simone Tortorella"
                    },
                    {
                        "name": "Giorgio De Simoni"
                    },
                    {
                        "name": "Francesco Giazotto"
                    }
                ],
                "author_detail": {
                    "name": "Francesco Giazotto"
                },
                "author": "Francesco Giazotto",
                "arxiv_comment": "17 pages, 4 figures, supporting information at the end of the paper",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.04501v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.04501v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cond-mat.mtrl-sci",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cond-mat.mtrl-sci",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cond-mat.mes-hall",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cond-mat.supr-con",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2312.11501v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2312.11501v2",
                "updated": "2025-01-16T10:35:59Z",
                "updated_parsed": [
                    2025,
                    1,
                    16,
                    10,
                    35,
                    59,
                    3,
                    16,
                    0
                ],
                "published": "2023-12-08T15:11:26Z",
                "published_parsed": [
                    2023,
                    12,
                    8,
                    15,
                    11,
                    26,
                    4,
                    342,
                    0
                ],
                "title": "Write+Sync: Software Cache Write Covert Channels Exploiting Memory-disk\n  Synchronization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Write+Sync: Software Cache Write Covert Channels Exploiting Memory-disk\n  Synchronization"
                },
                "summary": "Memory-disk synchronization is a critical technology for ensuring data\ncorrectness, integrity, and security, especially in systems that handle\nsensitive information like financial transactions and medical records. We\npropose SYNC+SYNC, a group of attacks that exploit the memory-disk\nsynchronization primitives. SYNC+SYNC works by subtly varying the timing of\nsynchronization on the write buffer, offering several advantages: 1)\nimplemented purely in software, enabling deployment on any hardware devices; 2)\nresilient against existing cache partitioning and randomization techniques; 3)\nunaffected by prefetching techniques and cache replacement strategies. We\npresent the principles of SYNC+SYNC through the implementation of two write\ncovert channel protocols, using either a single file or page, and introduce\nthree enhanced strategies that utilize multiple files and pages. The\nfeasibility of these channels is demonstrated in both cross-process and\ncross-sandbox scenarios across diverse operating systems (OSes). Experimental\nresults show that, the average rate can reach 2.036 Kb/s (with a peak rate of\n14.762 Kb/s) and the error rate is 0% on Linux; when running on macOS, the\naverage rate achieves 10.211 Kb/s (with a peak rate of 253.022 Kb/s) and the\nerror rate is 0.004%. To the best of our knowledge, SYNC+SYNC is the first\nhigh-speed write covert channel for software cache.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Memory-disk synchronization is a critical technology for ensuring data\ncorrectness, integrity, and security, especially in systems that handle\nsensitive information like financial transactions and medical records. We\npropose SYNC+SYNC, a group of attacks that exploit the memory-disk\nsynchronization primitives. SYNC+SYNC works by subtly varying the timing of\nsynchronization on the write buffer, offering several advantages: 1)\nimplemented purely in software, enabling deployment on any hardware devices; 2)\nresilient against existing cache partitioning and randomization techniques; 3)\nunaffected by prefetching techniques and cache replacement strategies. We\npresent the principles of SYNC+SYNC through the implementation of two write\ncovert channel protocols, using either a single file or page, and introduce\nthree enhanced strategies that utilize multiple files and pages. The\nfeasibility of these channels is demonstrated in both cross-process and\ncross-sandbox scenarios across diverse operating systems (OSes). Experimental\nresults show that, the average rate can reach 2.036 Kb/s (with a peak rate of\n14.762 Kb/s) and the error rate is 0% on Linux; when running on macOS, the\naverage rate achieves 10.211 Kb/s (with a peak rate of 253.022 Kb/s) and the\nerror rate is 0.004%. To the best of our knowledge, SYNC+SYNC is the first\nhigh-speed write covert channel for software cache."
                },
                "authors": [
                    {
                        "name": "Congcong Chen"
                    },
                    {
                        "name": "Jinhua Cui"
                    },
                    {
                        "name": "Gang Qu"
                    },
                    {
                        "name": "Jiliang Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Jiliang Zhang"
                },
                "author": "Jiliang Zhang",
                "arxiv_comment": "This manuscript was published in IEEE Transactions on Information\n  Forensics and Security, 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2312.11501v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2312.11501v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.09383v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.09383v1",
                "updated": "2025-01-16T08:52:38Z",
                "updated_parsed": [
                    2025,
                    1,
                    16,
                    8,
                    52,
                    38,
                    3,
                    16,
                    0
                ],
                "published": "2025-01-16T08:52:38Z",
                "published_parsed": [
                    2025,
                    1,
                    16,
                    8,
                    52,
                    38,
                    3,
                    16,
                    0
                ],
                "title": "Adaptive Contextual Caching for Mobile Edge Large Language Model Service",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Adaptive Contextual Caching for Mobile Edge Large Language Model Service"
                },
                "summary": "Mobile edge Large Language Model (LLM) deployments face inherent constraints,\nsuch as limited computational resources and network bandwidth. Although\nRetrieval-Augmented Generation (RAG) mitigates some challenges by integrating\nexternal knowledge bases, inefficient cache management can still result in high\nretrieval latency and frequent cache updates. To address these issues, we\npropose an Adaptive Contextual Caching (ACC) framework that anticipates user\nneeds by proactively caching semantically relevant data for mobile-edge LLMs.\nACC utilizes a deep reinforcement learning (DRL) module to refine cache\nreplacement policies, balancing user context, document similarity, and the\noverhead associated with cache misses. Experimental results demonstrate that\nACC increases cache hit rates to over 80\\% after only 11 training episodes,\noutperforming FIFO, LRU, and semantic-only caching while reducing retrieval\nlatency by up to 40\\%. In particular, ACC also reduces local caching overhead\n(i.e., the cost of updating the cache when a miss occurs) by as much as 55\\%,\nenabling scalable, low-latency LLM services in resource-constrained edge\nenvironments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mobile edge Large Language Model (LLM) deployments face inherent constraints,\nsuch as limited computational resources and network bandwidth. Although\nRetrieval-Augmented Generation (RAG) mitigates some challenges by integrating\nexternal knowledge bases, inefficient cache management can still result in high\nretrieval latency and frequent cache updates. To address these issues, we\npropose an Adaptive Contextual Caching (ACC) framework that anticipates user\nneeds by proactively caching semantically relevant data for mobile-edge LLMs.\nACC utilizes a deep reinforcement learning (DRL) module to refine cache\nreplacement policies, balancing user context, document similarity, and the\noverhead associated with cache misses. Experimental results demonstrate that\nACC increases cache hit rates to over 80\\% after only 11 training episodes,\noutperforming FIFO, LRU, and semantic-only caching while reducing retrieval\nlatency by up to 40\\%. In particular, ACC also reduces local caching overhead\n(i.e., the cost of updating the cache when a miss occurs) by as much as 55\\%,\nenabling scalable, low-latency LLM services in resource-constrained edge\nenvironments."
                },
                "authors": [
                    {
                        "name": "Guangyuan Liu"
                    },
                    {
                        "name": "Yinqiu Liu"
                    },
                    {
                        "name": "Jiacheng Wang"
                    },
                    {
                        "name": "Hongyang Du"
                    },
                    {
                        "name": "Dusit Niyato"
                    },
                    {
                        "name": "Jiawen Kang"
                    },
                    {
                        "name": "Zehui Xiong"
                    }
                ],
                "author_detail": {
                    "name": "Zehui Xiong"
                },
                "author": "Zehui Xiong",
                "arxiv_comment": "8 pages, 5 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.09383v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.09383v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.09290v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.09290v1",
                "updated": "2025-01-16T04:50:15Z",
                "updated_parsed": [
                    2025,
                    1,
                    16,
                    4,
                    50,
                    15,
                    3,
                    16,
                    0
                ],
                "published": "2025-01-16T04:50:15Z",
                "published_parsed": [
                    2025,
                    1,
                    16,
                    4,
                    50,
                    15,
                    3,
                    16,
                    0
                ],
                "title": "Interoceptive Robots for Convergent Shared Control in Collaborative\n  Construction Work",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Interoceptive Robots for Convergent Shared Control in Collaborative\n  Construction Work"
                },
                "summary": "Building autonomous mobile robots (AMRs) with optimized efficiency and\nadaptive capabilities-able to respond to changing task demands and dynamic\nenvironments-is a strongly desired goal for advancing construction robotics.\nSuch robots can play a critical role in enabling automation, reducing\noperational carbon footprints, and supporting modular construction processes.\nInspired by the adaptive autonomy of living organisms, we introduce\ninteroception, which centers on the robot's internal state representation, as a\nfoundation for developing self-reflection and conscious learning to enable\ncontinual learning and adaptability in robotic agents. In this paper, we\nfactorize internal state variables and mathematical properties as \"cognitive\ndissonance\" in shared control paradigms, where human interventions occasionally\noccur. We offer a new perspective on how interoception can help build adaptive\nmotion planning in AMRs by integrating the legacy of heuristic costs from\ngrid/graph-based algorithms with recent advances in neuroscience and\nreinforcement learning. Declarative and procedural knowledge extracted from\nhuman semantic inputs is encoded into a hypergraph model that overlaps with the\nspatial configuration of onsite layout for path planning. In addition, we\ndesign a velocity-replay module using an encoder-decoder architecture with\nfew-shot learning to enable robots to replicate velocity profiles in\ncontextualized scenarios for multi-robot synchronization and handover\ncollaboration. These \"cached\" knowledge representations are demonstrated in\nsimulated environments for multi-robot motion planning and stacking tasks. The\ninsights from this study pave the way toward artificial general intelligence in\nAMRs, fostering their progression from complexity to competence in construction\nautomation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Building autonomous mobile robots (AMRs) with optimized efficiency and\nadaptive capabilities-able to respond to changing task demands and dynamic\nenvironments-is a strongly desired goal for advancing construction robotics.\nSuch robots can play a critical role in enabling automation, reducing\noperational carbon footprints, and supporting modular construction processes.\nInspired by the adaptive autonomy of living organisms, we introduce\ninteroception, which centers on the robot's internal state representation, as a\nfoundation for developing self-reflection and conscious learning to enable\ncontinual learning and adaptability in robotic agents. In this paper, we\nfactorize internal state variables and mathematical properties as \"cognitive\ndissonance\" in shared control paradigms, where human interventions occasionally\noccur. We offer a new perspective on how interoception can help build adaptive\nmotion planning in AMRs by integrating the legacy of heuristic costs from\ngrid/graph-based algorithms with recent advances in neuroscience and\nreinforcement learning. Declarative and procedural knowledge extracted from\nhuman semantic inputs is encoded into a hypergraph model that overlaps with the\nspatial configuration of onsite layout for path planning. In addition, we\ndesign a velocity-replay module using an encoder-decoder architecture with\nfew-shot learning to enable robots to replicate velocity profiles in\ncontextualized scenarios for multi-robot synchronization and handover\ncollaboration. These \"cached\" knowledge representations are demonstrated in\nsimulated environments for multi-robot motion planning and stacking tasks. The\ninsights from this study pave the way toward artificial general intelligence in\nAMRs, fostering their progression from complexity to competence in construction\nautomation."
                },
                "authors": [
                    {
                        "name": "Xiaoshan Zhou"
                    },
                    {
                        "name": "Carol C. Menassa"
                    },
                    {
                        "name": "Vineet R. Kamat"
                    }
                ],
                "author_detail": {
                    "name": "Vineet R. Kamat"
                },
                "author": "Vineet R. Kamat",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.09290v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.09290v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.09253v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.09253v1",
                "updated": "2025-01-16T02:40:07Z",
                "updated_parsed": [
                    2025,
                    1,
                    16,
                    2,
                    40,
                    7,
                    3,
                    16,
                    0
                ],
                "published": "2025-01-16T02:40:07Z",
                "published_parsed": [
                    2025,
                    1,
                    16,
                    2,
                    40,
                    7,
                    3,
                    16,
                    0
                ],
                "title": "PATCHEDSERVE: A Patch Management Framework for SLO-Optimized Hybrid\n  Resolution Diffusion Serving",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PATCHEDSERVE: A Patch Management Framework for SLO-Optimized Hybrid\n  Resolution Diffusion Serving"
                },
                "summary": "The Text-to-Image (T2I) diffusion model is one of the most popular models in\nthe world. However, serving diffusion models at the entire image level faces\nseveral problems, especially when there are multiple candidate resolutions.\nFirst, image based serving system prevents requests with different resolutions\nfrom batching together. On the other hand, requests with hybrid resolutions\nalso indicate diverse locality features, which makes it hard to apply the same\ncache policy to all of them. To this end, we propose PATCHEDSERVE, A Patch\nManagement Framework for SLO-Optimized Hybrid Resolution Diffusion Serving that\nprovides a patch-level management strategy to gather hybrid resolution requests\ninto batches. Specifically, PATCHEDSERVE incorporates a novel patch-based\nprocessing workflow, significantly enhancing throughput for hybrid resolution\ninputs. Furthermore, PATCHEDSERVE designs a patch-level cache reuse policy to\nfully exploit the redundancy in diffusion. In addition, PATCHEDSERVE features\nan SLO-aware scheduling algorithm with lightweight online latency prediction,\nachieving higher SLO satisfaction rates. We show that PATCHEDSERVE can achieve\n30.1 % higher SLO satisfaction compared to SOTA diffusion serving system while\nnot hurt the image quality.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Text-to-Image (T2I) diffusion model is one of the most popular models in\nthe world. However, serving diffusion models at the entire image level faces\nseveral problems, especially when there are multiple candidate resolutions.\nFirst, image based serving system prevents requests with different resolutions\nfrom batching together. On the other hand, requests with hybrid resolutions\nalso indicate diverse locality features, which makes it hard to apply the same\ncache policy to all of them. To this end, we propose PATCHEDSERVE, A Patch\nManagement Framework for SLO-Optimized Hybrid Resolution Diffusion Serving that\nprovides a patch-level management strategy to gather hybrid resolution requests\ninto batches. Specifically, PATCHEDSERVE incorporates a novel patch-based\nprocessing workflow, significantly enhancing throughput for hybrid resolution\ninputs. Furthermore, PATCHEDSERVE designs a patch-level cache reuse policy to\nfully exploit the redundancy in diffusion. In addition, PATCHEDSERVE features\nan SLO-aware scheduling algorithm with lightweight online latency prediction,\nachieving higher SLO satisfaction rates. We show that PATCHEDSERVE can achieve\n30.1 % higher SLO satisfaction compared to SOTA diffusion serving system while\nnot hurt the image quality."
                },
                "authors": [
                    {
                        "name": "Desen Sun"
                    },
                    {
                        "name": "Zepeng Zhao"
                    },
                    {
                        "name": "Yuke Wang"
                    }
                ],
                "author_detail": {
                    "name": "Yuke Wang"
                },
                "author": "Yuke Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.09253v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.09253v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.10845v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.10845v2",
                "updated": "2025-01-15T21:09:22Z",
                "updated_parsed": [
                    2025,
                    1,
                    15,
                    21,
                    9,
                    22,
                    2,
                    15,
                    0
                ],
                "published": "2024-04-16T18:47:07Z",
                "published_parsed": [
                    2024,
                    4,
                    16,
                    18,
                    47,
                    7,
                    1,
                    107,
                    0
                ],
                "title": "Top-k Multi-Armed Bandit Learning for Content Dissemination in Swarms of\n  Micro-UAVs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Top-k Multi-Armed Bandit Learning for Content Dissemination in Swarms of\n  Micro-UAVs"
                },
                "summary": "This paper presents a Micro-Unmanned Aerial Vehicle (UAV)-enhanced content\nmanagement system for disaster scenarios where communication infrastructure is\ngenerally compromised. Utilizing a hybrid network of stationary and mobile\nMicro-UAVs, this system aims to provide crucial content access to isolated\ncommunities. In the developed architecture, stationary anchor UAVs, equipped\nwith vertical and lateral links, serve users in individual disaster-affected\ncommunities. and mobile micro-ferrying UAVs, with enhanced mobility, extend\ncoverage across multiple such communities. The primary goal is to devise a\ncontent dissemination system that dynamically learns caching policies to\nmaximize content accessibility to users left without communication\ninfrastructure. The core contribution is an adaptive content dissemination\nframework that employs a decentralized Top-k Multi-Armed Bandit learning\napproach for efficient UAV caching decisions. This approach accounts for\ngeo-temporal variations in content popularity and diverse user demands.\nAdditionally, a Selective Caching Algorithm is proposed to minimize redundant\ncontent copies by leveraging inter-UAV information sharing. Through functional\nverification and performance evaluation, the proposed framework demonstrates\nimproved system performance and adaptability across varying network sizes,\nmicro-UAV swarms, and content popularity distributions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper presents a Micro-Unmanned Aerial Vehicle (UAV)-enhanced content\nmanagement system for disaster scenarios where communication infrastructure is\ngenerally compromised. Utilizing a hybrid network of stationary and mobile\nMicro-UAVs, this system aims to provide crucial content access to isolated\ncommunities. In the developed architecture, stationary anchor UAVs, equipped\nwith vertical and lateral links, serve users in individual disaster-affected\ncommunities. and mobile micro-ferrying UAVs, with enhanced mobility, extend\ncoverage across multiple such communities. The primary goal is to devise a\ncontent dissemination system that dynamically learns caching policies to\nmaximize content accessibility to users left without communication\ninfrastructure. The core contribution is an adaptive content dissemination\nframework that employs a decentralized Top-k Multi-Armed Bandit learning\napproach for efficient UAV caching decisions. This approach accounts for\ngeo-temporal variations in content popularity and diverse user demands.\nAdditionally, a Selective Caching Algorithm is proposed to minimize redundant\ncontent copies by leveraging inter-UAV information sharing. Through functional\nverification and performance evaluation, the proposed framework demonstrates\nimproved system performance and adaptability across varying network sizes,\nmicro-UAV swarms, and content popularity distributions."
                },
                "authors": [
                    {
                        "name": "Amit Kumar Bhuyan"
                    },
                    {
                        "name": "Hrishikesh Dutta"
                    },
                    {
                        "name": "Subir Biswas"
                    }
                ],
                "author_detail": {
                    "name": "Subir Biswas"
                },
                "author": "Subir Biswas",
                "arxiv_comment": "16 pages, 8 figures, 2 algorithms, 2 tables, journal",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.10845v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.10845v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.11",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.09146v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.09146v1",
                "updated": "2025-01-15T20:55:13Z",
                "updated_parsed": [
                    2025,
                    1,
                    15,
                    20,
                    55,
                    13,
                    2,
                    15,
                    0
                ],
                "published": "2025-01-15T20:55:13Z",
                "published_parsed": [
                    2025,
                    1,
                    15,
                    20,
                    55,
                    13,
                    2,
                    15,
                    0
                ],
                "title": "Towards Federated Multi-Armed Bandit Learning for Content Dissemination\n  using Swarm of UAVs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Towards Federated Multi-Armed Bandit Learning for Content Dissemination\n  using Swarm of UAVs"
                },
                "summary": "This paper introduces an Unmanned Aerial Vehicle - enabled content management\narchitecture that is suitable for critical content access in communities of\nusers that are communication-isolated during diverse types of disaster\nscenarios. The proposed architecture leverages a hybrid network of stationary\nanchor UAVs and mobile Micro-UAVs for ubiquitous content dissemination. The\nanchor UAVs are equipped with both vertical and lateral communication links,\nand they serve local users, while the mobile micro-ferrying UAVs extend\ncoverage across communities with increased mobility. The focus is on developing\na content dissemination system that dynamically learns optimal caching policies\nto maximize content availability. The core innovation is an adaptive content\ndissemination framework based on distributed Federated Multi-Armed Bandit\nlearning. The goal is to optimize UAV content caching decisions based on\ngeo-temporal content popularity and user demand variations. A Selective Caching\nAlgorithm is also introduced to reduce redundant content replication by\nincorporating inter-UAV information sharing. This method strategically\npreserves the uniqueness in user preferences while amalgamating the\nintelligence across a distributed learning system. This approach improves the\nlearning algorithm's ability to adapt to diverse user preferences. Functional\nverification and performance evaluation confirm the proposed architecture's\nutility across different network sizes, UAV swarms, and content popularity\npatterns.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper introduces an Unmanned Aerial Vehicle - enabled content management\narchitecture that is suitable for critical content access in communities of\nusers that are communication-isolated during diverse types of disaster\nscenarios. The proposed architecture leverages a hybrid network of stationary\nanchor UAVs and mobile Micro-UAVs for ubiquitous content dissemination. The\nanchor UAVs are equipped with both vertical and lateral communication links,\nand they serve local users, while the mobile micro-ferrying UAVs extend\ncoverage across communities with increased mobility. The focus is on developing\na content dissemination system that dynamically learns optimal caching policies\nto maximize content availability. The core innovation is an adaptive content\ndissemination framework based on distributed Federated Multi-Armed Bandit\nlearning. The goal is to optimize UAV content caching decisions based on\ngeo-temporal content popularity and user demand variations. A Selective Caching\nAlgorithm is also introduced to reduce redundant content replication by\nincorporating inter-UAV information sharing. This method strategically\npreserves the uniqueness in user preferences while amalgamating the\nintelligence across a distributed learning system. This approach improves the\nlearning algorithm's ability to adapt to diverse user preferences. Functional\nverification and performance evaluation confirm the proposed architecture's\nutility across different network sizes, UAV swarms, and content popularity\npatterns."
                },
                "authors": [
                    {
                        "name": "Amit Kumar Bhuyan"
                    },
                    {
                        "name": "Hrishikesh Dutta"
                    },
                    {
                        "name": "Subir Biswas"
                    }
                ],
                "author_detail": {
                    "name": "Subir Biswas"
                },
                "author": "Subir Biswas",
                "arxiv_comment": "25 pages, 11 figures, 1 table, 4 algorithms, journal",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.09146v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.09146v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.11",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.20166v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.20166v2",
                "updated": "2025-01-15T01:34:46Z",
                "updated_parsed": [
                    2025,
                    1,
                    15,
                    1,
                    34,
                    46,
                    2,
                    15,
                    0
                ],
                "published": "2024-12-28T14:38:16Z",
                "published_parsed": [
                    2024,
                    12,
                    28,
                    14,
                    38,
                    16,
                    5,
                    363,
                    0
                ],
                "title": "LoL-PIM: Long-Context LLM Decoding with Scalable DRAM-PIM System",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LoL-PIM: Long-Context LLM Decoding with Scalable DRAM-PIM System"
                },
                "summary": "The expansion of large language models (LLMs) with hundreds of billions of\nparameters presents significant challenges to computational resources,\nparticularly data movement and memory bandwidth. Long-context LLMs, which\nprocess sequences of tens of thousands of tokens, further increase the demand\non the memory system as the complexity in attention layers and key-value cache\nsizes is proportional to the context length. Processing-in-Memory (PIM)\nmaximizes memory bandwidth by moving compute to the data and can address the\nmemory bandwidth challenges; however, PIM is not necessarily scalable to\naccelerate long-context LLM because of limited per-module memory capacity and\nthe inflexibility of fixed-functional unit PIM architecture and static memory\nmanagement. In this work, we propose LoL-PIM which is a multi-node PIM\narchitecture that accelerates long context LLM through hardware-software\nco-design. In particular, we propose how pipeline parallelism can be exploited\nacross a multi-PIM module while a direct PIM access (DPA) controller (or DMA\nfor PIM) is proposed that enables dynamic PIM memory management and results in\nefficient PIM utilization across a diverse range of context length. We\ndeveloped an MLIR-based compiler for LoL-PIM extending a commercial PIM-based\ncompiler where the software modifications were implemented and evaluated, while\nthe hardware changes were modeled in the simulator. Our evaluations demonstrate\nthat LoL-PIM significantly improves throughput and reduces latency for\nlong-context LLM inference, outperforming both multi-GPU and GPU-PIM systems\n(up to 8.54x and 16.0x speedup, respectively), thereby enabling more efficient\ndeployment of LLMs in real-world applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The expansion of large language models (LLMs) with hundreds of billions of\nparameters presents significant challenges to computational resources,\nparticularly data movement and memory bandwidth. Long-context LLMs, which\nprocess sequences of tens of thousands of tokens, further increase the demand\non the memory system as the complexity in attention layers and key-value cache\nsizes is proportional to the context length. Processing-in-Memory (PIM)\nmaximizes memory bandwidth by moving compute to the data and can address the\nmemory bandwidth challenges; however, PIM is not necessarily scalable to\naccelerate long-context LLM because of limited per-module memory capacity and\nthe inflexibility of fixed-functional unit PIM architecture and static memory\nmanagement. In this work, we propose LoL-PIM which is a multi-node PIM\narchitecture that accelerates long context LLM through hardware-software\nco-design. In particular, we propose how pipeline parallelism can be exploited\nacross a multi-PIM module while a direct PIM access (DPA) controller (or DMA\nfor PIM) is proposed that enables dynamic PIM memory management and results in\nefficient PIM utilization across a diverse range of context length. We\ndeveloped an MLIR-based compiler for LoL-PIM extending a commercial PIM-based\ncompiler where the software modifications were implemented and evaluated, while\nthe hardware changes were modeled in the simulator. Our evaluations demonstrate\nthat LoL-PIM significantly improves throughput and reduces latency for\nlong-context LLM inference, outperforming both multi-GPU and GPU-PIM systems\n(up to 8.54x and 16.0x speedup, respectively), thereby enabling more efficient\ndeployment of LLMs in real-world applications."
                },
                "authors": [
                    {
                        "name": "Hyucksung Kwon"
                    },
                    {
                        "name": "Kyungmo Koo"
                    },
                    {
                        "name": "Janghyeon Kim"
                    },
                    {
                        "name": "Woongkyu Lee"
                    },
                    {
                        "name": "Minjae Lee"
                    },
                    {
                        "name": "Hyungdeok Lee"
                    },
                    {
                        "name": "Yousub Jung"
                    },
                    {
                        "name": "Jaehan Park"
                    },
                    {
                        "name": "Yosub Song"
                    },
                    {
                        "name": "Byeongsu Yang"
                    },
                    {
                        "name": "Haerang Choi"
                    },
                    {
                        "name": "Guhyun Kim"
                    },
                    {
                        "name": "Jongsoon Won"
                    },
                    {
                        "name": "Woojae Shin"
                    },
                    {
                        "name": "Changhyun Kim"
                    },
                    {
                        "name": "Gyeongcheol Shin"
                    },
                    {
                        "name": "Yongkee Kwon"
                    },
                    {
                        "name": "Ilkon Kim"
                    },
                    {
                        "name": "Euicheol Lim"
                    },
                    {
                        "name": "John Kim"
                    },
                    {
                        "name": "Jungwook Choi"
                    }
                ],
                "author_detail": {
                    "name": "Jungwook Choi"
                },
                "author": "Jungwook Choi",
                "arxiv_comment": "15 pages, 12 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.20166v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.20166v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.08484v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.08484v1",
                "updated": "2025-01-14T23:13:14Z",
                "updated_parsed": [
                    2025,
                    1,
                    14,
                    23,
                    13,
                    14,
                    1,
                    14,
                    0
                ],
                "published": "2025-01-14T23:13:14Z",
                "published_parsed": [
                    2025,
                    1,
                    14,
                    23,
                    13,
                    14,
                    1,
                    14,
                    0
                ],
                "title": "CORD: Co-design of Resource Allocation and Deadline Decomposition with\n  Generative Profiling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CORD: Co-design of Resource Allocation and Deadline Decomposition with\n  Generative Profiling"
                },
                "summary": "As multicore hardware is becoming increasingly common in real-time systems,\ntraditional scheduling techniques that assume a single worst-case execution\ntime for a task are no longer adequate, since they ignore the impact of shared\nresources on execution time. When tasks execute concurrently on different\ncores, their execution times often vary substantially with their allocated\nbudgets of shared resources, such as cache and memory bandwidth. Even under a\nspecific resource allocation, the resource use pattern of a task also changes\nwith time during a job execution. It is therefore important to consider the\nrelationship between multicore resources and execution time in task modeling\nand scheduling algorithm design.\n  In this paper, we propose a much more precise execution model for DAG-based\nreal-time tasks that captures the time-varying resource use characteristics of\na task under different budgets of shared resources. We present a generative\nresource profiling algorithm that efficiently predicts, from limited\nmeasurement data, the resource profile of a task at any time during its\nexecution under a given resource budget. The generative profiles can then be\nused to construct the execution models for tasks, using which one can make\ninformed resource allocation decisions. We further introduce a multicore\nresource allocation and deadline decomposition co-design technique for\nDAG-based tasks that leverages the generated execution models to jointly\nallocate resources and deadlines to subtasks, to maximize resource efficiency\nand schedulability. Our evaluation results show that our generative profiling\nalgorithm achieves high accuracy while being efficient, and that our\nco-allocation technique substantially improves schedulability compared to a\nstate-of-the-art deadline decomposition method.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As multicore hardware is becoming increasingly common in real-time systems,\ntraditional scheduling techniques that assume a single worst-case execution\ntime for a task are no longer adequate, since they ignore the impact of shared\nresources on execution time. When tasks execute concurrently on different\ncores, their execution times often vary substantially with their allocated\nbudgets of shared resources, such as cache and memory bandwidth. Even under a\nspecific resource allocation, the resource use pattern of a task also changes\nwith time during a job execution. It is therefore important to consider the\nrelationship between multicore resources and execution time in task modeling\nand scheduling algorithm design.\n  In this paper, we propose a much more precise execution model for DAG-based\nreal-time tasks that captures the time-varying resource use characteristics of\na task under different budgets of shared resources. We present a generative\nresource profiling algorithm that efficiently predicts, from limited\nmeasurement data, the resource profile of a task at any time during its\nexecution under a given resource budget. The generative profiles can then be\nused to construct the execution models for tasks, using which one can make\ninformed resource allocation decisions. We further introduce a multicore\nresource allocation and deadline decomposition co-design technique for\nDAG-based tasks that leverages the generated execution models to jointly\nallocate resources and deadlines to subtasks, to maximize resource efficiency\nand schedulability. Our evaluation results show that our generative profiling\nalgorithm achieves high accuracy while being efficient, and that our\nco-allocation technique substantially improves schedulability compared to a\nstate-of-the-art deadline decomposition method."
                },
                "authors": [
                    {
                        "name": "Robert Gifford"
                    },
                    {
                        "name": "Abby Eisenklam"
                    },
                    {
                        "name": "Georgiy A. Bondar"
                    },
                    {
                        "name": "Yifan Cai"
                    },
                    {
                        "name": "Tushar Sial"
                    },
                    {
                        "name": "Linh Thi Xuan Phan"
                    },
                    {
                        "name": "Abhishek Halder"
                    }
                ],
                "author_detail": {
                    "name": "Abhishek Halder"
                },
                "author": "Abhishek Halder",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.08484v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.08484v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.OS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.OS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.02380v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.02380v3",
                "updated": "2025-01-14T20:04:15Z",
                "updated_parsed": [
                    2025,
                    1,
                    14,
                    20,
                    4,
                    15,
                    1,
                    14,
                    0
                ],
                "published": "2025-01-04T20:59:34Z",
                "published_parsed": [
                    2025,
                    1,
                    4,
                    20,
                    59,
                    34,
                    5,
                    4,
                    0
                ],
                "title": "Reciprocating Locks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reciprocating Locks"
                },
                "summary": "We present \"Reciprocating Locks\", a novel mutual exclusion locking algorithm,\ntargeting cache-coherent shared memory (CC), that enjoys a number of desirable\nproperties. The doorway arrival phase and the release operation both run in\nconstant-time. Waiting threads use local spinning and only a single waiting\nelement is required per thread, regardless of the number of locks a thread\nmight hold at a given time. While our lock does not provide strict FIFO\nadmission, it bounds bypass and has strong anti-starvation properties. The lock\nis compact, space efficient, and has been intentionally designed to be readily\nusable in real-world general purpose computing environments such as the linux\nkernel, pthreads, or C++. We show the lock exhibits high throughput under\ncontention and low latency in the uncontended case. The performance of\nReciprocating Locks is competitive with and often better than the best\nstate-of-the-art scalable spin locks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present \"Reciprocating Locks\", a novel mutual exclusion locking algorithm,\ntargeting cache-coherent shared memory (CC), that enjoys a number of desirable\nproperties. The doorway arrival phase and the release operation both run in\nconstant-time. Waiting threads use local spinning and only a single waiting\nelement is required per thread, regardless of the number of locks a thread\nmight hold at a given time. While our lock does not provide strict FIFO\nadmission, it bounds bypass and has strong anti-starvation properties. The lock\nis compact, space efficient, and has been intentionally designed to be readily\nusable in real-world general purpose computing environments such as the linux\nkernel, pthreads, or C++. We show the lock exhibits high throughput under\ncontention and low latency in the uncontended case. The performance of\nReciprocating Locks is competitive with and often better than the best\nstate-of-the-art scalable spin locks."
                },
                "authors": [
                    {
                        "name": "Dave Dice"
                    },
                    {
                        "name": "Alex Kogan"
                    }
                ],
                "author_detail": {
                    "name": "Alex Kogan"
                },
                "author": "Alex Kogan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.02380v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.02380v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "D.4.1",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.08192v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.08192v1",
                "updated": "2025-01-14T15:14:10Z",
                "updated_parsed": [
                    2025,
                    1,
                    14,
                    15,
                    14,
                    10,
                    1,
                    14,
                    0
                ],
                "published": "2025-01-14T15:14:10Z",
                "published_parsed": [
                    2025,
                    1,
                    14,
                    15,
                    14,
                    10,
                    1,
                    14,
                    0
                ],
                "title": "PRESERVE: Prefetching Model Weights and KV-Cache in Distributed LLM\n  Serving",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PRESERVE: Prefetching Model Weights and KV-Cache in Distributed LLM\n  Serving"
                },
                "summary": "Large language models (LLMs) are widely used across various applications, but\ntheir substantial computational requirements pose significant challenges,\nparticularly in terms of HBM bandwidth bottlenecks and inter-device\ncommunication overhead. In this paper, we present PRESERVE, a novel prefetching\nframework designed to optimize LLM inference by overlapping memory reads for\nmodel weights and KV-cache with collective communication operations. Through\nextensive experiments conducted on commercial AI accelerators, we demonstrate\nup to 1.6x end-to-end speedup on state-of-the-art, open-source LLMs.\nAdditionally, we perform a design space exploration that identifies the optimal\nhardware configuration for the proposed method, showing a further 1.25x\nimprovement in performance per cost by selecting the optimal L2 cache size. Our\nresults show that PRESERVE has the potential to mitigate the memory bottlenecks\nand communication overheads, offering a solution to improve the performance and\nscalability of the LLM inference systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) are widely used across various applications, but\ntheir substantial computational requirements pose significant challenges,\nparticularly in terms of HBM bandwidth bottlenecks and inter-device\ncommunication overhead. In this paper, we present PRESERVE, a novel prefetching\nframework designed to optimize LLM inference by overlapping memory reads for\nmodel weights and KV-cache with collective communication operations. Through\nextensive experiments conducted on commercial AI accelerators, we demonstrate\nup to 1.6x end-to-end speedup on state-of-the-art, open-source LLMs.\nAdditionally, we perform a design space exploration that identifies the optimal\nhardware configuration for the proposed method, showing a further 1.25x\nimprovement in performance per cost by selecting the optimal L2 cache size. Our\nresults show that PRESERVE has the potential to mitigate the memory bottlenecks\nand communication overheads, offering a solution to improve the performance and\nscalability of the LLM inference systems."
                },
                "authors": [
                    {
                        "name": "Ahmet Caner Yüzügüler"
                    },
                    {
                        "name": "Jiawei Zhuang"
                    },
                    {
                        "name": "Lukas Cavigelli"
                    }
                ],
                "author_detail": {
                    "name": "Lukas Cavigelli"
                },
                "author": "Lukas Cavigelli",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.08192v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.08192v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.15102v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.15102v2",
                "updated": "2025-01-14T14:07:55Z",
                "updated_parsed": [
                    2025,
                    1,
                    14,
                    14,
                    7,
                    55,
                    1,
                    14,
                    0
                ],
                "published": "2024-11-22T18:06:14Z",
                "published_parsed": [
                    2024,
                    11,
                    22,
                    18,
                    6,
                    14,
                    4,
                    327,
                    0
                ],
                "title": "AttriBoT: A Bag of Tricks for Efficiently Approximating Leave-One-Out\n  Context Attribution",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AttriBoT: A Bag of Tricks for Efficiently Approximating Leave-One-Out\n  Context Attribution"
                },
                "summary": "The influence of contextual input on the behavior of large language models\n(LLMs) has prompted the development of context attribution methods that aim to\nquantify each context span's effect on an LLM's generations. The leave-one-out\n(LOO) error, which measures the change in the likelihood of the LLM's response\nwhen a given span of the context is removed, provides a principled way to\nperform context attribution, but can be prohibitively expensive to compute for\nlarge models. In this work, we introduce AttriBoT, a series of novel techniques\nfor efficiently computing an approximation of the LOO error for context\nattribution. Specifically, AttriBoT uses cached activations to avoid redundant\noperations, performs hierarchical attribution to reduce computation, and\nemulates the behavior of large target models with smaller proxy models. Taken\ntogether, AttriBoT can provide a >300x speedup while remaining more faithful to\na target model's LOO error than prior context attribution methods. This stark\nincrease in performance makes computing context attributions for a given\nresponse 30x faster than generating the response itself, empowering real-world\napplications that require computing attributions at scale. We release a\nuser-friendly and efficient implementation of AttriBoT to enable efficient LLM\ninterpretability as well as encourage future development of efficient context\nattribution methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The influence of contextual input on the behavior of large language models\n(LLMs) has prompted the development of context attribution methods that aim to\nquantify each context span's effect on an LLM's generations. The leave-one-out\n(LOO) error, which measures the change in the likelihood of the LLM's response\nwhen a given span of the context is removed, provides a principled way to\nperform context attribution, but can be prohibitively expensive to compute for\nlarge models. In this work, we introduce AttriBoT, a series of novel techniques\nfor efficiently computing an approximation of the LOO error for context\nattribution. Specifically, AttriBoT uses cached activations to avoid redundant\noperations, performs hierarchical attribution to reduce computation, and\nemulates the behavior of large target models with smaller proxy models. Taken\ntogether, AttriBoT can provide a >300x speedup while remaining more faithful to\na target model's LOO error than prior context attribution methods. This stark\nincrease in performance makes computing context attributions for a given\nresponse 30x faster than generating the response itself, empowering real-world\napplications that require computing attributions at scale. We release a\nuser-friendly and efficient implementation of AttriBoT to enable efficient LLM\ninterpretability as well as encourage future development of efficient context\nattribution methods."
                },
                "authors": [
                    {
                        "name": "Fengyuan Liu"
                    },
                    {
                        "name": "Nikhil Kandpal"
                    },
                    {
                        "name": "Colin Raffel"
                    }
                ],
                "author_detail": {
                    "name": "Colin Raffel"
                },
                "author": "Colin Raffel",
                "arxiv_comment": "29 pages, 11 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.15102v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.15102v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.04987v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.04987v2",
                "updated": "2025-01-14T12:06:33Z",
                "updated_parsed": [
                    2025,
                    1,
                    14,
                    12,
                    6,
                    33,
                    1,
                    14,
                    0
                ],
                "published": "2025-01-09T06:00:27Z",
                "published_parsed": [
                    2025,
                    1,
                    9,
                    6,
                    0,
                    27,
                    3,
                    9,
                    0
                ],
                "title": "TreeKV: Smooth Key-Value Cache Compression with Tree Structures",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TreeKV: Smooth Key-Value Cache Compression with Tree Structures"
                },
                "summary": "Efficient key-value (KV) cache compression is critical for scaling\ntransformer-based Large Language Models (LLMs) in long sequences and\nresource-limited settings. Existing methods evict tokens based on their\npositions or importance scores, but position-based strategies can miss crucial\ninformation outside predefined regions, while those relying on global\nimportance scores resulting in strong regional biases, limiting the KV cache's\noverall context retention and potentially impairing the performance of LLMs on\ncomplex tasks. Our wavelet analysis reveals that as tokens approach the end of\nsequence, their contributions to generation gradually increase and tends to\ndiverge more from neighboring tokens, indicating a smooth transition with\nincreasing complexity and variability from distant to nearby context. Motivated\nby this observation, we propose TreeKV, an intuitive, training-free method that\nemploys a tree structure for smooth cache compression. TreeKV maintains a fixed\ncache size, allowing LLMs to deliver high-quality output even in long text\nscenarios. Unlike most compression methods, TreeKV is applicable to both the\ngeneration and prefilling stages. TreeKV consistently surpasses all baseline\nmodels in language modeling tasks on PG19 and OpenWebText2, allowing LLMs\ntrained with short context window to generalize to longer window with a 16x\ncache reduction. On the Longbench benchmark, TreeKV achieves the best\nperformance with only 6\\% of the budget at optimal efficiency.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient key-value (KV) cache compression is critical for scaling\ntransformer-based Large Language Models (LLMs) in long sequences and\nresource-limited settings. Existing methods evict tokens based on their\npositions or importance scores, but position-based strategies can miss crucial\ninformation outside predefined regions, while those relying on global\nimportance scores resulting in strong regional biases, limiting the KV cache's\noverall context retention and potentially impairing the performance of LLMs on\ncomplex tasks. Our wavelet analysis reveals that as tokens approach the end of\nsequence, their contributions to generation gradually increase and tends to\ndiverge more from neighboring tokens, indicating a smooth transition with\nincreasing complexity and variability from distant to nearby context. Motivated\nby this observation, we propose TreeKV, an intuitive, training-free method that\nemploys a tree structure for smooth cache compression. TreeKV maintains a fixed\ncache size, allowing LLMs to deliver high-quality output even in long text\nscenarios. Unlike most compression methods, TreeKV is applicable to both the\ngeneration and prefilling stages. TreeKV consistently surpasses all baseline\nmodels in language modeling tasks on PG19 and OpenWebText2, allowing LLMs\ntrained with short context window to generalize to longer window with a 16x\ncache reduction. On the Longbench benchmark, TreeKV achieves the best\nperformance with only 6\\% of the budget at optimal efficiency."
                },
                "authors": [
                    {
                        "name": "Ziwei He"
                    },
                    {
                        "name": "Jian Yuan"
                    },
                    {
                        "name": "Haoli Bai"
                    },
                    {
                        "name": "Jingwen Leng"
                    },
                    {
                        "name": "Bo Jiang"
                    }
                ],
                "author_detail": {
                    "name": "Bo Jiang"
                },
                "author": "Bo Jiang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.04987v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.04987v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2403.15896v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2403.15896v2",
                "updated": "2025-01-14T11:41:14Z",
                "updated_parsed": [
                    2025,
                    1,
                    14,
                    11,
                    41,
                    14,
                    1,
                    14,
                    0
                ],
                "published": "2024-03-23T17:38:57Z",
                "published_parsed": [
                    2024,
                    3,
                    23,
                    17,
                    38,
                    57,
                    5,
                    83,
                    0
                ],
                "title": "Cell-level modelling of homeostasis in confined epithelial monolayers",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cell-level modelling of homeostasis in confined epithelial monolayers"
                },
                "summary": "Tissue homeostasis, the biological process of maintaining a steady state in\ntissue via control of cell proliferation, death, and metabolic function, is\nessential for the development, growth, maintenance, and proper function of\nliving organisms. Disruptions to this process can lead to serious diseases and\neven death. In this study, we use the vertex model for the cell-level\ndescription of tissue mechanics to investigate the impact of the tissue\nmicroenvironment and local mechanical properties of cells on homeostasis in\nconfined epithelial tissues. We find a dynamic steady state, where the balance\nbetween cell divisions and removals sustains homeostasis. By characterising\nhomeostasis in terms of cell count, tissue area, and the cells' neighbour count\ndistribution, we identify the factors that govern regulated and ordered tissue\ngrowth. This work, therefore, sheds light on the mechanisms underlying tissue\nhomeostasis and highlights the importance of mechanics in the control of\nbiological processes such as tissue development and disease pathology.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Tissue homeostasis, the biological process of maintaining a steady state in\ntissue via control of cell proliferation, death, and metabolic function, is\nessential for the development, growth, maintenance, and proper function of\nliving organisms. Disruptions to this process can lead to serious diseases and\neven death. In this study, we use the vertex model for the cell-level\ndescription of tissue mechanics to investigate the impact of the tissue\nmicroenvironment and local mechanical properties of cells on homeostasis in\nconfined epithelial tissues. We find a dynamic steady state, where the balance\nbetween cell divisions and removals sustains homeostasis. By characterising\nhomeostasis in terms of cell count, tissue area, and the cells' neighbour count\ndistribution, we identify the factors that govern regulated and ordered tissue\ngrowth. This work, therefore, sheds light on the mechanisms underlying tissue\nhomeostasis and highlights the importance of mechanics in the control of\nbiological processes such as tissue development and disease pathology."
                },
                "authors": [
                    {
                        "name": "KVS Chaithanya"
                    },
                    {
                        "name": "Jan Rozman"
                    },
                    {
                        "name": "Andrej Košmrlj"
                    },
                    {
                        "name": "Rastko Sknepnek"
                    }
                ],
                "author_detail": {
                    "name": "Rastko Sknepnek"
                },
                "author": "Rastko Sknepnek",
                "arxiv_comment": "18 pages, 13 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2403.15896v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2403.15896v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.bio-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.bio-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cond-mat.soft",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.19255v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.19255v2",
                "updated": "2025-01-14T05:48:07Z",
                "updated_parsed": [
                    2025,
                    1,
                    14,
                    5,
                    48,
                    7,
                    1,
                    14,
                    0
                ],
                "published": "2024-12-26T15:45:45Z",
                "published_parsed": [
                    2024,
                    12,
                    26,
                    15,
                    45,
                    45,
                    3,
                    361,
                    0
                ],
                "title": "Multi-matrix Factorization Attention",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-matrix Factorization Attention"
                },
                "summary": "We propose novel attention architectures, Multi-matrix Factorization\nAttention (MFA) and MFA-Key-Reuse (MFA-KR). Existing variants for standard\nMulti-Head Attention (MHA), including SOTA methods like MLA, fail to maintain\nas strong performance under stringent Key-Value cache (KV cache) constraints.\nMFA enhances model capacity by efficiently scaling up both the number and\ndimension of attention heads through low-rank matrix factorization in the\nQuery-Key (QK) circuit. Extending MFA, MFA-KR further reduces memory\nrequirements by repurposing the key cache as value through value projection\nre-parameterization. MFA's design enables strong model capacity when working\nunder tight KV cache budget, while MFA-KR is suitable for even harsher KV cache\nlimits with minor performance trade-off. Notably, in our extensive and\nlarge-scale experiments, the proposed architecture outperforms MLA and performs\ncomparably to MHA, while reducing KV cache usage by up to 56% and 93.7%,\nrespectively.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We propose novel attention architectures, Multi-matrix Factorization\nAttention (MFA) and MFA-Key-Reuse (MFA-KR). Existing variants for standard\nMulti-Head Attention (MHA), including SOTA methods like MLA, fail to maintain\nas strong performance under stringent Key-Value cache (KV cache) constraints.\nMFA enhances model capacity by efficiently scaling up both the number and\ndimension of attention heads through low-rank matrix factorization in the\nQuery-Key (QK) circuit. Extending MFA, MFA-KR further reduces memory\nrequirements by repurposing the key cache as value through value projection\nre-parameterization. MFA's design enables strong model capacity when working\nunder tight KV cache budget, while MFA-KR is suitable for even harsher KV cache\nlimits with minor performance trade-off. Notably, in our extensive and\nlarge-scale experiments, the proposed architecture outperforms MLA and performs\ncomparably to MHA, while reducing KV cache usage by up to 56% and 93.7%,\nrespectively."
                },
                "authors": [
                    {
                        "name": "Jingcheng Hu"
                    },
                    {
                        "name": "Houyi Li"
                    },
                    {
                        "name": "Yinmin Zhang"
                    },
                    {
                        "name": "Zili Wang"
                    },
                    {
                        "name": "Shuigeng Zhou"
                    },
                    {
                        "name": "Xiangyu Zhang"
                    },
                    {
                        "name": "Heung-Yeung Shum"
                    },
                    {
                        "name": "Daxin Jiang"
                    }
                ],
                "author_detail": {
                    "name": "Daxin Jiang"
                },
                "author": "Daxin Jiang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.19255v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.19255v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.10480v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.10480v2",
                "updated": "2025-01-14T05:00:34Z",
                "updated_parsed": [
                    2025,
                    1,
                    14,
                    5,
                    0,
                    34,
                    1,
                    14,
                    0
                ],
                "published": "2024-05-17T00:52:39Z",
                "published_parsed": [
                    2024,
                    5,
                    17,
                    0,
                    52,
                    39,
                    4,
                    138,
                    0
                ],
                "title": "Lean Attention: Hardware-Aware Scalable Attention Mechanism for the\n  Decode-Phase of Transformers",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Lean Attention: Hardware-Aware Scalable Attention Mechanism for the\n  Decode-Phase of Transformers"
                },
                "summary": "Transformer-based models have emerged as one of the most widely used\narchitectures for natural language processing, natural language generation, and\nimage generation. The size of the state-of-the-art models has increased\nsteadily reaching billions of parameters. These huge models are memory hungry\nand incur significant inference latency even on cutting edge AI-accelerators,\nsuch as GPUs. Specifically, the time and memory complexity of the attention\noperation is quadratic in terms of the total context length, i.e., prompt and\noutput tokens. Thus, several optimizations such as key-value tensor caching and\nFlashAttention computation have been proposed to deliver the low latency\ndemands of applications relying on such large models. However, these techniques\ndo not cater to the computationally distinct nature of different phases during\ninference.\n  To that end, we propose LeanAttention, a scalable technique of computing\nself-attention for the token-generation phase (decode-phase) of decoder-only\ntransformer models. LeanAttention enables scaling the attention mechanism\nimplementation for the challenging case of long context lengths by re-designing\nthe execution flow for the decode-phase. We identify that the associative\nproperty of online softmax can be treated as a reduction operation thus\nallowing us to parallelize the attention computation over these large context\nlengths. We extend the \"stream-K\" style reduction of tiled calculation to\nself-attention to enable parallel computation resulting in an average of 2.6x\nattention execution speedup over FlashAttention-2 and up to 8.33x speedup for\n512k context lengths.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Transformer-based models have emerged as one of the most widely used\narchitectures for natural language processing, natural language generation, and\nimage generation. The size of the state-of-the-art models has increased\nsteadily reaching billions of parameters. These huge models are memory hungry\nand incur significant inference latency even on cutting edge AI-accelerators,\nsuch as GPUs. Specifically, the time and memory complexity of the attention\noperation is quadratic in terms of the total context length, i.e., prompt and\noutput tokens. Thus, several optimizations such as key-value tensor caching and\nFlashAttention computation have been proposed to deliver the low latency\ndemands of applications relying on such large models. However, these techniques\ndo not cater to the computationally distinct nature of different phases during\ninference.\n  To that end, we propose LeanAttention, a scalable technique of computing\nself-attention for the token-generation phase (decode-phase) of decoder-only\ntransformer models. LeanAttention enables scaling the attention mechanism\nimplementation for the challenging case of long context lengths by re-designing\nthe execution flow for the decode-phase. We identify that the associative\nproperty of online softmax can be treated as a reduction operation thus\nallowing us to parallelize the attention computation over these large context\nlengths. We extend the \"stream-K\" style reduction of tiled calculation to\nself-attention to enable parallel computation resulting in an average of 2.6x\nattention execution speedup over FlashAttention-2 and up to 8.33x speedup for\n512k context lengths."
                },
                "authors": [
                    {
                        "name": "Rya Sanovar"
                    },
                    {
                        "name": "Srikant Bharadwaj"
                    },
                    {
                        "name": "Renee St. Amant"
                    },
                    {
                        "name": "Victor Rühle"
                    },
                    {
                        "name": "Saravan Rajmohan"
                    }
                ],
                "author_detail": {
                    "name": "Saravan Rajmohan"
                },
                "author": "Saravan Rajmohan",
                "arxiv_comment": "13 pages, 10 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.10480v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.10480v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.7; C.1.4",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.05262v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.05262v2",
                "updated": "2025-01-14T02:02:01Z",
                "updated_parsed": [
                    2025,
                    1,
                    14,
                    2,
                    2,
                    1,
                    1,
                    14,
                    0
                ],
                "published": "2025-01-09T14:16:43Z",
                "published_parsed": [
                    2025,
                    1,
                    9,
                    14,
                    16,
                    43,
                    3,
                    9,
                    0
                ],
                "title": "QMDB: Quick Merkle Database",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "QMDB: Quick Merkle Database"
                },
                "summary": "Quick Merkle Database (QMDB) addresses longstanding bottlenecks in blockchain\nstate management by integrating key-value (KV) and Merkle tree storage into a\nsingle unified architecture. QMDB delivers a significant throughput improvement\nover existing architectures, achieving up to 6X over the widely used RocksDB\nand 8X over NOMT, a leading verifiable database. Its novel append-only\ntwig-based design enables one SSD read per state access, O(1) IOs for updates,\nand in-memory Merkleization on a memory footprint as small as 2.3 bytes per\nentry, enabling it to run on even modest consumer-grade PCs. QMDB scales\nseamlessly across both commodity and enterprise hardware, achieving up to 2.28\nmillion state updates per second. This performance enables support for 1\nmillion token transfers per second (TPS), marking QMDB as the first solution\nachieving such a milestone. QMDB has been benchmarked with workloads exceeding\n15 billion entries (10X Ethereum's 2024 state) and has proven the capacity to\nscale to 280 billion entries on a single server. Furthermore, QMDB introduces\nhistorical proofs, unlocking the ability to query its blockchain's historical\nstate at the latest block. QMDB not only meets the demands of current\nblockchains but also provides a robust foundation for building scalable,\nefficient, and verifiable decentralized applications across diverse use cases.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Quick Merkle Database (QMDB) addresses longstanding bottlenecks in blockchain\nstate management by integrating key-value (KV) and Merkle tree storage into a\nsingle unified architecture. QMDB delivers a significant throughput improvement\nover existing architectures, achieving up to 6X over the widely used RocksDB\nand 8X over NOMT, a leading verifiable database. Its novel append-only\ntwig-based design enables one SSD read per state access, O(1) IOs for updates,\nand in-memory Merkleization on a memory footprint as small as 2.3 bytes per\nentry, enabling it to run on even modest consumer-grade PCs. QMDB scales\nseamlessly across both commodity and enterprise hardware, achieving up to 2.28\nmillion state updates per second. This performance enables support for 1\nmillion token transfers per second (TPS), marking QMDB as the first solution\nachieving such a milestone. QMDB has been benchmarked with workloads exceeding\n15 billion entries (10X Ethereum's 2024 state) and has proven the capacity to\nscale to 280 billion entries on a single server. Furthermore, QMDB introduces\nhistorical proofs, unlocking the ability to query its blockchain's historical\nstate at the latest block. QMDB not only meets the demands of current\nblockchains but also provides a robust foundation for building scalable,\nefficient, and verifiable decentralized applications across diverse use cases."
                },
                "authors": [
                    {
                        "name": "Isaac Zhang"
                    },
                    {
                        "name": "Ryan Zarick"
                    },
                    {
                        "name": "Daniel Wong"
                    },
                    {
                        "name": "Thomas Kim"
                    },
                    {
                        "name": "Bryan Pellegrino"
                    },
                    {
                        "name": "Mignon Li"
                    },
                    {
                        "name": "Kelvin Wong"
                    }
                ],
                "author_detail": {
                    "name": "Kelvin Wong"
                },
                "author": "Kelvin Wong",
                "arxiv_comment": "11 pages, 3 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.05262v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.05262v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.07752v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.07752v2",
                "updated": "2025-01-13T17:34:22Z",
                "updated_parsed": [
                    2025,
                    1,
                    13,
                    17,
                    34,
                    22,
                    0,
                    13,
                    0
                ],
                "published": "2024-12-10T18:50:37Z",
                "published_parsed": [
                    2024,
                    12,
                    10,
                    18,
                    50,
                    37,
                    1,
                    345,
                    0
                ],
                "title": "FlashRNN: Optimizing Traditional RNNs on Modern Hardware",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FlashRNN: Optimizing Traditional RNNs on Modern Hardware"
                },
                "summary": "While Transformers and other sequence-parallelizable neural network\narchitectures seem like the current state of the art in sequence modeling, they\nspecifically lack state-tracking capabilities. These are important for\ntime-series tasks and logical reasoning. Traditional RNNs like LSTMs and GRUs,\nas well as modern variants like sLSTM do have these capabilities at the cost of\nstrictly sequential processing. While this is often seen as a strong\nlimitation, we show how fast these networks can get with our\nhardware-optimization FlashRNN in Triton and CUDA, optimizing kernels to the\nregister level on modern GPUs. We extend traditional RNNs with a\nparallelization variant that processes multiple RNNs of smaller hidden state in\nparallel, similar to the head-wise processing in Transformers. To enable\nflexibility on different GPU variants, we introduce a new optimization\nframework for hardware-internal cache sizes, memory and compute handling. It\nmodels the hardware in a setting using polyhedral-like constraints, including\nthe notion of divisibility. This speeds up the solution process in our\nConstrINT library for general integer constraint satisfaction problems (integer\nCSPs). We show that our kernels can achieve 50x speed-ups over a vanilla\nPyTorch implementation and allow 40x larger hidden sizes compared to our Triton\nimplementation. Our open-source kernels and the optimization library are\nreleased here to boost research in the direction of state-tracking enabled RNNs\nand sequence modeling: \\url{https://github.com/NX-AI/flashrnn}",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While Transformers and other sequence-parallelizable neural network\narchitectures seem like the current state of the art in sequence modeling, they\nspecifically lack state-tracking capabilities. These are important for\ntime-series tasks and logical reasoning. Traditional RNNs like LSTMs and GRUs,\nas well as modern variants like sLSTM do have these capabilities at the cost of\nstrictly sequential processing. While this is often seen as a strong\nlimitation, we show how fast these networks can get with our\nhardware-optimization FlashRNN in Triton and CUDA, optimizing kernels to the\nregister level on modern GPUs. We extend traditional RNNs with a\nparallelization variant that processes multiple RNNs of smaller hidden state in\nparallel, similar to the head-wise processing in Transformers. To enable\nflexibility on different GPU variants, we introduce a new optimization\nframework for hardware-internal cache sizes, memory and compute handling. It\nmodels the hardware in a setting using polyhedral-like constraints, including\nthe notion of divisibility. This speeds up the solution process in our\nConstrINT library for general integer constraint satisfaction problems (integer\nCSPs). We show that our kernels can achieve 50x speed-ups over a vanilla\nPyTorch implementation and allow 40x larger hidden sizes compared to our Triton\nimplementation. Our open-source kernels and the optimization library are\nreleased here to boost research in the direction of state-tracking enabled RNNs\nand sequence modeling: \\url{https://github.com/NX-AI/flashrnn}"
                },
                "authors": [
                    {
                        "name": "Korbinian Pöppel"
                    },
                    {
                        "name": "Maximilian Beck"
                    },
                    {
                        "name": "Sepp Hochreiter"
                    }
                ],
                "author_detail": {
                    "name": "Sepp Hochreiter"
                },
                "author": "Sepp Hochreiter",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.07752v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.07752v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.07533v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.07533v4",
                "updated": "2025-01-13T09:33:25Z",
                "updated_parsed": [
                    2025,
                    1,
                    13,
                    9,
                    33,
                    25,
                    0,
                    13,
                    0
                ],
                "published": "2024-05-13T08:03:32Z",
                "published_parsed": [
                    2024,
                    5,
                    13,
                    8,
                    3,
                    32,
                    0,
                    134,
                    0
                ],
                "title": "DID Link: Authentication in TLS with Decentralized Identifiers and\n  Verifiable Credentials",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DID Link: Authentication in TLS with Decentralized Identifiers and\n  Verifiable Credentials"
                },
                "summary": "Authentication in TLS is predominately carried out with X.509 digital\ncertificates issued by certificate authorities (CA). The centralized nature of\ncurrent public key infrastructures, however, comes along with severe risks,\nsuch as single points of failure and susceptibility to cyber-attacks,\npotentially undermining the security and trustworthiness of the entire system.\nWith Decentralized Identifiers (DID) alongside distributed ledger technology,\nit becomes technically feasible to prove ownership of a unique identifier\nwithout requiring an attestation of the proof's public key by a centralized and\ntherefore vulnerable CA. This article presents DID Link, a novel authentication\nscheme for TLS 1.3 that empowers entities to authenticate in a TLS-compliant\nway with self-issued X.509 certificates that are equipped with ledger-anchored\nDIDs instead of CA-issued identifiers. It facilitates the exchange of\ntamper-proof and 3rd-party attested claims in the form of DID-bound Verifiable\nCredentials after the TLS handshake to complete the authentication with a full\nidentification of the communication partner. A prototypical implementation\nshows comparable TLS handshake durations of DID Link if verification material\nis cached and reasonable prolongations if it is obtained from a ledger. The\nsignificant speed improvement of the resulting TLS channel over a widely used,\nDID-based alternative transport protocol on the application layer demonstrates\nthe potential of DID Link to become a viable solution for the establishment of\nsecure and trustful end-to-end communication links with decentrally managed\ndigital identities.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Authentication in TLS is predominately carried out with X.509 digital\ncertificates issued by certificate authorities (CA). The centralized nature of\ncurrent public key infrastructures, however, comes along with severe risks,\nsuch as single points of failure and susceptibility to cyber-attacks,\npotentially undermining the security and trustworthiness of the entire system.\nWith Decentralized Identifiers (DID) alongside distributed ledger technology,\nit becomes technically feasible to prove ownership of a unique identifier\nwithout requiring an attestation of the proof's public key by a centralized and\ntherefore vulnerable CA. This article presents DID Link, a novel authentication\nscheme for TLS 1.3 that empowers entities to authenticate in a TLS-compliant\nway with self-issued X.509 certificates that are equipped with ledger-anchored\nDIDs instead of CA-issued identifiers. It facilitates the exchange of\ntamper-proof and 3rd-party attested claims in the form of DID-bound Verifiable\nCredentials after the TLS handshake to complete the authentication with a full\nidentification of the communication partner. A prototypical implementation\nshows comparable TLS handshake durations of DID Link if verification material\nis cached and reasonable prolongations if it is obtained from a ledger. The\nsignificant speed improvement of the resulting TLS channel over a widely used,\nDID-based alternative transport protocol on the application layer demonstrates\nthe potential of DID Link to become a viable solution for the establishment of\nsecure and trustful end-to-end communication links with decentrally managed\ndigital identities."
                },
                "authors": [
                    {
                        "name": "Sandro Rodriguez Garzon"
                    },
                    {
                        "name": "Dennis Natusch"
                    },
                    {
                        "name": "Artur Philipp"
                    },
                    {
                        "name": "Axel Küpper"
                    },
                    {
                        "name": "Hans Joachim Einsiedler"
                    },
                    {
                        "name": "Daniela Schneider"
                    }
                ],
                "author_detail": {
                    "name": "Daniela Schneider"
                },
                "author": "Daniela Schneider",
                "arxiv_doi": "10.1109/PST62714.2024.10788053",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1109/PST62714.2024.10788053",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2405.07533v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.07533v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "Accepted by and presented at 21st Annual International Conference on\n  Privacy, Security, and Trust (PST2024)",
                "arxiv_journal_ref": "2024 21st Annual International Conference on Privacy, Security and\n  Trust (PST), 2024, pp. 1-11",
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.07056v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.07056v1",
                "updated": "2025-01-13T04:31:04Z",
                "updated_parsed": [
                    2025,
                    1,
                    13,
                    4,
                    31,
                    4,
                    0,
                    13,
                    0
                ],
                "published": "2025-01-13T04:31:04Z",
                "published_parsed": [
                    2025,
                    1,
                    13,
                    4,
                    31,
                    4,
                    0,
                    13,
                    0
                ],
                "title": "Generating Data Locality to Accelerate Sparse Matrix-Matrix\n  Multiplication on CPUs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Generating Data Locality to Accelerate Sparse Matrix-Matrix\n  Multiplication on CPUs"
                },
                "summary": "Sparse GEneral Matrix-matrix Multiplication (SpGEMM) is a critical operation\nin many applications. Current multithreaded implementations are based on\nGustavson's algorithm and often perform poorly on large matrices due to limited\ncache reuse by the accumulators. We present MAGNUS (Matrix Algebra for Gigantic\nNUmerical Systems), a novel algorithm to maximize data locality in SpGEMM. To\ngenerate locality, MAGNUS reorders the intermediate product into discrete\ncache-friendly chunks using a two-level hierarchical approach. The accumulator\nis applied to each chunk, where the chunk size is chosen such that the\naccumulator is cache-efficient. MAGNUS is input- and system-aware: based on the\nmatrix characteristics and target system specifications, the optimal number of\nchunks is computed by minimizing the storage cost of the necessary data\nstructures. MAGNUS allows for a hybrid accumulation strategy in which each\nchunk uses a different accumulator based on an input threshold. We consider two\naccumulators: an AVX-512 vectorized bitonic sorting algorithm and classical\ndense accumulation. An OpenMP implementation of MAGNUS is compared with several\nbaselines for a variety of different matrices on three Intel x86 architectures.\nFor matrices from the SuiteSparse collection, MAGNUS is faster than all the\nbaselines in most cases and is orders of magnitude faster than Intel MKL for\nseveral matrices. For massive random matrices that model social network graphs,\nMAGNUS scales to the largest matrix sizes, while the baselines fail to do so.\nFurthermore, MAGNUS is close to the optimal bound for these matrices,\nregardless of the matrix size, structure, and density.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Sparse GEneral Matrix-matrix Multiplication (SpGEMM) is a critical operation\nin many applications. Current multithreaded implementations are based on\nGustavson's algorithm and often perform poorly on large matrices due to limited\ncache reuse by the accumulators. We present MAGNUS (Matrix Algebra for Gigantic\nNUmerical Systems), a novel algorithm to maximize data locality in SpGEMM. To\ngenerate locality, MAGNUS reorders the intermediate product into discrete\ncache-friendly chunks using a two-level hierarchical approach. The accumulator\nis applied to each chunk, where the chunk size is chosen such that the\naccumulator is cache-efficient. MAGNUS is input- and system-aware: based on the\nmatrix characteristics and target system specifications, the optimal number of\nchunks is computed by minimizing the storage cost of the necessary data\nstructures. MAGNUS allows for a hybrid accumulation strategy in which each\nchunk uses a different accumulator based on an input threshold. We consider two\naccumulators: an AVX-512 vectorized bitonic sorting algorithm and classical\ndense accumulation. An OpenMP implementation of MAGNUS is compared with several\nbaselines for a variety of different matrices on three Intel x86 architectures.\nFor matrices from the SuiteSparse collection, MAGNUS is faster than all the\nbaselines in most cases and is orders of magnitude faster than Intel MKL for\nseveral matrices. For massive random matrices that model social network graphs,\nMAGNUS scales to the largest matrix sizes, while the baselines fail to do so.\nFurthermore, MAGNUS is close to the optimal bound for these matrices,\nregardless of the matrix size, structure, and density."
                },
                "authors": [
                    {
                        "name": "Jordi Wolfson-Pou"
                    },
                    {
                        "name": "Jan Laukemann"
                    },
                    {
                        "name": "Fabrizio Petrini"
                    }
                ],
                "author_detail": {
                    "name": "Fabrizio Petrini"
                },
                "author": "Fabrizio Petrini",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.07056v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.07056v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.03058v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.03058v5",
                "updated": "2025-01-13T03:11:28Z",
                "updated_parsed": [
                    2025,
                    1,
                    13,
                    3,
                    11,
                    28,
                    0,
                    13,
                    0
                ],
                "published": "2024-05-05T21:41:43Z",
                "published_parsed": [
                    2024,
                    5,
                    5,
                    21,
                    41,
                    43,
                    6,
                    126,
                    0
                ],
                "title": "A Unified Framework for Automated Code Transformation and Pragma\n  Insertion",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Unified Framework for Automated Code Transformation and Pragma\n  Insertion"
                },
                "summary": "High-level synthesis, source-to-source compilers, and various Design Space\nExploration techniques for pragma insertion have significantly improved the\nQuality of Results of generated designs. These tools offer benefits such as\nreduced development time and enhanced performance. However, achieving\nhigh-quality results often requires additional manual code transformations and\ntiling selections, which are typically performed separately or as\npre-processing steps. Although DSE techniques enable code transformation\nupfront, the vastness of the search space often limits the exploration of all\npossible code transformations, making it challenging to determine which\ntransformations are necessary. Additionally, ensuring correctness remains\nchallenging, especially for complex transformations and optimizations.\n  To tackle this obstacle, we first propose a comprehensive framework\nleveraging HLS compilers. Our system streamlines code transformation, pragma\ninsertion, and tiles size selection for on-chip data caching through a unified\noptimization problem, aiming to enhance parallelization, particularly\nbeneficial for computation-bound kernels. Them employing a novel Non-Linear\nProgramming (NLP) approach, we simultaneously ascertain transformations,\npragmas, and tile sizes, focusing on regular loop-based kernels. Our evaluation\ndemonstrates that our framework adeptly identifies the appropriate\ntransformations, including scenarios where no transformation is necessary, and\ninserts pragmas to achieve a favorable Quality of Results.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "High-level synthesis, source-to-source compilers, and various Design Space\nExploration techniques for pragma insertion have significantly improved the\nQuality of Results of generated designs. These tools offer benefits such as\nreduced development time and enhanced performance. However, achieving\nhigh-quality results often requires additional manual code transformations and\ntiling selections, which are typically performed separately or as\npre-processing steps. Although DSE techniques enable code transformation\nupfront, the vastness of the search space often limits the exploration of all\npossible code transformations, making it challenging to determine which\ntransformations are necessary. Additionally, ensuring correctness remains\nchallenging, especially for complex transformations and optimizations.\n  To tackle this obstacle, we first propose a comprehensive framework\nleveraging HLS compilers. Our system streamlines code transformation, pragma\ninsertion, and tiles size selection for on-chip data caching through a unified\noptimization problem, aiming to enhance parallelization, particularly\nbeneficial for computation-bound kernels. Them employing a novel Non-Linear\nProgramming (NLP) approach, we simultaneously ascertain transformations,\npragmas, and tile sizes, focusing on regular loop-based kernels. Our evaluation\ndemonstrates that our framework adeptly identifies the appropriate\ntransformations, including scenarios where no transformation is necessary, and\ninserts pragmas to achieve a favorable Quality of Results."
                },
                "authors": [
                    {
                        "name": "Stéphane Pouget"
                    },
                    {
                        "name": "Louis-Noël Pouchet"
                    },
                    {
                        "name": "Jason Cong"
                    }
                ],
                "author_detail": {
                    "name": "Jason Cong"
                },
                "author": "Jason Cong",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.03058v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.03058v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.06872v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.06872v1",
                "updated": "2025-01-12T17:01:40Z",
                "updated_parsed": [
                    2025,
                    1,
                    12,
                    17,
                    1,
                    40,
                    6,
                    12,
                    0
                ],
                "published": "2025-01-12T17:01:40Z",
                "published_parsed": [
                    2025,
                    1,
                    12,
                    17,
                    1,
                    40,
                    6,
                    12,
                    0
                ],
                "title": "On Optimizing Locality of Graph Transposition on Modern Architectures",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "On Optimizing Locality of Graph Transposition on Modern Architectures"
                },
                "summary": "This paper investigates the shared-memory Graph Transposition (GT) problem, a\nfundamental graph algorithm that is widely used in graph analytics and\nscientific computing.\n  Previous GT algorithms have significant memory requirements that are\nproportional to the number of vertices and threads which obstructs their use on\nlarge graphs. Moreover, atomic memory operations have become comparably fast on\nrecent CPU architectures, which creates new opportunities for improving the\nperformance of concurrent atomic accesses in GT.\n  We design PoTra, a GT algorithm which leverages graph structure and processor\nand memory architecture to optimize locality and performance. PoTra limits the\nsize of additional data structures close to CPU cache sizes and utilizes the\nskewed degree distribution of graph datasets to optimize locality and\nperformance. We present the performance model of PoTra to explain the\nconnection between cache and memory response times and graph locality.\n  Our evaluation of PoTra on three CPU architectures and 20 real-world and\nsynthetic graph datasets with up to 128 billion edges demonstrates that PoTra\nachieves up to 8.7 times speedup compared to previous works and if there is a\nperformance loss it remains limited to 15.7%, on average.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper investigates the shared-memory Graph Transposition (GT) problem, a\nfundamental graph algorithm that is widely used in graph analytics and\nscientific computing.\n  Previous GT algorithms have significant memory requirements that are\nproportional to the number of vertices and threads which obstructs their use on\nlarge graphs. Moreover, atomic memory operations have become comparably fast on\nrecent CPU architectures, which creates new opportunities for improving the\nperformance of concurrent atomic accesses in GT.\n  We design PoTra, a GT algorithm which leverages graph structure and processor\nand memory architecture to optimize locality and performance. PoTra limits the\nsize of additional data structures close to CPU cache sizes and utilizes the\nskewed degree distribution of graph datasets to optimize locality and\nperformance. We present the performance model of PoTra to explain the\nconnection between cache and memory response times and graph locality.\n  Our evaluation of PoTra on three CPU architectures and 20 real-world and\nsynthetic graph datasets with up to 128 billion edges demonstrates that PoTra\nachieves up to 8.7 times speedup compared to previous works and if there is a\nperformance loss it remains limited to 15.7%, on average."
                },
                "authors": [
                    {
                        "name": "Mohsen Koohi Esfahani"
                    },
                    {
                        "name": "Hans Vandierendonck"
                    }
                ],
                "author_detail": {
                    "name": "Hans Vandierendonck"
                },
                "author": "Hans Vandierendonck",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.06872v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.06872v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.06807v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.06807v1",
                "updated": "2025-01-12T13:18:04Z",
                "updated_parsed": [
                    2025,
                    1,
                    12,
                    13,
                    18,
                    4,
                    6,
                    12,
                    0
                ],
                "published": "2025-01-12T13:18:04Z",
                "published_parsed": [
                    2025,
                    1,
                    12,
                    13,
                    18,
                    4,
                    6,
                    12,
                    0
                ],
                "title": "MPCache: MPC-Friendly KV Cache Eviction for Efficient Private Large\n  Language Model Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MPCache: MPC-Friendly KV Cache Eviction for Efficient Private Large\n  Language Model Inference"
                },
                "summary": "Private large language model (LLM) inference based on secure multi-party\ncomputation (MPC) offers cryptographically-secure protection for both user\nprompt and proprietary model weights. However, it suffers from large latency\noverhead especially for long input sequences. While key-value (KV) cache\neviction algorithms have been proposed to reduce the computation and memory\ncost for plaintext inference, they are not designed for MPC and cannot benefit\nprivate inference easily. In this paper, we propose an accurate and\nMPC-friendly KV cache eviction framework, dubbed MPCache. MPCache is built on\nthe observation that historical tokens in a long sequence may have different\neffects on the downstream decoding. Hence, MPCache combines a look-once static\neviction algorithm to discard unimportant tokens and a query-aware dynamic\nselection algorithm to further select a small subset of tokens for attention\ncomputation. As existing dynamic selection algorithms incur too much latency,\nwe propose a series of optimizations to drastically reduce the KV cache\nselection overhead, including MPC-friendly similarity approximation,\nhierarchical KV cache clustering, and cross-layer index sharing strategy. With\nextensive experiments, we demonstrate that MPCache consistently outperforms\nprior-art KV cache eviction baselines across different LLM generation tasks and\nachieves 1.8~2.01x and 3.39~8.37x decoding latency and communication reduction\non different sequence lengths, respectively.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Private large language model (LLM) inference based on secure multi-party\ncomputation (MPC) offers cryptographically-secure protection for both user\nprompt and proprietary model weights. However, it suffers from large latency\noverhead especially for long input sequences. While key-value (KV) cache\neviction algorithms have been proposed to reduce the computation and memory\ncost for plaintext inference, they are not designed for MPC and cannot benefit\nprivate inference easily. In this paper, we propose an accurate and\nMPC-friendly KV cache eviction framework, dubbed MPCache. MPCache is built on\nthe observation that historical tokens in a long sequence may have different\neffects on the downstream decoding. Hence, MPCache combines a look-once static\neviction algorithm to discard unimportant tokens and a query-aware dynamic\nselection algorithm to further select a small subset of tokens for attention\ncomputation. As existing dynamic selection algorithms incur too much latency,\nwe propose a series of optimizations to drastically reduce the KV cache\nselection overhead, including MPC-friendly similarity approximation,\nhierarchical KV cache clustering, and cross-layer index sharing strategy. With\nextensive experiments, we demonstrate that MPCache consistently outperforms\nprior-art KV cache eviction baselines across different LLM generation tasks and\nachieves 1.8~2.01x and 3.39~8.37x decoding latency and communication reduction\non different sequence lengths, respectively."
                },
                "authors": [
                    {
                        "name": "Wenxuan Zeng"
                    },
                    {
                        "name": "Ye Dong"
                    },
                    {
                        "name": "Jinjin Zhou"
                    },
                    {
                        "name": "Junming Ma"
                    },
                    {
                        "name": "Jin Tan"
                    },
                    {
                        "name": "Runsheng Wang"
                    },
                    {
                        "name": "Meng Li"
                    }
                ],
                "author_detail": {
                    "name": "Meng Li"
                },
                "author": "Meng Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.06807v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.06807v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.02882v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.02882v2",
                "updated": "2025-01-12T12:01:47Z",
                "updated_parsed": [
                    2025,
                    1,
                    12,
                    12,
                    1,
                    47,
                    6,
                    12,
                    0
                ],
                "published": "2024-04-03T17:33:21Z",
                "published_parsed": [
                    2024,
                    4,
                    3,
                    17,
                    33,
                    21,
                    2,
                    94,
                    0
                ],
                "title": "Linear Attention Sequence Parallelism",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Linear Attention Sequence Parallelism"
                },
                "summary": "Sequence parallelism (SP) serves as a prevalent strategy to handle long\nsequences that exceed the memory limit of a single device. However, for linear\nsequence modeling methods like linear attention, existing SP approaches do not\ntake advantage of their right-product-first feature, resulting in sub-optimal\ncommunication efficiency and usability. In this paper, we introduce Linear\nAttention Sequence Parallelism (LASP), an efficient SP approach designed for\nlinear attention-based transformer models. Specifically, we design an efficient\npoint-to-point ring-style communication mechanism to leverage the right-product\nkernel trick of linear attention, which sharply decreases the communication\noverhead, comparing with existing SP methods. We enhance the computation\nefficiency of LASP by performing kernel fusion and intermediate state caching,\nmaking the implementation of LASP hardware-friendly on GPUs. Furthermore, we\nmeticulously ensure the compatibility of sequence-level LASP with all types of\nbatch-level data parallel methods, which is vital for distributed training on\nlarge clusters with very-long sequences. We also discuss the generalization of\nLASP on other linear sequence modeling methods. Extensive experiments on linear\nattention-based models are conducted with varying sequence lengths from 2K to\n4096K. LASP scales sequence length up to 4096K on 128 GPUs, which is 8$\\times$\nlonger than existing SP methods. The code is available at\nhttps://github.com/OpenNLPLab/LASP.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Sequence parallelism (SP) serves as a prevalent strategy to handle long\nsequences that exceed the memory limit of a single device. However, for linear\nsequence modeling methods like linear attention, existing SP approaches do not\ntake advantage of their right-product-first feature, resulting in sub-optimal\ncommunication efficiency and usability. In this paper, we introduce Linear\nAttention Sequence Parallelism (LASP), an efficient SP approach designed for\nlinear attention-based transformer models. Specifically, we design an efficient\npoint-to-point ring-style communication mechanism to leverage the right-product\nkernel trick of linear attention, which sharply decreases the communication\noverhead, comparing with existing SP methods. We enhance the computation\nefficiency of LASP by performing kernel fusion and intermediate state caching,\nmaking the implementation of LASP hardware-friendly on GPUs. Furthermore, we\nmeticulously ensure the compatibility of sequence-level LASP with all types of\nbatch-level data parallel methods, which is vital for distributed training on\nlarge clusters with very-long sequences. We also discuss the generalization of\nLASP on other linear sequence modeling methods. Extensive experiments on linear\nattention-based models are conducted with varying sequence lengths from 2K to\n4096K. LASP scales sequence length up to 4096K on 128 GPUs, which is 8$\\times$\nlonger than existing SP methods. The code is available at\nhttps://github.com/OpenNLPLab/LASP."
                },
                "authors": [
                    {
                        "name": "Weigao Sun"
                    },
                    {
                        "name": "Zhen Qin"
                    },
                    {
                        "name": "Dong Li"
                    },
                    {
                        "name": "Xuyang Shen"
                    },
                    {
                        "name": "Yu Qiao"
                    },
                    {
                        "name": "Yiran Zhong"
                    }
                ],
                "author_detail": {
                    "name": "Yiran Zhong"
                },
                "author": "Yiran Zhong",
                "arxiv_comment": "Technical report, 20 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.02882v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.02882v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.07196v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.07196v3",
                "updated": "2025-01-12T11:15:41Z",
                "updated_parsed": [
                    2025,
                    1,
                    12,
                    11,
                    15,
                    41,
                    6,
                    12,
                    0
                ],
                "published": "2024-09-11T11:40:23Z",
                "published_parsed": [
                    2024,
                    9,
                    11,
                    11,
                    40,
                    23,
                    2,
                    255,
                    0
                ],
                "title": "Sub-cycle Nanotip Field Emission of Electrons Driven by Air Plasma\n  Generated THz Pulses",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Sub-cycle Nanotip Field Emission of Electrons Driven by Air Plasma\n  Generated THz Pulses"
                },
                "summary": "Terahertz pulses generated by two-color laser plasmas have reported peak\nfield strengths exceeding MV/cm, and when illuminating metal nanotips the\nnear-field enhancement at the tip apex should result in extremely high bunch\ncharges and electron energies via sub-cycle cold field emission. Here, electron\nemission from tungsten nanotips driven by THz pulses generated by a long\nfilament air-plasma are reported. Electron energies up to 1.1 keV and bunch\ncharges up to 2x$10^5$ electrons per pulse were detected, well below values\nexpected for peak field calculated via the time averaged Poynting vector.\nInvestigations revealed a failure in the use of the time-averaged Poynting\nvector when applied to long filament THz pulses, due to spatio-temporal\nrestructuring of the THz pulse in the focus. Accounting for this restructuring\nsignificantly reduces the field strength to approximately 160 ~kV/cm,\nconsistent with the observed electron bunch charges, peak energies and their\ndependence on the tip position in the THz focus. Despite these findings, our\nresults surpass previous THz plasma-driven electron generation by an order of\nmagnitude in both electron energy and bunch charge and a path to increasing\nthese by an additional order of magnitude by modification of the THz optics is\nproposed.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Terahertz pulses generated by two-color laser plasmas have reported peak\nfield strengths exceeding MV/cm, and when illuminating metal nanotips the\nnear-field enhancement at the tip apex should result in extremely high bunch\ncharges and electron energies via sub-cycle cold field emission. Here, electron\nemission from tungsten nanotips driven by THz pulses generated by a long\nfilament air-plasma are reported. Electron energies up to 1.1 keV and bunch\ncharges up to 2x$10^5$ electrons per pulse were detected, well below values\nexpected for peak field calculated via the time averaged Poynting vector.\nInvestigations revealed a failure in the use of the time-averaged Poynting\nvector when applied to long filament THz pulses, due to spatio-temporal\nrestructuring of the THz pulse in the focus. Accounting for this restructuring\nsignificantly reduces the field strength to approximately 160 ~kV/cm,\nconsistent with the observed electron bunch charges, peak energies and their\ndependence on the tip position in the THz focus. Despite these findings, our\nresults surpass previous THz plasma-driven electron generation by an order of\nmagnitude in both electron energy and bunch charge and a path to increasing\nthese by an additional order of magnitude by modification of the THz optics is\nproposed."
                },
                "authors": [
                    {
                        "name": "Benjamin Colmey"
                    },
                    {
                        "name": "Rodrigo T. Paulino"
                    },
                    {
                        "name": "Gaspard Beaufort"
                    },
                    {
                        "name": "David G. Cooke"
                    }
                ],
                "author_detail": {
                    "name": "David G. Cooke"
                },
                "author": "David G. Cooke",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.07196v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.07196v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cond-mat.mtrl-sci",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cond-mat.mtrl-sci",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "physics.plasm-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.00857v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.00857v2",
                "updated": "2025-01-12T05:25:06Z",
                "updated_parsed": [
                    2025,
                    1,
                    12,
                    5,
                    25,
                    6,
                    6,
                    12,
                    0
                ],
                "published": "2024-12-01T15:45:26Z",
                "published_parsed": [
                    2024,
                    12,
                    1,
                    15,
                    45,
                    26,
                    6,
                    336,
                    0
                ],
                "title": "Advanced Video Inpainting Using Optical Flow-Guided Efficient Diffusion",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Advanced Video Inpainting Using Optical Flow-Guided Efficient Diffusion"
                },
                "summary": "Recently, diffusion-based methods have achieved great improvements in the\nvideo inpainting task. However, these methods still face many challenges, such\nas maintaining temporal consistency and the time-consuming issue. This paper\nproposes an advanced video inpainting framework using optical Flow-guided\nEfficient Diffusion, called FloED. Specifically, FloED employs a dual-branch\narchitecture, where a flow branch first restores corrupted flow and a\nmulti-scale flow adapter provides motion guidance to the main inpainting\nbranch. Additionally, a training-free latent interpolation method is proposed\nto accelerate the multi-step denoising process using flow warping. Further\nintroducing a flow attention cache mechanism, FLoED efficiently reduces the\ncomputational cost brought by incorporating optical flow. Comprehensive\nexperiments in both background restoration and object removal tasks demonstrate\nthat FloED outperforms state-of-the-art methods from the perspective of both\nperformance and efficiency.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recently, diffusion-based methods have achieved great improvements in the\nvideo inpainting task. However, these methods still face many challenges, such\nas maintaining temporal consistency and the time-consuming issue. This paper\nproposes an advanced video inpainting framework using optical Flow-guided\nEfficient Diffusion, called FloED. Specifically, FloED employs a dual-branch\narchitecture, where a flow branch first restores corrupted flow and a\nmulti-scale flow adapter provides motion guidance to the main inpainting\nbranch. Additionally, a training-free latent interpolation method is proposed\nto accelerate the multi-step denoising process using flow warping. Further\nintroducing a flow attention cache mechanism, FLoED efficiently reduces the\ncomputational cost brought by incorporating optical flow. Comprehensive\nexperiments in both background restoration and object removal tasks demonstrate\nthat FloED outperforms state-of-the-art methods from the perspective of both\nperformance and efficiency."
                },
                "authors": [
                    {
                        "name": "Bohai Gu"
                    },
                    {
                        "name": "Hao Luo"
                    },
                    {
                        "name": "Song Guo"
                    },
                    {
                        "name": "Peiran Dong"
                    }
                ],
                "author_detail": {
                    "name": "Peiran Dong"
                },
                "author": "Peiran Dong",
                "arxiv_comment": "Project page: https://nevsnev.github.io/FloED/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.00857v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.00857v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.06709v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.06709v1",
                "updated": "2025-01-12T04:29:39Z",
                "updated_parsed": [
                    2025,
                    1,
                    12,
                    4,
                    29,
                    39,
                    6,
                    12,
                    0
                ],
                "published": "2025-01-12T04:29:39Z",
                "published_parsed": [
                    2025,
                    1,
                    12,
                    4,
                    29,
                    39,
                    6,
                    12,
                    0
                ],
                "title": "Mell: Memory-Efficient Large Language Model Serving via Multi-GPU KV\n  Cache Management",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mell: Memory-Efficient Large Language Model Serving via Multi-GPU KV\n  Cache Management"
                },
                "summary": "Serving large language models (LLMs) for massive users is challenged by the\nsignificant memory footprint of the transient state, known as the key-value\n(KV) cache, which scales with sequence length and number of requests. Instead\nof renting or buying more expensive GPUs, the load imbalance of the KV cache\nacross GPUs, coupled with recent advances in inter-GPU communication, provides\nan opportunity to serve more requests via request migration. However, high\nmigration overhead and unpredictable request patterns make it challenging.\nTherefore, this paper proposes MELL, a memory-efficient LLM serving system via\nmulti-GPU KV cache management. It saves the number of GPUs needed in the system\nby considering the dynamic KV cache load and the costly request migration.\nSpecifically, we first develop an adaptive request migration mechanism to\nbalance the computational and communication overheads and adapt to diverse\nresource conditions. Then, we design an online algorithm tailored to a\nmulti-LLM request and multi-GPU scheduling problem with migration enabled. It\naims to minimise the required GPUs while limiting the number of migrations.\nFinally, we implement a prototype of MELL and demonstrate that it reduces the\nnumber of GPUs by 31% and increases the GPU utilization by 43% at most compared\nto existing LLM serving systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Serving large language models (LLMs) for massive users is challenged by the\nsignificant memory footprint of the transient state, known as the key-value\n(KV) cache, which scales with sequence length and number of requests. Instead\nof renting or buying more expensive GPUs, the load imbalance of the KV cache\nacross GPUs, coupled with recent advances in inter-GPU communication, provides\nan opportunity to serve more requests via request migration. However, high\nmigration overhead and unpredictable request patterns make it challenging.\nTherefore, this paper proposes MELL, a memory-efficient LLM serving system via\nmulti-GPU KV cache management. It saves the number of GPUs needed in the system\nby considering the dynamic KV cache load and the costly request migration.\nSpecifically, we first develop an adaptive request migration mechanism to\nbalance the computational and communication overheads and adapt to diverse\nresource conditions. Then, we design an online algorithm tailored to a\nmulti-LLM request and multi-GPU scheduling problem with migration enabled. It\naims to minimise the required GPUs while limiting the number of migrations.\nFinally, we implement a prototype of MELL and demonstrate that it reduces the\nnumber of GPUs by 31% and increases the GPU utilization by 43% at most compared\nto existing LLM serving systems."
                },
                "authors": [
                    {
                        "name": "Liu Qianli"
                    },
                    {
                        "name": "Hong Zicong"
                    },
                    {
                        "name": "Chen Fahao"
                    },
                    {
                        "name": "Li Peng"
                    },
                    {
                        "name": "Guo Song"
                    }
                ],
                "author_detail": {
                    "name": "Guo Song"
                },
                "author": "Guo Song",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.06709v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.06709v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.17918v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.17918v4",
                "updated": "2025-01-11T15:26:48Z",
                "updated_parsed": [
                    2025,
                    1,
                    11,
                    15,
                    26,
                    48,
                    5,
                    11,
                    0
                ],
                "published": "2024-06-25T20:00:32Z",
                "published_parsed": [
                    2024,
                    6,
                    25,
                    20,
                    0,
                    32,
                    1,
                    177,
                    0
                ],
                "title": "GraphSnapShot: Caching Local Structure for Fast Graph Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GraphSnapShot: Caching Local Structure for Fast Graph Learning"
                },
                "summary": "In our recent research, we have developed a framework called GraphSnapShot,\nwhich has been proven an useful tool for graph learning acceleration.\nGraphSnapShot is a framework for fast cache, storage, retrieval and computation\nfor graph learning. It can quickly store and update the local topology of graph\nstructure and allows us to track patterns in the structure of graph networks,\njust like take snapshots of the graphs. In experiments, GraphSnapShot shows\nefficiency, it can achieve up to 30% training acceleration and 73% memory\nreduction for lossless graph ML training compared to current baselines such as\ndgl.This technique is particular useful for large dynamic graph learning tasks\nsuch as social media analysis and recommendation systems to process complex\nrelationships between entities.\n  The code for GraphSnapShot is publicly available at\nhttps://github.com/NoakLiu/GraphSnapShot.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In our recent research, we have developed a framework called GraphSnapShot,\nwhich has been proven an useful tool for graph learning acceleration.\nGraphSnapShot is a framework for fast cache, storage, retrieval and computation\nfor graph learning. It can quickly store and update the local topology of graph\nstructure and allows us to track patterns in the structure of graph networks,\njust like take snapshots of the graphs. In experiments, GraphSnapShot shows\nefficiency, it can achieve up to 30% training acceleration and 73% memory\nreduction for lossless graph ML training compared to current baselines such as\ndgl.This technique is particular useful for large dynamic graph learning tasks\nsuch as social media analysis and recommendation systems to process complex\nrelationships between entities.\n  The code for GraphSnapShot is publicly available at\nhttps://github.com/NoakLiu/GraphSnapShot."
                },
                "authors": [
                    {
                        "name": "Dong Liu"
                    },
                    {
                        "name": "Roger Waleffe"
                    },
                    {
                        "name": "Meng Jiang"
                    },
                    {
                        "name": "Shivaram Venkataraman"
                    }
                ],
                "author_detail": {
                    "name": "Shivaram Venkataraman"
                },
                "author": "Shivaram Venkataraman",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.17918v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.17918v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.06428v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.06428v1",
                "updated": "2025-01-11T03:47:04Z",
                "updated_parsed": [
                    2025,
                    1,
                    11,
                    3,
                    47,
                    4,
                    5,
                    11,
                    0
                ],
                "published": "2025-01-11T03:47:04Z",
                "published_parsed": [
                    2025,
                    1,
                    11,
                    3,
                    47,
                    4,
                    5,
                    11,
                    0
                ],
                "title": "Optimizing digital experiences with content delivery networks:\n  Architectures, performance strategies, and future trends",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Optimizing digital experiences with content delivery networks:\n  Architectures, performance strategies, and future trends"
                },
                "summary": "This research investigates how CDNs (Content Delivery Networks) can improve\nthe digital experience, as consumers increasingly expect fast, efficient, and\neffortless access to online resources. CDNs play a crucial role in reducing\nlatency, enhancing scalability, and optimizing delivery mechanisms, which is\nevident across various platforms and regions. The study focuses on key CDN\nconcerns, such as foundational and modern CDN architectures, edge computing,\nhybrid CDNs, and multi-CDN strategies. It also explores performance-enhancing\ntopics, including caching, load balancing, and the novel features of HTTP/3 and\nQUIC.\n  Current trends, such as integrating CDNs with 5G networks, serverless\narchitectures, and AI-driven traffic management, are examined to demonstrate\nhow CDN technology is likely to evolve. The study also addresses challenges\nrelated to security, cost, and global regulations. Practical examples from the\ne-commerce, streaming, and gaming industries highlight how enhanced CDNs are\ntransforming these sectors.\n  The conclusions emphasize the need to evolve CDN strategies to meet growing\nuser expectations and adapt to the rapidly changing digital landscape.\nAdditionally, the research identifies future research opportunities,\nparticularly in exploring the impact of QC, the enhancement of AI services, and\nthe sustainability of CDN solutions. Overall, the study situates architectural\ndesign, performance strategies, and emerging trends to address gaps and create\na more efficient and secure approach for improving digital experiences.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This research investigates how CDNs (Content Delivery Networks) can improve\nthe digital experience, as consumers increasingly expect fast, efficient, and\neffortless access to online resources. CDNs play a crucial role in reducing\nlatency, enhancing scalability, and optimizing delivery mechanisms, which is\nevident across various platforms and regions. The study focuses on key CDN\nconcerns, such as foundational and modern CDN architectures, edge computing,\nhybrid CDNs, and multi-CDN strategies. It also explores performance-enhancing\ntopics, including caching, load balancing, and the novel features of HTTP/3 and\nQUIC.\n  Current trends, such as integrating CDNs with 5G networks, serverless\narchitectures, and AI-driven traffic management, are examined to demonstrate\nhow CDN technology is likely to evolve. The study also addresses challenges\nrelated to security, cost, and global regulations. Practical examples from the\ne-commerce, streaming, and gaming industries highlight how enhanced CDNs are\ntransforming these sectors.\n  The conclusions emphasize the need to evolve CDN strategies to meet growing\nuser expectations and adapt to the rapidly changing digital landscape.\nAdditionally, the research identifies future research opportunities,\nparticularly in exploring the impact of QC, the enhancement of AI services, and\nthe sustainability of CDN solutions. Overall, the study situates architectural\ndesign, performance strategies, and emerging trends to address gaps and create\na more efficient and secure approach for improving digital experiences."
                },
                "authors": [
                    {
                        "name": "Anuj Tyagi"
                    }
                ],
                "author_detail": {
                    "name": "Anuj Tyagi"
                },
                "author": "Anuj Tyagi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.06428v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.06428v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.06425v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.06425v1",
                "updated": "2025-01-11T03:37:10Z",
                "updated_parsed": [
                    2025,
                    1,
                    11,
                    3,
                    37,
                    10,
                    5,
                    11,
                    0
                ],
                "published": "2025-01-11T03:37:10Z",
                "published_parsed": [
                    2025,
                    1,
                    11,
                    3,
                    37,
                    10,
                    5,
                    11,
                    0
                ],
                "title": "Tensor Product Attention Is All You Need",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Tensor Product Attention Is All You Need"
                },
                "summary": "Scaling language models to handle longer input sequences typically\nnecessitates large key-value (KV) caches, resulting in substantial memory\noverhead during inference. In this paper, we propose Tensor Product Attention\n(TPA), a novel attention mechanism that uses tensor decompositions to represent\nqueries, keys, and values compactly, significantly shrinking KV cache size at\ninference time. By factorizing these representations into contextual low-rank\ncomponents (contextual factorization) and seamlessly integrating with RoPE, TPA\nachieves improved model quality alongside memory efficiency. Based on TPA, we\nintroduce the Tensor ProducT ATTenTion Transformer (T6), a new model\narchitecture for sequence modeling. Through extensive empirical evaluation of\nlanguage modeling tasks, we demonstrate that T6 exceeds the performance of\nstandard Transformer baselines including MHA, MQA, GQA, and MLA across various\nmetrics, including perplexity and a range of renowned evaluation benchmarks.\nNotably, TPAs memory efficiency enables the processing of significantly longer\nsequences under fixed resource constraints, addressing a critical scalability\nchallenge in modern language models. The code is available at\nhttps://github.com/tensorgi/T6.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Scaling language models to handle longer input sequences typically\nnecessitates large key-value (KV) caches, resulting in substantial memory\noverhead during inference. In this paper, we propose Tensor Product Attention\n(TPA), a novel attention mechanism that uses tensor decompositions to represent\nqueries, keys, and values compactly, significantly shrinking KV cache size at\ninference time. By factorizing these representations into contextual low-rank\ncomponents (contextual factorization) and seamlessly integrating with RoPE, TPA\nachieves improved model quality alongside memory efficiency. Based on TPA, we\nintroduce the Tensor ProducT ATTenTion Transformer (T6), a new model\narchitecture for sequence modeling. Through extensive empirical evaluation of\nlanguage modeling tasks, we demonstrate that T6 exceeds the performance of\nstandard Transformer baselines including MHA, MQA, GQA, and MLA across various\nmetrics, including perplexity and a range of renowned evaluation benchmarks.\nNotably, TPAs memory efficiency enables the processing of significantly longer\nsequences under fixed resource constraints, addressing a critical scalability\nchallenge in modern language models. The code is available at\nhttps://github.com/tensorgi/T6."
                },
                "authors": [
                    {
                        "name": "Yifan Zhang"
                    },
                    {
                        "name": "Yifeng Liu"
                    },
                    {
                        "name": "Huizhuo Yuan"
                    },
                    {
                        "name": "Zhen Qin"
                    },
                    {
                        "name": "Yang Yuan"
                    },
                    {
                        "name": "Quanquan Gu"
                    },
                    {
                        "name": "Andrew Chi-Chih Yao"
                    }
                ],
                "author_detail": {
                    "name": "Andrew Chi-Chih Yao"
                },
                "author": "Andrew Chi-Chih Yao",
                "arxiv_comment": "23 pages, 5 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.06425v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.06425v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.06394v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.06394v1",
                "updated": "2025-01-11T00:47:29Z",
                "updated_parsed": [
                    2025,
                    1,
                    11,
                    0,
                    47,
                    29,
                    5,
                    11,
                    0
                ],
                "published": "2025-01-11T00:47:29Z",
                "published_parsed": [
                    2025,
                    1,
                    11,
                    0,
                    47,
                    29,
                    5,
                    11,
                    0
                ],
                "title": "Unispeaker: A Unified Approach for Multimodality-driven Speaker\n  Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Unispeaker: A Unified Approach for Multimodality-driven Speaker\n  Generation"
                },
                "summary": "Recent advancements in personalized speech generation have brought synthetic\nspeech increasingly close to the realism of target speakers' recordings, yet\nmultimodal speaker generation remains on the rise. This paper introduces\nUniSpeaker, a unified approach for multimodality-driven speaker generation.\nSpecifically, we propose a unified voice aggregator based on KV-Former,\napplying soft contrastive loss to map diverse voice description modalities into\na shared voice space, ensuring that the generated voice aligns more closely\nwith the input descriptions. To evaluate multimodality-driven voice control, we\nbuild the first multimodality-based voice control (MVC) benchmark, focusing on\nvoice suitability, voice diversity, and speech quality. UniSpeaker is evaluated\nacross five tasks using the MVC benchmark, and the experimental results\ndemonstrate that UniSpeaker outperforms previous modality-specific models.\nSpeech samples are available at \\url{https://UniSpeaker.github.io}.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advancements in personalized speech generation have brought synthetic\nspeech increasingly close to the realism of target speakers' recordings, yet\nmultimodal speaker generation remains on the rise. This paper introduces\nUniSpeaker, a unified approach for multimodality-driven speaker generation.\nSpecifically, we propose a unified voice aggregator based on KV-Former,\napplying soft contrastive loss to map diverse voice description modalities into\na shared voice space, ensuring that the generated voice aligns more closely\nwith the input descriptions. To evaluate multimodality-driven voice control, we\nbuild the first multimodality-based voice control (MVC) benchmark, focusing on\nvoice suitability, voice diversity, and speech quality. UniSpeaker is evaluated\nacross five tasks using the MVC benchmark, and the experimental results\ndemonstrate that UniSpeaker outperforms previous modality-specific models.\nSpeech samples are available at \\url{https://UniSpeaker.github.io}."
                },
                "authors": [
                    {
                        "name": "Zhengyan Sheng"
                    },
                    {
                        "name": "Zhihao Du"
                    },
                    {
                        "name": "Heng Lu"
                    },
                    {
                        "name": "Shiliang Zhang"
                    },
                    {
                        "name": "Zhen-Hua Ling"
                    }
                ],
                "author_detail": {
                    "name": "Zhen-Hua Ling"
                },
                "author": "Zhen-Hua Ling",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.06394v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.06394v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SD",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.01030v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.01030v2",
                "updated": "2025-01-10T10:11:45Z",
                "updated_parsed": [
                    2025,
                    1,
                    10,
                    10,
                    11,
                    45,
                    4,
                    10,
                    0
                ],
                "published": "2024-07-01T07:25:08Z",
                "published_parsed": [
                    2024,
                    7,
                    1,
                    7,
                    25,
                    8,
                    0,
                    183,
                    0
                ],
                "title": "Tame fields, Graded Rings and Finite Complete Sequences of Key\n  Polynomials",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Tame fields, Graded Rings and Finite Complete Sequences of Key\n  Polynomials"
                },
                "summary": "In this paper, we present a criterion for $(K,v)$ to be henselian and\ndefectless in terms of finite complete sequences of key polynomials. For this,\nwe use the theory of Mac Lane-Vaqui\\'e chains and abstract key polynomials. We\nthen prove that a valued field $(K,v)$ is tame if and only if $vK$ is\n$p$-divisible, $Kv$ is perfect and every simple algebraic extension of $K$\nadmits a finite complete sequence of key polynomials. The properties $vK$\n$p$-divisible and $Kv$ perfect are described by the Frobenius endomorphism on\nthe associated graded ring. We also make considerations on simply defectless\nand algebraically maximal valued fields and purely inertial and purely ramified\nextensions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this paper, we present a criterion for $(K,v)$ to be henselian and\ndefectless in terms of finite complete sequences of key polynomials. For this,\nwe use the theory of Mac Lane-Vaqui\\'e chains and abstract key polynomials. We\nthen prove that a valued field $(K,v)$ is tame if and only if $vK$ is\n$p$-divisible, $Kv$ is perfect and every simple algebraic extension of $K$\nadmits a finite complete sequence of key polynomials. The properties $vK$\n$p$-divisible and $Kv$ perfect are described by the Frobenius endomorphism on\nthe associated graded ring. We also make considerations on simply defectless\nand algebraically maximal valued fields and purely inertial and purely ramified\nextensions."
                },
                "authors": [
                    {
                        "name": "Caio Henrique Silva de Souza"
                    }
                ],
                "author_detail": {
                    "name": "Caio Henrique Silva de Souza"
                },
                "author": "Caio Henrique Silva de Souza",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.01030v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.01030v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "math.AC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "math.AC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "13A18",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.20433v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.20433v2",
                "updated": "2025-01-09T15:14:05Z",
                "updated_parsed": [
                    2025,
                    1,
                    9,
                    15,
                    14,
                    5,
                    3,
                    9,
                    0
                ],
                "published": "2024-09-30T15:53:36Z",
                "published_parsed": [
                    2024,
                    9,
                    30,
                    15,
                    53,
                    36,
                    0,
                    274,
                    0
                ],
                "title": "Handover_Management_in_UAV_Networks_with_Blockages",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Handover_Management_in_UAV_Networks_with_Blockages"
                },
                "summary": "We investigate the performance of unmanned aerial vehicle (UAV)-based\nnetworks in urban environments characterized by blockages, focusing on their\ncapability to support the service demands of mobile users. The UAV-base\nstations (UAV-BSs) are modeled using a two-dimensional (2-D) marked- Poisson\npoint process (MPPP), where the marks represent the altitude of each UAV-BS.\nLeveraging stochastic geometry, we analyze the impact of blockages on network\nreliability by studying the meta distribution (MD) of the\nsignal-to-interference noise ratio (SINR) for a specific reliability threshold\nand the association probabilities for both line-of-sight (LoS) and non\nline-of-sight (NLoS) UAV-BSs. Furthermore, to enhance the performance of mobile\nusers, we propose a novel cache-based handover management strategy that\ndynamically selects the cell search time and delays the received signal\nstrength (RSS)-based base station (BS) associations. This strategy aims to\nminimize unnecessary handovers (HOs) experienced by users by leveraging caching\ncapabilities at user equipment (UE), thus reducing latency, ensuring seamless\nconnectivity, and maintaining the quality of service (QoS). This study provides\nvaluable insights into optimizing UAV network deployments to support the\nstringent requirements in the network, ensuring reliable, low-latency, and\nhigh-throughput communication for next-generation smart cities.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We investigate the performance of unmanned aerial vehicle (UAV)-based\nnetworks in urban environments characterized by blockages, focusing on their\ncapability to support the service demands of mobile users. The UAV-base\nstations (UAV-BSs) are modeled using a two-dimensional (2-D) marked- Poisson\npoint process (MPPP), where the marks represent the altitude of each UAV-BS.\nLeveraging stochastic geometry, we analyze the impact of blockages on network\nreliability by studying the meta distribution (MD) of the\nsignal-to-interference noise ratio (SINR) for a specific reliability threshold\nand the association probabilities for both line-of-sight (LoS) and non\nline-of-sight (NLoS) UAV-BSs. Furthermore, to enhance the performance of mobile\nusers, we propose a novel cache-based handover management strategy that\ndynamically selects the cell search time and delays the received signal\nstrength (RSS)-based base station (BS) associations. This strategy aims to\nminimize unnecessary handovers (HOs) experienced by users by leveraging caching\ncapabilities at user equipment (UE), thus reducing latency, ensuring seamless\nconnectivity, and maintaining the quality of service (QoS). This study provides\nvaluable insights into optimizing UAV network deployments to support the\nstringent requirements in the network, ensuring reliable, low-latency, and\nhigh-throughput communication for next-generation smart cities."
                },
                "authors": [
                    {
                        "name": "Neetu R R"
                    },
                    {
                        "name": "Gourab Ghatak"
                    },
                    {
                        "name": "Vivek Ashok Bohara"
                    }
                ],
                "author_detail": {
                    "name": "Vivek Ashok Bohara"
                },
                "author": "Vivek Ashok Bohara",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.20433v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.20433v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SP",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.04993v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.04993v1",
                "updated": "2025-01-09T06:18:39Z",
                "updated_parsed": [
                    2025,
                    1,
                    9,
                    6,
                    18,
                    39,
                    3,
                    9,
                    0
                ],
                "published": "2025-01-09T06:18:39Z",
                "published_parsed": [
                    2025,
                    1,
                    9,
                    6,
                    18,
                    39,
                    3,
                    9,
                    0
                ],
                "title": "ByteFS: System Support for (CXL-based) Memory-Semantic Solid-State\n  Drives",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ByteFS: System Support for (CXL-based) Memory-Semantic Solid-State\n  Drives"
                },
                "summary": "Unlike non-volatile memory that resides on the processor memory bus,\nmemory-semantic solid-state drives (SSDs) support both byte and block access\ngranularity via PCIe or CXL interconnects. They provide scalable memory\ncapacity using NAND flash at a much lower cost. In addition, they have\ndifferent performance characteristics for their dual byte/block interface\nrespectively, while offering essential memory semantics for upper-level\nsoftware. Such a byte-accessible storage device provides new implications on\nthe software system design.\n  In this paper, we develop a new file system, named ByteFS, by rethinking the\ndesign primitives of file systems and SSD firmware to exploit the advantages of\nboth byte and block-granular data accesses. ByteFS supports byte-granular data\npersistence to retain the persistence nature of SSDs. It extends the core data\nstructure of file systems by enabling dual byte/block-granular data accesses.\nTo facilitate the support for byte-granular writes, \\pname{} manages the\ninternal DRAM of SSD firmware in a log-structured manner and enables data\ncoalescing to reduce the unnecessary I/O traffic to flash chips. ByteFS also\nenables coordinated data caching between the host page cache and SSD cache for\nbest utilizing the precious memory resource. We implement ByteFS on both a real\nprogrammable SSD and an emulated memory-semantic SSD for sensitivity study.\nCompared to state-of-the-art file systems for non-volatile memory and\nconventional SSDs, ByteFS outperforms them by up to 2.7$\\times$, while\npreserving the essential properties of a file system. ByteFS also reduces the\nwrite traffic to SSDs by up to 5.1$\\times$ by alleviating unnecessary writes\ncaused by both metadata and data updates in file systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Unlike non-volatile memory that resides on the processor memory bus,\nmemory-semantic solid-state drives (SSDs) support both byte and block access\ngranularity via PCIe or CXL interconnects. They provide scalable memory\ncapacity using NAND flash at a much lower cost. In addition, they have\ndifferent performance characteristics for their dual byte/block interface\nrespectively, while offering essential memory semantics for upper-level\nsoftware. Such a byte-accessible storage device provides new implications on\nthe software system design.\n  In this paper, we develop a new file system, named ByteFS, by rethinking the\ndesign primitives of file systems and SSD firmware to exploit the advantages of\nboth byte and block-granular data accesses. ByteFS supports byte-granular data\npersistence to retain the persistence nature of SSDs. It extends the core data\nstructure of file systems by enabling dual byte/block-granular data accesses.\nTo facilitate the support for byte-granular writes, \\pname{} manages the\ninternal DRAM of SSD firmware in a log-structured manner and enables data\ncoalescing to reduce the unnecessary I/O traffic to flash chips. ByteFS also\nenables coordinated data caching between the host page cache and SSD cache for\nbest utilizing the precious memory resource. We implement ByteFS on both a real\nprogrammable SSD and an emulated memory-semantic SSD for sensitivity study.\nCompared to state-of-the-art file systems for non-volatile memory and\nconventional SSDs, ByteFS outperforms them by up to 2.7$\\times$, while\npreserving the essential properties of a file system. ByteFS also reduces the\nwrite traffic to SSDs by up to 5.1$\\times$ by alleviating unnecessary writes\ncaused by both metadata and data updates in file systems."
                },
                "authors": [
                    {
                        "name": "Shaobo Li"
                    },
                    {
                        "name": "Yirui Eric Zhou"
                    },
                    {
                        "name": "Hao Ren"
                    },
                    {
                        "name": "Jian Huang"
                    }
                ],
                "author_detail": {
                    "name": "Jian Huang"
                },
                "author": "Jian Huang",
                "arxiv_comment": "This paper is accepted at the 30th Conference on Architectural\n  Support for Programming Languages and Operating Systems (ASPLOS 2025)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.04993v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.04993v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.OS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.OS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.04216v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.04216v2",
                "updated": "2025-01-09T03:02:31Z",
                "updated_parsed": [
                    2025,
                    1,
                    9,
                    3,
                    2,
                    31,
                    3,
                    9,
                    0
                ],
                "published": "2025-01-08T01:23:29Z",
                "published_parsed": [
                    2025,
                    1,
                    8,
                    1,
                    23,
                    29,
                    2,
                    8,
                    0
                ],
                "title": "Optimal Oblivious Algorithms for Multi-way Joins",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Optimal Oblivious Algorithms for Multi-way Joins"
                },
                "summary": "In cloud databases, cloud computation over sensitive data uploaded by clients\ninevitably causes concern about data security and privacy. Even when encryption\nprimitives and trusted computing environments are integrated into query\nprocessing to safeguard the actual contents of the data, access patterns of\nalgorithms can still leak private information about the data. Oblivious Random\nAccess Memory (ORAM) and circuits are two generic approaches to address this\nissue, ensuring that access patterns of algorithms remain oblivious to the\ndata. However, deploying these methods on insecure algorithms, particularly for\nmulti-way join processing, is computationally expensive and inherently\nchallenging.\n  In this paper, we propose a novel sorting-based algorithm for multi-way join\nprocessing that operates without relying on ORAM simulations or other security\nassumptions. Our algorithm is a non-trivial, provably oblivious composition of\nbasic primitives, with time complexity matching the insecure worst-case optimal\njoin algorithm, up to a logarithmic factor. Furthermore, it is cache-agnostic,\nwith cache complexity matching the insecure lower bound, also up to a\nlogarithmic factor. This clean and straightforward approach has the potential\nto be extended to other security settings and implemented in practical database\nsystems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In cloud databases, cloud computation over sensitive data uploaded by clients\ninevitably causes concern about data security and privacy. Even when encryption\nprimitives and trusted computing environments are integrated into query\nprocessing to safeguard the actual contents of the data, access patterns of\nalgorithms can still leak private information about the data. Oblivious Random\nAccess Memory (ORAM) and circuits are two generic approaches to address this\nissue, ensuring that access patterns of algorithms remain oblivious to the\ndata. However, deploying these methods on insecure algorithms, particularly for\nmulti-way join processing, is computationally expensive and inherently\nchallenging.\n  In this paper, we propose a novel sorting-based algorithm for multi-way join\nprocessing that operates without relying on ORAM simulations or other security\nassumptions. Our algorithm is a non-trivial, provably oblivious composition of\nbasic primitives, with time complexity matching the insecure worst-case optimal\njoin algorithm, up to a logarithmic factor. Furthermore, it is cache-agnostic,\nwith cache complexity matching the insecure lower bound, also up to a\nlogarithmic factor. This clean and straightforward approach has the potential\nto be extended to other security settings and implemented in practical database\nsystems."
                },
                "authors": [
                    {
                        "name": "Xiao Hu"
                    },
                    {
                        "name": "Zhiang Wu"
                    }
                ],
                "author_detail": {
                    "name": "Zhiang Wu"
                },
                "author": "Zhiang Wu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.04216v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.04216v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.04394v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.04394v1",
                "updated": "2025-01-08T10:14:19Z",
                "updated_parsed": [
                    2025,
                    1,
                    8,
                    10,
                    14,
                    19,
                    2,
                    8,
                    0
                ],
                "published": "2025-01-08T10:14:19Z",
                "published_parsed": [
                    2025,
                    1,
                    8,
                    10,
                    14,
                    19,
                    2,
                    8,
                    0
                ],
                "title": "Modern Hardware Security: A Review of Attacks and Countermeasures",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Modern Hardware Security: A Review of Attacks and Countermeasures"
                },
                "summary": "With the exponential rise in the use of cloud services, smart devices, and\nIoT devices, advanced cyber attacks have become increasingly sophisticated and\nubiquitous. Furthermore, the rapid evolution of computing architectures and\nmemory technologies has created an urgent need to understand and address\nhardware security vulnerabilities. In this paper, we review the current state\nof vulnerabilities and mitigation strategies in contemporary computing systems.\nWe discuss cache side-channel attacks (including Spectre and Meltdown), power\nside-channel attacks (such as Simple Power Analysis, Differential Power\nAnalysis, Correlation Power Analysis, and Template Attacks), and advanced\ntechniques like Voltage Glitching and Electromagnetic Analysis to help\nunderstand and build robust cybersecurity defense systems and guide further\nresearch. We also examine memory encryption, focusing on confidentiality,\ngranularity, key management, masking, and re-keying strategies. Additionally,\nwe cover Cryptographic Instruction Set Architectures, Secure Boot, Root of\nTrust mechanisms, Physical Unclonable Functions, and hardware fault injection\ntechniques. The paper concludes with an analysis of the RISC-V architecture's\nunique security challenges. The comprehensive analysis presented in this paper\nis essential for building resilient hardware security solutions that can\nprotect against both current and emerging threats in an increasingly\nchallenging security landscape.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the exponential rise in the use of cloud services, smart devices, and\nIoT devices, advanced cyber attacks have become increasingly sophisticated and\nubiquitous. Furthermore, the rapid evolution of computing architectures and\nmemory technologies has created an urgent need to understand and address\nhardware security vulnerabilities. In this paper, we review the current state\nof vulnerabilities and mitigation strategies in contemporary computing systems.\nWe discuss cache side-channel attacks (including Spectre and Meltdown), power\nside-channel attacks (such as Simple Power Analysis, Differential Power\nAnalysis, Correlation Power Analysis, and Template Attacks), and advanced\ntechniques like Voltage Glitching and Electromagnetic Analysis to help\nunderstand and build robust cybersecurity defense systems and guide further\nresearch. We also examine memory encryption, focusing on confidentiality,\ngranularity, key management, masking, and re-keying strategies. Additionally,\nwe cover Cryptographic Instruction Set Architectures, Secure Boot, Root of\nTrust mechanisms, Physical Unclonable Functions, and hardware fault injection\ntechniques. The paper concludes with an analysis of the RISC-V architecture's\nunique security challenges. The comprehensive analysis presented in this paper\nis essential for building resilient hardware security solutions that can\nprotect against both current and emerging threats in an increasingly\nchallenging security landscape."
                },
                "authors": [
                    {
                        "name": "Jyotiprakash Mishra"
                    },
                    {
                        "name": "Sanjay K. Sahay"
                    }
                ],
                "author_detail": {
                    "name": "Sanjay K. Sahay"
                },
                "author": "Sanjay K. Sahay",
                "arxiv_comment": "25 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.04394v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.04394v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.00799v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.00799v2",
                "updated": "2025-01-07T17:32:19Z",
                "updated_parsed": [
                    2025,
                    1,
                    7,
                    17,
                    32,
                    19,
                    1,
                    7,
                    0
                ],
                "published": "2025-01-01T10:50:35Z",
                "published_parsed": [
                    2025,
                    1,
                    1,
                    10,
                    50,
                    35,
                    2,
                    1,
                    0
                ],
                "title": "Follow The Approximate Sparse Leader for No-Regret Online Sparse Linear\n  Approximation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Follow The Approximate Sparse Leader for No-Regret Online Sparse Linear\n  Approximation"
                },
                "summary": "We consider the problem of \\textit{online sparse linear approximation}, where\none predicts the best sparse approximation of a sequence of measurements in\nterms of linear combination of columns of a given measurement matrix. Such\nonline prediction problems are ubiquitous, ranging from medical trials to web\ncaching to resource allocation. The inherent difficulty of offline recovery\nalso makes the online problem challenging. In this letter, we propose\nFollow-The-Approximate-Sparse-Leader, an efficient online meta-policy to\naddress this online problem. Through a detailed theoretical analysis, we prove\nthat under certain assumptions on the measurement sequence, the proposed policy\nenjoys a data-dependent sublinear upper bound on the static regret, which can\nrange from logarithmic to square-root. Numerical simulations are performed to\ncorroborate the theoretical findings and demonstrate the efficacy of the\nproposed online policy.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We consider the problem of \\textit{online sparse linear approximation}, where\none predicts the best sparse approximation of a sequence of measurements in\nterms of linear combination of columns of a given measurement matrix. Such\nonline prediction problems are ubiquitous, ranging from medical trials to web\ncaching to resource allocation. The inherent difficulty of offline recovery\nalso makes the online problem challenging. In this letter, we propose\nFollow-The-Approximate-Sparse-Leader, an efficient online meta-policy to\naddress this online problem. Through a detailed theoretical analysis, we prove\nthat under certain assumptions on the measurement sequence, the proposed policy\nenjoys a data-dependent sublinear upper bound on the static regret, which can\nrange from logarithmic to square-root. Numerical simulations are performed to\ncorroborate the theoretical findings and demonstrate the efficacy of the\nproposed online policy."
                },
                "authors": [
                    {
                        "name": "Samrat Mukhopadhyay"
                    },
                    {
                        "name": "Debasmita Mukherjee"
                    }
                ],
                "author_detail": {
                    "name": "Debasmita Mukherjee"
                },
                "author": "Debasmita Mukherjee",
                "arxiv_comment": "12 pages, 5 figures, corrected title, added proof of a lemma in\n  appendix",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.00799v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.00799v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.OC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.09275v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.09275v2",
                "updated": "2025-01-06T23:16:22Z",
                "updated_parsed": [
                    2025,
                    1,
                    6,
                    23,
                    16,
                    22,
                    0,
                    6,
                    0
                ],
                "published": "2024-11-14T08:25:31Z",
                "published_parsed": [
                    2024,
                    11,
                    14,
                    8,
                    25,
                    31,
                    3,
                    319,
                    0
                ],
                "title": "Parallel $k$d-tree with Batch Updates",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Parallel $k$d-tree with Batch Updates"
                },
                "summary": "The $k$d-tree is one of the most widely used data structures to manage\nmulti-dimensional data. Due to the ever-growing data volume, it is imperative\nto consider parallelism in $k$d-trees. However, we observed challenges in\nexisting parallel kd-tree implementations, for both constructions and updates.\n  The goal of this paper is to develop efficient in-memory $k$d-trees by\nsupporting high parallelism and cache-efficiency. We propose the Pkd-tree\n(Parallel $k$d-tree), a parallel $k$d-tree that is efficient both in theory and\nin practice. The Pkd-tree supports parallel tree construction, batch update\n(insertion and deletion), and various queries including k-nearest neighbor\nsearch, range query, and range count. We proved that our algorithms have strong\ntheoretical bounds in work (sequential time complexity), span (parallelism),\nand cache complexity. Our key techniques include 1) an efficient construction\nalgorithm that optimizes work, span, and cache complexity simultaneously, and\n2) reconstruction-based update algorithms that guarantee the tree to be\nweight-balanced. With the new algorithmic insights and careful engineering\neffort, we achieved a highly optimized implementation of the Pkd-tree.\n  We tested Pkd-tree with various synthetic and real-world datasets, including\nboth uniform and highly skewed data. We compare the Pkd-tree with\nstate-of-the-art parallel $k$d-tree implementations. In all tests, with better\nor competitive query performance, Pkd-tree is much faster in construction and\nupdates consistently than all baselines. We released our code.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The $k$d-tree is one of the most widely used data structures to manage\nmulti-dimensional data. Due to the ever-growing data volume, it is imperative\nto consider parallelism in $k$d-trees. However, we observed challenges in\nexisting parallel kd-tree implementations, for both constructions and updates.\n  The goal of this paper is to develop efficient in-memory $k$d-trees by\nsupporting high parallelism and cache-efficiency. We propose the Pkd-tree\n(Parallel $k$d-tree), a parallel $k$d-tree that is efficient both in theory and\nin practice. The Pkd-tree supports parallel tree construction, batch update\n(insertion and deletion), and various queries including k-nearest neighbor\nsearch, range query, and range count. We proved that our algorithms have strong\ntheoretical bounds in work (sequential time complexity), span (parallelism),\nand cache complexity. Our key techniques include 1) an efficient construction\nalgorithm that optimizes work, span, and cache complexity simultaneously, and\n2) reconstruction-based update algorithms that guarantee the tree to be\nweight-balanced. With the new algorithmic insights and careful engineering\neffort, we achieved a highly optimized implementation of the Pkd-tree.\n  We tested Pkd-tree with various synthetic and real-world datasets, including\nboth uniform and highly skewed data. We compare the Pkd-tree with\nstate-of-the-art parallel $k$d-tree implementations. In all tests, with better\nor competitive query performance, Pkd-tree is much faster in construction and\nupdates consistently than all baselines. We released our code."
                },
                "authors": [
                    {
                        "name": "Ziyang Men"
                    },
                    {
                        "name": "Zheqi Shen"
                    },
                    {
                        "name": "Yan Gu"
                    },
                    {
                        "name": "Yihan Sun"
                    }
                ],
                "author_detail": {
                    "name": "Yihan Sun"
                },
                "author": "Yihan Sun",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.09275v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.09275v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.04052v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.04052v1",
                "updated": "2025-01-06T22:40:40Z",
                "updated_parsed": [
                    2025,
                    1,
                    6,
                    22,
                    40,
                    40,
                    0,
                    6,
                    0
                ],
                "published": "2025-01-06T22:40:40Z",
                "published_parsed": [
                    2025,
                    1,
                    6,
                    22,
                    40,
                    40,
                    0,
                    6,
                    0
                ],
                "title": "The Power of Negative Zero: Datatype Customization for Quantized Large\n  Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Power of Negative Zero: Datatype Customization for Quantized Large\n  Language Models"
                },
                "summary": "Large language models (LLMs) have demonstrated remarkable performance across\nvarious machine learning tasks, quickly becoming one of the most prevalent AI\nworkloads. Yet the substantial memory requirement of LLMs significantly hinders\ntheir deployment for end users. Post-training quantization (PTQ) serves as one\nof the most hardware-efficient methods to mitigate the memory and computational\ndemands of LLMs. Although the traditional integer (INT) datatype has received\nwidespread adoption in PTQ methods, floating-point (FP) quantization has\nemerged as a viable alternative thanks to its effectiveness in fitting LLM\nnumerical distributions. However, the FP datatype in sign-magnitude binary\nrepresentation contains both positive and negative zero, which constrains its\nrepresentation capability, particularly under low precision (3 and 4 bits). In\nthis paper, we extend the basic FP datatype to perform Redundant Zero Remapping\n(RaZeR), which remaps the negative zero FP encoding to a set of pre-defined\nspecial values to maximally utilize FP quantization encodings and to better fit\nLLM numerical distributions. Through careful selection of special values, RaZeR\noutperforms conventional asymmetric INT quantization while achieving high\ncomputational efficiency. We demonstrate that RaZeR can be seamlessly\nintegrated with quantization algorithms for both weights and KV-cache,\nincluding advanced methods with clipping and transformations, and consistently\nachieve better model accuracy. Additionally, we implement a fast GEMV kernel\nwith fused dequantization that efficiently converts the 4-bit RaZeR value to\nFP16 through novel bit-level manipulation. On modern GPUs, our evaluation shows\nthat RaZeR improves the GEMV speed by up to 7.56$\\times$ compared to the FP16\nimplementation, while achieving up to 2.72$\\times$ speedup in the LLM decoding\nthroughput.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have demonstrated remarkable performance across\nvarious machine learning tasks, quickly becoming one of the most prevalent AI\nworkloads. Yet the substantial memory requirement of LLMs significantly hinders\ntheir deployment for end users. Post-training quantization (PTQ) serves as one\nof the most hardware-efficient methods to mitigate the memory and computational\ndemands of LLMs. Although the traditional integer (INT) datatype has received\nwidespread adoption in PTQ methods, floating-point (FP) quantization has\nemerged as a viable alternative thanks to its effectiveness in fitting LLM\nnumerical distributions. However, the FP datatype in sign-magnitude binary\nrepresentation contains both positive and negative zero, which constrains its\nrepresentation capability, particularly under low precision (3 and 4 bits). In\nthis paper, we extend the basic FP datatype to perform Redundant Zero Remapping\n(RaZeR), which remaps the negative zero FP encoding to a set of pre-defined\nspecial values to maximally utilize FP quantization encodings and to better fit\nLLM numerical distributions. Through careful selection of special values, RaZeR\noutperforms conventional asymmetric INT quantization while achieving high\ncomputational efficiency. We demonstrate that RaZeR can be seamlessly\nintegrated with quantization algorithms for both weights and KV-cache,\nincluding advanced methods with clipping and transformations, and consistently\nachieve better model accuracy. Additionally, we implement a fast GEMV kernel\nwith fused dequantization that efficiently converts the 4-bit RaZeR value to\nFP16 through novel bit-level manipulation. On modern GPUs, our evaluation shows\nthat RaZeR improves the GEMV speed by up to 7.56$\\times$ compared to the FP16\nimplementation, while achieving up to 2.72$\\times$ speedup in the LLM decoding\nthroughput."
                },
                "authors": [
                    {
                        "name": "Yuzong Chen"
                    },
                    {
                        "name": "Xilai Dai"
                    },
                    {
                        "name": "Chi-chih Chang"
                    },
                    {
                        "name": "Yash Akhauri"
                    },
                    {
                        "name": "Mohamed S. Abdelfattah"
                    }
                ],
                "author_detail": {
                    "name": "Mohamed S. Abdelfattah"
                },
                "author": "Mohamed S. Abdelfattah",
                "arxiv_comment": "under submission",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.04052v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.04052v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.03322v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.03322v1",
                "updated": "2025-01-06T19:00:03Z",
                "updated_parsed": [
                    2025,
                    1,
                    6,
                    19,
                    0,
                    3,
                    0,
                    6,
                    0
                ],
                "published": "2025-01-06T19:00:03Z",
                "published_parsed": [
                    2025,
                    1,
                    6,
                    19,
                    0,
                    3,
                    0,
                    6,
                    0
                ],
                "title": "Twinkle: A GPU-based binary-lens microlensing code with contour\n  integration method",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Twinkle: A GPU-based binary-lens microlensing code with contour\n  integration method"
                },
                "summary": "With the rapidly increasing rate of microlensing planet detections,\nmicrolensing modeling software faces significant challenges in computation\nefficiency. Here, we develop the Twinkle code, an efficient and robust\nbinary-lens modeling software suite optimized for heterogeneous computing\ndevices, especially GPUs. Existing microlensing codes have the issue of\ncatastrophic cancellation that undermines the numerical stability and\nprecision, and Twinkle resolves them by refining the coefficients of the\nbinary-lens equation. We also devise an improved method for robustly\nidentifying ghost images, thereby enhancing computational reliability. We have\nadvanced the state of the art by optimizing Twinkle specifically for\nheterogeneous computing devices by taking into account the unique task and\ncache memory dispatching patterns of GPUs, while the compatibility with the\ntraditional computing architectures of CPUs is still maintained. Twinkle has\ndemonstrated an acceleration of approximately 2 orders of magnitude (>~100\ntimes) on contemporary GPUs. The enhancement in computational speed of Twinkle\nwill translate to the delivery of accurate and highly efficient data analysis\nfor ongoing and upcoming microlensing projects. Both GPU and CPU versions of\nTwinkle are open-source and publicly available.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the rapidly increasing rate of microlensing planet detections,\nmicrolensing modeling software faces significant challenges in computation\nefficiency. Here, we develop the Twinkle code, an efficient and robust\nbinary-lens modeling software suite optimized for heterogeneous computing\ndevices, especially GPUs. Existing microlensing codes have the issue of\ncatastrophic cancellation that undermines the numerical stability and\nprecision, and Twinkle resolves them by refining the coefficients of the\nbinary-lens equation. We also devise an improved method for robustly\nidentifying ghost images, thereby enhancing computational reliability. We have\nadvanced the state of the art by optimizing Twinkle specifically for\nheterogeneous computing devices by taking into account the unique task and\ncache memory dispatching patterns of GPUs, while the compatibility with the\ntraditional computing architectures of CPUs is still maintained. Twinkle has\ndemonstrated an acceleration of approximately 2 orders of magnitude (>~100\ntimes) on contemporary GPUs. The enhancement in computational speed of Twinkle\nwill translate to the delivery of accurate and highly efficient data analysis\nfor ongoing and upcoming microlensing projects. Both GPU and CPU versions of\nTwinkle are open-source and publicly available."
                },
                "authors": [
                    {
                        "name": "Suwei Wang"
                    },
                    {
                        "name": "Lile Wang"
                    },
                    {
                        "name": "Subo Dong"
                    }
                ],
                "author_detail": {
                    "name": "Subo Dong"
                },
                "author": "Subo Dong",
                "arxiv_doi": "10.3847/1538-4365/ad9b8d",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.3847/1538-4365/ad9b8d",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2501.03322v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.03322v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "Accepted by ApJS, GitHub link:\n  https://github.com/AsterLight0626/Twinkle",
                "arxiv_journal_ref": "Astrophys. j., suppl. ser. 276 (2025) 40",
                "arxiv_primary_category": {
                    "term": "astro-ph.IM",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.IM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "astro-ph.EP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "astro-ph.GA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "astro-ph.SR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.19919v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.19919v2",
                "updated": "2025-01-06T15:59:23Z",
                "updated_parsed": [
                    2025,
                    1,
                    6,
                    15,
                    59,
                    23,
                    0,
                    6,
                    0
                ],
                "published": "2024-12-27T20:47:23Z",
                "published_parsed": [
                    2024,
                    12,
                    27,
                    20,
                    47,
                    23,
                    4,
                    362,
                    0
                ],
                "title": "Direct Comparison of Magnetic Penetration Depth in Kagome\n  Superconductors AV$_3$Sb$_5$ (A = Cs, K, Rb)",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Direct Comparison of Magnetic Penetration Depth in Kagome\n  Superconductors AV$_3$Sb$_5$ (A = Cs, K, Rb)"
                },
                "summary": "We report measurements of the local temperature-dependent penetration depth,\n$\\lambda(T)$, in the Kagome superconductors AV$_3$Sb$_5$ (A = Cs, K, Rb) using\nscanning superconducting quantum interference device (SQUID) microscopy. Our\nresults suggest that the superconducting order in all three compounds is fully\ngapped, in contrast to reports of nodal superconductivity in KV$_3$Sb$_5$ and\nRbV$_3$Sb$_5$. Analysis of the temperature-dependent superfluid density,\n$\\rho_s(T)$, shows deviations from the behavior expected for a single isotropic\ngap, but the data are well described by models incorporating either a single\nanisotropic gap or two isotropic gaps. Notably, the temperature dependences of\n$\\lambda(T)$ and $\\rho_s(T)$ in KV$_3$Sb$_5$ and RbV$_3$Sb$_5$ are\nqualitatively more similar to each other than to CsV$_3$Sb$_5$, consistent with\nthe superconducting phase reflecting features of the normal-state band\nstructure. Our findings provide a direct comparison of the superconducting\nproperties across the AV$_3$Sb$_5$ family.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We report measurements of the local temperature-dependent penetration depth,\n$\\lambda(T)$, in the Kagome superconductors AV$_3$Sb$_5$ (A = Cs, K, Rb) using\nscanning superconducting quantum interference device (SQUID) microscopy. Our\nresults suggest that the superconducting order in all three compounds is fully\ngapped, in contrast to reports of nodal superconductivity in KV$_3$Sb$_5$ and\nRbV$_3$Sb$_5$. Analysis of the temperature-dependent superfluid density,\n$\\rho_s(T)$, shows deviations from the behavior expected for a single isotropic\ngap, but the data are well described by models incorporating either a single\nanisotropic gap or two isotropic gaps. Notably, the temperature dependences of\n$\\lambda(T)$ and $\\rho_s(T)$ in KV$_3$Sb$_5$ and RbV$_3$Sb$_5$ are\nqualitatively more similar to each other than to CsV$_3$Sb$_5$, consistent with\nthe superconducting phase reflecting features of the normal-state band\nstructure. Our findings provide a direct comparison of the superconducting\nproperties across the AV$_3$Sb$_5$ family."
                },
                "authors": [
                    {
                        "name": "Austin Kaczmarek"
                    },
                    {
                        "name": "Andrea Capa Salinas"
                    },
                    {
                        "name": "Stephen D. Wilson"
                    },
                    {
                        "name": "Katja C. Nowack"
                    }
                ],
                "author_detail": {
                    "name": "Katja C. Nowack"
                },
                "author": "Katja C. Nowack",
                "arxiv_comment": "5 pages, 3 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.19919v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.19919v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cond-mat.supr-con",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cond-mat.supr-con",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.02803v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.02803v1",
                "updated": "2025-01-06T06:44:13Z",
                "updated_parsed": [
                    2025,
                    1,
                    6,
                    6,
                    44,
                    13,
                    0,
                    6,
                    0
                ],
                "published": "2025-01-06T06:44:13Z",
                "published_parsed": [
                    2025,
                    1,
                    6,
                    6,
                    44,
                    13,
                    0,
                    6,
                    0
                ],
                "title": "Enhancing Lifelong Multi-Agent Path Finding with Cache Mechanism",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Enhancing Lifelong Multi-Agent Path Finding with Cache Mechanism"
                },
                "summary": "Multi-Agent Path Finding (MAPF), which focuses on finding collision-free\npaths for multiple robots, is crucial in autonomous warehouse operations.\nLifelong MAPF (L-MAPF), where agents are continuously reassigned new targets\nupon completing their current tasks, offers a more realistic approximation of\nreal-world warehouse scenarios. While cache storage systems can enhance\nefficiency and reduce operational costs, existing approaches primarily rely on\nexpectations and mathematical models, often without adequately addressing the\nchallenges of multi-robot planning and execution. In this paper, we introduce a\nnovel mechanism called Lifelong MAPF with Cache Mechanism (L-MAPF-CM), which\nintegrates high-level cache storage with low-level path planning. We have\ninvolved a new type of map grid called cache for temporary item storage.\nAdditionally, we involved a task assigner (TA) with a locking mechanism to\nbridge the gap between the new cache grid and L-MAPF algorithm. The TA\ndynamically allocates target locations to agents based on their status in\nvarious scenarios. We evaluated L-MAPF-CM using different cache replacement\npolicies and task distributions. L-MAPF-CM has demonstrated performance\nimprovements particularly with high cache hit rates and smooth traffic\nconditions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-Agent Path Finding (MAPF), which focuses on finding collision-free\npaths for multiple robots, is crucial in autonomous warehouse operations.\nLifelong MAPF (L-MAPF), where agents are continuously reassigned new targets\nupon completing their current tasks, offers a more realistic approximation of\nreal-world warehouse scenarios. While cache storage systems can enhance\nefficiency and reduce operational costs, existing approaches primarily rely on\nexpectations and mathematical models, often without adequately addressing the\nchallenges of multi-robot planning and execution. In this paper, we introduce a\nnovel mechanism called Lifelong MAPF with Cache Mechanism (L-MAPF-CM), which\nintegrates high-level cache storage with low-level path planning. We have\ninvolved a new type of map grid called cache for temporary item storage.\nAdditionally, we involved a task assigner (TA) with a locking mechanism to\nbridge the gap between the new cache grid and L-MAPF algorithm. The TA\ndynamically allocates target locations to agents based on their status in\nvarious scenarios. We evaluated L-MAPF-CM using different cache replacement\npolicies and task distributions. L-MAPF-CM has demonstrated performance\nimprovements particularly with high cache hit rates and smooth traffic\nconditions."
                },
                "authors": [
                    {
                        "name": "Yimin Tang"
                    },
                    {
                        "name": "Zhenghong Yu"
                    },
                    {
                        "name": "Yi Zheng"
                    },
                    {
                        "name": "T. K. Satish Kumar"
                    },
                    {
                        "name": "Jiaoyang Li"
                    },
                    {
                        "name": "Sven Koenig"
                    }
                ],
                "author_detail": {
                    "name": "Sven Koenig"
                },
                "author": "Sven Koenig",
                "arxiv_comment": "arXiv admin note: substantial text overlap with arXiv:2403.13421",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.02803v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.02803v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.07772v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.07772v2",
                "updated": "2025-01-06T01:26:42Z",
                "updated_parsed": [
                    2025,
                    1,
                    6,
                    1,
                    26,
                    42,
                    0,
                    6,
                    0
                ],
                "published": "2024-12-10T18:59:50Z",
                "published_parsed": [
                    2024,
                    12,
                    10,
                    18,
                    59,
                    50,
                    1,
                    345,
                    0
                ],
                "title": "From Slow Bidirectional to Fast Autoregressive Video Diffusion Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "From Slow Bidirectional to Fast Autoregressive Video Diffusion Models"
                },
                "summary": "Current video diffusion models achieve impressive generation quality but\nstruggle in interactive applications due to bidirectional attention\ndependencies. The generation of a single frame requires the model to process\nthe entire sequence, including the future. We address this limitation by\nadapting a pretrained bidirectional diffusion transformer to an autoregressive\ntransformer that generates frames on-the-fly. To further reduce latency, we\nextend distribution matching distillation (DMD) to videos, distilling 50-step\ndiffusion model into a 4-step generator. To enable stable and high-quality\ndistillation, we introduce a student initialization scheme based on teacher's\nODE trajectories, as well as an asymmetric distillation strategy that\nsupervises a causal student model with a bidirectional teacher. This approach\neffectively mitigates error accumulation in autoregressive generation, allowing\nlong-duration video synthesis despite training on short clips. Our model\nachieves a total score of 84.27 on the VBench-Long benchmark, surpassing all\nprevious video generation models. It enables fast streaming generation of\nhigh-quality videos at 9.4 FPS on a single GPU thanks to KV caching. Our\napproach also enables streaming video-to-video translation, image-to-video, and\ndynamic prompting in a zero-shot manner. We will release the code based on an\nopen-source model in the future.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Current video diffusion models achieve impressive generation quality but\nstruggle in interactive applications due to bidirectional attention\ndependencies. The generation of a single frame requires the model to process\nthe entire sequence, including the future. We address this limitation by\nadapting a pretrained bidirectional diffusion transformer to an autoregressive\ntransformer that generates frames on-the-fly. To further reduce latency, we\nextend distribution matching distillation (DMD) to videos, distilling 50-step\ndiffusion model into a 4-step generator. To enable stable and high-quality\ndistillation, we introduce a student initialization scheme based on teacher's\nODE trajectories, as well as an asymmetric distillation strategy that\nsupervises a causal student model with a bidirectional teacher. This approach\neffectively mitigates error accumulation in autoregressive generation, allowing\nlong-duration video synthesis despite training on short clips. Our model\nachieves a total score of 84.27 on the VBench-Long benchmark, surpassing all\nprevious video generation models. It enables fast streaming generation of\nhigh-quality videos at 9.4 FPS on a single GPU thanks to KV caching. Our\napproach also enables streaming video-to-video translation, image-to-video, and\ndynamic prompting in a zero-shot manner. We will release the code based on an\nopen-source model in the future."
                },
                "authors": [
                    {
                        "name": "Tianwei Yin"
                    },
                    {
                        "name": "Qiang Zhang"
                    },
                    {
                        "name": "Richard Zhang"
                    },
                    {
                        "name": "William T. Freeman"
                    },
                    {
                        "name": "Fredo Durand"
                    },
                    {
                        "name": "Eli Shechtman"
                    },
                    {
                        "name": "Xun Huang"
                    }
                ],
                "author_detail": {
                    "name": "Xun Huang"
                },
                "author": "Xun Huang",
                "arxiv_comment": "Project Page: https://causvid.github.io/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.07772v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.07772v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.20504v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.20504v2",
                "updated": "2025-01-05T14:11:48Z",
                "updated_parsed": [
                    2025,
                    1,
                    5,
                    14,
                    11,
                    48,
                    6,
                    5,
                    0
                ],
                "published": "2024-12-29T15:42:24Z",
                "published_parsed": [
                    2024,
                    12,
                    29,
                    15,
                    42,
                    24,
                    6,
                    364,
                    0
                ],
                "title": "ReTaKe: Reducing Temporal and Knowledge Redundancy for Long Video\n  Understanding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ReTaKe: Reducing Temporal and Knowledge Redundancy for Long Video\n  Understanding"
                },
                "summary": "Video Large Language Models (VideoLLMs) have achieved remarkable progress in\nvideo understanding. However, existing VideoLLMs often inherit the limitations\nof their backbone LLMs in handling long sequences, leading to challenges for\nlong video understanding. Common solutions either simply uniformly sample\nvideos' frames or compress visual tokens, which focus primarily on low-level\ntemporal visual redundancy, overlooking high-level knowledge redundancy. This\nlimits the achievable compression rate with minimal loss. To this end. we\nintroduce a training-free method, $\\textbf{ReTaKe}$, containing two novel\nmodules DPSelect and PivotKV, to jointly model and reduce both temporal visual\nredundancy and knowledge redundancy for long video understanding. Specifically,\nDPSelect identifies keyframes with local maximum peak distance based on their\nvisual features, which are closely aligned with human video perception. PivotKV\nemploys the obtained keyframes as pivots and conducts KV-Cache compression for\nthe non-pivot tokens with low attention scores, which are derived from the\nlearned prior knowledge of LLMs. Experiments on benchmarks VideoMME, MLVU, and\nLVBench, show that ReTaKe can support 4x longer video sequences with minimal\nperformance loss (<1%) and outperform all similar-size VideoLLMs with 3%-5%,\neven surpassing or on par with much larger ones. Our code is available at\nhttps://github.com/SCZwangxiao/video-ReTaKe",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Video Large Language Models (VideoLLMs) have achieved remarkable progress in\nvideo understanding. However, existing VideoLLMs often inherit the limitations\nof their backbone LLMs in handling long sequences, leading to challenges for\nlong video understanding. Common solutions either simply uniformly sample\nvideos' frames or compress visual tokens, which focus primarily on low-level\ntemporal visual redundancy, overlooking high-level knowledge redundancy. This\nlimits the achievable compression rate with minimal loss. To this end. we\nintroduce a training-free method, $\\textbf{ReTaKe}$, containing two novel\nmodules DPSelect and PivotKV, to jointly model and reduce both temporal visual\nredundancy and knowledge redundancy for long video understanding. Specifically,\nDPSelect identifies keyframes with local maximum peak distance based on their\nvisual features, which are closely aligned with human video perception. PivotKV\nemploys the obtained keyframes as pivots and conducts KV-Cache compression for\nthe non-pivot tokens with low attention scores, which are derived from the\nlearned prior knowledge of LLMs. Experiments on benchmarks VideoMME, MLVU, and\nLVBench, show that ReTaKe can support 4x longer video sequences with minimal\nperformance loss (<1%) and outperform all similar-size VideoLLMs with 3%-5%,\neven surpassing or on par with much larger ones. Our code is available at\nhttps://github.com/SCZwangxiao/video-ReTaKe"
                },
                "authors": [
                    {
                        "name": "Xiao Wang"
                    },
                    {
                        "name": "Qingyi Si"
                    },
                    {
                        "name": "Jianlong Wu"
                    },
                    {
                        "name": "Shiyu Zhu"
                    },
                    {
                        "name": "Li Cao"
                    },
                    {
                        "name": "Liqiang Nie"
                    }
                ],
                "author_detail": {
                    "name": "Liqiang Nie"
                },
                "author": "Liqiang Nie",
                "arxiv_comment": "Update performance in MLVU-dev and LVBench",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.20504v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.20504v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.02524v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.02524v1",
                "updated": "2025-01-05T12:51:08Z",
                "updated_parsed": [
                    2025,
                    1,
                    5,
                    12,
                    51,
                    8,
                    6,
                    5,
                    0
                ],
                "published": "2025-01-05T12:51:08Z",
                "published_parsed": [
                    2025,
                    1,
                    5,
                    12,
                    51,
                    8,
                    6,
                    5,
                    0
                ],
                "title": "A Full-System Simulation Framework for CXL-Based SSD Memory System",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Full-System Simulation Framework for CXL-Based SSD Memory System"
                },
                "summary": "Compute eXpress Link (CXL) is a promising technology for memory\ndisaggregation and expansion. Especially, CXL makes it more effectively for\nlarge-capacity storage devices such as Solid State Drive (SSD) to be deployed\nin the memory pool. However, CXL-based SSDs are still in early stages,\nnecessitating the development of reliable simulation tools. In this paper, we\npropose CXL-SSD-Sim, the first open-source full-system simulator designed to\nsimulate CXL-based SSD memory system. Constructed on the foundation of gem5 and\nSimpleSSD, CXL-SSD-Sim extends an high fidelity SSD memory expander model along\nwith the corresponding device driver. In addition, CXL-SSD-Sim models a DRAM\nlayer as a caching mechanism for the SSD, meticulously engineered to counteract\nlatency issues inherent to CXL-based SSD memory access. Experiments are\nperformed among five different memory devices with CXL-SSD-Sim in aspect of\nlatency, bandwidth and real-world benchmark performance. These experiments\nserve to underscore the efficacy of our simulation tool in providing a\ncomprehensive analysis of CXL-based SSD memory systems. The CXL-SSD-Sim\nsimulator is available at https://github.com/WangYaohuii/CXL-SSD-Sim.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Compute eXpress Link (CXL) is a promising technology for memory\ndisaggregation and expansion. Especially, CXL makes it more effectively for\nlarge-capacity storage devices such as Solid State Drive (SSD) to be deployed\nin the memory pool. However, CXL-based SSDs are still in early stages,\nnecessitating the development of reliable simulation tools. In this paper, we\npropose CXL-SSD-Sim, the first open-source full-system simulator designed to\nsimulate CXL-based SSD memory system. Constructed on the foundation of gem5 and\nSimpleSSD, CXL-SSD-Sim extends an high fidelity SSD memory expander model along\nwith the corresponding device driver. In addition, CXL-SSD-Sim models a DRAM\nlayer as a caching mechanism for the SSD, meticulously engineered to counteract\nlatency issues inherent to CXL-based SSD memory access. Experiments are\nperformed among five different memory devices with CXL-SSD-Sim in aspect of\nlatency, bandwidth and real-world benchmark performance. These experiments\nserve to underscore the efficacy of our simulation tool in providing a\ncomprehensive analysis of CXL-based SSD memory systems. The CXL-SSD-Sim\nsimulator is available at https://github.com/WangYaohuii/CXL-SSD-Sim."
                },
                "authors": [
                    {
                        "name": "Yaohui Wang"
                    },
                    {
                        "name": "Zicong Wang"
                    },
                    {
                        "name": "Fanfeng Meng"
                    },
                    {
                        "name": "Yanjing Wang"
                    },
                    {
                        "name": "Yang Ou"
                    },
                    {
                        "name": "Lizhou Wu"
                    },
                    {
                        "name": "Wentao Hong"
                    },
                    {
                        "name": "Xuran Ge"
                    },
                    {
                        "name": "Jijun Cao"
                    }
                ],
                "author_detail": {
                    "name": "Jijun Cao"
                },
                "author": "Jijun Cao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.02524v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.02524v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.02469v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.02469v1",
                "updated": "2025-01-05T07:41:53Z",
                "updated_parsed": [
                    2025,
                    1,
                    5,
                    7,
                    41,
                    53,
                    6,
                    5,
                    0
                ],
                "published": "2025-01-05T07:41:53Z",
                "published_parsed": [
                    2025,
                    1,
                    5,
                    7,
                    41,
                    53,
                    6,
                    5,
                    0
                ],
                "title": "LoRaConnect: Unlocking HTTP Potential on LoRa Backbones for Remote Areas\n  and Ad-Hoc Networks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LoRaConnect: Unlocking HTTP Potential on LoRa Backbones for Remote Areas\n  and Ad-Hoc Networks"
                },
                "summary": "The minimal infrastructure requirements of LoRa make it suitable for\ndeployments in remote and disaster-stricken areas. Concomitantly, the modern\nera is witnessing the proliferation of web applications in all aspects of human\nlife, including IoT and other network services. Contemporary IoT and network\nsolutions heavily rely on web applications to render services. However, despite\nthe recent research and development pivoted around LoRa, there is still a lack\nof studies focusing on web application access over LoRa networks. Specifically,\ntechnical challenges like payload size limitation, low data rate, and\ncontentions in multi-user setups limit the applicability of LoRa for web\napplications. Hence, we propose LoRaWeb, which enables web access over LoRa\nnetworks. The LoRaWeb hardware tethers a WiFi hotspot to which the client\ndevices connect and access the web pages using a web browser. LoRa backbone of\nthe network handles the web page transmission from the requester and receiver\ndevices. LoRaWeb implements a synchronization procedure to address the\naforementioned challenges for effective message exchange between requesters and\nresponders. The system implements a caching mechanism to reduce latency and\ncontention. Additionally, it implements a message-slicing mechanism in the\napplication layer to overcome the hardware limitations on the message length.\nThe actual hardware-based implementation results indicate seamless deployment,\nand the results indicate an average access time of ~$0.95 S$ for a $1.5 KB$ and\n~$6 S$ for a $10 KB$ size web page.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The minimal infrastructure requirements of LoRa make it suitable for\ndeployments in remote and disaster-stricken areas. Concomitantly, the modern\nera is witnessing the proliferation of web applications in all aspects of human\nlife, including IoT and other network services. Contemporary IoT and network\nsolutions heavily rely on web applications to render services. However, despite\nthe recent research and development pivoted around LoRa, there is still a lack\nof studies focusing on web application access over LoRa networks. Specifically,\ntechnical challenges like payload size limitation, low data rate, and\ncontentions in multi-user setups limit the applicability of LoRa for web\napplications. Hence, we propose LoRaWeb, which enables web access over LoRa\nnetworks. The LoRaWeb hardware tethers a WiFi hotspot to which the client\ndevices connect and access the web pages using a web browser. LoRa backbone of\nthe network handles the web page transmission from the requester and receiver\ndevices. LoRaWeb implements a synchronization procedure to address the\naforementioned challenges for effective message exchange between requesters and\nresponders. The system implements a caching mechanism to reduce latency and\ncontention. Additionally, it implements a message-slicing mechanism in the\napplication layer to overcome the hardware limitations on the message length.\nThe actual hardware-based implementation results indicate seamless deployment,\nand the results indicate an average access time of ~$0.95 S$ for a $1.5 KB$ and\n~$6 S$ for a $10 KB$ size web page."
                },
                "authors": [
                    {
                        "name": "Atonu Ghosh"
                    },
                    {
                        "name": "Sudip Misra"
                    }
                ],
                "author_detail": {
                    "name": "Sudip Misra"
                },
                "author": "Sudip Misra",
                "arxiv_comment": "11 pages, 15 figures, and 2 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.02469v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.02469v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.01805v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.01805v1",
                "updated": "2025-01-03T13:32:57Z",
                "updated_parsed": [
                    2025,
                    1,
                    3,
                    13,
                    32,
                    57,
                    4,
                    3,
                    0
                ],
                "published": "2025-01-03T13:32:57Z",
                "published_parsed": [
                    2025,
                    1,
                    3,
                    13,
                    32,
                    57,
                    4,
                    3,
                    0
                ],
                "title": "End-to-End Long Document Summarization using Gradient Caching",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "End-to-End Long Document Summarization using Gradient Caching"
                },
                "summary": "Training transformer-based encoder-decoder models for long document\nsummarization poses a significant challenge due to the quadratic memory\nconsumption during training. Several approaches have been proposed to extend\nthe input length at test time, but training with these approaches is still\ndifficult, requiring truncation of input documents and causing a mismatch\nbetween training and test conditions. In this work, we propose CachED (Gradient\n$\\textbf{Cach}$ing for $\\textbf{E}$ncoder-$\\textbf{D}$ecoder models), an\napproach that enables end-to-end training of existing transformer-based\nencoder-decoder models, using the entire document without truncation.\nSpecifically, we apply non-overlapping sliding windows to input documents,\nfollowed by fusion in decoder. During backpropagation, the gradients are cached\nat the decoder and are passed through the encoder in chunks by re-computing the\nhidden vectors, similar to gradient checkpointing. In the experiments on long\ndocument summarization, we extend BART to CachED BART, processing more than\n500K tokens during training and achieving superior performance without using\nany additional parameters.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Training transformer-based encoder-decoder models for long document\nsummarization poses a significant challenge due to the quadratic memory\nconsumption during training. Several approaches have been proposed to extend\nthe input length at test time, but training with these approaches is still\ndifficult, requiring truncation of input documents and causing a mismatch\nbetween training and test conditions. In this work, we propose CachED (Gradient\n$\\textbf{Cach}$ing for $\\textbf{E}$ncoder-$\\textbf{D}$ecoder models), an\napproach that enables end-to-end training of existing transformer-based\nencoder-decoder models, using the entire document without truncation.\nSpecifically, we apply non-overlapping sliding windows to input documents,\nfollowed by fusion in decoder. During backpropagation, the gradients are cached\nat the decoder and are passed through the encoder in chunks by re-computing the\nhidden vectors, similar to gradient checkpointing. In the experiments on long\ndocument summarization, we extend BART to CachED BART, processing more than\n500K tokens during training and achieving superior performance without using\nany additional parameters."
                },
                "authors": [
                    {
                        "name": "Rohit Saxena"
                    },
                    {
                        "name": "Hao Tang"
                    },
                    {
                        "name": "Frank Keller"
                    }
                ],
                "author_detail": {
                    "name": "Frank Keller"
                },
                "author": "Frank Keller",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.01805v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.01805v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.01792v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.01792v1",
                "updated": "2025-01-03T12:51:37Z",
                "updated_parsed": [
                    2025,
                    1,
                    3,
                    12,
                    51,
                    37,
                    4,
                    3,
                    0
                ],
                "published": "2025-01-03T12:51:37Z",
                "published_parsed": [
                    2025,
                    1,
                    3,
                    12,
                    51,
                    37,
                    4,
                    3,
                    0
                ],
                "title": "Efficient LLM Inference with Activation Checkpointing and Hybrid Caching",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient LLM Inference with Activation Checkpointing and Hybrid Caching"
                },
                "summary": "Recent large language models (LLMs) with enormous model sizes use many GPUs\nto meet memory capacity requirements incurring substantial costs for token\ngeneration. To provide cost-effective LLM inference with relaxed latency\nconstraints, extensive research has focused on expanding GPU memory by\nleveraging the host memory. However, LLM inference engines that utilize the\nhost memory often face underutilization of GPU compute units, as a considerable\nportion of inference time is spent in loading the model onto the GPU via\nhost-GPU interconnect. To tackle these challenges of the host memory offloading\nfor LLM, we introduce HybridServe, an LLM inference system with activation\ncheckpointing based on activation caching. The activation cache stores\nactivation checkpoints generated during intermediate inference stages, allowing\nthe fast recomputation of KV cache while model parameters are transferred to\nGPU from host memory. Unlike conventional methods that recompute the KV cache\nfrom scratch using token IDs, the activation cache allows bypassing projection\nand FFN operations. To balance between the activation recomputation and\nparameter loading overhead, this study proposes a KV-activation hybrid caching\nscheme which finds the best ratio of the key-value and activation caches to\nadjust the recomputation time. Our system achieves 2.19x throughput improvement\nover the state-of-the-art prior work for offloading both model weights and KV\ncache.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent large language models (LLMs) with enormous model sizes use many GPUs\nto meet memory capacity requirements incurring substantial costs for token\ngeneration. To provide cost-effective LLM inference with relaxed latency\nconstraints, extensive research has focused on expanding GPU memory by\nleveraging the host memory. However, LLM inference engines that utilize the\nhost memory often face underutilization of GPU compute units, as a considerable\nportion of inference time is spent in loading the model onto the GPU via\nhost-GPU interconnect. To tackle these challenges of the host memory offloading\nfor LLM, we introduce HybridServe, an LLM inference system with activation\ncheckpointing based on activation caching. The activation cache stores\nactivation checkpoints generated during intermediate inference stages, allowing\nthe fast recomputation of KV cache while model parameters are transferred to\nGPU from host memory. Unlike conventional methods that recompute the KV cache\nfrom scratch using token IDs, the activation cache allows bypassing projection\nand FFN operations. To balance between the activation recomputation and\nparameter loading overhead, this study proposes a KV-activation hybrid caching\nscheme which finds the best ratio of the key-value and activation caches to\nadjust the recomputation time. Our system achieves 2.19x throughput improvement\nover the state-of-the-art prior work for offloading both model weights and KV\ncache."
                },
                "authors": [
                    {
                        "name": "Sanghyeon Lee"
                    },
                    {
                        "name": "Hongbeen Kim"
                    },
                    {
                        "name": "Soojin Hwang"
                    },
                    {
                        "name": "Guseul Heo"
                    },
                    {
                        "name": "Minwoo Noh"
                    },
                    {
                        "name": "Jaehyuk Huh"
                    }
                ],
                "author_detail": {
                    "name": "Jaehyuk Huh"
                },
                "author": "Jaehyuk Huh",
                "arxiv_comment": "14 pages, 15 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.01792v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.01792v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.01424v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.01424v1",
                "updated": "2025-01-02T18:59:44Z",
                "updated_parsed": [
                    2025,
                    1,
                    2,
                    18,
                    59,
                    44,
                    3,
                    2,
                    0
                ],
                "published": "2025-01-02T18:59:44Z",
                "published_parsed": [
                    2025,
                    1,
                    2,
                    18,
                    59,
                    44,
                    3,
                    2,
                    0
                ],
                "title": "Object-level Visual Prompts for Compositional Image Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Object-level Visual Prompts for Compositional Image Generation"
                },
                "summary": "We introduce a method for composing object-level visual prompts within a\ntext-to-image diffusion model. Our approach addresses the task of generating\nsemantically coherent compositions across diverse scenes and styles, similar to\nthe versatility and expressiveness offered by text prompts. A key challenge in\nthis task is to preserve the identity of the objects depicted in the input\nvisual prompts, while also generating diverse compositions across different\nimages. To address this challenge, we introduce a new KV-mixed cross-attention\nmechanism, in which keys and values are learned from distinct visual\nrepresentations. The keys are derived from an encoder with a small bottleneck\nfor layout control, whereas the values come from a larger bottleneck encoder\nthat captures fine-grained appearance details. By mixing keys and values from\nthese complementary sources, our model preserves the identity of the visual\nprompts while supporting flexible variations in object arrangement, pose, and\ncomposition. During inference, we further propose object-level compositional\nguidance to improve the method's identity preservation and layout correctness.\nResults show that our technique produces diverse scene compositions that\npreserve the unique characteristics of each visual prompt, expanding the\ncreative potential of text-to-image generation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce a method for composing object-level visual prompts within a\ntext-to-image diffusion model. Our approach addresses the task of generating\nsemantically coherent compositions across diverse scenes and styles, similar to\nthe versatility and expressiveness offered by text prompts. A key challenge in\nthis task is to preserve the identity of the objects depicted in the input\nvisual prompts, while also generating diverse compositions across different\nimages. To address this challenge, we introduce a new KV-mixed cross-attention\nmechanism, in which keys and values are learned from distinct visual\nrepresentations. The keys are derived from an encoder with a small bottleneck\nfor layout control, whereas the values come from a larger bottleneck encoder\nthat captures fine-grained appearance details. By mixing keys and values from\nthese complementary sources, our model preserves the identity of the visual\nprompts while supporting flexible variations in object arrangement, pose, and\ncomposition. During inference, we further propose object-level compositional\nguidance to improve the method's identity preservation and layout correctness.\nResults show that our technique produces diverse scene compositions that\npreserve the unique characteristics of each visual prompt, expanding the\ncreative potential of text-to-image generation."
                },
                "authors": [
                    {
                        "name": "Gaurav Parmar"
                    },
                    {
                        "name": "Or Patashnik"
                    },
                    {
                        "name": "Kuan-Chieh Wang"
                    },
                    {
                        "name": "Daniil Ostashev"
                    },
                    {
                        "name": "Srinivasa Narasimhan"
                    },
                    {
                        "name": "Jun-Yan Zhu"
                    },
                    {
                        "name": "Daniel Cohen-Or"
                    },
                    {
                        "name": "Kfir Aberman"
                    }
                ],
                "author_detail": {
                    "name": "Kfir Aberman"
                },
                "author": "Kfir Aberman",
                "arxiv_comment": "Project: https://snap-research.github.io/visual-composer/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.01424v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.01424v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.GR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.01039v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.01039v1",
                "updated": "2025-01-02T03:41:32Z",
                "updated_parsed": [
                    2025,
                    1,
                    2,
                    3,
                    41,
                    32,
                    3,
                    2,
                    0
                ],
                "published": "2025-01-02T03:41:32Z",
                "published_parsed": [
                    2025,
                    1,
                    2,
                    3,
                    41,
                    32,
                    3,
                    2,
                    0
                ],
                "title": "MSWA: Refining Local Attention with Multi-ScaleWindow Attention",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MSWA: Refining Local Attention with Multi-ScaleWindow Attention"
                },
                "summary": "Transformer-based LLMs have achieved exceptional performance across a wide\nrange of NLP tasks. However, the standard self-attention mechanism suffers from\nquadratic time complexity and linearly increased cache size. Sliding window\nattention (SWA) solves this problem by restricting the attention range to a\nfixed-size local context window. Nevertheless, SWA employs a uniform window\nsize for each head in each layer, making it inefficient in capturing context of\nvarying scales. To mitigate this limitation, we propose Multi-Scale Window\nAttention (MSWA) which applies diverse window sizes across heads and layers in\nthe Transformer. It not only allows for different window sizes among heads\nwithin the same layer but also progressively increases window size allocation\nfrom shallow to deep layers, thus enabling the model to capture contextual\ninformation with different lengths and distances. Experimental results on\nlanguage modeling and common-sense reasoning tasks substantiate that MSWA\noutperforms traditional local attention in both effectiveness and efficiency.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Transformer-based LLMs have achieved exceptional performance across a wide\nrange of NLP tasks. However, the standard self-attention mechanism suffers from\nquadratic time complexity and linearly increased cache size. Sliding window\nattention (SWA) solves this problem by restricting the attention range to a\nfixed-size local context window. Nevertheless, SWA employs a uniform window\nsize for each head in each layer, making it inefficient in capturing context of\nvarying scales. To mitigate this limitation, we propose Multi-Scale Window\nAttention (MSWA) which applies diverse window sizes across heads and layers in\nthe Transformer. It not only allows for different window sizes among heads\nwithin the same layer but also progressively increases window size allocation\nfrom shallow to deep layers, thus enabling the model to capture contextual\ninformation with different lengths and distances. Experimental results on\nlanguage modeling and common-sense reasoning tasks substantiate that MSWA\noutperforms traditional local attention in both effectiveness and efficiency."
                },
                "authors": [
                    {
                        "name": "Yixing Xu"
                    },
                    {
                        "name": "Shivank Nag"
                    },
                    {
                        "name": "Dong Li"
                    },
                    {
                        "name": "Lu Tian"
                    },
                    {
                        "name": "Emad Barsoum"
                    }
                ],
                "author_detail": {
                    "name": "Emad Barsoum"
                },
                "author": "Emad Barsoum",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.01039v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.01039v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.19442v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.19442v2",
                "updated": "2025-01-02T03:40:15Z",
                "updated_parsed": [
                    2025,
                    1,
                    2,
                    3,
                    40,
                    15,
                    3,
                    2,
                    0
                ],
                "published": "2024-12-27T04:17:57Z",
                "published_parsed": [
                    2024,
                    12,
                    27,
                    4,
                    17,
                    57,
                    4,
                    362,
                    0
                ],
                "title": "A Survey on Large Language Model Acceleration based on KV Cache\n  Management",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Survey on Large Language Model Acceleration based on KV Cache\n  Management"
                },
                "summary": "Large Language Models (LLMs) have revolutionized a wide range of domains such\nas natural language processing, computer vision, and multi-modal tasks due to\ntheir ability to comprehend context and perform logical reasoning. However, the\ncomputational and memory demands of LLMs, particularly during inference, pose\nsignificant challenges when scaling them to real-world, long-context, and\nreal-time applications. Key-Value (KV) cache management has emerged as a\ncritical optimization technique for accelerating LLM inference by reducing\nredundant computations and improving memory utilization. This survey provides a\ncomprehensive overview of KV cache management strategies for LLM acceleration,\ncategorizing them into token-level, model-level, and system-level\noptimizations. Token-level strategies include KV cache selection, budget\nallocation, merging, quantization, and low-rank decomposition, while\nmodel-level optimizations focus on architectural innovations and attention\nmechanisms to enhance KV reuse. System-level approaches address memory\nmanagement, scheduling, and hardware-aware designs to improve efficiency across\ndiverse computing environments. Additionally, the survey provides an overview\nof both text and multimodal datasets and benchmarks used to evaluate these\nstrategies. By presenting detailed taxonomies and comparative analyses, this\nwork aims to offer useful insights for researchers and practitioners to support\nthe development of efficient and scalable KV cache management techniques,\ncontributing to the practical deployment of LLMs in real-world applications.\nThe curated paper list for KV cache management is in:\n\\href{https://github.com/TreeAI-Lab/Awesome-KV-Cache-Management}{https://github.com/TreeAI-Lab/Awesome-KV-Cache-Management}.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have revolutionized a wide range of domains such\nas natural language processing, computer vision, and multi-modal tasks due to\ntheir ability to comprehend context and perform logical reasoning. However, the\ncomputational and memory demands of LLMs, particularly during inference, pose\nsignificant challenges when scaling them to real-world, long-context, and\nreal-time applications. Key-Value (KV) cache management has emerged as a\ncritical optimization technique for accelerating LLM inference by reducing\nredundant computations and improving memory utilization. This survey provides a\ncomprehensive overview of KV cache management strategies for LLM acceleration,\ncategorizing them into token-level, model-level, and system-level\noptimizations. Token-level strategies include KV cache selection, budget\nallocation, merging, quantization, and low-rank decomposition, while\nmodel-level optimizations focus on architectural innovations and attention\nmechanisms to enhance KV reuse. System-level approaches address memory\nmanagement, scheduling, and hardware-aware designs to improve efficiency across\ndiverse computing environments. Additionally, the survey provides an overview\nof both text and multimodal datasets and benchmarks used to evaluate these\nstrategies. By presenting detailed taxonomies and comparative analyses, this\nwork aims to offer useful insights for researchers and practitioners to support\nthe development of efficient and scalable KV cache management techniques,\ncontributing to the practical deployment of LLMs in real-world applications.\nThe curated paper list for KV cache management is in:\n\\href{https://github.com/TreeAI-Lab/Awesome-KV-Cache-Management}{https://github.com/TreeAI-Lab/Awesome-KV-Cache-Management}."
                },
                "authors": [
                    {
                        "name": "Haoyang Li"
                    },
                    {
                        "name": "Yiming Li"
                    },
                    {
                        "name": "Anxin Tian"
                    },
                    {
                        "name": "Tianhao Tang"
                    },
                    {
                        "name": "Zhanchao Xu"
                    },
                    {
                        "name": "Xuejia Chen"
                    },
                    {
                        "name": "Nicole Hu"
                    },
                    {
                        "name": "Wei Dong"
                    },
                    {
                        "name": "Qing Li"
                    },
                    {
                        "name": "Lei Chen"
                    }
                ],
                "author_detail": {
                    "name": "Lei Chen"
                },
                "author": "Lei Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.19442v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.19442v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.01005v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.01005v1",
                "updated": "2025-01-02T02:02:20Z",
                "updated_parsed": [
                    2025,
                    1,
                    2,
                    2,
                    2,
                    20,
                    3,
                    2,
                    0
                ],
                "published": "2025-01-02T02:02:20Z",
                "published_parsed": [
                    2025,
                    1,
                    2,
                    2,
                    2,
                    20,
                    3,
                    2,
                    0
                ],
                "title": "FlashInfer: Efficient and Customizable Attention Engine for LLM\n  Inference Serving",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FlashInfer: Efficient and Customizable Attention Engine for LLM\n  Inference Serving"
                },
                "summary": "Transformers, driven by attention mechanisms, form the foundation of large\nlanguage models (LLMs). As these models scale up, efficient GPU attention\nkernels become essential for high-throughput and low-latency inference. Diverse\nLLM applications demand flexible and high-performance attention solutions. We\npresent FlashInfer: a customizable and efficient attention engine for LLM\nserving. FlashInfer tackles KV-cache storage heterogeneity using block-sparse\nformat and composable formats to optimize memory access and reduce redundancy.\nIt also offers a customizable attention template, enabling adaptation to\nvarious settings through Just-In-Time (JIT) compilation. Additionally,\nFlashInfer's load-balanced scheduling algorithm adjusts to dynamism of user\nrequests while maintaining compatibility with CUDAGraph which requires static\nconfiguration. FlashInfer have been integrated into leading LLM serving\nframeworks like SGLang, vLLM and MLC-Engine. Comprehensive kernel-level and\nend-to-end evaluations demonstrate FlashInfer's ability to significantly boost\nkernel performance across diverse inference scenarios: compared to\nstate-of-the-art LLM serving solutions, FlashInfer achieve 29-69%\ninter-token-latency reduction compared to compiler backends for LLM serving\nbenchmark, 28-30% latency reduction for long-context inference, and 13-17%\nspeedup for LLM serving with parallel generation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Transformers, driven by attention mechanisms, form the foundation of large\nlanguage models (LLMs). As these models scale up, efficient GPU attention\nkernels become essential for high-throughput and low-latency inference. Diverse\nLLM applications demand flexible and high-performance attention solutions. We\npresent FlashInfer: a customizable and efficient attention engine for LLM\nserving. FlashInfer tackles KV-cache storage heterogeneity using block-sparse\nformat and composable formats to optimize memory access and reduce redundancy.\nIt also offers a customizable attention template, enabling adaptation to\nvarious settings through Just-In-Time (JIT) compilation. Additionally,\nFlashInfer's load-balanced scheduling algorithm adjusts to dynamism of user\nrequests while maintaining compatibility with CUDAGraph which requires static\nconfiguration. FlashInfer have been integrated into leading LLM serving\nframeworks like SGLang, vLLM and MLC-Engine. Comprehensive kernel-level and\nend-to-end evaluations demonstrate FlashInfer's ability to significantly boost\nkernel performance across diverse inference scenarios: compared to\nstate-of-the-art LLM serving solutions, FlashInfer achieve 29-69%\ninter-token-latency reduction compared to compiler backends for LLM serving\nbenchmark, 28-30% latency reduction for long-context inference, and 13-17%\nspeedup for LLM serving with parallel generation."
                },
                "authors": [
                    {
                        "name": "Zihao Ye"
                    },
                    {
                        "name": "Lequn Chen"
                    },
                    {
                        "name": "Ruihang Lai"
                    },
                    {
                        "name": "Wuwei Lin"
                    },
                    {
                        "name": "Yineng Zhang"
                    },
                    {
                        "name": "Stephanie Wang"
                    },
                    {
                        "name": "Tianqi Chen"
                    },
                    {
                        "name": "Baris Kasikci"
                    },
                    {
                        "name": "Vinod Grover"
                    },
                    {
                        "name": "Arvind Krishnamurthy"
                    },
                    {
                        "name": "Luis Ceze"
                    }
                ],
                "author_detail": {
                    "name": "Luis Ceze"
                },
                "author": "Luis Ceze",
                "arxiv_comment": "code available at http://github.com/flashinfer-ai/flashinfer",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.01005v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.01005v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.00946v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.00946v1",
                "updated": "2025-01-01T20:16:27Z",
                "updated_parsed": [
                    2025,
                    1,
                    1,
                    20,
                    16,
                    27,
                    2,
                    1,
                    0
                ],
                "published": "2025-01-01T20:16:27Z",
                "published_parsed": [
                    2025,
                    1,
                    1,
                    20,
                    16,
                    27,
                    2,
                    1,
                    0
                ],
                "title": "Cached Adaptive Token Merging: Dynamic Token Reduction and Redundant\n  Computation Elimination in Diffusion Model",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cached Adaptive Token Merging: Dynamic Token Reduction and Redundant\n  Computation Elimination in Diffusion Model"
                },
                "summary": "Diffusion models have emerged as a promising approach for generating\nhigh-quality, high-dimensional images. Nevertheless, these models are hindered\nby their high computational cost and slow inference, partly due to the\nquadratic computational complexity of the self-attention mechanisms with\nrespect to input size. Various approaches have been proposed to address this\ndrawback. One such approach focuses on reducing the number of tokens fed into\nthe self-attention, known as token merging (ToMe). In our method, which is\ncalled cached adaptive token merging(CA-ToMe), we calculate the similarity\nbetween tokens and then merge the r proportion of the most similar tokens.\nHowever, due to the repetitive patterns observed in adjacent steps and the\nvariation in the frequency of similarities, we aim to enhance this approach by\nimplementing an adaptive threshold for merging tokens and adding a caching\nmechanism that stores similar pairs across several adjacent steps. Empirical\nresults demonstrate that our method operates as a training-free acceleration\nmethod, achieving a speedup factor of 1.24 in the denoising process while\nmaintaining the same FID scores compared to existing approaches.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion models have emerged as a promising approach for generating\nhigh-quality, high-dimensional images. Nevertheless, these models are hindered\nby their high computational cost and slow inference, partly due to the\nquadratic computational complexity of the self-attention mechanisms with\nrespect to input size. Various approaches have been proposed to address this\ndrawback. One such approach focuses on reducing the number of tokens fed into\nthe self-attention, known as token merging (ToMe). In our method, which is\ncalled cached adaptive token merging(CA-ToMe), we calculate the similarity\nbetween tokens and then merge the r proportion of the most similar tokens.\nHowever, due to the repetitive patterns observed in adjacent steps and the\nvariation in the frequency of similarities, we aim to enhance this approach by\nimplementing an adaptive threshold for merging tokens and adding a caching\nmechanism that stores similar pairs across several adjacent steps. Empirical\nresults demonstrate that our method operates as a training-free acceleration\nmethod, achieving a speedup factor of 1.24 in the denoising process while\nmaintaining the same FID scores compared to existing approaches."
                },
                "authors": [
                    {
                        "name": "Omid Saghatchian"
                    },
                    {
                        "name": "Atiyeh Gh. Moghadam"
                    },
                    {
                        "name": "Ahmad Nickabadi"
                    }
                ],
                "author_detail": {
                    "name": "Ahmad Nickabadi"
                },
                "author": "Ahmad Nickabadi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.00946v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.00946v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.21023v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.21023v2",
                "updated": "2024-12-31T20:40:43Z",
                "updated_parsed": [
                    2024,
                    12,
                    31,
                    20,
                    40,
                    43,
                    1,
                    366,
                    0
                ],
                "published": "2024-12-30T15:46:53Z",
                "published_parsed": [
                    2024,
                    12,
                    30,
                    15,
                    46,
                    53,
                    0,
                    365,
                    0
                ],
                "title": "EdgeRAG: Online-Indexed RAG for Edge Devices",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "EdgeRAG: Online-Indexed RAG for Edge Devices"
                },
                "summary": "Deploying Retrieval Augmented Generation (RAG) on resource-constrained edge\ndevices is challenging due to limited memory and processing power. In this\nwork, we propose EdgeRAG which addresses the memory constraint by pruning\nembeddings within clusters and generating embeddings on-demand during\nretrieval. To avoid the latency of generating embeddings for large tail\nclusters, EdgeRAG pre-computes and stores embeddings for these clusters, while\nadaptively caching remaining embeddings to minimize redundant computations and\nfurther optimize latency. The result from BEIR suite shows that EdgeRAG offers\nsignificant latency reduction over the baseline IVF index, but with similar\ngeneration quality while allowing all of our evaluated datasets to fit into the\nmemory.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Deploying Retrieval Augmented Generation (RAG) on resource-constrained edge\ndevices is challenging due to limited memory and processing power. In this\nwork, we propose EdgeRAG which addresses the memory constraint by pruning\nembeddings within clusters and generating embeddings on-demand during\nretrieval. To avoid the latency of generating embeddings for large tail\nclusters, EdgeRAG pre-computes and stores embeddings for these clusters, while\nadaptively caching remaining embeddings to minimize redundant computations and\nfurther optimize latency. The result from BEIR suite shows that EdgeRAG offers\nsignificant latency reduction over the baseline IVF index, but with similar\ngeneration quality while allowing all of our evaluated datasets to fit into the\nmemory."
                },
                "authors": [
                    {
                        "name": "Korakit Seemakhupt"
                    },
                    {
                        "name": "Sihang Liu"
                    },
                    {
                        "name": "Samira Khan"
                    }
                ],
                "author_detail": {
                    "name": "Samira Khan"
                },
                "author": "Samira Khan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.21023v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.21023v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.00375v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.00375v1",
                "updated": "2024-12-31T09:56:40Z",
                "updated_parsed": [
                    2024,
                    12,
                    31,
                    9,
                    56,
                    40,
                    1,
                    366,
                    0
                ],
                "published": "2024-12-31T09:56:40Z",
                "published_parsed": [
                    2024,
                    12,
                    31,
                    9,
                    56,
                    40,
                    1,
                    366,
                    0
                ],
                "title": "Token Pruning for Caching Better: 9 Times Acceleration on Stable\n  Diffusion for Free",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Token Pruning for Caching Better: 9 Times Acceleration on Stable\n  Diffusion for Free"
                },
                "summary": "Stable Diffusion has achieved remarkable success in the field of\ntext-to-image generation, with its powerful generative capabilities and diverse\ngeneration results making a lasting impact. However, its iterative denoising\nintroduces high computational costs and slows generation speed, limiting\nbroader adoption. The community has made numerous efforts to reduce this\ncomputational burden, with methods like feature caching attracting attention\ndue to their effectiveness and simplicity. Nonetheless, simply reusing features\ncomputed at previous timesteps causes the features across adjacent timesteps to\nbecome similar, reducing the dynamics of features over time and ultimately\ncompromising the quality of generated images. In this paper, we introduce a\ndynamics-aware token pruning (DaTo) approach that addresses the limitations of\nfeature caching. DaTo selectively prunes tokens with lower dynamics, allowing\nonly high-dynamic tokens to participate in self-attention layers, thereby\nextending feature dynamics across timesteps. DaTo combines feature caching with\ntoken pruning in a training-free manner, achieving both temporal and token-wise\ninformation reuse. Applied to Stable Diffusion on the ImageNet, our approach\ndelivered a 9$\\times$ speedup while reducing FID by 0.33, indicating enhanced\nimage quality. On the COCO-30k, we observed a 7$\\times$ acceleration coupled\nwith a notable FID reduction of 2.17.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Stable Diffusion has achieved remarkable success in the field of\ntext-to-image generation, with its powerful generative capabilities and diverse\ngeneration results making a lasting impact. However, its iterative denoising\nintroduces high computational costs and slows generation speed, limiting\nbroader adoption. The community has made numerous efforts to reduce this\ncomputational burden, with methods like feature caching attracting attention\ndue to their effectiveness and simplicity. Nonetheless, simply reusing features\ncomputed at previous timesteps causes the features across adjacent timesteps to\nbecome similar, reducing the dynamics of features over time and ultimately\ncompromising the quality of generated images. In this paper, we introduce a\ndynamics-aware token pruning (DaTo) approach that addresses the limitations of\nfeature caching. DaTo selectively prunes tokens with lower dynamics, allowing\nonly high-dynamic tokens to participate in self-attention layers, thereby\nextending feature dynamics across timesteps. DaTo combines feature caching with\ntoken pruning in a training-free manner, achieving both temporal and token-wise\ninformation reuse. Applied to Stable Diffusion on the ImageNet, our approach\ndelivered a 9$\\times$ speedup while reducing FID by 0.33, indicating enhanced\nimage quality. On the COCO-30k, we observed a 7$\\times$ acceleration coupled\nwith a notable FID reduction of 2.17."
                },
                "authors": [
                    {
                        "name": "Evelyn Zhang"
                    },
                    {
                        "name": "Bang Xiao"
                    },
                    {
                        "name": "Jiayi Tang"
                    },
                    {
                        "name": "Qianli Ma"
                    },
                    {
                        "name": "Chang Zou"
                    },
                    {
                        "name": "Xuefei Ning"
                    },
                    {
                        "name": "Xuming Hu"
                    },
                    {
                        "name": "Linfeng Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Linfeng Zhang"
                },
                "author": "Linfeng Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.00375v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.00375v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.10516v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.10516v3",
                "updated": "2024-12-31T07:11:00Z",
                "updated_parsed": [
                    2024,
                    12,
                    31,
                    7,
                    11,
                    0,
                    1,
                    366,
                    0
                ],
                "published": "2024-09-16T17:59:52Z",
                "published_parsed": [
                    2024,
                    9,
                    16,
                    17,
                    59,
                    52,
                    0,
                    260,
                    0
                ],
                "title": "RetrievalAttention: Accelerating Long-Context LLM Inference via Vector\n  Retrieval",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RetrievalAttention: Accelerating Long-Context LLM Inference via Vector\n  Retrieval"
                },
                "summary": "Transformer-based Large Language Models (LLMs) have become increasingly\nimportant. However, due to the quadratic time complexity of attention\ncomputation, scaling LLMs to longer contexts incurs extremely slow inference\nspeed and high GPU memory consumption for caching key-value (KV) vectors. This\npaper proposes RetrievalAttention, a training-free approach to both accelerate\nattention computation and reduce GPU memory consumption. By leveraging the\ndynamic sparsity of attention mechanism, RetrievalAttention proposes to build\napproximate nearest neighbor search (ANNS) indexes for KV vectors in CPU memory\nand retrieve the most relevant ones through vector search during generation.\nUnfortunately, we observe that the off-the-shelf ANNS indexes are often\nineffective for such retrieval tasks due to the out-of-distribution (OOD)\nbetween query vectors and key vectors in the attention mechanism.\nRetrievalAttention addresses the OOD challenge by designing an attention-aware\nvector search algorithm that can adapt to the distribution of query vectors.\nOur evaluation demonstrates that RetrievalAttention achieves near full\nattention accuracy while only requiring access to 1--3% of the data. This leads\nto a significant reduction in the inference cost of long-context LLMs, with a\nmuch lower GPU memory footprint. In particular, RetrievalAttention only needs a\nsingle NVIDIA RTX4090 (24GB) to serve 128K tokens for LLMs with 8B parameters,\nwhich is capable of generating one token in 0.188 seconds.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Transformer-based Large Language Models (LLMs) have become increasingly\nimportant. However, due to the quadratic time complexity of attention\ncomputation, scaling LLMs to longer contexts incurs extremely slow inference\nspeed and high GPU memory consumption for caching key-value (KV) vectors. This\npaper proposes RetrievalAttention, a training-free approach to both accelerate\nattention computation and reduce GPU memory consumption. By leveraging the\ndynamic sparsity of attention mechanism, RetrievalAttention proposes to build\napproximate nearest neighbor search (ANNS) indexes for KV vectors in CPU memory\nand retrieve the most relevant ones through vector search during generation.\nUnfortunately, we observe that the off-the-shelf ANNS indexes are often\nineffective for such retrieval tasks due to the out-of-distribution (OOD)\nbetween query vectors and key vectors in the attention mechanism.\nRetrievalAttention addresses the OOD challenge by designing an attention-aware\nvector search algorithm that can adapt to the distribution of query vectors.\nOur evaluation demonstrates that RetrievalAttention achieves near full\nattention accuracy while only requiring access to 1--3% of the data. This leads\nto a significant reduction in the inference cost of long-context LLMs, with a\nmuch lower GPU memory footprint. In particular, RetrievalAttention only needs a\nsingle NVIDIA RTX4090 (24GB) to serve 128K tokens for LLMs with 8B parameters,\nwhich is capable of generating one token in 0.188 seconds."
                },
                "authors": [
                    {
                        "name": "Di Liu"
                    },
                    {
                        "name": "Meng Chen"
                    },
                    {
                        "name": "Baotong Lu"
                    },
                    {
                        "name": "Huiqiang Jiang"
                    },
                    {
                        "name": "Zhenhua Han"
                    },
                    {
                        "name": "Qianxi Zhang"
                    },
                    {
                        "name": "Qi Chen"
                    },
                    {
                        "name": "Chengruidong Zhang"
                    },
                    {
                        "name": "Bailu Ding"
                    },
                    {
                        "name": "Kai Zhang"
                    },
                    {
                        "name": "Chen Chen"
                    },
                    {
                        "name": "Fan Yang"
                    },
                    {
                        "name": "Yuqing Yang"
                    },
                    {
                        "name": "Lili Qiu"
                    }
                ],
                "author_detail": {
                    "name": "Lili Qiu"
                },
                "author": "Lili Qiu",
                "arxiv_comment": "19 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.10516v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.10516v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.00279v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.00279v1",
                "updated": "2024-12-31T05:24:30Z",
                "updated_parsed": [
                    2024,
                    12,
                    31,
                    5,
                    24,
                    30,
                    1,
                    366,
                    0
                ],
                "published": "2024-12-31T05:24:30Z",
                "published_parsed": [
                    2024,
                    12,
                    31,
                    5,
                    24,
                    30,
                    1,
                    366,
                    0
                ],
                "title": "Performant Automatic BLAS Offloading on Unified Memory Architecture with\n  OpenMP First-Touch Style Data Movement",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Performant Automatic BLAS Offloading on Unified Memory Architecture with\n  OpenMP First-Touch Style Data Movement"
                },
                "summary": "BLAS is a fundamental building block of advanced linear algebra libraries and\nmany modern scientific computing applications. GPUs are known for their strong\narithmetic computing capabilities and are highly suited for BLAS operations.\nHowever, porting code to GPUs often requires significant effort, especially for\nlarge, complex codes or legacy codes, even for BLAS-heavy applications. While\nvarious tools exist to automatically offload BLAS to GPUs, they are often\nimpractical due to the high costs associated with mandatory data transfers. The\nadvent of unified memory architectures in recent GPU designs, such as the\nNVIDIA Grace-Hopper, allows cache-coherent memory access across all types of\nmemory for both CPU and GPU, potentially eliminating the bottlenecks faced in\nconventional architectures. This breakthrough paves the way for innovative\napplication developments and porting strategies. Building on our preliminary\nwork demonstrating the potential of automatic *gemm offload, this paper extends\nthe framework to all level-3 BLAS operations and introduces SCILIB-Accel, a\nnovel tool for automatic BLAS offload. SCILIB-Accel leverages the memory\ncoherency in Grace-Hopper and introduces a Device First-Use data movement\npolicy inspired by the OpenMP First-Touch approach in multi-socket CPU\nprogramming, minimizing CPU-GPU data transfers for typical scientific computing\ncodes. Additionally, utilizing dynamic binary instrumentation, the tool\nintercepts BLAS symbols directly from a CPU binary, requiring no code\nmodifications or recompilation. SCILIB-Accel has been evaluated using multiple\nquantum physics codes on up to a few hundred GPU nodes, yielding promising\nspeedups. Notably, for the LSMS method in the MuST suite, a 3x speedup was\nachieved on Grace-Hopper compared to Grace-Grace.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "BLAS is a fundamental building block of advanced linear algebra libraries and\nmany modern scientific computing applications. GPUs are known for their strong\narithmetic computing capabilities and are highly suited for BLAS operations.\nHowever, porting code to GPUs often requires significant effort, especially for\nlarge, complex codes or legacy codes, even for BLAS-heavy applications. While\nvarious tools exist to automatically offload BLAS to GPUs, they are often\nimpractical due to the high costs associated with mandatory data transfers. The\nadvent of unified memory architectures in recent GPU designs, such as the\nNVIDIA Grace-Hopper, allows cache-coherent memory access across all types of\nmemory for both CPU and GPU, potentially eliminating the bottlenecks faced in\nconventional architectures. This breakthrough paves the way for innovative\napplication developments and porting strategies. Building on our preliminary\nwork demonstrating the potential of automatic *gemm offload, this paper extends\nthe framework to all level-3 BLAS operations and introduces SCILIB-Accel, a\nnovel tool for automatic BLAS offload. SCILIB-Accel leverages the memory\ncoherency in Grace-Hopper and introduces a Device First-Use data movement\npolicy inspired by the OpenMP First-Touch approach in multi-socket CPU\nprogramming, minimizing CPU-GPU data transfers for typical scientific computing\ncodes. Additionally, utilizing dynamic binary instrumentation, the tool\nintercepts BLAS symbols directly from a CPU binary, requiring no code\nmodifications or recompilation. SCILIB-Accel has been evaluated using multiple\nquantum physics codes on up to a few hundred GPU nodes, yielding promising\nspeedups. Notably, for the LSMS method in the MuST suite, a 3x speedup was\nachieved on Grace-Hopper compared to Grace-Grace."
                },
                "authors": [
                    {
                        "name": "Junjie Li"
                    }
                ],
                "author_detail": {
                    "name": "Junjie Li"
                },
                "author": "Junjie Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.00279v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.00279v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.00243v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.00243v1",
                "updated": "2024-12-31T03:19:38Z",
                "updated_parsed": [
                    2024,
                    12,
                    31,
                    3,
                    19,
                    38,
                    1,
                    366,
                    0
                ],
                "published": "2024-12-31T03:19:38Z",
                "published_parsed": [
                    2024,
                    12,
                    31,
                    3,
                    19,
                    38,
                    1,
                    366,
                    0
                ],
                "title": "Cross-Layer Cache Aggregation for Token Reduction in Ultra-Fine-Grained\n  Image Recognition",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cross-Layer Cache Aggregation for Token Reduction in Ultra-Fine-Grained\n  Image Recognition"
                },
                "summary": "Ultra-fine-grained image recognition (UFGIR) is a challenging task that\ninvolves classifying images within a macro-category. While traditional FGIR\ndeals with classifying different species, UFGIR goes beyond by classifying\nsub-categories within a species such as cultivars of a plant. In recent times\nthe usage of Vision Transformer-based backbones has allowed methods to obtain\noutstanding recognition performances in this task but this comes at a\nsignificant cost in terms of computation specially since this task\nsignificantly benefits from incorporating higher resolution images. Therefore,\ntechniques such as token reduction have emerged to reduce the computational\ncost. However, dropping tokens leads to loss of essential information for\nfine-grained categories, specially as the token keep rate is reduced.\nTherefore, to counteract the loss of information brought by the usage of token\nreduction we propose a novel Cross-Layer Aggregation Classification Head and a\nCross-Layer Cache mechanism to recover and access information from previous\nlayers in later locations. Extensive experiments covering more than 2000 runs\nacross diverse settings including 5 datasets, 9 backbones, 7 token reduction\nmethods, 5 keep rates, and 2 image sizes demonstrate the effectiveness of the\nproposed plug-and-play modules and allow us to push the boundaries of accuracy\nvs cost for UFGIR by reducing the kept tokens to extremely low ratios of up to\n10\\% while maintaining a competitive accuracy to state-of-the-art models. Code\nis available at: \\url{https://github.com/arkel23/CLCA}",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Ultra-fine-grained image recognition (UFGIR) is a challenging task that\ninvolves classifying images within a macro-category. While traditional FGIR\ndeals with classifying different species, UFGIR goes beyond by classifying\nsub-categories within a species such as cultivars of a plant. In recent times\nthe usage of Vision Transformer-based backbones has allowed methods to obtain\noutstanding recognition performances in this task but this comes at a\nsignificant cost in terms of computation specially since this task\nsignificantly benefits from incorporating higher resolution images. Therefore,\ntechniques such as token reduction have emerged to reduce the computational\ncost. However, dropping tokens leads to loss of essential information for\nfine-grained categories, specially as the token keep rate is reduced.\nTherefore, to counteract the loss of information brought by the usage of token\nreduction we propose a novel Cross-Layer Aggregation Classification Head and a\nCross-Layer Cache mechanism to recover and access information from previous\nlayers in later locations. Extensive experiments covering more than 2000 runs\nacross diverse settings including 5 datasets, 9 backbones, 7 token reduction\nmethods, 5 keep rates, and 2 image sizes demonstrate the effectiveness of the\nproposed plug-and-play modules and allow us to push the boundaries of accuracy\nvs cost for UFGIR by reducing the kept tokens to extremely low ratios of up to\n10\\% while maintaining a competitive accuracy to state-of-the-art models. Code\nis available at: \\url{https://github.com/arkel23/CLCA}"
                },
                "authors": [
                    {
                        "name": "Edwin Arkel Rios"
                    },
                    {
                        "name": "Jansen Christopher Yuanda"
                    },
                    {
                        "name": "Vincent Leon Ghanz"
                    },
                    {
                        "name": "Cheng-Wei Yu"
                    },
                    {
                        "name": "Bo-Cheng Lai"
                    },
                    {
                        "name": "Min-Chun Hu"
                    }
                ],
                "author_detail": {
                    "name": "Min-Chun Hu"
                },
                "author": "Min-Chun Hu",
                "arxiv_comment": "Accepted to ICASSP 2025. Main: 5 pages, 4 figures, 1 table",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.00243v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.00243v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2; I.4",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.21015v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.21015v1",
                "updated": "2024-12-30T15:33:19Z",
                "updated_parsed": [
                    2024,
                    12,
                    30,
                    15,
                    33,
                    19,
                    0,
                    365,
                    0
                ],
                "published": "2024-12-30T15:33:19Z",
                "published_parsed": [
                    2024,
                    12,
                    30,
                    15,
                    33,
                    19,
                    0,
                    365,
                    0
                ],
                "title": "MapQaTor: A System for Efficient Annotation of Map Query Datasets",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MapQaTor: A System for Efficient Annotation of Map Query Datasets"
                },
                "summary": "Mapping and navigation services like Google Maps, Apple Maps, Openstreet\nMaps, are essential for accessing various location-based data, yet they often\nstruggle to handle natural language geospatial queries. Recent advancements in\nLarge Language Models (LLMs) show promise in question answering (QA), but\ncreating reliable geospatial QA datasets from map services remains challenging.\nWe introduce MapQaTor, a web application that streamlines the creation of\nreproducible, traceable map-based QA datasets. With its plug-and-play\narchitecture, MapQaTor enables seamless integration with any maps API, allowing\nusers to gather and visualize data from diverse sources with minimal setup. By\ncaching API responses, the platform ensures consistent ground truth, enhancing\nthe reliability of the data even as real-world information evolves. MapQaTor\ncentralizes data retrieval, annotation, and visualization within a single\nplatform, offering a unique opportunity to evaluate the current state of\nLLM-based geospatial reasoning while advancing their capabilities for improved\ngeospatial understanding. Evaluation metrics show that, MapQaTor speeds up the\nannotation process by at least 30 times compared to manual methods,\nunderscoring its potential for developing geospatial resources, such as complex\nmap reasoning datasets. The website is live at: https://mapqator.github.io/ and\na demo video is available at: https://youtu.be/7_aV9Wmhs6Q.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mapping and navigation services like Google Maps, Apple Maps, Openstreet\nMaps, are essential for accessing various location-based data, yet they often\nstruggle to handle natural language geospatial queries. Recent advancements in\nLarge Language Models (LLMs) show promise in question answering (QA), but\ncreating reliable geospatial QA datasets from map services remains challenging.\nWe introduce MapQaTor, a web application that streamlines the creation of\nreproducible, traceable map-based QA datasets. With its plug-and-play\narchitecture, MapQaTor enables seamless integration with any maps API, allowing\nusers to gather and visualize data from diverse sources with minimal setup. By\ncaching API responses, the platform ensures consistent ground truth, enhancing\nthe reliability of the data even as real-world information evolves. MapQaTor\ncentralizes data retrieval, annotation, and visualization within a single\nplatform, offering a unique opportunity to evaluate the current state of\nLLM-based geospatial reasoning while advancing their capabilities for improved\ngeospatial understanding. Evaluation metrics show that, MapQaTor speeds up the\nannotation process by at least 30 times compared to manual methods,\nunderscoring its potential for developing geospatial resources, such as complex\nmap reasoning datasets. The website is live at: https://mapqator.github.io/ and\na demo video is available at: https://youtu.be/7_aV9Wmhs6Q."
                },
                "authors": [
                    {
                        "name": "Mahir Labib Dihan"
                    },
                    {
                        "name": "Mohammed Eunus Ali"
                    },
                    {
                        "name": "Md Rizwan Parvez"
                    }
                ],
                "author_detail": {
                    "name": "Md Rizwan Parvez"
                },
                "author": "Md Rizwan Parvez",
                "arxiv_comment": "13 pages, 35 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.21015v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.21015v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.12094v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.12094v3",
                "updated": "2024-12-30T14:54:29Z",
                "updated_parsed": [
                    2024,
                    12,
                    30,
                    14,
                    54,
                    29,
                    0,
                    365,
                    0
                ],
                "published": "2024-12-16T18:58:57Z",
                "published_parsed": [
                    2024,
                    12,
                    16,
                    18,
                    58,
                    57,
                    0,
                    351,
                    0
                ],
                "title": "SepLLM: Accelerate Large Language Models by Compressing One Segment into\n  One Separator",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SepLLM: Accelerate Large Language Models by Compressing One Segment into\n  One Separator"
                },
                "summary": "Large Language Models (LLMs) have exhibited exceptional performance across a\nspectrum of natural language processing tasks. However, their substantial sizes\npose considerable challenges, particularly in computational demands and\ninference speed, due to their quadratic complexity. In this work, we have\nidentified a key pattern: certain seemingly meaningless special tokens (i.e.,\nseparators) contribute disproportionately to attention scores compared to\nsemantically meaningful tokens. This observation suggests that information of\nthe segments between these separator tokens can be effectively condensed into\nthe separator tokens themselves without significant information loss. Guided by\nthis insight, we introduce SepLLM, a plug-and-play framework that accelerates\ninference by compressing these segments and eliminating redundant tokens.\nAdditionally, we implement efficient kernels for training acceleration.\nExperimental results across training-free, training-from-scratch, and\npost-training settings demonstrate SepLLM's effectiveness. Notably, using the\nLlama-3-8B backbone, SepLLM achieves over 50% reduction in KV cache on the\nGSM8K-CoT benchmark while maintaining comparable performance. Furthermore, in\nstreaming settings, SepLLM effectively processes sequences of up to 4 million\ntokens or more while maintaining consistent language modeling capabilities.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have exhibited exceptional performance across a\nspectrum of natural language processing tasks. However, their substantial sizes\npose considerable challenges, particularly in computational demands and\ninference speed, due to their quadratic complexity. In this work, we have\nidentified a key pattern: certain seemingly meaningless special tokens (i.e.,\nseparators) contribute disproportionately to attention scores compared to\nsemantically meaningful tokens. This observation suggests that information of\nthe segments between these separator tokens can be effectively condensed into\nthe separator tokens themselves without significant information loss. Guided by\nthis insight, we introduce SepLLM, a plug-and-play framework that accelerates\ninference by compressing these segments and eliminating redundant tokens.\nAdditionally, we implement efficient kernels for training acceleration.\nExperimental results across training-free, training-from-scratch, and\npost-training settings demonstrate SepLLM's effectiveness. Notably, using the\nLlama-3-8B backbone, SepLLM achieves over 50% reduction in KV cache on the\nGSM8K-CoT benchmark while maintaining comparable performance. Furthermore, in\nstreaming settings, SepLLM effectively processes sequences of up to 4 million\ntokens or more while maintaining consistent language modeling capabilities."
                },
                "authors": [
                    {
                        "name": "Guoxuan Chen"
                    },
                    {
                        "name": "Han Shi"
                    },
                    {
                        "name": "Jiawei Li"
                    },
                    {
                        "name": "Yihang Gao"
                    },
                    {
                        "name": "Xiaozhe Ren"
                    },
                    {
                        "name": "Yimeng Chen"
                    },
                    {
                        "name": "Xin Jiang"
                    },
                    {
                        "name": "Zhenguo Li"
                    },
                    {
                        "name": "Weiyang Liu"
                    },
                    {
                        "name": "Chao Huang"
                    }
                ],
                "author_detail": {
                    "name": "Chao Huang"
                },
                "author": "Chao Huang",
                "arxiv_comment": "We have made our code publicly available at sepllm.github.io. Our\n  codebase supports efficient multi-node distributed training with accelerated\n  attention module Sep-Attention and also supports numerous existing Fusion\n  Operators to accelerate the training process, such as fused rope, etc. If you\n  find our code helpful, please kindly consider giving us a **star** on\n  GitHub^_^. Thank you very much!",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.12094v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.12094v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.20887v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.20887v1",
                "updated": "2024-12-30T11:54:19Z",
                "updated_parsed": [
                    2024,
                    12,
                    30,
                    11,
                    54,
                    19,
                    0,
                    365,
                    0
                ],
                "published": "2024-12-30T11:54:19Z",
                "published_parsed": [
                    2024,
                    12,
                    30,
                    11,
                    54,
                    19,
                    0,
                    365,
                    0
                ],
                "title": "A Hidden Quantum Paraelectric Phase in SrTiO3 Induced by Terahertz Field",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Hidden Quantum Paraelectric Phase in SrTiO3 Induced by Terahertz Field"
                },
                "summary": "Coherent manipulation of lattice vibrations using ultrafast light pulses\nenables access to nonequilibrium 'hidden' phases with designed functionalities\nin quantum materials. However, expanding the understanding of nonlinear\nlight-phonon interaction mechanisms remains crucial for developing new\nstrategies. Here, we report re-entrant ultrafast phase transitions in SrTiO3\ndriven by intense terahertz excitation. As the terahertz field increases, the\nsystem transitions from the quantum paraelectric (QPE) ground state to an\nintermediate ferroelectric phase, and then unexpectedly reverts to a QPE state\nabove ~500 kV/cm. The latter hidden QPE phase exhibits distinct lattice\ndynamics compared to the initial phases, highlighting activated\nantiferrodistortive phonon modes. Aided by first-principles dynamical\ncalculations, we identify the mechanism for these complex behaviors as a\nsuperposition of multiple coherently excited eigenstates of the polar soft\nmode. Our results reveal a previously uncharted quantum facet of SrTiO3 and\nopen pathways for harnessing high-order excitations to engineer quantum\nmaterials in the ultrafast regime.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Coherent manipulation of lattice vibrations using ultrafast light pulses\nenables access to nonequilibrium 'hidden' phases with designed functionalities\nin quantum materials. However, expanding the understanding of nonlinear\nlight-phonon interaction mechanisms remains crucial for developing new\nstrategies. Here, we report re-entrant ultrafast phase transitions in SrTiO3\ndriven by intense terahertz excitation. As the terahertz field increases, the\nsystem transitions from the quantum paraelectric (QPE) ground state to an\nintermediate ferroelectric phase, and then unexpectedly reverts to a QPE state\nabove ~500 kV/cm. The latter hidden QPE phase exhibits distinct lattice\ndynamics compared to the initial phases, highlighting activated\nantiferrodistortive phonon modes. Aided by first-principles dynamical\ncalculations, we identify the mechanism for these complex behaviors as a\nsuperposition of multiple coherently excited eigenstates of the polar soft\nmode. Our results reveal a previously uncharted quantum facet of SrTiO3 and\nopen pathways for harnessing high-order excitations to engineer quantum\nmaterials in the ultrafast regime."
                },
                "authors": [
                    {
                        "name": "Wei Li"
                    },
                    {
                        "name": "Hanbyul Kim"
                    },
                    {
                        "name": "Xinbo Wang"
                    },
                    {
                        "name": "Jianlin Luo"
                    },
                    {
                        "name": "Simone Latini"
                    },
                    {
                        "name": "Dongbin Shin"
                    },
                    {
                        "name": "Jun-Ming Liu"
                    },
                    {
                        "name": "Jing-Feng Li"
                    },
                    {
                        "name": "Angel Rubio"
                    },
                    {
                        "name": "Ce-Wen Nan"
                    },
                    {
                        "name": "Qian Li"
                    }
                ],
                "author_detail": {
                    "name": "Qian Li"
                },
                "author": "Qian Li",
                "arxiv_comment": "18 pages, 4 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.20887v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.20887v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cond-mat.mtrl-sci",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cond-mat.mtrl-sci",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "physics.optics",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.08545v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.08545v2",
                "updated": "2024-12-30T05:01:44Z",
                "updated_parsed": [
                    2024,
                    12,
                    30,
                    5,
                    1,
                    44,
                    0,
                    365,
                    0
                ],
                "published": "2024-08-16T06:11:21Z",
                "published_parsed": [
                    2024,
                    8,
                    16,
                    6,
                    11,
                    21,
                    4,
                    229,
                    0
                ],
                "title": "SelectLLM: Query-Aware Efficient Selection Algorithm for Large Language\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SelectLLM: Query-Aware Efficient Selection Algorithm for Large Language\n  Models"
                },
                "summary": "Large language models (LLMs) have seen widespread adoption due to their\nremarkable performance across various applications, driving the accelerated\ndevelopment of a large number of diverse LLMs. However, these individual LLMs\nshow limitations in generalization and performance on complex tasks due to\ninherent training biases, model size constraints, and the quality or diversity\nof pre-training datasets. A promising direction is to efficiently harness the\ndiverse capabilities of LLMs to overcome these individual limitations. To\naddress these limitations, we introduce a novel LLM selection algorithm called\nSelectLLM, which efficiently directs input queries to the most suitable subset\nof LLMs from a large pool, ensuring that the selected models collectively\nprovide accurate responses. SelectLLM employs a multi-label classifier and\npolicy based on the classifier's predictions and confidence scores in selecting\nan optimal, query-aware, and lightweight subset of LLMs. Our findings indicate\nthat the proposed model outperforms existing ensemble-based baselines and\nachieves competitive performance with similarly sized top-performing LLMs while\nmaintaining efficiency. Specifically, it achieves a huge reduction in inference\nlatency on two challenging reasoning benchmarks: 13% on GSM8K and 70% on MMLU,\ncompared to the top-performing baselines. Also, we establish a theoretical\nupper bound by an oracle with LLMs and explore in-depth linguistic analysis to\nunderstand the performance gap between Oracle and SelectLLM.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have seen widespread adoption due to their\nremarkable performance across various applications, driving the accelerated\ndevelopment of a large number of diverse LLMs. However, these individual LLMs\nshow limitations in generalization and performance on complex tasks due to\ninherent training biases, model size constraints, and the quality or diversity\nof pre-training datasets. A promising direction is to efficiently harness the\ndiverse capabilities of LLMs to overcome these individual limitations. To\naddress these limitations, we introduce a novel LLM selection algorithm called\nSelectLLM, which efficiently directs input queries to the most suitable subset\nof LLMs from a large pool, ensuring that the selected models collectively\nprovide accurate responses. SelectLLM employs a multi-label classifier and\npolicy based on the classifier's predictions and confidence scores in selecting\nan optimal, query-aware, and lightweight subset of LLMs. Our findings indicate\nthat the proposed model outperforms existing ensemble-based baselines and\nachieves competitive performance with similarly sized top-performing LLMs while\nmaintaining efficiency. Specifically, it achieves a huge reduction in inference\nlatency on two challenging reasoning benchmarks: 13% on GSM8K and 70% on MMLU,\ncompared to the top-performing baselines. Also, we establish a theoretical\nupper bound by an oracle with LLMs and explore in-depth linguistic analysis to\nunderstand the performance gap between Oracle and SelectLLM."
                },
                "authors": [
                    {
                        "name": "Kaushal Kumar Maurya"
                    },
                    {
                        "name": "KV Aditya Srivatsa"
                    },
                    {
                        "name": "Ekaterina Kochmar"
                    }
                ],
                "author_detail": {
                    "name": "Ekaterina Kochmar"
                },
                "author": "Ekaterina Kochmar",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.08545v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.08545v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.20677v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.20677v1",
                "updated": "2024-12-30T03:05:45Z",
                "updated_parsed": [
                    2024,
                    12,
                    30,
                    3,
                    5,
                    45,
                    0,
                    365,
                    0
                ],
                "published": "2024-12-30T03:05:45Z",
                "published_parsed": [
                    2024,
                    12,
                    30,
                    3,
                    5,
                    45,
                    0,
                    365,
                    0
                ],
                "title": "Align Attention Heads Before Merging Them: An Effective Way for\n  Converting MHA to GQA",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Align Attention Heads Before Merging Them: An Effective Way for\n  Converting MHA to GQA"
                },
                "summary": "Large language models have been shown to perform well on a variety of natural\nlanguage processing problems. However, as the model size and the input\nsequence's length increase, the rapid increase of KV Cache significantly slows\ndown inference speed. Therefore GQA model, as an alternative to MHA model, has\nbeen widely introduced into LLMs. In this work, we propose a low-cost method\nfor pruning MHA models into GQA models with any compression ratio of key-value\nheads. Our method is based on $\\mathit{L_0}$ masks to gradually remove\nredundant parameters. In addition, we apply orthogonal transformations to\nattention heads without changing the model to increase similarity between\nattention heads before pruning training, in order to further improve\nperformance of the model. Our method can be compatible with rotary position\nembedding (RoPE), which means the model after training can be fully adapted to\nthe mainstream standard GQA framework. Experiments demonstrate that our\nstrategy can compress up to 87.5% of key-value heads of the LLaMA2-7B model\nwithout too much performance degradation, just achieved through supervised\nfine-tuning.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models have been shown to perform well on a variety of natural\nlanguage processing problems. However, as the model size and the input\nsequence's length increase, the rapid increase of KV Cache significantly slows\ndown inference speed. Therefore GQA model, as an alternative to MHA model, has\nbeen widely introduced into LLMs. In this work, we propose a low-cost method\nfor pruning MHA models into GQA models with any compression ratio of key-value\nheads. Our method is based on $\\mathit{L_0}$ masks to gradually remove\nredundant parameters. In addition, we apply orthogonal transformations to\nattention heads without changing the model to increase similarity between\nattention heads before pruning training, in order to further improve\nperformance of the model. Our method can be compatible with rotary position\nembedding (RoPE), which means the model after training can be fully adapted to\nthe mainstream standard GQA framework. Experiments demonstrate that our\nstrategy can compress up to 87.5% of key-value heads of the LLaMA2-7B model\nwithout too much performance degradation, just achieved through supervised\nfine-tuning."
                },
                "authors": [
                    {
                        "name": "Qingyun Jin"
                    },
                    {
                        "name": "Xiaohui Song"
                    },
                    {
                        "name": "Feng Zhou"
                    },
                    {
                        "name": "Zengchang Qin"
                    }
                ],
                "author_detail": {
                    "name": "Zengchang Qin"
                },
                "author": "Zengchang Qin",
                "arxiv_comment": "12 pages, 4 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.20677v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.20677v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.00068v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.00068v1",
                "updated": "2024-12-29T17:41:40Z",
                "updated_parsed": [
                    2024,
                    12,
                    29,
                    17,
                    41,
                    40,
                    6,
                    364,
                    0
                ],
                "published": "2024-12-29T17:41:40Z",
                "published_parsed": [
                    2024,
                    12,
                    29,
                    17,
                    41,
                    40,
                    6,
                    364,
                    0
                ],
                "title": "Dynamic Optimization of Storage Systems Using Reinforcement Learning\n  Techniques",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Dynamic Optimization of Storage Systems Using Reinforcement Learning\n  Techniques"
                },
                "summary": "The exponential growth of data-intensive applications has placed\nunprecedented demands on modern storage systems, necessitating dynamic and\nefficient optimization strategies. Traditional heuristics employed for storage\nperformance optimization often fail to adapt to the variability and complexity\nof contemporary workloads, leading to significant performance bottlenecks and\nresource inefficiencies. To address these challenges, this paper introduces\nRL-Storage, a novel reinforcement learning (RL)-based framework designed to\ndynamically optimize storage system configurations. RL-Storage leverages deep\nQ-learning algorithms to continuously learn from real-time I/O patterns and\npredict optimal storage parameters, such as cache size, queue depths, and\nreadahead settings[1]. The proposed framework operates within the storage\nkernel, ensuring minimal latency and low computational overhead. Through an\nadaptive feedback mechanism, RL-Storage dynamically adjusts critical\nparameters, achieving efficient resource utilization across a wide range of\nworkloads. Experimental evaluations conducted on a range of benchmarks,\nincluding RocksDB and PostgreSQL, demonstrate significant improvements, with\nthroughput gains of up to 2.6x and latency reductions of 43% compared to\nbaseline heuristics. Additionally, RL-Storage achieves these performance\nenhancements with a negligible CPU overhead of 0.11% and a memory footprint of\nonly 5 KB, making it suitable for seamless deployment in production\nenvironments. This work underscores the transformative potential of\nreinforcement learning techniques in addressing the dynamic nature of modern\nstorage systems. By autonomously adapting to workload variations in real time,\nRL-Storage provides a robust and scalable solution for optimizing storage\nperformance, paving the way for next-generation intelligent storage\ninfrastructures.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The exponential growth of data-intensive applications has placed\nunprecedented demands on modern storage systems, necessitating dynamic and\nefficient optimization strategies. Traditional heuristics employed for storage\nperformance optimization often fail to adapt to the variability and complexity\nof contemporary workloads, leading to significant performance bottlenecks and\nresource inefficiencies. To address these challenges, this paper introduces\nRL-Storage, a novel reinforcement learning (RL)-based framework designed to\ndynamically optimize storage system configurations. RL-Storage leverages deep\nQ-learning algorithms to continuously learn from real-time I/O patterns and\npredict optimal storage parameters, such as cache size, queue depths, and\nreadahead settings[1]. The proposed framework operates within the storage\nkernel, ensuring minimal latency and low computational overhead. Through an\nadaptive feedback mechanism, RL-Storage dynamically adjusts critical\nparameters, achieving efficient resource utilization across a wide range of\nworkloads. Experimental evaluations conducted on a range of benchmarks,\nincluding RocksDB and PostgreSQL, demonstrate significant improvements, with\nthroughput gains of up to 2.6x and latency reductions of 43% compared to\nbaseline heuristics. Additionally, RL-Storage achieves these performance\nenhancements with a negligible CPU overhead of 0.11% and a memory footprint of\nonly 5 KB, making it suitable for seamless deployment in production\nenvironments. This work underscores the transformative potential of\nreinforcement learning techniques in addressing the dynamic nature of modern\nstorage systems. By autonomously adapting to workload variations in real time,\nRL-Storage provides a robust and scalable solution for optimizing storage\nperformance, paving the way for next-generation intelligent storage\ninfrastructures."
                },
                "authors": [
                    {
                        "name": "Chiyu Cheng"
                    },
                    {
                        "name": "Chang Zhou"
                    },
                    {
                        "name": "Yang Zhao"
                    },
                    {
                        "name": "Jin Cao"
                    }
                ],
                "author_detail": {
                    "name": "Jin Cao"
                },
                "author": "Jin Cao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.00068v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.00068v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.OS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.OS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.20524v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.20524v1",
                "updated": "2024-12-29T17:18:21Z",
                "updated_parsed": [
                    2024,
                    12,
                    29,
                    17,
                    18,
                    21,
                    6,
                    364,
                    0
                ],
                "published": "2024-12-29T17:18:21Z",
                "published_parsed": [
                    2024,
                    12,
                    29,
                    17,
                    18,
                    21,
                    6,
                    364,
                    0
                ],
                "title": "Ns3 meets Sionna: Using Realistic Channels in Network Simulation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Ns3 meets Sionna: Using Realistic Channels in Network Simulation"
                },
                "summary": "Network simulators are indispensable tools for the advancement of wireless\nnetwork technologies, offering a cost-effective and controlled environment to\nsimulate real-world network behavior. However, traditional simulators, such as\nthe widely used ns-3, exhibit limitations in accurately modeling indoor and\noutdoor scenarios due to their reliance on simplified statistical and\nstochastic channel propagation models, which often fail to accurately capture\nphysical phenomena like multipath signal propagation and shadowing by obstacles\nin the line-of-sight path. We present Ns3Sionna, which integrates a ray\ntracing-based channel model, implemented using the Sionna RT framework, within\nthe ns-3 network simulator. It allows to simulate environment-specific and\nphysically accurate channel realizations for a given 3D scene and wireless\ndevice positions. Additionally, a mobility model based on ray tracing was\ndeveloped to accurately represent device movements within the simulated 3D\nspace. Ns3Sionna provides more realistic path and delay loss estimates for both\nindoor and outdoor environments than existing ns-3 propagation models,\nparticularly in terms of spatial and temporal correlation. Moreover,\nfine-grained channel state information is provided, which could be used for the\ndevelopment of sensing applications. Due to the significant computational\ndemands of ray tracing, Ns3Sionna takes advantage of the parallel execution\ncapabilities of modern GPUs and multi-core CPUs by incorporating intelligent\npre-caching mechanisms that leverage the channel's coherence time to optimize\nruntime performance. This enables the efficient simulation of scenarios with a\nsmall to medium number of mobile nodes.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Network simulators are indispensable tools for the advancement of wireless\nnetwork technologies, offering a cost-effective and controlled environment to\nsimulate real-world network behavior. However, traditional simulators, such as\nthe widely used ns-3, exhibit limitations in accurately modeling indoor and\noutdoor scenarios due to their reliance on simplified statistical and\nstochastic channel propagation models, which often fail to accurately capture\nphysical phenomena like multipath signal propagation and shadowing by obstacles\nin the line-of-sight path. We present Ns3Sionna, which integrates a ray\ntracing-based channel model, implemented using the Sionna RT framework, within\nthe ns-3 network simulator. It allows to simulate environment-specific and\nphysically accurate channel realizations for a given 3D scene and wireless\ndevice positions. Additionally, a mobility model based on ray tracing was\ndeveloped to accurately represent device movements within the simulated 3D\nspace. Ns3Sionna provides more realistic path and delay loss estimates for both\nindoor and outdoor environments than existing ns-3 propagation models,\nparticularly in terms of spatial and temporal correlation. Moreover,\nfine-grained channel state information is provided, which could be used for the\ndevelopment of sensing applications. Due to the significant computational\ndemands of ray tracing, Ns3Sionna takes advantage of the parallel execution\ncapabilities of modern GPUs and multi-core CPUs by incorporating intelligent\npre-caching mechanisms that leverage the channel's coherence time to optimize\nruntime performance. This enables the efficient simulation of scenarios with a\nsmall to medium number of mobile nodes."
                },
                "authors": [
                    {
                        "name": "Anatolij Zubow"
                    },
                    {
                        "name": "Yannik Pilz"
                    },
                    {
                        "name": "Sascha Rösler"
                    },
                    {
                        "name": "Falko Dressler"
                    }
                ],
                "author_detail": {
                    "name": "Falko Dressler"
                },
                "author": "Falko Dressler",
                "arxiv_comment": "9 pages, 11 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.20524v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.20524v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.20221v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.20221v1",
                "updated": "2024-12-28T17:17:03Z",
                "updated_parsed": [
                    2024,
                    12,
                    28,
                    17,
                    17,
                    3,
                    5,
                    363,
                    0
                ],
                "published": "2024-12-28T17:17:03Z",
                "published_parsed": [
                    2024,
                    12,
                    28,
                    17,
                    17,
                    3,
                    5,
                    363,
                    0
                ],
                "title": "Revisiting Cache Freshness for Emerging Real-Time Applications",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Revisiting Cache Freshness for Emerging Real-Time Applications"
                },
                "summary": "Caching is widely used in industry to improve application performance by\nreducing data-access latency and taking the load off the backend\ninfrastructure. TTLs have become the de-facto mechanism used to keep cached\ndata reasonably fresh (i.e., not too out of date with the backend). However,\nthe emergence of real-time applications requires tighter data freshness, which\nis impractical to achieve with TTLs. We discuss why this is the case, and\npropose a simple yet effective adaptive policy to achieve the desired\nfreshness.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Caching is widely used in industry to improve application performance by\nreducing data-access latency and taking the load off the backend\ninfrastructure. TTLs have become the de-facto mechanism used to keep cached\ndata reasonably fresh (i.e., not too out of date with the backend). However,\nthe emergence of real-time applications requires tighter data freshness, which\nis impractical to achieve with TTLs. We discuss why this is the case, and\npropose a simple yet effective adaptive policy to achieve the desired\nfreshness."
                },
                "authors": [
                    {
                        "name": "Ziming Mao"
                    },
                    {
                        "name": "Rishabh Iyer"
                    },
                    {
                        "name": "Scott Shenker"
                    },
                    {
                        "name": "Ion Stoica"
                    }
                ],
                "author_detail": {
                    "name": "Ion Stoica"
                },
                "author": "Ion Stoica",
                "arxiv_doi": "10.1145/3696348.3696858",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3696348.3696858",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2412.20221v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.20221v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "HotNets '24",
                "arxiv_primary_category": {
                    "term": "cs.OS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.OS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.20105v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.20105v1",
                "updated": "2024-12-28T10:17:29Z",
                "updated_parsed": [
                    2024,
                    12,
                    28,
                    10,
                    17,
                    29,
                    5,
                    363,
                    0
                ],
                "published": "2024-12-28T10:17:29Z",
                "published_parsed": [
                    2024,
                    12,
                    28,
                    10,
                    17,
                    29,
                    5,
                    363,
                    0
                ],
                "title": "ST$^3$: Accelerating Multimodal Large Language Model by Spatial-Temporal\n  Visual Token Trimming",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ST$^3$: Accelerating Multimodal Large Language Model by Spatial-Temporal\n  Visual Token Trimming"
                },
                "summary": "Multimodal large language models (MLLMs) enhance their perceptual\ncapabilities by integrating visual and textual information. However, processing\nthe massive number of visual tokens incurs a significant computational cost.\nExisting analysis of the MLLM attention mechanisms remains shallow, leading to\ncoarse-grain token pruning strategies that fail to effectively balance speed\nand accuracy. In this paper, we conduct a comprehensive investigation of MLLM\nattention mechanisms with LLaVA. We find that numerous visual tokens and\npartial attention computations are redundant during the decoding process. Based\non this insight, we propose Spatial-Temporal Visual Token Trimming\n($\\textbf{ST}^{3}$), a framework designed to accelerate MLLM inference without\nretraining. $\\textbf{ST}^{3}$ consists of two primary components: 1)\nProgressive Visual Token Pruning (\\textbf{PVTP}), which eliminates inattentive\nvisual tokens across layers, and 2) Visual Token Annealing (\\textbf{VTA}),\nwhich dynamically reduces the number of visual tokens in each layer as the\ngenerated tokens grow. Together, these techniques deliver around\n$\\mathbf{2\\times}$ faster inference with only about $\\mathbf{30\\%}$ KV cache\nmemory compared to the original LLaVA, while maintaining consistent performance\nacross various datasets. Crucially, $\\textbf{ST}^{3}$ can be seamlessly\nintegrated into existing pre-trained MLLMs, providing a plug-and-play solution\nfor efficient inference.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multimodal large language models (MLLMs) enhance their perceptual\ncapabilities by integrating visual and textual information. However, processing\nthe massive number of visual tokens incurs a significant computational cost.\nExisting analysis of the MLLM attention mechanisms remains shallow, leading to\ncoarse-grain token pruning strategies that fail to effectively balance speed\nand accuracy. In this paper, we conduct a comprehensive investigation of MLLM\nattention mechanisms with LLaVA. We find that numerous visual tokens and\npartial attention computations are redundant during the decoding process. Based\non this insight, we propose Spatial-Temporal Visual Token Trimming\n($\\textbf{ST}^{3}$), a framework designed to accelerate MLLM inference without\nretraining. $\\textbf{ST}^{3}$ consists of two primary components: 1)\nProgressive Visual Token Pruning (\\textbf{PVTP}), which eliminates inattentive\nvisual tokens across layers, and 2) Visual Token Annealing (\\textbf{VTA}),\nwhich dynamically reduces the number of visual tokens in each layer as the\ngenerated tokens grow. Together, these techniques deliver around\n$\\mathbf{2\\times}$ faster inference with only about $\\mathbf{30\\%}$ KV cache\nmemory compared to the original LLaVA, while maintaining consistent performance\nacross various datasets. Crucially, $\\textbf{ST}^{3}$ can be seamlessly\nintegrated into existing pre-trained MLLMs, providing a plug-and-play solution\nfor efficient inference."
                },
                "authors": [
                    {
                        "name": "Jiedong Zhuang"
                    },
                    {
                        "name": "Lu Lu"
                    },
                    {
                        "name": "Ming Dai"
                    },
                    {
                        "name": "Rui Hu"
                    },
                    {
                        "name": "Jian Chen"
                    },
                    {
                        "name": "Qiang Liu"
                    },
                    {
                        "name": "Haoji Hu"
                    }
                ],
                "author_detail": {
                    "name": "Haoji Hu"
                },
                "author": "Haoji Hu",
                "arxiv_comment": "Accepted to AAAI2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.20105v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.20105v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.19991v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.19991v1",
                "updated": "2024-12-28T03:28:52Z",
                "updated_parsed": [
                    2024,
                    12,
                    28,
                    3,
                    28,
                    52,
                    5,
                    363,
                    0
                ],
                "published": "2024-12-28T03:28:52Z",
                "published_parsed": [
                    2024,
                    12,
                    28,
                    3,
                    28,
                    52,
                    5,
                    363,
                    0
                ],
                "title": "A Robust Federated Learning Framework for Undependable Devices at Scale",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Robust Federated Learning Framework for Undependable Devices at Scale"
                },
                "summary": "In a federated learning (FL) system, many devices, such as smartphones, are\noften undependable (e.g., frequently disconnected from WiFi) during training.\nExisting FL frameworks always assume a dependable environment and exclude\nundependable devices from training, leading to poor model performance and\nresource wastage. In this paper, we propose FLUDE to effectively deal with\nundependable environments. First, FLUDE assesses the dependability of devices\nbased on the probability distribution of their historical behaviors (e.g., the\nlikelihood of successfully completing training). Based on this assessment,\nFLUDE adaptively selects devices with high dependability for training. To\nmitigate resource wastage during the training phase, FLUDE maintains a model\ncache on each device, aiming to preserve the latest training state for later\nuse in case local training on an undependable device is interrupted. Moreover,\nFLUDE proposes a staleness-aware strategy to judiciously distribute the global\nmodel to a subset of devices, thus significantly reducing resource wastage\nwhile maintaining model performance. We have implemented FLUDE on two physical\nplatforms with 120 smartphones and NVIDIA Jetson devices. Extensive\nexperimental results demonstrate that FLUDE can effectively improve model\nperformance and resource efficiency of FL training in undependable\nenvironments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In a federated learning (FL) system, many devices, such as smartphones, are\noften undependable (e.g., frequently disconnected from WiFi) during training.\nExisting FL frameworks always assume a dependable environment and exclude\nundependable devices from training, leading to poor model performance and\nresource wastage. In this paper, we propose FLUDE to effectively deal with\nundependable environments. First, FLUDE assesses the dependability of devices\nbased on the probability distribution of their historical behaviors (e.g., the\nlikelihood of successfully completing training). Based on this assessment,\nFLUDE adaptively selects devices with high dependability for training. To\nmitigate resource wastage during the training phase, FLUDE maintains a model\ncache on each device, aiming to preserve the latest training state for later\nuse in case local training on an undependable device is interrupted. Moreover,\nFLUDE proposes a staleness-aware strategy to judiciously distribute the global\nmodel to a subset of devices, thus significantly reducing resource wastage\nwhile maintaining model performance. We have implemented FLUDE on two physical\nplatforms with 120 smartphones and NVIDIA Jetson devices. Extensive\nexperimental results demonstrate that FLUDE can effectively improve model\nperformance and resource efficiency of FL training in undependable\nenvironments."
                },
                "authors": [
                    {
                        "name": "Shilong Wang"
                    },
                    {
                        "name": "Jianchun Liu"
                    },
                    {
                        "name": "Hongli Xu"
                    },
                    {
                        "name": "Chunming Qiao"
                    },
                    {
                        "name": "Huarong Deng"
                    },
                    {
                        "name": "Qiuye Zheng"
                    },
                    {
                        "name": "Jiantao Gong"
                    }
                ],
                "author_detail": {
                    "name": "Jiantao Gong"
                },
                "author": "Jiantao Gong",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.19991v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.19991v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            }
        ]
    },
    {
        "keyword": "LLM Inference",
        "arxiv_results": [
            {
                "id": "http://arxiv.org/abs/2501.13928v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.13928v1",
                "updated": "2025-01-23T18:59:55Z",
                "updated_parsed": [
                    2025,
                    1,
                    23,
                    18,
                    59,
                    55,
                    3,
                    23,
                    0
                ],
                "published": "2025-01-23T18:59:55Z",
                "published_parsed": [
                    2025,
                    1,
                    23,
                    18,
                    59,
                    55,
                    3,
                    23,
                    0
                ],
                "title": "Fast3R: Towards 3D Reconstruction of 1000+ Images in One Forward Pass",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Fast3R: Towards 3D Reconstruction of 1000+ Images in One Forward Pass"
                },
                "summary": "Multi-view 3D reconstruction remains a core challenge in computer vision,\nparticularly in applications requiring accurate and scalable representations\nacross diverse perspectives. Current leading methods such as DUSt3R employ a\nfundamentally pairwise approach, processing images in pairs and necessitating\ncostly global alignment procedures to reconstruct from multiple views. In this\nwork, we propose Fast 3D Reconstruction (Fast3R), a novel multi-view\ngeneralization to DUSt3R that achieves efficient and scalable 3D reconstruction\nby processing many views in parallel. Fast3R's Transformer-based architecture\nforwards N images in a single forward pass, bypassing the need for iterative\nalignment. Through extensive experiments on camera pose estimation and 3D\nreconstruction, Fast3R demonstrates state-of-the-art performance, with\nsignificant improvements in inference speed and reduced error accumulation.\nThese results establish Fast3R as a robust alternative for multi-view\napplications, offering enhanced scalability without compromising reconstruction\naccuracy.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-view 3D reconstruction remains a core challenge in computer vision,\nparticularly in applications requiring accurate and scalable representations\nacross diverse perspectives. Current leading methods such as DUSt3R employ a\nfundamentally pairwise approach, processing images in pairs and necessitating\ncostly global alignment procedures to reconstruct from multiple views. In this\nwork, we propose Fast 3D Reconstruction (Fast3R), a novel multi-view\ngeneralization to DUSt3R that achieves efficient and scalable 3D reconstruction\nby processing many views in parallel. Fast3R's Transformer-based architecture\nforwards N images in a single forward pass, bypassing the need for iterative\nalignment. Through extensive experiments on camera pose estimation and 3D\nreconstruction, Fast3R demonstrates state-of-the-art performance, with\nsignificant improvements in inference speed and reduced error accumulation.\nThese results establish Fast3R as a robust alternative for multi-view\napplications, offering enhanced scalability without compromising reconstruction\naccuracy."
                },
                "authors": [
                    {
                        "name": "Jianing Yang"
                    },
                    {
                        "name": "Alexander Sax"
                    },
                    {
                        "name": "Kevin J. Liang"
                    },
                    {
                        "name": "Mikael Henaff"
                    },
                    {
                        "name": "Hao Tang"
                    },
                    {
                        "name": "Ang Cao"
                    },
                    {
                        "name": "Joyce Chai"
                    },
                    {
                        "name": "Franziska Meier"
                    },
                    {
                        "name": "Matt Feiszli"
                    }
                ],
                "author_detail": {
                    "name": "Matt Feiszli"
                },
                "author": "Matt Feiszli",
                "arxiv_comment": "Project website: https://fast3r-3d.github.io/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.13928v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.13928v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.GR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.13927v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.13927v1",
                "updated": "2025-01-23T18:59:47Z",
                "updated_parsed": [
                    2025,
                    1,
                    23,
                    18,
                    59,
                    47,
                    3,
                    23,
                    0
                ],
                "published": "2025-01-23T18:59:47Z",
                "published_parsed": [
                    2025,
                    1,
                    23,
                    18,
                    59,
                    47,
                    3,
                    23,
                    0
                ],
                "title": "CRPO: Confidence-Reward Driven Preference Optimization for Machine\n  Translation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CRPO: Confidence-Reward Driven Preference Optimization for Machine\n  Translation"
                },
                "summary": "Large language models (LLMs) have shown great potential in natural language\nprocessing tasks, but their application to machine translation (MT) remains\nchallenging due to pretraining on English-centric data and the complexity of\nreinforcement learning from human feedback (RLHF). Direct Preference\nOptimization (DPO) has emerged as a simpler and more efficient alternative, but\nits performance depends heavily on the quality of preference data. To address\nthis, we propose Confidence-Reward driven Preference Optimization (CRPO), a\nnovel method that combines reward scores with model confidence to improve data\nselection for fine-tuning. CRPO selects challenging sentence pairs where the\nmodel is uncertain or underperforms, leading to more effective learning. While\nprimarily designed for LLMs, CRPO also generalizes to encoder-decoder models\nlike NLLB, demonstrating its versatility. Empirical results show that CRPO\noutperforms existing methods such as RS-DPO, RSO and MBR score in both\ntranslation accuracy and data efficiency.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have shown great potential in natural language\nprocessing tasks, but their application to machine translation (MT) remains\nchallenging due to pretraining on English-centric data and the complexity of\nreinforcement learning from human feedback (RLHF). Direct Preference\nOptimization (DPO) has emerged as a simpler and more efficient alternative, but\nits performance depends heavily on the quality of preference data. To address\nthis, we propose Confidence-Reward driven Preference Optimization (CRPO), a\nnovel method that combines reward scores with model confidence to improve data\nselection for fine-tuning. CRPO selects challenging sentence pairs where the\nmodel is uncertain or underperforms, leading to more effective learning. While\nprimarily designed for LLMs, CRPO also generalizes to encoder-decoder models\nlike NLLB, demonstrating its versatility. Empirical results show that CRPO\noutperforms existing methods such as RS-DPO, RSO and MBR score in both\ntranslation accuracy and data efficiency."
                },
                "authors": [
                    {
                        "name": "Guofeng Cui"
                    },
                    {
                        "name": "Pichao Wang"
                    },
                    {
                        "name": "Yang Liu"
                    },
                    {
                        "name": "Zemian Ke"
                    },
                    {
                        "name": "Zhu Liu"
                    },
                    {
                        "name": "Vimal Bhat"
                    }
                ],
                "author_detail": {
                    "name": "Vimal Bhat"
                },
                "author": "Vimal Bhat",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.13927v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.13927v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.13921v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.13921v1",
                "updated": "2025-01-23T18:59:02Z",
                "updated_parsed": [
                    2025,
                    1,
                    23,
                    18,
                    59,
                    2,
                    3,
                    23,
                    0
                ],
                "published": "2025-01-23T18:59:02Z",
                "published_parsed": [
                    2025,
                    1,
                    23,
                    18,
                    59,
                    2,
                    3,
                    23,
                    0
                ],
                "title": "The Breeze 2 Herd of Models: Traditional Chinese LLMs Based on Llama\n  with Vision-Aware and Function-Calling Capabilities",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Breeze 2 Herd of Models: Traditional Chinese LLMs Based on Llama\n  with Vision-Aware and Function-Calling Capabilities"
                },
                "summary": "Breeze 2 is a suite of advanced multi-modal language models, available in 3B\nand 8B parameter configurations, specifically designed to enhance Traditional\nChinese language representation. Building upon the Llama 3, Breeze 2 continues\npretraining on an extensive corpus to enhance the linguistic and cultural\nheritage of Traditional Chinese. It incorporates vision-aware capabilities\nthrough a visual encoder and a bridge module, and supports function-calling via\nprompt templates and post-training on function-calling data. The effectiveness\nof Breeze 2 is benchmarked across various tasks, including Taiwan general\nknowledge, instruction-following, long context, function calling, and vision\nunderstanding. Furthermore, we showcase the capabilities of the its 3B model in\na mobile application. We are publicly releasing all Breeze 2 models under the\nLlama 3 Community License.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Breeze 2 is a suite of advanced multi-modal language models, available in 3B\nand 8B parameter configurations, specifically designed to enhance Traditional\nChinese language representation. Building upon the Llama 3, Breeze 2 continues\npretraining on an extensive corpus to enhance the linguistic and cultural\nheritage of Traditional Chinese. It incorporates vision-aware capabilities\nthrough a visual encoder and a bridge module, and supports function-calling via\nprompt templates and post-training on function-calling data. The effectiveness\nof Breeze 2 is benchmarked across various tasks, including Taiwan general\nknowledge, instruction-following, long context, function calling, and vision\nunderstanding. Furthermore, we showcase the capabilities of the its 3B model in\na mobile application. We are publicly releasing all Breeze 2 models under the\nLlama 3 Community License."
                },
                "authors": [
                    {
                        "name": "Chan-Jan Hsu"
                    },
                    {
                        "name": "Chia-Sheng Liu"
                    },
                    {
                        "name": "Meng-Hsi Chen"
                    },
                    {
                        "name": "Muxi Chen"
                    },
                    {
                        "name": "Po-Chun Hsu"
                    },
                    {
                        "name": "Yi-Chang Chen"
                    },
                    {
                        "name": "Da-Shan Shiu"
                    }
                ],
                "author_detail": {
                    "name": "Da-Shan Shiu"
                },
                "author": "Da-Shan Shiu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.13921v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.13921v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.13918v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.13918v1",
                "updated": "2025-01-23T18:55:41Z",
                "updated_parsed": [
                    2025,
                    1,
                    23,
                    18,
                    55,
                    41,
                    3,
                    23,
                    0
                ],
                "published": "2025-01-23T18:55:41Z",
                "published_parsed": [
                    2025,
                    1,
                    23,
                    18,
                    55,
                    41,
                    3,
                    23,
                    0
                ],
                "title": "Improving Video Generation with Human Feedback",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Improving Video Generation with Human Feedback"
                },
                "summary": "Video generation has achieved significant advances through rectified flow\ntechniques, but issues like unsmooth motion and misalignment between videos and\nprompts persist. In this work, we develop a systematic pipeline that harnesses\nhuman feedback to mitigate these problems and refine the video generation\nmodel. Specifically, we begin by constructing a large-scale human preference\ndataset focused on modern video generation models, incorporating pairwise\nannotations across multi-dimensions. We then introduce VideoReward, a\nmulti-dimensional video reward model, and examine how annotations and various\ndesign choices impact its rewarding efficacy. From a unified reinforcement\nlearning perspective aimed at maximizing reward with KL regularization, we\nintroduce three alignment algorithms for flow-based models by extending those\nfrom diffusion models. These include two training-time strategies: direct\npreference optimization for flow (Flow-DPO) and reward weighted regression for\nflow (Flow-RWR), and an inference-time technique, Flow-NRG, which applies\nreward guidance directly to noisy videos. Experimental results indicate that\nVideoReward significantly outperforms existing reward models, and Flow-DPO\ndemonstrates superior performance compared to both Flow-RWR and standard\nsupervised fine-tuning methods. Additionally, Flow-NRG lets users assign custom\nweights to multiple objectives during inference, meeting personalized video\nquality needs. Project page: https://gongyeliu.github.io/videoalign.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Video generation has achieved significant advances through rectified flow\ntechniques, but issues like unsmooth motion and misalignment between videos and\nprompts persist. In this work, we develop a systematic pipeline that harnesses\nhuman feedback to mitigate these problems and refine the video generation\nmodel. Specifically, we begin by constructing a large-scale human preference\ndataset focused on modern video generation models, incorporating pairwise\nannotations across multi-dimensions. We then introduce VideoReward, a\nmulti-dimensional video reward model, and examine how annotations and various\ndesign choices impact its rewarding efficacy. From a unified reinforcement\nlearning perspective aimed at maximizing reward with KL regularization, we\nintroduce three alignment algorithms for flow-based models by extending those\nfrom diffusion models. These include two training-time strategies: direct\npreference optimization for flow (Flow-DPO) and reward weighted regression for\nflow (Flow-RWR), and an inference-time technique, Flow-NRG, which applies\nreward guidance directly to noisy videos. Experimental results indicate that\nVideoReward significantly outperforms existing reward models, and Flow-DPO\ndemonstrates superior performance compared to both Flow-RWR and standard\nsupervised fine-tuning methods. Additionally, Flow-NRG lets users assign custom\nweights to multiple objectives during inference, meeting personalized video\nquality needs. Project page: https://gongyeliu.github.io/videoalign."
                },
                "authors": [
                    {
                        "name": "Jie Liu"
                    },
                    {
                        "name": "Gongye Liu"
                    },
                    {
                        "name": "Jiajun Liang"
                    },
                    {
                        "name": "Ziyang Yuan"
                    },
                    {
                        "name": "Xiaokun Liu"
                    },
                    {
                        "name": "Mingwu Zheng"
                    },
                    {
                        "name": "Xiele Wu"
                    },
                    {
                        "name": "Qiulin Wang"
                    },
                    {
                        "name": "Wenyu Qin"
                    },
                    {
                        "name": "Menghan Xia"
                    },
                    {
                        "name": "Xintao Wang"
                    },
                    {
                        "name": "Xiaohong Liu"
                    },
                    {
                        "name": "Fei Yang"
                    },
                    {
                        "name": "Pengfei Wan"
                    },
                    {
                        "name": "Di Zhang"
                    },
                    {
                        "name": "Kun Gai"
                    },
                    {
                        "name": "Yujiu Yang"
                    },
                    {
                        "name": "Wanli Ouyang"
                    }
                ],
                "author_detail": {
                    "name": "Wanli Ouyang"
                },
                "author": "Wanli Ouyang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.13918v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.13918v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.GR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.13915v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.13915v1",
                "updated": "2025-01-23T18:52:47Z",
                "updated_parsed": [
                    2025,
                    1,
                    23,
                    18,
                    52,
                    47,
                    3,
                    23,
                    0
                ],
                "published": "2025-01-23T18:52:47Z",
                "published_parsed": [
                    2025,
                    1,
                    23,
                    18,
                    52,
                    47,
                    3,
                    23,
                    0
                ],
                "title": "Binary Diffusion Probabilistic Model",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Binary Diffusion Probabilistic Model"
                },
                "summary": "We introduce the Binary Diffusion Probabilistic Model (BDPM), a novel\ngenerative model optimized for binary data representations. While denoising\ndiffusion probabilistic models (DDPMs) have demonstrated notable success in\ntasks like image synthesis and restoration, traditional DDPMs rely on\ncontinuous data representations and mean squared error (MSE) loss for training,\napplying Gaussian noise models that may not be optimal for discrete or binary\ndata structures. BDPM addresses this by decomposing images into bitplanes and\nemploying XOR-based noise transformations, with a denoising model trained using\nbinary cross-entropy loss. This approach enables precise noise control and\ncomputationally efficient inference, significantly lowering computational costs\nand improving model convergence. When evaluated on image restoration tasks such\nas image super-resolution, inpainting, and blind image restoration, BDPM\noutperforms state-of-the-art methods on the FFHQ, CelebA, and CelebA-HQ\ndatasets. Notably, BDPM requires fewer inference steps than traditional DDPM\nmodels to reach optimal results, showcasing enhanced inference efficiency.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce the Binary Diffusion Probabilistic Model (BDPM), a novel\ngenerative model optimized for binary data representations. While denoising\ndiffusion probabilistic models (DDPMs) have demonstrated notable success in\ntasks like image synthesis and restoration, traditional DDPMs rely on\ncontinuous data representations and mean squared error (MSE) loss for training,\napplying Gaussian noise models that may not be optimal for discrete or binary\ndata structures. BDPM addresses this by decomposing images into bitplanes and\nemploying XOR-based noise transformations, with a denoising model trained using\nbinary cross-entropy loss. This approach enables precise noise control and\ncomputationally efficient inference, significantly lowering computational costs\nand improving model convergence. When evaluated on image restoration tasks such\nas image super-resolution, inpainting, and blind image restoration, BDPM\noutperforms state-of-the-art methods on the FFHQ, CelebA, and CelebA-HQ\ndatasets. Notably, BDPM requires fewer inference steps than traditional DDPM\nmodels to reach optimal results, showcasing enhanced inference efficiency."
                },
                "authors": [
                    {
                        "name": "Vitaliy Kinakh"
                    },
                    {
                        "name": "Slava Voloshynovskiy"
                    }
                ],
                "author_detail": {
                    "name": "Slava Voloshynovskiy"
                },
                "author": "Slava Voloshynovskiy",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.13915v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.13915v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.13912v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.13912v1",
                "updated": "2025-01-23T18:49:33Z",
                "updated_parsed": [
                    2025,
                    1,
                    23,
                    18,
                    49,
                    33,
                    3,
                    23,
                    0
                ],
                "published": "2025-01-23T18:49:33Z",
                "published_parsed": [
                    2025,
                    1,
                    23,
                    18,
                    49,
                    33,
                    3,
                    23,
                    0
                ],
                "title": "Analysis of Indic Language Capabilities in LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Analysis of Indic Language Capabilities in LLMs"
                },
                "summary": "This report evaluates the performance of text-in text-out Large Language\nModels (LLMs) to understand and generate Indic languages. This evaluation is\nused to identify and prioritize Indic languages suited for inclusion in safety\nbenchmarks. We conduct this study by reviewing existing evaluation studies and\ndatasets; and a set of twenty-eight LLMs that support Indic languages. We\nanalyze the LLMs on the basis of the training data, license for model and data,\ntype of access and model developers. We also compare Indic language performance\nacross evaluation datasets and find that significant performance disparities in\nperformance across Indic languages. Hindi is the most widely represented\nlanguage in models. While model performance roughly correlates with number of\nspeakers for the top five languages, the assessment after that varies.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This report evaluates the performance of text-in text-out Large Language\nModels (LLMs) to understand and generate Indic languages. This evaluation is\nused to identify and prioritize Indic languages suited for inclusion in safety\nbenchmarks. We conduct this study by reviewing existing evaluation studies and\ndatasets; and a set of twenty-eight LLMs that support Indic languages. We\nanalyze the LLMs on the basis of the training data, license for model and data,\ntype of access and model developers. We also compare Indic language performance\nacross evaluation datasets and find that significant performance disparities in\nperformance across Indic languages. Hindi is the most widely represented\nlanguage in models. While model performance roughly correlates with number of\nspeakers for the top five languages, the assessment after that varies."
                },
                "authors": [
                    {
                        "name": "Aatman Vaidya"
                    },
                    {
                        "name": "Tarunima Prabhakar"
                    },
                    {
                        "name": "Denny George"
                    },
                    {
                        "name": "Swair Shah"
                    }
                ],
                "author_detail": {
                    "name": "Swair Shah"
                },
                "author": "Swair Shah",
                "arxiv_comment": "17 pages, 2 figures, 5 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.13912v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.13912v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2311.13303v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2311.13303v3",
                "updated": "2025-01-23T18:47:11Z",
                "updated_parsed": [
                    2025,
                    1,
                    23,
                    18,
                    47,
                    11,
                    3,
                    23,
                    0
                ],
                "published": "2023-11-22T10:47:28Z",
                "published_parsed": [
                    2023,
                    11,
                    22,
                    10,
                    47,
                    28,
                    2,
                    326,
                    0
                ],
                "title": "Long-term spin-down and low luminosity regime in the Be/X-ray binary\n  pulsar GX 304-1",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Long-term spin-down and low luminosity regime in the Be/X-ray binary\n  pulsar GX 304-1"
                },
                "summary": "We carry out timing and spectral studies of the Be/X-ray binary pulsar GX\n304-1 using NuStar and XMM-Newton observations. We construct the long-term spin\nperiod evolution of the pulsar which changes from a long-term spin-up ($\\sim\n1.3 \\times 10^{-13} $Hz \\~s$^{-1}$) to a long-term spin-down ($\\sim -3.4 \\times\n10^{-14} $Hz \\~s$^{-1}$) trend during a low luminosity state ($\\sim 10^{34-35}\n$erg \\~s$^{-1}$). A prolonged low luminosity regime ($L_X \\sim 10^{34-35} $erg\n\\~s$^{-1}$) was detected during 2005-2010 and spanning nearly five years since\n2018 December. The XMM-Newton and NuStar spectra can be described with a power\nlaw plus blackbody model having an estimated luminosity of $\\sim 2.5 \\times\n10^{33} $erg \\~s$^{-1}$ and $\\sim 3.6 \\times 10^{33} $erg \\~s$^{-1}$\nrespectively. The inferred radius of the blackbody emission is about 100-110 m\nwhich suggests a polar-cap origin of this component. From long-term ultraviolet\nobservations of the companion star, an increase in the ultraviolet signatures\nis detected preceding the X-ray outbursts. The spectral energy distribution of\nthe companion star is constructed which provides a clue of possible UV excess\nwhen X-ray outbursts were detected from the neutron star compared to the\nquiescent phase. We explore plausible mechanisms to explain the long-term\nspin-down and extended low luminosity manifestation in this pulsar. We find\nthat sustained accretion from a cold disc may explain the prolonged low\nluminosity state of the pulsar since December 2018 but the pulsar was\nundergoing normal accretion during the low luminosity period spanning\n2005-2010.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We carry out timing and spectral studies of the Be/X-ray binary pulsar GX\n304-1 using NuStar and XMM-Newton observations. We construct the long-term spin\nperiod evolution of the pulsar which changes from a long-term spin-up ($\\sim\n1.3 \\times 10^{-13} $Hz \\~s$^{-1}$) to a long-term spin-down ($\\sim -3.4 \\times\n10^{-14} $Hz \\~s$^{-1}$) trend during a low luminosity state ($\\sim 10^{34-35}\n$erg \\~s$^{-1}$). A prolonged low luminosity regime ($L_X \\sim 10^{34-35} $erg\n\\~s$^{-1}$) was detected during 2005-2010 and spanning nearly five years since\n2018 December. The XMM-Newton and NuStar spectra can be described with a power\nlaw plus blackbody model having an estimated luminosity of $\\sim 2.5 \\times\n10^{33} $erg \\~s$^{-1}$ and $\\sim 3.6 \\times 10^{33} $erg \\~s$^{-1}$\nrespectively. The inferred radius of the blackbody emission is about 100-110 m\nwhich suggests a polar-cap origin of this component. From long-term ultraviolet\nobservations of the companion star, an increase in the ultraviolet signatures\nis detected preceding the X-ray outbursts. The spectral energy distribution of\nthe companion star is constructed which provides a clue of possible UV excess\nwhen X-ray outbursts were detected from the neutron star compared to the\nquiescent phase. We explore plausible mechanisms to explain the long-term\nspin-down and extended low luminosity manifestation in this pulsar. We find\nthat sustained accretion from a cold disc may explain the prolonged low\nluminosity state of the pulsar since December 2018 but the pulsar was\nundergoing normal accretion during the low luminosity period spanning\n2005-2010."
                },
                "authors": [
                    {
                        "name": "Amar Deo Chandra"
                    }
                ],
                "author_detail": {
                    "name": "Amar Deo Chandra"
                },
                "author": "Amar Deo Chandra",
                "arxiv_comment": "18 pages, 13 figures; accepted for publication in the Publications of\n  the Astronomical Society of Australia (PASA)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2311.13303v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2311.13303v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.HE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.HE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.03797v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.03797v2",
                "updated": "2025-01-23T18:44:28Z",
                "updated_parsed": [
                    2025,
                    1,
                    23,
                    18,
                    44,
                    28,
                    3,
                    23,
                    0
                ],
                "published": "2024-09-04T17:53:24Z",
                "published_parsed": [
                    2024,
                    9,
                    4,
                    17,
                    53,
                    24,
                    2,
                    248,
                    0
                ],
                "title": "NESTFUL: A Benchmark for Evaluating LLMs on Nested Sequences of API\n  Calls",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "NESTFUL: A Benchmark for Evaluating LLMs on Nested Sequences of API\n  Calls"
                },
                "summary": "The resurgence of autonomous agents built using large language models (LLMs)\nto solve complex real-world tasks has brought increased focus on LLMs'\nfundamental ability of tool or function calling. At the core of these agents,\nan LLM must plan, execute, and respond using external tools, APIs, and custom\nfunctions. Research on tool calling has gathered momentum, but evaluation\nbenchmarks and datasets representing the complexity of the tasks have lagged\nbehind. In this work, we focus on one such complexity, nested sequencing, with\nthe goal of extending existing benchmarks and evaluation. Specifically, we\npresent NESTFUL, a benchmark to evaluate LLMs on nested sequences of API calls,\ni.e., sequences where the output of one API call is passed as input to a\nsubsequent call. NESTFUL contains 1800+ nested sequences where all the function\ncalls are executable. Experimental results on multiple models and settings show\nthat the best-performing model on the dataset has a full sequence match\naccuracy of 25% and win-rate of 34% necessitating a large scope for improvement\nin the nested sequencing aspect of function calling. Our analysis of these\nresults provides possible future research directions for the community, in\naddition to a benchmark to track progress. We have released the NESTFUL dataset\nunder the Apache 2.0 license at https://github.com/IBM/NESTFUL.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The resurgence of autonomous agents built using large language models (LLMs)\nto solve complex real-world tasks has brought increased focus on LLMs'\nfundamental ability of tool or function calling. At the core of these agents,\nan LLM must plan, execute, and respond using external tools, APIs, and custom\nfunctions. Research on tool calling has gathered momentum, but evaluation\nbenchmarks and datasets representing the complexity of the tasks have lagged\nbehind. In this work, we focus on one such complexity, nested sequencing, with\nthe goal of extending existing benchmarks and evaluation. Specifically, we\npresent NESTFUL, a benchmark to evaluate LLMs on nested sequences of API calls,\ni.e., sequences where the output of one API call is passed as input to a\nsubsequent call. NESTFUL contains 1800+ nested sequences where all the function\ncalls are executable. Experimental results on multiple models and settings show\nthat the best-performing model on the dataset has a full sequence match\naccuracy of 25% and win-rate of 34% necessitating a large scope for improvement\nin the nested sequencing aspect of function calling. Our analysis of these\nresults provides possible future research directions for the community, in\naddition to a benchmark to track progress. We have released the NESTFUL dataset\nunder the Apache 2.0 license at https://github.com/IBM/NESTFUL."
                },
                "authors": [
                    {
                        "name": "Kinjal Basu"
                    },
                    {
                        "name": "Ibrahim Abdelaziz"
                    },
                    {
                        "name": "Kiran Kate"
                    },
                    {
                        "name": "Mayank Agarwal"
                    },
                    {
                        "name": "Maxwell Crouse"
                    },
                    {
                        "name": "Yara Rizk"
                    },
                    {
                        "name": "Kelsey Bradford"
                    },
                    {
                        "name": "Asim Munawar"
                    },
                    {
                        "name": "Sadhana Kumaravel"
                    },
                    {
                        "name": "Saurabh Goyal"
                    },
                    {
                        "name": "Xin Wang"
                    },
                    {
                        "name": "Luis A. Lastras"
                    },
                    {
                        "name": "Pavan Kapanipathi"
                    }
                ],
                "author_detail": {
                    "name": "Pavan Kapanipathi"
                },
                "author": "Pavan Kapanipathi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.03797v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.03797v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.13904v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.13904v1",
                "updated": "2025-01-23T18:34:09Z",
                "updated_parsed": [
                    2025,
                    1,
                    23,
                    18,
                    34,
                    9,
                    3,
                    23,
                    0
                ],
                "published": "2025-01-23T18:34:09Z",
                "published_parsed": [
                    2025,
                    1,
                    23,
                    18,
                    34,
                    9,
                    3,
                    23,
                    0
                ],
                "title": "Privacy-Preserving Personalized Federated Prompt Learning for Multimodal\n  Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Privacy-Preserving Personalized Federated Prompt Learning for Multimodal\n  Large Language Models"
                },
                "summary": "Multimodal Large Language Models (LLMs) are pivotal in revolutionizing\ncustomer support and operations by integrating multiple modalities such as\ntext, images, and audio. Federated Prompt Learning (FPL) is a recently proposed\napproach that combines pre-trained multimodal LLMs such as vision-language\nmodels with federated learning to create personalized, privacy-preserving AI\nsystems. However, balancing the competing goals of personalization,\ngeneralization, and privacy remains a significant challenge.\nOver-personalization can lead to overfitting, reducing generalizability, while\nstringent privacy measures, such as differential privacy, can hinder both\npersonalization and generalization. In this paper, we propose a Differentially\nPrivate Federated Prompt Learning (DP-FPL) approach to tackle this challenge by\nleveraging a low-rank adaptation scheme to capture generalization while\nmaintaining a residual term that preserves expressiveness for personalization.\nTo ensure privacy, we introduce a novel method where we apply local\ndifferential privacy to the two low-rank components of the local prompt, and\nglobal differential privacy to the global prompt. Our approach mitigates the\nimpact of privacy noise on the model performance while balancing the tradeoff\nbetween personalization and generalization. Extensive experiments demonstrate\nthe effectiveness of our approach over other benchmarks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multimodal Large Language Models (LLMs) are pivotal in revolutionizing\ncustomer support and operations by integrating multiple modalities such as\ntext, images, and audio. Federated Prompt Learning (FPL) is a recently proposed\napproach that combines pre-trained multimodal LLMs such as vision-language\nmodels with federated learning to create personalized, privacy-preserving AI\nsystems. However, balancing the competing goals of personalization,\ngeneralization, and privacy remains a significant challenge.\nOver-personalization can lead to overfitting, reducing generalizability, while\nstringent privacy measures, such as differential privacy, can hinder both\npersonalization and generalization. In this paper, we propose a Differentially\nPrivate Federated Prompt Learning (DP-FPL) approach to tackle this challenge by\nleveraging a low-rank adaptation scheme to capture generalization while\nmaintaining a residual term that preserves expressiveness for personalization.\nTo ensure privacy, we introduce a novel method where we apply local\ndifferential privacy to the two low-rank components of the local prompt, and\nglobal differential privacy to the global prompt. Our approach mitigates the\nimpact of privacy noise on the model performance while balancing the tradeoff\nbetween personalization and generalization. Extensive experiments demonstrate\nthe effectiveness of our approach over other benchmarks."
                },
                "authors": [
                    {
                        "name": "Linh Tran"
                    },
                    {
                        "name": "Wei Sun"
                    },
                    {
                        "name": "Stacy Patterson"
                    },
                    {
                        "name": "Ana Milanova"
                    }
                ],
                "author_detail": {
                    "name": "Ana Milanova"
                },
                "author": "Ana Milanova",
                "arxiv_comment": "Accepted to ICLR 2025 main conference track",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.13904v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.13904v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.13896v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.13896v1",
                "updated": "2025-01-23T18:16:21Z",
                "updated_parsed": [
                    2025,
                    1,
                    23,
                    18,
                    16,
                    21,
                    3,
                    23,
                    0
                ],
                "published": "2025-01-23T18:16:21Z",
                "published_parsed": [
                    2025,
                    1,
                    23,
                    18,
                    16,
                    21,
                    3,
                    23,
                    0
                ],
                "title": "GUI-Bee: Align GUI Action Grounding to Novel Environments via Autonomous\n  Exploration",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GUI-Bee: Align GUI Action Grounding to Novel Environments via Autonomous\n  Exploration"
                },
                "summary": "Graphical User Interface (GUI) action grounding is a critical step in GUI\nautomation that maps language instructions to actionable elements on GUI\nscreens. Most recent works of GUI action grounding leverage large GUI datasets\nto fine-tune MLLMs. However, the fine-tuning data always covers limited GUI\nenvironments, and we find the performance of the resulting model deteriorates\nin novel environments. We argue that the GUI grounding models should be further\naligned to the novel environments to reveal their full potential, when the\ninference is known to involve novel environments, i.e., environments not used\nduring the previous fine-tuning. To realize this, we first propose GUI-Bee, an\nMLLM-based autonomous agent, to collect high-quality, environment-specific data\nthrough exploration and then continuously fine-tune GUI grounding models with\nthe collected data. Our agent leverages a novel Q-value-Incentive In-Context\nReinforcement Learning (Q-ICRL) method to optimize exploration efficiency and\ndata quality. Additionally, we introduce NovelScreenSpot, a benchmark for\ntesting how well the data can help align GUI action grounding models to novel\nenvironments and demonstrate the effectiveness of data collected by GUI-Bee in\nthe experiments. Furthermore, we conduct an ablation study to validate the\nQ-ICRL method in enhancing the efficiency of GUI-Bee. Project page:\nhttps://gui-bee.github.io",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Graphical User Interface (GUI) action grounding is a critical step in GUI\nautomation that maps language instructions to actionable elements on GUI\nscreens. Most recent works of GUI action grounding leverage large GUI datasets\nto fine-tune MLLMs. However, the fine-tuning data always covers limited GUI\nenvironments, and we find the performance of the resulting model deteriorates\nin novel environments. We argue that the GUI grounding models should be further\naligned to the novel environments to reveal their full potential, when the\ninference is known to involve novel environments, i.e., environments not used\nduring the previous fine-tuning. To realize this, we first propose GUI-Bee, an\nMLLM-based autonomous agent, to collect high-quality, environment-specific data\nthrough exploration and then continuously fine-tune GUI grounding models with\nthe collected data. Our agent leverages a novel Q-value-Incentive In-Context\nReinforcement Learning (Q-ICRL) method to optimize exploration efficiency and\ndata quality. Additionally, we introduce NovelScreenSpot, a benchmark for\ntesting how well the data can help align GUI action grounding models to novel\nenvironments and demonstrate the effectiveness of data collected by GUI-Bee in\nthe experiments. Furthermore, we conduct an ablation study to validate the\nQ-ICRL method in enhancing the efficiency of GUI-Bee. Project page:\nhttps://gui-bee.github.io"
                },
                "authors": [
                    {
                        "name": "Yue Fan"
                    },
                    {
                        "name": "Handong Zhao"
                    },
                    {
                        "name": "Ruiyi Zhang"
                    },
                    {
                        "name": "Yu Shen"
                    },
                    {
                        "name": "Xin Eric Wang"
                    },
                    {
                        "name": "Gang Wu"
                    }
                ],
                "author_detail": {
                    "name": "Gang Wu"
                },
                "author": "Gang Wu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.13896v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.13896v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.13107v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.13107v2",
                "updated": "2025-01-23T18:13:35Z",
                "updated_parsed": [
                    2025,
                    1,
                    23,
                    18,
                    13,
                    35,
                    3,
                    23,
                    0
                ],
                "published": "2025-01-22T18:59:58Z",
                "published_parsed": [
                    2025,
                    1,
                    22,
                    18,
                    59,
                    58,
                    2,
                    22,
                    0
                ],
                "title": "Accelerate High-Quality Diffusion Models with Inner Loop Feedback",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Accelerate High-Quality Diffusion Models with Inner Loop Feedback"
                },
                "summary": "We propose Inner Loop Feedback (ILF), a novel approach to accelerate\ndiffusion models' inference. ILF trains a lightweight module to predict future\nfeatures in the denoising process by leveraging the outputs from a chosen\ndiffusion backbone block at a given time step. This approach exploits two key\nintuitions; (1) the outputs of a given block at adjacent time steps are\nsimilar, and (2) performing partial computations for a step imposes a lower\nburden on the model than skipping the step entirely. Our method is highly\nflexible, since we find that the feedback module itself can simply be a block\nfrom the diffusion backbone, with all settings copied. Its influence on the\ndiffusion forward can be tempered with a learnable scaling factor from zero\ninitialization. We train this module using distillation losses; however, unlike\nsome prior work where a full diffusion backbone serves as the student, our\nmodel freezes the backbone, training only the feedback module. While many\nefforts to optimize diffusion models focus on achieving acceptable image\nquality in extremely few steps (1-4 steps), our emphasis is on matching best\ncase results (typically achieved in 20 steps) while significantly reducing\nruntime. ILF achieves this balance effectively, demonstrating strong\nperformance for both class-to-image generation with diffusion transformer (DiT)\nand text-to-image generation with DiT-based PixArt-alpha and PixArt-sigma. The\nquality of ILF's 1.7x-1.8x speedups are confirmed by FID, CLIP score, CLIP\nImage Quality Assessment, ImageReward, and qualitative comparisons. Project\ninformation is available at https://mgwillia.github.io/ilf.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We propose Inner Loop Feedback (ILF), a novel approach to accelerate\ndiffusion models' inference. ILF trains a lightweight module to predict future\nfeatures in the denoising process by leveraging the outputs from a chosen\ndiffusion backbone block at a given time step. This approach exploits two key\nintuitions; (1) the outputs of a given block at adjacent time steps are\nsimilar, and (2) performing partial computations for a step imposes a lower\nburden on the model than skipping the step entirely. Our method is highly\nflexible, since we find that the feedback module itself can simply be a block\nfrom the diffusion backbone, with all settings copied. Its influence on the\ndiffusion forward can be tempered with a learnable scaling factor from zero\ninitialization. We train this module using distillation losses; however, unlike\nsome prior work where a full diffusion backbone serves as the student, our\nmodel freezes the backbone, training only the feedback module. While many\nefforts to optimize diffusion models focus on achieving acceptable image\nquality in extremely few steps (1-4 steps), our emphasis is on matching best\ncase results (typically achieved in 20 steps) while significantly reducing\nruntime. ILF achieves this balance effectively, demonstrating strong\nperformance for both class-to-image generation with diffusion transformer (DiT)\nand text-to-image generation with DiT-based PixArt-alpha and PixArt-sigma. The\nquality of ILF's 1.7x-1.8x speedups are confirmed by FID, CLIP score, CLIP\nImage Quality Assessment, ImageReward, and qualitative comparisons. Project\ninformation is available at https://mgwillia.github.io/ilf."
                },
                "authors": [
                    {
                        "name": "Matthew Gwilliam"
                    },
                    {
                        "name": "Han Cai"
                    },
                    {
                        "name": "Di Wu"
                    },
                    {
                        "name": "Abhinav Shrivastava"
                    },
                    {
                        "name": "Zhiyu Cheng"
                    }
                ],
                "author_detail": {
                    "name": "Zhiyu Cheng"
                },
                "author": "Zhiyu Cheng",
                "arxiv_comment": "submission currently under review; 20 pages, 17 figures, 6 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.13107v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.13107v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.13888v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.13888v1",
                "updated": "2025-01-23T18:01:01Z",
                "updated_parsed": [
                    2025,
                    1,
                    23,
                    18,
                    1,
                    1,
                    3,
                    23,
                    0
                ],
                "published": "2025-01-23T18:01:01Z",
                "published_parsed": [
                    2025,
                    1,
                    23,
                    18,
                    1,
                    1,
                    3,
                    23,
                    0
                ],
                "title": "Multimodal Sensor Dataset for Monitoring Older Adults Post Lower-Limb\n  Fractures in Community Settings",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multimodal Sensor Dataset for Monitoring Older Adults Post Lower-Limb\n  Fractures in Community Settings"
                },
                "summary": "Lower-Limb Fractures (LLF) are a major health concern for older adults, often\nleading to reduced mobility and prolonged recovery, potentially impairing daily\nactivities and independence. During recovery, older adults frequently face\nsocial isolation and functional decline, complicating rehabilitation and\nadversely affecting physical and mental health. Multi-modal sensor platforms\nthat continuously collect data and analyze it using machine-learning algorithms\ncan remotely monitor this population and infer health outcomes. They can also\nalert clinicians to individuals at risk of isolation and decline. This paper\npresents a new publicly available multi-modal sensor dataset, MAISON-LLF,\ncollected from older adults recovering from LLF in community settings. The\ndataset includes data from smartphone and smartwatch sensors, motion detectors,\nsleep-tracking mattresses, and clinical questionnaires on isolation and\ndecline. The dataset was collected from ten older adults living alone at home\nfor eight weeks each, totaling 560 days of 24-hour sensor data. For technical\nvalidation, supervised machine-learning and deep-learning models were developed\nusing the sensor and clinical questionnaire data, providing a foundational\ncomparison for the research community.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Lower-Limb Fractures (LLF) are a major health concern for older adults, often\nleading to reduced mobility and prolonged recovery, potentially impairing daily\nactivities and independence. During recovery, older adults frequently face\nsocial isolation and functional decline, complicating rehabilitation and\nadversely affecting physical and mental health. Multi-modal sensor platforms\nthat continuously collect data and analyze it using machine-learning algorithms\ncan remotely monitor this population and infer health outcomes. They can also\nalert clinicians to individuals at risk of isolation and decline. This paper\npresents a new publicly available multi-modal sensor dataset, MAISON-LLF,\ncollected from older adults recovering from LLF in community settings. The\ndataset includes data from smartphone and smartwatch sensors, motion detectors,\nsleep-tracking mattresses, and clinical questionnaires on isolation and\ndecline. The dataset was collected from ten older adults living alone at home\nfor eight weeks each, totaling 560 days of 24-hour sensor data. For technical\nvalidation, supervised machine-learning and deep-learning models were developed\nusing the sensor and clinical questionnaire data, providing a foundational\ncomparison for the research community."
                },
                "authors": [
                    {
                        "name": "Ali Abedi"
                    },
                    {
                        "name": "Charlene H. Chu"
                    },
                    {
                        "name": "Shehroz S. Khan"
                    }
                ],
                "author_detail": {
                    "name": "Shehroz S. Khan"
                },
                "author": "Shehroz S. Khan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.13888v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.13888v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.03368v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.03368v2",
                "updated": "2025-01-23T17:57:28Z",
                "updated_parsed": [
                    2025,
                    1,
                    23,
                    17,
                    57,
                    28,
                    3,
                    23,
                    0
                ],
                "published": "2024-06-05T15:23:08Z",
                "published_parsed": [
                    2024,
                    6,
                    5,
                    15,
                    23,
                    8,
                    2,
                    157,
                    0
                ],
                "title": "IrokoBench: A New Benchmark for African Languages in the Age of Large\n  Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "IrokoBench: A New Benchmark for African Languages in the Age of Large\n  Language Models"
                },
                "summary": "Despite the widespread adoption of Large language models (LLMs), their\nremarkable capabilities remain limited to a few high-resource languages.\nAdditionally, many low-resource languages (\\eg African languages) are often\nevaluated only on basic text classification tasks due to the lack of\nappropriate or comprehensive benchmarks outside of high-resource languages. In\nthis paper, we introduce IrokoBench -- a human-translated benchmark dataset for\n17 typologically-diverse low-resource African languages covering three tasks:\nnatural language inference~(AfriXNLI), mathematical reasoning~(AfriMGSM), and\nmulti-choice knowledge-based question answering~(AfriMMLU). We use IrokoBench\nto evaluate zero-shot, few-shot, and translate-test settings~(where test sets\nare translated into English) across 10 open and six proprietary LLMs. Our\nevaluation reveals a significant performance gap between high-resource\nlanguages~(such as English and French) and low-resource African languages. We\nobserve a significant performance gap between open and proprietary models, with\nthe highest performing open model, Gemma 2 27B only at 63\\% of the\nbest-performing proprietary model GPT-4o performance. In addition, machine\ntranslating the test set to English before evaluation helped to close the gap\nfor larger models that are English-centric, such as Gemma 2 27B and LLaMa 3.1\n70B. These findings suggest that more efforts are needed to develop and adapt\nLLMs for African languages.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Despite the widespread adoption of Large language models (LLMs), their\nremarkable capabilities remain limited to a few high-resource languages.\nAdditionally, many low-resource languages (\\eg African languages) are often\nevaluated only on basic text classification tasks due to the lack of\nappropriate or comprehensive benchmarks outside of high-resource languages. In\nthis paper, we introduce IrokoBench -- a human-translated benchmark dataset for\n17 typologically-diverse low-resource African languages covering three tasks:\nnatural language inference~(AfriXNLI), mathematical reasoning~(AfriMGSM), and\nmulti-choice knowledge-based question answering~(AfriMMLU). We use IrokoBench\nto evaluate zero-shot, few-shot, and translate-test settings~(where test sets\nare translated into English) across 10 open and six proprietary LLMs. Our\nevaluation reveals a significant performance gap between high-resource\nlanguages~(such as English and French) and low-resource African languages. We\nobserve a significant performance gap between open and proprietary models, with\nthe highest performing open model, Gemma 2 27B only at 63\\% of the\nbest-performing proprietary model GPT-4o performance. In addition, machine\ntranslating the test set to English before evaluation helped to close the gap\nfor larger models that are English-centric, such as Gemma 2 27B and LLaMa 3.1\n70B. These findings suggest that more efforts are needed to develop and adapt\nLLMs for African languages."
                },
                "authors": [
                    {
                        "name": "David Ifeoluwa Adelani"
                    },
                    {
                        "name": "Jessica Ojo"
                    },
                    {
                        "name": "Israel Abebe Azime"
                    },
                    {
                        "name": "Jian Yun Zhuang"
                    },
                    {
                        "name": "Jesujoba O. Alabi"
                    },
                    {
                        "name": "Xuanli He"
                    },
                    {
                        "name": "Millicent Ochieng"
                    },
                    {
                        "name": "Sara Hooker"
                    },
                    {
                        "name": "Andiswa Bukula"
                    },
                    {
                        "name": "En-Shiun Annie Lee"
                    },
                    {
                        "name": "Chiamaka Chukwuneke"
                    },
                    {
                        "name": "Happy Buzaaba"
                    },
                    {
                        "name": "Blessing Sibanda"
                    },
                    {
                        "name": "Godson Kalipe"
                    },
                    {
                        "name": "Jonathan Mukiibi"
                    },
                    {
                        "name": "Salomon Kabongo"
                    },
                    {
                        "name": "Foutse Yuehgoh"
                    },
                    {
                        "name": "Mmasibidi Setaka"
                    },
                    {
                        "name": "Lolwethu Ndolela"
                    },
                    {
                        "name": "Nkiruka Odu"
                    },
                    {
                        "name": "Rooweither Mabuya"
                    },
                    {
                        "name": "Shamsuddeen Hassan Muhammad"
                    },
                    {
                        "name": "Salomey Osei"
                    },
                    {
                        "name": "Sokhar Samb"
                    },
                    {
                        "name": "Tadesse Kebede Guge"
                    },
                    {
                        "name": "Tombekai Vangoni Sherman"
                    },
                    {
                        "name": "Pontus Stenetorp"
                    }
                ],
                "author_detail": {
                    "name": "Pontus Stenetorp"
                },
                "author": "Pontus Stenetorp",
                "arxiv_comment": "Accepted to NAACL 2025 (main conference)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.03368v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.03368v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.13884v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.13884v1",
                "updated": "2025-01-23T17:57:18Z",
                "updated_parsed": [
                    2025,
                    1,
                    23,
                    17,
                    57,
                    18,
                    3,
                    23,
                    0
                ],
                "published": "2025-01-23T17:57:18Z",
                "published_parsed": [
                    2025,
                    1,
                    23,
                    17,
                    57,
                    18,
                    3,
                    23,
                    0
                ],
                "title": "Exploring Finetuned Audio-LLM on Heart Murmur Features",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Exploring Finetuned Audio-LLM on Heart Murmur Features"
                },
                "summary": "Large language models (LLMs) for audio have excelled in recognizing and\nanalyzing human speech, music, and environmental sounds. However, their\npotential for understanding other types of sounds, particularly biomedical\nsounds, remains largely underexplored despite significant scientific interest.\nIn this study, we focus on diagnosing cardiovascular diseases using\nphonocardiograms, i.e., heart sounds. Most existing deep neural network (DNN)\nparadigms are restricted to heart murmur classification (healthy vs unhealthy)\nand do not predict other acoustic features of the murmur such as timing,\ngrading, harshness, pitch, and quality, which are important in helping\nphysicians diagnose the underlying heart conditions. We propose to finetune an\naudio LLM, Qwen2-Audio, on the PhysioNet CirCor DigiScope phonocardiogram (PCG)\ndataset and evaluate its performance in classifying 11 expert-labeled murmur\nfeatures. Additionally, we aim to achieve more noise-robust and generalizable\nsystem by exploring a preprocessing segmentation algorithm using an audio\nrepresentation model, SSAMBA. Our results indicate that the LLM-based model\noutperforms state-of-the-art methods in 8 of the 11 features and performs\ncomparably in the remaining 3. Moreover, the LLM successfully classifies\nlong-tail murmur features with limited training data, a task that all previous\nmethods have failed to classify. These findings underscore the potential of\naudio LLMs as assistants to human cardiologists in enhancing heart disease\ndiagnosis.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) for audio have excelled in recognizing and\nanalyzing human speech, music, and environmental sounds. However, their\npotential for understanding other types of sounds, particularly biomedical\nsounds, remains largely underexplored despite significant scientific interest.\nIn this study, we focus on diagnosing cardiovascular diseases using\nphonocardiograms, i.e., heart sounds. Most existing deep neural network (DNN)\nparadigms are restricted to heart murmur classification (healthy vs unhealthy)\nand do not predict other acoustic features of the murmur such as timing,\ngrading, harshness, pitch, and quality, which are important in helping\nphysicians diagnose the underlying heart conditions. We propose to finetune an\naudio LLM, Qwen2-Audio, on the PhysioNet CirCor DigiScope phonocardiogram (PCG)\ndataset and evaluate its performance in classifying 11 expert-labeled murmur\nfeatures. Additionally, we aim to achieve more noise-robust and generalizable\nsystem by exploring a preprocessing segmentation algorithm using an audio\nrepresentation model, SSAMBA. Our results indicate that the LLM-based model\noutperforms state-of-the-art methods in 8 of the 11 features and performs\ncomparably in the remaining 3. Moreover, the LLM successfully classifies\nlong-tail murmur features with limited training data, a task that all previous\nmethods have failed to classify. These findings underscore the potential of\naudio LLMs as assistants to human cardiologists in enhancing heart disease\ndiagnosis."
                },
                "authors": [
                    {
                        "name": "Adrian Florea"
                    },
                    {
                        "name": "Xilin Jiang"
                    },
                    {
                        "name": "Nima Mesgarani"
                    },
                    {
                        "name": "Xiaofan Jiang"
                    }
                ],
                "author_detail": {
                    "name": "Xiaofan Jiang"
                },
                "author": "Xiaofan Jiang",
                "arxiv_comment": "5 pages, 1 figure, and 3 tables. Submitted to IEEE/ACM Conference on\n  Connected Health: Applications, Systems , and Engineering Technologies",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.13884v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.13884v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.AS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.13881v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.13881v1",
                "updated": "2025-01-23T17:56:07Z",
                "updated_parsed": [
                    2025,
                    1,
                    23,
                    17,
                    56,
                    7,
                    3,
                    23,
                    0
                ],
                "published": "2025-01-23T17:56:07Z",
                "published_parsed": [
                    2025,
                    1,
                    23,
                    17,
                    56,
                    7,
                    3,
                    23,
                    0
                ],
                "title": "The machine learning platform for developers of large systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The machine learning platform for developers of large systems"
                },
                "summary": "The machine learning system in the form of Retrieval Augmented Generation\n(RAG) has developed steadily since about 2021. RAG could be observed as a\nversion of the knowledge transfer. In the studied case, the large computing\nsystems are observed as the application point of RAG, which includes large\nlanguage model (LLM), as a partner for the developing team. Such an approach\nhas advantages during the development process and further in exploitation time.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The machine learning system in the form of Retrieval Augmented Generation\n(RAG) has developed steadily since about 2021. RAG could be observed as a\nversion of the knowledge transfer. In the studied case, the large computing\nsystems are observed as the application point of RAG, which includes large\nlanguage model (LLM), as a partner for the developing team. Such an approach\nhas advantages during the development process and further in exploitation time."
                },
                "authors": [
                    {
                        "name": "Alexey Naikov"
                    },
                    {
                        "name": "Anatoly Oreshkin"
                    },
                    {
                        "name": "Alexey Shvetsov"
                    },
                    {
                        "name": "Andrey Shevel"
                    }
                ],
                "author_detail": {
                    "name": "Andrey Shevel"
                },
                "author": "Andrey Shevel",
                "arxiv_comment": "5 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.13881v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.13881v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "hep-ex",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "hep-ex",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "J.2",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.13880v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.13880v1",
                "updated": "2025-01-23T17:54:19Z",
                "updated_parsed": [
                    2025,
                    1,
                    23,
                    17,
                    54,
                    19,
                    3,
                    23,
                    0
                ],
                "published": "2025-01-23T17:54:19Z",
                "published_parsed": [
                    2025,
                    1,
                    23,
                    17,
                    54,
                    19,
                    3,
                    23,
                    0
                ],
                "title": "A RAG-Based Institutional Assistant",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A RAG-Based Institutional Assistant"
                },
                "summary": "Although large language models (LLMs) demonstrate strong text generation\ncapabilities, they struggle in scenarios requiring access to structured\nknowledge bases or specific documents, limiting their effectiveness in\nknowledge-intensive tasks. To address this limitation, retrieval-augmented\ngeneration (RAG) models have been developed, enabling generative models to\nincorporate relevant document fragments into their inputs. In this paper, we\ndesign and evaluate a RAG-based virtual assistant specifically tailored for the\nUniversity of S\\~ao Paulo. Our system architecture comprises two key modules: a\nretriever and a generative model. We experiment with different types of models\nfor both components, adjusting hyperparameters such as chunk size and the\nnumber of retrieved documents. Our optimal retriever model achieves a Top-5\naccuracy of 30%, while our most effective generative model scores 22.04\\%\nagainst ground truth answers. Notably, when the correct document chunks are\nsupplied to the LLMs, accuracy significantly improves to 54.02%, an increase of\nover 30 percentage points. Conversely, without contextual input, performance\ndeclines to 13.68%. These findings highlight the critical role of database\naccess in enhancing LLM performance. They also reveal the limitations of\ncurrent semantic search methods in accurately identifying relevant documents\nand underscore the ongoing challenges LLMs face in generating precise\nresponses.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Although large language models (LLMs) demonstrate strong text generation\ncapabilities, they struggle in scenarios requiring access to structured\nknowledge bases or specific documents, limiting their effectiveness in\nknowledge-intensive tasks. To address this limitation, retrieval-augmented\ngeneration (RAG) models have been developed, enabling generative models to\nincorporate relevant document fragments into their inputs. In this paper, we\ndesign and evaluate a RAG-based virtual assistant specifically tailored for the\nUniversity of S\\~ao Paulo. Our system architecture comprises two key modules: a\nretriever and a generative model. We experiment with different types of models\nfor both components, adjusting hyperparameters such as chunk size and the\nnumber of retrieved documents. Our optimal retriever model achieves a Top-5\naccuracy of 30%, while our most effective generative model scores 22.04\\%\nagainst ground truth answers. Notably, when the correct document chunks are\nsupplied to the LLMs, accuracy significantly improves to 54.02%, an increase of\nover 30 percentage points. Conversely, without contextual input, performance\ndeclines to 13.68%. These findings highlight the critical role of database\naccess in enhancing LLM performance. They also reveal the limitations of\ncurrent semantic search methods in accurately identifying relevant documents\nand underscore the ongoing challenges LLMs face in generating precise\nresponses."
                },
                "authors": [
                    {
                        "name": "Gustavo Kuratomi"
                    },
                    {
                        "name": "Paulo Pirozelli"
                    },
                    {
                        "name": "Fabio G. Cozman"
                    },
                    {
                        "name": "Sarajane M. Peres"
                    }
                ],
                "author_detail": {
                    "name": "Sarajane M. Peres"
                },
                "author": "Sarajane M. Peres",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.13880v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.13880v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.13879v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.13879v1",
                "updated": "2025-01-23T17:54:16Z",
                "updated_parsed": [
                    2025,
                    1,
                    23,
                    17,
                    54,
                    16,
                    3,
                    23,
                    0
                ],
                "published": "2025-01-23T17:54:16Z",
                "published_parsed": [
                    2025,
                    1,
                    23,
                    17,
                    54,
                    16,
                    3,
                    23,
                    0
                ],
                "title": "Finite mixture representations of zero-&-$N$-inflated distributions for\n  count-compositional data",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Finite mixture representations of zero-&-$N$-inflated distributions for\n  count-compositional data"
                },
                "summary": "We provide novel probabilistic portrayals of two multivariate models designed\nto handle zero-inflation in count-compositional data. We develop a new unifying\nframework that represents both as finite mixture distributions. One of these\ndistributions, based on Dirichlet-multinomial components, has been studied\nbefore, but has not yet been properly characterised as a sampling distribution\nof the counts. The other, based on multinomial components, is a new\ncontribution. Using our finite mixture representations enables us to derive key\nstatistical properties, including moments, marginal distributions, and special\ncases for both distributions. We develop enhanced Bayesian inference schemes\nwith efficient Gibbs sampling updates, wherever possible, for parameters and\nauxiliary variables, demonstrating improvements over existing methods in the\nliterature. We conduct simulation studies to evaluate the efficiency of the\nBayesian inference procedures and to illustrate the practical utility of the\nproposed distributions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We provide novel probabilistic portrayals of two multivariate models designed\nto handle zero-inflation in count-compositional data. We develop a new unifying\nframework that represents both as finite mixture distributions. One of these\ndistributions, based on Dirichlet-multinomial components, has been studied\nbefore, but has not yet been properly characterised as a sampling distribution\nof the counts. The other, based on multinomial components, is a new\ncontribution. Using our finite mixture representations enables us to derive key\nstatistical properties, including moments, marginal distributions, and special\ncases for both distributions. We develop enhanced Bayesian inference schemes\nwith efficient Gibbs sampling updates, wherever possible, for parameters and\nauxiliary variables, demonstrating improvements over existing methods in the\nliterature. We conduct simulation studies to evaluate the efficiency of the\nBayesian inference procedures and to illustrate the practical utility of the\nproposed distributions."
                },
                "authors": [
                    {
                        "name": "André F. B. Menezes"
                    },
                    {
                        "name": "Andrew C. Parnell"
                    },
                    {
                        "name": "Keefe Murphy"
                    }
                ],
                "author_detail": {
                    "name": "Keefe Murphy"
                },
                "author": "Keefe Murphy",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.13879v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.13879v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ME",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.ST",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.TH",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.13859v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.13859v1",
                "updated": "2025-01-23T17:30:27Z",
                "updated_parsed": [
                    2025,
                    1,
                    23,
                    17,
                    30,
                    27,
                    3,
                    23,
                    0
                ],
                "published": "2025-01-23T17:30:27Z",
                "published_parsed": [
                    2025,
                    1,
                    23,
                    17,
                    30,
                    27,
                    3,
                    23,
                    0
                ],
                "title": "Dual-Modal Prototype Joint Learning for Compositional Zero-Shot Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Dual-Modal Prototype Joint Learning for Compositional Zero-Shot Learning"
                },
                "summary": "Compositional Zero-Shot Learning (CZSL) aims to recognize novel compositions\nof attributes and objects by leveraging knowledge learned from seen\ncompositions. Recent approaches have explored the use of Vision-Language Models\n(VLMs) to align textual and visual modalities. These methods typically employ\nprompt engineering, parameter-tuning, and modality fusion to generate rich\ntextual prototypes that serve as class prototypes for CZSL. However, the\nmodality gap results in textual prototypes being unable to fully capture the\noptimal representations of all class prototypes, particularly those with\nfine-grained features, which can be directly obtained from the visual modality.\nIn this paper, we propose a novel Dual-Modal Prototype Joint Learning framework\nfor the CZSL task. Our approach, based on VLMs, introduces prototypes in both\nthe textual and visual modalities. The textual prototype is optimized to\ncapture broad conceptual information, aiding the model's generalization across\nunseen compositions. Meanwhile, the visual prototype is used to mitigate the\nclassification errors caused by the modality gap and capture fine-grained\ndetails to distinguish images with similar appearances. To effectively optimize\nthese prototypes, we design specialized decomposition modules and a joint\nlearning strategy that enrich the features from both modalities. These\nprototypes not only capture key category information during training but also\nserve as crucial reference targets during inference. Experimental results\ndemonstrate that our approach achieves state-of-the-art performance in the\nclosed-world setting and competitive performance in the open-world setting\nacross three publicly available CZSL benchmarks. These findings validate the\neffectiveness of our method in advancing compositional generalization.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Compositional Zero-Shot Learning (CZSL) aims to recognize novel compositions\nof attributes and objects by leveraging knowledge learned from seen\ncompositions. Recent approaches have explored the use of Vision-Language Models\n(VLMs) to align textual and visual modalities. These methods typically employ\nprompt engineering, parameter-tuning, and modality fusion to generate rich\ntextual prototypes that serve as class prototypes for CZSL. However, the\nmodality gap results in textual prototypes being unable to fully capture the\noptimal representations of all class prototypes, particularly those with\nfine-grained features, which can be directly obtained from the visual modality.\nIn this paper, we propose a novel Dual-Modal Prototype Joint Learning framework\nfor the CZSL task. Our approach, based on VLMs, introduces prototypes in both\nthe textual and visual modalities. The textual prototype is optimized to\ncapture broad conceptual information, aiding the model's generalization across\nunseen compositions. Meanwhile, the visual prototype is used to mitigate the\nclassification errors caused by the modality gap and capture fine-grained\ndetails to distinguish images with similar appearances. To effectively optimize\nthese prototypes, we design specialized decomposition modules and a joint\nlearning strategy that enrich the features from both modalities. These\nprototypes not only capture key category information during training but also\nserve as crucial reference targets during inference. Experimental results\ndemonstrate that our approach achieves state-of-the-art performance in the\nclosed-world setting and competitive performance in the open-world setting\nacross three publicly available CZSL benchmarks. These findings validate the\neffectiveness of our method in advancing compositional generalization."
                },
                "authors": [
                    {
                        "name": "Shiyu Zhang"
                    },
                    {
                        "name": "Cheng Yan"
                    },
                    {
                        "name": "Yang Liu"
                    },
                    {
                        "name": "Chenchen Jing"
                    },
                    {
                        "name": "Lei Zhou"
                    },
                    {
                        "name": "Wenjun Wang"
                    }
                ],
                "author_detail": {
                    "name": "Wenjun Wang"
                },
                "author": "Wenjun Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.13859v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.13859v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.11610v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.11610v4",
                "updated": "2025-01-23T17:18:07Z",
                "updated_parsed": [
                    2025,
                    1,
                    23,
                    17,
                    18,
                    7,
                    3,
                    23,
                    0
                ],
                "published": "2024-10-15T13:46:19Z",
                "published_parsed": [
                    2024,
                    10,
                    15,
                    13,
                    46,
                    19,
                    1,
                    289,
                    0
                ],
                "title": "Enhanced Encoder-Decoder Architecture for Accurate Monocular Depth\n  Estimation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Enhanced Encoder-Decoder Architecture for Accurate Monocular Depth\n  Estimation"
                },
                "summary": "Estimating depth from a single 2D image is a challenging task due to the lack\nof stereo or multi-view data, which are typically required for depth\nperception. In state-of-the-art architectures, the main challenge is to\nefficiently capture complex objects and fine-grained details, which are often\ndifficult to predict. This paper introduces a novel deep learning-based\napproach using an enhanced encoder-decoder architecture, where the\nInception-ResNet-v2 model serves as the encoder. This is the first instance of\nutilizing Inception-ResNet-v2 as an encoder for monocular depth estimation,\ndemonstrating improved performance over previous models. It incorporates\nmulti-scale feature extraction to enhance depth prediction accuracy across\nvarious object sizes and distances. We propose a composite loss function\ncomprising depth loss, gradient edge loss, and Structural Similarity Index\nMeasure (SSIM) loss, with fine-tuned weights to optimize the weighted sum,\nensuring a balance across different aspects of depth estimation. Experimental\nresults on the KITTI dataset show that our model achieves a significantly\nfaster inference time of 0.019 seconds, outperforming vision transformers in\nefficiency while maintaining good accuracy. On the NYU Depth V2 dataset, the\nmodel establishes state-of-the-art performance, with an Absolute Relative Error\n(ARE) of 0.064, a Root Mean Square Error (RMSE) of 0.228, and an accuracy of\n89.3% for $\\delta$ < 1.25. These metrics demonstrate that our model can\naccurately and efficiently predict depth even in challenging scenarios,\nproviding a practical solution for real-time applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Estimating depth from a single 2D image is a challenging task due to the lack\nof stereo or multi-view data, which are typically required for depth\nperception. In state-of-the-art architectures, the main challenge is to\nefficiently capture complex objects and fine-grained details, which are often\ndifficult to predict. This paper introduces a novel deep learning-based\napproach using an enhanced encoder-decoder architecture, where the\nInception-ResNet-v2 model serves as the encoder. This is the first instance of\nutilizing Inception-ResNet-v2 as an encoder for monocular depth estimation,\ndemonstrating improved performance over previous models. It incorporates\nmulti-scale feature extraction to enhance depth prediction accuracy across\nvarious object sizes and distances. We propose a composite loss function\ncomprising depth loss, gradient edge loss, and Structural Similarity Index\nMeasure (SSIM) loss, with fine-tuned weights to optimize the weighted sum,\nensuring a balance across different aspects of depth estimation. Experimental\nresults on the KITTI dataset show that our model achieves a significantly\nfaster inference time of 0.019 seconds, outperforming vision transformers in\nefficiency while maintaining good accuracy. On the NYU Depth V2 dataset, the\nmodel establishes state-of-the-art performance, with an Absolute Relative Error\n(ARE) of 0.064, a Root Mean Square Error (RMSE) of 0.228, and an accuracy of\n89.3% for $\\delta$ < 1.25. These metrics demonstrate that our model can\naccurately and efficiently predict depth even in challenging scenarios,\nproviding a practical solution for real-time applications."
                },
                "authors": [
                    {
                        "name": "Dabbrata Das"
                    },
                    {
                        "name": "Argho Deb Das"
                    },
                    {
                        "name": "Farhan Sadaf"
                    }
                ],
                "author_detail": {
                    "name": "Farhan Sadaf"
                },
                "author": "Farhan Sadaf",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.11610v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.11610v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.IV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.08159v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.08159v2",
                "updated": "2025-01-23T17:08:57Z",
                "updated_parsed": [
                    2025,
                    1,
                    23,
                    17,
                    8,
                    57,
                    3,
                    23,
                    0
                ],
                "published": "2024-10-10T17:41:54Z",
                "published_parsed": [
                    2024,
                    10,
                    10,
                    17,
                    41,
                    54,
                    3,
                    284,
                    0
                ],
                "title": "DART: Denoising Autoregressive Transformer for Scalable Text-to-Image\n  Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DART: Denoising Autoregressive Transformer for Scalable Text-to-Image\n  Generation"
                },
                "summary": "Diffusion models have become the dominant approach for visual generation.\nThey are trained by denoising a Markovian process which gradually adds noise to\nthe input. We argue that the Markovian property limits the model's ability to\nfully utilize the generation trajectory, leading to inefficiencies during\ntraining and inference. In this paper, we propose DART, a transformer-based\nmodel that unifies autoregressive (AR) and diffusion within a non-Markovian\nframework. DART iteratively denoises image patches spatially and spectrally\nusing an AR model that has the same architecture as standard language models.\nDART does not rely on image quantization, which enables more effective image\nmodeling while maintaining flexibility. Furthermore, DART seamlessly trains\nwith both text and image data in a unified model. Our approach demonstrates\ncompetitive performance on class-conditioned and text-to-image generation\ntasks, offering a scalable, efficient alternative to traditional diffusion\nmodels. Through this unified framework, DART sets a new benchmark for scalable,\nhigh-quality image synthesis.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion models have become the dominant approach for visual generation.\nThey are trained by denoising a Markovian process which gradually adds noise to\nthe input. We argue that the Markovian property limits the model's ability to\nfully utilize the generation trajectory, leading to inefficiencies during\ntraining and inference. In this paper, we propose DART, a transformer-based\nmodel that unifies autoregressive (AR) and diffusion within a non-Markovian\nframework. DART iteratively denoises image patches spatially and spectrally\nusing an AR model that has the same architecture as standard language models.\nDART does not rely on image quantization, which enables more effective image\nmodeling while maintaining flexibility. Furthermore, DART seamlessly trains\nwith both text and image data in a unified model. Our approach demonstrates\ncompetitive performance on class-conditioned and text-to-image generation\ntasks, offering a scalable, efficient alternative to traditional diffusion\nmodels. Through this unified framework, DART sets a new benchmark for scalable,\nhigh-quality image synthesis."
                },
                "authors": [
                    {
                        "name": "Jiatao Gu"
                    },
                    {
                        "name": "Yuyang Wang"
                    },
                    {
                        "name": "Yizhe Zhang"
                    },
                    {
                        "name": "Qihang Zhang"
                    },
                    {
                        "name": "Dinghuai Zhang"
                    },
                    {
                        "name": "Navdeep Jaitly"
                    },
                    {
                        "name": "Josh Susskind"
                    },
                    {
                        "name": "Shuangfei Zhai"
                    }
                ],
                "author_detail": {
                    "name": "Shuangfei Zhai"
                },
                "author": "Shuangfei Zhai",
                "arxiv_comment": "Accepted by ICLR2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.08159v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.08159v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.19599v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.19599v3",
                "updated": "2025-01-23T17:05:40Z",
                "updated_parsed": [
                    2025,
                    1,
                    23,
                    17,
                    5,
                    40,
                    3,
                    23,
                    0
                ],
                "published": "2024-10-25T14:46:07Z",
                "published_parsed": [
                    2024,
                    10,
                    25,
                    14,
                    46,
                    7,
                    4,
                    299,
                    0
                ],
                "title": "Take Caution in Using LLMs as Human Surrogates: Scylla Ex Machina",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Take Caution in Using LLMs as Human Surrogates: Scylla Ex Machina"
                },
                "summary": "Recent studies suggest large language models (LLMs) can exhibit human-like\nreasoning, aligning with human behavior in economic experiments, surveys, and\npolitical discourse. This has led many to propose that LLMs can be used as\nsurrogates or simulations for humans in social science research. However, LLMs\ndiffer fundamentally from humans, relying on probabilistic patterns, absent the\nembodied experiences or survival objectives that shape human cognition. We\nassess the reasoning depth of LLMs using the 11-20 money request game. Nearly\nall advanced approaches fail to replicate human behavior distributions across\nmany models. Causes of failure are diverse and unpredictable, relating to input\nlanguage, roles, and safeguarding. These results advise caution when using LLMs\nto study human behavior or as surrogates or simulations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent studies suggest large language models (LLMs) can exhibit human-like\nreasoning, aligning with human behavior in economic experiments, surveys, and\npolitical discourse. This has led many to propose that LLMs can be used as\nsurrogates or simulations for humans in social science research. However, LLMs\ndiffer fundamentally from humans, relying on probabilistic patterns, absent the\nembodied experiences or survival objectives that shape human cognition. We\nassess the reasoning depth of LLMs using the 11-20 money request game. Nearly\nall advanced approaches fail to replicate human behavior distributions across\nmany models. Causes of failure are diverse and unpredictable, relating to input\nlanguage, roles, and safeguarding. These results advise caution when using LLMs\nto study human behavior or as surrogates or simulations."
                },
                "authors": [
                    {
                        "name": "Yuan Gao"
                    },
                    {
                        "name": "Dokyun Lee"
                    },
                    {
                        "name": "Gordon Burtch"
                    },
                    {
                        "name": "Sina Fazelpour"
                    }
                ],
                "author_detail": {
                    "name": "Sina Fazelpour"
                },
                "author": "Sina Fazelpour",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.19599v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.19599v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "econ.GN",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "econ.GN",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "q-fin.EC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.13833v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.13833v1",
                "updated": "2025-01-23T16:58:18Z",
                "updated_parsed": [
                    2025,
                    1,
                    23,
                    16,
                    58,
                    18,
                    3,
                    23,
                    0
                ],
                "published": "2025-01-23T16:58:18Z",
                "published_parsed": [
                    2025,
                    1,
                    23,
                    16,
                    58,
                    18,
                    3,
                    23,
                    0
                ],
                "title": "On the Reasoning Capacity of AI Models and How to Quantify It",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "On the Reasoning Capacity of AI Models and How to Quantify It"
                },
                "summary": "Recent advances in Large Language Models (LLMs) have intensified the debate\nsurrounding the fundamental nature of their reasoning capabilities. While\nachieving high performance on benchmarks such as GPQA and MMLU, these models\nexhibit limitations in more complex reasoning tasks, highlighting the need for\nmore rigorous evaluation methodologies. We propose a novel phenomenological\napproach that goes beyond traditional accuracy metrics to probe the underlying\nmechanisms of model behavior, establishing a framework that could broadly\nimpact how we analyze and understand AI systems. Using positional bias in\nmultiple-choice reasoning tasks as a case study, we demonstrate how systematic\nperturbations can reveal fundamental aspects of model decision-making. To\nanalyze these behaviors, we develop two complementary phenomenological models:\na Probabilistic Mixture Model (PMM) that decomposes model responses into\nreasoning, memorization, and guessing components and an Information-Theoretic\nConsistency (ITC) analysis that quantifies the relationship between model\nconfidence and strategy selection. Through controlled experiments on reasoning\nbenchmarks, we show that true reasoning remains challenging for current models,\nwith apparent success often relying on sophisticated combinations of\nmemorization and pattern matching rather than genuine logical deduction. More\nfundamentally, we demonstrate that accuracy alone often overstates a model's\nreasoning abilities, as model behavior can be characterized through underlying\nmechanisms in the phase space of cognitive strategies, revealing how models\ndynamically balance different approaches when responding to queries. This\nframework enables quantitative criteria for real-world deployments, allowing\napplications to specify reliability thresholds based on strategy distributions\nrather than aggregate performance metrics.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advances in Large Language Models (LLMs) have intensified the debate\nsurrounding the fundamental nature of their reasoning capabilities. While\nachieving high performance on benchmarks such as GPQA and MMLU, these models\nexhibit limitations in more complex reasoning tasks, highlighting the need for\nmore rigorous evaluation methodologies. We propose a novel phenomenological\napproach that goes beyond traditional accuracy metrics to probe the underlying\nmechanisms of model behavior, establishing a framework that could broadly\nimpact how we analyze and understand AI systems. Using positional bias in\nmultiple-choice reasoning tasks as a case study, we demonstrate how systematic\nperturbations can reveal fundamental aspects of model decision-making. To\nanalyze these behaviors, we develop two complementary phenomenological models:\na Probabilistic Mixture Model (PMM) that decomposes model responses into\nreasoning, memorization, and guessing components and an Information-Theoretic\nConsistency (ITC) analysis that quantifies the relationship between model\nconfidence and strategy selection. Through controlled experiments on reasoning\nbenchmarks, we show that true reasoning remains challenging for current models,\nwith apparent success often relying on sophisticated combinations of\nmemorization and pattern matching rather than genuine logical deduction. More\nfundamentally, we demonstrate that accuracy alone often overstates a model's\nreasoning abilities, as model behavior can be characterized through underlying\nmechanisms in the phase space of cognitive strategies, revealing how models\ndynamically balance different approaches when responding to queries. This\nframework enables quantitative criteria for real-world deployments, allowing\napplications to specify reliability thresholds based on strategy distributions\nrather than aggregate performance metrics."
                },
                "authors": [
                    {
                        "name": "Santosh Kumar Radha"
                    },
                    {
                        "name": "Oktay Goktas"
                    }
                ],
                "author_detail": {
                    "name": "Oktay Goktas"
                },
                "author": "Oktay Goktas",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.13833v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.13833v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.13831v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.13831v1",
                "updated": "2025-01-23T16:54:27Z",
                "updated_parsed": [
                    2025,
                    1,
                    23,
                    16,
                    54,
                    27,
                    3,
                    23,
                    0
                ],
                "published": "2025-01-23T16:54:27Z",
                "published_parsed": [
                    2025,
                    1,
                    23,
                    16,
                    54,
                    27,
                    3,
                    23,
                    0
                ],
                "title": "Predicting Compact Phrasal Rewrites with Large Language Models for ASR\n  Post Editing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Predicting Compact Phrasal Rewrites with Large Language Models for ASR\n  Post Editing"
                },
                "summary": "Large Language Models (LLMs) excel at rewriting tasks such as text style\ntransfer and grammatical error correction. While there is considerable overlap\nbetween the inputs and outputs in these tasks, the decoding cost still\nincreases with output length, regardless of the amount of overlap. By\nleveraging the overlap between the input and the output, Kaneko and Okazaki\n(2023) proposed model-agnostic edit span representations to compress the\nrewrites to save computation. They reported an output length reduction rate of\nnearly 80% with minimal accuracy impact in four rewriting tasks. In this paper,\nwe propose alternative edit phrase representations inspired by phrase-based\nstatistical machine translation. We systematically compare our phrasal\nrepresentations with their span representations. We apply the LLM rewriting\nmodel to the task of Automatic Speech Recognition (ASR) post editing and show\nthat our target-phrase-only edit representation has the best\nefficiency-accuracy trade-off. On the LibriSpeech test set, our method closes\n50-60% of the WER gap between the edit span model and the full rewrite model\nwhile losing only 10-20% of the length reduction rate of the edit span model.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) excel at rewriting tasks such as text style\ntransfer and grammatical error correction. While there is considerable overlap\nbetween the inputs and outputs in these tasks, the decoding cost still\nincreases with output length, regardless of the amount of overlap. By\nleveraging the overlap between the input and the output, Kaneko and Okazaki\n(2023) proposed model-agnostic edit span representations to compress the\nrewrites to save computation. They reported an output length reduction rate of\nnearly 80% with minimal accuracy impact in four rewriting tasks. In this paper,\nwe propose alternative edit phrase representations inspired by phrase-based\nstatistical machine translation. We systematically compare our phrasal\nrepresentations with their span representations. We apply the LLM rewriting\nmodel to the task of Automatic Speech Recognition (ASR) post editing and show\nthat our target-phrase-only edit representation has the best\nefficiency-accuracy trade-off. On the LibriSpeech test set, our method closes\n50-60% of the WER gap between the edit span model and the full rewrite model\nwhile losing only 10-20% of the length reduction rate of the edit span model."
                },
                "authors": [
                    {
                        "name": "Hao Zhang"
                    },
                    {
                        "name": "Felix Stahlberg"
                    },
                    {
                        "name": "Shankar Kumar"
                    }
                ],
                "author_detail": {
                    "name": "Shankar Kumar"
                },
                "author": "Shankar Kumar",
                "arxiv_comment": "accepted by ICASSP 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.13831v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.13831v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.13829v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.13829v1",
                "updated": "2025-01-23T16:53:46Z",
                "updated_parsed": [
                    2025,
                    1,
                    23,
                    16,
                    53,
                    46,
                    3,
                    23,
                    0
                ],
                "published": "2025-01-23T16:53:46Z",
                "published_parsed": [
                    2025,
                    1,
                    23,
                    16,
                    53,
                    46,
                    3,
                    23,
                    0
                ],
                "title": "MV-GMN: State Space Model for Multi-View Action Recognition",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MV-GMN: State Space Model for Multi-View Action Recognition"
                },
                "summary": "Recent advancements in multi-view action recognition have largely relied on\nTransformer-based models. While effective and adaptable, these models often\nrequire substantial computational resources, especially in scenarios with\nmultiple views and multiple temporal sequences. Addressing this limitation,\nthis paper introduces the MV-GMN model, a state-space model specifically\ndesigned to efficiently aggregate multi-modal data (RGB and skeleton),\nmulti-view perspectives, and multi-temporal information for action recognition\nwith reduced computational complexity. The MV-GMN model employs an innovative\nMulti-View Graph Mamba network comprising a series of MV-GMN blocks. Each block\nincludes a proposed Bidirectional State Space Block and a GCN module. The\nBidirectional State Space Block introduces four scanning strategies, including\nview-prioritized and time-prioritized approaches. The GCN module leverages\nrule-based and KNN-based methods to construct the graph network, effectively\nintegrating features from different viewpoints and temporal instances.\nDemonstrating its efficacy, MV-GMN outperforms the state-of-the-arts on several\ndatasets, achieving notable accuracies of 97.3\\% and 96.7\\% on the NTU RGB+D\n120 dataset in cross-subject and cross-view scenarios, respectively. MV-GMN\nalso surpasses Transformer-based baselines while requiring only linear\ninference complexity, underscoring the model's ability to reduce computational\nload and enhance the scalability and applicability of multi-view action\nrecognition technologies.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advancements in multi-view action recognition have largely relied on\nTransformer-based models. While effective and adaptable, these models often\nrequire substantial computational resources, especially in scenarios with\nmultiple views and multiple temporal sequences. Addressing this limitation,\nthis paper introduces the MV-GMN model, a state-space model specifically\ndesigned to efficiently aggregate multi-modal data (RGB and skeleton),\nmulti-view perspectives, and multi-temporal information for action recognition\nwith reduced computational complexity. The MV-GMN model employs an innovative\nMulti-View Graph Mamba network comprising a series of MV-GMN blocks. Each block\nincludes a proposed Bidirectional State Space Block and a GCN module. The\nBidirectional State Space Block introduces four scanning strategies, including\nview-prioritized and time-prioritized approaches. The GCN module leverages\nrule-based and KNN-based methods to construct the graph network, effectively\nintegrating features from different viewpoints and temporal instances.\nDemonstrating its efficacy, MV-GMN outperforms the state-of-the-arts on several\ndatasets, achieving notable accuracies of 97.3\\% and 96.7\\% on the NTU RGB+D\n120 dataset in cross-subject and cross-view scenarios, respectively. MV-GMN\nalso surpasses Transformer-based baselines while requiring only linear\ninference complexity, underscoring the model's ability to reduce computational\nload and enhance the scalability and applicability of multi-view action\nrecognition technologies."
                },
                "authors": [
                    {
                        "name": "Yuhui Lin"
                    },
                    {
                        "name": "Jiaxuan Lu"
                    },
                    {
                        "name": "Yue Yong"
                    },
                    {
                        "name": "Jiahao Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Jiahao Zhang"
                },
                "author": "Jiahao Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.13829v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.13829v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.13824v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.13824v1",
                "updated": "2025-01-23T16:45:51Z",
                "updated_parsed": [
                    2025,
                    1,
                    23,
                    16,
                    45,
                    51,
                    3,
                    23,
                    0
                ],
                "published": "2025-01-23T16:45:51Z",
                "published_parsed": [
                    2025,
                    1,
                    23,
                    16,
                    45,
                    51,
                    3,
                    23,
                    0
                ],
                "title": "Hallucinations Can Improve Large Language Models in Drug Discovery",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Hallucinations Can Improve Large Language Models in Drug Discovery"
                },
                "summary": "Concerns about hallucinations in Large Language Models (LLMs) have been\nraised by researchers, yet their potential in areas where creativity is vital,\nsuch as drug discovery, merits exploration. In this paper, we come up with the\nhypothesis that hallucinations can improve LLMs in drug discovery. To verify\nthis hypothesis, we use LLMs to describe the SMILES string of molecules in\nnatural language and then incorporate these descriptions as part of the prompt\nto address specific tasks in drug discovery. Evaluated on seven LLMs and five\nclassification tasks, our findings confirm the hypothesis: LLMs can achieve\nbetter performance with text containing hallucinations. Notably, Llama-3.1-8B\nachieves an 18.35% gain in ROC-AUC compared to the baseline without\nhallucination. Furthermore, hallucinations generated by GPT-4o provide the most\nconsistent improvements across models. Additionally, we conduct empirical\nanalyses and a case study to investigate key factors affecting performance and\nthe underlying reasons. Our research sheds light on the potential use of\nhallucinations for LLMs and offers new perspectives for future research\nleveraging LLMs in drug discovery.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Concerns about hallucinations in Large Language Models (LLMs) have been\nraised by researchers, yet their potential in areas where creativity is vital,\nsuch as drug discovery, merits exploration. In this paper, we come up with the\nhypothesis that hallucinations can improve LLMs in drug discovery. To verify\nthis hypothesis, we use LLMs to describe the SMILES string of molecules in\nnatural language and then incorporate these descriptions as part of the prompt\nto address specific tasks in drug discovery. Evaluated on seven LLMs and five\nclassification tasks, our findings confirm the hypothesis: LLMs can achieve\nbetter performance with text containing hallucinations. Notably, Llama-3.1-8B\nachieves an 18.35% gain in ROC-AUC compared to the baseline without\nhallucination. Furthermore, hallucinations generated by GPT-4o provide the most\nconsistent improvements across models. Additionally, we conduct empirical\nanalyses and a case study to investigate key factors affecting performance and\nthe underlying reasons. Our research sheds light on the potential use of\nhallucinations for LLMs and offers new perspectives for future research\nleveraging LLMs in drug discovery."
                },
                "authors": [
                    {
                        "name": "Shuzhou Yuan"
                    },
                    {
                        "name": "Michael Färber"
                    }
                ],
                "author_detail": {
                    "name": "Michael Färber"
                },
                "author": "Michael Färber",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.13824v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.13824v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.13823v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.13823v1",
                "updated": "2025-01-23T16:45:21Z",
                "updated_parsed": [
                    2025,
                    1,
                    23,
                    16,
                    45,
                    21,
                    3,
                    23,
                    0
                ],
                "published": "2025-01-23T16:45:21Z",
                "published_parsed": [
                    2025,
                    1,
                    23,
                    16,
                    45,
                    21,
                    3,
                    23,
                    0
                ],
                "title": "Modelling superspreading dynamics and circadian rhythms in online\n  discussion boards using Hawkes processes",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Modelling superspreading dynamics and circadian rhythms in online\n  discussion boards using Hawkes processes"
                },
                "summary": "Online boards offer a platform for sharing and discussing content, where\ndiscussion emerges as a cascade of comments in response to a post. Branching\npoint process models offer a practical approach to modelling these cascades;\nhowever, existing models do not account for apparent features of empirical\ndata. We address this gap by illustrating the flexibility of Hawkes processes\nto model data arising from this context as well as outlining the computational\ntools needed to service this class of models. For example, the distribution of\nreplies within discussions tends to have a heavy tail. As such, a small number\nof posts and comments may generate many replies, while most generate few or\nnone, similar to `superspreading' in epidemics. Here, we propose a novel model\nfor online discussion, motivated by a dataset arising from discussions on the\nr/ireland subreddit, that accommodates such phenomena and develop a framework\nfor Bayesian inference that considers in- and out-of-sample tests for\ngoodness-of-fit. This analysis shows that discussions within this community\nfollow a circadian rhythm and are subject to moderate superspreading dynamics.\nFor example, we estimate that the expected discussion size is approximately\nfour for initial posts between 04:00 and 12:00 but approximately 2.5 from 15:00\nto 02:00. We also estimate that 58% to 62% of posts fail to generate any\ndiscussion, with 95% posterior probability. Thus, we demonstrate that our\nframework offers a general approach to modelling discussion on online boards.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Online boards offer a platform for sharing and discussing content, where\ndiscussion emerges as a cascade of comments in response to a post. Branching\npoint process models offer a practical approach to modelling these cascades;\nhowever, existing models do not account for apparent features of empirical\ndata. We address this gap by illustrating the flexibility of Hawkes processes\nto model data arising from this context as well as outlining the computational\ntools needed to service this class of models. For example, the distribution of\nreplies within discussions tends to have a heavy tail. As such, a small number\nof posts and comments may generate many replies, while most generate few or\nnone, similar to `superspreading' in epidemics. Here, we propose a novel model\nfor online discussion, motivated by a dataset arising from discussions on the\nr/ireland subreddit, that accommodates such phenomena and develop a framework\nfor Bayesian inference that considers in- and out-of-sample tests for\ngoodness-of-fit. This analysis shows that discussions within this community\nfollow a circadian rhythm and are subject to moderate superspreading dynamics.\nFor example, we estimate that the expected discussion size is approximately\nfour for initial posts between 04:00 and 12:00 but approximately 2.5 from 15:00\nto 02:00. We also estimate that 58% to 62% of posts fail to generate any\ndiscussion, with 95% posterior probability. Thus, we demonstrate that our\nframework offers a general approach to modelling discussion on online boards."
                },
                "authors": [
                    {
                        "name": "Joe Meagher"
                    },
                    {
                        "name": "Nial Friel"
                    }
                ],
                "author_detail": {
                    "name": "Nial Friel"
                },
                "author": "Nial Friel",
                "arxiv_comment": "30 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.13823v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.13823v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.AP",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.AP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2308.07870v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2308.07870v2",
                "updated": "2025-01-23T16:42:45Z",
                "updated_parsed": [
                    2025,
                    1,
                    23,
                    16,
                    42,
                    45,
                    3,
                    23,
                    0
                ],
                "published": "2023-08-15T16:37:16Z",
                "published_parsed": [
                    2023,
                    8,
                    15,
                    16,
                    37,
                    16,
                    1,
                    227,
                    0
                ],
                "title": "A Survey on Brain-Inspired Deep Learning via Predictive Coding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Survey on Brain-Inspired Deep Learning via Predictive Coding"
                },
                "summary": "Artificial intelligence (AI) is rapidly becoming one of the key technologies\nof this century. The majority of results in AI thus far have been achieved\nusing deep neural networks trained with the error backpropagation learning\nalgorithm. However, the ubiquitous adoption of this approach has highlighted\nsome important limitations such as substantial computational cost, difficulty\nin quantifying uncertainty, lack of robustness, unreliability, and biological\nimplausibility. It is possible that addressing these limitations may require\nschemes that are inspired and guided by neuroscience theories. One such theory,\ncalled predictive coding (PC), has shown promising performance in machine\nintelligence tasks, exhibiting exciting properties that make it potentially\nvaluable for the machine learning community: PC can model information\nprocessing in different brain areas, can be used in cognitive control and\nrobotics, and has a solid mathematical grounding in variational inference,\noffering a powerful inversion scheme for a specific class of continuous-state\ngenerative models. With the hope of foregrounding research in this direction,\nwe survey the literature that has contributed to this perspective, highlighting\nthe many ways that PC might play a role in the future of machine learning and\ncomputational intelligence at large.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Artificial intelligence (AI) is rapidly becoming one of the key technologies\nof this century. The majority of results in AI thus far have been achieved\nusing deep neural networks trained with the error backpropagation learning\nalgorithm. However, the ubiquitous adoption of this approach has highlighted\nsome important limitations such as substantial computational cost, difficulty\nin quantifying uncertainty, lack of robustness, unreliability, and biological\nimplausibility. It is possible that addressing these limitations may require\nschemes that are inspired and guided by neuroscience theories. One such theory,\ncalled predictive coding (PC), has shown promising performance in machine\nintelligence tasks, exhibiting exciting properties that make it potentially\nvaluable for the machine learning community: PC can model information\nprocessing in different brain areas, can be used in cognitive control and\nrobotics, and has a solid mathematical grounding in variational inference,\noffering a powerful inversion scheme for a specific class of continuous-state\ngenerative models. With the hope of foregrounding research in this direction,\nwe survey the literature that has contributed to this perspective, highlighting\nthe many ways that PC might play a role in the future of machine learning and\ncomputational intelligence at large."
                },
                "authors": [
                    {
                        "name": "Tommaso Salvatori"
                    },
                    {
                        "name": "Ankur Mali"
                    },
                    {
                        "name": "Christopher L. Buckley"
                    },
                    {
                        "name": "Thomas Lukasiewicz"
                    },
                    {
                        "name": "Rajesh P. N. Rao"
                    },
                    {
                        "name": "Karl Friston"
                    },
                    {
                        "name": "Alexander Ororbia"
                    }
                ],
                "author_detail": {
                    "name": "Alexander Ororbia"
                },
                "author": "Alexander Ororbia",
                "arxiv_comment": "37 Pages, 9 Figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2308.07870v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2308.07870v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.13816v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.13816v1",
                "updated": "2025-01-23T16:37:44Z",
                "updated_parsed": [
                    2025,
                    1,
                    23,
                    16,
                    37,
                    44,
                    3,
                    23,
                    0
                ],
                "published": "2025-01-23T16:37:44Z",
                "published_parsed": [
                    2025,
                    1,
                    23,
                    16,
                    37,
                    44,
                    3,
                    23,
                    0
                ],
                "title": "Large Language Model driven Policy Exploration for Recommender Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Model driven Policy Exploration for Recommender Systems"
                },
                "summary": "Recent advancements in Recommender Systems (RS) have incorporated\nReinforcement Learning (RL), framing the recommendation as a Markov Decision\nProcess (MDP). However, offline RL policies trained on static user data are\nvulnerable to distribution shift when deployed in dynamic online environments.\nAdditionally, excessive focus on exploiting short-term relevant items can\nhinder exploration, leading to suboptimal recommendations and negatively\nimpacting long-term user gains. Online RL-based RS also face challenges in\nproduction deployment, due to the risks of exposing users to untrained or\nunstable policies. Large Language Models (LLMs) offer a promising solution to\nmimic user objectives and preferences for pre-training policies offline to\nenhance the initial recommendations in online settings. Effectively managing\ndistribution shift and balancing exploration are crucial for improving RL-based\nRS, especially when leveraging LLM-based pre-training. To address these\nchallenges, we propose an Interaction-Augmented Learned Policy (iALP) that\nutilizes user preferences distilled from an LLM. Our approach involves\nprompting the LLM with user states to extract item preferences, learning\nrewards based on feedback, and updating the RL policy using an actor-critic\nframework. Furthermore, to deploy iALP in an online scenario, we introduce an\nadaptive variant, A-iALP, that implements a simple fine-tuning strategy\n(A-iALP$_{ft}$), and an adaptive approach (A-iALP$_{ap}$) designed to mitigate\nissues with compromised policies and limited exploration. Experiments across\nthree simulated environments demonstrate that A-iALP introduces substantial\nperformance improvements",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advancements in Recommender Systems (RS) have incorporated\nReinforcement Learning (RL), framing the recommendation as a Markov Decision\nProcess (MDP). However, offline RL policies trained on static user data are\nvulnerable to distribution shift when deployed in dynamic online environments.\nAdditionally, excessive focus on exploiting short-term relevant items can\nhinder exploration, leading to suboptimal recommendations and negatively\nimpacting long-term user gains. Online RL-based RS also face challenges in\nproduction deployment, due to the risks of exposing users to untrained or\nunstable policies. Large Language Models (LLMs) offer a promising solution to\nmimic user objectives and preferences for pre-training policies offline to\nenhance the initial recommendations in online settings. Effectively managing\ndistribution shift and balancing exploration are crucial for improving RL-based\nRS, especially when leveraging LLM-based pre-training. To address these\nchallenges, we propose an Interaction-Augmented Learned Policy (iALP) that\nutilizes user preferences distilled from an LLM. Our approach involves\nprompting the LLM with user states to extract item preferences, learning\nrewards based on feedback, and updating the RL policy using an actor-critic\nframework. Furthermore, to deploy iALP in an online scenario, we introduce an\nadaptive variant, A-iALP, that implements a simple fine-tuning strategy\n(A-iALP$_{ft}$), and an adaptive approach (A-iALP$_{ap}$) designed to mitigate\nissues with compromised policies and limited exploration. Experiments across\nthree simulated environments demonstrate that A-iALP introduces substantial\nperformance improvements"
                },
                "authors": [
                    {
                        "name": "Jie Wang"
                    },
                    {
                        "name": "Alexandros Karatzoglou"
                    },
                    {
                        "name": "Ioannis Arapakis"
                    },
                    {
                        "name": "Joemon M. Jose"
                    }
                ],
                "author_detail": {
                    "name": "Joemon M. Jose"
                },
                "author": "Joemon M. Jose",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.13816v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.13816v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.05513v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.05513v2",
                "updated": "2025-01-23T16:37:38Z",
                "updated_parsed": [
                    2025,
                    1,
                    23,
                    16,
                    37,
                    38,
                    3,
                    23,
                    0
                ],
                "published": "2025-01-09T19:00:02Z",
                "published_parsed": [
                    2025,
                    1,
                    9,
                    19,
                    0,
                    2,
                    3,
                    9,
                    0
                ],
                "title": "Discovery of a likely Type II SN at $z$=3.6 with JWST",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Discovery of a likely Type II SN at $z$=3.6 with JWST"
                },
                "summary": "Transient astronomy in the early, high-redshift (z > 3) Universe is an\nunexplored regime that offers the possibility of probing the first stars and\nthe Epoch of Reionization. During Cycles 1 and 2 of the James Webb Space\nTelescope (JWST), the JWST Advanced Deep Extragalactic Survey (JADES) program\nenabled one of the first searches for transients in deep images (~30 AB mag)\nover a relatively wide area (25 arcmin^2). One transient, AT 2023adsv, was\ndiscovered with an F200W magnitude of 28.04 AB mag, and subsequent JWST\nobservations revealed that the transient is a likely supernova (SN) in a host\nwith z_spec = 3.613 +/- 0.001 and an inferred metallicity at the position of\nthe SN of Z_* = 0.3 +/- 0.1 Z_{\\odot}. At this redshift, the first detections\nin F115W and F150W show that AT 2023adsv had bright rest-frame ultraviolet flux\nat the time of discovery. The multi-band light curve of AT 2023adsv is best\nmatched by a template of an SN IIP with a peak absolute magnitude of M_B ~\n-18.3 AB mag. We find a good match to a 20 M_{\\odot} red supergiant progenitor\nstar with an explosion energy of 2.0x10^51 ergs, likely higher than normally\nobserved in the local Universe, but consistent with SNe IIP drawn from local,\nlower metallicity environments. AT 2023adsv is the most distant photometrically\nclassified SN IIP yet discovered with a spectroscopic redshift measurement, and\nmay represent a global shift in SNe IIP properties as a function of redshift.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Transient astronomy in the early, high-redshift (z > 3) Universe is an\nunexplored regime that offers the possibility of probing the first stars and\nthe Epoch of Reionization. During Cycles 1 and 2 of the James Webb Space\nTelescope (JWST), the JWST Advanced Deep Extragalactic Survey (JADES) program\nenabled one of the first searches for transients in deep images (~30 AB mag)\nover a relatively wide area (25 arcmin^2). One transient, AT 2023adsv, was\ndiscovered with an F200W magnitude of 28.04 AB mag, and subsequent JWST\nobservations revealed that the transient is a likely supernova (SN) in a host\nwith z_spec = 3.613 +/- 0.001 and an inferred metallicity at the position of\nthe SN of Z_* = 0.3 +/- 0.1 Z_{\\odot}. At this redshift, the first detections\nin F115W and F150W show that AT 2023adsv had bright rest-frame ultraviolet flux\nat the time of discovery. The multi-band light curve of AT 2023adsv is best\nmatched by a template of an SN IIP with a peak absolute magnitude of M_B ~\n-18.3 AB mag. We find a good match to a 20 M_{\\odot} red supergiant progenitor\nstar with an explosion energy of 2.0x10^51 ergs, likely higher than normally\nobserved in the local Universe, but consistent with SNe IIP drawn from local,\nlower metallicity environments. AT 2023adsv is the most distant photometrically\nclassified SN IIP yet discovered with a spectroscopic redshift measurement, and\nmay represent a global shift in SNe IIP properties as a function of redshift."
                },
                "authors": [
                    {
                        "name": "D. A. Coulter"
                    },
                    {
                        "name": "J. D. R. Pierel"
                    },
                    {
                        "name": "C. DeCoursey"
                    },
                    {
                        "name": "T. J. Moriya"
                    },
                    {
                        "name": "M. R. Siebert"
                    },
                    {
                        "name": "B. A. Joshi"
                    },
                    {
                        "name": "M. Engesser"
                    },
                    {
                        "name": "A. Rest"
                    },
                    {
                        "name": "E. Egami"
                    },
                    {
                        "name": "M. Shahbandeh"
                    },
                    {
                        "name": "W. Chen"
                    },
                    {
                        "name": "O. D. Fox"
                    },
                    {
                        "name": "L. G. Strolger"
                    },
                    {
                        "name": "Y. Zenati"
                    },
                    {
                        "name": "A. J. Bunker"
                    },
                    {
                        "name": "P. A. Cargile"
                    },
                    {
                        "name": "M. Curti"
                    },
                    {
                        "name": "D. J. Eisenstein"
                    },
                    {
                        "name": "S. Gezari"
                    },
                    {
                        "name": "S. Gomez"
                    },
                    {
                        "name": "M. Guolo"
                    },
                    {
                        "name": "K. Hainline"
                    },
                    {
                        "name": "J. Jencson"
                    },
                    {
                        "name": "B. D. Johnson"
                    },
                    {
                        "name": "M. Karmen"
                    },
                    {
                        "name": "R. Maiolino"
                    },
                    {
                        "name": "R. M. Quimby"
                    },
                    {
                        "name": "P. Rinaldi"
                    },
                    {
                        "name": "B. Robertson"
                    },
                    {
                        "name": "S. Tacchella"
                    },
                    {
                        "name": "F. Sun"
                    },
                    {
                        "name": "Q. Wang"
                    },
                    {
                        "name": "T. Wevers"
                    }
                ],
                "author_detail": {
                    "name": "T. Wevers"
                },
                "author": "T. Wevers",
                "arxiv_comment": "20 pages, 9 figures, submitted to ApJ",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.05513v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.05513v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.HE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.HE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "astro-ph.GA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.12461v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.12461v2",
                "updated": "2025-01-23T16:31:51Z",
                "updated_parsed": [
                    2025,
                    1,
                    23,
                    16,
                    31,
                    51,
                    3,
                    23,
                    0
                ],
                "published": "2025-01-21T19:17:46Z",
                "published_parsed": [
                    2025,
                    1,
                    21,
                    19,
                    17,
                    46,
                    1,
                    21,
                    0
                ],
                "title": "Empowering AIOps: Leveraging Large Language Models for IT Operations\n  Management",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Empowering AIOps: Leveraging Large Language Models for IT Operations\n  Management"
                },
                "summary": "The integration of Artificial Intelligence (AI) into IT Operations Management\n(ITOM), commonly referred to as AIOps, offers substantial potential for\nautomating workflows, enhancing efficiency, and supporting informed\ndecision-making. However, implementing AI within IT operations is not without\nits challenges, including issues related to data quality, the complexity of IT\nenvironments, and skill gaps within teams. The advent of Large Language Models\n(LLMs) presents an opportunity to address some of these challenges,\nparticularly through their advanced natural language understanding\ncapabilities. These features enable organizations to process and analyze vast\namounts of unstructured data, such as system logs, incident reports, and\ntechnical documentation. This ability aligns with the motivation behind our\nresearch, where we aim to integrate traditional predictive machine learning\nmodels with generative AI technologies like LLMs. By combining these\napproaches, we propose innovative methods to tackle persistent challenges in\nAIOps and enhance the capabilities of IT operations management.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The integration of Artificial Intelligence (AI) into IT Operations Management\n(ITOM), commonly referred to as AIOps, offers substantial potential for\nautomating workflows, enhancing efficiency, and supporting informed\ndecision-making. However, implementing AI within IT operations is not without\nits challenges, including issues related to data quality, the complexity of IT\nenvironments, and skill gaps within teams. The advent of Large Language Models\n(LLMs) presents an opportunity to address some of these challenges,\nparticularly through their advanced natural language understanding\ncapabilities. These features enable organizations to process and analyze vast\namounts of unstructured data, such as system logs, incident reports, and\ntechnical documentation. This ability aligns with the motivation behind our\nresearch, where we aim to integrate traditional predictive machine learning\nmodels with generative AI technologies like LLMs. By combining these\napproaches, we propose innovative methods to tackle persistent challenges in\nAIOps and enhance the capabilities of IT operations management."
                },
                "authors": [
                    {
                        "name": "Arthur Vitui"
                    },
                    {
                        "name": "Tse-Hsun Chen"
                    }
                ],
                "author_detail": {
                    "name": "Tse-Hsun Chen"
                },
                "author": "Tse-Hsun Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.12461v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.12461v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.12574v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.12574v4",
                "updated": "2025-01-23T16:31:49Z",
                "updated_parsed": [
                    2025,
                    1,
                    23,
                    16,
                    31,
                    49,
                    3,
                    23,
                    0
                ],
                "published": "2024-08-22T17:41:45Z",
                "published_parsed": [
                    2024,
                    8,
                    22,
                    17,
                    41,
                    45,
                    3,
                    235,
                    0
                ],
                "title": "MuMA-ToM: Multi-modal Multi-Agent Theory of Mind",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MuMA-ToM: Multi-modal Multi-Agent Theory of Mind"
                },
                "summary": "Understanding people's social interactions in complex real-world scenarios\noften relies on intricate mental reasoning. To truly understand how and why\npeople interact with one another, we must infer the underlying mental states\nthat give rise to the social interactions, i.e., Theory of Mind reasoning in\nmulti-agent interactions. Additionally, social interactions are often\nmulti-modal -- we can watch people's actions, hear their conversations, and/or\nread about their past behaviors. For AI systems to successfully and safely\ninteract with people in real-world environments, they also need to understand\npeople's mental states as well as their inferences about each other's mental\nstates based on multi-modal information about their interactions. For this, we\nintroduce MuMA-ToM, a Multi-modal Multi-Agent Theory of Mind benchmark.\nMuMA-ToM is the first multi-modal Theory of Mind benchmark that evaluates\nmental reasoning in embodied multi-agent interactions. In MuMA-ToM, we provide\nvideo and text descriptions of people's multi-modal behavior in realistic\nhousehold environments. Based on the context, we then ask questions about\npeople's goals, beliefs, and beliefs about others' goals. We validated MuMA-ToM\nin a human experiment and provided a human baseline. We also proposed a novel\nmulti-modal, multi-agent ToM model, LIMP (Language model-based Inverse\nMulti-agent Planning). Our experimental results show that LIMP significantly\noutperforms state-of-the-art methods, including large multi-modal models (e.g.,\nGPT-4o, Gemini-1.5 Pro) and a recent multi-modal ToM model, BIP-ALM.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Understanding people's social interactions in complex real-world scenarios\noften relies on intricate mental reasoning. To truly understand how and why\npeople interact with one another, we must infer the underlying mental states\nthat give rise to the social interactions, i.e., Theory of Mind reasoning in\nmulti-agent interactions. Additionally, social interactions are often\nmulti-modal -- we can watch people's actions, hear their conversations, and/or\nread about their past behaviors. For AI systems to successfully and safely\ninteract with people in real-world environments, they also need to understand\npeople's mental states as well as their inferences about each other's mental\nstates based on multi-modal information about their interactions. For this, we\nintroduce MuMA-ToM, a Multi-modal Multi-Agent Theory of Mind benchmark.\nMuMA-ToM is the first multi-modal Theory of Mind benchmark that evaluates\nmental reasoning in embodied multi-agent interactions. In MuMA-ToM, we provide\nvideo and text descriptions of people's multi-modal behavior in realistic\nhousehold environments. Based on the context, we then ask questions about\npeople's goals, beliefs, and beliefs about others' goals. We validated MuMA-ToM\nin a human experiment and provided a human baseline. We also proposed a novel\nmulti-modal, multi-agent ToM model, LIMP (Language model-based Inverse\nMulti-agent Planning). Our experimental results show that LIMP significantly\noutperforms state-of-the-art methods, including large multi-modal models (e.g.,\nGPT-4o, Gemini-1.5 Pro) and a recent multi-modal ToM model, BIP-ALM."
                },
                "authors": [
                    {
                        "name": "Haojun Shi"
                    },
                    {
                        "name": "Suyu Ye"
                    },
                    {
                        "name": "Xinyu Fang"
                    },
                    {
                        "name": "Chuanyang Jin"
                    },
                    {
                        "name": "Leyla Isik"
                    },
                    {
                        "name": "Yen-Ling Kuo"
                    },
                    {
                        "name": "Tianmin Shu"
                    }
                ],
                "author_detail": {
                    "name": "Tianmin Shu"
                },
                "author": "Tianmin Shu",
                "arxiv_comment": "AAAI-25 (Oral). Project website:\n  https://scai.cs.jhu.edu/projects/MuMA-ToM/ Code:\n  https://github.com/SCAI-JHU/MuMA-ToM",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.12574v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.12574v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.13802v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.13802v1",
                "updated": "2025-01-23T16:21:15Z",
                "updated_parsed": [
                    2025,
                    1,
                    23,
                    16,
                    21,
                    15,
                    3,
                    23,
                    0
                ],
                "published": "2025-01-23T16:21:15Z",
                "published_parsed": [
                    2025,
                    1,
                    23,
                    16,
                    21,
                    15,
                    3,
                    23,
                    0
                ],
                "title": "Enhancing LLMs for Governance with Human Oversight: Evaluating and\n  Aligning LLMs on Expert Classification of Climate Misinformation for\n  Detecting False or Misleading Claims about Climate Change",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Enhancing LLMs for Governance with Human Oversight: Evaluating and\n  Aligning LLMs on Expert Classification of Climate Misinformation for\n  Detecting False or Misleading Claims about Climate Change"
                },
                "summary": "Climate misinformation is a problem that has the potential to be\nsubstantially aggravated by the development of Large Language Models (LLMs). In\nthis study we evaluate the potential for LLMs to be part of the solution for\nmitigating online dis/misinformation rather than the problem. Employing a\npublic expert annotated dataset and a curated sample of social media content we\nevaluate the performance of proprietary vs. open source LLMs on climate\nmisinformation classification task, comparing them to existing climate-focused\ncomputer-assisted tools and expert assessments. Results show (1)\nstate-of-the-art (SOTA) open-source models substantially under-perform in\nclassifying climate misinformation compared to proprietary models, (2) existing\nclimate-focused computer-assisted tools leveraging expert-annotated datasets\ncontinues to outperform many of proprietary models, including GPT-4o, and (3)\ndemonstrate the efficacy and generalizability of fine-tuning GPT-3.5-turbo on\nexpert annotated dataset in classifying claims about climate change at the\nequivalency of climate change experts with over 20 years of experience in\nclimate communication. These findings highlight 1) the importance of\nincorporating human-oversight, such as incorporating expert-annotated datasets\nin training LLMs, for governance tasks that require subject-matter expertise\nlike classifying climate misinformation, and 2) the potential for LLMs in\nfacilitating civil society organizations to engage in various governance tasks\nsuch as classifying false or misleading claims in domains beyond climate change\nsuch as politics and health science.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Climate misinformation is a problem that has the potential to be\nsubstantially aggravated by the development of Large Language Models (LLMs). In\nthis study we evaluate the potential for LLMs to be part of the solution for\nmitigating online dis/misinformation rather than the problem. Employing a\npublic expert annotated dataset and a curated sample of social media content we\nevaluate the performance of proprietary vs. open source LLMs on climate\nmisinformation classification task, comparing them to existing climate-focused\ncomputer-assisted tools and expert assessments. Results show (1)\nstate-of-the-art (SOTA) open-source models substantially under-perform in\nclassifying climate misinformation compared to proprietary models, (2) existing\nclimate-focused computer-assisted tools leveraging expert-annotated datasets\ncontinues to outperform many of proprietary models, including GPT-4o, and (3)\ndemonstrate the efficacy and generalizability of fine-tuning GPT-3.5-turbo on\nexpert annotated dataset in classifying claims about climate change at the\nequivalency of climate change experts with over 20 years of experience in\nclimate communication. These findings highlight 1) the importance of\nincorporating human-oversight, such as incorporating expert-annotated datasets\nin training LLMs, for governance tasks that require subject-matter expertise\nlike classifying climate misinformation, and 2) the potential for LLMs in\nfacilitating civil society organizations to engage in various governance tasks\nsuch as classifying false or misleading claims in domains beyond climate change\nsuch as politics and health science."
                },
                "authors": [
                    {
                        "name": "Mowafak Allaham"
                    },
                    {
                        "name": "Ayse D. Lokmanoglu"
                    },
                    {
                        "name": "Sol P. Hart"
                    },
                    {
                        "name": "Erik C. Nisbet"
                    }
                ],
                "author_detail": {
                    "name": "Erik C. Nisbet"
                },
                "author": "Erik C. Nisbet",
                "arxiv_comment": "Accepted to the AI Governance Workshop at AAAI 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.13802v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.13802v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.13797v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.13797v1",
                "updated": "2025-01-23T16:15:45Z",
                "updated_parsed": [
                    2025,
                    1,
                    23,
                    16,
                    15,
                    45,
                    3,
                    23,
                    0
                ],
                "published": "2025-01-23T16:15:45Z",
                "published_parsed": [
                    2025,
                    1,
                    23,
                    16,
                    15,
                    45,
                    3,
                    23,
                    0
                ],
                "title": "Inference for generalized additive mixed models via penalized marginal\n  likelihood",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Inference for generalized additive mixed models via penalized marginal\n  likelihood"
                },
                "summary": "Existing methods for fitting generalized additive mixed models to\nlongitudinal repeated measures data rely on Laplace-approximate marginal\nlikelihood for estimation of variance components and smoothing penalty\nparameters. This is thought to be appropriate due to the Laplace approximation\nbeing established as an appropriate tool for smoothing penalty parameter\nestimation in spline models and the well-known connection between penalized\nregression and random effects. This paper argues that the Laplace approximation\nis sometimes not sufficiently accurate for smoothing parameter estimation in\ngeneralized additive mixed models leading to estimates that exhibit increasing\nbias and decreasing confidence interval coverage as more groups are sampled. A\nnovel estimation strategy based on penalizing an adaptive quadrature\napproximate marginal likelihood is proposed that solves this problem and leads\nto estimates exhibiting the correct statistical properties.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Existing methods for fitting generalized additive mixed models to\nlongitudinal repeated measures data rely on Laplace-approximate marginal\nlikelihood for estimation of variance components and smoothing penalty\nparameters. This is thought to be appropriate due to the Laplace approximation\nbeing established as an appropriate tool for smoothing penalty parameter\nestimation in spline models and the well-known connection between penalized\nregression and random effects. This paper argues that the Laplace approximation\nis sometimes not sufficiently accurate for smoothing parameter estimation in\ngeneralized additive mixed models leading to estimates that exhibit increasing\nbias and decreasing confidence interval coverage as more groups are sampled. A\nnovel estimation strategy based on penalizing an adaptive quadrature\napproximate marginal likelihood is proposed that solves this problem and leads\nto estimates exhibiting the correct statistical properties."
                },
                "authors": [
                    {
                        "name": "Alex Stringer"
                    }
                ],
                "author_detail": {
                    "name": "Alex Stringer"
                },
                "author": "Alex Stringer",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.13797v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.13797v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ME",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.10362v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.10362v3",
                "updated": "2025-01-23T16:11:11Z",
                "updated_parsed": [
                    2025,
                    1,
                    23,
                    16,
                    11,
                    11,
                    3,
                    23,
                    0
                ],
                "published": "2024-11-15T17:11:13Z",
                "published_parsed": [
                    2024,
                    11,
                    15,
                    17,
                    11,
                    13,
                    4,
                    320,
                    0
                ],
                "title": "Interactive Cycle Model: The Linkage Combination among Automatic Speech\n  Recognition, Large Language Models and Smart Glasses",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Interactive Cycle Model: The Linkage Combination among Automatic Speech\n  Recognition, Large Language Models and Smart Glasses"
                },
                "summary": "This research proposes the interaction loop model \"ASR-LLMs-Smart Glasses\",\nwhich model combines automatic speech recognition, large language model and\nsmart glasses to facilitate seamless human-computer interaction. And the\nmethodology of this research involves decomposing the interaction process into\ndifferent stages and elements. Speech is captured and processed by ASR, then\nanalyzed and interpreted by LLMs. The results are then transmitted to smart\nglasses for display. The feedback loop is complete when the user interacts with\nthe displayed data. Mathematical formulas are used to quantify the performance\nof the model that revolves around core evaluation points: accuracy, coherence,\nand latency during ASR speech-to-text conversion. The research results are\nprovided theoretically to test and evaluate the feasibility and performance of\nthe model. Detailed architectural details and experimental process have been\nuploaded to Github, the link\nis:https://github.com/brucewang123456789/GeniusTrail.git.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This research proposes the interaction loop model \"ASR-LLMs-Smart Glasses\",\nwhich model combines automatic speech recognition, large language model and\nsmart glasses to facilitate seamless human-computer interaction. And the\nmethodology of this research involves decomposing the interaction process into\ndifferent stages and elements. Speech is captured and processed by ASR, then\nanalyzed and interpreted by LLMs. The results are then transmitted to smart\nglasses for display. The feedback loop is complete when the user interacts with\nthe displayed data. Mathematical formulas are used to quantify the performance\nof the model that revolves around core evaluation points: accuracy, coherence,\nand latency during ASR speech-to-text conversion. The research results are\nprovided theoretically to test and evaluate the feasibility and performance of\nthe model. Detailed architectural details and experimental process have been\nuploaded to Github, the link\nis:https://github.com/brucewang123456789/GeniusTrail.git."
                },
                "authors": [
                    {
                        "name": "Libo Wang"
                    }
                ],
                "author_detail": {
                    "name": "Libo Wang"
                },
                "author": "Libo Wang",
                "arxiv_comment": "OpenReview submitted. 10 pages of text and 2 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.10362v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.10362v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.09111v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.09111v5",
                "updated": "2025-01-23T16:09:32Z",
                "updated_parsed": [
                    2025,
                    1,
                    23,
                    16,
                    9,
                    32,
                    3,
                    23,
                    0
                ],
                "published": "2024-11-14T00:59:13Z",
                "published_parsed": [
                    2024,
                    11,
                    14,
                    0,
                    59,
                    13,
                    3,
                    319,
                    0
                ],
                "title": "Reducing Reasoning Costs: The Path of Optimization for Chain of Thought\n  via Sparse Attention Mechanism",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reducing Reasoning Costs: The Path of Optimization for Chain of Thought\n  via Sparse Attention Mechanism"
                },
                "summary": "In order to address the chain of thought in the large language model\ninference cost surge, this research proposes to use a sparse attention\nmechanism that only focuses on a few relevant tokens. The researcher\nconstructed a new attention mechanism and used GiantRabbit trained with custom\nGPTs as an experimental tool. The experiment tested and compared the reasoning\ntime, correctness score and chain of thought length of this model and o1\nPreview in solving the linear algebra test questions of MIT OpenCourseWare. The\nresults show that GiantRabbit's reasoning time and chain of thought length are\nsignificantly lower than o1 Preview. It verifies the feasibility of sparse\nattention mechanism for optimizing chain of thought reasoning. Detailed\narchitectural details and experimental process have been uploaded to Github,\nthe link is:https://github.com/brucewang123456789/GeniusTrail.git.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In order to address the chain of thought in the large language model\ninference cost surge, this research proposes to use a sparse attention\nmechanism that only focuses on a few relevant tokens. The researcher\nconstructed a new attention mechanism and used GiantRabbit trained with custom\nGPTs as an experimental tool. The experiment tested and compared the reasoning\ntime, correctness score and chain of thought length of this model and o1\nPreview in solving the linear algebra test questions of MIT OpenCourseWare. The\nresults show that GiantRabbit's reasoning time and chain of thought length are\nsignificantly lower than o1 Preview. It verifies the feasibility of sparse\nattention mechanism for optimizing chain of thought reasoning. Detailed\narchitectural details and experimental process have been uploaded to Github,\nthe link is:https://github.com/brucewang123456789/GeniusTrail.git."
                },
                "authors": [
                    {
                        "name": "Libo Wang"
                    }
                ],
                "author_detail": {
                    "name": "Libo Wang"
                },
                "author": "Libo Wang",
                "arxiv_comment": "The main text is 5 pages, totaling 9 pages; 4 figures, 1 table. It\n  have been submitted to NeurIPS 2024 Workshop MusIML and OpenReview",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.09111v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.09111v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.10500v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.10500v2",
                "updated": "2025-01-23T16:05:16Z",
                "updated_parsed": [
                    2025,
                    1,
                    23,
                    16,
                    5,
                    16,
                    3,
                    23,
                    0
                ],
                "published": "2025-01-17T18:38:25Z",
                "published_parsed": [
                    2025,
                    1,
                    17,
                    18,
                    38,
                    25,
                    4,
                    17,
                    0
                ],
                "title": "A Bayesian Approach to Inferring Accretion Signatures in Young Stellar\n  Objects: A Case Study with VIRUS",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Bayesian Approach to Inferring Accretion Signatures in Young Stellar\n  Objects: A Case Study with VIRUS"
                },
                "summary": "The mass accretion rates of young stellar objects (YSOs) are key to\nunderstanding how stars form, how their circumstellar disks evolve, and even\nhow planets form. We develop a Bayesian framework to determine the accretion\nrates of a sample of 15 YSOs using archival data from the VIRUS spectrograph\n($R \\sim 800$, 3500-5500\\r{A}) on the Hobby-Eberly Telescope. We are publicly\nreleasing our developed tool, dubbed nuts-for-ysos, as a Python package which\ncan also be applied to other spectroscopic datasets. The nuts-for-ysos code\nfits a simple accretion model to the near-UV and optical continuum of each\nVIRUS spectrum. Our Bayesian approach aims to identify correlations between\nmodel parameters using the No U-Turn Sampler (NUTS). Moreover, this approach\nself-consistently incorporates all parameter uncertainties, allowing for a\nthorough estimation of the probability distribution for accretion rate not\naccomplished in previous works. Using nuts-for-ysos, we derive accretion rates\nof each YSO. We then verify the reliability of our method by comparing to\nresults separately derived from only the spectral emission lines, and to\nresults from earlier studies of the Lupus, Chamaeleon I, and NGC1333 regions.\nFinally, we discuss what qualitative trends, covariances, and degeneracies were\nfound among model parameters. The technique developed in this paper is a useful\nimprovement that can be applied in the future to larger samples of YSOs\nobserved by VIRUS or other spectrographs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The mass accretion rates of young stellar objects (YSOs) are key to\nunderstanding how stars form, how their circumstellar disks evolve, and even\nhow planets form. We develop a Bayesian framework to determine the accretion\nrates of a sample of 15 YSOs using archival data from the VIRUS spectrograph\n($R \\sim 800$, 3500-5500\\r{A}) on the Hobby-Eberly Telescope. We are publicly\nreleasing our developed tool, dubbed nuts-for-ysos, as a Python package which\ncan also be applied to other spectroscopic datasets. The nuts-for-ysos code\nfits a simple accretion model to the near-UV and optical continuum of each\nVIRUS spectrum. Our Bayesian approach aims to identify correlations between\nmodel parameters using the No U-Turn Sampler (NUTS). Moreover, this approach\nself-consistently incorporates all parameter uncertainties, allowing for a\nthorough estimation of the probability distribution for accretion rate not\naccomplished in previous works. Using nuts-for-ysos, we derive accretion rates\nof each YSO. We then verify the reliability of our method by comparing to\nresults separately derived from only the spectral emission lines, and to\nresults from earlier studies of the Lupus, Chamaeleon I, and NGC1333 regions.\nFinally, we discuss what qualitative trends, covariances, and degeneracies were\nfound among model parameters. The technique developed in this paper is a useful\nimprovement that can be applied in the future to larger samples of YSOs\nobserved by VIRUS or other spectrographs."
                },
                "authors": [
                    {
                        "name": "Lauren Halstead Willett"
                    },
                    {
                        "name": "Joe P. Ninan"
                    },
                    {
                        "name": "Suvrath Mahadevan"
                    },
                    {
                        "name": "Gregory R. Zeimann"
                    },
                    {
                        "name": "Steven Janowiecki"
                    },
                    {
                        "name": "Gary J. Hill"
                    }
                ],
                "author_detail": {
                    "name": "Gary J. Hill"
                },
                "author": "Gary J. Hill",
                "arxiv_comment": "51 pages, 50 figures, accepted for publication in The Astronomical\n  Journal. New version with typo/formatting corrections",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.10500v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.10500v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.SR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.SR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "astro-ph.GA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "astro-ph.IM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.13788v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.13788v1",
                "updated": "2025-01-23T16:04:51Z",
                "updated_parsed": [
                    2025,
                    1,
                    23,
                    16,
                    4,
                    51,
                    3,
                    23,
                    0
                ],
                "published": "2025-01-23T16:04:51Z",
                "published_parsed": [
                    2025,
                    1,
                    23,
                    16,
                    4,
                    51,
                    3,
                    23,
                    0
                ],
                "title": "Free-Streaming Neutrinos and Their Phase Shift in Current and Future CMB\n  Power Spectra",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Free-Streaming Neutrinos and Their Phase Shift in Current and Future CMB\n  Power Spectra"
                },
                "summary": "The cosmic neutrino background and other light relics leave distinct imprints\nin the cosmic microwave background anisotropies through their gravitational\ninfluence. Since neutrinos decoupled from the primordial plasma about one\nsecond after the big bang, they have been free-streaming through the universe.\nThis induced a characteristic phase shift in the acoustic peaks as a unique\nsignature. In this work, we constrain the free-streaming nature of these\nrelativistic species and other light relics beyond the Standard Model of\nparticle physics by establishing two complementary template-based approaches to\nrobustly infer the size of this phase shift from the temperature and\npolarization power spectra. One template shifts the multipoles in these\nspectra, while the other novel template more fundamentally isolates the phase\nshift at the level of the underlying photon-baryon perturbations. Applying\nthese methods to Planck data, we detect the neutrino-induced phase shift at\nabout $10\\sigma$ significance, which rises to roughly $14\\sigma$ with\nadditional data from the Atacama Cosmology Telescope and the South Pole\nTelescope. We also infer that the data is consistent with the Standard Model\nprediction of three free-streaming neutrinos. In addition, we forecast the\ncapabilities of future experiments which will enable significantly more precise\nphase-shift measurements, with the Simons Observatory and CMB-S4 reducing the\n$1\\sigma$ uncertainties to roughly 4.3% and 2.5%, respectively. More generally,\nwe establish a new analysis pipeline for the phase shift induced by neutrinos\nand other free-streaming dark radiation which additionally offers new avenues\nfor exploring physics beyond the Standard Model in a signature-driven and\nmodel-agnostic way.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The cosmic neutrino background and other light relics leave distinct imprints\nin the cosmic microwave background anisotropies through their gravitational\ninfluence. Since neutrinos decoupled from the primordial plasma about one\nsecond after the big bang, they have been free-streaming through the universe.\nThis induced a characteristic phase shift in the acoustic peaks as a unique\nsignature. In this work, we constrain the free-streaming nature of these\nrelativistic species and other light relics beyond the Standard Model of\nparticle physics by establishing two complementary template-based approaches to\nrobustly infer the size of this phase shift from the temperature and\npolarization power spectra. One template shifts the multipoles in these\nspectra, while the other novel template more fundamentally isolates the phase\nshift at the level of the underlying photon-baryon perturbations. Applying\nthese methods to Planck data, we detect the neutrino-induced phase shift at\nabout $10\\sigma$ significance, which rises to roughly $14\\sigma$ with\nadditional data from the Atacama Cosmology Telescope and the South Pole\nTelescope. We also infer that the data is consistent with the Standard Model\nprediction of three free-streaming neutrinos. In addition, we forecast the\ncapabilities of future experiments which will enable significantly more precise\nphase-shift measurements, with the Simons Observatory and CMB-S4 reducing the\n$1\\sigma$ uncertainties to roughly 4.3% and 2.5%, respectively. More generally,\nwe establish a new analysis pipeline for the phase shift induced by neutrinos\nand other free-streaming dark radiation which additionally offers new avenues\nfor exploring physics beyond the Standard Model in a signature-driven and\nmodel-agnostic way."
                },
                "authors": [
                    {
                        "name": "Gabriele Montefalcone"
                    },
                    {
                        "name": "Benjamin Wallisch"
                    },
                    {
                        "name": "Katherine Freese"
                    }
                ],
                "author_detail": {
                    "name": "Katherine Freese"
                },
                "author": "Katherine Freese",
                "arxiv_comment": "50 pages, 14 figures, 8 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.13788v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.13788v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.CO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.CO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "hep-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "hep-th",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.13778v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.13778v1",
                "updated": "2025-01-23T15:55:07Z",
                "updated_parsed": [
                    2025,
                    1,
                    23,
                    15,
                    55,
                    7,
                    3,
                    23,
                    0
                ],
                "published": "2025-01-23T15:55:07Z",
                "published_parsed": [
                    2025,
                    1,
                    23,
                    15,
                    55,
                    7,
                    3,
                    23,
                    0
                ],
                "title": "Explainable XR: Understanding User Behaviors of XR Environments using\n  LLM-assisted Analytics Framework",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Explainable XR: Understanding User Behaviors of XR Environments using\n  LLM-assisted Analytics Framework"
                },
                "summary": "We present Explainable XR, an end-to-end framework for analyzing user\nbehavior in diverse eXtended Reality (XR) environments by leveraging Large\nLanguage Models (LLMs) for data interpretation assistance. Existing XR user\nanalytics frameworks face challenges in handling cross-virtuality - AR, VR, MR\n- transitions, multi-user collaborative application scenarios, and the\ncomplexity of multimodal data. Explainable XR addresses these challenges by\nproviding a virtuality-agnostic solution for the collection, analysis, and\nvisualization of immersive sessions. We propose three main components in our\nframework: (1) A novel user data recording schema, called User Action\nDescriptor (UAD), that can capture the users' multimodal actions, along with\ntheir intents and the contexts; (2) a platform-agnostic XR session recorder,\nand (3) a visual analytics interface that offers LLM-assisted insights tailored\nto the analysts' perspectives, facilitating the exploration and analysis of the\nrecorded XR session data. We demonstrate the versatility of Explainable XR by\ndemonstrating five use-case scenarios, in both individual and collaborative XR\napplications across virtualities. Our technical evaluation and user studies\nshow that Explainable XR provides a highly usable analytics solution for\nunderstanding user actions and delivering multifaceted, actionable insights\ninto user behaviors in immersive environments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present Explainable XR, an end-to-end framework for analyzing user\nbehavior in diverse eXtended Reality (XR) environments by leveraging Large\nLanguage Models (LLMs) for data interpretation assistance. Existing XR user\nanalytics frameworks face challenges in handling cross-virtuality - AR, VR, MR\n- transitions, multi-user collaborative application scenarios, and the\ncomplexity of multimodal data. Explainable XR addresses these challenges by\nproviding a virtuality-agnostic solution for the collection, analysis, and\nvisualization of immersive sessions. We propose three main components in our\nframework: (1) A novel user data recording schema, called User Action\nDescriptor (UAD), that can capture the users' multimodal actions, along with\ntheir intents and the contexts; (2) a platform-agnostic XR session recorder,\nand (3) a visual analytics interface that offers LLM-assisted insights tailored\nto the analysts' perspectives, facilitating the exploration and analysis of the\nrecorded XR session data. We demonstrate the versatility of Explainable XR by\ndemonstrating five use-case scenarios, in both individual and collaborative XR\napplications across virtualities. Our technical evaluation and user studies\nshow that Explainable XR provides a highly usable analytics solution for\nunderstanding user actions and delivering multifaceted, actionable insights\ninto user behaviors in immersive environments."
                },
                "authors": [
                    {
                        "name": "Yoonsang Kim"
                    },
                    {
                        "name": "Zainab Aamir"
                    },
                    {
                        "name": "Mithilesh Singh"
                    },
                    {
                        "name": "Saeed Boorboor"
                    },
                    {
                        "name": "Klaus Mueller"
                    },
                    {
                        "name": "Arie E. Kaufman"
                    }
                ],
                "author_detail": {
                    "name": "Arie E. Kaufman"
                },
                "author": "Arie E. Kaufman",
                "arxiv_comment": "11 pages, 8 figures. This is the author's version of the article that\n  has been accepted for publication in IEEE Transactions on Visualization and\n  Computer Graphics",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.13778v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.13778v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.13773v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.13773v1",
                "updated": "2025-01-23T15:52:34Z",
                "updated_parsed": [
                    2025,
                    1,
                    23,
                    15,
                    52,
                    34,
                    3,
                    23,
                    0
                ],
                "published": "2025-01-23T15:52:34Z",
                "published_parsed": [
                    2025,
                    1,
                    23,
                    15,
                    52,
                    34,
                    3,
                    23,
                    0
                ],
                "title": "Do Large Language Models Truly Understand Geometric Structures?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Do Large Language Models Truly Understand Geometric Structures?"
                },
                "summary": "Geometric ability is a significant challenge for large language models (LLMs)\ndue to the need for advanced spatial comprehension and abstract thinking.\nExisting datasets primarily evaluate LLMs on their final answers, but they\ncannot truly measure their true understanding of geometric structures, as LLMs\ncan arrive at correct answers by coincidence. To fill this gap, we introduce\nthe GeomRel dataset, designed to evaluate LLMs' understanding of geometric\nstructures by isolating the core step of geometric relationship identification\nin problem-solving. Using this benchmark, we conduct thorough evaluations of\ndiverse LLMs and identify key limitations in understanding geometric\nstructures. We further propose the Geometry Chain-of-Thought (GeoCoT) method,\nwhich enhances LLMs' ability to identify geometric relationships, resulting in\nsignificant performance improvements.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Geometric ability is a significant challenge for large language models (LLMs)\ndue to the need for advanced spatial comprehension and abstract thinking.\nExisting datasets primarily evaluate LLMs on their final answers, but they\ncannot truly measure their true understanding of geometric structures, as LLMs\ncan arrive at correct answers by coincidence. To fill this gap, we introduce\nthe GeomRel dataset, designed to evaluate LLMs' understanding of geometric\nstructures by isolating the core step of geometric relationship identification\nin problem-solving. Using this benchmark, we conduct thorough evaluations of\ndiverse LLMs and identify key limitations in understanding geometric\nstructures. We further propose the Geometry Chain-of-Thought (GeoCoT) method,\nwhich enhances LLMs' ability to identify geometric relationships, resulting in\nsignificant performance improvements."
                },
                "authors": [
                    {
                        "name": "Xiaofeng Wang"
                    },
                    {
                        "name": "Yiming Wang"
                    },
                    {
                        "name": "Wenhong Zhu"
                    },
                    {
                        "name": "Rui Wang"
                    }
                ],
                "author_detail": {
                    "name": "Rui Wang"
                },
                "author": "Rui Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.13773v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.13773v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.13772v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.13772v1",
                "updated": "2025-01-23T15:51:38Z",
                "updated_parsed": [
                    2025,
                    1,
                    23,
                    15,
                    51,
                    38,
                    3,
                    23,
                    0
                ],
                "published": "2025-01-23T15:51:38Z",
                "published_parsed": [
                    2025,
                    1,
                    23,
                    15,
                    51,
                    38,
                    3,
                    23,
                    0
                ],
                "title": "Tune In, Act Up: Exploring the Impact of Audio Modality-Specific Edits\n  on Large Audio Language Models in Jailbreak",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Tune In, Act Up: Exploring the Impact of Audio Modality-Specific Edits\n  on Large Audio Language Models in Jailbreak"
                },
                "summary": "Large Language Models (LLMs) demonstrate remarkable zero-shot performance\nacross various natural language processing tasks. The integration of multimodal\nencoders extends their capabilities, enabling the development of Multimodal\nLarge Language Models that process vision, audio, and text. However, these\ncapabilities also raise significant security concerns, as these models can be\nmanipulated to generate harmful or inappropriate content through jailbreak.\nWhile extensive research explores the impact of modality-specific input edits\non text-based LLMs and Large Vision-Language Models in jailbreak, the effects\nof audio-specific edits on Large Audio-Language Models (LALMs) remain\nunderexplored. Hence, this paper addresses this gap by investigating how\naudio-specific edits influence LALMs inference regarding jailbreak. We\nintroduce the Audio Editing Toolbox (AET), which enables audio-modality edits\nsuch as tone adjustment, word emphasis, and noise injection, and the Edited\nAudio Datasets (EADs), a comprehensive audio jailbreak benchmark. We also\nconduct extensive evaluations of state-of-the-art LALMs to assess their\nrobustness under different audio edits. This work lays the groundwork for\nfuture explorations on audio-modality interactions in LALMs security.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) demonstrate remarkable zero-shot performance\nacross various natural language processing tasks. The integration of multimodal\nencoders extends their capabilities, enabling the development of Multimodal\nLarge Language Models that process vision, audio, and text. However, these\ncapabilities also raise significant security concerns, as these models can be\nmanipulated to generate harmful or inappropriate content through jailbreak.\nWhile extensive research explores the impact of modality-specific input edits\non text-based LLMs and Large Vision-Language Models in jailbreak, the effects\nof audio-specific edits on Large Audio-Language Models (LALMs) remain\nunderexplored. Hence, this paper addresses this gap by investigating how\naudio-specific edits influence LALMs inference regarding jailbreak. We\nintroduce the Audio Editing Toolbox (AET), which enables audio-modality edits\nsuch as tone adjustment, word emphasis, and noise injection, and the Edited\nAudio Datasets (EADs), a comprehensive audio jailbreak benchmark. We also\nconduct extensive evaluations of state-of-the-art LALMs to assess their\nrobustness under different audio edits. This work lays the groundwork for\nfuture explorations on audio-modality interactions in LALMs security."
                },
                "authors": [
                    {
                        "name": "Erjia Xiao"
                    },
                    {
                        "name": "Hao Cheng"
                    },
                    {
                        "name": "Jing Shao"
                    },
                    {
                        "name": "Jinhao Duan"
                    },
                    {
                        "name": "Kaidi Xu"
                    },
                    {
                        "name": "Le Yang"
                    },
                    {
                        "name": "Jindong Gu"
                    },
                    {
                        "name": "Renjing Xu"
                    }
                ],
                "author_detail": {
                    "name": "Renjing Xu"
                },
                "author": "Renjing Xu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.13772v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.13772v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SD",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.13769v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.13769v1",
                "updated": "2025-01-23T15:48:41Z",
                "updated_parsed": [
                    2025,
                    1,
                    23,
                    15,
                    48,
                    41,
                    3,
                    23,
                    0
                ],
                "published": "2025-01-23T15:48:41Z",
                "published_parsed": [
                    2025,
                    1,
                    23,
                    15,
                    48,
                    41,
                    3,
                    23,
                    0
                ],
                "title": "Considerations on the Origin of IRAS 19312+1950 Based on Long-Term Maser\n  Observations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Considerations on the Origin of IRAS 19312+1950 Based on Long-Term Maser\n  Observations"
                },
                "summary": "IRAS source 19312+1950 (hereafter I19312) is an infrared point source with\nmaser emissions of SiO, H$_2$O, and OH molecules. Although initial observations\nsuggested that I19312 might be an evolved star, its characteristics are not\nfully consistent with this classification. This study aims to further\ninvestigate the nature of I19312 by conducting long-term monitoring of its\nmaser emissions and comparing the results with other known astrophysical\nobjects. We conducted long-term monitoring of SiO, H$_2$O, and OH maser\nemissions using single-dish radio telescopes. The results were then compared\nwith historical maser data and the characteristics of similar objects to infer\nthe possible origin of I19312. The SiO maser emissions from I19312 were\ndetected over a wide velocity range and exhibited significant time variability.\nThe OH maser lines suggest characteristics of an evolved star, while the H$_2$O\nmaser lines indicate molecular outflows. These features suggest that I19312\ncould be a candidate for a Water Fountain (WF) star, though there are\ninconsistencies, such as the large molecular gas mass, that challenge this\nhypothesis. The possibility of I19312 being a Red Nova Remnant (RNR) is also\nconsidered, but this remains speculative due to the lack of direct evidence.\nThe evolutionary stage of I19312 remains unclear, but it shares multiple\ncharacteristics with both evolved stars with peculiar properties and RNRs.\nFurther long-term monitoring and high-resolution interferometric observations\nare required to better constrain the nature of this object.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "IRAS source 19312+1950 (hereafter I19312) is an infrared point source with\nmaser emissions of SiO, H$_2$O, and OH molecules. Although initial observations\nsuggested that I19312 might be an evolved star, its characteristics are not\nfully consistent with this classification. This study aims to further\ninvestigate the nature of I19312 by conducting long-term monitoring of its\nmaser emissions and comparing the results with other known astrophysical\nobjects. We conducted long-term monitoring of SiO, H$_2$O, and OH maser\nemissions using single-dish radio telescopes. The results were then compared\nwith historical maser data and the characteristics of similar objects to infer\nthe possible origin of I19312. The SiO maser emissions from I19312 were\ndetected over a wide velocity range and exhibited significant time variability.\nThe OH maser lines suggest characteristics of an evolved star, while the H$_2$O\nmaser lines indicate molecular outflows. These features suggest that I19312\ncould be a candidate for a Water Fountain (WF) star, though there are\ninconsistencies, such as the large molecular gas mass, that challenge this\nhypothesis. The possibility of I19312 being a Red Nova Remnant (RNR) is also\nconsidered, but this remains speculative due to the lack of direct evidence.\nThe evolutionary stage of I19312 remains unclear, but it shares multiple\ncharacteristics with both evolved stars with peculiar properties and RNRs.\nFurther long-term monitoring and high-resolution interferometric observations\nare required to better constrain the nature of this object."
                },
                "authors": [
                    {
                        "name": "Huan-Xue Feng"
                    },
                    {
                        "name": "Jun-ichi Nakashima"
                    },
                    {
                        "name": "D. Engels"
                    },
                    {
                        "name": "S. Etoka"
                    },
                    {
                        "name": "Jaeheon Kim"
                    },
                    {
                        "name": "Yong Zhang"
                    },
                    {
                        "name": "Jia-Yong Xie"
                    },
                    {
                        "name": "Jian-Jie Qiu"
                    }
                ],
                "author_detail": {
                    "name": "Jian-Jie Qiu"
                },
                "author": "Jian-Jie Qiu",
                "arxiv_comment": "36 pages, 17 figures, Accepted for publication in ApJ",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.13769v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.13769v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.GA",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.GA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.14804v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.14804v2",
                "updated": "2025-01-23T15:47:09Z",
                "updated_parsed": [
                    2025,
                    1,
                    23,
                    15,
                    47,
                    9,
                    3,
                    23,
                    0
                ],
                "published": "2024-05-23T17:13:50Z",
                "published_parsed": [
                    2024,
                    5,
                    23,
                    17,
                    13,
                    50,
                    3,
                    144,
                    0
                ],
                "title": "Can LLMs Solve longer Math Word Problems Better?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Can LLMs Solve longer Math Word Problems Better?"
                },
                "summary": "Math Word Problems (MWPs) play a vital role in assessing the capabilities of\nLarge Language Models (LLMs), yet current research primarily focuses on\nquestions with concise contexts. The impact of longer contexts on mathematical\nreasoning remains under-explored. This study pioneers the investigation of\nContext Length Generalizability (CoLeG), which refers to the ability of LLMs to\nsolve MWPs with extended narratives. We introduce Extended Grade-School Math\n(E-GSM), a collection of MWPs featuring lengthy narratives, and propose two\nnovel metrics to evaluate the efficacy and resilience of LLMs in tackling these\nproblems. Our analysis of existing zero-shot prompting techniques with\nproprietary LLMs along with open-source LLMs reveals a general deficiency in\nCoLeG. To alleviate these issues, we propose tailored approaches for different\ncategories of LLMs. For proprietary LLMs, we introduce a new instructional\nprompt designed to mitigate the impact of long contexts. For open-source LLMs,\nwe develop a novel auxiliary task for fine-tuning to enhance CoLeG. Our\ncomprehensive results demonstrate the effectiveness of our proposed methods,\nshowing improved performance on E-GSM. Additionally, we conduct an in-depth\nanalysis to differentiate the effects of semantic understanding and reasoning\nefficacy, showing that our methods improves the latter. We also establish the\ngeneralizability of our methods across several other MWP benchmarks. Our\nfindings highlight the limitations of current LLMs and offer practical\nsolutions correspondingly, paving the way for further exploration of model\ngeneralizability and training methodologies.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Math Word Problems (MWPs) play a vital role in assessing the capabilities of\nLarge Language Models (LLMs), yet current research primarily focuses on\nquestions with concise contexts. The impact of longer contexts on mathematical\nreasoning remains under-explored. This study pioneers the investigation of\nContext Length Generalizability (CoLeG), which refers to the ability of LLMs to\nsolve MWPs with extended narratives. We introduce Extended Grade-School Math\n(E-GSM), a collection of MWPs featuring lengthy narratives, and propose two\nnovel metrics to evaluate the efficacy and resilience of LLMs in tackling these\nproblems. Our analysis of existing zero-shot prompting techniques with\nproprietary LLMs along with open-source LLMs reveals a general deficiency in\nCoLeG. To alleviate these issues, we propose tailored approaches for different\ncategories of LLMs. For proprietary LLMs, we introduce a new instructional\nprompt designed to mitigate the impact of long contexts. For open-source LLMs,\nwe develop a novel auxiliary task for fine-tuning to enhance CoLeG. Our\ncomprehensive results demonstrate the effectiveness of our proposed methods,\nshowing improved performance on E-GSM. Additionally, we conduct an in-depth\nanalysis to differentiate the effects of semantic understanding and reasoning\nefficacy, showing that our methods improves the latter. We also establish the\ngeneralizability of our methods across several other MWP benchmarks. Our\nfindings highlight the limitations of current LLMs and offer practical\nsolutions correspondingly, paving the way for further exploration of model\ngeneralizability and training methodologies."
                },
                "authors": [
                    {
                        "name": "Xin Xu"
                    },
                    {
                        "name": "Tong Xiao"
                    },
                    {
                        "name": "Zitong Chao"
                    },
                    {
                        "name": "Zhenya Huang"
                    },
                    {
                        "name": "Can Yang"
                    },
                    {
                        "name": "Yang Wang"
                    }
                ],
                "author_detail": {
                    "name": "Yang Wang"
                },
                "author": "Yang Wang",
                "arxiv_comment": "Accepted to ICLR 2025",
                "arxiv_journal_ref": "International Conference on Learning Representations (ICLR 2025)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.14804v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.14804v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.13767v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.13767v1",
                "updated": "2025-01-23T15:47:04Z",
                "updated_parsed": [
                    2025,
                    1,
                    23,
                    15,
                    47,
                    4,
                    3,
                    23,
                    0
                ],
                "published": "2025-01-23T15:47:04Z",
                "published_parsed": [
                    2025,
                    1,
                    23,
                    15,
                    47,
                    4,
                    3,
                    23,
                    0
                ],
                "title": "An Efficient Diffusion-based Non-Autoregressive Solver for Traveling\n  Salesman Problem",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "An Efficient Diffusion-based Non-Autoregressive Solver for Traveling\n  Salesman Problem"
                },
                "summary": "Recent advances in neural models have shown considerable promise in solving\nTraveling Salesman Problems (TSPs) without relying on much hand-crafted\nengineering. However, while non-autoregressive (NAR) approaches benefit from\nfaster inference through parallelism, they typically deliver solutions of\ninferior quality compared to autoregressive ones. To enhance the solution\nquality while maintaining fast inference, we propose DEITSP, a diffusion model\nwith efficient iterations tailored for TSP that operates in a NAR manner.\nFirstly, we introduce a one-step diffusion model that integrates the controlled\ndiscrete noise addition process with self-consistency enhancement, enabling\noptimal solution prediction through simultaneous denoising of multiple\nsolutions. Secondly, we design a dual-modality graph transformer to bolster the\nextraction and fusion of features from node and edge modalities, while further\naccelerating the inference with fewer layers. Thirdly, we develop an efficient\niterative strategy that alternates between adding and removing noise to improve\nexploration compared to previous diffusion methods. Additionally, we devise a\nscheduling framework to progressively refine the solution space by adjusting\nnoise levels, facilitating a smooth search for optimal solutions. Extensive\nexperiments on real-world and large-scale TSP instances demonstrate that DEITSP\nperforms favorably against existing neural approaches in terms of solution\nquality, inference latency, and generalization ability. Our code is available\nat $\\href{https://github.com/DEITSP/DEITSP}{https://github.com/DEITSP/DEITSP}$.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advances in neural models have shown considerable promise in solving\nTraveling Salesman Problems (TSPs) without relying on much hand-crafted\nengineering. However, while non-autoregressive (NAR) approaches benefit from\nfaster inference through parallelism, they typically deliver solutions of\ninferior quality compared to autoregressive ones. To enhance the solution\nquality while maintaining fast inference, we propose DEITSP, a diffusion model\nwith efficient iterations tailored for TSP that operates in a NAR manner.\nFirstly, we introduce a one-step diffusion model that integrates the controlled\ndiscrete noise addition process with self-consistency enhancement, enabling\noptimal solution prediction through simultaneous denoising of multiple\nsolutions. Secondly, we design a dual-modality graph transformer to bolster the\nextraction and fusion of features from node and edge modalities, while further\naccelerating the inference with fewer layers. Thirdly, we develop an efficient\niterative strategy that alternates between adding and removing noise to improve\nexploration compared to previous diffusion methods. Additionally, we devise a\nscheduling framework to progressively refine the solution space by adjusting\nnoise levels, facilitating a smooth search for optimal solutions. Extensive\nexperiments on real-world and large-scale TSP instances demonstrate that DEITSP\nperforms favorably against existing neural approaches in terms of solution\nquality, inference latency, and generalization ability. Our code is available\nat $\\href{https://github.com/DEITSP/DEITSP}{https://github.com/DEITSP/DEITSP}$."
                },
                "authors": [
                    {
                        "name": "Mingzhao Wang"
                    },
                    {
                        "name": "You Zhou"
                    },
                    {
                        "name": "Zhiguang Cao"
                    },
                    {
                        "name": "Yubin Xiao"
                    },
                    {
                        "name": "Xuan Wu"
                    },
                    {
                        "name": "Wei Pang"
                    },
                    {
                        "name": "Yuan Jiang"
                    },
                    {
                        "name": "Hui Yang"
                    },
                    {
                        "name": "Peng Zhao"
                    },
                    {
                        "name": "Yuanshu Li"
                    }
                ],
                "author_detail": {
                    "name": "Yuanshu Li"
                },
                "author": "Yuanshu Li",
                "arxiv_comment": "Accepted at KDD2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.13767v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.13767v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.13766v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.13766v1",
                "updated": "2025-01-23T15:46:43Z",
                "updated_parsed": [
                    2025,
                    1,
                    23,
                    15,
                    46,
                    43,
                    3,
                    23,
                    0
                ],
                "published": "2025-01-23T15:46:43Z",
                "published_parsed": [
                    2025,
                    1,
                    23,
                    15,
                    46,
                    43,
                    3,
                    23,
                    0
                ],
                "title": "UGMathBench: A Diverse and Dynamic Benchmark for Undergraduate-Level\n  Mathematical Reasoning with Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "UGMathBench: A Diverse and Dynamic Benchmark for Undergraduate-Level\n  Mathematical Reasoning with Large Language Models"
                },
                "summary": "Large Language Models (LLMs) have made significant strides in mathematical\nreasoning, underscoring the need for a comprehensive and fair evaluation of\ntheir capabilities. However, existing benchmarks often fall short, either\nlacking extensive coverage of undergraduate-level mathematical problems or\nprobably suffering from test-set contamination. To address these issues, we\nintroduce UGMathBench, a diverse and dynamic benchmark specifically designed\nfor evaluating undergraduate-level mathematical reasoning with LLMs.\nUGMathBench comprises 5,062 problems across 16 subjects and 111 topics,\nfeaturing 10 distinct answer types. Each problem includes three randomized\nversions, with additional versions planned for release as leading open-source\nLLMs become saturated in UGMathBench. Furthermore, we propose two key metrics:\neffective accuracy (EAcc), which measures the percentage of correctly solved\nproblems across all three versions, and reasoning gap ($\\Delta$), which\nassesses reasoning robustness by calculating the difference between the average\naccuracy across all versions and EAcc. Our extensive evaluation of 23 leading\nLLMs reveals that the highest EAcc achieved is 56.3\\% by OpenAI-o1-mini, with\nlarge $\\Delta$ values observed across different models. This highlights the\nneed for future research aimed at developing \"large reasoning models\" with high\nEAcc and $\\Delta = 0$. We anticipate that the release of UGMathBench, along\nwith its detailed evaluation codes, will serve as a valuable resource to\nadvance the development of LLMs in solving mathematical problems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have made significant strides in mathematical\nreasoning, underscoring the need for a comprehensive and fair evaluation of\ntheir capabilities. However, existing benchmarks often fall short, either\nlacking extensive coverage of undergraduate-level mathematical problems or\nprobably suffering from test-set contamination. To address these issues, we\nintroduce UGMathBench, a diverse and dynamic benchmark specifically designed\nfor evaluating undergraduate-level mathematical reasoning with LLMs.\nUGMathBench comprises 5,062 problems across 16 subjects and 111 topics,\nfeaturing 10 distinct answer types. Each problem includes three randomized\nversions, with additional versions planned for release as leading open-source\nLLMs become saturated in UGMathBench. Furthermore, we propose two key metrics:\neffective accuracy (EAcc), which measures the percentage of correctly solved\nproblems across all three versions, and reasoning gap ($\\Delta$), which\nassesses reasoning robustness by calculating the difference between the average\naccuracy across all versions and EAcc. Our extensive evaluation of 23 leading\nLLMs reveals that the highest EAcc achieved is 56.3\\% by OpenAI-o1-mini, with\nlarge $\\Delta$ values observed across different models. This highlights the\nneed for future research aimed at developing \"large reasoning models\" with high\nEAcc and $\\Delta = 0$. We anticipate that the release of UGMathBench, along\nwith its detailed evaluation codes, will serve as a valuable resource to\nadvance the development of LLMs in solving mathematical problems."
                },
                "authors": [
                    {
                        "name": "Xin Xu"
                    },
                    {
                        "name": "Jiaxin Zhang"
                    },
                    {
                        "name": "Tianhao Chen"
                    },
                    {
                        "name": "Zitong Chao"
                    },
                    {
                        "name": "Jishan Hu"
                    },
                    {
                        "name": "Can Yang"
                    }
                ],
                "author_detail": {
                    "name": "Can Yang"
                },
                "author": "Can Yang",
                "arxiv_comment": "Accepted to ICLR 2025",
                "arxiv_journal_ref": "International Conference on Learning Representations (ICLR 2025)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.13766v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.13766v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.11380v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.11380v2",
                "updated": "2025-01-23T15:44:07Z",
                "updated_parsed": [
                    2025,
                    1,
                    23,
                    15,
                    44,
                    7,
                    3,
                    23,
                    0
                ],
                "published": "2024-06-17T09:56:46Z",
                "published_parsed": [
                    2024,
                    6,
                    17,
                    9,
                    56,
                    46,
                    0,
                    169,
                    0
                ],
                "title": "Evaluating LLMs for Quotation Attribution in Literary Texts: A Case\n  Study of LLaMa3",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Evaluating LLMs for Quotation Attribution in Literary Texts: A Case\n  Study of LLaMa3"
                },
                "summary": "Large Language Models (LLMs) have shown promising results in a variety of\nliterary tasks, often using complex memorized details of narration and\nfictional characters. In this work, we evaluate the ability of Llama-3 at\nattributing utterances of direct-speech to their speaker in novels. The LLM\nshows impressive results on a corpus of 28 novels, surpassing published results\nwith ChatGPT and encoder-based baselines by a large margin. We then validate\nthese results by assessing the impact of book memorization and annotation\ncontamination. We found that these types of memorization do not explain the\nlarge performance gain, making Llama-3 the new state-of-the-art for quotation\nattribution in English literature. We release publicly our code and data.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have shown promising results in a variety of\nliterary tasks, often using complex memorized details of narration and\nfictional characters. In this work, we evaluate the ability of Llama-3 at\nattributing utterances of direct-speech to their speaker in novels. The LLM\nshows impressive results on a corpus of 28 novels, surpassing published results\nwith ChatGPT and encoder-based baselines by a large margin. We then validate\nthese results by assessing the impact of book memorization and annotation\ncontamination. We found that these types of memorization do not explain the\nlarge performance gain, making Llama-3 the new state-of-the-art for quotation\nattribution in English literature. We release publicly our code and data."
                },
                "authors": [
                    {
                        "name": "Gaspard Michel"
                    },
                    {
                        "name": "Elena V. Epure"
                    },
                    {
                        "name": "Romain Hennequin"
                    },
                    {
                        "name": "Christophe Cerisara"
                    }
                ],
                "author_detail": {
                    "name": "Christophe Cerisara"
                },
                "author": "Christophe Cerisara",
                "arxiv_comment": "NAACL 2025 Main Conference -- short paper",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.11380v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.11380v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.13748v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.13748v1",
                "updated": "2025-01-23T15:25:40Z",
                "updated_parsed": [
                    2025,
                    1,
                    23,
                    15,
                    25,
                    40,
                    3,
                    23,
                    0
                ],
                "published": "2025-01-23T15:25:40Z",
                "published_parsed": [
                    2025,
                    1,
                    23,
                    15,
                    25,
                    40,
                    3,
                    23,
                    0
                ],
                "title": "Exact Soft Analytical Side-Channel Attacks using Tractable Circuits",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Exact Soft Analytical Side-Channel Attacks using Tractable Circuits"
                },
                "summary": "Detecting weaknesses in cryptographic algorithms is of utmost importance for\ndesigning secure information systems. The state-of-the-art soft analytical\nside-channel attack (SASCA) uses physical leakage information to make\nprobabilistic predictions about intermediate computations and combines these\n\"guesses\" with the known algorithmic logic to compute the posterior\ndistribution over the key. This attack is commonly performed via loopy belief\npropagation, which, however, lacks guarantees in terms of convergence and\ninference quality. In this paper, we develop a fast and exact inference method\nfor SASCA, denoted as ExSASCA, by leveraging knowledge compilation and\ntractable probabilistic circuits. When attacking the Advanced Encryption\nStandard (AES), the most widely used encryption algorithm to date, ExSASCA\noutperforms SASCA by more than 31% top-1 success rate absolute. By leveraging\nsparse belief messages, this performance is achieved with little more\ncomputational cost than SASCA, and about 3 orders of magnitude less than exact\ninference via exhaustive enumeration. Even with dense belief messages, ExSASCA\nstill uses 6 times less computations than exhaustive inference.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Detecting weaknesses in cryptographic algorithms is of utmost importance for\ndesigning secure information systems. The state-of-the-art soft analytical\nside-channel attack (SASCA) uses physical leakage information to make\nprobabilistic predictions about intermediate computations and combines these\n\"guesses\" with the known algorithmic logic to compute the posterior\ndistribution over the key. This attack is commonly performed via loopy belief\npropagation, which, however, lacks guarantees in terms of convergence and\ninference quality. In this paper, we develop a fast and exact inference method\nfor SASCA, denoted as ExSASCA, by leveraging knowledge compilation and\ntractable probabilistic circuits. When attacking the Advanced Encryption\nStandard (AES), the most widely used encryption algorithm to date, ExSASCA\noutperforms SASCA by more than 31% top-1 success rate absolute. By leveraging\nsparse belief messages, this performance is achieved with little more\ncomputational cost than SASCA, and about 3 orders of magnitude less than exact\ninference via exhaustive enumeration. Even with dense belief messages, ExSASCA\nstill uses 6 times less computations than exhaustive inference."
                },
                "authors": [
                    {
                        "name": "Thomas Wedenig"
                    },
                    {
                        "name": "Rishub Nagpal"
                    },
                    {
                        "name": "Gaëtan Cassiers"
                    },
                    {
                        "name": "Stefan Mangard"
                    },
                    {
                        "name": "Robert Peharz"
                    }
                ],
                "author_detail": {
                    "name": "Robert Peharz"
                },
                "author": "Robert Peharz",
                "arxiv_comment": "ICML 2024 Conference Paper",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.13748v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.13748v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.13746v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.13746v1",
                "updated": "2025-01-23T15:22:25Z",
                "updated_parsed": [
                    2025,
                    1,
                    23,
                    15,
                    22,
                    25,
                    3,
                    23,
                    0
                ],
                "published": "2025-01-23T15:22:25Z",
                "published_parsed": [
                    2025,
                    1,
                    23,
                    15,
                    22,
                    25,
                    3,
                    23,
                    0
                ],
                "title": "EICopilot: Search and Explore Enterprise Information over Large-scale\n  Knowledge Graphs with LLM-driven Agents",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "EICopilot: Search and Explore Enterprise Information over Large-scale\n  Knowledge Graphs with LLM-driven Agents"
                },
                "summary": "The paper introduces EICopilot, an novel agent-based solution enhancing\nsearch and exploration of enterprise registration data within extensive online\nknowledge graphs like those detailing legal entities, registered capital, and\nmajor shareholders. Traditional methods necessitate text-based queries and\nmanual subgraph explorations, often resulting in time-consuming processes.\nEICopilot, deployed as a chatbot via Baidu Enterprise Search, improves this\nlandscape by utilizing Large Language Models (LLMs) to interpret natural\nlanguage queries. This solution automatically generates and executes Gremlin\nscripts, providing efficient summaries of complex enterprise relationships.\nDistinct feature a data pre-processing pipeline that compiles and annotates\nrepresentative queries into a vector database of examples for In-context\nlearning (ICL), a comprehensive reasoning pipeline combining Chain-of-Thought\nwith ICL to enhance Gremlin script generation for knowledge graph search and\nexploration, and a novel query masking strategy that improves intent\nrecognition for heightened script accuracy. Empirical evaluations demonstrate\nthe superior performance of EICopilot, including speed and accuracy, over\nbaseline methods, with the \\emph{Full Mask} variant achieving a syntax error\nrate reduction to as low as 10.00% and an execution correctness of up to\n82.14%. These components collectively contribute to superior querying\ncapabilities and summarization of intricate datasets, positioning EICopilot as\na groundbreaking tool in the exploration and exploitation of large-scale\nknowledge graphs for enterprise information search.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The paper introduces EICopilot, an novel agent-based solution enhancing\nsearch and exploration of enterprise registration data within extensive online\nknowledge graphs like those detailing legal entities, registered capital, and\nmajor shareholders. Traditional methods necessitate text-based queries and\nmanual subgraph explorations, often resulting in time-consuming processes.\nEICopilot, deployed as a chatbot via Baidu Enterprise Search, improves this\nlandscape by utilizing Large Language Models (LLMs) to interpret natural\nlanguage queries. This solution automatically generates and executes Gremlin\nscripts, providing efficient summaries of complex enterprise relationships.\nDistinct feature a data pre-processing pipeline that compiles and annotates\nrepresentative queries into a vector database of examples for In-context\nlearning (ICL), a comprehensive reasoning pipeline combining Chain-of-Thought\nwith ICL to enhance Gremlin script generation for knowledge graph search and\nexploration, and a novel query masking strategy that improves intent\nrecognition for heightened script accuracy. Empirical evaluations demonstrate\nthe superior performance of EICopilot, including speed and accuracy, over\nbaseline methods, with the \\emph{Full Mask} variant achieving a syntax error\nrate reduction to as low as 10.00% and an execution correctness of up to\n82.14%. These components collectively contribute to superior querying\ncapabilities and summarization of intricate datasets, positioning EICopilot as\na groundbreaking tool in the exploration and exploitation of large-scale\nknowledge graphs for enterprise information search."
                },
                "authors": [
                    {
                        "name": "Yuhui Yun"
                    },
                    {
                        "name": "Huilong Ye"
                    },
                    {
                        "name": "Xinru Li"
                    },
                    {
                        "name": "Ruojia Li"
                    },
                    {
                        "name": "Jingfeng Deng"
                    },
                    {
                        "name": "Li Li"
                    },
                    {
                        "name": "Haoyi Xiong"
                    }
                ],
                "author_detail": {
                    "name": "Haoyi Xiong"
                },
                "author": "Haoyi Xiong",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.13746v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.13746v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.13745v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.13745v1",
                "updated": "2025-01-23T15:20:04Z",
                "updated_parsed": [
                    2025,
                    1,
                    23,
                    15,
                    20,
                    4,
                    3,
                    23,
                    0
                ],
                "published": "2025-01-23T15:20:04Z",
                "published_parsed": [
                    2025,
                    1,
                    23,
                    15,
                    20,
                    4,
                    3,
                    23,
                    0
                ],
                "title": "Reconciling Binary Replicates: Beyond the Average",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reconciling Binary Replicates: Beyond the Average"
                },
                "summary": "Binary observations are often repeated to improve data quality, creating\ntechnical replicates. Several scoring methods are commonly used to infer the\nactual individual state and obtain a probability for each state. The common\npractice of averaging replicates has limitations, and alternative methods for\nscoring and classifying individuals are proposed. Additionally, an indecisive\nresponse might be wiser than classifying all individuals based on their\nreplicates in the medical context, where 1 indicates a particular health\ncondition. Building on the inherent limitations of the averaging approach,\nthree alternative methods are examined: the median, maximum penalized\nlikelihood estimation, and a Bayesian algorithm. The theoretical analysis\nsuggests that the proposed alternatives outperform the averaging approach,\nespecially the Bayesian method, which incorporates uncertainty and provides\ncredible intervals. Simulations and real-world medical datasets are used to\ndemonstrate the practical implications of these methods for improving\ndiagnostic accuracy and disease prevalence estimation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Binary observations are often repeated to improve data quality, creating\ntechnical replicates. Several scoring methods are commonly used to infer the\nactual individual state and obtain a probability for each state. The common\npractice of averaging replicates has limitations, and alternative methods for\nscoring and classifying individuals are proposed. Additionally, an indecisive\nresponse might be wiser than classifying all individuals based on their\nreplicates in the medical context, where 1 indicates a particular health\ncondition. Building on the inherent limitations of the averaging approach,\nthree alternative methods are examined: the median, maximum penalized\nlikelihood estimation, and a Bayesian algorithm. The theoretical analysis\nsuggests that the proposed alternatives outperform the averaging approach,\nespecially the Bayesian method, which incorporates uncertainty and provides\ncredible intervals. Simulations and real-world medical datasets are used to\ndemonstrate the practical implications of these methods for improving\ndiagnostic accuracy and disease prevalence estimation."
                },
                "authors": [
                    {
                        "name": "Manuela Royer-Carenzi"
                    },
                    {
                        "name": "Hadrien Lorenzo"
                    },
                    {
                        "name": "Pierre Pudlo"
                    }
                ],
                "author_detail": {
                    "name": "Pierre Pudlo"
                },
                "author": "Pierre Pudlo",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.13745v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.13745v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ME",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.13743v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.13743v1",
                "updated": "2025-01-23T15:18:22Z",
                "updated_parsed": [
                    2025,
                    1,
                    23,
                    15,
                    18,
                    22,
                    3,
                    23,
                    0
                ],
                "published": "2025-01-23T15:18:22Z",
                "published_parsed": [
                    2025,
                    1,
                    23,
                    15,
                    18,
                    22,
                    3,
                    23,
                    0
                ],
                "title": "GPT-HTree: A Decision Tree Framework Integrating Hierarchical Clustering\n  and Large Language Models for Explainable Classification",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GPT-HTree: A Decision Tree Framework Integrating Hierarchical Clustering\n  and Large Language Models for Explainable Classification"
                },
                "summary": "This paper introduces GPT-HTree, a framework combining hierarchical\nclustering, decision trees, and large language models (LLMs) to address this\nchallenge. By leveraging hierarchical clustering to segment individuals based\non salient features, resampling techniques to balance class distributions, and\ndecision trees to tailor classification paths within each cluster, GPT-HTree\nensures both accuracy and interpretability. LLMs enhance the framework by\ngenerating human-readable cluster descriptions, bridging quantitative analysis\nwith actionable insights.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper introduces GPT-HTree, a framework combining hierarchical\nclustering, decision trees, and large language models (LLMs) to address this\nchallenge. By leveraging hierarchical clustering to segment individuals based\non salient features, resampling techniques to balance class distributions, and\ndecision trees to tailor classification paths within each cluster, GPT-HTree\nensures both accuracy and interpretability. LLMs enhance the framework by\ngenerating human-readable cluster descriptions, bridging quantitative analysis\nwith actionable insights."
                },
                "authors": [
                    {
                        "name": "Te Pei"
                    },
                    {
                        "name": "Fuat Alican"
                    },
                    {
                        "name": "Aaron Ontoyin Yin"
                    },
                    {
                        "name": "Yigit Ihlamur"
                    }
                ],
                "author_detail": {
                    "name": "Yigit Ihlamur"
                },
                "author": "Yigit Ihlamur",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.13743v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.13743v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.13735v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.13735v1",
                "updated": "2025-01-23T15:11:27Z",
                "updated_parsed": [
                    2025,
                    1,
                    23,
                    15,
                    11,
                    27,
                    3,
                    23,
                    0
                ],
                "published": "2025-01-23T15:11:27Z",
                "published_parsed": [
                    2025,
                    1,
                    23,
                    15,
                    11,
                    27,
                    3,
                    23,
                    0
                ],
                "title": "A Study of the Plausibility of Attention between RNN Encoders in Natural\n  Language Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Study of the Plausibility of Attention between RNN Encoders in Natural\n  Language Inference"
                },
                "summary": "Attention maps in neural models for NLP are appealing to explain the decision\nmade by a model, hopefully emphasizing words that justify the decision. While\nmany empirical studies hint that attention maps can provide such justification\nfrom the analysis of sound examples, only a few assess the plausibility of\nexplanations based on attention maps, i.e., the usefulness of attention maps\nfor humans to understand the decision. These studies furthermore focus on text\nclassification. In this paper, we report on a preliminary assessment of\nattention maps in a sentence comparison task, namely natural language\ninference. We compare the cross-attention weights between two RNN encoders with\nhuman-based and heuristic-based annotations on the eSNLI corpus. We show that\nthe heuristic reasonably correlates with human annotations and can thus\nfacilitate evaluation of plausible explanations in sentence comparison tasks.\nRaw attention weights however remain only loosely related to a plausible\nexplanation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Attention maps in neural models for NLP are appealing to explain the decision\nmade by a model, hopefully emphasizing words that justify the decision. While\nmany empirical studies hint that attention maps can provide such justification\nfrom the analysis of sound examples, only a few assess the plausibility of\nexplanations based on attention maps, i.e., the usefulness of attention maps\nfor humans to understand the decision. These studies furthermore focus on text\nclassification. In this paper, we report on a preliminary assessment of\nattention maps in a sentence comparison task, namely natural language\ninference. We compare the cross-attention weights between two RNN encoders with\nhuman-based and heuristic-based annotations on the eSNLI corpus. We show that\nthe heuristic reasonably correlates with human annotations and can thus\nfacilitate evaluation of plausible explanations in sentence comparison tasks.\nRaw attention weights however remain only loosely related to a plausible\nexplanation."
                },
                "authors": [
                    {
                        "name": "Duc Hau Nguyen"
                    },
                    {
                        "name": "Duc Hau Nguyen"
                    },
                    {
                        "name": "Pascale Sébillot"
                    }
                ],
                "author_detail": {
                    "name": "Pascale Sébillot"
                },
                "author": "Pascale Sébillot",
                "arxiv_doi": "10.1109/ICMLA52953.2021.00259",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1109/ICMLA52953.2021.00259",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2501.13735v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.13735v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.13731v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.13731v1",
                "updated": "2025-01-23T15:04:22Z",
                "updated_parsed": [
                    2025,
                    1,
                    23,
                    15,
                    4,
                    22,
                    3,
                    23,
                    0
                ],
                "published": "2025-01-23T15:04:22Z",
                "published_parsed": [
                    2025,
                    1,
                    23,
                    15,
                    4,
                    22,
                    3,
                    23,
                    0
                ],
                "title": "Pseudocode-Injection Magic: Enabling LLMs to Tackle Graph Computational\n  Tasks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Pseudocode-Injection Magic: Enabling LLMs to Tackle Graph Computational\n  Tasks"
                },
                "summary": "Graph computational tasks are inherently challenging and often demand the\ndevelopment of advanced algorithms for effective solutions. With the emergence\nof large language models (LLMs), researchers have begun investigating their\npotential to address these tasks. However, existing approaches are constrained\nby LLMs' limited capability to comprehend complex graph structures and their\nhigh inference costs, rendering them impractical for handling large-scale\ngraphs. Inspired by human approaches to graph problems, we introduce a novel\nframework, PIE (Pseudocode-Injection-Enhanced LLM Reasoning for Graph\nComputational Tasks), which consists of three key steps: problem understanding,\nprompt design, and code generation. In this framework, LLMs are tasked with\nunderstanding the problem and extracting relevant information to generate\ncorrect code. The responsibility for analyzing the graph structure and\nexecuting the code is delegated to the interpreter. We inject task-related\npseudocodes into the prompts to further assist the LLMs in generating efficient\ncode. We also employ cost-effective trial-and-error techniques to ensure that\nthe LLM-generated code executes correctly. Unlike other methods that require\ninvoking LLMs for each individual test case, PIE only calls the LLM during the\ncode generation phase, allowing the generated code to be reused and\nsignificantly reducing inference costs. Extensive experiments demonstrate that\nPIE outperforms existing baselines in terms of both accuracy and computational\nefficiency.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Graph computational tasks are inherently challenging and often demand the\ndevelopment of advanced algorithms for effective solutions. With the emergence\nof large language models (LLMs), researchers have begun investigating their\npotential to address these tasks. However, existing approaches are constrained\nby LLMs' limited capability to comprehend complex graph structures and their\nhigh inference costs, rendering them impractical for handling large-scale\ngraphs. Inspired by human approaches to graph problems, we introduce a novel\nframework, PIE (Pseudocode-Injection-Enhanced LLM Reasoning for Graph\nComputational Tasks), which consists of three key steps: problem understanding,\nprompt design, and code generation. In this framework, LLMs are tasked with\nunderstanding the problem and extracting relevant information to generate\ncorrect code. The responsibility for analyzing the graph structure and\nexecuting the code is delegated to the interpreter. We inject task-related\npseudocodes into the prompts to further assist the LLMs in generating efficient\ncode. We also employ cost-effective trial-and-error techniques to ensure that\nthe LLM-generated code executes correctly. Unlike other methods that require\ninvoking LLMs for each individual test case, PIE only calls the LLM during the\ncode generation phase, allowing the generated code to be reused and\nsignificantly reducing inference costs. Extensive experiments demonstrate that\nPIE outperforms existing baselines in terms of both accuracy and computational\nefficiency."
                },
                "authors": [
                    {
                        "name": "Chang Gong"
                    },
                    {
                        "name": "Wanrui Bian"
                    },
                    {
                        "name": "Zhijie Zhang"
                    },
                    {
                        "name": "Weiguo Zheng"
                    }
                ],
                "author_detail": {
                    "name": "Weiguo Zheng"
                },
                "author": "Weiguo Zheng",
                "arxiv_comment": "24 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.13731v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.13731v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.13726v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.13726v1",
                "updated": "2025-01-23T14:58:56Z",
                "updated_parsed": [
                    2025,
                    1,
                    23,
                    14,
                    58,
                    56,
                    3,
                    23,
                    0
                ],
                "published": "2025-01-23T14:58:56Z",
                "published_parsed": [
                    2025,
                    1,
                    23,
                    14,
                    58,
                    56,
                    3,
                    23,
                    0
                ],
                "title": "RPO: Retrieval Preference Optimization for Robust Retrieval-Augmented\n  Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RPO: Retrieval Preference Optimization for Robust Retrieval-Augmented\n  Generation"
                },
                "summary": "While Retrieval-Augmented Generation (RAG) has exhibited promise in utilizing\nexternal knowledge, its generation process heavily depends on the quality and\naccuracy of the retrieved context. Large language models (LLMs) struggle to\nevaluate the correctness of non-parametric knowledge retrieved externally when\nit differs from internal memorization, leading to knowledge conflicts during\nresponse generation. To this end, we introduce the Retrieval Preference\nOptimization (RPO), a lightweight and effective alignment method to adaptively\nleverage multi-source knowledge based on retrieval relevance. An implicit\nrepresentation of retrieval relevance is derived and incorporated into the\nreward model to integrate retrieval evaluation and response generation into a\nsingle model, solving the problem that previous methods necessitate the\nadditional procedure to assess the retrieval quality. Notably, RPO is the only\nRAG-dedicated alignment approach that quantifies the awareness of retrieval\nrelevance in training, overcoming mathematical obstacles. Experiments on four\ndatasets demonstrate that RPO outperforms RAG by 4-10% in accuracy without any\nextra component, exhibiting its robust generalization.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While Retrieval-Augmented Generation (RAG) has exhibited promise in utilizing\nexternal knowledge, its generation process heavily depends on the quality and\naccuracy of the retrieved context. Large language models (LLMs) struggle to\nevaluate the correctness of non-parametric knowledge retrieved externally when\nit differs from internal memorization, leading to knowledge conflicts during\nresponse generation. To this end, we introduce the Retrieval Preference\nOptimization (RPO), a lightweight and effective alignment method to adaptively\nleverage multi-source knowledge based on retrieval relevance. An implicit\nrepresentation of retrieval relevance is derived and incorporated into the\nreward model to integrate retrieval evaluation and response generation into a\nsingle model, solving the problem that previous methods necessitate the\nadditional procedure to assess the retrieval quality. Notably, RPO is the only\nRAG-dedicated alignment approach that quantifies the awareness of retrieval\nrelevance in training, overcoming mathematical obstacles. Experiments on four\ndatasets demonstrate that RPO outperforms RAG by 4-10% in accuracy without any\nextra component, exhibiting its robust generalization."
                },
                "authors": [
                    {
                        "name": "Shi-Qi Yan"
                    },
                    {
                        "name": "Zhen-Hua Ling"
                    }
                ],
                "author_detail": {
                    "name": "Zhen-Hua Ling"
                },
                "author": "Zhen-Hua Ling",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.13726v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.13726v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.13720v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.13720v1",
                "updated": "2025-01-23T14:50:37Z",
                "updated_parsed": [
                    2025,
                    1,
                    23,
                    14,
                    50,
                    37,
                    3,
                    23,
                    0
                ],
                "published": "2025-01-23T14:50:37Z",
                "published_parsed": [
                    2025,
                    1,
                    23,
                    14,
                    50,
                    37,
                    3,
                    23,
                    0
                ],
                "title": "Musical ethnocentrism in Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Musical ethnocentrism in Large Language Models"
                },
                "summary": "Large Language Models (LLMs) reflect the biases in their training data and,\nby extension, those of the people who created this training data. Detecting,\nanalyzing, and mitigating such biases is becoming a focus of research. One type\nof bias that has been understudied so far are geocultural biases. Those can be\ncaused by an imbalance in the representation of different geographic regions\nand cultures in the training data, but also by value judgments contained\ntherein. In this paper, we make a first step towards analyzing musical biases\nin LLMs, particularly ChatGPT and Mixtral. We conduct two experiments. In the\nfirst, we prompt LLMs to provide lists of the \"Top 100\" musical contributors of\nvarious categories and analyze their countries of origin. In the second\nexperiment, we ask the LLMs to numerically rate various aspects of the musical\ncultures of different countries. Our results indicate a strong preference of\nthe LLMs for Western music cultures in both experiments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) reflect the biases in their training data and,\nby extension, those of the people who created this training data. Detecting,\nanalyzing, and mitigating such biases is becoming a focus of research. One type\nof bias that has been understudied so far are geocultural biases. Those can be\ncaused by an imbalance in the representation of different geographic regions\nand cultures in the training data, but also by value judgments contained\ntherein. In this paper, we make a first step towards analyzing musical biases\nin LLMs, particularly ChatGPT and Mixtral. We conduct two experiments. In the\nfirst, we prompt LLMs to provide lists of the \"Top 100\" musical contributors of\nvarious categories and analyze their countries of origin. In the second\nexperiment, we ask the LLMs to numerically rate various aspects of the musical\ncultures of different countries. Our results indicate a strong preference of\nthe LLMs for Western music cultures in both experiments."
                },
                "authors": [
                    {
                        "name": "Anna Kruspe"
                    }
                ],
                "author_detail": {
                    "name": "Anna Kruspe"
                },
                "author": "Anna Kruspe",
                "arxiv_journal_ref": "Proceedings of the 3rd Workshop on NLP for Music and Audio\n  (NLP4MusA) 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.13720v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.13720v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.11683v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.11683v3",
                "updated": "2025-01-23T14:45:03Z",
                "updated_parsed": [
                    2025,
                    1,
                    23,
                    14,
                    45,
                    3,
                    3,
                    23,
                    0
                ],
                "published": "2024-11-18T16:09:26Z",
                "published_parsed": [
                    2024,
                    11,
                    18,
                    16,
                    9,
                    26,
                    0,
                    323,
                    0
                ],
                "title": "TrojanRobot: Physical-World Backdoor Attacks Against VLM-based Robotic\n  Manipulation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TrojanRobot: Physical-World Backdoor Attacks Against VLM-based Robotic\n  Manipulation"
                },
                "summary": "Robotic manipulation in the physical world is increasingly empowered by\n\\textit{large language models} (LLMs) and \\textit{vision-language models}\n(VLMs), leveraging their understanding and perception capabilities. Recently,\nvarious attacks against such robotic policies have been proposed, with backdoor\nattacks drawing considerable attention for their high stealth and strong\npersistence capabilities. However, existing backdoor efforts are limited to\nsimulators and suffer from physical-world realization. To address this, we\npropose \\textit{TrojanRobot}, a highly stealthy and broadly effective robotic\nbackdoor attack in the physical world. Specifically, we introduce a\nmodule-poisoning approach by embedding a backdoor module into the modular\nrobotic policy, enabling backdoor control over the policy's visual perception\nmodule thereby backdooring the entire robotic policy. Our vanilla\nimplementation leverages a backdoor-finetuned VLM to serve as the backdoor\nmodule. To enhance its generalization in physical environments, we propose a\nprime implementation, leveraging the LVLM-as-a-backdoor paradigm and developing\nthree types of prime attacks, \\ie, \\textit{permutation}, \\textit{stagnation},\nand \\textit{intentional} attacks, thus achieving finer-grained backdoors.\nExtensive experiments on the UR3e manipulator with 18 task instructions using\nrobotic policies based on four VLMs demonstrate the broad effectiveness and\nphysical-world stealth of TrojanRobot. Our attack's video demonstrations are\navailable via a github link \\url{https://trojanrobot.github.io}.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Robotic manipulation in the physical world is increasingly empowered by\n\\textit{large language models} (LLMs) and \\textit{vision-language models}\n(VLMs), leveraging their understanding and perception capabilities. Recently,\nvarious attacks against such robotic policies have been proposed, with backdoor\nattacks drawing considerable attention for their high stealth and strong\npersistence capabilities. However, existing backdoor efforts are limited to\nsimulators and suffer from physical-world realization. To address this, we\npropose \\textit{TrojanRobot}, a highly stealthy and broadly effective robotic\nbackdoor attack in the physical world. Specifically, we introduce a\nmodule-poisoning approach by embedding a backdoor module into the modular\nrobotic policy, enabling backdoor control over the policy's visual perception\nmodule thereby backdooring the entire robotic policy. Our vanilla\nimplementation leverages a backdoor-finetuned VLM to serve as the backdoor\nmodule. To enhance its generalization in physical environments, we propose a\nprime implementation, leveraging the LVLM-as-a-backdoor paradigm and developing\nthree types of prime attacks, \\ie, \\textit{permutation}, \\textit{stagnation},\nand \\textit{intentional} attacks, thus achieving finer-grained backdoors.\nExtensive experiments on the UR3e manipulator with 18 task instructions using\nrobotic policies based on four VLMs demonstrate the broad effectiveness and\nphysical-world stealth of TrojanRobot. Our attack's video demonstrations are\navailable via a github link \\url{https://trojanrobot.github.io}."
                },
                "authors": [
                    {
                        "name": "Xianlong Wang"
                    },
                    {
                        "name": "Hewen Pan"
                    },
                    {
                        "name": "Hangtao Zhang"
                    },
                    {
                        "name": "Minghui Li"
                    },
                    {
                        "name": "Shengshan Hu"
                    },
                    {
                        "name": "Ziqi Zhou"
                    },
                    {
                        "name": "Lulu Xue"
                    },
                    {
                        "name": "Peijin Guo"
                    },
                    {
                        "name": "Yichen Wang"
                    },
                    {
                        "name": "Wei Wan"
                    },
                    {
                        "name": "Aishan Liu"
                    },
                    {
                        "name": "Leo Yu Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Leo Yu Zhang"
                },
                "author": "Leo Yu Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.11683v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.11683v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.13711v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.13711v1",
                "updated": "2025-01-23T14:41:26Z",
                "updated_parsed": [
                    2025,
                    1,
                    23,
                    14,
                    41,
                    26,
                    3,
                    23,
                    0
                ],
                "published": "2025-01-23T14:41:26Z",
                "published_parsed": [
                    2025,
                    1,
                    23,
                    14,
                    41,
                    26,
                    3,
                    23,
                    0
                ],
                "title": "Rapid wavefront shaping using an optical gradient acquisition",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Rapid wavefront shaping using an optical gradient acquisition"
                },
                "summary": "Wavefront shaping systems aim to image deep into scattering tissue by\nreshaping incoming and outgoing light to correct aberrations caused by tissue\ninhomogeneity However, the desired modulation depends on the unknown tissue\nstructure and therefore its estimation is a challenging time-consuming task.\nMost strategies rely on coordinate descent optimization, which sequentially\nvaries each modulation parameter and assesses its impact on the resulting\nimage. We propose a rapid wavefront shaping scheme that transitions from\ncoordinate descent to gradient descent optimization, using the same measurement\nto update all modulation parameters simultaneously. To achieve this, we have\ndeveloped an analytical framework that expresses the gradient of the wavefront\nshaping score with respect to all modulation parameters. Although this gradient\ndepends on the unknown tissue structure, we demonstrate how it can be inferred\nfrom the optical system's measurements. Our new framework enables rapid\ninference of wavefront shaping modulations. Additionally, since the complexity\nof our algorithm does not scale with the number of modulation parameters, we\ncan achieve very high-resolution modulations, leading to better corrections in\nthicker tissue layers. We showcase the effectiveness of our framework in\ncorrecting aberrations in a coherent confocal microscope.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Wavefront shaping systems aim to image deep into scattering tissue by\nreshaping incoming and outgoing light to correct aberrations caused by tissue\ninhomogeneity However, the desired modulation depends on the unknown tissue\nstructure and therefore its estimation is a challenging time-consuming task.\nMost strategies rely on coordinate descent optimization, which sequentially\nvaries each modulation parameter and assesses its impact on the resulting\nimage. We propose a rapid wavefront shaping scheme that transitions from\ncoordinate descent to gradient descent optimization, using the same measurement\nto update all modulation parameters simultaneously. To achieve this, we have\ndeveloped an analytical framework that expresses the gradient of the wavefront\nshaping score with respect to all modulation parameters. Although this gradient\ndepends on the unknown tissue structure, we demonstrate how it can be inferred\nfrom the optical system's measurements. Our new framework enables rapid\ninference of wavefront shaping modulations. Additionally, since the complexity\nof our algorithm does not scale with the number of modulation parameters, we\ncan achieve very high-resolution modulations, leading to better corrections in\nthicker tissue layers. We showcase the effectiveness of our framework in\ncorrecting aberrations in a coherent confocal microscope."
                },
                "authors": [
                    {
                        "name": "Sagi Monin"
                    },
                    {
                        "name": "Marina Alterman"
                    },
                    {
                        "name": "Anat Levin"
                    }
                ],
                "author_detail": {
                    "name": "Anat Levin"
                },
                "author": "Anat Levin",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.13711v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.13711v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.optics",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.optics",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.13106v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.13106v2",
                "updated": "2025-01-23T14:41:06Z",
                "updated_parsed": [
                    2025,
                    1,
                    23,
                    14,
                    41,
                    6,
                    3,
                    23,
                    0
                ],
                "published": "2025-01-22T18:59:46Z",
                "published_parsed": [
                    2025,
                    1,
                    22,
                    18,
                    59,
                    46,
                    2,
                    22,
                    0
                ],
                "title": "VideoLLaMA 3: Frontier Multimodal Foundation Models for Image and Video\n  Understanding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "VideoLLaMA 3: Frontier Multimodal Foundation Models for Image and Video\n  Understanding"
                },
                "summary": "In this paper, we propose VideoLLaMA3, a more advanced multimodal foundation\nmodel for image and video understanding. The core design philosophy of\nVideoLLaMA3 is vision-centric. The meaning of \"vision-centric\" is two-fold: the\nvision-centric training paradigm and vision-centric framework design. The key\ninsight of our vision-centric training paradigm is that high-quality image-text\ndata is crucial for both image and video understanding. Instead of preparing\nmassive video-text datasets, we focus on constructing large-scale and\nhigh-quality image-text datasets. VideoLLaMA3 has four training stages: 1)\nVision Encoder Adaptation, which enables vision encoder to accept images of\nvariable resolutions as input; 2) Vision-Language Alignment, which jointly\ntunes the vision encoder, projector, and LLM with large-scale image-text data\ncovering multiple types (including scene images, documents, charts) as well as\ntext-only data. 3) Multi-task Fine-tuning, which incorporates image-text SFT\ndata for downstream tasks and video-text data to establish a foundation for\nvideo understanding. 4) Video-centric Fine-tuning, which further improves the\nmodel's capability in video understanding. As for the framework design, to\nbetter capture fine-grained details in images, the pretrained vision encoder is\nadapted to encode images of varying sizes into vision tokens with corresponding\nnumbers, rather than a fixed number of tokens. For video inputs, we reduce the\nnumber of vision tokens according to their similarity so that the\nrepresentation of videos will be more precise and compact. Benefit from\nvision-centric designs, VideoLLaMA3 achieves compelling performances in both\nimage and video understanding benchmarks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this paper, we propose VideoLLaMA3, a more advanced multimodal foundation\nmodel for image and video understanding. The core design philosophy of\nVideoLLaMA3 is vision-centric. The meaning of \"vision-centric\" is two-fold: the\nvision-centric training paradigm and vision-centric framework design. The key\ninsight of our vision-centric training paradigm is that high-quality image-text\ndata is crucial for both image and video understanding. Instead of preparing\nmassive video-text datasets, we focus on constructing large-scale and\nhigh-quality image-text datasets. VideoLLaMA3 has four training stages: 1)\nVision Encoder Adaptation, which enables vision encoder to accept images of\nvariable resolutions as input; 2) Vision-Language Alignment, which jointly\ntunes the vision encoder, projector, and LLM with large-scale image-text data\ncovering multiple types (including scene images, documents, charts) as well as\ntext-only data. 3) Multi-task Fine-tuning, which incorporates image-text SFT\ndata for downstream tasks and video-text data to establish a foundation for\nvideo understanding. 4) Video-centric Fine-tuning, which further improves the\nmodel's capability in video understanding. As for the framework design, to\nbetter capture fine-grained details in images, the pretrained vision encoder is\nadapted to encode images of varying sizes into vision tokens with corresponding\nnumbers, rather than a fixed number of tokens. For video inputs, we reduce the\nnumber of vision tokens according to their similarity so that the\nrepresentation of videos will be more precise and compact. Benefit from\nvision-centric designs, VideoLLaMA3 achieves compelling performances in both\nimage and video understanding benchmarks."
                },
                "authors": [
                    {
                        "name": "Boqiang Zhang"
                    },
                    {
                        "name": "Kehan Li"
                    },
                    {
                        "name": "Zesen Cheng"
                    },
                    {
                        "name": "Zhiqiang Hu"
                    },
                    {
                        "name": "Yuqian Yuan"
                    },
                    {
                        "name": "Guanzheng Chen"
                    },
                    {
                        "name": "Sicong Leng"
                    },
                    {
                        "name": "Yuming Jiang"
                    },
                    {
                        "name": "Hang Zhang"
                    },
                    {
                        "name": "Xin Li"
                    },
                    {
                        "name": "Peng Jin"
                    },
                    {
                        "name": "Wenqi Zhang"
                    },
                    {
                        "name": "Fan Wang"
                    },
                    {
                        "name": "Lidong Bing"
                    },
                    {
                        "name": "Deli Zhao"
                    }
                ],
                "author_detail": {
                    "name": "Deli Zhao"
                },
                "author": "Deli Zhao",
                "arxiv_comment": "BZ, KL, ZC, ZH, YY, GC, SL, YJ, HZ, and XL contributed equally to\n  this project. Code: https://github.com/DAMO-NLP-SG/VideoLLaMA3",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.13106v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.13106v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.12538v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.12538v2",
                "updated": "2025-01-23T14:38:58Z",
                "updated_parsed": [
                    2025,
                    1,
                    23,
                    14,
                    38,
                    58,
                    3,
                    23,
                    0
                ],
                "published": "2025-01-21T23:05:12Z",
                "published_parsed": [
                    2025,
                    1,
                    21,
                    23,
                    5,
                    12,
                    1,
                    21,
                    0
                ],
                "title": "Academic Case Reports Lack Diversity: Assessing the Presence and\n  Diversity of Sociodemographic and Behavioral Factors related to Post COVID-19\n  Condition",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Academic Case Reports Lack Diversity: Assessing the Presence and\n  Diversity of Sociodemographic and Behavioral Factors related to Post COVID-19\n  Condition"
                },
                "summary": "Understanding the prevalence, disparities, and symptom variations of Post\nCOVID-19 Condition (PCC) for vulnerable populations is crucial to improving\ncare and addressing intersecting inequities. This study aims to develop a\ncomprehensive framework for integrating social determinants of health (SDOH)\ninto PCC research by leveraging NLP techniques to analyze disparities and\nvariations in SDOH representation within PCC case reports. Following\nconstruction of a PCC Case Report Corpus, comprising over 7,000 case reports\nfrom the LitCOVID repository, a subset of 709 reports were annotated with 26\ncore SDOH-related entity types using pre-trained named entity recognition (NER)\nmodels, human review, and data augmentation to improve quality, diversity and\nrepresentation of entity types. An NLP pipeline integrating NER, natural\nlanguage inference (NLI), trigram and frequency analyses was developed to\nextract and analyze these entities. Both encoder-only transformer models and\nRNN-based models were assessed for the NER objective.\n  Fine-tuned encoder-only BERT models outperformed traditional RNN-based models\nin generalizability to distinct sentence structures and greater class sparsity.\nExploratory analysis revealed variability in entity richness, with prevalent\nentities like condition, age, and access to care, and underrepresentation of\nsensitive categories like race and housing status. Trigram analysis highlighted\nfrequent co-occurrences among entities, including age, gender, and condition.\nThe NLI objective (entailment and contradiction analysis) showed attributes\nlike \"Experienced violence or abuse\" and \"Has medical insurance\" had high\nentailment rates (82.4%-80.3%), while attributes such as \"Is\nfemale-identifying,\" \"Is married,\" and \"Has a terminal condition\" exhibited\nhigh contradiction rates (70.8%-98.5%).",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Understanding the prevalence, disparities, and symptom variations of Post\nCOVID-19 Condition (PCC) for vulnerable populations is crucial to improving\ncare and addressing intersecting inequities. This study aims to develop a\ncomprehensive framework for integrating social determinants of health (SDOH)\ninto PCC research by leveraging NLP techniques to analyze disparities and\nvariations in SDOH representation within PCC case reports. Following\nconstruction of a PCC Case Report Corpus, comprising over 7,000 case reports\nfrom the LitCOVID repository, a subset of 709 reports were annotated with 26\ncore SDOH-related entity types using pre-trained named entity recognition (NER)\nmodels, human review, and data augmentation to improve quality, diversity and\nrepresentation of entity types. An NLP pipeline integrating NER, natural\nlanguage inference (NLI), trigram and frequency analyses was developed to\nextract and analyze these entities. Both encoder-only transformer models and\nRNN-based models were assessed for the NER objective.\n  Fine-tuned encoder-only BERT models outperformed traditional RNN-based models\nin generalizability to distinct sentence structures and greater class sparsity.\nExploratory analysis revealed variability in entity richness, with prevalent\nentities like condition, age, and access to care, and underrepresentation of\nsensitive categories like race and housing status. Trigram analysis highlighted\nfrequent co-occurrences among entities, including age, gender, and condition.\nThe NLI objective (entailment and contradiction analysis) showed attributes\nlike \"Experienced violence or abuse\" and \"Has medical insurance\" had high\nentailment rates (82.4%-80.3%), while attributes such as \"Is\nfemale-identifying,\" \"Is married,\" and \"Has a terminal condition\" exhibited\nhigh contradiction rates (70.8%-98.5%)."
                },
                "authors": [
                    {
                        "name": "Juan Andres Medina Florez"
                    },
                    {
                        "name": "Shaina Raza"
                    },
                    {
                        "name": "Rashida Lynn"
                    },
                    {
                        "name": "Zahra Shakeri"
                    },
                    {
                        "name": "Brendan T. Smith"
                    },
                    {
                        "name": "Elham Dolatabadi"
                    }
                ],
                "author_detail": {
                    "name": "Elham Dolatabadi"
                },
                "author": "Elham Dolatabadi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.12538v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.12538v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.05411v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.05411v3",
                "updated": "2025-01-23T14:30:40Z",
                "updated_parsed": [
                    2025,
                    1,
                    23,
                    14,
                    30,
                    40,
                    3,
                    23,
                    0
                ],
                "published": "2024-10-07T18:23:00Z",
                "published_parsed": [
                    2024,
                    10,
                    7,
                    18,
                    23,
                    0,
                    0,
                    281,
                    0
                ],
                "title": "Filtering Discomforting Recommendations with Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Filtering Discomforting Recommendations with Large Language Models"
                },
                "summary": "Personalized algorithms can inadvertently expose users to discomforting\nrecommendations, potentially triggering negative consequences. The subjectivity\nof discomfort and the black-box nature of these algorithms make it challenging\nto effectively identify and filter such content. To address this, we first\nconducted a formative study to understand users' practices and expectations\nregarding discomforting recommendation filtering. Then, we designed a Large\nLanguage Model (LLM)-based tool named DiscomfortFilter, which constructs an\neditable preference profile for a user and helps the user express filtering\nneeds through conversation to mask discomforting preferences within the\nprofile. Based on the edited profile, DiscomfortFilter facilitates the\ndiscomforting recommendations filtering in a plug-and-play manner, maintaining\nflexibility and transparency. The constructed preference profile improves LLM\nreasoning and simplifies user alignment, enabling a 3.8B open-source LLM to\nrival top commercial models in an offline proxy task. A one-week user study\nwith 24 participants demonstrated the effectiveness of DiscomfortFilter, while\nalso highlighting its potential impact on platform recommendation outcomes. We\nconclude by discussing the ongoing challenges, highlighting its relevance to\nbroader research, assessing stakeholder impact, and outlining future research\ndirections.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Personalized algorithms can inadvertently expose users to discomforting\nrecommendations, potentially triggering negative consequences. The subjectivity\nof discomfort and the black-box nature of these algorithms make it challenging\nto effectively identify and filter such content. To address this, we first\nconducted a formative study to understand users' practices and expectations\nregarding discomforting recommendation filtering. Then, we designed a Large\nLanguage Model (LLM)-based tool named DiscomfortFilter, which constructs an\neditable preference profile for a user and helps the user express filtering\nneeds through conversation to mask discomforting preferences within the\nprofile. Based on the edited profile, DiscomfortFilter facilitates the\ndiscomforting recommendations filtering in a plug-and-play manner, maintaining\nflexibility and transparency. The constructed preference profile improves LLM\nreasoning and simplifies user alignment, enabling a 3.8B open-source LLM to\nrival top commercial models in an offline proxy task. A one-week user study\nwith 24 participants demonstrated the effectiveness of DiscomfortFilter, while\nalso highlighting its potential impact on platform recommendation outcomes. We\nconclude by discussing the ongoing challenges, highlighting its relevance to\nbroader research, assessing stakeholder impact, and outlining future research\ndirections."
                },
                "authors": [
                    {
                        "name": "Jiahao Liu"
                    },
                    {
                        "name": "Yiyang Shao"
                    },
                    {
                        "name": "Peng Zhang"
                    },
                    {
                        "name": "Dongsheng Li"
                    },
                    {
                        "name": "Hansu Gu"
                    },
                    {
                        "name": "Chao Chen"
                    },
                    {
                        "name": "Longzhi Du"
                    },
                    {
                        "name": "Tun Lu"
                    },
                    {
                        "name": "Ning Gu"
                    }
                ],
                "author_detail": {
                    "name": "Ning Gu"
                },
                "author": "Ning Gu",
                "arxiv_comment": "Accepted by WWW 2025, 16 pages, full version",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.05411v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.05411v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.13699v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.13699v1",
                "updated": "2025-01-23T14:27:11Z",
                "updated_parsed": [
                    2025,
                    1,
                    23,
                    14,
                    27,
                    11,
                    3,
                    23,
                    0
                ],
                "published": "2025-01-23T14:27:11Z",
                "published_parsed": [
                    2025,
                    1,
                    23,
                    14,
                    27,
                    11,
                    3,
                    23,
                    0
                ],
                "title": "DI-BENCH: Benchmarking Large Language Models on Dependency Inference\n  with Testable Repositories at Scale",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DI-BENCH: Benchmarking Large Language Models on Dependency Inference\n  with Testable Repositories at Scale"
                },
                "summary": "Large Language Models have advanced automated software development, however,\nit remains a challenge to correctly infer dependencies, namely, identifying the\ninternal components and external packages required for a repository to\nsuccessfully run. Existing studies highlight that dependency-related issues\ncause over 40\\% of observed runtime errors on the generated repository. To\naddress this, we introduce DI-BENCH, a large-scale benchmark and evaluation\nframework specifically designed to assess LLMs' capability on dependency\ninference. The benchmark features 581 repositories with testing environments\nacross Python, C#, Rust, and JavaScript. Extensive experiments with textual and\nexecution-based metrics reveal that the current best-performing model achieves\nonly a 42.9% execution pass rate, indicating significant room for improvement.\nDI-BENCH establishes a new viewpoint for evaluating LLM performance on\nrepositories, paving the way for more robust end-to-end software synthesis.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models have advanced automated software development, however,\nit remains a challenge to correctly infer dependencies, namely, identifying the\ninternal components and external packages required for a repository to\nsuccessfully run. Existing studies highlight that dependency-related issues\ncause over 40\\% of observed runtime errors on the generated repository. To\naddress this, we introduce DI-BENCH, a large-scale benchmark and evaluation\nframework specifically designed to assess LLMs' capability on dependency\ninference. The benchmark features 581 repositories with testing environments\nacross Python, C#, Rust, and JavaScript. Extensive experiments with textual and\nexecution-based metrics reveal that the current best-performing model achieves\nonly a 42.9% execution pass rate, indicating significant room for improvement.\nDI-BENCH establishes a new viewpoint for evaluating LLM performance on\nrepositories, paving the way for more robust end-to-end software synthesis."
                },
                "authors": [
                    {
                        "name": "Linghao Zhang"
                    },
                    {
                        "name": "Junhao Wang"
                    },
                    {
                        "name": "Shilin He"
                    },
                    {
                        "name": "Chaoyun Zhang"
                    },
                    {
                        "name": "Yu Kang"
                    },
                    {
                        "name": "Bowen Li"
                    },
                    {
                        "name": "Jiaheng Wen"
                    },
                    {
                        "name": "Chengxing Xie"
                    },
                    {
                        "name": "Maoquan Wang"
                    },
                    {
                        "name": "Yufan Huang"
                    },
                    {
                        "name": "Elsie Nallipogu"
                    },
                    {
                        "name": "Qingwei Lin"
                    },
                    {
                        "name": "Yingnong Dang"
                    },
                    {
                        "name": "Saravan Rajmohan"
                    },
                    {
                        "name": "Dongmei Zhang"
                    },
                    {
                        "name": "Qi Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Qi Zhang"
                },
                "author": "Qi Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.13699v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.13699v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.11223v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.11223v3",
                "updated": "2025-01-23T14:26:08Z",
                "updated_parsed": [
                    2025,
                    1,
                    23,
                    14,
                    26,
                    8,
                    3,
                    23,
                    0
                ],
                "published": "2025-01-20T02:16:19Z",
                "published_parsed": [
                    2025,
                    1,
                    20,
                    2,
                    16,
                    19,
                    0,
                    20,
                    0
                ],
                "title": "Reasoning Language Models: A Blueprint",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reasoning Language Models: A Blueprint"
                },
                "summary": "Reasoning language models (RLMs), also known as Large Reasoning Models\n(LRMs), such as OpenAI's o1 and o3, DeepSeek-V3, and Alibaba's QwQ, have\nredefined AI's problem-solving capabilities by extending LLMs with advanced\nreasoning mechanisms. Yet, their high costs, proprietary nature, and complex\narchitectures - uniquely combining Reinforcement Learning (RL), search\nheuristics, and LLMs - present accessibility and scalability challenges. To\naddress these, we propose a comprehensive blueprint that organizes RLM\ncomponents into a modular framework, based on a survey and analysis of all RLM\nworks. This blueprint incorporates diverse reasoning structures (chains, trees,\ngraphs, and nested forms), reasoning strategies (e.g., Monte Carlo Tree Search,\nBeam Search), RL concepts (policy, value models and others), supervision\nschemes (Outcome-Based and Process-Based Supervision), and other related\nconcepts (e.g., Test-Time Compute, Retrieval-Augmented Generation, agent\ntools). We also provide detailed mathematical formulations and algorithmic\nspecifications to simplify RLM implementation. By showing how schemes like\nLLaMA-Berry, QwQ, Journey Learning, and Graph of Thoughts fit as special cases,\nwe demonstrate the blueprint's versatility and unifying potential. To\nillustrate its utility, we introduce x1, a modular implementation for rapid RLM\nprototyping and experimentation. Using x1 and a literature review, we provide\nkey insights, such as multi-phase training for policy and value models, and the\nimportance of familiar training distributions. Finally, we discuss scalable RLM\ncloud deployments and we outline how RLMs can integrate with a broader LLM\necosystem. Our work demystifies RLM construction, democratizes advanced\nreasoning capabilities, and fosters innovation, aiming to mitigate the gap\nbetween \"rich AI\" and \"poor AI\" by lowering barriers to RLM design and\nexperimentation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reasoning language models (RLMs), also known as Large Reasoning Models\n(LRMs), such as OpenAI's o1 and o3, DeepSeek-V3, and Alibaba's QwQ, have\nredefined AI's problem-solving capabilities by extending LLMs with advanced\nreasoning mechanisms. Yet, their high costs, proprietary nature, and complex\narchitectures - uniquely combining Reinforcement Learning (RL), search\nheuristics, and LLMs - present accessibility and scalability challenges. To\naddress these, we propose a comprehensive blueprint that organizes RLM\ncomponents into a modular framework, based on a survey and analysis of all RLM\nworks. This blueprint incorporates diverse reasoning structures (chains, trees,\ngraphs, and nested forms), reasoning strategies (e.g., Monte Carlo Tree Search,\nBeam Search), RL concepts (policy, value models and others), supervision\nschemes (Outcome-Based and Process-Based Supervision), and other related\nconcepts (e.g., Test-Time Compute, Retrieval-Augmented Generation, agent\ntools). We also provide detailed mathematical formulations and algorithmic\nspecifications to simplify RLM implementation. By showing how schemes like\nLLaMA-Berry, QwQ, Journey Learning, and Graph of Thoughts fit as special cases,\nwe demonstrate the blueprint's versatility and unifying potential. To\nillustrate its utility, we introduce x1, a modular implementation for rapid RLM\nprototyping and experimentation. Using x1 and a literature review, we provide\nkey insights, such as multi-phase training for policy and value models, and the\nimportance of familiar training distributions. Finally, we discuss scalable RLM\ncloud deployments and we outline how RLMs can integrate with a broader LLM\necosystem. Our work demystifies RLM construction, democratizes advanced\nreasoning capabilities, and fosters innovation, aiming to mitigate the gap\nbetween \"rich AI\" and \"poor AI\" by lowering barriers to RLM design and\nexperimentation."
                },
                "authors": [
                    {
                        "name": "Maciej Besta"
                    },
                    {
                        "name": "Julia Barth"
                    },
                    {
                        "name": "Eric Schreiber"
                    },
                    {
                        "name": "Ales Kubicek"
                    },
                    {
                        "name": "Afonso Catarino"
                    },
                    {
                        "name": "Robert Gerstenberger"
                    },
                    {
                        "name": "Piotr Nyczyk"
                    },
                    {
                        "name": "Patrick Iff"
                    },
                    {
                        "name": "Yueling Li"
                    },
                    {
                        "name": "Sam Houliston"
                    },
                    {
                        "name": "Tomasz Sternal"
                    },
                    {
                        "name": "Marcin Copik"
                    },
                    {
                        "name": "Grzegorz Kwaśniewski"
                    },
                    {
                        "name": "Jürgen Müller"
                    },
                    {
                        "name": "Łukasz Flis"
                    },
                    {
                        "name": "Hannes Eberhard"
                    },
                    {
                        "name": "Hubert Niewiadomski"
                    },
                    {
                        "name": "Torsten Hoefler"
                    }
                ],
                "author_detail": {
                    "name": "Torsten Hoefler"
                },
                "author": "Torsten Hoefler",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.11223v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.11223v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.03820v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.03820v3",
                "updated": "2025-01-23T14:17:44Z",
                "updated_parsed": [
                    2025,
                    1,
                    23,
                    14,
                    17,
                    44,
                    3,
                    23,
                    0
                ],
                "published": "2025-01-07T14:35:03Z",
                "published_parsed": [
                    2025,
                    1,
                    7,
                    14,
                    35,
                    3,
                    1,
                    7,
                    0
                ],
                "title": "Reconstructing ecological community dynamics from limited observations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reconstructing ecological community dynamics from limited observations"
                },
                "summary": "Ecosystems tend to fluctuate around stable equilibria in response to internal\ndynamics and environmental factors. Occasionally, they enter an unstable\ntipping region and collapse into an alternative stable state. Our understanding\nof how ecological communities vary over time and respond to perturbations\ndepends on our ability to quantify and predict these dynamics. However, the\nscarcity of long, dense time series data poses a severe bottleneck for\ncharacterising community dynamics using existing methods. We overcome this\nlimitation by combining information across multiple short time series using\nBayesian inference. By decomposing dynamics into deterministic and stochastic\ncomponents using Gaussian process priors, we predict stable and tipping regions\nalong the community landscape and quantify resilience while addressing\nuncertainty. After validation with simulated and real ecological time series,\nwe use the model to question common assumptions underlying classical potential\nanalysis and re-evaluate the stability of previously proposed \"tipping\nelements\" in the human gut microbiota.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Ecosystems tend to fluctuate around stable equilibria in response to internal\ndynamics and environmental factors. Occasionally, they enter an unstable\ntipping region and collapse into an alternative stable state. Our understanding\nof how ecological communities vary over time and respond to perturbations\ndepends on our ability to quantify and predict these dynamics. However, the\nscarcity of long, dense time series data poses a severe bottleneck for\ncharacterising community dynamics using existing methods. We overcome this\nlimitation by combining information across multiple short time series using\nBayesian inference. By decomposing dynamics into deterministic and stochastic\ncomponents using Gaussian process priors, we predict stable and tipping regions\nalong the community landscape and quantify resilience while addressing\nuncertainty. After validation with simulated and real ecological time series,\nwe use the model to question common assumptions underlying classical potential\nanalysis and re-evaluate the stability of previously proposed \"tipping\nelements\" in the human gut microbiota."
                },
                "authors": [
                    {
                        "name": "Chandler Ross"
                    },
                    {
                        "name": "Ville Laitinen"
                    },
                    {
                        "name": "Moein Khalighi"
                    },
                    {
                        "name": "Jarkko Salojärvi"
                    },
                    {
                        "name": "Willem de Vos"
                    },
                    {
                        "name": "Guilhem Sommeria-Klein"
                    },
                    {
                        "name": "Leo Lahti"
                    }
                ],
                "author_detail": {
                    "name": "Leo Lahti"
                },
                "author": "Leo Lahti",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.03820v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.03820v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.AP",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.AP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.03335v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.03335v2",
                "updated": "2025-01-23T14:17:08Z",
                "updated_parsed": [
                    2025,
                    1,
                    23,
                    14,
                    17,
                    8,
                    3,
                    23,
                    0
                ],
                "published": "2025-01-06T19:01:47Z",
                "published_parsed": [
                    2025,
                    1,
                    6,
                    19,
                    1,
                    47,
                    0,
                    6,
                    0
                ],
                "title": "Time-resolved Hubble Space Telescope UV observations of an X-ray\n  quasi-periodic eruption source",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Time-resolved Hubble Space Telescope UV observations of an X-ray\n  quasi-periodic eruption source"
                },
                "summary": "X-ray quasi-periodic eruptions (QPEs) are a novel mode of variability in\nnearby galactic nuclei whose origin remains unknown. Their multi-wavelength\nproperties are poorly constrained, as studies have focused almost entirely on\nthe X-ray band. Here we report on time-resolved, coordinated Hubble Space\nTelescope far ultraviolet and XMM-Newton X-ray observations of the shortest\nperiod X-ray QPE source currently known, eRO-QPE2. We detect a bright UV point\nsource ($L_{\\rm FUV} \\approx {\\rm few} \\times 10^{41}$ erg s$^{-1}$) that does\nnot show statistically significant variability between the X-ray eruption and\nquiescent phases. This emission is unlikely to be powered by a young stellar\npopulation in a nuclear stellar cluster. The X-ray-to-UV spectral energy\ndistribution can be described by a compact accretion disk ($R_{\\rm out} =\n343^{+202}_{-138} \\ R_{\\rm g}$). Such compact disks are incompatible with\ntypical disks in active galactic nuclei, but form naturally following the tidal\ndisruption of a star. Our results rule out models (for eRO-QPE2) invoking i) a\nclassic AGN accretion disk and ii) no accretion disk at all. For orbiter\nmodels, the expected radius derived from the timing properties would naturally\nlead to disk-orbiter interactions for both quasi-spherical and eccentric\ntrajectories. We infer a black hole mass of log$_{10}(M_{\\rm BH}) = 5.9 \\pm\n0.3$ M$_{\\odot}$ and Eddington ratio of 0.13$^{+0.18}_{-0.07}$; in combination\nwith the compact outer radius this is inconsistent with existing disk\ninstability models. After accounting for the quiescent disk emission, we\nconstrain the ratio of X-ray to FUV luminosity of the eruption component to be\n$L_{\\rm X} / L_{\\rm FUV} > 16-85$ (depending on the intrinsic extinction).",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "X-ray quasi-periodic eruptions (QPEs) are a novel mode of variability in\nnearby galactic nuclei whose origin remains unknown. Their multi-wavelength\nproperties are poorly constrained, as studies have focused almost entirely on\nthe X-ray band. Here we report on time-resolved, coordinated Hubble Space\nTelescope far ultraviolet and XMM-Newton X-ray observations of the shortest\nperiod X-ray QPE source currently known, eRO-QPE2. We detect a bright UV point\nsource ($L_{\\rm FUV} \\approx {\\rm few} \\times 10^{41}$ erg s$^{-1}$) that does\nnot show statistically significant variability between the X-ray eruption and\nquiescent phases. This emission is unlikely to be powered by a young stellar\npopulation in a nuclear stellar cluster. The X-ray-to-UV spectral energy\ndistribution can be described by a compact accretion disk ($R_{\\rm out} =\n343^{+202}_{-138} \\ R_{\\rm g}$). Such compact disks are incompatible with\ntypical disks in active galactic nuclei, but form naturally following the tidal\ndisruption of a star. Our results rule out models (for eRO-QPE2) invoking i) a\nclassic AGN accretion disk and ii) no accretion disk at all. For orbiter\nmodels, the expected radius derived from the timing properties would naturally\nlead to disk-orbiter interactions for both quasi-spherical and eccentric\ntrajectories. We infer a black hole mass of log$_{10}(M_{\\rm BH}) = 5.9 \\pm\n0.3$ M$_{\\odot}$ and Eddington ratio of 0.13$^{+0.18}_{-0.07}$; in combination\nwith the compact outer radius this is inconsistent with existing disk\ninstability models. After accounting for the quiescent disk emission, we\nconstrain the ratio of X-ray to FUV luminosity of the eruption component to be\n$L_{\\rm X} / L_{\\rm FUV} > 16-85$ (depending on the intrinsic extinction)."
                },
                "authors": [
                    {
                        "name": "Thomas Wevers"
                    },
                    {
                        "name": "Muryel Guolo"
                    },
                    {
                        "name": "Sean Lockwood"
                    },
                    {
                        "name": "Andrew Mummery"
                    },
                    {
                        "name": "Dheeraj R. Pasham"
                    },
                    {
                        "name": "Riccardo Arcodia"
                    }
                ],
                "author_detail": {
                    "name": "Riccardo Arcodia"
                },
                "author": "Riccardo Arcodia",
                "arxiv_doi": "10.3847/2041-8213/adace9",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.3847/2041-8213/adace9",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2501.03335v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.03335v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "11 pages and 4 figures; accepted for publication in ApJ Letters",
                "arxiv_primary_category": {
                    "term": "astro-ph.HE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.HE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.13687v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.13687v1",
                "updated": "2025-01-23T14:13:56Z",
                "updated_parsed": [
                    2025,
                    1,
                    23,
                    14,
                    13,
                    56,
                    3,
                    23,
                    0
                ],
                "published": "2025-01-23T14:13:56Z",
                "published_parsed": [
                    2025,
                    1,
                    23,
                    14,
                    13,
                    56,
                    3,
                    23,
                    0
                ],
                "title": "Question Answering on Patient Medical Records with Private Fine-Tuned\n  LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Question Answering on Patient Medical Records with Private Fine-Tuned\n  LLMs"
                },
                "summary": "Healthcare systems continuously generate vast amounts of electronic health\nrecords (EHRs), commonly stored in the Fast Healthcare Interoperability\nResources (FHIR) standard. Despite the wealth of information in these records,\ntheir complexity and volume make it difficult for users to retrieve and\ninterpret crucial health insights. Recent advances in Large Language Models\n(LLMs) offer a solution, enabling semantic question answering (QA) over medical\ndata, allowing users to interact with their health records more effectively.\nHowever, ensuring privacy and compliance requires edge and private deployments\nof LLMs.\n  This paper proposes a novel approach to semantic QA over EHRs by first\nidentifying the most relevant FHIR resources for a user query (Task1) and\nsubsequently answering the query based on these resources (Task2). We explore\nthe performance of privately hosted, fine-tuned LLMs, evaluating them against\nbenchmark models such as GPT-4 and GPT-4o. Our results demonstrate that\nfine-tuned LLMs, while 250x smaller in size, outperform GPT-4 family models by\n0.55% in F1 score on Task1 and 42% on Meteor Task in Task2. Additionally, we\nexamine advanced aspects of LLM usage, including sequential fine-tuning, model\nself-evaluation (narcissistic evaluation), and the impact of training data size\non performance. The models and datasets are available here:\nhttps://huggingface.co/genloop",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Healthcare systems continuously generate vast amounts of electronic health\nrecords (EHRs), commonly stored in the Fast Healthcare Interoperability\nResources (FHIR) standard. Despite the wealth of information in these records,\ntheir complexity and volume make it difficult for users to retrieve and\ninterpret crucial health insights. Recent advances in Large Language Models\n(LLMs) offer a solution, enabling semantic question answering (QA) over medical\ndata, allowing users to interact with their health records more effectively.\nHowever, ensuring privacy and compliance requires edge and private deployments\nof LLMs.\n  This paper proposes a novel approach to semantic QA over EHRs by first\nidentifying the most relevant FHIR resources for a user query (Task1) and\nsubsequently answering the query based on these resources (Task2). We explore\nthe performance of privately hosted, fine-tuned LLMs, evaluating them against\nbenchmark models such as GPT-4 and GPT-4o. Our results demonstrate that\nfine-tuned LLMs, while 250x smaller in size, outperform GPT-4 family models by\n0.55% in F1 score on Task1 and 42% on Meteor Task in Task2. Additionally, we\nexamine advanced aspects of LLM usage, including sequential fine-tuning, model\nself-evaluation (narcissistic evaluation), and the impact of training data size\non performance. The models and datasets are available here:\nhttps://huggingface.co/genloop"
                },
                "authors": [
                    {
                        "name": "Sara Kothari"
                    },
                    {
                        "name": "Ayush Gupta"
                    }
                ],
                "author_detail": {
                    "name": "Ayush Gupta"
                },
                "author": "Ayush Gupta",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.13687v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.13687v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.13683v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.13683v1",
                "updated": "2025-01-23T14:10:02Z",
                "updated_parsed": [
                    2025,
                    1,
                    23,
                    14,
                    10,
                    2,
                    3,
                    23,
                    0
                ],
                "published": "2025-01-23T14:10:02Z",
                "published_parsed": [
                    2025,
                    1,
                    23,
                    14,
                    10,
                    2,
                    3,
                    23,
                    0
                ],
                "title": "Unlearning Clients, Features and Samples in Vertical Federated Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Unlearning Clients, Features and Samples in Vertical Federated Learning"
                },
                "summary": "Federated Learning (FL) has emerged as a prominent distributed learning\nparadigm. Within the scope of privacy preservation, information privacy\nregulations such as GDPR entitle users to request the removal (or unlearning)\nof their contribution from a service that is hosting the model. For this\npurpose, a server hosting an ML model must be able to unlearn certain\ninformation in cases such as copyright infringement or security issues that can\nmake the model vulnerable or impact the performance of a service based on that\nmodel. While most unlearning approaches in FL focus on Horizontal FL (HFL),\nwhere clients share the feature space and the global model, Vertical FL (VFL)\nhas received less attention from the research community. VFL involves clients\n(passive parties) sharing the sample space among them while not having access\nto the labels. In this paper, we explore unlearning in VFL from three\nperspectives: unlearning clients, unlearning features, and unlearning samples.\nTo unlearn clients and features we introduce VFU-KD which is based on knowledge\ndistillation (KD) while to unlearn samples, VFU-GA is introduced which is based\non gradient ascent. To provide evidence of approximate unlearning, we utilize\nMembership Inference Attack (MIA) to audit the effectiveness of our unlearning\napproach. Our experiments across six tabular datasets and two image datasets\ndemonstrate that VFU-KD and VFU-GA achieve performance comparable to or better\nthan both retraining from scratch and the benchmark R2S method in many cases,\nwith improvements of $(0-2\\%)$. In the remaining cases, utility scores remain\ncomparable, with a modest utility loss ranging from $1-5\\%$. Unlike existing\nmethods, VFU-KD and VFU-GA require no communication between active and passive\nparties during unlearning. However, they do require the active party to store\nthe previously communicated embeddings.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Federated Learning (FL) has emerged as a prominent distributed learning\nparadigm. Within the scope of privacy preservation, information privacy\nregulations such as GDPR entitle users to request the removal (or unlearning)\nof their contribution from a service that is hosting the model. For this\npurpose, a server hosting an ML model must be able to unlearn certain\ninformation in cases such as copyright infringement or security issues that can\nmake the model vulnerable or impact the performance of a service based on that\nmodel. While most unlearning approaches in FL focus on Horizontal FL (HFL),\nwhere clients share the feature space and the global model, Vertical FL (VFL)\nhas received less attention from the research community. VFL involves clients\n(passive parties) sharing the sample space among them while not having access\nto the labels. In this paper, we explore unlearning in VFL from three\nperspectives: unlearning clients, unlearning features, and unlearning samples.\nTo unlearn clients and features we introduce VFU-KD which is based on knowledge\ndistillation (KD) while to unlearn samples, VFU-GA is introduced which is based\non gradient ascent. To provide evidence of approximate unlearning, we utilize\nMembership Inference Attack (MIA) to audit the effectiveness of our unlearning\napproach. Our experiments across six tabular datasets and two image datasets\ndemonstrate that VFU-KD and VFU-GA achieve performance comparable to or better\nthan both retraining from scratch and the benchmark R2S method in many cases,\nwith improvements of $(0-2\\%)$. In the remaining cases, utility scores remain\ncomparable, with a modest utility loss ranging from $1-5\\%$. Unlike existing\nmethods, VFU-KD and VFU-GA require no communication between active and passive\nparties during unlearning. However, they do require the active party to store\nthe previously communicated embeddings."
                },
                "authors": [
                    {
                        "name": "Ayush K. Varshney"
                    },
                    {
                        "name": "Konstantinos Vandikas"
                    },
                    {
                        "name": "Vicenç Torra"
                    }
                ],
                "author_detail": {
                    "name": "Vicenç Torra"
                },
                "author": "Vicenç Torra",
                "arxiv_comment": "Paper accepted for publication in PETS 2025, Issue II",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.13683v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.13683v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.13677v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.13677v1",
                "updated": "2025-01-23T14:02:51Z",
                "updated_parsed": [
                    2025,
                    1,
                    23,
                    14,
                    2,
                    51,
                    3,
                    23,
                    0
                ],
                "published": "2025-01-23T14:02:51Z",
                "published_parsed": [
                    2025,
                    1,
                    23,
                    14,
                    2,
                    51,
                    3,
                    23,
                    0
                ],
                "title": "HumorReject: Decoupling LLM Safety from Refusal Prefix via A Little\n  Humor",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "HumorReject: Decoupling LLM Safety from Refusal Prefix via A Little\n  Humor"
                },
                "summary": "Large Language Models (LLMs) commonly rely on explicit refusal prefixes for\nsafety, making them vulnerable to prefix injection attacks. We introduce\nHumorReject, a novel data-driven approach that fundamentally reimagines LLM\nsafety by decoupling it from refusal prefixes through the use of humor as an\nindirect refusal strategy. Rather than explicitly rejecting harmful\ninstructions, HumorReject responds with contextually appropriate humor that\nnaturally defuses potentially dangerous requests while maintaining engaging\ninteractions. Our approach effectively addresses the common \"over-defense\"\nissues in existing safety mechanisms, demonstrating superior robustness against\nvarious attack vectors while preserving natural and high-quality interactions\non legitimate tasks. Our findings suggest that innovations at the data level\nare even more fundamental than the alignment algorithm itself in achieving\neffective LLM safety, opening new directions for developing more resilient and\nuser-friendly AI systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) commonly rely on explicit refusal prefixes for\nsafety, making them vulnerable to prefix injection attacks. We introduce\nHumorReject, a novel data-driven approach that fundamentally reimagines LLM\nsafety by decoupling it from refusal prefixes through the use of humor as an\nindirect refusal strategy. Rather than explicitly rejecting harmful\ninstructions, HumorReject responds with contextually appropriate humor that\nnaturally defuses potentially dangerous requests while maintaining engaging\ninteractions. Our approach effectively addresses the common \"over-defense\"\nissues in existing safety mechanisms, demonstrating superior robustness against\nvarious attack vectors while preserving natural and high-quality interactions\non legitimate tasks. Our findings suggest that innovations at the data level\nare even more fundamental than the alignment algorithm itself in achieving\neffective LLM safety, opening new directions for developing more resilient and\nuser-friendly AI systems."
                },
                "authors": [
                    {
                        "name": "Zihui Wu"
                    },
                    {
                        "name": "Haichang Gao"
                    },
                    {
                        "name": "Jiacheng Luo"
                    },
                    {
                        "name": "Zhaoxiang Liu"
                    }
                ],
                "author_detail": {
                    "name": "Zhaoxiang Liu"
                },
                "author": "Zhaoxiang Liu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.13677v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.13677v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.13669v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.13669v1",
                "updated": "2025-01-23T13:54:53Z",
                "updated_parsed": [
                    2025,
                    1,
                    23,
                    13,
                    54,
                    53,
                    3,
                    23,
                    0
                ],
                "published": "2025-01-23T13:54:53Z",
                "published_parsed": [
                    2025,
                    1,
                    23,
                    13,
                    54,
                    53,
                    3,
                    23,
                    0
                ],
                "title": "How to Complete Domain Tuning while Keeping General Ability in LLM:\n  Adaptive Layer-wise and Element-wise Regularization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "How to Complete Domain Tuning while Keeping General Ability in LLM:\n  Adaptive Layer-wise and Element-wise Regularization"
                },
                "summary": "Large Language Models (LLMs) exhibit strong general-purpose language\ncapabilities. However, fine-tuning these models on domain-specific tasks often\nleads to catastrophic forgetting, where the model overwrites or loses essential\nknowledge acquired during pretraining. This phenomenon significantly limits the\nbroader applicability of LLMs. To address this challenge, we propose a novel\napproach to compute the element-wise importance of model parameters crucial for\npreserving general knowledge during fine-tuning. Our method utilizes a\ndual-objective optimization strategy: (1) regularization loss to retain the\nparameter crucial for general knowledge; (2) cross-entropy loss to adapt to\ndomain-specific tasks. Additionally, we introduce layer-wise coefficients to\naccount for the varying contributions of different layers, dynamically\nbalancing the dual-objective optimization. Extensive experiments on scientific,\nmedical, and physical tasks using GPT-J and LLaMA-3 demonstrate that our\napproach mitigates catastrophic forgetting while enhancing model adaptability.\nCompared to previous methods, our solution is approximately 20 times faster and\nrequires only 10%-15% of the storage, highlighting the practical efficiency.\nThe code will be released.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) exhibit strong general-purpose language\ncapabilities. However, fine-tuning these models on domain-specific tasks often\nleads to catastrophic forgetting, where the model overwrites or loses essential\nknowledge acquired during pretraining. This phenomenon significantly limits the\nbroader applicability of LLMs. To address this challenge, we propose a novel\napproach to compute the element-wise importance of model parameters crucial for\npreserving general knowledge during fine-tuning. Our method utilizes a\ndual-objective optimization strategy: (1) regularization loss to retain the\nparameter crucial for general knowledge; (2) cross-entropy loss to adapt to\ndomain-specific tasks. Additionally, we introduce layer-wise coefficients to\naccount for the varying contributions of different layers, dynamically\nbalancing the dual-objective optimization. Extensive experiments on scientific,\nmedical, and physical tasks using GPT-J and LLaMA-3 demonstrate that our\napproach mitigates catastrophic forgetting while enhancing model adaptability.\nCompared to previous methods, our solution is approximately 20 times faster and\nrequires only 10%-15% of the storage, highlighting the practical efficiency.\nThe code will be released."
                },
                "authors": [
                    {
                        "name": "Shezheng Song"
                    },
                    {
                        "name": "Hao Xu"
                    },
                    {
                        "name": "Jun Ma"
                    },
                    {
                        "name": "Shasha Li"
                    },
                    {
                        "name": "Long Peng"
                    },
                    {
                        "name": "Qian Wan"
                    },
                    {
                        "name": "Xiaodong Liu"
                    },
                    {
                        "name": "Jie Yu"
                    }
                ],
                "author_detail": {
                    "name": "Jie Yu"
                },
                "author": "Jie Yu",
                "arxiv_comment": "Work in progress",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.13669v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.13669v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.13392v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.13392v2",
                "updated": "2025-01-23T13:54:36Z",
                "updated_parsed": [
                    2025,
                    1,
                    23,
                    13,
                    54,
                    36,
                    3,
                    23,
                    0
                ],
                "published": "2024-10-17T09:42:30Z",
                "published_parsed": [
                    2024,
                    10,
                    17,
                    9,
                    42,
                    30,
                    3,
                    291,
                    0
                ],
                "title": "Judgment of Learning: A Human Ability Beyond Generative Artificial\n  Intelligence",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Judgment of Learning: A Human Ability Beyond Generative Artificial\n  Intelligence"
                },
                "summary": "Large language models (LLMs) increasingly mimic human cognition in various\nlanguage-based tasks. However, their capacity for metacognition - particularly\nin predicting memory performance - remains unexplored. Here, we introduce a\ncross-agent prediction model to assess whether ChatGPT-based LLMs align with\nhuman judgments of learning (JOL), a metacognitive measure where individuals\npredict their own future memory performance. We tested humans and LLMs on pairs\nof sentences, one of which was a garden-path sentence - a sentence that\ninitially misleads the reader toward an incorrect interpretation before\nrequiring reanalysis. By manipulating contextual fit (fitting vs. unfitting\nsentences), we probed how intrinsic cues (i.e., relatedness) affect both LLM\nand human JOL. Our results revealed that while human JOL reliably predicted\nactual memory performance, none of the tested LLMs (GPT-3.5-turbo, GPT-4-turbo,\nand GPT-4o) demonstrated comparable predictive accuracy. This discrepancy\nemerged regardless of whether sentences appeared in fitting or unfitting\ncontexts. These findings indicate that, despite LLMs' demonstrated capacity to\nmodel human cognition at the object-level, they struggle at the meta-level,\nfailing to capture the variability in individual memory predictions. By\nidentifying this shortcoming, our study underscores the need for further\nrefinements in LLMs' self-monitoring abilities, which could enhance their\nutility in educational settings, personalized learning, and human-AI\ninteractions. Strengthening LLMs' metacognitive performance may reduce the\nreliance on human oversight, paving the way for more autonomous and seamless\nintegration of AI into tasks requiring deeper cognitive awareness.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) increasingly mimic human cognition in various\nlanguage-based tasks. However, their capacity for metacognition - particularly\nin predicting memory performance - remains unexplored. Here, we introduce a\ncross-agent prediction model to assess whether ChatGPT-based LLMs align with\nhuman judgments of learning (JOL), a metacognitive measure where individuals\npredict their own future memory performance. We tested humans and LLMs on pairs\nof sentences, one of which was a garden-path sentence - a sentence that\ninitially misleads the reader toward an incorrect interpretation before\nrequiring reanalysis. By manipulating contextual fit (fitting vs. unfitting\nsentences), we probed how intrinsic cues (i.e., relatedness) affect both LLM\nand human JOL. Our results revealed that while human JOL reliably predicted\nactual memory performance, none of the tested LLMs (GPT-3.5-turbo, GPT-4-turbo,\nand GPT-4o) demonstrated comparable predictive accuracy. This discrepancy\nemerged regardless of whether sentences appeared in fitting or unfitting\ncontexts. These findings indicate that, despite LLMs' demonstrated capacity to\nmodel human cognition at the object-level, they struggle at the meta-level,\nfailing to capture the variability in individual memory predictions. By\nidentifying this shortcoming, our study underscores the need for further\nrefinements in LLMs' self-monitoring abilities, which could enhance their\nutility in educational settings, personalized learning, and human-AI\ninteractions. Strengthening LLMs' metacognitive performance may reduce the\nreliance on human oversight, paving the way for more autonomous and seamless\nintegration of AI into tasks requiring deeper cognitive awareness."
                },
                "authors": [
                    {
                        "name": "Markus Huff"
                    },
                    {
                        "name": "Elanur Ulakçı"
                    }
                ],
                "author_detail": {
                    "name": "Elanur Ulakçı"
                },
                "author": "Elanur Ulakçı",
                "arxiv_comment": "31 pages, 2 figures. Note: The appendix is similar to\n  arXiv:2403.05152 because the same stimulus material was used",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.13392v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.13392v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.13652v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.13652v1",
                "updated": "2025-01-23T13:31:51Z",
                "updated_parsed": [
                    2025,
                    1,
                    23,
                    13,
                    31,
                    51,
                    3,
                    23,
                    0
                ],
                "published": "2025-01-23T13:31:51Z",
                "published_parsed": [
                    2025,
                    1,
                    23,
                    13,
                    31,
                    51,
                    3,
                    23,
                    0
                ],
                "title": "LVPruning: An Effective yet Simple Language-Guided Vision Token Pruning\n  Approach for Multi-modal Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LVPruning: An Effective yet Simple Language-Guided Vision Token Pruning\n  Approach for Multi-modal Large Language Models"
                },
                "summary": "Multi-modal Large Language Models (MLLMs) have achieved remarkable success by\nintegrating visual and textual modalities. However, they incur significant\ncomputational overhead due to the large number of vision tokens processed,\nlimiting their practicality in resource-constrained environments. We introduce\nLanguage-Guided Vision Token Pruning (LVPruning) for MLLMs, an effective yet\nsimple method that significantly reduces the computational burden while\npreserving model performance. LVPruning employs cross-attention modules to\ncompute the importance of vision tokens based on their interaction with\nlanguage tokens, determining which to prune. Importantly, LVPruning can be\nintegrated without modifying the original MLLM parameters, which makes\nLVPruning simple to apply or remove. Our experiments show that LVPruning can\neffectively reduce up to 90% of vision tokens by the middle layer of LLaVA-1.5,\nresulting in a 62.1% decrease in inference Tera Floating-Point Operations Per\nSecond (TFLOPs), with an average performance loss of just 0.45% across nine\nmulti-modal benchmarks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-modal Large Language Models (MLLMs) have achieved remarkable success by\nintegrating visual and textual modalities. However, they incur significant\ncomputational overhead due to the large number of vision tokens processed,\nlimiting their practicality in resource-constrained environments. We introduce\nLanguage-Guided Vision Token Pruning (LVPruning) for MLLMs, an effective yet\nsimple method that significantly reduces the computational burden while\npreserving model performance. LVPruning employs cross-attention modules to\ncompute the importance of vision tokens based on their interaction with\nlanguage tokens, determining which to prune. Importantly, LVPruning can be\nintegrated without modifying the original MLLM parameters, which makes\nLVPruning simple to apply or remove. Our experiments show that LVPruning can\neffectively reduce up to 90% of vision tokens by the middle layer of LLaVA-1.5,\nresulting in a 62.1% decrease in inference Tera Floating-Point Operations Per\nSecond (TFLOPs), with an average performance loss of just 0.45% across nine\nmulti-modal benchmarks."
                },
                "authors": [
                    {
                        "name": "Yizheng Sun"
                    },
                    {
                        "name": "Yanze Xin"
                    },
                    {
                        "name": "Hao Li"
                    },
                    {
                        "name": "Jingyuan Sun"
                    },
                    {
                        "name": "Chenghua Lin"
                    },
                    {
                        "name": "Riza Batista-Navarro"
                    }
                ],
                "author_detail": {
                    "name": "Riza Batista-Navarro"
                },
                "author": "Riza Batista-Navarro",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.13652v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.13652v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.13648v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.13648v1",
                "updated": "2025-01-23T13:27:14Z",
                "updated_parsed": [
                    2025,
                    1,
                    23,
                    13,
                    27,
                    14,
                    3,
                    23,
                    0
                ],
                "published": "2025-01-23T13:27:14Z",
                "published_parsed": [
                    2025,
                    1,
                    23,
                    13,
                    27,
                    14,
                    3,
                    23,
                    0
                ],
                "title": "Revisiting Online Learning Approach to Inverse Linear Optimization: A\n  Fenchel--Young Loss Perspective and Gap-Dependent Regret Analysis",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Revisiting Online Learning Approach to Inverse Linear Optimization: A\n  Fenchel--Young Loss Perspective and Gap-Dependent Regret Analysis"
                },
                "summary": "This paper revisits the online learning approach to inverse linear\noptimization studied by B\\\"armann et al. (2017), where the goal is to infer an\nunknown linear objective function of an agent from sequential observations of\nthe agent's input-output pairs. First, we provide a simple understanding of the\nonline learning approach through its connection to online convex optimization\nof \\emph{Fenchel--Young losses}. As a byproduct, we present an offline\nguarantee on the \\emph{suboptimality loss}, which measures how well predicted\nobjectives explain the agent's choices, without assuming the optimality of the\nagent's choices. Second, assuming that there is a gap between optimal and\nsuboptimal objective values in the agent's decision problems, we obtain an\nupper bound independent of the time horizon $T$ on the sum of suboptimality and\n\\emph{estimate losses}, where the latter measures the quality of solutions\nrecommended by predicted objectives. Interestingly, our gap-dependent analysis\nachieves a faster rate than the standard $O(\\sqrt{T})$ regret bound by\nexploiting structures specific to inverse linear optimization, even though\nneither the loss functions nor their domains enjoy desirable properties, such\nas strong convexity.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper revisits the online learning approach to inverse linear\noptimization studied by B\\\"armann et al. (2017), where the goal is to infer an\nunknown linear objective function of an agent from sequential observations of\nthe agent's input-output pairs. First, we provide a simple understanding of the\nonline learning approach through its connection to online convex optimization\nof \\emph{Fenchel--Young losses}. As a byproduct, we present an offline\nguarantee on the \\emph{suboptimality loss}, which measures how well predicted\nobjectives explain the agent's choices, without assuming the optimality of the\nagent's choices. Second, assuming that there is a gap between optimal and\nsuboptimal objective values in the agent's decision problems, we obtain an\nupper bound independent of the time horizon $T$ on the sum of suboptimality and\n\\emph{estimate losses}, where the latter measures the quality of solutions\nrecommended by predicted objectives. Interestingly, our gap-dependent analysis\nachieves a faster rate than the standard $O(\\sqrt{T})$ regret bound by\nexploiting structures specific to inverse linear optimization, even though\nneither the loss functions nor their domains enjoy desirable properties, such\nas strong convexity."
                },
                "authors": [
                    {
                        "name": "Shinsaku Sakaue"
                    },
                    {
                        "name": "Han Bao"
                    },
                    {
                        "name": "Taira Tsuchiya"
                    }
                ],
                "author_detail": {
                    "name": "Taira Tsuchiya"
                },
                "author": "Taira Tsuchiya",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.13648v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.13648v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.00708v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.00708v3",
                "updated": "2025-01-23T13:17:55Z",
                "updated_parsed": [
                    2025,
                    1,
                    23,
                    13,
                    17,
                    55,
                    3,
                    23,
                    0
                ],
                "published": "2024-11-01T16:18:33Z",
                "published_parsed": [
                    2024,
                    11,
                    1,
                    16,
                    18,
                    33,
                    4,
                    306,
                    0
                ],
                "title": "Simplifying and Characterizing DAGs and Phylogenetic Networks via Least\n  Common Ancestor Constraints",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Simplifying and Characterizing DAGs and Phylogenetic Networks via Least\n  Common Ancestor Constraints"
                },
                "summary": "Rooted phylogenetic networks, or more generally, directed acyclic graphs\n(DAGs), are widely used to model species or gene relationships that traditional\nrooted trees cannot fully capture, especially in the presence of reticulate\nprocesses or horizontal gene transfers. Such networks or DAGs are typically\ninferred from observable data (e.g. genomic sequences of extant species),\nproviding only an estimate of the true evolutionary history. However, these\ninferred DAGs are often complex and difficult to interpret. In particular, many\ncontain vertices that do not serve as least common ancestors (LCAs) for any\nsubset of the underlying genes or species, thus may lack direct support from\nthe observable data. In contrast, LCA vertices are witnessed by historical\ntraces justifying their existence and thus represent ancestral states\nsubstantiated by the data. To reduce unnecessary complexity and eliminate\nunsupported vertices, we aim to simplify a DAG to retain only LCA vertices\nwhile preserving essential evolutionary information.\n  In this paper, we characterize $\\mathrm{LCA}$-relevant and\n$\\mathrm{lca}$-relevant DAGs, defined as those in which every vertex serves as\nan LCA (or unique LCA) for some subset of taxa. We introduce methods to\nidentify LCAs in DAGs and efficiently transform any DAG into an\n$\\mathrm{LCA}$-relevant or $\\mathrm{lca}$-relevant one while preserving key\nstructural properties of the original DAG or network. This transformation is\nachieved using a simple operator ``$\\ominus$'' that mimics vertex suppression.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Rooted phylogenetic networks, or more generally, directed acyclic graphs\n(DAGs), are widely used to model species or gene relationships that traditional\nrooted trees cannot fully capture, especially in the presence of reticulate\nprocesses or horizontal gene transfers. Such networks or DAGs are typically\ninferred from observable data (e.g. genomic sequences of extant species),\nproviding only an estimate of the true evolutionary history. However, these\ninferred DAGs are often complex and difficult to interpret. In particular, many\ncontain vertices that do not serve as least common ancestors (LCAs) for any\nsubset of the underlying genes or species, thus may lack direct support from\nthe observable data. In contrast, LCA vertices are witnessed by historical\ntraces justifying their existence and thus represent ancestral states\nsubstantiated by the data. To reduce unnecessary complexity and eliminate\nunsupported vertices, we aim to simplify a DAG to retain only LCA vertices\nwhile preserving essential evolutionary information.\n  In this paper, we characterize $\\mathrm{LCA}$-relevant and\n$\\mathrm{lca}$-relevant DAGs, defined as those in which every vertex serves as\nan LCA (or unique LCA) for some subset of taxa. We introduce methods to\nidentify LCAs in DAGs and efficiently transform any DAG into an\n$\\mathrm{LCA}$-relevant or $\\mathrm{lca}$-relevant one while preserving key\nstructural properties of the original DAG or network. This transformation is\nachieved using a simple operator ``$\\ominus$'' that mimics vertex suppression."
                },
                "authors": [
                    {
                        "name": "Anna Lindeberg"
                    },
                    {
                        "name": "Marc Hellmuth"
                    }
                ],
                "author_detail": {
                    "name": "Marc Hellmuth"
                },
                "author": "Marc Hellmuth",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.00708v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.00708v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "q-bio.PE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "q-bio.PE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.CO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.04686v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.04686v2",
                "updated": "2025-01-23T13:16:39Z",
                "updated_parsed": [
                    2025,
                    1,
                    23,
                    13,
                    16,
                    39,
                    3,
                    23,
                    0
                ],
                "published": "2025-01-08T18:49:41Z",
                "published_parsed": [
                    2025,
                    1,
                    8,
                    18,
                    49,
                    41,
                    2,
                    8,
                    0
                ],
                "title": "URSA: Understanding and Verifying Chain-of-thought Reasoning in\n  Multimodal Mathematics",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "URSA: Understanding and Verifying Chain-of-thought Reasoning in\n  Multimodal Mathematics"
                },
                "summary": "Chain-of-Thought (CoT) reasoning is widely used to enhance the mathematical\nreasoning capabilities of large language models (LLMs). The introduction of\nprocess supervision for CoT trajectories has sparked discussions on improving\ntest-time scaling, thereby unlocking the System 2-style thinking capabilities\nof these models. However, in multimodal mathematical reasoning, the scarcity of\nhigh-quality CoT training data has hindered existing models from achieving both\ndeliberate reasoning and fine-grained verification. In this work, we propose a\nnovel framework that introduces System 2-style thinking to multimodal\nmathematical reasoning. We introduce a three-module CoT data synthesis process\nthat integrates CoT distillation, trajectory-format rewriting, and format\nunification. This process generates MMathCoT-1M, a high-quality CoT reasoning\ninstruction fine-tuning dataset. Furthermore, we implement a dual-view\ntrajectory labeling automation that targets both visual grounding fidelity and\ndeductive chain validity, resulting in the DualMath-1.1M dataset. The URSA-8B\nmodel, trained on MMathCoT-1M, achieves new state-of-the-art (SOTA) performance\namong similarly sized multimodal LLMs on six popular reasoning benchmarks.\nTraining URSA-8B further on the DualMath-1.1M dataset yields URSA-RM-8B, a\nverifier that enhances URSA-8B's test-time performance and surpasses strong\nclosed-source multimodal MLLMs like GPT-4o. The model weights, training data,\nand code have been open-sourced: https://github.com/URSA-MATH/URSA-MATH.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Chain-of-Thought (CoT) reasoning is widely used to enhance the mathematical\nreasoning capabilities of large language models (LLMs). The introduction of\nprocess supervision for CoT trajectories has sparked discussions on improving\ntest-time scaling, thereby unlocking the System 2-style thinking capabilities\nof these models. However, in multimodal mathematical reasoning, the scarcity of\nhigh-quality CoT training data has hindered existing models from achieving both\ndeliberate reasoning and fine-grained verification. In this work, we propose a\nnovel framework that introduces System 2-style thinking to multimodal\nmathematical reasoning. We introduce a three-module CoT data synthesis process\nthat integrates CoT distillation, trajectory-format rewriting, and format\nunification. This process generates MMathCoT-1M, a high-quality CoT reasoning\ninstruction fine-tuning dataset. Furthermore, we implement a dual-view\ntrajectory labeling automation that targets both visual grounding fidelity and\ndeductive chain validity, resulting in the DualMath-1.1M dataset. The URSA-8B\nmodel, trained on MMathCoT-1M, achieves new state-of-the-art (SOTA) performance\namong similarly sized multimodal LLMs on six popular reasoning benchmarks.\nTraining URSA-8B further on the DualMath-1.1M dataset yields URSA-RM-8B, a\nverifier that enhances URSA-8B's test-time performance and surpasses strong\nclosed-source multimodal MLLMs like GPT-4o. The model weights, training data,\nand code have been open-sourced: https://github.com/URSA-MATH/URSA-MATH."
                },
                "authors": [
                    {
                        "name": "Ruilin Luo"
                    },
                    {
                        "name": "Zhuofan Zheng"
                    },
                    {
                        "name": "Yifan Wang"
                    },
                    {
                        "name": "Yiyao Yu"
                    },
                    {
                        "name": "Xinzhe Ni"
                    },
                    {
                        "name": "Zicheng Lin"
                    },
                    {
                        "name": "Jin Zeng"
                    },
                    {
                        "name": "Yujiu Yang"
                    }
                ],
                "author_detail": {
                    "name": "Yujiu Yang"
                },
                "author": "Yujiu Yang",
                "arxiv_comment": "Add benchmarks and baselines. 27 pages, 11 tables, 17 figures.\n  Models, training data and code have been open-sourced. Project url:\n  https://ursa-math.github.io",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.04686v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.04686v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.13629v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.13629v1",
                "updated": "2025-01-23T12:58:14Z",
                "updated_parsed": [
                    2025,
                    1,
                    23,
                    12,
                    58,
                    14,
                    3,
                    23,
                    0
                ],
                "published": "2025-01-23T12:58:14Z",
                "published_parsed": [
                    2025,
                    1,
                    23,
                    12,
                    58,
                    14,
                    3,
                    23,
                    0
                ],
                "title": "Sigma: Differential Rescaling of Query, Key and Value for Efficient\n  Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Sigma: Differential Rescaling of Query, Key and Value for Efficient\n  Language Models"
                },
                "summary": "We introduce Sigma, an efficient large language model specialized for the\nsystem domain, empowered by a novel architecture including DiffQKV attention,\nand pre-trained on our meticulously collected system domain data. DiffQKV\nattention significantly enhances the inference efficiency of Sigma by\noptimizing the Query (Q), Key (K), and Value (V) components in the attention\nmechanism differentially, based on their varying impacts on the model\nperformance and efficiency indicators. Specifically, we (1) conduct extensive\nexperiments that demonstrate the model's varying sensitivity to the compression\nof K and V components, leading to the development of differentially compressed\nKV, and (2) propose augmented Q to expand the Q head dimension, which enhances\nthe model's representation capacity with minimal impacts on the inference\nspeed. Rigorous theoretical and empirical analyses reveal that DiffQKV\nattention significantly enhances efficiency, achieving up to a 33.36%\nimprovement in inference speed over the conventional grouped-query attention\n(GQA) in long-context scenarios. We pre-train Sigma on 6T tokens from various\nsources, including 19.5B system domain data that we carefully collect and 1T\ntokens of synthesized and rewritten data. In general domains, Sigma achieves\ncomparable performance to other state-of-arts models. In the system domain, we\nintroduce the first comprehensive benchmark AIMicius, where Sigma demonstrates\nremarkable performance across all tasks, significantly outperforming GPT-4 with\nan absolute improvement up to 52.5%.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce Sigma, an efficient large language model specialized for the\nsystem domain, empowered by a novel architecture including DiffQKV attention,\nand pre-trained on our meticulously collected system domain data. DiffQKV\nattention significantly enhances the inference efficiency of Sigma by\noptimizing the Query (Q), Key (K), and Value (V) components in the attention\nmechanism differentially, based on their varying impacts on the model\nperformance and efficiency indicators. Specifically, we (1) conduct extensive\nexperiments that demonstrate the model's varying sensitivity to the compression\nof K and V components, leading to the development of differentially compressed\nKV, and (2) propose augmented Q to expand the Q head dimension, which enhances\nthe model's representation capacity with minimal impacts on the inference\nspeed. Rigorous theoretical and empirical analyses reveal that DiffQKV\nattention significantly enhances efficiency, achieving up to a 33.36%\nimprovement in inference speed over the conventional grouped-query attention\n(GQA) in long-context scenarios. We pre-train Sigma on 6T tokens from various\nsources, including 19.5B system domain data that we carefully collect and 1T\ntokens of synthesized and rewritten data. In general domains, Sigma achieves\ncomparable performance to other state-of-arts models. In the system domain, we\nintroduce the first comprehensive benchmark AIMicius, where Sigma demonstrates\nremarkable performance across all tasks, significantly outperforming GPT-4 with\nan absolute improvement up to 52.5%."
                },
                "authors": [
                    {
                        "name": "Zhenghao Lin"
                    },
                    {
                        "name": "Zihao Tang"
                    },
                    {
                        "name": "Xiao Liu"
                    },
                    {
                        "name": "Yeyun Gong"
                    },
                    {
                        "name": "Yi Cheng"
                    },
                    {
                        "name": "Qi Chen"
                    },
                    {
                        "name": "Hang Li"
                    },
                    {
                        "name": "Ying Xin"
                    },
                    {
                        "name": "Ziyue Yang"
                    },
                    {
                        "name": "Kailai Yang"
                    },
                    {
                        "name": "Yu Yan"
                    },
                    {
                        "name": "Xiao Liang"
                    },
                    {
                        "name": "Shuai Lu"
                    },
                    {
                        "name": "Yiming Huang"
                    },
                    {
                        "name": "Zheheng Luo"
                    },
                    {
                        "name": "Lei Qu"
                    },
                    {
                        "name": "Xuan Feng"
                    },
                    {
                        "name": "Yaoxiang Wang"
                    },
                    {
                        "name": "Yuqing Xia"
                    },
                    {
                        "name": "Feiyang Chen"
                    },
                    {
                        "name": "Yuting Jiang"
                    },
                    {
                        "name": "Yasen Hu"
                    },
                    {
                        "name": "Hao Ni"
                    },
                    {
                        "name": "Binyang Li"
                    },
                    {
                        "name": "Guoshuai Zhao"
                    },
                    {
                        "name": "Jui-Hao Chiang"
                    },
                    {
                        "name": "Zhongxin Guo"
                    },
                    {
                        "name": "Chen Lin"
                    },
                    {
                        "name": "Kun Kuang"
                    },
                    {
                        "name": "Wenjie Li"
                    },
                    {
                        "name": "Yelong Shen"
                    },
                    {
                        "name": "Jian Jiao"
                    },
                    {
                        "name": "Peng Cheng"
                    },
                    {
                        "name": "Mao Yang"
                    }
                ],
                "author_detail": {
                    "name": "Mao Yang"
                },
                "author": "Mao Yang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.13629v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.13629v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.09893v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.09893v2",
                "updated": "2025-01-23T12:55:33Z",
                "updated_parsed": [
                    2025,
                    1,
                    23,
                    12,
                    55,
                    33,
                    3,
                    23,
                    0
                ],
                "published": "2024-12-13T06:26:28Z",
                "published_parsed": [
                    2024,
                    12,
                    13,
                    6,
                    26,
                    28,
                    4,
                    348,
                    0
                ],
                "title": "Mass-transferring binary stars as progenitors of interacting\n  hydrogen-free supernovae",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mass-transferring binary stars as progenitors of interacting\n  hydrogen-free supernovae"
                },
                "summary": "Stripped-envelope supernovae (SNe) are H-poor transients produced at the end\nof the life of massive stars that previously lost their H-rich envelope. Their\nprogenitors are thought to be donor stars in mass-transferring binary systems,\nwhich were stripped of their H-rich envelopes some $10^6$yr before core\ncollapse. A subset of the stripped-envelope SNe exhibit spectral and\nphotometric features indicative of interaction between their ejecta and nearby\ncircumstellar material (CSM). We examine whether mass transfer during, or\nshortly before, core collapse in massive binary systems can produce the CSM\ninferred from the observations of interacting H-poor SNe. We select 44 models\nfrom a comprehensive grid of detailed binary evolution models in which the mass\ndonors are H-free and explode while transferring mass to a main-sequence\ncompanion. We find that in these models, mass transfer starts less than\n$\\sim20$kyr before, and often continues until the core collapse of the donor\nstar. Up to $0.8M_\\odot$ of H-free material are removed from the donor star\nduring this phase, which may produce a He-rich circumbinary material. We\nexplore plausible assumptions for its spatial distribution at the time of\nexplosion. When assuming that the CSM accumulates in a circumbinary disk, we\nfind qualitative agreement with the supernova and CSM properties inferred from\nobserved Type Ibn SNe, and to a lesser extent with constraints from Type Icn\nSNe. We find that our mass transferring stripped envelope SN progenitor models\nmay produce up to $\\sim$10% of all stripped envelope supernovae. The binary\nchannel proposed in this work can qualitatively account for the observed key\nproperties and rate of interacting H-poor SNe. Models for the evolution of the\ncircumbinary material and the spectral evolution of exploding progenitors from\nthis channel are needed to further test its significance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Stripped-envelope supernovae (SNe) are H-poor transients produced at the end\nof the life of massive stars that previously lost their H-rich envelope. Their\nprogenitors are thought to be donor stars in mass-transferring binary systems,\nwhich were stripped of their H-rich envelopes some $10^6$yr before core\ncollapse. A subset of the stripped-envelope SNe exhibit spectral and\nphotometric features indicative of interaction between their ejecta and nearby\ncircumstellar material (CSM). We examine whether mass transfer during, or\nshortly before, core collapse in massive binary systems can produce the CSM\ninferred from the observations of interacting H-poor SNe. We select 44 models\nfrom a comprehensive grid of detailed binary evolution models in which the mass\ndonors are H-free and explode while transferring mass to a main-sequence\ncompanion. We find that in these models, mass transfer starts less than\n$\\sim20$kyr before, and often continues until the core collapse of the donor\nstar. Up to $0.8M_\\odot$ of H-free material are removed from the donor star\nduring this phase, which may produce a He-rich circumbinary material. We\nexplore plausible assumptions for its spatial distribution at the time of\nexplosion. When assuming that the CSM accumulates in a circumbinary disk, we\nfind qualitative agreement with the supernova and CSM properties inferred from\nobserved Type Ibn SNe, and to a lesser extent with constraints from Type Icn\nSNe. We find that our mass transferring stripped envelope SN progenitor models\nmay produce up to $\\sim$10% of all stripped envelope supernovae. The binary\nchannel proposed in this work can qualitatively account for the observed key\nproperties and rate of interacting H-poor SNe. Models for the evolution of the\ncircumbinary material and the spectral evolution of exploding progenitors from\nthis channel are needed to further test its significance."
                },
                "authors": [
                    {
                        "name": "Andrea Ercolino"
                    },
                    {
                        "name": "Harim Jin"
                    },
                    {
                        "name": "Norbert Langer"
                    },
                    {
                        "name": "Luc Dessart"
                    }
                ],
                "author_detail": {
                    "name": "Luc Dessart"
                },
                "author": "Luc Dessart",
                "arxiv_comment": "26 pages, 21 figures, Abstract is abridged. Submitted to Astronomy &\n  Astrophysics and updated following comments from the referee. Comments are\n  welcome!",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.09893v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.09893v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.SR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.SR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "astro-ph.HE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.13625v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.13625v1",
                "updated": "2025-01-23T12:45:33Z",
                "updated_parsed": [
                    2025,
                    1,
                    23,
                    12,
                    45,
                    33,
                    3,
                    23,
                    0
                ],
                "published": "2025-01-23T12:45:33Z",
                "published_parsed": [
                    2025,
                    1,
                    23,
                    12,
                    45,
                    33,
                    3,
                    23,
                    0
                ],
                "title": "Information-theoretic limits and approximate message-passing for\n  high-dimensional time series",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Information-theoretic limits and approximate message-passing for\n  high-dimensional time series"
                },
                "summary": "High-dimensional time series appear in many scientific setups, demanding a\nnuanced approach to model and analyze the underlying dependence structure.\nHowever, theoretical advancements so far often rely on stringent assumptions\nregarding the sparsity of the underlying signals. In this contribution, we\nexpand the scope by investigating a high-dimensional time series model wherein\nthe number of features grows proportionally to the number of sampling points,\nwithout assuming sparsity in the signal. Specifically, we consider the\nstochastic regression model and derive a single-letter formula for the\nnormalized mutual information between observations and the signal. We also\nempirically study the vector approximate message passing (VAMP) algorithm and\nshow that, despite a lack of theoretical guarantees, its performance for\ninference in our time series model is robust and often statistically optimal.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "High-dimensional time series appear in many scientific setups, demanding a\nnuanced approach to model and analyze the underlying dependence structure.\nHowever, theoretical advancements so far often rely on stringent assumptions\nregarding the sparsity of the underlying signals. In this contribution, we\nexpand the scope by investigating a high-dimensional time series model wherein\nthe number of features grows proportionally to the number of sampling points,\nwithout assuming sparsity in the signal. Specifically, we consider the\nstochastic regression model and derive a single-letter formula for the\nnormalized mutual information between observations and the signal. We also\nempirically study the vector approximate message passing (VAMP) algorithm and\nshow that, despite a lack of theoretical guarantees, its performance for\ninference in our time series model is robust and often statistically optimal."
                },
                "authors": [
                    {
                        "name": "Daria Tieplova"
                    },
                    {
                        "name": "Samriddha Lahiry"
                    },
                    {
                        "name": "Jean Barbier"
                    }
                ],
                "author_detail": {
                    "name": "Jean Barbier"
                },
                "author": "Jean Barbier",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.13625v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.13625v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IT",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cond-mat.dis-nn",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.ST",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.TH",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "82Bxx, 62Mxx",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.11551v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.11551v2",
                "updated": "2025-01-23T12:42:49Z",
                "updated_parsed": [
                    2025,
                    1,
                    23,
                    12,
                    42,
                    49,
                    3,
                    23,
                    0
                ],
                "published": "2025-01-20T15:39:39Z",
                "published_parsed": [
                    2025,
                    1,
                    20,
                    15,
                    39,
                    39,
                    0,
                    20,
                    0
                ],
                "title": "PIKE-RAG: sPecIalized KnowledgE and Rationale Augmented Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PIKE-RAG: sPecIalized KnowledgE and Rationale Augmented Generation"
                },
                "summary": "Despite notable advancements in Retrieval-Augmented Generation (RAG) systems\nthat expand large language model (LLM) capabilities through external retrieval,\nthese systems often struggle to meet the complex and diverse needs of\nreal-world industrial applications. The reliance on retrieval alone proves\ninsufficient for extracting deep, domain-specific knowledge performing in\nlogical reasoning from specialized corpora. To address this, we introduce\nsPecIalized KnowledgE and Rationale Augmentation Generation (PIKE-RAG),\nfocusing on extracting, understanding, and applying specialized knowledge,\nwhile constructing coherent rationale to incrementally steer LLMs toward\naccurate responses. Recognizing the diverse challenges of industrial tasks, we\nintroduce a new paradigm that classifies tasks based on their complexity in\nknowledge extraction and application, allowing for a systematic evaluation of\nRAG systems' problem-solving capabilities. This strategic approach offers a\nroadmap for the phased development and enhancement of RAG systems, tailored to\nmeet the evolving demands of industrial applications. Furthermore, we propose\nknowledge atomizing and knowledge-aware task decomposition to effectively\nextract multifaceted knowledge from the data chunks and iteratively construct\nthe rationale based on original query and the accumulated knowledge,\nrespectively, showcasing exceptional performance across various benchmarks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Despite notable advancements in Retrieval-Augmented Generation (RAG) systems\nthat expand large language model (LLM) capabilities through external retrieval,\nthese systems often struggle to meet the complex and diverse needs of\nreal-world industrial applications. The reliance on retrieval alone proves\ninsufficient for extracting deep, domain-specific knowledge performing in\nlogical reasoning from specialized corpora. To address this, we introduce\nsPecIalized KnowledgE and Rationale Augmentation Generation (PIKE-RAG),\nfocusing on extracting, understanding, and applying specialized knowledge,\nwhile constructing coherent rationale to incrementally steer LLMs toward\naccurate responses. Recognizing the diverse challenges of industrial tasks, we\nintroduce a new paradigm that classifies tasks based on their complexity in\nknowledge extraction and application, allowing for a systematic evaluation of\nRAG systems' problem-solving capabilities. This strategic approach offers a\nroadmap for the phased development and enhancement of RAG systems, tailored to\nmeet the evolving demands of industrial applications. Furthermore, we propose\nknowledge atomizing and knowledge-aware task decomposition to effectively\nextract multifaceted knowledge from the data chunks and iteratively construct\nthe rationale based on original query and the accumulated knowledge,\nrespectively, showcasing exceptional performance across various benchmarks."
                },
                "authors": [
                    {
                        "name": "Jinyu Wang"
                    },
                    {
                        "name": "Jingjing Fu"
                    },
                    {
                        "name": "Rui Wang"
                    },
                    {
                        "name": "Lei Song"
                    },
                    {
                        "name": "Jiang Bian"
                    }
                ],
                "author_detail": {
                    "name": "Jiang Bian"
                },
                "author": "Jiang Bian",
                "arxiv_comment": "36 pages, 18 figures, technique report",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.11551v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.11551v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.13610v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.13610v1",
                "updated": "2025-01-23T12:30:04Z",
                "updated_parsed": [
                    2025,
                    1,
                    23,
                    12,
                    30,
                    4,
                    3,
                    23,
                    0
                ],
                "published": "2025-01-23T12:30:04Z",
                "published_parsed": [
                    2025,
                    1,
                    23,
                    12,
                    30,
                    4,
                    3,
                    23,
                    0
                ],
                "title": "Efficient Synaptic Delay Implementation in Digital Event-Driven AI\n  Accelerators",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient Synaptic Delay Implementation in Digital Event-Driven AI\n  Accelerators"
                },
                "summary": "Synaptic delay parameterization of neural network models have remained\nlargely unexplored but recent literature has been showing promising results,\nsuggesting the delay parameterized models are simpler, smaller, sparser, and\nthus more energy efficient than similar performing (e.g. task accuracy)\nnon-delay parameterized ones. We introduce Shared Circular Delay Queue (SCDQ),\na novel hardware structure for supporting synaptic delays on digital\nneuromorphic accelerators. Our analysis and hardware results show that it\nscales better in terms of memory, than current commonly used approaches, and is\nmore amortizable to algorithm-hardware co-optimizations, where in fact, memory\nscaling is modulated by model sparsity and not merely network size. Next to\nmemory we also report performance on latency area and energy per inference.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Synaptic delay parameterization of neural network models have remained\nlargely unexplored but recent literature has been showing promising results,\nsuggesting the delay parameterized models are simpler, smaller, sparser, and\nthus more energy efficient than similar performing (e.g. task accuracy)\nnon-delay parameterized ones. We introduce Shared Circular Delay Queue (SCDQ),\na novel hardware structure for supporting synaptic delays on digital\nneuromorphic accelerators. Our analysis and hardware results show that it\nscales better in terms of memory, than current commonly used approaches, and is\nmore amortizable to algorithm-hardware co-optimizations, where in fact, memory\nscaling is modulated by model sparsity and not merely network size. Next to\nmemory we also report performance on latency area and energy per inference."
                },
                "authors": [
                    {
                        "name": "Roy Meijer"
                    },
                    {
                        "name": "Paul Detterer"
                    },
                    {
                        "name": "Amirreza Yousefzadeh"
                    },
                    {
                        "name": "Alberto Patino-Saucedo"
                    },
                    {
                        "name": "Guanghzi Tang"
                    },
                    {
                        "name": "Kanishkan Vadivel"
                    },
                    {
                        "name": "Yinfu Xu"
                    },
                    {
                        "name": "Manil-Dev Gomony"
                    },
                    {
                        "name": "Federico Corradi"
                    },
                    {
                        "name": "Bernabe Linares-Barranco"
                    },
                    {
                        "name": "Manolis Sifalakis"
                    }
                ],
                "author_detail": {
                    "name": "Manolis Sifalakis"
                },
                "author": "Manolis Sifalakis",
                "arxiv_comment": "arXiv admin note: substantial text overlap with arXiv:2404.10597",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.13610v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.13610v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.12339v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.12339v2",
                "updated": "2025-01-23T12:15:42Z",
                "updated_parsed": [
                    2025,
                    1,
                    23,
                    12,
                    15,
                    42,
                    3,
                    23,
                    0
                ],
                "published": "2025-01-21T18:13:43Z",
                "published_parsed": [
                    2025,
                    1,
                    21,
                    18,
                    13,
                    43,
                    1,
                    21,
                    0
                ],
                "title": "Treefix: Enabling Execution with a Tree of Prefixes",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Treefix: Enabling Execution with a Tree of Prefixes"
                },
                "summary": "The ability to execute code is a prerequisite for various dynamic program\nanalyses. Learning-guided execution has been proposed as an approach to enable\nthe execution of arbitrary code snippets by letting a neural model predict\nlikely values for any missing variables. Although state-of-the-art\nlearning-guided execution approaches, such as LExecutor, can enable the\nexecution of a relative high amount of code, they are limited to predicting a\nrestricted set of possible values and do not use any feedback from previous\nexecutions to execute even more code. This paper presents Treefix, a novel\nlearning-guided execution approach that leverages LLMs to iteratively create\ncode prefixes that enable the execution of a given code snippet. The approach\naddresses the problem in a multi-step fashion, where each step uses feedback\nabout the code snippet and its execution to instruct an LLM to improve a\npreviously generated prefix. This process iteratively creates a tree of\nprefixes, a subset of which is returned to the user as prefixes that maximize\nthe number of executed lines in the code snippet. In our experiments with two\ndatasets of Python code snippets, Treefix achieves 25% and 7% more coverage\nrelative to the current state of the art in learning-guided execution, covering\na total of 84% and 82% of all lines in the code snippets.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The ability to execute code is a prerequisite for various dynamic program\nanalyses. Learning-guided execution has been proposed as an approach to enable\nthe execution of arbitrary code snippets by letting a neural model predict\nlikely values for any missing variables. Although state-of-the-art\nlearning-guided execution approaches, such as LExecutor, can enable the\nexecution of a relative high amount of code, they are limited to predicting a\nrestricted set of possible values and do not use any feedback from previous\nexecutions to execute even more code. This paper presents Treefix, a novel\nlearning-guided execution approach that leverages LLMs to iteratively create\ncode prefixes that enable the execution of a given code snippet. The approach\naddresses the problem in a multi-step fashion, where each step uses feedback\nabout the code snippet and its execution to instruct an LLM to improve a\npreviously generated prefix. This process iteratively creates a tree of\nprefixes, a subset of which is returned to the user as prefixes that maximize\nthe number of executed lines in the code snippet. In our experiments with two\ndatasets of Python code snippets, Treefix achieves 25% and 7% more coverage\nrelative to the current state of the art in learning-guided execution, covering\na total of 84% and 82% of all lines in the code snippets."
                },
                "authors": [
                    {
                        "name": "Beatriz Souza"
                    },
                    {
                        "name": "Michael Pradel"
                    }
                ],
                "author_detail": {
                    "name": "Michael Pradel"
                },
                "author": "Michael Pradel",
                "arxiv_comment": "Accepted in research track of the IEEE/ACM International Conference\n  on Software Engineering (ICSE) 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.12339v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.12339v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.13604v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.13604v1",
                "updated": "2025-01-23T12:12:59Z",
                "updated_parsed": [
                    2025,
                    1,
                    23,
                    12,
                    12,
                    59,
                    3,
                    23,
                    0
                ],
                "published": "2025-01-23T12:12:59Z",
                "published_parsed": [
                    2025,
                    1,
                    23,
                    12,
                    12,
                    59,
                    3,
                    23,
                    0
                ],
                "title": "FedPref: Federated Learning Across Heterogeneous Multi-objective\n  Preferences",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FedPref: Federated Learning Across Heterogeneous Multi-objective\n  Preferences"
                },
                "summary": "Federated Learning (FL) is a distributed machine learning strategy, developed\nfor settings where training data is owned by distributed devices and cannot be\nshared. FL circumvents this constraint by carrying out model training in\ndistribution. The parameters of these local models are shared intermittently\namong participants and aggregated to enhance model accuracy. This strategy has\nbeen rapidly adopted by the industry in efforts to overcome privacy and\nresource constraints in model training. However, the application of FL to\nreal-world settings brings additional challenges associated with heterogeneity\nbetween participants. Research into mitigating these difficulties in FL has\nlargely focused on only two types of heterogeneity: the unbalanced distribution\nof training data, and differences in client resources. Yet more types of\nheterogeneity are becoming relevant as the capability of FL expands to cover\nmore complex problems, from the tuning of LLMs to enabling machine learning on\nedge devices. In this work, we discuss a novel type of heterogeneity that is\nlikely to become increasingly relevant in future applications: this is\npreference heterogeneity, emerging when clients learn under multiple\nobjectives, with different importance assigned to each objective on different\nclients. In this work, we discuss the implications of this type of\nheterogeneity and propose FedPref, a first algorithm designed to facilitate\npersonalised FL in this setting. We demonstrate the effectiveness of the\nalgorithm across different problems, preference distributions and model\narchitectures. In addition, we introduce a new analytical point of view, based\non multi-objective metrics, for evaluating the performance of FL algorithms in\nthis setting beyond the traditional client-focused metrics. We perform a second\nexperimental analysis based in this view, and show that FedPref outperforms\ncompared algorithms.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Federated Learning (FL) is a distributed machine learning strategy, developed\nfor settings where training data is owned by distributed devices and cannot be\nshared. FL circumvents this constraint by carrying out model training in\ndistribution. The parameters of these local models are shared intermittently\namong participants and aggregated to enhance model accuracy. This strategy has\nbeen rapidly adopted by the industry in efforts to overcome privacy and\nresource constraints in model training. However, the application of FL to\nreal-world settings brings additional challenges associated with heterogeneity\nbetween participants. Research into mitigating these difficulties in FL has\nlargely focused on only two types of heterogeneity: the unbalanced distribution\nof training data, and differences in client resources. Yet more types of\nheterogeneity are becoming relevant as the capability of FL expands to cover\nmore complex problems, from the tuning of LLMs to enabling machine learning on\nedge devices. In this work, we discuss a novel type of heterogeneity that is\nlikely to become increasingly relevant in future applications: this is\npreference heterogeneity, emerging when clients learn under multiple\nobjectives, with different importance assigned to each objective on different\nclients. In this work, we discuss the implications of this type of\nheterogeneity and propose FedPref, a first algorithm designed to facilitate\npersonalised FL in this setting. We demonstrate the effectiveness of the\nalgorithm across different problems, preference distributions and model\narchitectures. In addition, we introduce a new analytical point of view, based\non multi-objective metrics, for evaluating the performance of FL algorithms in\nthis setting beyond the traditional client-focused metrics. We perform a second\nexperimental analysis based in this view, and show that FedPref outperforms\ncompared algorithms."
                },
                "authors": [
                    {
                        "name": "Maria Hartmann"
                    },
                    {
                        "name": "Grégoire Danoy"
                    },
                    {
                        "name": "Pascal Bouvry"
                    }
                ],
                "author_detail": {
                    "name": "Pascal Bouvry"
                },
                "author": "Pascal Bouvry",
                "arxiv_doi": "10.1145/3708984",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3708984",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2501.13604v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.13604v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "Accepted to ACM ToMPECS journal",
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.05200v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.05200v4",
                "updated": "2025-01-23T12:06:37Z",
                "updated_parsed": [
                    2025,
                    1,
                    23,
                    12,
                    6,
                    37,
                    3,
                    23,
                    0
                ],
                "published": "2024-08-09T17:44:45Z",
                "published_parsed": [
                    2024,
                    8,
                    9,
                    17,
                    44,
                    45,
                    4,
                    222,
                    0
                ],
                "title": "KIF: Knowledge Identification and Fusion for Language Model Continual\n  Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "KIF: Knowledge Identification and Fusion for Language Model Continual\n  Learning"
                },
                "summary": "Language model continual learning (CL) has recently attracted significant\ninterest for its ability to adapt large language models (LLMs) to dynamic\nreal-world scenarios without retraining. A major challenge in this domain is\ncatastrophic forgetting, where models lose previously acquired knowledge upon\nlearning new tasks. Existing approaches commonly utilize multiple\nparameter-efficient fine-tuning (PEFT) blocks to acquire task-specific\nknowledge, yet these methods are inefficient and fail to leverage potential\nknowledge transfer across tasks. In this paper, we introduce a novel CL\nframework for language models, named Knowledge Identification and Fusion (KIF),\nwhich boosts knowledge transfer without depending on memory replay. KIF\ninitially segregates the model into 'skill units' based on parameter\ndependencies, allowing for more precise control. Subsequently, it employs a\nnovel group-wise knowledge identification technique to ascertain the importance\ndistribution of skill units for a new task. By comparing this importance\ndistribution with those from previous tasks, we implement a fine-grained\nknowledge fusion strategy that retains task-specific knowledge, thereby\npreventing forgetting, and updates task-shared knowledge, which facilitates\nbi-directional knowledge transfer. As a result, KIF achieves an optimal balance\nbetween retaining prior knowledge and excelling in new tasks. KIF also\ndemonstrates strong generalizability, making it suitable for various base\nmodels and adaptable to PEFT methods like LoRA. Furthermore, it offers notable\nextensibility, supporting enhancements through integration with memory replay\ntechniques. Comprehensive experiments conducted on two CL benchmarks, involving\nmodels ranging from 220M to 7B parameters, affirm the effectiveness of KIF and\nits variants across different settings.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Language model continual learning (CL) has recently attracted significant\ninterest for its ability to adapt large language models (LLMs) to dynamic\nreal-world scenarios without retraining. A major challenge in this domain is\ncatastrophic forgetting, where models lose previously acquired knowledge upon\nlearning new tasks. Existing approaches commonly utilize multiple\nparameter-efficient fine-tuning (PEFT) blocks to acquire task-specific\nknowledge, yet these methods are inefficient and fail to leverage potential\nknowledge transfer across tasks. In this paper, we introduce a novel CL\nframework for language models, named Knowledge Identification and Fusion (KIF),\nwhich boosts knowledge transfer without depending on memory replay. KIF\ninitially segregates the model into 'skill units' based on parameter\ndependencies, allowing for more precise control. Subsequently, it employs a\nnovel group-wise knowledge identification technique to ascertain the importance\ndistribution of skill units for a new task. By comparing this importance\ndistribution with those from previous tasks, we implement a fine-grained\nknowledge fusion strategy that retains task-specific knowledge, thereby\npreventing forgetting, and updates task-shared knowledge, which facilitates\nbi-directional knowledge transfer. As a result, KIF achieves an optimal balance\nbetween retaining prior knowledge and excelling in new tasks. KIF also\ndemonstrates strong generalizability, making it suitable for various base\nmodels and adaptable to PEFT methods like LoRA. Furthermore, it offers notable\nextensibility, supporting enhancements through integration with memory replay\ntechniques. Comprehensive experiments conducted on two CL benchmarks, involving\nmodels ranging from 220M to 7B parameters, affirm the effectiveness of KIF and\nits variants across different settings."
                },
                "authors": [
                    {
                        "name": "Yujie Feng"
                    },
                    {
                        "name": "Xu Chu"
                    },
                    {
                        "name": "Yongxin Xu"
                    },
                    {
                        "name": "Zexin Lu"
                    },
                    {
                        "name": "Bo Liu"
                    },
                    {
                        "name": "Philip S. Yu"
                    },
                    {
                        "name": "Xiao-Ming Wu"
                    }
                ],
                "author_detail": {
                    "name": "Xiao-Ming Wu"
                },
                "author": "Xiao-Ming Wu",
                "arxiv_comment": "This version updates the model name from Task Skill Localization and\n  Consolidation (TaSL) to Knowledge Identification and Fusion (KIF). It is an\n  extension of the ACL 2024 paper titled Continual Dialog State Tracking via\n  Task Skill Localization and Consolidation",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.05200v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.05200v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.13598v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.13598v1",
                "updated": "2025-01-23T12:06:33Z",
                "updated_parsed": [
                    2025,
                    1,
                    23,
                    12,
                    6,
                    33,
                    3,
                    23,
                    0
                ],
                "published": "2025-01-23T12:06:33Z",
                "published_parsed": [
                    2025,
                    1,
                    23,
                    12,
                    6,
                    33,
                    3,
                    23,
                    0
                ],
                "title": "A Transformer-based Autoregressive Decoder Architecture for Hierarchical\n  Text Classification",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Transformer-based Autoregressive Decoder Architecture for Hierarchical\n  Text Classification"
                },
                "summary": "Recent approaches in hierarchical text classification (HTC) rely on the\ncapabilities of a pre-trained transformer model and exploit the label semantics\nand a graph encoder for the label hierarchy. In this paper, we introduce an\neffective hierarchical text classifier RADAr (Transformer-based Autoregressive\nDecoder Architecture) that is based only on an off-the-shelf RoBERTa\ntransformer to process the input and a custom autoregressive decoder with two\ndecoder layers for generating the classification output. Thus, unlike existing\napproaches for HTC, the encoder of RADAr has no explicit encoding of the label\nhierarchy and the decoder solely relies on the label sequences of the samples\nobserved during training. We demonstrate on three benchmark datasets that RADAr\nachieves results competitive to the state of the art with less training and\ninference time. Our model consistently performs better when organizing the\nlabel sequences from children to parents versus the inverse, as done in\nexisting HTC approaches. Our experiments show that neither the label semantics\nnor an explicit graph encoder for the hierarchy is needed. This has strong\npractical implications for HTC as the architecture has fewer requirements and\nprovides a speed-up by a factor of 2 at inference time. Moreover, training a\nseparate decoder from scratch in conjunction with fine-tuning the encoder\nallows future researchers and practitioners to exchange the encoder part as new\nmodels arise. The source code is available at\nhttps://github.com/yousef-younes/RADAr.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent approaches in hierarchical text classification (HTC) rely on the\ncapabilities of a pre-trained transformer model and exploit the label semantics\nand a graph encoder for the label hierarchy. In this paper, we introduce an\neffective hierarchical text classifier RADAr (Transformer-based Autoregressive\nDecoder Architecture) that is based only on an off-the-shelf RoBERTa\ntransformer to process the input and a custom autoregressive decoder with two\ndecoder layers for generating the classification output. Thus, unlike existing\napproaches for HTC, the encoder of RADAr has no explicit encoding of the label\nhierarchy and the decoder solely relies on the label sequences of the samples\nobserved during training. We demonstrate on three benchmark datasets that RADAr\nachieves results competitive to the state of the art with less training and\ninference time. Our model consistently performs better when organizing the\nlabel sequences from children to parents versus the inverse, as done in\nexisting HTC approaches. Our experiments show that neither the label semantics\nnor an explicit graph encoder for the hierarchy is needed. This has strong\npractical implications for HTC as the architecture has fewer requirements and\nprovides a speed-up by a factor of 2 at inference time. Moreover, training a\nseparate decoder from scratch in conjunction with fine-tuning the encoder\nallows future researchers and practitioners to exchange the encoder part as new\nmodels arise. The source code is available at\nhttps://github.com/yousef-younes/RADAr."
                },
                "authors": [
                    {
                        "name": "Younes Yousef"
                    },
                    {
                        "name": "Lukas Galke"
                    },
                    {
                        "name": "Ansgar Scherp"
                    }
                ],
                "author_detail": {
                    "name": "Ansgar Scherp"
                },
                "author": "Ansgar Scherp",
                "arxiv_doi": "10.3233/FAIA240661",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.3233/FAIA240661",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2501.13598v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.13598v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "7 pages + 1 for references. 2 Figure. ECAI conference",
                "arxiv_journal_ref": "27th European Conference on Artificial Intelligence 2024 (ECAI\n  2024). 2024",
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.13594v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.13594v1",
                "updated": "2025-01-23T12:03:29Z",
                "updated_parsed": [
                    2025,
                    1,
                    23,
                    12,
                    3,
                    29,
                    3,
                    23,
                    0
                ],
                "published": "2025-01-23T12:03:29Z",
                "published_parsed": [
                    2025,
                    1,
                    23,
                    12,
                    3,
                    29,
                    3,
                    23,
                    0
                ],
                "title": "Text-to-SQL based on Large Language Models and Database Keyword Search",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Text-to-SQL based on Large Language Models and Database Keyword Search"
                },
                "summary": "Text-to-SQL prompt strategies based on Large Language Models (LLMs) achieve\nremarkable performance on well-known benchmarks. However, when applied to\nreal-world databases, their performance is significantly less than for these\nbenchmarks, especially for Natural Language (NL) questions requiring complex\nfilters and joins to be processed. This paper then proposes a strategy to\ncompile NL questions into SQL queries that incorporates a dynamic few-shot\nexamples strategy and leverages the services provided by a database keyword\nsearch (KwS) platform. The paper details how the precision and recall of the\nschema-linking process are improved with the help of the examples provided and\nthe keyword-matching service that the KwS platform offers. Then, it shows how\nthe KwS platform can be used to synthesize a view that captures the joins\nrequired to process an input NL question and thereby simplify the SQL query\ncompilation step. The paper includes experiments with a real-world relational\ndatabase to assess the performance of the proposed strategy. The experiments\nsuggest that the strategy achieves an accuracy on the real-world relational\ndatabase that surpasses state-of-the-art approaches. The paper concludes by\ndiscussing the results obtained.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Text-to-SQL prompt strategies based on Large Language Models (LLMs) achieve\nremarkable performance on well-known benchmarks. However, when applied to\nreal-world databases, their performance is significantly less than for these\nbenchmarks, especially for Natural Language (NL) questions requiring complex\nfilters and joins to be processed. This paper then proposes a strategy to\ncompile NL questions into SQL queries that incorporates a dynamic few-shot\nexamples strategy and leverages the services provided by a database keyword\nsearch (KwS) platform. The paper details how the precision and recall of the\nschema-linking process are improved with the help of the examples provided and\nthe keyword-matching service that the KwS platform offers. Then, it shows how\nthe KwS platform can be used to synthesize a view that captures the joins\nrequired to process an input NL question and thereby simplify the SQL query\ncompilation step. The paper includes experiments with a real-world relational\ndatabase to assess the performance of the proposed strategy. The experiments\nsuggest that the strategy achieves an accuracy on the real-world relational\ndatabase that surpasses state-of-the-art approaches. The paper concludes by\ndiscussing the results obtained."
                },
                "authors": [
                    {
                        "name": "Eduardo R. Nascimento"
                    },
                    {
                        "name": "Caio Viktor S. Avila"
                    },
                    {
                        "name": "Yenier T. Izquierdo"
                    },
                    {
                        "name": "Grettel M. García"
                    },
                    {
                        "name": "Lucas Feijó L. Andrade"
                    },
                    {
                        "name": "Michelle S. P. Facina"
                    },
                    {
                        "name": "Melissa Lemos"
                    },
                    {
                        "name": "Marco A. Casanova"
                    }
                ],
                "author_detail": {
                    "name": "Marco A. Casanova"
                },
                "arxiv_affiliation": "Departamento de Informática, PUC-Rio, Rio de Janeiro, Brazil",
                "author": "Marco A. Casanova",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.13594v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.13594v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "68T50",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "H.2.3; I.2.7",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.13573v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.13573v1",
                "updated": "2025-01-23T11:23:25Z",
                "updated_parsed": [
                    2025,
                    1,
                    23,
                    11,
                    23,
                    25,
                    3,
                    23,
                    0
                ],
                "published": "2025-01-23T11:23:25Z",
                "published_parsed": [
                    2025,
                    1,
                    23,
                    11,
                    23,
                    25,
                    3,
                    23,
                    0
                ],
                "title": "Improving Contextual Faithfulness of Large Language Models via Retrieval\n  Heads-Induced Optimization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Improving Contextual Faithfulness of Large Language Models via Retrieval\n  Heads-Induced Optimization"
                },
                "summary": "Ensuring contextual faithfulness in retrieval-augmented large language models\n(LLMs) is crucial for building trustworthy information-seeking systems,\nparticularly in long-form question-answering (LFQA) scenarios. In this work, we\nidentify a salient correlation between LFQA faithfulness and retrieval heads, a\nset of attention heads responsible for retrieving contextual information.\nLeveraging this insight, we propose RHIO, a framework designed to teach LLMs to\nexplicitly discriminate between faithful and unfaithful generations. RHIO first\naugments unfaithful samples that simulate realistic model-intrinsic errors by\nselectively masking retrieval heads. Then, these samples are incorporated into\njoint training, enabling the model to distinguish unfaithful outputs from\nfaithful ones conditioned on control tokens. Furthermore, these control tokens\nare leveraged to self-induce contrastive outputs, amplifying their difference\nthrough contrastive decoding. Additionally, to facilitate the evaluation of\ncontextual faithfulness, we also introduce GroundBench, a comprehensive\nbenchmark compiled from five existing LFQA datasets. Extensive experimental\nresults on GroundBench demonstrate that RHIO significantly improves\nfaithfulness, even outperforming GPT-4o.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Ensuring contextual faithfulness in retrieval-augmented large language models\n(LLMs) is crucial for building trustworthy information-seeking systems,\nparticularly in long-form question-answering (LFQA) scenarios. In this work, we\nidentify a salient correlation between LFQA faithfulness and retrieval heads, a\nset of attention heads responsible for retrieving contextual information.\nLeveraging this insight, we propose RHIO, a framework designed to teach LLMs to\nexplicitly discriminate between faithful and unfaithful generations. RHIO first\naugments unfaithful samples that simulate realistic model-intrinsic errors by\nselectively masking retrieval heads. Then, these samples are incorporated into\njoint training, enabling the model to distinguish unfaithful outputs from\nfaithful ones conditioned on control tokens. Furthermore, these control tokens\nare leveraged to self-induce contrastive outputs, amplifying their difference\nthrough contrastive decoding. Additionally, to facilitate the evaluation of\ncontextual faithfulness, we also introduce GroundBench, a comprehensive\nbenchmark compiled from five existing LFQA datasets. Extensive experimental\nresults on GroundBench demonstrate that RHIO significantly improves\nfaithfulness, even outperforming GPT-4o."
                },
                "authors": [
                    {
                        "name": "Lei Huang"
                    },
                    {
                        "name": "Xiaocheng Feng"
                    },
                    {
                        "name": "Weitao Ma"
                    },
                    {
                        "name": "Yuchun Fan"
                    },
                    {
                        "name": "Xiachong Feng"
                    },
                    {
                        "name": "Yangfan Ye"
                    },
                    {
                        "name": "Weihong Zhong"
                    },
                    {
                        "name": "Yuxuan Gu"
                    },
                    {
                        "name": "Baoxin Wang"
                    },
                    {
                        "name": "Dayong Wu"
                    },
                    {
                        "name": "Guoping Hu"
                    },
                    {
                        "name": "Bing Qin"
                    }
                ],
                "author_detail": {
                    "name": "Bing Qin"
                },
                "author": "Bing Qin",
                "arxiv_comment": "Submitted to ARR October 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.13573v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.13573v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.00132v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.00132v3",
                "updated": "2025-01-23T11:22:20Z",
                "updated_parsed": [
                    2025,
                    1,
                    23,
                    11,
                    22,
                    20,
                    3,
                    23,
                    0
                ],
                "published": "2024-06-28T08:45:02Z",
                "published_parsed": [
                    2024,
                    6,
                    28,
                    8,
                    45,
                    2,
                    4,
                    180,
                    0
                ],
                "title": "ShortcutsBench: A Large-Scale Real-world Benchmark for API-based Agents",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ShortcutsBench: A Large-Scale Real-world Benchmark for API-based Agents"
                },
                "summary": "Recent advancements in integrating large language models (LLMs) with\napplication programming interfaces (APIs) have gained significant interest in\nboth academia and industry. Recent work demonstrates that these API-based\nagents exhibit relatively strong autonomy and planning capabilities. However,\ntheir ability to handle multi-dimensional difficulty levels, diverse task\ntypes, and real-world demands remains unknown. In this paper, we introduce\n\\textsc{ShortcutsBench}, a large-scale benchmark for the comprehensive\nevaluation of API-based agents in solving real-world complex tasks.\n\\textsc{ShortcutsBench} includes a wealth of real APIs from Apple Inc., refined\nuser queries, human-annotated high-quality action sequences, detailed parameter\nfilling values, and parameters requesting necessary input from the system or\nuser. We revealed how existing benchmarks~/~datasets struggle to accommodate\nthe advanced reasoning capabilities of existing more intelligent LLMs.\nMoreover, our extensive evaluation of agents built with $5$ leading open-source\n(size $\\geq$ 57B) and $5$ closed-source LLMs (e.g. Gemini-1.5-Pro and\nGPT-4o-mini) with varying intelligence level reveals significant limitations of\nexisting API-based agents in the whole process of handling complex queries\nrelated to API selection, parameter filling, and requesting necessary input\nfrom the system and the user. These findings highlight the great challenges\nthat API-based agents face in effectively fulfilling real and complex user\nqueries. All datasets, code, experimental logs, and results are available at\n\\url{https://github.com/EachSheep/ShortcutsBench}.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advancements in integrating large language models (LLMs) with\napplication programming interfaces (APIs) have gained significant interest in\nboth academia and industry. Recent work demonstrates that these API-based\nagents exhibit relatively strong autonomy and planning capabilities. However,\ntheir ability to handle multi-dimensional difficulty levels, diverse task\ntypes, and real-world demands remains unknown. In this paper, we introduce\n\\textsc{ShortcutsBench}, a large-scale benchmark for the comprehensive\nevaluation of API-based agents in solving real-world complex tasks.\n\\textsc{ShortcutsBench} includes a wealth of real APIs from Apple Inc., refined\nuser queries, human-annotated high-quality action sequences, detailed parameter\nfilling values, and parameters requesting necessary input from the system or\nuser. We revealed how existing benchmarks~/~datasets struggle to accommodate\nthe advanced reasoning capabilities of existing more intelligent LLMs.\nMoreover, our extensive evaluation of agents built with $5$ leading open-source\n(size $\\geq$ 57B) and $5$ closed-source LLMs (e.g. Gemini-1.5-Pro and\nGPT-4o-mini) with varying intelligence level reveals significant limitations of\nexisting API-based agents in the whole process of handling complex queries\nrelated to API selection, parameter filling, and requesting necessary input\nfrom the system and the user. These findings highlight the great challenges\nthat API-based agents face in effectively fulfilling real and complex user\nqueries. All datasets, code, experimental logs, and results are available at\n\\url{https://github.com/EachSheep/ShortcutsBench}."
                },
                "authors": [
                    {
                        "name": "Haiyang Shen"
                    },
                    {
                        "name": "Yue Li"
                    },
                    {
                        "name": "Desong Meng"
                    },
                    {
                        "name": "Dongqi Cai"
                    },
                    {
                        "name": "Sheng Qi"
                    },
                    {
                        "name": "Li Zhang"
                    },
                    {
                        "name": "Mengwei Xu"
                    },
                    {
                        "name": "Yun Ma"
                    }
                ],
                "author_detail": {
                    "name": "Yun Ma"
                },
                "author": "Yun Ma",
                "arxiv_comment": "ICLR'25: https://openreview.net/forum?id=kKILfPkhSz",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.00132v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.00132v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.04059v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.04059v2",
                "updated": "2025-01-23T11:16:46Z",
                "updated_parsed": [
                    2025,
                    1,
                    23,
                    11,
                    16,
                    46,
                    3,
                    23,
                    0
                ],
                "published": "2024-12-05T10:54:59Z",
                "published_parsed": [
                    2024,
                    12,
                    5,
                    10,
                    54,
                    59,
                    3,
                    340,
                    0
                ],
                "title": "The puzzling long GRB 191019A: Evidence for Kilonova Light",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The puzzling long GRB 191019A: Evidence for Kilonova Light"
                },
                "summary": "GRB 191019A was a long Gamma-ray burst (GRB) lasting about 65 s and, as such,\noriginally thought to be linked to a core-collapse supernova. However, even\nthough follow-up observations identified the optical counterpart close to the\nbright nucleus of a nearby ancient galaxy (z=0.248), no associated supernova\nwas found. This led to the suggestion that the burst was caused by the merger\nof two compact stellar objects, likely in a dense circumnuclear environment. By\nusing a recently developed diagnostic tool based on prompt emission temporal\nproperties, we noticed that GRB 191019A falls among those long GRBs which are\nassociated with compact mergers and with evidence of kilonova light. We thus\nre-analyzed unpublished GROND multi-color (g'r'i'z'JHK_s) data obtained between\n0.4 and 15 days post trigger. Image subtraction confirmed the optical\ncounterpart in all four optical bands, with GROND tracking its fading until 1.5\ndays post-burst. Incorporating publicly available Swift-XRT data, a joint fit\nof an afterglow plus a kilonova model revealed a better match than an\nafterglow-only scenario. The resulting kilonova properties resemble those of\nAT2017gfo associated with the binary neutron star merger GW170817, with a total\nejected mass of about 0.06 solar mass. Contrary to previous findings inferring\na high-density circumburst environment (n0=10^7-10^8 cm^-3), our analysis finds\nstandard conditions (n0 = 1 cm^-3), suggesting the long duration of GRB 191019A\nwas intrinsic rather than due to jet interaction with a dense external medium.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GRB 191019A was a long Gamma-ray burst (GRB) lasting about 65 s and, as such,\noriginally thought to be linked to a core-collapse supernova. However, even\nthough follow-up observations identified the optical counterpart close to the\nbright nucleus of a nearby ancient galaxy (z=0.248), no associated supernova\nwas found. This led to the suggestion that the burst was caused by the merger\nof two compact stellar objects, likely in a dense circumnuclear environment. By\nusing a recently developed diagnostic tool based on prompt emission temporal\nproperties, we noticed that GRB 191019A falls among those long GRBs which are\nassociated with compact mergers and with evidence of kilonova light. We thus\nre-analyzed unpublished GROND multi-color (g'r'i'z'JHK_s) data obtained between\n0.4 and 15 days post trigger. Image subtraction confirmed the optical\ncounterpart in all four optical bands, with GROND tracking its fading until 1.5\ndays post-burst. Incorporating publicly available Swift-XRT data, a joint fit\nof an afterglow plus a kilonova model revealed a better match than an\nafterglow-only scenario. The resulting kilonova properties resemble those of\nAT2017gfo associated with the binary neutron star merger GW170817, with a total\nejected mass of about 0.06 solar mass. Contrary to previous findings inferring\na high-density circumburst environment (n0=10^7-10^8 cm^-3), our analysis finds\nstandard conditions (n0 = 1 cm^-3), suggesting the long duration of GRB 191019A\nwas intrinsic rather than due to jet interaction with a dense external medium."
                },
                "authors": [
                    {
                        "name": "G. Stratta"
                    },
                    {
                        "name": "A. M. Nicuesa Guelbenzu"
                    },
                    {
                        "name": "S. Klose"
                    },
                    {
                        "name": "A. Rossi"
                    },
                    {
                        "name": "P. Singh"
                    },
                    {
                        "name": "E. Palazzi"
                    },
                    {
                        "name": "C. Guidorzi"
                    },
                    {
                        "name": "A. Camisasca"
                    },
                    {
                        "name": "S. Bernuzzi"
                    },
                    {
                        "name": "A. Rau"
                    },
                    {
                        "name": "M. Bulla"
                    },
                    {
                        "name": "F. Ragosta"
                    },
                    {
                        "name": "E. Maiorano"
                    },
                    {
                        "name": "D. Paris"
                    }
                ],
                "author_detail": {
                    "name": "D. Paris"
                },
                "author": "D. Paris",
                "arxiv_doi": "10.3847/1538-4357/ad9b7b",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.3847/1538-4357/ad9b7b",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2412.04059v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.04059v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "21 pages, 7 figures, accepted for publication (acknowledgments\n  updated)",
                "arxiv_journal_ref": "The Astrophysical Journal, 979, 159, 2025",
                "arxiv_primary_category": {
                    "term": "astro-ph.HE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.HE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.12883v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.12883v2",
                "updated": "2025-01-23T11:12:59Z",
                "updated_parsed": [
                    2025,
                    1,
                    23,
                    11,
                    12,
                    59,
                    3,
                    23,
                    0
                ],
                "published": "2025-01-22T13:44:44Z",
                "published_parsed": [
                    2025,
                    1,
                    22,
                    13,
                    44,
                    44,
                    2,
                    22,
                    0
                ],
                "title": "Generative AI Misuse Potential in Cyber Security Education: A Case Study\n  of a UK Degree Program",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Generative AI Misuse Potential in Cyber Security Education: A Case Study\n  of a UK Degree Program"
                },
                "summary": "Recent advances in generative artificial intelligence (AI), such as ChatGPT,\nGoogle Gemini, and other large language models (LLMs), pose significant\nchallenges to upholding academic integrity in higher education. This paper\ninvestigates the susceptibility of a Master's-level cyber security degree\nprogram at a UK Russell Group university, accredited by a leading national\nbody, to LLM misuse. Through the application and extension of a quantitative\nassessment framework, we identify a high exposure to misuse, particularly in\nindependent project- and report-based assessments. Contributing factors,\nincluding block teaching and a predominantly international cohort, are\nhighlighted as potential amplifiers of these vulnerabilities. To address these\nchallenges, we discuss the adoption of LLM-resistant assessments, detection\ntools, and the importance of fostering an ethical learning environment. These\napproaches aim to uphold academic standards while preparing students for the\ncomplexities of real-world cyber security.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advances in generative artificial intelligence (AI), such as ChatGPT,\nGoogle Gemini, and other large language models (LLMs), pose significant\nchallenges to upholding academic integrity in higher education. This paper\ninvestigates the susceptibility of a Master's-level cyber security degree\nprogram at a UK Russell Group university, accredited by a leading national\nbody, to LLM misuse. Through the application and extension of a quantitative\nassessment framework, we identify a high exposure to misuse, particularly in\nindependent project- and report-based assessments. Contributing factors,\nincluding block teaching and a predominantly international cohort, are\nhighlighted as potential amplifiers of these vulnerabilities. To address these\nchallenges, we discuss the adoption of LLM-resistant assessments, detection\ntools, and the importance of fostering an ethical learning environment. These\napproaches aim to uphold academic standards while preparing students for the\ncomplexities of real-world cyber security."
                },
                "authors": [
                    {
                        "name": "Carlton Shepherd"
                    }
                ],
                "author_detail": {
                    "name": "Carlton Shepherd"
                },
                "author": "Carlton Shepherd",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.12883v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.12883v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.09916v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.09916v3",
                "updated": "2025-01-23T11:03:13Z",
                "updated_parsed": [
                    2025,
                    1,
                    23,
                    11,
                    3,
                    13,
                    3,
                    23,
                    0
                ],
                "published": "2024-08-19T11:44:40Z",
                "published_parsed": [
                    2024,
                    8,
                    19,
                    11,
                    44,
                    40,
                    0,
                    232,
                    0
                ],
                "title": "Attribution Analysis Meets Model Editing: Advancing Knowledge Correction\n  in Vision Language Models with VisEdit",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Attribution Analysis Meets Model Editing: Advancing Knowledge Correction\n  in Vision Language Models with VisEdit"
                },
                "summary": "Model editing aims to correct outdated or erroneous knowledge in large models\nwithout costly retraining. Recent research discovered that the mid-layer\nrepresentation of the subject's final token in a prompt has a strong influence\non factual predictions, and developed Large Language Model (LLM) editing\ntechniques based on this observation. However, for Vision-LLMs (VLLMs), how\nvisual representations impact the predictions from a decoder-only language\nmodel remains largely unexplored. To the best of our knowledge, model editing\nfor VLLMs has not been extensively studied in the literature. In this work, we\nemploy the contribution allocation and noise perturbation methods to measure\nthe contributions of visual representations for token predictions. Our\nattribution analysis shows that visual representations in mid-to-later layers\nthat are highly relevant to the prompt contribute significantly to predictions.\nBased on these insights, we propose VisEdit, a novel model editor for VLLMs\nthat effectively corrects knowledge by editing intermediate visual\nrepresentations in regions important to the edit prompt. We evaluated VisEdit\nusing multiple VLLM backbones and public VLLM editing benchmark datasets. The\nresults show the superiority of VisEdit over the strong baselines adapted from\nexisting state-of-the-art editors for LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Model editing aims to correct outdated or erroneous knowledge in large models\nwithout costly retraining. Recent research discovered that the mid-layer\nrepresentation of the subject's final token in a prompt has a strong influence\non factual predictions, and developed Large Language Model (LLM) editing\ntechniques based on this observation. However, for Vision-LLMs (VLLMs), how\nvisual representations impact the predictions from a decoder-only language\nmodel remains largely unexplored. To the best of our knowledge, model editing\nfor VLLMs has not been extensively studied in the literature. In this work, we\nemploy the contribution allocation and noise perturbation methods to measure\nthe contributions of visual representations for token predictions. Our\nattribution analysis shows that visual representations in mid-to-later layers\nthat are highly relevant to the prompt contribute significantly to predictions.\nBased on these insights, we propose VisEdit, a novel model editor for VLLMs\nthat effectively corrects knowledge by editing intermediate visual\nrepresentations in regions important to the edit prompt. We evaluated VisEdit\nusing multiple VLLM backbones and public VLLM editing benchmark datasets. The\nresults show the superiority of VisEdit over the strong baselines adapted from\nexisting state-of-the-art editors for LLMs."
                },
                "authors": [
                    {
                        "name": "Qizhou Chen"
                    },
                    {
                        "name": "Taolin Zhang"
                    },
                    {
                        "name": "Chengyu Wang"
                    },
                    {
                        "name": "Xiaofeng He"
                    },
                    {
                        "name": "Dakan Wang"
                    },
                    {
                        "name": "Tingting Liu"
                    }
                ],
                "author_detail": {
                    "name": "Tingting Liu"
                },
                "author": "Tingting Liu",
                "arxiv_comment": "Accepted to AAAI-2025 as an oral presentation",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.09916v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.09916v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2401.06769v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2401.06769v3",
                "updated": "2025-01-23T10:59:49Z",
                "updated_parsed": [
                    2025,
                    1,
                    23,
                    10,
                    59,
                    49,
                    3,
                    23,
                    0
                ],
                "published": "2024-01-12T18:59:02Z",
                "published_parsed": [
                    2024,
                    1,
                    12,
                    18,
                    59,
                    2,
                    4,
                    12,
                    0
                ],
                "title": "Machine Translation Models are Zero-Shot Detectors of Translation\n  Direction",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Machine Translation Models are Zero-Shot Detectors of Translation\n  Direction"
                },
                "summary": "Detecting the translation direction of parallel text has applications for\nmachine translation training and evaluation, but also has forensic applications\nsuch as resolving plagiarism or forgery allegations. In this work, we explore\nan unsupervised approach to translation direction detection based on the simple\nhypothesis that\n$p(\\text{translation}|\\text{original})>p(\\text{original}|\\text{translation})$,\nmotivated by the well-known simplification effect in translationese or\nmachine-translationese. In experiments with massively multilingual machine\ntranslation models across 20 translation directions, we confirm the\neffectiveness of the approach for high-resource language pairs, achieving\ndocument-level accuracies of 82--96% for NMT-produced translations, and 60--81%\nfor human translations, depending on the model used. Code and demo are\navailable at https://github.com/ZurichNLP/translation-direction-detection",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Detecting the translation direction of parallel text has applications for\nmachine translation training and evaluation, but also has forensic applications\nsuch as resolving plagiarism or forgery allegations. In this work, we explore\nan unsupervised approach to translation direction detection based on the simple\nhypothesis that\n$p(\\text{translation}|\\text{original})>p(\\text{original}|\\text{translation})$,\nmotivated by the well-known simplification effect in translationese or\nmachine-translationese. In experiments with massively multilingual machine\ntranslation models across 20 translation directions, we confirm the\neffectiveness of the approach for high-resource language pairs, achieving\ndocument-level accuracies of 82--96% for NMT-produced translations, and 60--81%\nfor human translations, depending on the model used. Code and demo are\navailable at https://github.com/ZurichNLP/translation-direction-detection"
                },
                "authors": [
                    {
                        "name": "Michelle Wastl"
                    },
                    {
                        "name": "Jannis Vamvas"
                    },
                    {
                        "name": "Rico Sennrich"
                    }
                ],
                "author_detail": {
                    "name": "Rico Sennrich"
                },
                "author": "Rico Sennrich",
                "arxiv_comment": "21 pages, 2 figures, 10 tables in main part, 12 tables in appendix.\n  Added to this version: alternative supervised system, performance on LLM\n  generated translations, additional running text in appendix, typo fixes",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2401.06769v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2401.06769v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2311.16377v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2311.16377v3",
                "updated": "2025-01-23T10:55:45Z",
                "updated_parsed": [
                    2025,
                    1,
                    23,
                    10,
                    55,
                    45,
                    3,
                    23,
                    0
                ],
                "published": "2023-11-27T23:50:55Z",
                "published_parsed": [
                    2023,
                    11,
                    27,
                    23,
                    50,
                    55,
                    0,
                    331,
                    0
                ],
                "title": "$5 σ$ tension between Planck cosmic microwave background and eBOSS\n  Lyman-alpha forest and constraints on physics beyond $Λ$CDM",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "$5 σ$ tension between Planck cosmic microwave background and eBOSS\n  Lyman-alpha forest and constraints on physics beyond $Λ$CDM"
                },
                "summary": "We find that combined Planck cosmic microwave background, baryon acoustic\noscillations and supernovae data analyzed under $\\Lambda$CDM are in 4.9$\\sigma$\ntension with eBOSS Ly$\\alpha$ forest in inference of the linear matter power\nspectrum at wavenumber $\\sim 1 h\\,\\mathrm{Mpc}^{-1}$ and redshift = 3. Model\nextensions can alleviate this tension: running in the tilt of the primordial\npower spectrum ($\\alpha_\\mathrm{s} \\sim -0.01$); a fraction $\\sim (1 - 5)\\%$ of\nultra-light axion dark matter (DM) with particle mass $\\sim 10^{-25}$ eV or\nwarm DM with mass $\\sim 10$ eV. The new DESI survey, coupled with high-accuracy\nmodeling, will help distinguish the source of this discrepancy.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We find that combined Planck cosmic microwave background, baryon acoustic\noscillations and supernovae data analyzed under $\\Lambda$CDM are in 4.9$\\sigma$\ntension with eBOSS Ly$\\alpha$ forest in inference of the linear matter power\nspectrum at wavenumber $\\sim 1 h\\,\\mathrm{Mpc}^{-1}$ and redshift = 3. Model\nextensions can alleviate this tension: running in the tilt of the primordial\npower spectrum ($\\alpha_\\mathrm{s} \\sim -0.01$); a fraction $\\sim (1 - 5)\\%$ of\nultra-light axion dark matter (DM) with particle mass $\\sim 10^{-25}$ eV or\nwarm DM with mass $\\sim 10$ eV. The new DESI survey, coupled with high-accuracy\nmodeling, will help distinguish the source of this discrepancy."
                },
                "authors": [
                    {
                        "name": "Keir K. Rogers"
                    },
                    {
                        "name": "Vivian Poulin"
                    }
                ],
                "author_detail": {
                    "name": "Vivian Poulin"
                },
                "author": "Vivian Poulin",
                "arxiv_doi": "10.1103/PhysRevResearch.7.L012018",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1103/PhysRevResearch.7.L012018",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2311.16377v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2311.16377v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "16 pages, 9 figures. Minor changes to match published letter",
                "arxiv_journal_ref": "Phys. Rev. Research 7, L012018 (2025)",
                "arxiv_primary_category": {
                    "term": "astro-ph.CO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.CO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "hep-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.13545v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.13545v1",
                "updated": "2025-01-23T10:46:14Z",
                "updated_parsed": [
                    2025,
                    1,
                    23,
                    10,
                    46,
                    14,
                    3,
                    23,
                    0
                ],
                "published": "2025-01-23T10:46:14Z",
                "published_parsed": [
                    2025,
                    1,
                    23,
                    10,
                    46,
                    14,
                    3,
                    23,
                    0
                ],
                "title": "LLMs Can Plan Only If We Tell Them",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLMs Can Plan Only If We Tell Them"
                },
                "summary": "Large language models (LLMs) have demonstrated significant capabilities in\nnatural language processing and reasoning, yet their effectiveness in\nautonomous planning has been under debate. While existing studies have utilized\nLLMs with external feedback mechanisms or in controlled environments for\nplanning, these approaches often involve substantial computational and\ndevelopment resources due to the requirement for careful design and iterative\nbackprompting. Moreover, even the most advanced LLMs like GPT-4 struggle to\nmatch human performance on standard planning benchmarks, such as the\nBlocksworld, without additional support. This paper investigates whether LLMs\ncan independently generate long-horizon plans that rival human baselines. Our\nnovel enhancements to Algorithm-of-Thoughts (AoT), which we dub AoT+, help\nachieve state-of-the-art results in planning benchmarks out-competing prior\nmethods and human baselines all autonomously.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have demonstrated significant capabilities in\nnatural language processing and reasoning, yet their effectiveness in\nautonomous planning has been under debate. While existing studies have utilized\nLLMs with external feedback mechanisms or in controlled environments for\nplanning, these approaches often involve substantial computational and\ndevelopment resources due to the requirement for careful design and iterative\nbackprompting. Moreover, even the most advanced LLMs like GPT-4 struggle to\nmatch human performance on standard planning benchmarks, such as the\nBlocksworld, without additional support. This paper investigates whether LLMs\ncan independently generate long-horizon plans that rival human baselines. Our\nnovel enhancements to Algorithm-of-Thoughts (AoT), which we dub AoT+, help\nachieve state-of-the-art results in planning benchmarks out-competing prior\nmethods and human baselines all autonomously."
                },
                "authors": [
                    {
                        "name": "Bilgehan Sel"
                    },
                    {
                        "name": "Ruoxi Jia"
                    },
                    {
                        "name": "Ming Jin"
                    }
                ],
                "author_detail": {
                    "name": "Ming Jin"
                },
                "author": "Ming Jin",
                "arxiv_comment": "ICLR 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.13545v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.13545v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.13528v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.13528v1",
                "updated": "2025-01-23T10:23:04Z",
                "updated_parsed": [
                    2025,
                    1,
                    23,
                    10,
                    23,
                    4,
                    3,
                    23,
                    0
                ],
                "published": "2025-01-23T10:23:04Z",
                "published_parsed": [
                    2025,
                    1,
                    23,
                    10,
                    23,
                    4,
                    3,
                    23,
                    0
                ],
                "title": "Diffusion-based Perceptual Neural Video Compression with Temporal\n  Diffusion Information Reuse",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion-based Perceptual Neural Video Compression with Temporal\n  Diffusion Information Reuse"
                },
                "summary": "Recently, foundational diffusion models have attracted considerable attention\nin image compression tasks, whereas their application to video compression\nremains largely unexplored. In this article, we introduce DiffVC, a\ndiffusion-based perceptual neural video compression framework that effectively\nintegrates foundational diffusion model with the video conditional coding\nparadigm. This framework uses temporal context from previously decoded frame\nand the reconstructed latent representation of the current frame to guide the\ndiffusion model in generating high-quality results. To accelerate the iterative\ninference process of diffusion model, we propose the Temporal Diffusion\nInformation Reuse (TDIR) strategy, which significantly enhances inference\nefficiency with minimal performance loss by reusing the diffusion information\nfrom previous frames. Additionally, to address the challenges posed by\ndistortion differences across various bitrates, we propose the Quantization\nParameter-based Prompting (QPP) mechanism, which utilizes quantization\nparameters as prompts fed into the foundational diffusion model to explicitly\nmodulate intermediate features, thereby enabling a robust variable bitrate\ndiffusion-based neural compression framework. Experimental results demonstrate\nthat our proposed solution delivers excellent performance in both perception\nmetrics and visual quality.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recently, foundational diffusion models have attracted considerable attention\nin image compression tasks, whereas their application to video compression\nremains largely unexplored. In this article, we introduce DiffVC, a\ndiffusion-based perceptual neural video compression framework that effectively\nintegrates foundational diffusion model with the video conditional coding\nparadigm. This framework uses temporal context from previously decoded frame\nand the reconstructed latent representation of the current frame to guide the\ndiffusion model in generating high-quality results. To accelerate the iterative\ninference process of diffusion model, we propose the Temporal Diffusion\nInformation Reuse (TDIR) strategy, which significantly enhances inference\nefficiency with minimal performance loss by reusing the diffusion information\nfrom previous frames. Additionally, to address the challenges posed by\ndistortion differences across various bitrates, we propose the Quantization\nParameter-based Prompting (QPP) mechanism, which utilizes quantization\nparameters as prompts fed into the foundational diffusion model to explicitly\nmodulate intermediate features, thereby enabling a robust variable bitrate\ndiffusion-based neural compression framework. Experimental results demonstrate\nthat our proposed solution delivers excellent performance in both perception\nmetrics and visual quality."
                },
                "authors": [
                    {
                        "name": "Wenzhuo Ma"
                    },
                    {
                        "name": "Zhenzhong Chen"
                    }
                ],
                "author_detail": {
                    "name": "Zhenzhong Chen"
                },
                "author": "Zhenzhong Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.13528v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.13528v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.IV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.17296v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.17296v2",
                "updated": "2025-01-23T10:20:18Z",
                "updated_parsed": [
                    2025,
                    1,
                    23,
                    10,
                    20,
                    18,
                    3,
                    23,
                    0
                ],
                "published": "2024-09-25T19:15:34Z",
                "published_parsed": [
                    2024,
                    9,
                    25,
                    19,
                    15,
                    34,
                    2,
                    269,
                    0
                ],
                "title": "Eruptive mass loss less than a year before the explosion of\n  superluminous supernovae: I. The cases of SN 2020xga and SN 2022xgc",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Eruptive mass loss less than a year before the explosion of\n  superluminous supernovae: I. The cases of SN 2020xga and SN 2022xgc"
                },
                "summary": "We present photometric and spectroscopic observations of SN 2020xga and SN\n2022xgc, two hydrogen-poor superluminous supernovae (SLSNe-I) at $z = 0.4296$\nand $z = 0.3103$, respectively, which show an additional set of broad Mg II\nabsorption lines, blueshifted by a few thousands kilometer second$^{-1}$ with\nrespect to the host galaxy absorption system. Previous work interpreted this as\ndue to resonance line scattering of the SLSN continuum by rapidly expanding\ncircumstellar material (CSM) expelled shortly before the explosion. The peak\nrest-frame $g$-band magnitude of SN 2020xga is $-22.30 \\pm 0.04$ mag and of SN\n2022xgc is $-21.97 \\pm 0.05$ mag, placing them among the brightest SLSNe-I. We\nused high-quality spectra from ultraviolet to near-infrared wavelengths to\nmodel the Mg II line profiles and infer the properties of the CSM shells. We\nfind that the CSM shell of SN 2020xga resides at $\\sim 1.3 \\times 10^{16}~\\rm\ncm$, moving with a maximum velocity of $4275~\\rm km~s^{-1}$, and the shell of\nSN 2022xgc is located at $\\sim 0.8 \\times 10^{16}~\\rm cm$, reaching up to\n$4400~\\rm km~s^{-1}$. These shells were expelled $\\sim 11$ and $\\sim 5$ months\nbefore the explosions of SN 2020xga and SN 2022xgc, respectively, possibly as a\nresult of luminous-blue-variable-like eruptions or pulsational pair instability\n(PPI) mass loss. We also analyzed optical photometric data and modeled the\nlight curves, considering powering from the magnetar spin-down mechanism. The\nresults support very energetic magnetars, approaching the mass-shedding limit,\npowering these SNe with ejecta masses of $\\sim 7-9~\\rm M_\\odot$. The ejecta\nmasses inferred from the magnetar modeling are not consistent with the PPI\nscenario pointing toward stars $> 50~\\rm M_\\odot$ He-core; hence, alternative\nscenarios such as fallback accretion and CSM interaction are discussed.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present photometric and spectroscopic observations of SN 2020xga and SN\n2022xgc, two hydrogen-poor superluminous supernovae (SLSNe-I) at $z = 0.4296$\nand $z = 0.3103$, respectively, which show an additional set of broad Mg II\nabsorption lines, blueshifted by a few thousands kilometer second$^{-1}$ with\nrespect to the host galaxy absorption system. Previous work interpreted this as\ndue to resonance line scattering of the SLSN continuum by rapidly expanding\ncircumstellar material (CSM) expelled shortly before the explosion. The peak\nrest-frame $g$-band magnitude of SN 2020xga is $-22.30 \\pm 0.04$ mag and of SN\n2022xgc is $-21.97 \\pm 0.05$ mag, placing them among the brightest SLSNe-I. We\nused high-quality spectra from ultraviolet to near-infrared wavelengths to\nmodel the Mg II line profiles and infer the properties of the CSM shells. We\nfind that the CSM shell of SN 2020xga resides at $\\sim 1.3 \\times 10^{16}~\\rm\ncm$, moving with a maximum velocity of $4275~\\rm km~s^{-1}$, and the shell of\nSN 2022xgc is located at $\\sim 0.8 \\times 10^{16}~\\rm cm$, reaching up to\n$4400~\\rm km~s^{-1}$. These shells were expelled $\\sim 11$ and $\\sim 5$ months\nbefore the explosions of SN 2020xga and SN 2022xgc, respectively, possibly as a\nresult of luminous-blue-variable-like eruptions or pulsational pair instability\n(PPI) mass loss. We also analyzed optical photometric data and modeled the\nlight curves, considering powering from the magnetar spin-down mechanism. The\nresults support very energetic magnetars, approaching the mass-shedding limit,\npowering these SNe with ejecta masses of $\\sim 7-9~\\rm M_\\odot$. The ejecta\nmasses inferred from the magnetar modeling are not consistent with the PPI\nscenario pointing toward stars $> 50~\\rm M_\\odot$ He-core; hence, alternative\nscenarios such as fallback accretion and CSM interaction are discussed."
                },
                "authors": [
                    {
                        "name": "A. Gkini"
                    },
                    {
                        "name": "C. Fransson"
                    },
                    {
                        "name": "R. Lunnan"
                    },
                    {
                        "name": "S. Schulze"
                    },
                    {
                        "name": "F. Poidevin"
                    },
                    {
                        "name": "N. Sarin"
                    },
                    {
                        "name": "R. Könyves-Tóth"
                    },
                    {
                        "name": "J. Sollerman"
                    },
                    {
                        "name": "C. M. B. Omand"
                    },
                    {
                        "name": "S. J. Brennan"
                    },
                    {
                        "name": "K. R. Hinds"
                    },
                    {
                        "name": "J. P. Anderson"
                    },
                    {
                        "name": "M. Bronikowski"
                    },
                    {
                        "name": "T. -W. Chen"
                    },
                    {
                        "name": "R. Dekany"
                    },
                    {
                        "name": "M. Fraser"
                    },
                    {
                        "name": "C. Fremling"
                    },
                    {
                        "name": "L. Galbany"
                    },
                    {
                        "name": "A. Gal-Yam"
                    },
                    {
                        "name": "A. Gangopadhyay"
                    },
                    {
                        "name": "S. Geier"
                    },
                    {
                        "name": "E. P. Gonzalez"
                    },
                    {
                        "name": "M. Gromadzki"
                    },
                    {
                        "name": "S. L. Groom"
                    },
                    {
                        "name": "C. P. Gutiérrez"
                    },
                    {
                        "name": "D. Hiramatsu"
                    },
                    {
                        "name": "D. A. Howell"
                    },
                    {
                        "name": "Y. Hu"
                    },
                    {
                        "name": "C. Inserra"
                    },
                    {
                        "name": "M. Kopsacheili"
                    },
                    {
                        "name": "L. Lacroix"
                    },
                    {
                        "name": "F. J. Masci"
                    },
                    {
                        "name": "K. Matilainen"
                    },
                    {
                        "name": "C. McCully"
                    },
                    {
                        "name": "T. Moore"
                    },
                    {
                        "name": "T. E. Müller-Bravo"
                    },
                    {
                        "name": "M. Nicholl"
                    },
                    {
                        "name": "C. Pellegrino"
                    },
                    {
                        "name": "I. Pérez-Fournon"
                    },
                    {
                        "name": "D. A. Perley"
                    },
                    {
                        "name": "P. J. Pessi"
                    },
                    {
                        "name": "T. Petrushevska"
                    },
                    {
                        "name": "G. Pignata"
                    },
                    {
                        "name": "F. Ragosta"
                    },
                    {
                        "name": "A. Sahu"
                    },
                    {
                        "name": "A. Singh"
                    },
                    {
                        "name": "S. Srivastav"
                    },
                    {
                        "name": "J. L. Wise"
                    },
                    {
                        "name": "L. Yan"
                    },
                    {
                        "name": "D. R. Young"
                    }
                ],
                "author_detail": {
                    "name": "D. R. Young"
                },
                "author": "D. R. Young",
                "arxiv_doi": "10.1051/0004-6361/202452357",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1051/0004-6361/202452357",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2409.17296v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.17296v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "22 pages text, 7 pages appendix, 20 figures. Accepted for publication\n  at A&A on December 19, 2024",
                "arxiv_primary_category": {
                    "term": "astro-ph.HE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.HE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2401.06403v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2401.06403v3",
                "updated": "2025-01-23T10:11:31Z",
                "updated_parsed": [
                    2025,
                    1,
                    23,
                    10,
                    11,
                    31,
                    3,
                    23,
                    0
                ],
                "published": "2024-01-12T06:54:22Z",
                "published_parsed": [
                    2024,
                    1,
                    12,
                    6,
                    54,
                    22,
                    4,
                    12,
                    0
                ],
                "title": "Fourier analysis of spatial point processes",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Fourier analysis of spatial point processes"
                },
                "summary": "In this article, we develop comprehensive frequency domain methods for\nestimating and inferring the second-order structure of spatial point processes.\nThe main element here is on utilizing the discrete Fourier transform (DFT) of\nthe point pattern and its tapered counterpart. Under second-order stationarity,\nwe show that both the DFTs and the tapered DFTs are asymptotically jointly\nindependent Gaussian even when the DFTs share the same limiting frequencies.\nBased on these results, we establish an $\\alpha$-mixing central limit theorem\nfor a statistic formulated as a quadratic form of the tapered DFT. As\napplications, we derive the asymptotic distribution of the kernel spectral\ndensity estimator and establish a frequency domain inferential method for\nparametric stationary point processes. For the latter, the resulting model\nparameter estimator is computationally tractable and yields meaningful\ninterpretations even in the case of model misspecification. We investigate the\nfinite sample performance of our estimator through simulations, considering\nscenarios of both correctly specified and misspecified models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this article, we develop comprehensive frequency domain methods for\nestimating and inferring the second-order structure of spatial point processes.\nThe main element here is on utilizing the discrete Fourier transform (DFT) of\nthe point pattern and its tapered counterpart. Under second-order stationarity,\nwe show that both the DFTs and the tapered DFTs are asymptotically jointly\nindependent Gaussian even when the DFTs share the same limiting frequencies.\nBased on these results, we establish an $\\alpha$-mixing central limit theorem\nfor a statistic formulated as a quadratic form of the tapered DFT. As\napplications, we derive the asymptotic distribution of the kernel spectral\ndensity estimator and establish a frequency domain inferential method for\nparametric stationary point processes. For the latter, the resulting model\nparameter estimator is computationally tractable and yields meaningful\ninterpretations even in the case of model misspecification. We investigate the\nfinite sample performance of our estimator through simulations, considering\nscenarios of both correctly specified and misspecified models."
                },
                "authors": [
                    {
                        "name": "Junho Yang"
                    },
                    {
                        "name": "Yongtao Guan"
                    }
                ],
                "author_detail": {
                    "name": "Yongtao Guan"
                },
                "author": "Yongtao Guan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2401.06403v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2401.06403v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ME",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.ST",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.TH",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.11433v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.11433v2",
                "updated": "2025-01-23T09:29:19Z",
                "updated_parsed": [
                    2025,
                    1,
                    23,
                    9,
                    29,
                    19,
                    3,
                    23,
                    0
                ],
                "published": "2025-01-20T12:12:09Z",
                "published_parsed": [
                    2025,
                    1,
                    20,
                    12,
                    12,
                    9,
                    0,
                    20,
                    0
                ],
                "title": "One Does Not Simply Meme Alone: Evaluating Co-Creativity Between LLMs\n  and Humans in the Generation of Humor",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "One Does Not Simply Meme Alone: Evaluating Co-Creativity Between LLMs\n  and Humans in the Generation of Humor"
                },
                "summary": "Collaboration has been shown to enhance creativity, leading to more\ninnovative and effective outcomes. While previous research has explored the\nabilities of Large Language Models (LLMs) to serve as co-creative partners in\ntasks like writing poetry or creating narratives, the collaborative potential\nof LLMs in humor-rich and culturally nuanced domains remains an open question.\nTo address this gap, we conducted a user study to explore the potential of LLMs\nin co-creating memes - a humor-driven and culturally specific form of creative\nexpression. We conducted a user study with three groups of 50 participants\neach: a human-only group creating memes without AI assistance, a human-AI\ncollaboration group interacting with a state-of-the-art LLM model, and an\nAI-only group where the LLM autonomously generated memes. We assessed the\nquality of the generated memes through crowdsourcing, with each meme rated on\ncreativity, humor, and shareability. Our results showed that LLM assistance\nincreased the number of ideas generated and reduced the effort participants\nfelt. However, it did not improve the quality of the memes when humans\ncollaborated with LLM. Interestingly, memes created entirely by AI performed\nbetter than both human-only and human-AI collaborative memes in all areas on\naverage. However, when looking at the top-performing memes, human-created ones\nwere better in humor, while human-AI collaborations stood out in creativity and\nshareability. These findings highlight the complexities of human-AI\ncollaboration in creative tasks. While AI can boost productivity and create\ncontent that appeals to a broad audience, human creativity remains crucial for\ncontent that connects on a deeper level.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Collaboration has been shown to enhance creativity, leading to more\ninnovative and effective outcomes. While previous research has explored the\nabilities of Large Language Models (LLMs) to serve as co-creative partners in\ntasks like writing poetry or creating narratives, the collaborative potential\nof LLMs in humor-rich and culturally nuanced domains remains an open question.\nTo address this gap, we conducted a user study to explore the potential of LLMs\nin co-creating memes - a humor-driven and culturally specific form of creative\nexpression. We conducted a user study with three groups of 50 participants\neach: a human-only group creating memes without AI assistance, a human-AI\ncollaboration group interacting with a state-of-the-art LLM model, and an\nAI-only group where the LLM autonomously generated memes. We assessed the\nquality of the generated memes through crowdsourcing, with each meme rated on\ncreativity, humor, and shareability. Our results showed that LLM assistance\nincreased the number of ideas generated and reduced the effort participants\nfelt. However, it did not improve the quality of the memes when humans\ncollaborated with LLM. Interestingly, memes created entirely by AI performed\nbetter than both human-only and human-AI collaborative memes in all areas on\naverage. However, when looking at the top-performing memes, human-created ones\nwere better in humor, while human-AI collaborations stood out in creativity and\nshareability. These findings highlight the complexities of human-AI\ncollaboration in creative tasks. While AI can boost productivity and create\ncontent that appeals to a broad audience, human creativity remains crucial for\ncontent that connects on a deeper level."
                },
                "authors": [
                    {
                        "name": "Zhikun Wu"
                    },
                    {
                        "name": "Thomas Weber"
                    },
                    {
                        "name": "Florian Müller"
                    }
                ],
                "author_detail": {
                    "name": "Florian Müller"
                },
                "arxiv_affiliation": "TU Darmstadt",
                "author": "Florian Müller",
                "arxiv_doi": "10.1145/3708359.3712094",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3708359.3712094",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2501.11433v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.11433v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "to appear in: 30th International Conference on Intelligent User\n  Interfaces IUI 25 March 2427 2025 Cagliari Italy",
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.13491v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.13491v1",
                "updated": "2025-01-23T09:14:07Z",
                "updated_parsed": [
                    2025,
                    1,
                    23,
                    9,
                    14,
                    7,
                    3,
                    23,
                    0
                ],
                "published": "2025-01-23T09:14:07Z",
                "published_parsed": [
                    2025,
                    1,
                    23,
                    9,
                    14,
                    7,
                    3,
                    23,
                    0
                ],
                "title": "RECALL: Library-Like Behavior In Language Models is Enhanced by\n  Self-Referencing Causal Cycles",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RECALL: Library-Like Behavior In Language Models is Enhanced by\n  Self-Referencing Causal Cycles"
                },
                "summary": "We introduce the concept of the self-referencing causal cycle (abbreviated\nRECALL) - a mechanism that enables large language models (LLMs) to bypass the\nlimitations of unidirectional causality, which underlies a phenomenon known as\nthe reversal curse. When an LLM is prompted with sequential data, it often\nfails to recall preceding context. For example, when we ask an LLM to recall\nthe line preceding \"O say does that star-spangled banner yet wave\" in the U.S.\nNational Anthem, it often fails to correctly return \"Gave proof through the\nnight that our flag was still there\" - this is due to the reversal curse. It\noccurs because language models such as ChatGPT and Llama generate text based on\npreceding tokens, requiring facts to be learned and reproduced in a consistent\ntoken order. While the reversal curse is often viewed as a limitation, we offer\nevidence of an alternative view: it is not always an obstacle in practice. We\nfind that RECALL is driven by what we designate as cycle tokens - sequences\nthat connect different parts of the training data, enabling recall of preceding\ntokens from succeeding ones. Through rigorous probabilistic formalization and\ncontrolled experiments, we demonstrate how the cycles they induce influence a\nmodel's ability to reproduce information. To facilitate reproducibility, we\nprovide our code and experimental details at\nhttps://anonymous.4open.science/r/remember-B0B8/.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce the concept of the self-referencing causal cycle (abbreviated\nRECALL) - a mechanism that enables large language models (LLMs) to bypass the\nlimitations of unidirectional causality, which underlies a phenomenon known as\nthe reversal curse. When an LLM is prompted with sequential data, it often\nfails to recall preceding context. For example, when we ask an LLM to recall\nthe line preceding \"O say does that star-spangled banner yet wave\" in the U.S.\nNational Anthem, it often fails to correctly return \"Gave proof through the\nnight that our flag was still there\" - this is due to the reversal curse. It\noccurs because language models such as ChatGPT and Llama generate text based on\npreceding tokens, requiring facts to be learned and reproduced in a consistent\ntoken order. While the reversal curse is often viewed as a limitation, we offer\nevidence of an alternative view: it is not always an obstacle in practice. We\nfind that RECALL is driven by what we designate as cycle tokens - sequences\nthat connect different parts of the training data, enabling recall of preceding\ntokens from succeeding ones. Through rigorous probabilistic formalization and\ncontrolled experiments, we demonstrate how the cycles they induce influence a\nmodel's ability to reproduce information. To facilitate reproducibility, we\nprovide our code and experimental details at\nhttps://anonymous.4open.science/r/remember-B0B8/."
                },
                "authors": [
                    {
                        "name": "Munachiso Nwadike"
                    },
                    {
                        "name": "Zangir Iklassov"
                    },
                    {
                        "name": "Toluwani Aremu"
                    },
                    {
                        "name": "Tatsuya Hiraoka"
                    },
                    {
                        "name": "Velibor Bojkovic"
                    },
                    {
                        "name": "Benjamin Heinzerling"
                    },
                    {
                        "name": "Hilal Alqaubeh"
                    },
                    {
                        "name": "Martin Takáč"
                    },
                    {
                        "name": "Kentaro Inui"
                    }
                ],
                "author_detail": {
                    "name": "Kentaro Inui"
                },
                "author": "Kentaro Inui",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.13491v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.13491v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.04454v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.04454v2",
                "updated": "2025-01-23T09:11:30Z",
                "updated_parsed": [
                    2025,
                    1,
                    23,
                    9,
                    11,
                    30,
                    3,
                    23,
                    0
                ],
                "published": "2024-10-06T11:41:39Z",
                "published_parsed": [
                    2024,
                    10,
                    6,
                    11,
                    41,
                    39,
                    6,
                    280,
                    0
                ],
                "title": "Inner-Probe: Discovering Copyright-related Data Generation in LLM\n  Architecture",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Inner-Probe: Discovering Copyright-related Data Generation in LLM\n  Architecture"
                },
                "summary": "Large Language Models (LLMs) utilize extensive knowledge databases and show\npowerful text generation ability. However, their reliance on high-quality\ncopyrighted datasets raises concerns about copyright infringements in generated\ntexts. Current research often employs prompt engineering or semantic\nclassifiers to identify copyrighted content, but these approaches have two\nsignificant limitations: (1) Challenging to identify which specific sub-dataset\n(e.g., works from particular authors) influences an LLM's output. (2) Treating\nthe entire training database as copyrighted, hence overlooking the inclusion of\nnon-copyrighted training data.\n  We propose InnerProbe, a lightweight framework designed to evaluate the\ninfluence of copyrighted sub-datasets on LLM-generated texts. Unlike\ntraditional methods relying solely on text, we discover that the results of\nmulti-head attention (MHA) during LLM output generation provide more effective\ninformation. Thus, InnerProbe performs sub-dataset contribution analysis using\na lightweight LSTM-based network trained on MHA results in a supervised manner.\nHarnessing such a prior, InnerProbe enables non-copyrighted text detection\nthrough a concatenated global projector trained with unsupervised contrastive\nlearning. InnerProbe demonstrates 3x improved efficiency compared to semantic\nmodel training in sub-dataset contribution analysis on Books3, achieves\n15.04%-58.7% higher accuracy over baselines on the Pile, and delivers a 0.104\nincrease in AUC for non-copyrighted data filtering.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) utilize extensive knowledge databases and show\npowerful text generation ability. However, their reliance on high-quality\ncopyrighted datasets raises concerns about copyright infringements in generated\ntexts. Current research often employs prompt engineering or semantic\nclassifiers to identify copyrighted content, but these approaches have two\nsignificant limitations: (1) Challenging to identify which specific sub-dataset\n(e.g., works from particular authors) influences an LLM's output. (2) Treating\nthe entire training database as copyrighted, hence overlooking the inclusion of\nnon-copyrighted training data.\n  We propose InnerProbe, a lightweight framework designed to evaluate the\ninfluence of copyrighted sub-datasets on LLM-generated texts. Unlike\ntraditional methods relying solely on text, we discover that the results of\nmulti-head attention (MHA) during LLM output generation provide more effective\ninformation. Thus, InnerProbe performs sub-dataset contribution analysis using\na lightweight LSTM-based network trained on MHA results in a supervised manner.\nHarnessing such a prior, InnerProbe enables non-copyrighted text detection\nthrough a concatenated global projector trained with unsupervised contrastive\nlearning. InnerProbe demonstrates 3x improved efficiency compared to semantic\nmodel training in sub-dataset contribution analysis on Books3, achieves\n15.04%-58.7% higher accuracy over baselines on the Pile, and delivers a 0.104\nincrease in AUC for non-copyrighted data filtering."
                },
                "authors": [
                    {
                        "name": "Qichao Ma"
                    },
                    {
                        "name": "Rui-Jie Zhu"
                    },
                    {
                        "name": "Peiye Liu"
                    },
                    {
                        "name": "Renye Yan"
                    },
                    {
                        "name": "Fahong Zhang"
                    },
                    {
                        "name": "Ling Liang"
                    },
                    {
                        "name": "Meng Li"
                    },
                    {
                        "name": "Zhaofei Yu"
                    },
                    {
                        "name": "Zongwei Wang"
                    },
                    {
                        "name": "Yimao Cai"
                    },
                    {
                        "name": "Tiejun Huang"
                    }
                ],
                "author_detail": {
                    "name": "Tiejun Huang"
                },
                "author": "Tiejun Huang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.04454v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.04454v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.13483v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.13483v1",
                "updated": "2025-01-23T08:57:02Z",
                "updated_parsed": [
                    2025,
                    1,
                    23,
                    8,
                    57,
                    2,
                    3,
                    23,
                    0
                ],
                "published": "2025-01-23T08:57:02Z",
                "published_parsed": [
                    2025,
                    1,
                    23,
                    8,
                    57,
                    2,
                    3,
                    23,
                    0
                ],
                "title": "Robust Amortized Bayesian Inference with Self-Consistency Losses on\n  Unlabeled Data",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Robust Amortized Bayesian Inference with Self-Consistency Losses on\n  Unlabeled Data"
                },
                "summary": "Neural amortized Bayesian inference (ABI) can solve probabilistic inverse\nproblems orders of magnitude faster than classical methods. However, neural ABI\nis not yet sufficiently robust for widespread and safe applicability. In\nparticular, when performing inference on observations outside of the scope of\nthe simulated data seen during training, for example, because of model\nmisspecification, the posterior approximations are likely to become highly\nbiased. Due to the bad pre-asymptotic behavior of current neural posterior\nestimators in the out-of-simulation regime, the resulting estimation biases\ncannot be fixed in acceptable time by just simulating more training data. In\nthis proof-of-concept paper, we propose a semi-supervised approach that enables\ntraining not only on (labeled) simulated data generated from the model, but\nalso on unlabeled data originating from any source, including real-world data.\nTo achieve the latter, we exploit Bayesian self-consistency properties that can\nbe transformed into strictly proper losses without requiring knowledge of true\nparameter values, that is, without requiring data labels. The results of our\ninitial experiments show remarkable improvements in the robustness of ABI on\nout-of-simulation data. Even if the observed data is far away from both labeled\nand unlabeled training data, inference remains highly accurate. If our findings\nalso generalize to other scenarios and model classes, we believe that our new\nmethod represents a major breakthrough in neural ABI.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Neural amortized Bayesian inference (ABI) can solve probabilistic inverse\nproblems orders of magnitude faster than classical methods. However, neural ABI\nis not yet sufficiently robust for widespread and safe applicability. In\nparticular, when performing inference on observations outside of the scope of\nthe simulated data seen during training, for example, because of model\nmisspecification, the posterior approximations are likely to become highly\nbiased. Due to the bad pre-asymptotic behavior of current neural posterior\nestimators in the out-of-simulation regime, the resulting estimation biases\ncannot be fixed in acceptable time by just simulating more training data. In\nthis proof-of-concept paper, we propose a semi-supervised approach that enables\ntraining not only on (labeled) simulated data generated from the model, but\nalso on unlabeled data originating from any source, including real-world data.\nTo achieve the latter, we exploit Bayesian self-consistency properties that can\nbe transformed into strictly proper losses without requiring knowledge of true\nparameter values, that is, without requiring data labels. The results of our\ninitial experiments show remarkable improvements in the robustness of ABI on\nout-of-simulation data. Even if the observed data is far away from both labeled\nand unlabeled training data, inference remains highly accurate. If our findings\nalso generalize to other scenarios and model classes, we believe that our new\nmethod represents a major breakthrough in neural ABI."
                },
                "authors": [
                    {
                        "name": "Aayush Mishra"
                    },
                    {
                        "name": "Daniel Habermann"
                    },
                    {
                        "name": "Marvin Schmitt"
                    },
                    {
                        "name": "Stefan T. Radev"
                    },
                    {
                        "name": "Paul-Christian Bürkner"
                    }
                ],
                "author_detail": {
                    "name": "Paul-Christian Bürkner"
                },
                "author": "Paul-Christian Bürkner",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.13483v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.13483v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ML",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.13480v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.13480v1",
                "updated": "2025-01-23T08:53:12Z",
                "updated_parsed": [
                    2025,
                    1,
                    23,
                    8,
                    53,
                    12,
                    3,
                    23,
                    0
                ],
                "published": "2025-01-23T08:53:12Z",
                "published_parsed": [
                    2025,
                    1,
                    23,
                    8,
                    53,
                    12,
                    3,
                    23,
                    0
                ],
                "title": "Adaptive Testing for LLM-Based Applications: A Diversity-based Approach",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Adaptive Testing for LLM-Based Applications: A Diversity-based Approach"
                },
                "summary": "The recent surge of building software systems powered by Large Language\nModels (LLMs) has led to the development of various testing frameworks,\nprimarily focused on treating prompt templates as the unit of testing. Despite\nthe significant costs associated with test input execution and output\nassessment, the curation of optimized test suites is yet overlooked in these\ntools, which calls for tailored test selection or prioritization strategies. In\nthis paper, we show that diversity-based testing techniques, such as Adaptive\nRandom Testing (ART) with appropriate string distance metrics, can be\neffectively applied to the testing of prompt templates. Our proposed adaptive\ntesting approach adjusts the conventional ART process to this context by\nselecting new test inputs based on scores derived from existing test suite and\ntheir labelling results. Our results, obtained using various implementations\nthat explore several string-based distances, confirm that our approach enables\nthe discovery of failures with reduced testing budgets and promotes the\ngeneration of more varied outputs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The recent surge of building software systems powered by Large Language\nModels (LLMs) has led to the development of various testing frameworks,\nprimarily focused on treating prompt templates as the unit of testing. Despite\nthe significant costs associated with test input execution and output\nassessment, the curation of optimized test suites is yet overlooked in these\ntools, which calls for tailored test selection or prioritization strategies. In\nthis paper, we show that diversity-based testing techniques, such as Adaptive\nRandom Testing (ART) with appropriate string distance metrics, can be\neffectively applied to the testing of prompt templates. Our proposed adaptive\ntesting approach adjusts the conventional ART process to this context by\nselecting new test inputs based on scores derived from existing test suite and\ntheir labelling results. Our results, obtained using various implementations\nthat explore several string-based distances, confirm that our approach enables\nthe discovery of failures with reduced testing budgets and promotes the\ngeneration of more varied outputs."
                },
                "authors": [
                    {
                        "name": "Juyeon Yoon"
                    },
                    {
                        "name": "Robert Feldt"
                    },
                    {
                        "name": "Shin Yoo"
                    }
                ],
                "author_detail": {
                    "name": "Shin Yoo"
                },
                "author": "Shin Yoo",
                "arxiv_comment": "9 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.13480v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.13480v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.00525v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.00525v2",
                "updated": "2025-01-23T08:47:52Z",
                "updated_parsed": [
                    2025,
                    1,
                    23,
                    8,
                    47,
                    52,
                    3,
                    23,
                    0
                ],
                "published": "2024-11-30T16:19:25Z",
                "published_parsed": [
                    2024,
                    11,
                    30,
                    16,
                    19,
                    25,
                    5,
                    335,
                    0
                ],
                "title": "GloCOM: A Short Text Neural Topic Model via Global Clustering Context",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GloCOM: A Short Text Neural Topic Model via Global Clustering Context"
                },
                "summary": "Uncovering hidden topics from short texts is challenging for traditional and\nneural models due to data sparsity, which limits word co-occurrence patterns,\nand label sparsity, stemming from incomplete reconstruction targets. Although\ndata aggregation offers a potential solution, existing neural topic models\noften overlook it due to time complexity, poor aggregation quality, and\ndifficulty in inferring topic proportions for individual documents. In this\npaper, we propose a novel model, GloCOM (Global Clustering COntexts for Topic\nModels), which addresses these challenges by constructing aggregated global\nclustering contexts for short documents, leveraging text embeddings from\npre-trained language models. GloCOM can infer both global topic distributions\nfor clustering contexts and local distributions for individual short texts.\nAdditionally, the model incorporates these global contexts to augment the\nreconstruction loss, effectively handling the label sparsity issue. Extensive\nexperiments on short text datasets show that our approach outperforms other\nstate-of-the-art models in both topic quality and document representations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Uncovering hidden topics from short texts is challenging for traditional and\nneural models due to data sparsity, which limits word co-occurrence patterns,\nand label sparsity, stemming from incomplete reconstruction targets. Although\ndata aggregation offers a potential solution, existing neural topic models\noften overlook it due to time complexity, poor aggregation quality, and\ndifficulty in inferring topic proportions for individual documents. In this\npaper, we propose a novel model, GloCOM (Global Clustering COntexts for Topic\nModels), which addresses these challenges by constructing aggregated global\nclustering contexts for short documents, leveraging text embeddings from\npre-trained language models. GloCOM can infer both global topic distributions\nfor clustering contexts and local distributions for individual short texts.\nAdditionally, the model incorporates these global contexts to augment the\nreconstruction loss, effectively handling the label sparsity issue. Extensive\nexperiments on short text datasets show that our approach outperforms other\nstate-of-the-art models in both topic quality and document representations."
                },
                "authors": [
                    {
                        "name": "Quang Duc Nguyen"
                    },
                    {
                        "name": "Tung Nguyen"
                    },
                    {
                        "name": "Duc Anh Nguyen"
                    },
                    {
                        "name": "Linh Ngo Van"
                    },
                    {
                        "name": "Sang Dinh"
                    },
                    {
                        "name": "Thien Huu Nguyen"
                    }
                ],
                "author_detail": {
                    "name": "Thien Huu Nguyen"
                },
                "author": "Thien Huu Nguyen",
                "arxiv_comment": "Accepted to NAACL 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.00525v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.00525v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.19825v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.19825v2",
                "updated": "2025-01-23T08:45:52Z",
                "updated_parsed": [
                    2025,
                    1,
                    23,
                    8,
                    45,
                    52,
                    3,
                    23,
                    0
                ],
                "published": "2024-07-29T09:21:52Z",
                "published_parsed": [
                    2024,
                    7,
                    29,
                    9,
                    21,
                    52,
                    0,
                    211,
                    0
                ],
                "title": "Concise Thoughts: Impact of Output Length on LLM Reasoning and Cost",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Concise Thoughts: Impact of Output Length on LLM Reasoning and Cost"
                },
                "summary": "Today's large language models (LLMs) can solve challenging question-answering\ntasks, and prompt engineering techniques, such as chain-of-thought (CoT), have\ngained attention for enhancing the explanation and correctness of outputs.\nHowever, many models and techniques tend to produce excessively verbose and\nlengthy answers, leading to issues with both conciseness and generation time.\nTo address this, this paper analyzes the impact of output lengths on LLM\ninference pipelines by introducing and proposing novel metrics to evaluate the\n\\textit{correct conciseness} of a model and related prompting techniques. Then,\nwe examine the impact of controlling output length through a refined prompt\nengineering strategy, Constrained-CoT (CCoT), which encourages the model to\nproduce more concise outputs. To better understand the effects of such a\nprompt, we also introduce two additional scores for analyzing the conciseness,\nmeasured in terms of redundancy and information flow in generated answers.\nExperiments on pretrained LLMs and multiple datasets demonstrate the benefits\nof the proposed metrics and the effectiveness of CCoT across different models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Today's large language models (LLMs) can solve challenging question-answering\ntasks, and prompt engineering techniques, such as chain-of-thought (CoT), have\ngained attention for enhancing the explanation and correctness of outputs.\nHowever, many models and techniques tend to produce excessively verbose and\nlengthy answers, leading to issues with both conciseness and generation time.\nTo address this, this paper analyzes the impact of output lengths on LLM\ninference pipelines by introducing and proposing novel metrics to evaluate the\n\\textit{correct conciseness} of a model and related prompting techniques. Then,\nwe examine the impact of controlling output length through a refined prompt\nengineering strategy, Constrained-CoT (CCoT), which encourages the model to\nproduce more concise outputs. To better understand the effects of such a\nprompt, we also introduce two additional scores for analyzing the conciseness,\nmeasured in terms of redundancy and information flow in generated answers.\nExperiments on pretrained LLMs and multiple datasets demonstrate the benefits\nof the proposed metrics and the effectiveness of CCoT across different models."
                },
                "authors": [
                    {
                        "name": "Sania Nayab"
                    },
                    {
                        "name": "Giulio Rossolini"
                    },
                    {
                        "name": "Marco Simoni"
                    },
                    {
                        "name": "Andrea Saracino"
                    },
                    {
                        "name": "Giorgio Buttazzo"
                    },
                    {
                        "name": "Nicolamaria Manes"
                    },
                    {
                        "name": "Fabrizio Giacomelli"
                    }
                ],
                "author_detail": {
                    "name": "Fabrizio Giacomelli"
                },
                "author": "Fabrizio Giacomelli",
                "arxiv_comment": "Preprint version, under review",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.19825v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.19825v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.09686v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.09686v3",
                "updated": "2025-01-23T08:44:44Z",
                "updated_parsed": [
                    2025,
                    1,
                    23,
                    8,
                    44,
                    44,
                    3,
                    23,
                    0
                ],
                "published": "2025-01-16T17:37:58Z",
                "published_parsed": [
                    2025,
                    1,
                    16,
                    17,
                    37,
                    58,
                    3,
                    16,
                    0
                ],
                "title": "Towards Large Reasoning Models: A Survey of Reinforced Reasoning with\n  Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Towards Large Reasoning Models: A Survey of Reinforced Reasoning with\n  Large Language Models"
                },
                "summary": "Language has long been conceived as an essential tool for human reasoning.\nThe breakthrough of Large Language Models (LLMs) has sparked significant\nresearch interest in leveraging these models to tackle complex reasoning tasks.\nResearchers have moved beyond simple autoregressive token generation by\nintroducing the concept of \"thought\" -- a sequence of tokens representing\nintermediate steps in the reasoning process. This innovative paradigm enables\nLLMs' to mimic complex human reasoning processes, such as tree search and\nreflective thinking. Recently, an emerging trend of learning to reason has\napplied reinforcement learning (RL) to train LLMs to master reasoning\nprocesses. This approach enables the automatic generation of high-quality\nreasoning trajectories through trial-and-error search algorithms, significantly\nexpanding LLMs' reasoning capacity by providing substantially more training\ndata. Furthermore, recent studies demonstrate that encouraging LLMs to \"think\"\nwith more tokens during test-time inference can further significantly boost\nreasoning accuracy. Therefore, the train-time and test-time scaling combined to\nshow a new research frontier -- a path toward Large Reasoning Model. The\nintroduction of OpenAI's o1 series marks a significant milestone in this\nresearch direction. In this survey, we present a comprehensive review of recent\nprogress in LLM reasoning. We begin by introducing the foundational background\nof LLMs and then explore the key technical components driving the development\nof large reasoning models, with a focus on automated data construction,\nlearning-to-reason techniques, and test-time scaling. We also analyze popular\nopen-source projects at building large reasoning models, and conclude with open\nchallenges and future research directions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Language has long been conceived as an essential tool for human reasoning.\nThe breakthrough of Large Language Models (LLMs) has sparked significant\nresearch interest in leveraging these models to tackle complex reasoning tasks.\nResearchers have moved beyond simple autoregressive token generation by\nintroducing the concept of \"thought\" -- a sequence of tokens representing\nintermediate steps in the reasoning process. This innovative paradigm enables\nLLMs' to mimic complex human reasoning processes, such as tree search and\nreflective thinking. Recently, an emerging trend of learning to reason has\napplied reinforcement learning (RL) to train LLMs to master reasoning\nprocesses. This approach enables the automatic generation of high-quality\nreasoning trajectories through trial-and-error search algorithms, significantly\nexpanding LLMs' reasoning capacity by providing substantially more training\ndata. Furthermore, recent studies demonstrate that encouraging LLMs to \"think\"\nwith more tokens during test-time inference can further significantly boost\nreasoning accuracy. Therefore, the train-time and test-time scaling combined to\nshow a new research frontier -- a path toward Large Reasoning Model. The\nintroduction of OpenAI's o1 series marks a significant milestone in this\nresearch direction. In this survey, we present a comprehensive review of recent\nprogress in LLM reasoning. We begin by introducing the foundational background\nof LLMs and then explore the key technical components driving the development\nof large reasoning models, with a focus on automated data construction,\nlearning-to-reason techniques, and test-time scaling. We also analyze popular\nopen-source projects at building large reasoning models, and conclude with open\nchallenges and future research directions."
                },
                "authors": [
                    {
                        "name": "Fengli Xu"
                    },
                    {
                        "name": "Qianyue Hao"
                    },
                    {
                        "name": "Zefang Zong"
                    },
                    {
                        "name": "Jingwei Wang"
                    },
                    {
                        "name": "Yunke Zhang"
                    },
                    {
                        "name": "Jingyi Wang"
                    },
                    {
                        "name": "Xiaochong Lan"
                    },
                    {
                        "name": "Jiahui Gong"
                    },
                    {
                        "name": "Tianjian Ouyang"
                    },
                    {
                        "name": "Fanjin Meng"
                    },
                    {
                        "name": "Chenyang Shao"
                    },
                    {
                        "name": "Yuwei Yan"
                    },
                    {
                        "name": "Qinglong Yang"
                    },
                    {
                        "name": "Yiwen Song"
                    },
                    {
                        "name": "Sijian Ren"
                    },
                    {
                        "name": "Xinyuan Hu"
                    },
                    {
                        "name": "Yu Li"
                    },
                    {
                        "name": "Jie Feng"
                    },
                    {
                        "name": "Chen Gao"
                    },
                    {
                        "name": "Yong Li"
                    }
                ],
                "author_detail": {
                    "name": "Yong Li"
                },
                "author": "Yong Li",
                "arxiv_comment": "36 pages, 5 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.09686v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.09686v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            }
        ]
    },
    {
        "keyword": "LLM Deployment",
        "arxiv_results": [
            {
                "id": "http://arxiv.org/abs/2501.13927v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.13927v1",
                "updated": "2025-01-23T18:59:47Z",
                "updated_parsed": [
                    2025,
                    1,
                    23,
                    18,
                    59,
                    47,
                    3,
                    23,
                    0
                ],
                "published": "2025-01-23T18:59:47Z",
                "published_parsed": [
                    2025,
                    1,
                    23,
                    18,
                    59,
                    47,
                    3,
                    23,
                    0
                ],
                "title": "CRPO: Confidence-Reward Driven Preference Optimization for Machine\n  Translation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CRPO: Confidence-Reward Driven Preference Optimization for Machine\n  Translation"
                },
                "summary": "Large language models (LLMs) have shown great potential in natural language\nprocessing tasks, but their application to machine translation (MT) remains\nchallenging due to pretraining on English-centric data and the complexity of\nreinforcement learning from human feedback (RLHF). Direct Preference\nOptimization (DPO) has emerged as a simpler and more efficient alternative, but\nits performance depends heavily on the quality of preference data. To address\nthis, we propose Confidence-Reward driven Preference Optimization (CRPO), a\nnovel method that combines reward scores with model confidence to improve data\nselection for fine-tuning. CRPO selects challenging sentence pairs where the\nmodel is uncertain or underperforms, leading to more effective learning. While\nprimarily designed for LLMs, CRPO also generalizes to encoder-decoder models\nlike NLLB, demonstrating its versatility. Empirical results show that CRPO\noutperforms existing methods such as RS-DPO, RSO and MBR score in both\ntranslation accuracy and data efficiency.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have shown great potential in natural language\nprocessing tasks, but their application to machine translation (MT) remains\nchallenging due to pretraining on English-centric data and the complexity of\nreinforcement learning from human feedback (RLHF). Direct Preference\nOptimization (DPO) has emerged as a simpler and more efficient alternative, but\nits performance depends heavily on the quality of preference data. To address\nthis, we propose Confidence-Reward driven Preference Optimization (CRPO), a\nnovel method that combines reward scores with model confidence to improve data\nselection for fine-tuning. CRPO selects challenging sentence pairs where the\nmodel is uncertain or underperforms, leading to more effective learning. While\nprimarily designed for LLMs, CRPO also generalizes to encoder-decoder models\nlike NLLB, demonstrating its versatility. Empirical results show that CRPO\noutperforms existing methods such as RS-DPO, RSO and MBR score in both\ntranslation accuracy and data efficiency."
                },
                "authors": [
                    {
                        "name": "Guofeng Cui"
                    },
                    {
                        "name": "Pichao Wang"
                    },
                    {
                        "name": "Yang Liu"
                    },
                    {
                        "name": "Zemian Ke"
                    },
                    {
                        "name": "Zhu Liu"
                    },
                    {
                        "name": "Vimal Bhat"
                    }
                ],
                "author_detail": {
                    "name": "Vimal Bhat"
                },
                "author": "Vimal Bhat",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.13927v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.13927v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.13921v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.13921v1",
                "updated": "2025-01-23T18:59:02Z",
                "updated_parsed": [
                    2025,
                    1,
                    23,
                    18,
                    59,
                    2,
                    3,
                    23,
                    0
                ],
                "published": "2025-01-23T18:59:02Z",
                "published_parsed": [
                    2025,
                    1,
                    23,
                    18,
                    59,
                    2,
                    3,
                    23,
                    0
                ],
                "title": "The Breeze 2 Herd of Models: Traditional Chinese LLMs Based on Llama\n  with Vision-Aware and Function-Calling Capabilities",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Breeze 2 Herd of Models: Traditional Chinese LLMs Based on Llama\n  with Vision-Aware and Function-Calling Capabilities"
                },
                "summary": "Breeze 2 is a suite of advanced multi-modal language models, available in 3B\nand 8B parameter configurations, specifically designed to enhance Traditional\nChinese language representation. Building upon the Llama 3, Breeze 2 continues\npretraining on an extensive corpus to enhance the linguistic and cultural\nheritage of Traditional Chinese. It incorporates vision-aware capabilities\nthrough a visual encoder and a bridge module, and supports function-calling via\nprompt templates and post-training on function-calling data. The effectiveness\nof Breeze 2 is benchmarked across various tasks, including Taiwan general\nknowledge, instruction-following, long context, function calling, and vision\nunderstanding. Furthermore, we showcase the capabilities of the its 3B model in\na mobile application. We are publicly releasing all Breeze 2 models under the\nLlama 3 Community License.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Breeze 2 is a suite of advanced multi-modal language models, available in 3B\nand 8B parameter configurations, specifically designed to enhance Traditional\nChinese language representation. Building upon the Llama 3, Breeze 2 continues\npretraining on an extensive corpus to enhance the linguistic and cultural\nheritage of Traditional Chinese. It incorporates vision-aware capabilities\nthrough a visual encoder and a bridge module, and supports function-calling via\nprompt templates and post-training on function-calling data. The effectiveness\nof Breeze 2 is benchmarked across various tasks, including Taiwan general\nknowledge, instruction-following, long context, function calling, and vision\nunderstanding. Furthermore, we showcase the capabilities of the its 3B model in\na mobile application. We are publicly releasing all Breeze 2 models under the\nLlama 3 Community License."
                },
                "authors": [
                    {
                        "name": "Chan-Jan Hsu"
                    },
                    {
                        "name": "Chia-Sheng Liu"
                    },
                    {
                        "name": "Meng-Hsi Chen"
                    },
                    {
                        "name": "Muxi Chen"
                    },
                    {
                        "name": "Po-Chun Hsu"
                    },
                    {
                        "name": "Yi-Chang Chen"
                    },
                    {
                        "name": "Da-Shan Shiu"
                    }
                ],
                "author_detail": {
                    "name": "Da-Shan Shiu"
                },
                "author": "Da-Shan Shiu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.13921v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.13921v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.13912v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.13912v1",
                "updated": "2025-01-23T18:49:33Z",
                "updated_parsed": [
                    2025,
                    1,
                    23,
                    18,
                    49,
                    33,
                    3,
                    23,
                    0
                ],
                "published": "2025-01-23T18:49:33Z",
                "published_parsed": [
                    2025,
                    1,
                    23,
                    18,
                    49,
                    33,
                    3,
                    23,
                    0
                ],
                "title": "Analysis of Indic Language Capabilities in LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Analysis of Indic Language Capabilities in LLMs"
                },
                "summary": "This report evaluates the performance of text-in text-out Large Language\nModels (LLMs) to understand and generate Indic languages. This evaluation is\nused to identify and prioritize Indic languages suited for inclusion in safety\nbenchmarks. We conduct this study by reviewing existing evaluation studies and\ndatasets; and a set of twenty-eight LLMs that support Indic languages. We\nanalyze the LLMs on the basis of the training data, license for model and data,\ntype of access and model developers. We also compare Indic language performance\nacross evaluation datasets and find that significant performance disparities in\nperformance across Indic languages. Hindi is the most widely represented\nlanguage in models. While model performance roughly correlates with number of\nspeakers for the top five languages, the assessment after that varies.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This report evaluates the performance of text-in text-out Large Language\nModels (LLMs) to understand and generate Indic languages. This evaluation is\nused to identify and prioritize Indic languages suited for inclusion in safety\nbenchmarks. We conduct this study by reviewing existing evaluation studies and\ndatasets; and a set of twenty-eight LLMs that support Indic languages. We\nanalyze the LLMs on the basis of the training data, license for model and data,\ntype of access and model developers. We also compare Indic language performance\nacross evaluation datasets and find that significant performance disparities in\nperformance across Indic languages. Hindi is the most widely represented\nlanguage in models. While model performance roughly correlates with number of\nspeakers for the top five languages, the assessment after that varies."
                },
                "authors": [
                    {
                        "name": "Aatman Vaidya"
                    },
                    {
                        "name": "Tarunima Prabhakar"
                    },
                    {
                        "name": "Denny George"
                    },
                    {
                        "name": "Swair Shah"
                    }
                ],
                "author_detail": {
                    "name": "Swair Shah"
                },
                "author": "Swair Shah",
                "arxiv_comment": "17 pages, 2 figures, 5 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.13912v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.13912v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.03797v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.03797v2",
                "updated": "2025-01-23T18:44:28Z",
                "updated_parsed": [
                    2025,
                    1,
                    23,
                    18,
                    44,
                    28,
                    3,
                    23,
                    0
                ],
                "published": "2024-09-04T17:53:24Z",
                "published_parsed": [
                    2024,
                    9,
                    4,
                    17,
                    53,
                    24,
                    2,
                    248,
                    0
                ],
                "title": "NESTFUL: A Benchmark for Evaluating LLMs on Nested Sequences of API\n  Calls",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "NESTFUL: A Benchmark for Evaluating LLMs on Nested Sequences of API\n  Calls"
                },
                "summary": "The resurgence of autonomous agents built using large language models (LLMs)\nto solve complex real-world tasks has brought increased focus on LLMs'\nfundamental ability of tool or function calling. At the core of these agents,\nan LLM must plan, execute, and respond using external tools, APIs, and custom\nfunctions. Research on tool calling has gathered momentum, but evaluation\nbenchmarks and datasets representing the complexity of the tasks have lagged\nbehind. In this work, we focus on one such complexity, nested sequencing, with\nthe goal of extending existing benchmarks and evaluation. Specifically, we\npresent NESTFUL, a benchmark to evaluate LLMs on nested sequences of API calls,\ni.e., sequences where the output of one API call is passed as input to a\nsubsequent call. NESTFUL contains 1800+ nested sequences where all the function\ncalls are executable. Experimental results on multiple models and settings show\nthat the best-performing model on the dataset has a full sequence match\naccuracy of 25% and win-rate of 34% necessitating a large scope for improvement\nin the nested sequencing aspect of function calling. Our analysis of these\nresults provides possible future research directions for the community, in\naddition to a benchmark to track progress. We have released the NESTFUL dataset\nunder the Apache 2.0 license at https://github.com/IBM/NESTFUL.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The resurgence of autonomous agents built using large language models (LLMs)\nto solve complex real-world tasks has brought increased focus on LLMs'\nfundamental ability of tool or function calling. At the core of these agents,\nan LLM must plan, execute, and respond using external tools, APIs, and custom\nfunctions. Research on tool calling has gathered momentum, but evaluation\nbenchmarks and datasets representing the complexity of the tasks have lagged\nbehind. In this work, we focus on one such complexity, nested sequencing, with\nthe goal of extending existing benchmarks and evaluation. Specifically, we\npresent NESTFUL, a benchmark to evaluate LLMs on nested sequences of API calls,\ni.e., sequences where the output of one API call is passed as input to a\nsubsequent call. NESTFUL contains 1800+ nested sequences where all the function\ncalls are executable. Experimental results on multiple models and settings show\nthat the best-performing model on the dataset has a full sequence match\naccuracy of 25% and win-rate of 34% necessitating a large scope for improvement\nin the nested sequencing aspect of function calling. Our analysis of these\nresults provides possible future research directions for the community, in\naddition to a benchmark to track progress. We have released the NESTFUL dataset\nunder the Apache 2.0 license at https://github.com/IBM/NESTFUL."
                },
                "authors": [
                    {
                        "name": "Kinjal Basu"
                    },
                    {
                        "name": "Ibrahim Abdelaziz"
                    },
                    {
                        "name": "Kiran Kate"
                    },
                    {
                        "name": "Mayank Agarwal"
                    },
                    {
                        "name": "Maxwell Crouse"
                    },
                    {
                        "name": "Yara Rizk"
                    },
                    {
                        "name": "Kelsey Bradford"
                    },
                    {
                        "name": "Asim Munawar"
                    },
                    {
                        "name": "Sadhana Kumaravel"
                    },
                    {
                        "name": "Saurabh Goyal"
                    },
                    {
                        "name": "Xin Wang"
                    },
                    {
                        "name": "Luis A. Lastras"
                    },
                    {
                        "name": "Pavan Kapanipathi"
                    }
                ],
                "author_detail": {
                    "name": "Pavan Kapanipathi"
                },
                "author": "Pavan Kapanipathi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.03797v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.03797v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.13904v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.13904v1",
                "updated": "2025-01-23T18:34:09Z",
                "updated_parsed": [
                    2025,
                    1,
                    23,
                    18,
                    34,
                    9,
                    3,
                    23,
                    0
                ],
                "published": "2025-01-23T18:34:09Z",
                "published_parsed": [
                    2025,
                    1,
                    23,
                    18,
                    34,
                    9,
                    3,
                    23,
                    0
                ],
                "title": "Privacy-Preserving Personalized Federated Prompt Learning for Multimodal\n  Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Privacy-Preserving Personalized Federated Prompt Learning for Multimodal\n  Large Language Models"
                },
                "summary": "Multimodal Large Language Models (LLMs) are pivotal in revolutionizing\ncustomer support and operations by integrating multiple modalities such as\ntext, images, and audio. Federated Prompt Learning (FPL) is a recently proposed\napproach that combines pre-trained multimodal LLMs such as vision-language\nmodels with federated learning to create personalized, privacy-preserving AI\nsystems. However, balancing the competing goals of personalization,\ngeneralization, and privacy remains a significant challenge.\nOver-personalization can lead to overfitting, reducing generalizability, while\nstringent privacy measures, such as differential privacy, can hinder both\npersonalization and generalization. In this paper, we propose a Differentially\nPrivate Federated Prompt Learning (DP-FPL) approach to tackle this challenge by\nleveraging a low-rank adaptation scheme to capture generalization while\nmaintaining a residual term that preserves expressiveness for personalization.\nTo ensure privacy, we introduce a novel method where we apply local\ndifferential privacy to the two low-rank components of the local prompt, and\nglobal differential privacy to the global prompt. Our approach mitigates the\nimpact of privacy noise on the model performance while balancing the tradeoff\nbetween personalization and generalization. Extensive experiments demonstrate\nthe effectiveness of our approach over other benchmarks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multimodal Large Language Models (LLMs) are pivotal in revolutionizing\ncustomer support and operations by integrating multiple modalities such as\ntext, images, and audio. Federated Prompt Learning (FPL) is a recently proposed\napproach that combines pre-trained multimodal LLMs such as vision-language\nmodels with federated learning to create personalized, privacy-preserving AI\nsystems. However, balancing the competing goals of personalization,\ngeneralization, and privacy remains a significant challenge.\nOver-personalization can lead to overfitting, reducing generalizability, while\nstringent privacy measures, such as differential privacy, can hinder both\npersonalization and generalization. In this paper, we propose a Differentially\nPrivate Federated Prompt Learning (DP-FPL) approach to tackle this challenge by\nleveraging a low-rank adaptation scheme to capture generalization while\nmaintaining a residual term that preserves expressiveness for personalization.\nTo ensure privacy, we introduce a novel method where we apply local\ndifferential privacy to the two low-rank components of the local prompt, and\nglobal differential privacy to the global prompt. Our approach mitigates the\nimpact of privacy noise on the model performance while balancing the tradeoff\nbetween personalization and generalization. Extensive experiments demonstrate\nthe effectiveness of our approach over other benchmarks."
                },
                "authors": [
                    {
                        "name": "Linh Tran"
                    },
                    {
                        "name": "Wei Sun"
                    },
                    {
                        "name": "Stacy Patterson"
                    },
                    {
                        "name": "Ana Milanova"
                    }
                ],
                "author_detail": {
                    "name": "Ana Milanova"
                },
                "author": "Ana Milanova",
                "arxiv_comment": "Accepted to ICLR 2025 main conference track",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.13904v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.13904v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.03368v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.03368v2",
                "updated": "2025-01-23T17:57:28Z",
                "updated_parsed": [
                    2025,
                    1,
                    23,
                    17,
                    57,
                    28,
                    3,
                    23,
                    0
                ],
                "published": "2024-06-05T15:23:08Z",
                "published_parsed": [
                    2024,
                    6,
                    5,
                    15,
                    23,
                    8,
                    2,
                    157,
                    0
                ],
                "title": "IrokoBench: A New Benchmark for African Languages in the Age of Large\n  Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "IrokoBench: A New Benchmark for African Languages in the Age of Large\n  Language Models"
                },
                "summary": "Despite the widespread adoption of Large language models (LLMs), their\nremarkable capabilities remain limited to a few high-resource languages.\nAdditionally, many low-resource languages (\\eg African languages) are often\nevaluated only on basic text classification tasks due to the lack of\nappropriate or comprehensive benchmarks outside of high-resource languages. In\nthis paper, we introduce IrokoBench -- a human-translated benchmark dataset for\n17 typologically-diverse low-resource African languages covering three tasks:\nnatural language inference~(AfriXNLI), mathematical reasoning~(AfriMGSM), and\nmulti-choice knowledge-based question answering~(AfriMMLU). We use IrokoBench\nto evaluate zero-shot, few-shot, and translate-test settings~(where test sets\nare translated into English) across 10 open and six proprietary LLMs. Our\nevaluation reveals a significant performance gap between high-resource\nlanguages~(such as English and French) and low-resource African languages. We\nobserve a significant performance gap between open and proprietary models, with\nthe highest performing open model, Gemma 2 27B only at 63\\% of the\nbest-performing proprietary model GPT-4o performance. In addition, machine\ntranslating the test set to English before evaluation helped to close the gap\nfor larger models that are English-centric, such as Gemma 2 27B and LLaMa 3.1\n70B. These findings suggest that more efforts are needed to develop and adapt\nLLMs for African languages.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Despite the widespread adoption of Large language models (LLMs), their\nremarkable capabilities remain limited to a few high-resource languages.\nAdditionally, many low-resource languages (\\eg African languages) are often\nevaluated only on basic text classification tasks due to the lack of\nappropriate or comprehensive benchmarks outside of high-resource languages. In\nthis paper, we introduce IrokoBench -- a human-translated benchmark dataset for\n17 typologically-diverse low-resource African languages covering three tasks:\nnatural language inference~(AfriXNLI), mathematical reasoning~(AfriMGSM), and\nmulti-choice knowledge-based question answering~(AfriMMLU). We use IrokoBench\nto evaluate zero-shot, few-shot, and translate-test settings~(where test sets\nare translated into English) across 10 open and six proprietary LLMs. Our\nevaluation reveals a significant performance gap between high-resource\nlanguages~(such as English and French) and low-resource African languages. We\nobserve a significant performance gap between open and proprietary models, with\nthe highest performing open model, Gemma 2 27B only at 63\\% of the\nbest-performing proprietary model GPT-4o performance. In addition, machine\ntranslating the test set to English before evaluation helped to close the gap\nfor larger models that are English-centric, such as Gemma 2 27B and LLaMa 3.1\n70B. These findings suggest that more efforts are needed to develop and adapt\nLLMs for African languages."
                },
                "authors": [
                    {
                        "name": "David Ifeoluwa Adelani"
                    },
                    {
                        "name": "Jessica Ojo"
                    },
                    {
                        "name": "Israel Abebe Azime"
                    },
                    {
                        "name": "Jian Yun Zhuang"
                    },
                    {
                        "name": "Jesujoba O. Alabi"
                    },
                    {
                        "name": "Xuanli He"
                    },
                    {
                        "name": "Millicent Ochieng"
                    },
                    {
                        "name": "Sara Hooker"
                    },
                    {
                        "name": "Andiswa Bukula"
                    },
                    {
                        "name": "En-Shiun Annie Lee"
                    },
                    {
                        "name": "Chiamaka Chukwuneke"
                    },
                    {
                        "name": "Happy Buzaaba"
                    },
                    {
                        "name": "Blessing Sibanda"
                    },
                    {
                        "name": "Godson Kalipe"
                    },
                    {
                        "name": "Jonathan Mukiibi"
                    },
                    {
                        "name": "Salomon Kabongo"
                    },
                    {
                        "name": "Foutse Yuehgoh"
                    },
                    {
                        "name": "Mmasibidi Setaka"
                    },
                    {
                        "name": "Lolwethu Ndolela"
                    },
                    {
                        "name": "Nkiruka Odu"
                    },
                    {
                        "name": "Rooweither Mabuya"
                    },
                    {
                        "name": "Shamsuddeen Hassan Muhammad"
                    },
                    {
                        "name": "Salomey Osei"
                    },
                    {
                        "name": "Sokhar Samb"
                    },
                    {
                        "name": "Tadesse Kebede Guge"
                    },
                    {
                        "name": "Tombekai Vangoni Sherman"
                    },
                    {
                        "name": "Pontus Stenetorp"
                    }
                ],
                "author_detail": {
                    "name": "Pontus Stenetorp"
                },
                "author": "Pontus Stenetorp",
                "arxiv_comment": "Accepted to NAACL 2025 (main conference)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.03368v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.03368v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.13884v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.13884v1",
                "updated": "2025-01-23T17:57:18Z",
                "updated_parsed": [
                    2025,
                    1,
                    23,
                    17,
                    57,
                    18,
                    3,
                    23,
                    0
                ],
                "published": "2025-01-23T17:57:18Z",
                "published_parsed": [
                    2025,
                    1,
                    23,
                    17,
                    57,
                    18,
                    3,
                    23,
                    0
                ],
                "title": "Exploring Finetuned Audio-LLM on Heart Murmur Features",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Exploring Finetuned Audio-LLM on Heart Murmur Features"
                },
                "summary": "Large language models (LLMs) for audio have excelled in recognizing and\nanalyzing human speech, music, and environmental sounds. However, their\npotential for understanding other types of sounds, particularly biomedical\nsounds, remains largely underexplored despite significant scientific interest.\nIn this study, we focus on diagnosing cardiovascular diseases using\nphonocardiograms, i.e., heart sounds. Most existing deep neural network (DNN)\nparadigms are restricted to heart murmur classification (healthy vs unhealthy)\nand do not predict other acoustic features of the murmur such as timing,\ngrading, harshness, pitch, and quality, which are important in helping\nphysicians diagnose the underlying heart conditions. We propose to finetune an\naudio LLM, Qwen2-Audio, on the PhysioNet CirCor DigiScope phonocardiogram (PCG)\ndataset and evaluate its performance in classifying 11 expert-labeled murmur\nfeatures. Additionally, we aim to achieve more noise-robust and generalizable\nsystem by exploring a preprocessing segmentation algorithm using an audio\nrepresentation model, SSAMBA. Our results indicate that the LLM-based model\noutperforms state-of-the-art methods in 8 of the 11 features and performs\ncomparably in the remaining 3. Moreover, the LLM successfully classifies\nlong-tail murmur features with limited training data, a task that all previous\nmethods have failed to classify. These findings underscore the potential of\naudio LLMs as assistants to human cardiologists in enhancing heart disease\ndiagnosis.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) for audio have excelled in recognizing and\nanalyzing human speech, music, and environmental sounds. However, their\npotential for understanding other types of sounds, particularly biomedical\nsounds, remains largely underexplored despite significant scientific interest.\nIn this study, we focus on diagnosing cardiovascular diseases using\nphonocardiograms, i.e., heart sounds. Most existing deep neural network (DNN)\nparadigms are restricted to heart murmur classification (healthy vs unhealthy)\nand do not predict other acoustic features of the murmur such as timing,\ngrading, harshness, pitch, and quality, which are important in helping\nphysicians diagnose the underlying heart conditions. We propose to finetune an\naudio LLM, Qwen2-Audio, on the PhysioNet CirCor DigiScope phonocardiogram (PCG)\ndataset and evaluate its performance in classifying 11 expert-labeled murmur\nfeatures. Additionally, we aim to achieve more noise-robust and generalizable\nsystem by exploring a preprocessing segmentation algorithm using an audio\nrepresentation model, SSAMBA. Our results indicate that the LLM-based model\noutperforms state-of-the-art methods in 8 of the 11 features and performs\ncomparably in the remaining 3. Moreover, the LLM successfully classifies\nlong-tail murmur features with limited training data, a task that all previous\nmethods have failed to classify. These findings underscore the potential of\naudio LLMs as assistants to human cardiologists in enhancing heart disease\ndiagnosis."
                },
                "authors": [
                    {
                        "name": "Adrian Florea"
                    },
                    {
                        "name": "Xilin Jiang"
                    },
                    {
                        "name": "Nima Mesgarani"
                    },
                    {
                        "name": "Xiaofan Jiang"
                    }
                ],
                "author_detail": {
                    "name": "Xiaofan Jiang"
                },
                "author": "Xiaofan Jiang",
                "arxiv_comment": "5 pages, 1 figure, and 3 tables. Submitted to IEEE/ACM Conference on\n  Connected Health: Applications, Systems , and Engineering Technologies",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.13884v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.13884v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.AS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.13881v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.13881v1",
                "updated": "2025-01-23T17:56:07Z",
                "updated_parsed": [
                    2025,
                    1,
                    23,
                    17,
                    56,
                    7,
                    3,
                    23,
                    0
                ],
                "published": "2025-01-23T17:56:07Z",
                "published_parsed": [
                    2025,
                    1,
                    23,
                    17,
                    56,
                    7,
                    3,
                    23,
                    0
                ],
                "title": "The machine learning platform for developers of large systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The machine learning platform for developers of large systems"
                },
                "summary": "The machine learning system in the form of Retrieval Augmented Generation\n(RAG) has developed steadily since about 2021. RAG could be observed as a\nversion of the knowledge transfer. In the studied case, the large computing\nsystems are observed as the application point of RAG, which includes large\nlanguage model (LLM), as a partner for the developing team. Such an approach\nhas advantages during the development process and further in exploitation time.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The machine learning system in the form of Retrieval Augmented Generation\n(RAG) has developed steadily since about 2021. RAG could be observed as a\nversion of the knowledge transfer. In the studied case, the large computing\nsystems are observed as the application point of RAG, which includes large\nlanguage model (LLM), as a partner for the developing team. Such an approach\nhas advantages during the development process and further in exploitation time."
                },
                "authors": [
                    {
                        "name": "Alexey Naikov"
                    },
                    {
                        "name": "Anatoly Oreshkin"
                    },
                    {
                        "name": "Alexey Shvetsov"
                    },
                    {
                        "name": "Andrey Shevel"
                    }
                ],
                "author_detail": {
                    "name": "Andrey Shevel"
                },
                "author": "Andrey Shevel",
                "arxiv_comment": "5 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.13881v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.13881v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "hep-ex",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "hep-ex",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "J.2",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.13880v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.13880v1",
                "updated": "2025-01-23T17:54:19Z",
                "updated_parsed": [
                    2025,
                    1,
                    23,
                    17,
                    54,
                    19,
                    3,
                    23,
                    0
                ],
                "published": "2025-01-23T17:54:19Z",
                "published_parsed": [
                    2025,
                    1,
                    23,
                    17,
                    54,
                    19,
                    3,
                    23,
                    0
                ],
                "title": "A RAG-Based Institutional Assistant",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A RAG-Based Institutional Assistant"
                },
                "summary": "Although large language models (LLMs) demonstrate strong text generation\ncapabilities, they struggle in scenarios requiring access to structured\nknowledge bases or specific documents, limiting their effectiveness in\nknowledge-intensive tasks. To address this limitation, retrieval-augmented\ngeneration (RAG) models have been developed, enabling generative models to\nincorporate relevant document fragments into their inputs. In this paper, we\ndesign and evaluate a RAG-based virtual assistant specifically tailored for the\nUniversity of S\\~ao Paulo. Our system architecture comprises two key modules: a\nretriever and a generative model. We experiment with different types of models\nfor both components, adjusting hyperparameters such as chunk size and the\nnumber of retrieved documents. Our optimal retriever model achieves a Top-5\naccuracy of 30%, while our most effective generative model scores 22.04\\%\nagainst ground truth answers. Notably, when the correct document chunks are\nsupplied to the LLMs, accuracy significantly improves to 54.02%, an increase of\nover 30 percentage points. Conversely, without contextual input, performance\ndeclines to 13.68%. These findings highlight the critical role of database\naccess in enhancing LLM performance. They also reveal the limitations of\ncurrent semantic search methods in accurately identifying relevant documents\nand underscore the ongoing challenges LLMs face in generating precise\nresponses.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Although large language models (LLMs) demonstrate strong text generation\ncapabilities, they struggle in scenarios requiring access to structured\nknowledge bases or specific documents, limiting their effectiveness in\nknowledge-intensive tasks. To address this limitation, retrieval-augmented\ngeneration (RAG) models have been developed, enabling generative models to\nincorporate relevant document fragments into their inputs. In this paper, we\ndesign and evaluate a RAG-based virtual assistant specifically tailored for the\nUniversity of S\\~ao Paulo. Our system architecture comprises two key modules: a\nretriever and a generative model. We experiment with different types of models\nfor both components, adjusting hyperparameters such as chunk size and the\nnumber of retrieved documents. Our optimal retriever model achieves a Top-5\naccuracy of 30%, while our most effective generative model scores 22.04\\%\nagainst ground truth answers. Notably, when the correct document chunks are\nsupplied to the LLMs, accuracy significantly improves to 54.02%, an increase of\nover 30 percentage points. Conversely, without contextual input, performance\ndeclines to 13.68%. These findings highlight the critical role of database\naccess in enhancing LLM performance. They also reveal the limitations of\ncurrent semantic search methods in accurately identifying relevant documents\nand underscore the ongoing challenges LLMs face in generating precise\nresponses."
                },
                "authors": [
                    {
                        "name": "Gustavo Kuratomi"
                    },
                    {
                        "name": "Paulo Pirozelli"
                    },
                    {
                        "name": "Fabio G. Cozman"
                    },
                    {
                        "name": "Sarajane M. Peres"
                    }
                ],
                "author_detail": {
                    "name": "Sarajane M. Peres"
                },
                "author": "Sarajane M. Peres",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.13880v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.13880v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.13876v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.13876v1",
                "updated": "2025-01-23T17:49:49Z",
                "updated_parsed": [
                    2025,
                    1,
                    23,
                    17,
                    49,
                    49,
                    3,
                    23,
                    0
                ],
                "published": "2025-01-23T17:49:49Z",
                "published_parsed": [
                    2025,
                    1,
                    23,
                    17,
                    49,
                    49,
                    3,
                    23,
                    0
                ],
                "title": "FAST-LIVO2 on Resource-Constrained Platforms: LiDAR-Inertial-Visual\n  Odometry with Efficient Memory and Computation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FAST-LIVO2 on Resource-Constrained Platforms: LiDAR-Inertial-Visual\n  Odometry with Efficient Memory and Computation"
                },
                "summary": "This paper presents a lightweight LiDAR-inertial-visual odometry system\noptimized for resource-constrained platforms. It integrates a\ndegeneration-aware adaptive visual frame selector into error-state iterated\nKalman filter (ESIKF) with sequential updates, improving computation efficiency\nsignificantly while maintaining a similar level of robustness. Additionally, a\nmemory-efficient mapping structure combining a locally unified visual-LiDAR map\nand a long-term visual map achieves a good trade-off between performance and\nmemory usage. Extensive experiments on x86 and ARM platforms demonstrate the\nsystem's robustness and efficiency. On the Hilti dataset, our system achieves a\n33% reduction in per-frame runtime and 47% lower memory usage compared to\nFAST-LIVO2, with only a 3 cm increase in RMSE. Despite this slight accuracy\ntrade-off, our system remains competitive, outperforming state-of-the-art\n(SOTA) LIO methods such as FAST-LIO2 and most existing LIVO systems. These\nresults validate the system's capability for scalable deployment on\nresource-constrained edge computing platforms.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper presents a lightweight LiDAR-inertial-visual odometry system\noptimized for resource-constrained platforms. It integrates a\ndegeneration-aware adaptive visual frame selector into error-state iterated\nKalman filter (ESIKF) with sequential updates, improving computation efficiency\nsignificantly while maintaining a similar level of robustness. Additionally, a\nmemory-efficient mapping structure combining a locally unified visual-LiDAR map\nand a long-term visual map achieves a good trade-off between performance and\nmemory usage. Extensive experiments on x86 and ARM platforms demonstrate the\nsystem's robustness and efficiency. On the Hilti dataset, our system achieves a\n33% reduction in per-frame runtime and 47% lower memory usage compared to\nFAST-LIVO2, with only a 3 cm increase in RMSE. Despite this slight accuracy\ntrade-off, our system remains competitive, outperforming state-of-the-art\n(SOTA) LIO methods such as FAST-LIO2 and most existing LIVO systems. These\nresults validate the system's capability for scalable deployment on\nresource-constrained edge computing platforms."
                },
                "authors": [
                    {
                        "name": "Bingyang Zhou"
                    },
                    {
                        "name": "Chunran Zheng"
                    },
                    {
                        "name": "Ziming Wang"
                    },
                    {
                        "name": "Fangcheng Zhu"
                    },
                    {
                        "name": "Yixi Cai"
                    },
                    {
                        "name": "Fu Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Fu Zhang"
                },
                "author": "Fu Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.13876v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.13876v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.19599v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.19599v3",
                "updated": "2025-01-23T17:05:40Z",
                "updated_parsed": [
                    2025,
                    1,
                    23,
                    17,
                    5,
                    40,
                    3,
                    23,
                    0
                ],
                "published": "2024-10-25T14:46:07Z",
                "published_parsed": [
                    2024,
                    10,
                    25,
                    14,
                    46,
                    7,
                    4,
                    299,
                    0
                ],
                "title": "Take Caution in Using LLMs as Human Surrogates: Scylla Ex Machina",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Take Caution in Using LLMs as Human Surrogates: Scylla Ex Machina"
                },
                "summary": "Recent studies suggest large language models (LLMs) can exhibit human-like\nreasoning, aligning with human behavior in economic experiments, surveys, and\npolitical discourse. This has led many to propose that LLMs can be used as\nsurrogates or simulations for humans in social science research. However, LLMs\ndiffer fundamentally from humans, relying on probabilistic patterns, absent the\nembodied experiences or survival objectives that shape human cognition. We\nassess the reasoning depth of LLMs using the 11-20 money request game. Nearly\nall advanced approaches fail to replicate human behavior distributions across\nmany models. Causes of failure are diverse and unpredictable, relating to input\nlanguage, roles, and safeguarding. These results advise caution when using LLMs\nto study human behavior or as surrogates or simulations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent studies suggest large language models (LLMs) can exhibit human-like\nreasoning, aligning with human behavior in economic experiments, surveys, and\npolitical discourse. This has led many to propose that LLMs can be used as\nsurrogates or simulations for humans in social science research. However, LLMs\ndiffer fundamentally from humans, relying on probabilistic patterns, absent the\nembodied experiences or survival objectives that shape human cognition. We\nassess the reasoning depth of LLMs using the 11-20 money request game. Nearly\nall advanced approaches fail to replicate human behavior distributions across\nmany models. Causes of failure are diverse and unpredictable, relating to input\nlanguage, roles, and safeguarding. These results advise caution when using LLMs\nto study human behavior or as surrogates or simulations."
                },
                "authors": [
                    {
                        "name": "Yuan Gao"
                    },
                    {
                        "name": "Dokyun Lee"
                    },
                    {
                        "name": "Gordon Burtch"
                    },
                    {
                        "name": "Sina Fazelpour"
                    }
                ],
                "author_detail": {
                    "name": "Sina Fazelpour"
                },
                "author": "Sina Fazelpour",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.19599v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.19599v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "econ.GN",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "econ.GN",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "q-fin.EC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.13833v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.13833v1",
                "updated": "2025-01-23T16:58:18Z",
                "updated_parsed": [
                    2025,
                    1,
                    23,
                    16,
                    58,
                    18,
                    3,
                    23,
                    0
                ],
                "published": "2025-01-23T16:58:18Z",
                "published_parsed": [
                    2025,
                    1,
                    23,
                    16,
                    58,
                    18,
                    3,
                    23,
                    0
                ],
                "title": "On the Reasoning Capacity of AI Models and How to Quantify It",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "On the Reasoning Capacity of AI Models and How to Quantify It"
                },
                "summary": "Recent advances in Large Language Models (LLMs) have intensified the debate\nsurrounding the fundamental nature of their reasoning capabilities. While\nachieving high performance on benchmarks such as GPQA and MMLU, these models\nexhibit limitations in more complex reasoning tasks, highlighting the need for\nmore rigorous evaluation methodologies. We propose a novel phenomenological\napproach that goes beyond traditional accuracy metrics to probe the underlying\nmechanisms of model behavior, establishing a framework that could broadly\nimpact how we analyze and understand AI systems. Using positional bias in\nmultiple-choice reasoning tasks as a case study, we demonstrate how systematic\nperturbations can reveal fundamental aspects of model decision-making. To\nanalyze these behaviors, we develop two complementary phenomenological models:\na Probabilistic Mixture Model (PMM) that decomposes model responses into\nreasoning, memorization, and guessing components and an Information-Theoretic\nConsistency (ITC) analysis that quantifies the relationship between model\nconfidence and strategy selection. Through controlled experiments on reasoning\nbenchmarks, we show that true reasoning remains challenging for current models,\nwith apparent success often relying on sophisticated combinations of\nmemorization and pattern matching rather than genuine logical deduction. More\nfundamentally, we demonstrate that accuracy alone often overstates a model's\nreasoning abilities, as model behavior can be characterized through underlying\nmechanisms in the phase space of cognitive strategies, revealing how models\ndynamically balance different approaches when responding to queries. This\nframework enables quantitative criteria for real-world deployments, allowing\napplications to specify reliability thresholds based on strategy distributions\nrather than aggregate performance metrics.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advances in Large Language Models (LLMs) have intensified the debate\nsurrounding the fundamental nature of their reasoning capabilities. While\nachieving high performance on benchmarks such as GPQA and MMLU, these models\nexhibit limitations in more complex reasoning tasks, highlighting the need for\nmore rigorous evaluation methodologies. We propose a novel phenomenological\napproach that goes beyond traditional accuracy metrics to probe the underlying\nmechanisms of model behavior, establishing a framework that could broadly\nimpact how we analyze and understand AI systems. Using positional bias in\nmultiple-choice reasoning tasks as a case study, we demonstrate how systematic\nperturbations can reveal fundamental aspects of model decision-making. To\nanalyze these behaviors, we develop two complementary phenomenological models:\na Probabilistic Mixture Model (PMM) that decomposes model responses into\nreasoning, memorization, and guessing components and an Information-Theoretic\nConsistency (ITC) analysis that quantifies the relationship between model\nconfidence and strategy selection. Through controlled experiments on reasoning\nbenchmarks, we show that true reasoning remains challenging for current models,\nwith apparent success often relying on sophisticated combinations of\nmemorization and pattern matching rather than genuine logical deduction. More\nfundamentally, we demonstrate that accuracy alone often overstates a model's\nreasoning abilities, as model behavior can be characterized through underlying\nmechanisms in the phase space of cognitive strategies, revealing how models\ndynamically balance different approaches when responding to queries. This\nframework enables quantitative criteria for real-world deployments, allowing\napplications to specify reliability thresholds based on strategy distributions\nrather than aggregate performance metrics."
                },
                "authors": [
                    {
                        "name": "Santosh Kumar Radha"
                    },
                    {
                        "name": "Oktay Goktas"
                    }
                ],
                "author_detail": {
                    "name": "Oktay Goktas"
                },
                "author": "Oktay Goktas",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.13833v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.13833v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.13831v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.13831v1",
                "updated": "2025-01-23T16:54:27Z",
                "updated_parsed": [
                    2025,
                    1,
                    23,
                    16,
                    54,
                    27,
                    3,
                    23,
                    0
                ],
                "published": "2025-01-23T16:54:27Z",
                "published_parsed": [
                    2025,
                    1,
                    23,
                    16,
                    54,
                    27,
                    3,
                    23,
                    0
                ],
                "title": "Predicting Compact Phrasal Rewrites with Large Language Models for ASR\n  Post Editing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Predicting Compact Phrasal Rewrites with Large Language Models for ASR\n  Post Editing"
                },
                "summary": "Large Language Models (LLMs) excel at rewriting tasks such as text style\ntransfer and grammatical error correction. While there is considerable overlap\nbetween the inputs and outputs in these tasks, the decoding cost still\nincreases with output length, regardless of the amount of overlap. By\nleveraging the overlap between the input and the output, Kaneko and Okazaki\n(2023) proposed model-agnostic edit span representations to compress the\nrewrites to save computation. They reported an output length reduction rate of\nnearly 80% with minimal accuracy impact in four rewriting tasks. In this paper,\nwe propose alternative edit phrase representations inspired by phrase-based\nstatistical machine translation. We systematically compare our phrasal\nrepresentations with their span representations. We apply the LLM rewriting\nmodel to the task of Automatic Speech Recognition (ASR) post editing and show\nthat our target-phrase-only edit representation has the best\nefficiency-accuracy trade-off. On the LibriSpeech test set, our method closes\n50-60% of the WER gap between the edit span model and the full rewrite model\nwhile losing only 10-20% of the length reduction rate of the edit span model.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) excel at rewriting tasks such as text style\ntransfer and grammatical error correction. While there is considerable overlap\nbetween the inputs and outputs in these tasks, the decoding cost still\nincreases with output length, regardless of the amount of overlap. By\nleveraging the overlap between the input and the output, Kaneko and Okazaki\n(2023) proposed model-agnostic edit span representations to compress the\nrewrites to save computation. They reported an output length reduction rate of\nnearly 80% with minimal accuracy impact in four rewriting tasks. In this paper,\nwe propose alternative edit phrase representations inspired by phrase-based\nstatistical machine translation. We systematically compare our phrasal\nrepresentations with their span representations. We apply the LLM rewriting\nmodel to the task of Automatic Speech Recognition (ASR) post editing and show\nthat our target-phrase-only edit representation has the best\nefficiency-accuracy trade-off. On the LibriSpeech test set, our method closes\n50-60% of the WER gap between the edit span model and the full rewrite model\nwhile losing only 10-20% of the length reduction rate of the edit span model."
                },
                "authors": [
                    {
                        "name": "Hao Zhang"
                    },
                    {
                        "name": "Felix Stahlberg"
                    },
                    {
                        "name": "Shankar Kumar"
                    }
                ],
                "author_detail": {
                    "name": "Shankar Kumar"
                },
                "author": "Shankar Kumar",
                "arxiv_comment": "accepted by ICASSP 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.13831v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.13831v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.13824v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.13824v1",
                "updated": "2025-01-23T16:45:51Z",
                "updated_parsed": [
                    2025,
                    1,
                    23,
                    16,
                    45,
                    51,
                    3,
                    23,
                    0
                ],
                "published": "2025-01-23T16:45:51Z",
                "published_parsed": [
                    2025,
                    1,
                    23,
                    16,
                    45,
                    51,
                    3,
                    23,
                    0
                ],
                "title": "Hallucinations Can Improve Large Language Models in Drug Discovery",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Hallucinations Can Improve Large Language Models in Drug Discovery"
                },
                "summary": "Concerns about hallucinations in Large Language Models (LLMs) have been\nraised by researchers, yet their potential in areas where creativity is vital,\nsuch as drug discovery, merits exploration. In this paper, we come up with the\nhypothesis that hallucinations can improve LLMs in drug discovery. To verify\nthis hypothesis, we use LLMs to describe the SMILES string of molecules in\nnatural language and then incorporate these descriptions as part of the prompt\nto address specific tasks in drug discovery. Evaluated on seven LLMs and five\nclassification tasks, our findings confirm the hypothesis: LLMs can achieve\nbetter performance with text containing hallucinations. Notably, Llama-3.1-8B\nachieves an 18.35% gain in ROC-AUC compared to the baseline without\nhallucination. Furthermore, hallucinations generated by GPT-4o provide the most\nconsistent improvements across models. Additionally, we conduct empirical\nanalyses and a case study to investigate key factors affecting performance and\nthe underlying reasons. Our research sheds light on the potential use of\nhallucinations for LLMs and offers new perspectives for future research\nleveraging LLMs in drug discovery.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Concerns about hallucinations in Large Language Models (LLMs) have been\nraised by researchers, yet their potential in areas where creativity is vital,\nsuch as drug discovery, merits exploration. In this paper, we come up with the\nhypothesis that hallucinations can improve LLMs in drug discovery. To verify\nthis hypothesis, we use LLMs to describe the SMILES string of molecules in\nnatural language and then incorporate these descriptions as part of the prompt\nto address specific tasks in drug discovery. Evaluated on seven LLMs and five\nclassification tasks, our findings confirm the hypothesis: LLMs can achieve\nbetter performance with text containing hallucinations. Notably, Llama-3.1-8B\nachieves an 18.35% gain in ROC-AUC compared to the baseline without\nhallucination. Furthermore, hallucinations generated by GPT-4o provide the most\nconsistent improvements across models. Additionally, we conduct empirical\nanalyses and a case study to investigate key factors affecting performance and\nthe underlying reasons. Our research sheds light on the potential use of\nhallucinations for LLMs and offers new perspectives for future research\nleveraging LLMs in drug discovery."
                },
                "authors": [
                    {
                        "name": "Shuzhou Yuan"
                    },
                    {
                        "name": "Michael Färber"
                    }
                ],
                "author_detail": {
                    "name": "Michael Färber"
                },
                "author": "Michael Färber",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.13824v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.13824v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.13816v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.13816v1",
                "updated": "2025-01-23T16:37:44Z",
                "updated_parsed": [
                    2025,
                    1,
                    23,
                    16,
                    37,
                    44,
                    3,
                    23,
                    0
                ],
                "published": "2025-01-23T16:37:44Z",
                "published_parsed": [
                    2025,
                    1,
                    23,
                    16,
                    37,
                    44,
                    3,
                    23,
                    0
                ],
                "title": "Large Language Model driven Policy Exploration for Recommender Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Model driven Policy Exploration for Recommender Systems"
                },
                "summary": "Recent advancements in Recommender Systems (RS) have incorporated\nReinforcement Learning (RL), framing the recommendation as a Markov Decision\nProcess (MDP). However, offline RL policies trained on static user data are\nvulnerable to distribution shift when deployed in dynamic online environments.\nAdditionally, excessive focus on exploiting short-term relevant items can\nhinder exploration, leading to suboptimal recommendations and negatively\nimpacting long-term user gains. Online RL-based RS also face challenges in\nproduction deployment, due to the risks of exposing users to untrained or\nunstable policies. Large Language Models (LLMs) offer a promising solution to\nmimic user objectives and preferences for pre-training policies offline to\nenhance the initial recommendations in online settings. Effectively managing\ndistribution shift and balancing exploration are crucial for improving RL-based\nRS, especially when leveraging LLM-based pre-training. To address these\nchallenges, we propose an Interaction-Augmented Learned Policy (iALP) that\nutilizes user preferences distilled from an LLM. Our approach involves\nprompting the LLM with user states to extract item preferences, learning\nrewards based on feedback, and updating the RL policy using an actor-critic\nframework. Furthermore, to deploy iALP in an online scenario, we introduce an\nadaptive variant, A-iALP, that implements a simple fine-tuning strategy\n(A-iALP$_{ft}$), and an adaptive approach (A-iALP$_{ap}$) designed to mitigate\nissues with compromised policies and limited exploration. Experiments across\nthree simulated environments demonstrate that A-iALP introduces substantial\nperformance improvements",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advancements in Recommender Systems (RS) have incorporated\nReinforcement Learning (RL), framing the recommendation as a Markov Decision\nProcess (MDP). However, offline RL policies trained on static user data are\nvulnerable to distribution shift when deployed in dynamic online environments.\nAdditionally, excessive focus on exploiting short-term relevant items can\nhinder exploration, leading to suboptimal recommendations and negatively\nimpacting long-term user gains. Online RL-based RS also face challenges in\nproduction deployment, due to the risks of exposing users to untrained or\nunstable policies. Large Language Models (LLMs) offer a promising solution to\nmimic user objectives and preferences for pre-training policies offline to\nenhance the initial recommendations in online settings. Effectively managing\ndistribution shift and balancing exploration are crucial for improving RL-based\nRS, especially when leveraging LLM-based pre-training. To address these\nchallenges, we propose an Interaction-Augmented Learned Policy (iALP) that\nutilizes user preferences distilled from an LLM. Our approach involves\nprompting the LLM with user states to extract item preferences, learning\nrewards based on feedback, and updating the RL policy using an actor-critic\nframework. Furthermore, to deploy iALP in an online scenario, we introduce an\nadaptive variant, A-iALP, that implements a simple fine-tuning strategy\n(A-iALP$_{ft}$), and an adaptive approach (A-iALP$_{ap}$) designed to mitigate\nissues with compromised policies and limited exploration. Experiments across\nthree simulated environments demonstrate that A-iALP introduces substantial\nperformance improvements"
                },
                "authors": [
                    {
                        "name": "Jie Wang"
                    },
                    {
                        "name": "Alexandros Karatzoglou"
                    },
                    {
                        "name": "Ioannis Arapakis"
                    },
                    {
                        "name": "Joemon M. Jose"
                    }
                ],
                "author_detail": {
                    "name": "Joemon M. Jose"
                },
                "author": "Joemon M. Jose",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.13816v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.13816v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.12461v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.12461v2",
                "updated": "2025-01-23T16:31:51Z",
                "updated_parsed": [
                    2025,
                    1,
                    23,
                    16,
                    31,
                    51,
                    3,
                    23,
                    0
                ],
                "published": "2025-01-21T19:17:46Z",
                "published_parsed": [
                    2025,
                    1,
                    21,
                    19,
                    17,
                    46,
                    1,
                    21,
                    0
                ],
                "title": "Empowering AIOps: Leveraging Large Language Models for IT Operations\n  Management",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Empowering AIOps: Leveraging Large Language Models for IT Operations\n  Management"
                },
                "summary": "The integration of Artificial Intelligence (AI) into IT Operations Management\n(ITOM), commonly referred to as AIOps, offers substantial potential for\nautomating workflows, enhancing efficiency, and supporting informed\ndecision-making. However, implementing AI within IT operations is not without\nits challenges, including issues related to data quality, the complexity of IT\nenvironments, and skill gaps within teams. The advent of Large Language Models\n(LLMs) presents an opportunity to address some of these challenges,\nparticularly through their advanced natural language understanding\ncapabilities. These features enable organizations to process and analyze vast\namounts of unstructured data, such as system logs, incident reports, and\ntechnical documentation. This ability aligns with the motivation behind our\nresearch, where we aim to integrate traditional predictive machine learning\nmodels with generative AI technologies like LLMs. By combining these\napproaches, we propose innovative methods to tackle persistent challenges in\nAIOps and enhance the capabilities of IT operations management.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The integration of Artificial Intelligence (AI) into IT Operations Management\n(ITOM), commonly referred to as AIOps, offers substantial potential for\nautomating workflows, enhancing efficiency, and supporting informed\ndecision-making. However, implementing AI within IT operations is not without\nits challenges, including issues related to data quality, the complexity of IT\nenvironments, and skill gaps within teams. The advent of Large Language Models\n(LLMs) presents an opportunity to address some of these challenges,\nparticularly through their advanced natural language understanding\ncapabilities. These features enable organizations to process and analyze vast\namounts of unstructured data, such as system logs, incident reports, and\ntechnical documentation. This ability aligns with the motivation behind our\nresearch, where we aim to integrate traditional predictive machine learning\nmodels with generative AI technologies like LLMs. By combining these\napproaches, we propose innovative methods to tackle persistent challenges in\nAIOps and enhance the capabilities of IT operations management."
                },
                "authors": [
                    {
                        "name": "Arthur Vitui"
                    },
                    {
                        "name": "Tse-Hsun Chen"
                    }
                ],
                "author_detail": {
                    "name": "Tse-Hsun Chen"
                },
                "author": "Tse-Hsun Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.12461v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.12461v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.13802v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.13802v1",
                "updated": "2025-01-23T16:21:15Z",
                "updated_parsed": [
                    2025,
                    1,
                    23,
                    16,
                    21,
                    15,
                    3,
                    23,
                    0
                ],
                "published": "2025-01-23T16:21:15Z",
                "published_parsed": [
                    2025,
                    1,
                    23,
                    16,
                    21,
                    15,
                    3,
                    23,
                    0
                ],
                "title": "Enhancing LLMs for Governance with Human Oversight: Evaluating and\n  Aligning LLMs on Expert Classification of Climate Misinformation for\n  Detecting False or Misleading Claims about Climate Change",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Enhancing LLMs for Governance with Human Oversight: Evaluating and\n  Aligning LLMs on Expert Classification of Climate Misinformation for\n  Detecting False or Misleading Claims about Climate Change"
                },
                "summary": "Climate misinformation is a problem that has the potential to be\nsubstantially aggravated by the development of Large Language Models (LLMs). In\nthis study we evaluate the potential for LLMs to be part of the solution for\nmitigating online dis/misinformation rather than the problem. Employing a\npublic expert annotated dataset and a curated sample of social media content we\nevaluate the performance of proprietary vs. open source LLMs on climate\nmisinformation classification task, comparing them to existing climate-focused\ncomputer-assisted tools and expert assessments. Results show (1)\nstate-of-the-art (SOTA) open-source models substantially under-perform in\nclassifying climate misinformation compared to proprietary models, (2) existing\nclimate-focused computer-assisted tools leveraging expert-annotated datasets\ncontinues to outperform many of proprietary models, including GPT-4o, and (3)\ndemonstrate the efficacy and generalizability of fine-tuning GPT-3.5-turbo on\nexpert annotated dataset in classifying claims about climate change at the\nequivalency of climate change experts with over 20 years of experience in\nclimate communication. These findings highlight 1) the importance of\nincorporating human-oversight, such as incorporating expert-annotated datasets\nin training LLMs, for governance tasks that require subject-matter expertise\nlike classifying climate misinformation, and 2) the potential for LLMs in\nfacilitating civil society organizations to engage in various governance tasks\nsuch as classifying false or misleading claims in domains beyond climate change\nsuch as politics and health science.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Climate misinformation is a problem that has the potential to be\nsubstantially aggravated by the development of Large Language Models (LLMs). In\nthis study we evaluate the potential for LLMs to be part of the solution for\nmitigating online dis/misinformation rather than the problem. Employing a\npublic expert annotated dataset and a curated sample of social media content we\nevaluate the performance of proprietary vs. open source LLMs on climate\nmisinformation classification task, comparing them to existing climate-focused\ncomputer-assisted tools and expert assessments. Results show (1)\nstate-of-the-art (SOTA) open-source models substantially under-perform in\nclassifying climate misinformation compared to proprietary models, (2) existing\nclimate-focused computer-assisted tools leveraging expert-annotated datasets\ncontinues to outperform many of proprietary models, including GPT-4o, and (3)\ndemonstrate the efficacy and generalizability of fine-tuning GPT-3.5-turbo on\nexpert annotated dataset in classifying claims about climate change at the\nequivalency of climate change experts with over 20 years of experience in\nclimate communication. These findings highlight 1) the importance of\nincorporating human-oversight, such as incorporating expert-annotated datasets\nin training LLMs, for governance tasks that require subject-matter expertise\nlike classifying climate misinformation, and 2) the potential for LLMs in\nfacilitating civil society organizations to engage in various governance tasks\nsuch as classifying false or misleading claims in domains beyond climate change\nsuch as politics and health science."
                },
                "authors": [
                    {
                        "name": "Mowafak Allaham"
                    },
                    {
                        "name": "Ayse D. Lokmanoglu"
                    },
                    {
                        "name": "Sol P. Hart"
                    },
                    {
                        "name": "Erik C. Nisbet"
                    }
                ],
                "author_detail": {
                    "name": "Erik C. Nisbet"
                },
                "author": "Erik C. Nisbet",
                "arxiv_comment": "Accepted to the AI Governance Workshop at AAAI 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.13802v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.13802v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.10362v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.10362v3",
                "updated": "2025-01-23T16:11:11Z",
                "updated_parsed": [
                    2025,
                    1,
                    23,
                    16,
                    11,
                    11,
                    3,
                    23,
                    0
                ],
                "published": "2024-11-15T17:11:13Z",
                "published_parsed": [
                    2024,
                    11,
                    15,
                    17,
                    11,
                    13,
                    4,
                    320,
                    0
                ],
                "title": "Interactive Cycle Model: The Linkage Combination among Automatic Speech\n  Recognition, Large Language Models and Smart Glasses",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Interactive Cycle Model: The Linkage Combination among Automatic Speech\n  Recognition, Large Language Models and Smart Glasses"
                },
                "summary": "This research proposes the interaction loop model \"ASR-LLMs-Smart Glasses\",\nwhich model combines automatic speech recognition, large language model and\nsmart glasses to facilitate seamless human-computer interaction. And the\nmethodology of this research involves decomposing the interaction process into\ndifferent stages and elements. Speech is captured and processed by ASR, then\nanalyzed and interpreted by LLMs. The results are then transmitted to smart\nglasses for display. The feedback loop is complete when the user interacts with\nthe displayed data. Mathematical formulas are used to quantify the performance\nof the model that revolves around core evaluation points: accuracy, coherence,\nand latency during ASR speech-to-text conversion. The research results are\nprovided theoretically to test and evaluate the feasibility and performance of\nthe model. Detailed architectural details and experimental process have been\nuploaded to Github, the link\nis:https://github.com/brucewang123456789/GeniusTrail.git.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This research proposes the interaction loop model \"ASR-LLMs-Smart Glasses\",\nwhich model combines automatic speech recognition, large language model and\nsmart glasses to facilitate seamless human-computer interaction. And the\nmethodology of this research involves decomposing the interaction process into\ndifferent stages and elements. Speech is captured and processed by ASR, then\nanalyzed and interpreted by LLMs. The results are then transmitted to smart\nglasses for display. The feedback loop is complete when the user interacts with\nthe displayed data. Mathematical formulas are used to quantify the performance\nof the model that revolves around core evaluation points: accuracy, coherence,\nand latency during ASR speech-to-text conversion. The research results are\nprovided theoretically to test and evaluate the feasibility and performance of\nthe model. Detailed architectural details and experimental process have been\nuploaded to Github, the link\nis:https://github.com/brucewang123456789/GeniusTrail.git."
                },
                "authors": [
                    {
                        "name": "Libo Wang"
                    }
                ],
                "author_detail": {
                    "name": "Libo Wang"
                },
                "author": "Libo Wang",
                "arxiv_comment": "OpenReview submitted. 10 pages of text and 2 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.10362v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.10362v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.13778v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.13778v1",
                "updated": "2025-01-23T15:55:07Z",
                "updated_parsed": [
                    2025,
                    1,
                    23,
                    15,
                    55,
                    7,
                    3,
                    23,
                    0
                ],
                "published": "2025-01-23T15:55:07Z",
                "published_parsed": [
                    2025,
                    1,
                    23,
                    15,
                    55,
                    7,
                    3,
                    23,
                    0
                ],
                "title": "Explainable XR: Understanding User Behaviors of XR Environments using\n  LLM-assisted Analytics Framework",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Explainable XR: Understanding User Behaviors of XR Environments using\n  LLM-assisted Analytics Framework"
                },
                "summary": "We present Explainable XR, an end-to-end framework for analyzing user\nbehavior in diverse eXtended Reality (XR) environments by leveraging Large\nLanguage Models (LLMs) for data interpretation assistance. Existing XR user\nanalytics frameworks face challenges in handling cross-virtuality - AR, VR, MR\n- transitions, multi-user collaborative application scenarios, and the\ncomplexity of multimodal data. Explainable XR addresses these challenges by\nproviding a virtuality-agnostic solution for the collection, analysis, and\nvisualization of immersive sessions. We propose three main components in our\nframework: (1) A novel user data recording schema, called User Action\nDescriptor (UAD), that can capture the users' multimodal actions, along with\ntheir intents and the contexts; (2) a platform-agnostic XR session recorder,\nand (3) a visual analytics interface that offers LLM-assisted insights tailored\nto the analysts' perspectives, facilitating the exploration and analysis of the\nrecorded XR session data. We demonstrate the versatility of Explainable XR by\ndemonstrating five use-case scenarios, in both individual and collaborative XR\napplications across virtualities. Our technical evaluation and user studies\nshow that Explainable XR provides a highly usable analytics solution for\nunderstanding user actions and delivering multifaceted, actionable insights\ninto user behaviors in immersive environments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present Explainable XR, an end-to-end framework for analyzing user\nbehavior in diverse eXtended Reality (XR) environments by leveraging Large\nLanguage Models (LLMs) for data interpretation assistance. Existing XR user\nanalytics frameworks face challenges in handling cross-virtuality - AR, VR, MR\n- transitions, multi-user collaborative application scenarios, and the\ncomplexity of multimodal data. Explainable XR addresses these challenges by\nproviding a virtuality-agnostic solution for the collection, analysis, and\nvisualization of immersive sessions. We propose three main components in our\nframework: (1) A novel user data recording schema, called User Action\nDescriptor (UAD), that can capture the users' multimodal actions, along with\ntheir intents and the contexts; (2) a platform-agnostic XR session recorder,\nand (3) a visual analytics interface that offers LLM-assisted insights tailored\nto the analysts' perspectives, facilitating the exploration and analysis of the\nrecorded XR session data. We demonstrate the versatility of Explainable XR by\ndemonstrating five use-case scenarios, in both individual and collaborative XR\napplications across virtualities. Our technical evaluation and user studies\nshow that Explainable XR provides a highly usable analytics solution for\nunderstanding user actions and delivering multifaceted, actionable insights\ninto user behaviors in immersive environments."
                },
                "authors": [
                    {
                        "name": "Yoonsang Kim"
                    },
                    {
                        "name": "Zainab Aamir"
                    },
                    {
                        "name": "Mithilesh Singh"
                    },
                    {
                        "name": "Saeed Boorboor"
                    },
                    {
                        "name": "Klaus Mueller"
                    },
                    {
                        "name": "Arie E. Kaufman"
                    }
                ],
                "author_detail": {
                    "name": "Arie E. Kaufman"
                },
                "author": "Arie E. Kaufman",
                "arxiv_comment": "11 pages, 8 figures. This is the author's version of the article that\n  has been accepted for publication in IEEE Transactions on Visualization and\n  Computer Graphics",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.13778v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.13778v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.13773v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.13773v1",
                "updated": "2025-01-23T15:52:34Z",
                "updated_parsed": [
                    2025,
                    1,
                    23,
                    15,
                    52,
                    34,
                    3,
                    23,
                    0
                ],
                "published": "2025-01-23T15:52:34Z",
                "published_parsed": [
                    2025,
                    1,
                    23,
                    15,
                    52,
                    34,
                    3,
                    23,
                    0
                ],
                "title": "Do Large Language Models Truly Understand Geometric Structures?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Do Large Language Models Truly Understand Geometric Structures?"
                },
                "summary": "Geometric ability is a significant challenge for large language models (LLMs)\ndue to the need for advanced spatial comprehension and abstract thinking.\nExisting datasets primarily evaluate LLMs on their final answers, but they\ncannot truly measure their true understanding of geometric structures, as LLMs\ncan arrive at correct answers by coincidence. To fill this gap, we introduce\nthe GeomRel dataset, designed to evaluate LLMs' understanding of geometric\nstructures by isolating the core step of geometric relationship identification\nin problem-solving. Using this benchmark, we conduct thorough evaluations of\ndiverse LLMs and identify key limitations in understanding geometric\nstructures. We further propose the Geometry Chain-of-Thought (GeoCoT) method,\nwhich enhances LLMs' ability to identify geometric relationships, resulting in\nsignificant performance improvements.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Geometric ability is a significant challenge for large language models (LLMs)\ndue to the need for advanced spatial comprehension and abstract thinking.\nExisting datasets primarily evaluate LLMs on their final answers, but they\ncannot truly measure their true understanding of geometric structures, as LLMs\ncan arrive at correct answers by coincidence. To fill this gap, we introduce\nthe GeomRel dataset, designed to evaluate LLMs' understanding of geometric\nstructures by isolating the core step of geometric relationship identification\nin problem-solving. Using this benchmark, we conduct thorough evaluations of\ndiverse LLMs and identify key limitations in understanding geometric\nstructures. We further propose the Geometry Chain-of-Thought (GeoCoT) method,\nwhich enhances LLMs' ability to identify geometric relationships, resulting in\nsignificant performance improvements."
                },
                "authors": [
                    {
                        "name": "Xiaofeng Wang"
                    },
                    {
                        "name": "Yiming Wang"
                    },
                    {
                        "name": "Wenhong Zhu"
                    },
                    {
                        "name": "Rui Wang"
                    }
                ],
                "author_detail": {
                    "name": "Rui Wang"
                },
                "author": "Rui Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.13773v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.13773v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.13772v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.13772v1",
                "updated": "2025-01-23T15:51:38Z",
                "updated_parsed": [
                    2025,
                    1,
                    23,
                    15,
                    51,
                    38,
                    3,
                    23,
                    0
                ],
                "published": "2025-01-23T15:51:38Z",
                "published_parsed": [
                    2025,
                    1,
                    23,
                    15,
                    51,
                    38,
                    3,
                    23,
                    0
                ],
                "title": "Tune In, Act Up: Exploring the Impact of Audio Modality-Specific Edits\n  on Large Audio Language Models in Jailbreak",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Tune In, Act Up: Exploring the Impact of Audio Modality-Specific Edits\n  on Large Audio Language Models in Jailbreak"
                },
                "summary": "Large Language Models (LLMs) demonstrate remarkable zero-shot performance\nacross various natural language processing tasks. The integration of multimodal\nencoders extends their capabilities, enabling the development of Multimodal\nLarge Language Models that process vision, audio, and text. However, these\ncapabilities also raise significant security concerns, as these models can be\nmanipulated to generate harmful or inappropriate content through jailbreak.\nWhile extensive research explores the impact of modality-specific input edits\non text-based LLMs and Large Vision-Language Models in jailbreak, the effects\nof audio-specific edits on Large Audio-Language Models (LALMs) remain\nunderexplored. Hence, this paper addresses this gap by investigating how\naudio-specific edits influence LALMs inference regarding jailbreak. We\nintroduce the Audio Editing Toolbox (AET), which enables audio-modality edits\nsuch as tone adjustment, word emphasis, and noise injection, and the Edited\nAudio Datasets (EADs), a comprehensive audio jailbreak benchmark. We also\nconduct extensive evaluations of state-of-the-art LALMs to assess their\nrobustness under different audio edits. This work lays the groundwork for\nfuture explorations on audio-modality interactions in LALMs security.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) demonstrate remarkable zero-shot performance\nacross various natural language processing tasks. The integration of multimodal\nencoders extends their capabilities, enabling the development of Multimodal\nLarge Language Models that process vision, audio, and text. However, these\ncapabilities also raise significant security concerns, as these models can be\nmanipulated to generate harmful or inappropriate content through jailbreak.\nWhile extensive research explores the impact of modality-specific input edits\non text-based LLMs and Large Vision-Language Models in jailbreak, the effects\nof audio-specific edits on Large Audio-Language Models (LALMs) remain\nunderexplored. Hence, this paper addresses this gap by investigating how\naudio-specific edits influence LALMs inference regarding jailbreak. We\nintroduce the Audio Editing Toolbox (AET), which enables audio-modality edits\nsuch as tone adjustment, word emphasis, and noise injection, and the Edited\nAudio Datasets (EADs), a comprehensive audio jailbreak benchmark. We also\nconduct extensive evaluations of state-of-the-art LALMs to assess their\nrobustness under different audio edits. This work lays the groundwork for\nfuture explorations on audio-modality interactions in LALMs security."
                },
                "authors": [
                    {
                        "name": "Erjia Xiao"
                    },
                    {
                        "name": "Hao Cheng"
                    },
                    {
                        "name": "Jing Shao"
                    },
                    {
                        "name": "Jinhao Duan"
                    },
                    {
                        "name": "Kaidi Xu"
                    },
                    {
                        "name": "Le Yang"
                    },
                    {
                        "name": "Jindong Gu"
                    },
                    {
                        "name": "Renjing Xu"
                    }
                ],
                "author_detail": {
                    "name": "Renjing Xu"
                },
                "author": "Renjing Xu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.13772v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.13772v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SD",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.14804v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.14804v2",
                "updated": "2025-01-23T15:47:09Z",
                "updated_parsed": [
                    2025,
                    1,
                    23,
                    15,
                    47,
                    9,
                    3,
                    23,
                    0
                ],
                "published": "2024-05-23T17:13:50Z",
                "published_parsed": [
                    2024,
                    5,
                    23,
                    17,
                    13,
                    50,
                    3,
                    144,
                    0
                ],
                "title": "Can LLMs Solve longer Math Word Problems Better?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Can LLMs Solve longer Math Word Problems Better?"
                },
                "summary": "Math Word Problems (MWPs) play a vital role in assessing the capabilities of\nLarge Language Models (LLMs), yet current research primarily focuses on\nquestions with concise contexts. The impact of longer contexts on mathematical\nreasoning remains under-explored. This study pioneers the investigation of\nContext Length Generalizability (CoLeG), which refers to the ability of LLMs to\nsolve MWPs with extended narratives. We introduce Extended Grade-School Math\n(E-GSM), a collection of MWPs featuring lengthy narratives, and propose two\nnovel metrics to evaluate the efficacy and resilience of LLMs in tackling these\nproblems. Our analysis of existing zero-shot prompting techniques with\nproprietary LLMs along with open-source LLMs reveals a general deficiency in\nCoLeG. To alleviate these issues, we propose tailored approaches for different\ncategories of LLMs. For proprietary LLMs, we introduce a new instructional\nprompt designed to mitigate the impact of long contexts. For open-source LLMs,\nwe develop a novel auxiliary task for fine-tuning to enhance CoLeG. Our\ncomprehensive results demonstrate the effectiveness of our proposed methods,\nshowing improved performance on E-GSM. Additionally, we conduct an in-depth\nanalysis to differentiate the effects of semantic understanding and reasoning\nefficacy, showing that our methods improves the latter. We also establish the\ngeneralizability of our methods across several other MWP benchmarks. Our\nfindings highlight the limitations of current LLMs and offer practical\nsolutions correspondingly, paving the way for further exploration of model\ngeneralizability and training methodologies.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Math Word Problems (MWPs) play a vital role in assessing the capabilities of\nLarge Language Models (LLMs), yet current research primarily focuses on\nquestions with concise contexts. The impact of longer contexts on mathematical\nreasoning remains under-explored. This study pioneers the investigation of\nContext Length Generalizability (CoLeG), which refers to the ability of LLMs to\nsolve MWPs with extended narratives. We introduce Extended Grade-School Math\n(E-GSM), a collection of MWPs featuring lengthy narratives, and propose two\nnovel metrics to evaluate the efficacy and resilience of LLMs in tackling these\nproblems. Our analysis of existing zero-shot prompting techniques with\nproprietary LLMs along with open-source LLMs reveals a general deficiency in\nCoLeG. To alleviate these issues, we propose tailored approaches for different\ncategories of LLMs. For proprietary LLMs, we introduce a new instructional\nprompt designed to mitigate the impact of long contexts. For open-source LLMs,\nwe develop a novel auxiliary task for fine-tuning to enhance CoLeG. Our\ncomprehensive results demonstrate the effectiveness of our proposed methods,\nshowing improved performance on E-GSM. Additionally, we conduct an in-depth\nanalysis to differentiate the effects of semantic understanding and reasoning\nefficacy, showing that our methods improves the latter. We also establish the\ngeneralizability of our methods across several other MWP benchmarks. Our\nfindings highlight the limitations of current LLMs and offer practical\nsolutions correspondingly, paving the way for further exploration of model\ngeneralizability and training methodologies."
                },
                "authors": [
                    {
                        "name": "Xin Xu"
                    },
                    {
                        "name": "Tong Xiao"
                    },
                    {
                        "name": "Zitong Chao"
                    },
                    {
                        "name": "Zhenya Huang"
                    },
                    {
                        "name": "Can Yang"
                    },
                    {
                        "name": "Yang Wang"
                    }
                ],
                "author_detail": {
                    "name": "Yang Wang"
                },
                "author": "Yang Wang",
                "arxiv_comment": "Accepted to ICLR 2025",
                "arxiv_journal_ref": "International Conference on Learning Representations (ICLR 2025)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.14804v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.14804v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.13766v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.13766v1",
                "updated": "2025-01-23T15:46:43Z",
                "updated_parsed": [
                    2025,
                    1,
                    23,
                    15,
                    46,
                    43,
                    3,
                    23,
                    0
                ],
                "published": "2025-01-23T15:46:43Z",
                "published_parsed": [
                    2025,
                    1,
                    23,
                    15,
                    46,
                    43,
                    3,
                    23,
                    0
                ],
                "title": "UGMathBench: A Diverse and Dynamic Benchmark for Undergraduate-Level\n  Mathematical Reasoning with Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "UGMathBench: A Diverse and Dynamic Benchmark for Undergraduate-Level\n  Mathematical Reasoning with Large Language Models"
                },
                "summary": "Large Language Models (LLMs) have made significant strides in mathematical\nreasoning, underscoring the need for a comprehensive and fair evaluation of\ntheir capabilities. However, existing benchmarks often fall short, either\nlacking extensive coverage of undergraduate-level mathematical problems or\nprobably suffering from test-set contamination. To address these issues, we\nintroduce UGMathBench, a diverse and dynamic benchmark specifically designed\nfor evaluating undergraduate-level mathematical reasoning with LLMs.\nUGMathBench comprises 5,062 problems across 16 subjects and 111 topics,\nfeaturing 10 distinct answer types. Each problem includes three randomized\nversions, with additional versions planned for release as leading open-source\nLLMs become saturated in UGMathBench. Furthermore, we propose two key metrics:\neffective accuracy (EAcc), which measures the percentage of correctly solved\nproblems across all three versions, and reasoning gap ($\\Delta$), which\nassesses reasoning robustness by calculating the difference between the average\naccuracy across all versions and EAcc. Our extensive evaluation of 23 leading\nLLMs reveals that the highest EAcc achieved is 56.3\\% by OpenAI-o1-mini, with\nlarge $\\Delta$ values observed across different models. This highlights the\nneed for future research aimed at developing \"large reasoning models\" with high\nEAcc and $\\Delta = 0$. We anticipate that the release of UGMathBench, along\nwith its detailed evaluation codes, will serve as a valuable resource to\nadvance the development of LLMs in solving mathematical problems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have made significant strides in mathematical\nreasoning, underscoring the need for a comprehensive and fair evaluation of\ntheir capabilities. However, existing benchmarks often fall short, either\nlacking extensive coverage of undergraduate-level mathematical problems or\nprobably suffering from test-set contamination. To address these issues, we\nintroduce UGMathBench, a diverse and dynamic benchmark specifically designed\nfor evaluating undergraduate-level mathematical reasoning with LLMs.\nUGMathBench comprises 5,062 problems across 16 subjects and 111 topics,\nfeaturing 10 distinct answer types. Each problem includes three randomized\nversions, with additional versions planned for release as leading open-source\nLLMs become saturated in UGMathBench. Furthermore, we propose two key metrics:\neffective accuracy (EAcc), which measures the percentage of correctly solved\nproblems across all three versions, and reasoning gap ($\\Delta$), which\nassesses reasoning robustness by calculating the difference between the average\naccuracy across all versions and EAcc. Our extensive evaluation of 23 leading\nLLMs reveals that the highest EAcc achieved is 56.3\\% by OpenAI-o1-mini, with\nlarge $\\Delta$ values observed across different models. This highlights the\nneed for future research aimed at developing \"large reasoning models\" with high\nEAcc and $\\Delta = 0$. We anticipate that the release of UGMathBench, along\nwith its detailed evaluation codes, will serve as a valuable resource to\nadvance the development of LLMs in solving mathematical problems."
                },
                "authors": [
                    {
                        "name": "Xin Xu"
                    },
                    {
                        "name": "Jiaxin Zhang"
                    },
                    {
                        "name": "Tianhao Chen"
                    },
                    {
                        "name": "Zitong Chao"
                    },
                    {
                        "name": "Jishan Hu"
                    },
                    {
                        "name": "Can Yang"
                    }
                ],
                "author_detail": {
                    "name": "Can Yang"
                },
                "author": "Can Yang",
                "arxiv_comment": "Accepted to ICLR 2025",
                "arxiv_journal_ref": "International Conference on Learning Representations (ICLR 2025)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.13766v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.13766v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.11380v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.11380v2",
                "updated": "2025-01-23T15:44:07Z",
                "updated_parsed": [
                    2025,
                    1,
                    23,
                    15,
                    44,
                    7,
                    3,
                    23,
                    0
                ],
                "published": "2024-06-17T09:56:46Z",
                "published_parsed": [
                    2024,
                    6,
                    17,
                    9,
                    56,
                    46,
                    0,
                    169,
                    0
                ],
                "title": "Evaluating LLMs for Quotation Attribution in Literary Texts: A Case\n  Study of LLaMa3",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Evaluating LLMs for Quotation Attribution in Literary Texts: A Case\n  Study of LLaMa3"
                },
                "summary": "Large Language Models (LLMs) have shown promising results in a variety of\nliterary tasks, often using complex memorized details of narration and\nfictional characters. In this work, we evaluate the ability of Llama-3 at\nattributing utterances of direct-speech to their speaker in novels. The LLM\nshows impressive results on a corpus of 28 novels, surpassing published results\nwith ChatGPT and encoder-based baselines by a large margin. We then validate\nthese results by assessing the impact of book memorization and annotation\ncontamination. We found that these types of memorization do not explain the\nlarge performance gain, making Llama-3 the new state-of-the-art for quotation\nattribution in English literature. We release publicly our code and data.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have shown promising results in a variety of\nliterary tasks, often using complex memorized details of narration and\nfictional characters. In this work, we evaluate the ability of Llama-3 at\nattributing utterances of direct-speech to their speaker in novels. The LLM\nshows impressive results on a corpus of 28 novels, surpassing published results\nwith ChatGPT and encoder-based baselines by a large margin. We then validate\nthese results by assessing the impact of book memorization and annotation\ncontamination. We found that these types of memorization do not explain the\nlarge performance gain, making Llama-3 the new state-of-the-art for quotation\nattribution in English literature. We release publicly our code and data."
                },
                "authors": [
                    {
                        "name": "Gaspard Michel"
                    },
                    {
                        "name": "Elena V. Epure"
                    },
                    {
                        "name": "Romain Hennequin"
                    },
                    {
                        "name": "Christophe Cerisara"
                    }
                ],
                "author_detail": {
                    "name": "Christophe Cerisara"
                },
                "author": "Christophe Cerisara",
                "arxiv_comment": "NAACL 2025 Main Conference -- short paper",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.11380v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.11380v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2305.14109v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2305.14109v3",
                "updated": "2025-01-23T15:32:17Z",
                "updated_parsed": [
                    2025,
                    1,
                    23,
                    15,
                    32,
                    17,
                    3,
                    23,
                    0
                ],
                "published": "2023-05-23T14:31:52Z",
                "published_parsed": [
                    2023,
                    5,
                    23,
                    14,
                    31,
                    52,
                    1,
                    143,
                    0
                ],
                "title": "Combining Multi-Objective Bayesian Optimization with Reinforcement\n  Learning for TinyML",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Combining Multi-Objective Bayesian Optimization with Reinforcement\n  Learning for TinyML"
                },
                "summary": "Deploying deep neural networks (DNNs) on microcontrollers (TinyML) is a\ncommon trend to process the increasing amount of sensor data generated at the\nedge, but in practice, resource and latency constraints make it difficult to\nfind optimal DNN candidates. Neural architecture search (NAS) is an excellent\napproach to automate this search and can easily be combined with DNN\ncompression techniques commonly used in TinyML. However, many NAS techniques\nare not only computationally expensive, especially hyperparameter optimization\n(HPO), but also often focus on optimizing only a single objective, e.g.,\nmaximizing accuracy, without considering additional objectives such as memory\nrequirements or computational complexity of a DNN, which are key to making\ndeployment at the edge feasible. In this paper, we propose a novel NAS strategy\nfor TinyML based on multi-objective Bayesian optimization (MOBOpt) and an\nensemble of competing parametric policies trained using Augmented Random Search\n(ARS) reinforcement learning (RL) agents. Our methodology aims at efficiently\nfinding tradeoffs between a DNN's predictive accuracy, memory requirements on a\ngiven target system, and computational complexity. Our experiments show that we\nconsistently outperform existing MOBOpt approaches on different datasets and\narchitectures such as ResNet-18 and MobileNetV3.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Deploying deep neural networks (DNNs) on microcontrollers (TinyML) is a\ncommon trend to process the increasing amount of sensor data generated at the\nedge, but in practice, resource and latency constraints make it difficult to\nfind optimal DNN candidates. Neural architecture search (NAS) is an excellent\napproach to automate this search and can easily be combined with DNN\ncompression techniques commonly used in TinyML. However, many NAS techniques\nare not only computationally expensive, especially hyperparameter optimization\n(HPO), but also often focus on optimizing only a single objective, e.g.,\nmaximizing accuracy, without considering additional objectives such as memory\nrequirements or computational complexity of a DNN, which are key to making\ndeployment at the edge feasible. In this paper, we propose a novel NAS strategy\nfor TinyML based on multi-objective Bayesian optimization (MOBOpt) and an\nensemble of competing parametric policies trained using Augmented Random Search\n(ARS) reinforcement learning (RL) agents. Our methodology aims at efficiently\nfinding tradeoffs between a DNN's predictive accuracy, memory requirements on a\ngiven target system, and computational complexity. Our experiments show that we\nconsistently outperform existing MOBOpt approaches on different datasets and\narchitectures such as ResNet-18 and MobileNetV3."
                },
                "authors": [
                    {
                        "name": "Mark Deutel"
                    },
                    {
                        "name": "Georgios Kontes"
                    },
                    {
                        "name": "Christopher Mutschler"
                    },
                    {
                        "name": "Jürgen Teich"
                    }
                ],
                "author_detail": {
                    "name": "Jürgen Teich"
                },
                "author": "Jürgen Teich",
                "arxiv_doi": "10.1145/3715012",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3715012",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2305.14109v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2305.14109v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "ACM Transactions on Evolutionary Learning and Optimization, 14 pages,\n  9 figures",
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.13746v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.13746v1",
                "updated": "2025-01-23T15:22:25Z",
                "updated_parsed": [
                    2025,
                    1,
                    23,
                    15,
                    22,
                    25,
                    3,
                    23,
                    0
                ],
                "published": "2025-01-23T15:22:25Z",
                "published_parsed": [
                    2025,
                    1,
                    23,
                    15,
                    22,
                    25,
                    3,
                    23,
                    0
                ],
                "title": "EICopilot: Search and Explore Enterprise Information over Large-scale\n  Knowledge Graphs with LLM-driven Agents",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "EICopilot: Search and Explore Enterprise Information over Large-scale\n  Knowledge Graphs with LLM-driven Agents"
                },
                "summary": "The paper introduces EICopilot, an novel agent-based solution enhancing\nsearch and exploration of enterprise registration data within extensive online\nknowledge graphs like those detailing legal entities, registered capital, and\nmajor shareholders. Traditional methods necessitate text-based queries and\nmanual subgraph explorations, often resulting in time-consuming processes.\nEICopilot, deployed as a chatbot via Baidu Enterprise Search, improves this\nlandscape by utilizing Large Language Models (LLMs) to interpret natural\nlanguage queries. This solution automatically generates and executes Gremlin\nscripts, providing efficient summaries of complex enterprise relationships.\nDistinct feature a data pre-processing pipeline that compiles and annotates\nrepresentative queries into a vector database of examples for In-context\nlearning (ICL), a comprehensive reasoning pipeline combining Chain-of-Thought\nwith ICL to enhance Gremlin script generation for knowledge graph search and\nexploration, and a novel query masking strategy that improves intent\nrecognition for heightened script accuracy. Empirical evaluations demonstrate\nthe superior performance of EICopilot, including speed and accuracy, over\nbaseline methods, with the \\emph{Full Mask} variant achieving a syntax error\nrate reduction to as low as 10.00% and an execution correctness of up to\n82.14%. These components collectively contribute to superior querying\ncapabilities and summarization of intricate datasets, positioning EICopilot as\na groundbreaking tool in the exploration and exploitation of large-scale\nknowledge graphs for enterprise information search.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The paper introduces EICopilot, an novel agent-based solution enhancing\nsearch and exploration of enterprise registration data within extensive online\nknowledge graphs like those detailing legal entities, registered capital, and\nmajor shareholders. Traditional methods necessitate text-based queries and\nmanual subgraph explorations, often resulting in time-consuming processes.\nEICopilot, deployed as a chatbot via Baidu Enterprise Search, improves this\nlandscape by utilizing Large Language Models (LLMs) to interpret natural\nlanguage queries. This solution automatically generates and executes Gremlin\nscripts, providing efficient summaries of complex enterprise relationships.\nDistinct feature a data pre-processing pipeline that compiles and annotates\nrepresentative queries into a vector database of examples for In-context\nlearning (ICL), a comprehensive reasoning pipeline combining Chain-of-Thought\nwith ICL to enhance Gremlin script generation for knowledge graph search and\nexploration, and a novel query masking strategy that improves intent\nrecognition for heightened script accuracy. Empirical evaluations demonstrate\nthe superior performance of EICopilot, including speed and accuracy, over\nbaseline methods, with the \\emph{Full Mask} variant achieving a syntax error\nrate reduction to as low as 10.00% and an execution correctness of up to\n82.14%. These components collectively contribute to superior querying\ncapabilities and summarization of intricate datasets, positioning EICopilot as\na groundbreaking tool in the exploration and exploitation of large-scale\nknowledge graphs for enterprise information search."
                },
                "authors": [
                    {
                        "name": "Yuhui Yun"
                    },
                    {
                        "name": "Huilong Ye"
                    },
                    {
                        "name": "Xinru Li"
                    },
                    {
                        "name": "Ruojia Li"
                    },
                    {
                        "name": "Jingfeng Deng"
                    },
                    {
                        "name": "Li Li"
                    },
                    {
                        "name": "Haoyi Xiong"
                    }
                ],
                "author_detail": {
                    "name": "Haoyi Xiong"
                },
                "author": "Haoyi Xiong",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.13746v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.13746v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.13743v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.13743v1",
                "updated": "2025-01-23T15:18:22Z",
                "updated_parsed": [
                    2025,
                    1,
                    23,
                    15,
                    18,
                    22,
                    3,
                    23,
                    0
                ],
                "published": "2025-01-23T15:18:22Z",
                "published_parsed": [
                    2025,
                    1,
                    23,
                    15,
                    18,
                    22,
                    3,
                    23,
                    0
                ],
                "title": "GPT-HTree: A Decision Tree Framework Integrating Hierarchical Clustering\n  and Large Language Models for Explainable Classification",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GPT-HTree: A Decision Tree Framework Integrating Hierarchical Clustering\n  and Large Language Models for Explainable Classification"
                },
                "summary": "This paper introduces GPT-HTree, a framework combining hierarchical\nclustering, decision trees, and large language models (LLMs) to address this\nchallenge. By leveraging hierarchical clustering to segment individuals based\non salient features, resampling techniques to balance class distributions, and\ndecision trees to tailor classification paths within each cluster, GPT-HTree\nensures both accuracy and interpretability. LLMs enhance the framework by\ngenerating human-readable cluster descriptions, bridging quantitative analysis\nwith actionable insights.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper introduces GPT-HTree, a framework combining hierarchical\nclustering, decision trees, and large language models (LLMs) to address this\nchallenge. By leveraging hierarchical clustering to segment individuals based\non salient features, resampling techniques to balance class distributions, and\ndecision trees to tailor classification paths within each cluster, GPT-HTree\nensures both accuracy and interpretability. LLMs enhance the framework by\ngenerating human-readable cluster descriptions, bridging quantitative analysis\nwith actionable insights."
                },
                "authors": [
                    {
                        "name": "Te Pei"
                    },
                    {
                        "name": "Fuat Alican"
                    },
                    {
                        "name": "Aaron Ontoyin Yin"
                    },
                    {
                        "name": "Yigit Ihlamur"
                    }
                ],
                "author_detail": {
                    "name": "Yigit Ihlamur"
                },
                "author": "Yigit Ihlamur",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.13743v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.13743v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.13731v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.13731v1",
                "updated": "2025-01-23T15:04:22Z",
                "updated_parsed": [
                    2025,
                    1,
                    23,
                    15,
                    4,
                    22,
                    3,
                    23,
                    0
                ],
                "published": "2025-01-23T15:04:22Z",
                "published_parsed": [
                    2025,
                    1,
                    23,
                    15,
                    4,
                    22,
                    3,
                    23,
                    0
                ],
                "title": "Pseudocode-Injection Magic: Enabling LLMs to Tackle Graph Computational\n  Tasks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Pseudocode-Injection Magic: Enabling LLMs to Tackle Graph Computational\n  Tasks"
                },
                "summary": "Graph computational tasks are inherently challenging and often demand the\ndevelopment of advanced algorithms for effective solutions. With the emergence\nof large language models (LLMs), researchers have begun investigating their\npotential to address these tasks. However, existing approaches are constrained\nby LLMs' limited capability to comprehend complex graph structures and their\nhigh inference costs, rendering them impractical for handling large-scale\ngraphs. Inspired by human approaches to graph problems, we introduce a novel\nframework, PIE (Pseudocode-Injection-Enhanced LLM Reasoning for Graph\nComputational Tasks), which consists of three key steps: problem understanding,\nprompt design, and code generation. In this framework, LLMs are tasked with\nunderstanding the problem and extracting relevant information to generate\ncorrect code. The responsibility for analyzing the graph structure and\nexecuting the code is delegated to the interpreter. We inject task-related\npseudocodes into the prompts to further assist the LLMs in generating efficient\ncode. We also employ cost-effective trial-and-error techniques to ensure that\nthe LLM-generated code executes correctly. Unlike other methods that require\ninvoking LLMs for each individual test case, PIE only calls the LLM during the\ncode generation phase, allowing the generated code to be reused and\nsignificantly reducing inference costs. Extensive experiments demonstrate that\nPIE outperforms existing baselines in terms of both accuracy and computational\nefficiency.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Graph computational tasks are inherently challenging and often demand the\ndevelopment of advanced algorithms for effective solutions. With the emergence\nof large language models (LLMs), researchers have begun investigating their\npotential to address these tasks. However, existing approaches are constrained\nby LLMs' limited capability to comprehend complex graph structures and their\nhigh inference costs, rendering them impractical for handling large-scale\ngraphs. Inspired by human approaches to graph problems, we introduce a novel\nframework, PIE (Pseudocode-Injection-Enhanced LLM Reasoning for Graph\nComputational Tasks), which consists of three key steps: problem understanding,\nprompt design, and code generation. In this framework, LLMs are tasked with\nunderstanding the problem and extracting relevant information to generate\ncorrect code. The responsibility for analyzing the graph structure and\nexecuting the code is delegated to the interpreter. We inject task-related\npseudocodes into the prompts to further assist the LLMs in generating efficient\ncode. We also employ cost-effective trial-and-error techniques to ensure that\nthe LLM-generated code executes correctly. Unlike other methods that require\ninvoking LLMs for each individual test case, PIE only calls the LLM during the\ncode generation phase, allowing the generated code to be reused and\nsignificantly reducing inference costs. Extensive experiments demonstrate that\nPIE outperforms existing baselines in terms of both accuracy and computational\nefficiency."
                },
                "authors": [
                    {
                        "name": "Chang Gong"
                    },
                    {
                        "name": "Wanrui Bian"
                    },
                    {
                        "name": "Zhijie Zhang"
                    },
                    {
                        "name": "Weiguo Zheng"
                    }
                ],
                "author_detail": {
                    "name": "Weiguo Zheng"
                },
                "author": "Weiguo Zheng",
                "arxiv_comment": "24 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.13731v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.13731v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.13726v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.13726v1",
                "updated": "2025-01-23T14:58:56Z",
                "updated_parsed": [
                    2025,
                    1,
                    23,
                    14,
                    58,
                    56,
                    3,
                    23,
                    0
                ],
                "published": "2025-01-23T14:58:56Z",
                "published_parsed": [
                    2025,
                    1,
                    23,
                    14,
                    58,
                    56,
                    3,
                    23,
                    0
                ],
                "title": "RPO: Retrieval Preference Optimization for Robust Retrieval-Augmented\n  Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RPO: Retrieval Preference Optimization for Robust Retrieval-Augmented\n  Generation"
                },
                "summary": "While Retrieval-Augmented Generation (RAG) has exhibited promise in utilizing\nexternal knowledge, its generation process heavily depends on the quality and\naccuracy of the retrieved context. Large language models (LLMs) struggle to\nevaluate the correctness of non-parametric knowledge retrieved externally when\nit differs from internal memorization, leading to knowledge conflicts during\nresponse generation. To this end, we introduce the Retrieval Preference\nOptimization (RPO), a lightweight and effective alignment method to adaptively\nleverage multi-source knowledge based on retrieval relevance. An implicit\nrepresentation of retrieval relevance is derived and incorporated into the\nreward model to integrate retrieval evaluation and response generation into a\nsingle model, solving the problem that previous methods necessitate the\nadditional procedure to assess the retrieval quality. Notably, RPO is the only\nRAG-dedicated alignment approach that quantifies the awareness of retrieval\nrelevance in training, overcoming mathematical obstacles. Experiments on four\ndatasets demonstrate that RPO outperforms RAG by 4-10% in accuracy without any\nextra component, exhibiting its robust generalization.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While Retrieval-Augmented Generation (RAG) has exhibited promise in utilizing\nexternal knowledge, its generation process heavily depends on the quality and\naccuracy of the retrieved context. Large language models (LLMs) struggle to\nevaluate the correctness of non-parametric knowledge retrieved externally when\nit differs from internal memorization, leading to knowledge conflicts during\nresponse generation. To this end, we introduce the Retrieval Preference\nOptimization (RPO), a lightweight and effective alignment method to adaptively\nleverage multi-source knowledge based on retrieval relevance. An implicit\nrepresentation of retrieval relevance is derived and incorporated into the\nreward model to integrate retrieval evaluation and response generation into a\nsingle model, solving the problem that previous methods necessitate the\nadditional procedure to assess the retrieval quality. Notably, RPO is the only\nRAG-dedicated alignment approach that quantifies the awareness of retrieval\nrelevance in training, overcoming mathematical obstacles. Experiments on four\ndatasets demonstrate that RPO outperforms RAG by 4-10% in accuracy without any\nextra component, exhibiting its robust generalization."
                },
                "authors": [
                    {
                        "name": "Shi-Qi Yan"
                    },
                    {
                        "name": "Zhen-Hua Ling"
                    }
                ],
                "author_detail": {
                    "name": "Zhen-Hua Ling"
                },
                "author": "Zhen-Hua Ling",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.13726v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.13726v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.13720v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.13720v1",
                "updated": "2025-01-23T14:50:37Z",
                "updated_parsed": [
                    2025,
                    1,
                    23,
                    14,
                    50,
                    37,
                    3,
                    23,
                    0
                ],
                "published": "2025-01-23T14:50:37Z",
                "published_parsed": [
                    2025,
                    1,
                    23,
                    14,
                    50,
                    37,
                    3,
                    23,
                    0
                ],
                "title": "Musical ethnocentrism in Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Musical ethnocentrism in Large Language Models"
                },
                "summary": "Large Language Models (LLMs) reflect the biases in their training data and,\nby extension, those of the people who created this training data. Detecting,\nanalyzing, and mitigating such biases is becoming a focus of research. One type\nof bias that has been understudied so far are geocultural biases. Those can be\ncaused by an imbalance in the representation of different geographic regions\nand cultures in the training data, but also by value judgments contained\ntherein. In this paper, we make a first step towards analyzing musical biases\nin LLMs, particularly ChatGPT and Mixtral. We conduct two experiments. In the\nfirst, we prompt LLMs to provide lists of the \"Top 100\" musical contributors of\nvarious categories and analyze their countries of origin. In the second\nexperiment, we ask the LLMs to numerically rate various aspects of the musical\ncultures of different countries. Our results indicate a strong preference of\nthe LLMs for Western music cultures in both experiments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) reflect the biases in their training data and,\nby extension, those of the people who created this training data. Detecting,\nanalyzing, and mitigating such biases is becoming a focus of research. One type\nof bias that has been understudied so far are geocultural biases. Those can be\ncaused by an imbalance in the representation of different geographic regions\nand cultures in the training data, but also by value judgments contained\ntherein. In this paper, we make a first step towards analyzing musical biases\nin LLMs, particularly ChatGPT and Mixtral. We conduct two experiments. In the\nfirst, we prompt LLMs to provide lists of the \"Top 100\" musical contributors of\nvarious categories and analyze their countries of origin. In the second\nexperiment, we ask the LLMs to numerically rate various aspects of the musical\ncultures of different countries. Our results indicate a strong preference of\nthe LLMs for Western music cultures in both experiments."
                },
                "authors": [
                    {
                        "name": "Anna Kruspe"
                    }
                ],
                "author_detail": {
                    "name": "Anna Kruspe"
                },
                "author": "Anna Kruspe",
                "arxiv_journal_ref": "Proceedings of the 3rd Workshop on NLP for Music and Audio\n  (NLP4MusA) 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.13720v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.13720v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.01957v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.01957v2",
                "updated": "2025-01-23T14:49:53Z",
                "updated_parsed": [
                    2025,
                    1,
                    23,
                    14,
                    49,
                    53,
                    3,
                    23,
                    0
                ],
                "published": "2024-12-02T20:36:41Z",
                "published_parsed": [
                    2024,
                    12,
                    2,
                    20,
                    36,
                    41,
                    0,
                    337,
                    0
                ],
                "title": "Usage Governance Advisor: From Intent to AI Governance",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Usage Governance Advisor: From Intent to AI Governance"
                },
                "summary": "Evaluating the safety of AI Systems is a pressing concern for organizations\ndeploying them. In addition to the societal damage done by the lack of fairness\nof those systems, deployers are concerned about the legal repercussions and the\nreputational damage incurred by the use of models that are unsafe. Safety\ncovers both what a model does; e.g., can it be used to reveal personal\ninformation from its training set, and how a model was built; e.g., was it only\ntrained on licensed data sets. Determining the safety of an AI system requires\ngathering information from a wide set of heterogeneous sources including safety\nbenchmarks and technical documentation for the set of models used in that\nsystem. In addition, responsible use is encouraged through mechanisms that\nadvise and help the user to take mitigating actions where safety risks are\ndetected. We present Usage Governance Advisor which creates semi-structured\ngovernance information, identifies and prioritizes risks according to the\nintended use case, recommends appropriate benchmarks and risk assessments and\nimportantly proposes mitigation strategies and actions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Evaluating the safety of AI Systems is a pressing concern for organizations\ndeploying them. In addition to the societal damage done by the lack of fairness\nof those systems, deployers are concerned about the legal repercussions and the\nreputational damage incurred by the use of models that are unsafe. Safety\ncovers both what a model does; e.g., can it be used to reveal personal\ninformation from its training set, and how a model was built; e.g., was it only\ntrained on licensed data sets. Determining the safety of an AI system requires\ngathering information from a wide set of heterogeneous sources including safety\nbenchmarks and technical documentation for the set of models used in that\nsystem. In addition, responsible use is encouraged through mechanisms that\nadvise and help the user to take mitigating actions where safety risks are\ndetected. We present Usage Governance Advisor which creates semi-structured\ngovernance information, identifies and prioritizes risks according to the\nintended use case, recommends appropriate benchmarks and risk assessments and\nimportantly proposes mitigation strategies and actions."
                },
                "authors": [
                    {
                        "name": "Elizabeth M. Daly"
                    },
                    {
                        "name": "Sean Rooney"
                    },
                    {
                        "name": "Seshu Tirupathi"
                    },
                    {
                        "name": "Luis Garces-Erice"
                    },
                    {
                        "name": "Inge Vejsbjerg"
                    },
                    {
                        "name": "Frank Bagehorn"
                    },
                    {
                        "name": "Dhaval Salwala"
                    },
                    {
                        "name": "Christopher Giblin"
                    },
                    {
                        "name": "Mira L. Wolf-Bauwens"
                    },
                    {
                        "name": "Ioana Giurgiu"
                    },
                    {
                        "name": "Michael Hind"
                    },
                    {
                        "name": "Peter Urbanetz"
                    }
                ],
                "author_detail": {
                    "name": "Peter Urbanetz"
                },
                "author": "Peter Urbanetz",
                "arxiv_comment": "9 pages, 8 figures, AAAI workshop submission",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.01957v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.01957v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.11683v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.11683v3",
                "updated": "2025-01-23T14:45:03Z",
                "updated_parsed": [
                    2025,
                    1,
                    23,
                    14,
                    45,
                    3,
                    3,
                    23,
                    0
                ],
                "published": "2024-11-18T16:09:26Z",
                "published_parsed": [
                    2024,
                    11,
                    18,
                    16,
                    9,
                    26,
                    0,
                    323,
                    0
                ],
                "title": "TrojanRobot: Physical-World Backdoor Attacks Against VLM-based Robotic\n  Manipulation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TrojanRobot: Physical-World Backdoor Attacks Against VLM-based Robotic\n  Manipulation"
                },
                "summary": "Robotic manipulation in the physical world is increasingly empowered by\n\\textit{large language models} (LLMs) and \\textit{vision-language models}\n(VLMs), leveraging their understanding and perception capabilities. Recently,\nvarious attacks against such robotic policies have been proposed, with backdoor\nattacks drawing considerable attention for their high stealth and strong\npersistence capabilities. However, existing backdoor efforts are limited to\nsimulators and suffer from physical-world realization. To address this, we\npropose \\textit{TrojanRobot}, a highly stealthy and broadly effective robotic\nbackdoor attack in the physical world. Specifically, we introduce a\nmodule-poisoning approach by embedding a backdoor module into the modular\nrobotic policy, enabling backdoor control over the policy's visual perception\nmodule thereby backdooring the entire robotic policy. Our vanilla\nimplementation leverages a backdoor-finetuned VLM to serve as the backdoor\nmodule. To enhance its generalization in physical environments, we propose a\nprime implementation, leveraging the LVLM-as-a-backdoor paradigm and developing\nthree types of prime attacks, \\ie, \\textit{permutation}, \\textit{stagnation},\nand \\textit{intentional} attacks, thus achieving finer-grained backdoors.\nExtensive experiments on the UR3e manipulator with 18 task instructions using\nrobotic policies based on four VLMs demonstrate the broad effectiveness and\nphysical-world stealth of TrojanRobot. Our attack's video demonstrations are\navailable via a github link \\url{https://trojanrobot.github.io}.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Robotic manipulation in the physical world is increasingly empowered by\n\\textit{large language models} (LLMs) and \\textit{vision-language models}\n(VLMs), leveraging their understanding and perception capabilities. Recently,\nvarious attacks against such robotic policies have been proposed, with backdoor\nattacks drawing considerable attention for their high stealth and strong\npersistence capabilities. However, existing backdoor efforts are limited to\nsimulators and suffer from physical-world realization. To address this, we\npropose \\textit{TrojanRobot}, a highly stealthy and broadly effective robotic\nbackdoor attack in the physical world. Specifically, we introduce a\nmodule-poisoning approach by embedding a backdoor module into the modular\nrobotic policy, enabling backdoor control over the policy's visual perception\nmodule thereby backdooring the entire robotic policy. Our vanilla\nimplementation leverages a backdoor-finetuned VLM to serve as the backdoor\nmodule. To enhance its generalization in physical environments, we propose a\nprime implementation, leveraging the LVLM-as-a-backdoor paradigm and developing\nthree types of prime attacks, \\ie, \\textit{permutation}, \\textit{stagnation},\nand \\textit{intentional} attacks, thus achieving finer-grained backdoors.\nExtensive experiments on the UR3e manipulator with 18 task instructions using\nrobotic policies based on four VLMs demonstrate the broad effectiveness and\nphysical-world stealth of TrojanRobot. Our attack's video demonstrations are\navailable via a github link \\url{https://trojanrobot.github.io}."
                },
                "authors": [
                    {
                        "name": "Xianlong Wang"
                    },
                    {
                        "name": "Hewen Pan"
                    },
                    {
                        "name": "Hangtao Zhang"
                    },
                    {
                        "name": "Minghui Li"
                    },
                    {
                        "name": "Shengshan Hu"
                    },
                    {
                        "name": "Ziqi Zhou"
                    },
                    {
                        "name": "Lulu Xue"
                    },
                    {
                        "name": "Peijin Guo"
                    },
                    {
                        "name": "Yichen Wang"
                    },
                    {
                        "name": "Wei Wan"
                    },
                    {
                        "name": "Aishan Liu"
                    },
                    {
                        "name": "Leo Yu Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Leo Yu Zhang"
                },
                "author": "Leo Yu Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.11683v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.11683v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.13106v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.13106v2",
                "updated": "2025-01-23T14:41:06Z",
                "updated_parsed": [
                    2025,
                    1,
                    23,
                    14,
                    41,
                    6,
                    3,
                    23,
                    0
                ],
                "published": "2025-01-22T18:59:46Z",
                "published_parsed": [
                    2025,
                    1,
                    22,
                    18,
                    59,
                    46,
                    2,
                    22,
                    0
                ],
                "title": "VideoLLaMA 3: Frontier Multimodal Foundation Models for Image and Video\n  Understanding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "VideoLLaMA 3: Frontier Multimodal Foundation Models for Image and Video\n  Understanding"
                },
                "summary": "In this paper, we propose VideoLLaMA3, a more advanced multimodal foundation\nmodel for image and video understanding. The core design philosophy of\nVideoLLaMA3 is vision-centric. The meaning of \"vision-centric\" is two-fold: the\nvision-centric training paradigm and vision-centric framework design. The key\ninsight of our vision-centric training paradigm is that high-quality image-text\ndata is crucial for both image and video understanding. Instead of preparing\nmassive video-text datasets, we focus on constructing large-scale and\nhigh-quality image-text datasets. VideoLLaMA3 has four training stages: 1)\nVision Encoder Adaptation, which enables vision encoder to accept images of\nvariable resolutions as input; 2) Vision-Language Alignment, which jointly\ntunes the vision encoder, projector, and LLM with large-scale image-text data\ncovering multiple types (including scene images, documents, charts) as well as\ntext-only data. 3) Multi-task Fine-tuning, which incorporates image-text SFT\ndata for downstream tasks and video-text data to establish a foundation for\nvideo understanding. 4) Video-centric Fine-tuning, which further improves the\nmodel's capability in video understanding. As for the framework design, to\nbetter capture fine-grained details in images, the pretrained vision encoder is\nadapted to encode images of varying sizes into vision tokens with corresponding\nnumbers, rather than a fixed number of tokens. For video inputs, we reduce the\nnumber of vision tokens according to their similarity so that the\nrepresentation of videos will be more precise and compact. Benefit from\nvision-centric designs, VideoLLaMA3 achieves compelling performances in both\nimage and video understanding benchmarks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this paper, we propose VideoLLaMA3, a more advanced multimodal foundation\nmodel for image and video understanding. The core design philosophy of\nVideoLLaMA3 is vision-centric. The meaning of \"vision-centric\" is two-fold: the\nvision-centric training paradigm and vision-centric framework design. The key\ninsight of our vision-centric training paradigm is that high-quality image-text\ndata is crucial for both image and video understanding. Instead of preparing\nmassive video-text datasets, we focus on constructing large-scale and\nhigh-quality image-text datasets. VideoLLaMA3 has four training stages: 1)\nVision Encoder Adaptation, which enables vision encoder to accept images of\nvariable resolutions as input; 2) Vision-Language Alignment, which jointly\ntunes the vision encoder, projector, and LLM with large-scale image-text data\ncovering multiple types (including scene images, documents, charts) as well as\ntext-only data. 3) Multi-task Fine-tuning, which incorporates image-text SFT\ndata for downstream tasks and video-text data to establish a foundation for\nvideo understanding. 4) Video-centric Fine-tuning, which further improves the\nmodel's capability in video understanding. As for the framework design, to\nbetter capture fine-grained details in images, the pretrained vision encoder is\nadapted to encode images of varying sizes into vision tokens with corresponding\nnumbers, rather than a fixed number of tokens. For video inputs, we reduce the\nnumber of vision tokens according to their similarity so that the\nrepresentation of videos will be more precise and compact. Benefit from\nvision-centric designs, VideoLLaMA3 achieves compelling performances in both\nimage and video understanding benchmarks."
                },
                "authors": [
                    {
                        "name": "Boqiang Zhang"
                    },
                    {
                        "name": "Kehan Li"
                    },
                    {
                        "name": "Zesen Cheng"
                    },
                    {
                        "name": "Zhiqiang Hu"
                    },
                    {
                        "name": "Yuqian Yuan"
                    },
                    {
                        "name": "Guanzheng Chen"
                    },
                    {
                        "name": "Sicong Leng"
                    },
                    {
                        "name": "Yuming Jiang"
                    },
                    {
                        "name": "Hang Zhang"
                    },
                    {
                        "name": "Xin Li"
                    },
                    {
                        "name": "Peng Jin"
                    },
                    {
                        "name": "Wenqi Zhang"
                    },
                    {
                        "name": "Fan Wang"
                    },
                    {
                        "name": "Lidong Bing"
                    },
                    {
                        "name": "Deli Zhao"
                    }
                ],
                "author_detail": {
                    "name": "Deli Zhao"
                },
                "author": "Deli Zhao",
                "arxiv_comment": "BZ, KL, ZC, ZH, YY, GC, SL, YJ, HZ, and XL contributed equally to\n  this project. Code: https://github.com/DAMO-NLP-SG/VideoLLaMA3",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.13106v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.13106v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.05411v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.05411v3",
                "updated": "2025-01-23T14:30:40Z",
                "updated_parsed": [
                    2025,
                    1,
                    23,
                    14,
                    30,
                    40,
                    3,
                    23,
                    0
                ],
                "published": "2024-10-07T18:23:00Z",
                "published_parsed": [
                    2024,
                    10,
                    7,
                    18,
                    23,
                    0,
                    0,
                    281,
                    0
                ],
                "title": "Filtering Discomforting Recommendations with Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Filtering Discomforting Recommendations with Large Language Models"
                },
                "summary": "Personalized algorithms can inadvertently expose users to discomforting\nrecommendations, potentially triggering negative consequences. The subjectivity\nof discomfort and the black-box nature of these algorithms make it challenging\nto effectively identify and filter such content. To address this, we first\nconducted a formative study to understand users' practices and expectations\nregarding discomforting recommendation filtering. Then, we designed a Large\nLanguage Model (LLM)-based tool named DiscomfortFilter, which constructs an\neditable preference profile for a user and helps the user express filtering\nneeds through conversation to mask discomforting preferences within the\nprofile. Based on the edited profile, DiscomfortFilter facilitates the\ndiscomforting recommendations filtering in a plug-and-play manner, maintaining\nflexibility and transparency. The constructed preference profile improves LLM\nreasoning and simplifies user alignment, enabling a 3.8B open-source LLM to\nrival top commercial models in an offline proxy task. A one-week user study\nwith 24 participants demonstrated the effectiveness of DiscomfortFilter, while\nalso highlighting its potential impact on platform recommendation outcomes. We\nconclude by discussing the ongoing challenges, highlighting its relevance to\nbroader research, assessing stakeholder impact, and outlining future research\ndirections.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Personalized algorithms can inadvertently expose users to discomforting\nrecommendations, potentially triggering negative consequences. The subjectivity\nof discomfort and the black-box nature of these algorithms make it challenging\nto effectively identify and filter such content. To address this, we first\nconducted a formative study to understand users' practices and expectations\nregarding discomforting recommendation filtering. Then, we designed a Large\nLanguage Model (LLM)-based tool named DiscomfortFilter, which constructs an\neditable preference profile for a user and helps the user express filtering\nneeds through conversation to mask discomforting preferences within the\nprofile. Based on the edited profile, DiscomfortFilter facilitates the\ndiscomforting recommendations filtering in a plug-and-play manner, maintaining\nflexibility and transparency. The constructed preference profile improves LLM\nreasoning and simplifies user alignment, enabling a 3.8B open-source LLM to\nrival top commercial models in an offline proxy task. A one-week user study\nwith 24 participants demonstrated the effectiveness of DiscomfortFilter, while\nalso highlighting its potential impact on platform recommendation outcomes. We\nconclude by discussing the ongoing challenges, highlighting its relevance to\nbroader research, assessing stakeholder impact, and outlining future research\ndirections."
                },
                "authors": [
                    {
                        "name": "Jiahao Liu"
                    },
                    {
                        "name": "Yiyang Shao"
                    },
                    {
                        "name": "Peng Zhang"
                    },
                    {
                        "name": "Dongsheng Li"
                    },
                    {
                        "name": "Hansu Gu"
                    },
                    {
                        "name": "Chao Chen"
                    },
                    {
                        "name": "Longzhi Du"
                    },
                    {
                        "name": "Tun Lu"
                    },
                    {
                        "name": "Ning Gu"
                    }
                ],
                "author_detail": {
                    "name": "Ning Gu"
                },
                "author": "Ning Gu",
                "arxiv_comment": "Accepted by WWW 2025, 16 pages, full version",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.05411v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.05411v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.13699v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.13699v1",
                "updated": "2025-01-23T14:27:11Z",
                "updated_parsed": [
                    2025,
                    1,
                    23,
                    14,
                    27,
                    11,
                    3,
                    23,
                    0
                ],
                "published": "2025-01-23T14:27:11Z",
                "published_parsed": [
                    2025,
                    1,
                    23,
                    14,
                    27,
                    11,
                    3,
                    23,
                    0
                ],
                "title": "DI-BENCH: Benchmarking Large Language Models on Dependency Inference\n  with Testable Repositories at Scale",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DI-BENCH: Benchmarking Large Language Models on Dependency Inference\n  with Testable Repositories at Scale"
                },
                "summary": "Large Language Models have advanced automated software development, however,\nit remains a challenge to correctly infer dependencies, namely, identifying the\ninternal components and external packages required for a repository to\nsuccessfully run. Existing studies highlight that dependency-related issues\ncause over 40\\% of observed runtime errors on the generated repository. To\naddress this, we introduce DI-BENCH, a large-scale benchmark and evaluation\nframework specifically designed to assess LLMs' capability on dependency\ninference. The benchmark features 581 repositories with testing environments\nacross Python, C#, Rust, and JavaScript. Extensive experiments with textual and\nexecution-based metrics reveal that the current best-performing model achieves\nonly a 42.9% execution pass rate, indicating significant room for improvement.\nDI-BENCH establishes a new viewpoint for evaluating LLM performance on\nrepositories, paving the way for more robust end-to-end software synthesis.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models have advanced automated software development, however,\nit remains a challenge to correctly infer dependencies, namely, identifying the\ninternal components and external packages required for a repository to\nsuccessfully run. Existing studies highlight that dependency-related issues\ncause over 40\\% of observed runtime errors on the generated repository. To\naddress this, we introduce DI-BENCH, a large-scale benchmark and evaluation\nframework specifically designed to assess LLMs' capability on dependency\ninference. The benchmark features 581 repositories with testing environments\nacross Python, C#, Rust, and JavaScript. Extensive experiments with textual and\nexecution-based metrics reveal that the current best-performing model achieves\nonly a 42.9% execution pass rate, indicating significant room for improvement.\nDI-BENCH establishes a new viewpoint for evaluating LLM performance on\nrepositories, paving the way for more robust end-to-end software synthesis."
                },
                "authors": [
                    {
                        "name": "Linghao Zhang"
                    },
                    {
                        "name": "Junhao Wang"
                    },
                    {
                        "name": "Shilin He"
                    },
                    {
                        "name": "Chaoyun Zhang"
                    },
                    {
                        "name": "Yu Kang"
                    },
                    {
                        "name": "Bowen Li"
                    },
                    {
                        "name": "Jiaheng Wen"
                    },
                    {
                        "name": "Chengxing Xie"
                    },
                    {
                        "name": "Maoquan Wang"
                    },
                    {
                        "name": "Yufan Huang"
                    },
                    {
                        "name": "Elsie Nallipogu"
                    },
                    {
                        "name": "Qingwei Lin"
                    },
                    {
                        "name": "Yingnong Dang"
                    },
                    {
                        "name": "Saravan Rajmohan"
                    },
                    {
                        "name": "Dongmei Zhang"
                    },
                    {
                        "name": "Qi Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Qi Zhang"
                },
                "author": "Qi Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.13699v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.13699v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.11223v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.11223v3",
                "updated": "2025-01-23T14:26:08Z",
                "updated_parsed": [
                    2025,
                    1,
                    23,
                    14,
                    26,
                    8,
                    3,
                    23,
                    0
                ],
                "published": "2025-01-20T02:16:19Z",
                "published_parsed": [
                    2025,
                    1,
                    20,
                    2,
                    16,
                    19,
                    0,
                    20,
                    0
                ],
                "title": "Reasoning Language Models: A Blueprint",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reasoning Language Models: A Blueprint"
                },
                "summary": "Reasoning language models (RLMs), also known as Large Reasoning Models\n(LRMs), such as OpenAI's o1 and o3, DeepSeek-V3, and Alibaba's QwQ, have\nredefined AI's problem-solving capabilities by extending LLMs with advanced\nreasoning mechanisms. Yet, their high costs, proprietary nature, and complex\narchitectures - uniquely combining Reinforcement Learning (RL), search\nheuristics, and LLMs - present accessibility and scalability challenges. To\naddress these, we propose a comprehensive blueprint that organizes RLM\ncomponents into a modular framework, based on a survey and analysis of all RLM\nworks. This blueprint incorporates diverse reasoning structures (chains, trees,\ngraphs, and nested forms), reasoning strategies (e.g., Monte Carlo Tree Search,\nBeam Search), RL concepts (policy, value models and others), supervision\nschemes (Outcome-Based and Process-Based Supervision), and other related\nconcepts (e.g., Test-Time Compute, Retrieval-Augmented Generation, agent\ntools). We also provide detailed mathematical formulations and algorithmic\nspecifications to simplify RLM implementation. By showing how schemes like\nLLaMA-Berry, QwQ, Journey Learning, and Graph of Thoughts fit as special cases,\nwe demonstrate the blueprint's versatility and unifying potential. To\nillustrate its utility, we introduce x1, a modular implementation for rapid RLM\nprototyping and experimentation. Using x1 and a literature review, we provide\nkey insights, such as multi-phase training for policy and value models, and the\nimportance of familiar training distributions. Finally, we discuss scalable RLM\ncloud deployments and we outline how RLMs can integrate with a broader LLM\necosystem. Our work demystifies RLM construction, democratizes advanced\nreasoning capabilities, and fosters innovation, aiming to mitigate the gap\nbetween \"rich AI\" and \"poor AI\" by lowering barriers to RLM design and\nexperimentation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reasoning language models (RLMs), also known as Large Reasoning Models\n(LRMs), such as OpenAI's o1 and o3, DeepSeek-V3, and Alibaba's QwQ, have\nredefined AI's problem-solving capabilities by extending LLMs with advanced\nreasoning mechanisms. Yet, their high costs, proprietary nature, and complex\narchitectures - uniquely combining Reinforcement Learning (RL), search\nheuristics, and LLMs - present accessibility and scalability challenges. To\naddress these, we propose a comprehensive blueprint that organizes RLM\ncomponents into a modular framework, based on a survey and analysis of all RLM\nworks. This blueprint incorporates diverse reasoning structures (chains, trees,\ngraphs, and nested forms), reasoning strategies (e.g., Monte Carlo Tree Search,\nBeam Search), RL concepts (policy, value models and others), supervision\nschemes (Outcome-Based and Process-Based Supervision), and other related\nconcepts (e.g., Test-Time Compute, Retrieval-Augmented Generation, agent\ntools). We also provide detailed mathematical formulations and algorithmic\nspecifications to simplify RLM implementation. By showing how schemes like\nLLaMA-Berry, QwQ, Journey Learning, and Graph of Thoughts fit as special cases,\nwe demonstrate the blueprint's versatility and unifying potential. To\nillustrate its utility, we introduce x1, a modular implementation for rapid RLM\nprototyping and experimentation. Using x1 and a literature review, we provide\nkey insights, such as multi-phase training for policy and value models, and the\nimportance of familiar training distributions. Finally, we discuss scalable RLM\ncloud deployments and we outline how RLMs can integrate with a broader LLM\necosystem. Our work demystifies RLM construction, democratizes advanced\nreasoning capabilities, and fosters innovation, aiming to mitigate the gap\nbetween \"rich AI\" and \"poor AI\" by lowering barriers to RLM design and\nexperimentation."
                },
                "authors": [
                    {
                        "name": "Maciej Besta"
                    },
                    {
                        "name": "Julia Barth"
                    },
                    {
                        "name": "Eric Schreiber"
                    },
                    {
                        "name": "Ales Kubicek"
                    },
                    {
                        "name": "Afonso Catarino"
                    },
                    {
                        "name": "Robert Gerstenberger"
                    },
                    {
                        "name": "Piotr Nyczyk"
                    },
                    {
                        "name": "Patrick Iff"
                    },
                    {
                        "name": "Yueling Li"
                    },
                    {
                        "name": "Sam Houliston"
                    },
                    {
                        "name": "Tomasz Sternal"
                    },
                    {
                        "name": "Marcin Copik"
                    },
                    {
                        "name": "Grzegorz Kwaśniewski"
                    },
                    {
                        "name": "Jürgen Müller"
                    },
                    {
                        "name": "Łukasz Flis"
                    },
                    {
                        "name": "Hannes Eberhard"
                    },
                    {
                        "name": "Hubert Niewiadomski"
                    },
                    {
                        "name": "Torsten Hoefler"
                    }
                ],
                "author_detail": {
                    "name": "Torsten Hoefler"
                },
                "author": "Torsten Hoefler",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.11223v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.11223v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.13687v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.13687v1",
                "updated": "2025-01-23T14:13:56Z",
                "updated_parsed": [
                    2025,
                    1,
                    23,
                    14,
                    13,
                    56,
                    3,
                    23,
                    0
                ],
                "published": "2025-01-23T14:13:56Z",
                "published_parsed": [
                    2025,
                    1,
                    23,
                    14,
                    13,
                    56,
                    3,
                    23,
                    0
                ],
                "title": "Question Answering on Patient Medical Records with Private Fine-Tuned\n  LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Question Answering on Patient Medical Records with Private Fine-Tuned\n  LLMs"
                },
                "summary": "Healthcare systems continuously generate vast amounts of electronic health\nrecords (EHRs), commonly stored in the Fast Healthcare Interoperability\nResources (FHIR) standard. Despite the wealth of information in these records,\ntheir complexity and volume make it difficult for users to retrieve and\ninterpret crucial health insights. Recent advances in Large Language Models\n(LLMs) offer a solution, enabling semantic question answering (QA) over medical\ndata, allowing users to interact with their health records more effectively.\nHowever, ensuring privacy and compliance requires edge and private deployments\nof LLMs.\n  This paper proposes a novel approach to semantic QA over EHRs by first\nidentifying the most relevant FHIR resources for a user query (Task1) and\nsubsequently answering the query based on these resources (Task2). We explore\nthe performance of privately hosted, fine-tuned LLMs, evaluating them against\nbenchmark models such as GPT-4 and GPT-4o. Our results demonstrate that\nfine-tuned LLMs, while 250x smaller in size, outperform GPT-4 family models by\n0.55% in F1 score on Task1 and 42% on Meteor Task in Task2. Additionally, we\nexamine advanced aspects of LLM usage, including sequential fine-tuning, model\nself-evaluation (narcissistic evaluation), and the impact of training data size\non performance. The models and datasets are available here:\nhttps://huggingface.co/genloop",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Healthcare systems continuously generate vast amounts of electronic health\nrecords (EHRs), commonly stored in the Fast Healthcare Interoperability\nResources (FHIR) standard. Despite the wealth of information in these records,\ntheir complexity and volume make it difficult for users to retrieve and\ninterpret crucial health insights. Recent advances in Large Language Models\n(LLMs) offer a solution, enabling semantic question answering (QA) over medical\ndata, allowing users to interact with their health records more effectively.\nHowever, ensuring privacy and compliance requires edge and private deployments\nof LLMs.\n  This paper proposes a novel approach to semantic QA over EHRs by first\nidentifying the most relevant FHIR resources for a user query (Task1) and\nsubsequently answering the query based on these resources (Task2). We explore\nthe performance of privately hosted, fine-tuned LLMs, evaluating them against\nbenchmark models such as GPT-4 and GPT-4o. Our results demonstrate that\nfine-tuned LLMs, while 250x smaller in size, outperform GPT-4 family models by\n0.55% in F1 score on Task1 and 42% on Meteor Task in Task2. Additionally, we\nexamine advanced aspects of LLM usage, including sequential fine-tuning, model\nself-evaluation (narcissistic evaluation), and the impact of training data size\non performance. The models and datasets are available here:\nhttps://huggingface.co/genloop"
                },
                "authors": [
                    {
                        "name": "Sara Kothari"
                    },
                    {
                        "name": "Ayush Gupta"
                    }
                ],
                "author_detail": {
                    "name": "Ayush Gupta"
                },
                "author": "Ayush Gupta",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.13687v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.13687v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.13677v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.13677v1",
                "updated": "2025-01-23T14:02:51Z",
                "updated_parsed": [
                    2025,
                    1,
                    23,
                    14,
                    2,
                    51,
                    3,
                    23,
                    0
                ],
                "published": "2025-01-23T14:02:51Z",
                "published_parsed": [
                    2025,
                    1,
                    23,
                    14,
                    2,
                    51,
                    3,
                    23,
                    0
                ],
                "title": "HumorReject: Decoupling LLM Safety from Refusal Prefix via A Little\n  Humor",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "HumorReject: Decoupling LLM Safety from Refusal Prefix via A Little\n  Humor"
                },
                "summary": "Large Language Models (LLMs) commonly rely on explicit refusal prefixes for\nsafety, making them vulnerable to prefix injection attacks. We introduce\nHumorReject, a novel data-driven approach that fundamentally reimagines LLM\nsafety by decoupling it from refusal prefixes through the use of humor as an\nindirect refusal strategy. Rather than explicitly rejecting harmful\ninstructions, HumorReject responds with contextually appropriate humor that\nnaturally defuses potentially dangerous requests while maintaining engaging\ninteractions. Our approach effectively addresses the common \"over-defense\"\nissues in existing safety mechanisms, demonstrating superior robustness against\nvarious attack vectors while preserving natural and high-quality interactions\non legitimate tasks. Our findings suggest that innovations at the data level\nare even more fundamental than the alignment algorithm itself in achieving\neffective LLM safety, opening new directions for developing more resilient and\nuser-friendly AI systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) commonly rely on explicit refusal prefixes for\nsafety, making them vulnerable to prefix injection attacks. We introduce\nHumorReject, a novel data-driven approach that fundamentally reimagines LLM\nsafety by decoupling it from refusal prefixes through the use of humor as an\nindirect refusal strategy. Rather than explicitly rejecting harmful\ninstructions, HumorReject responds with contextually appropriate humor that\nnaturally defuses potentially dangerous requests while maintaining engaging\ninteractions. Our approach effectively addresses the common \"over-defense\"\nissues in existing safety mechanisms, demonstrating superior robustness against\nvarious attack vectors while preserving natural and high-quality interactions\non legitimate tasks. Our findings suggest that innovations at the data level\nare even more fundamental than the alignment algorithm itself in achieving\neffective LLM safety, opening new directions for developing more resilient and\nuser-friendly AI systems."
                },
                "authors": [
                    {
                        "name": "Zihui Wu"
                    },
                    {
                        "name": "Haichang Gao"
                    },
                    {
                        "name": "Jiacheng Luo"
                    },
                    {
                        "name": "Zhaoxiang Liu"
                    }
                ],
                "author_detail": {
                    "name": "Zhaoxiang Liu"
                },
                "author": "Zhaoxiang Liu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.13677v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.13677v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.13669v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.13669v1",
                "updated": "2025-01-23T13:54:53Z",
                "updated_parsed": [
                    2025,
                    1,
                    23,
                    13,
                    54,
                    53,
                    3,
                    23,
                    0
                ],
                "published": "2025-01-23T13:54:53Z",
                "published_parsed": [
                    2025,
                    1,
                    23,
                    13,
                    54,
                    53,
                    3,
                    23,
                    0
                ],
                "title": "How to Complete Domain Tuning while Keeping General Ability in LLM:\n  Adaptive Layer-wise and Element-wise Regularization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "How to Complete Domain Tuning while Keeping General Ability in LLM:\n  Adaptive Layer-wise and Element-wise Regularization"
                },
                "summary": "Large Language Models (LLMs) exhibit strong general-purpose language\ncapabilities. However, fine-tuning these models on domain-specific tasks often\nleads to catastrophic forgetting, where the model overwrites or loses essential\nknowledge acquired during pretraining. This phenomenon significantly limits the\nbroader applicability of LLMs. To address this challenge, we propose a novel\napproach to compute the element-wise importance of model parameters crucial for\npreserving general knowledge during fine-tuning. Our method utilizes a\ndual-objective optimization strategy: (1) regularization loss to retain the\nparameter crucial for general knowledge; (2) cross-entropy loss to adapt to\ndomain-specific tasks. Additionally, we introduce layer-wise coefficients to\naccount for the varying contributions of different layers, dynamically\nbalancing the dual-objective optimization. Extensive experiments on scientific,\nmedical, and physical tasks using GPT-J and LLaMA-3 demonstrate that our\napproach mitigates catastrophic forgetting while enhancing model adaptability.\nCompared to previous methods, our solution is approximately 20 times faster and\nrequires only 10%-15% of the storage, highlighting the practical efficiency.\nThe code will be released.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) exhibit strong general-purpose language\ncapabilities. However, fine-tuning these models on domain-specific tasks often\nleads to catastrophic forgetting, where the model overwrites or loses essential\nknowledge acquired during pretraining. This phenomenon significantly limits the\nbroader applicability of LLMs. To address this challenge, we propose a novel\napproach to compute the element-wise importance of model parameters crucial for\npreserving general knowledge during fine-tuning. Our method utilizes a\ndual-objective optimization strategy: (1) regularization loss to retain the\nparameter crucial for general knowledge; (2) cross-entropy loss to adapt to\ndomain-specific tasks. Additionally, we introduce layer-wise coefficients to\naccount for the varying contributions of different layers, dynamically\nbalancing the dual-objective optimization. Extensive experiments on scientific,\nmedical, and physical tasks using GPT-J and LLaMA-3 demonstrate that our\napproach mitigates catastrophic forgetting while enhancing model adaptability.\nCompared to previous methods, our solution is approximately 20 times faster and\nrequires only 10%-15% of the storage, highlighting the practical efficiency.\nThe code will be released."
                },
                "authors": [
                    {
                        "name": "Shezheng Song"
                    },
                    {
                        "name": "Hao Xu"
                    },
                    {
                        "name": "Jun Ma"
                    },
                    {
                        "name": "Shasha Li"
                    },
                    {
                        "name": "Long Peng"
                    },
                    {
                        "name": "Qian Wan"
                    },
                    {
                        "name": "Xiaodong Liu"
                    },
                    {
                        "name": "Jie Yu"
                    }
                ],
                "author_detail": {
                    "name": "Jie Yu"
                },
                "author": "Jie Yu",
                "arxiv_comment": "Work in progress",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.13669v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.13669v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.13392v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.13392v2",
                "updated": "2025-01-23T13:54:36Z",
                "updated_parsed": [
                    2025,
                    1,
                    23,
                    13,
                    54,
                    36,
                    3,
                    23,
                    0
                ],
                "published": "2024-10-17T09:42:30Z",
                "published_parsed": [
                    2024,
                    10,
                    17,
                    9,
                    42,
                    30,
                    3,
                    291,
                    0
                ],
                "title": "Judgment of Learning: A Human Ability Beyond Generative Artificial\n  Intelligence",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Judgment of Learning: A Human Ability Beyond Generative Artificial\n  Intelligence"
                },
                "summary": "Large language models (LLMs) increasingly mimic human cognition in various\nlanguage-based tasks. However, their capacity for metacognition - particularly\nin predicting memory performance - remains unexplored. Here, we introduce a\ncross-agent prediction model to assess whether ChatGPT-based LLMs align with\nhuman judgments of learning (JOL), a metacognitive measure where individuals\npredict their own future memory performance. We tested humans and LLMs on pairs\nof sentences, one of which was a garden-path sentence - a sentence that\ninitially misleads the reader toward an incorrect interpretation before\nrequiring reanalysis. By manipulating contextual fit (fitting vs. unfitting\nsentences), we probed how intrinsic cues (i.e., relatedness) affect both LLM\nand human JOL. Our results revealed that while human JOL reliably predicted\nactual memory performance, none of the tested LLMs (GPT-3.5-turbo, GPT-4-turbo,\nand GPT-4o) demonstrated comparable predictive accuracy. This discrepancy\nemerged regardless of whether sentences appeared in fitting or unfitting\ncontexts. These findings indicate that, despite LLMs' demonstrated capacity to\nmodel human cognition at the object-level, they struggle at the meta-level,\nfailing to capture the variability in individual memory predictions. By\nidentifying this shortcoming, our study underscores the need for further\nrefinements in LLMs' self-monitoring abilities, which could enhance their\nutility in educational settings, personalized learning, and human-AI\ninteractions. Strengthening LLMs' metacognitive performance may reduce the\nreliance on human oversight, paving the way for more autonomous and seamless\nintegration of AI into tasks requiring deeper cognitive awareness.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) increasingly mimic human cognition in various\nlanguage-based tasks. However, their capacity for metacognition - particularly\nin predicting memory performance - remains unexplored. Here, we introduce a\ncross-agent prediction model to assess whether ChatGPT-based LLMs align with\nhuman judgments of learning (JOL), a metacognitive measure where individuals\npredict their own future memory performance. We tested humans and LLMs on pairs\nof sentences, one of which was a garden-path sentence - a sentence that\ninitially misleads the reader toward an incorrect interpretation before\nrequiring reanalysis. By manipulating contextual fit (fitting vs. unfitting\nsentences), we probed how intrinsic cues (i.e., relatedness) affect both LLM\nand human JOL. Our results revealed that while human JOL reliably predicted\nactual memory performance, none of the tested LLMs (GPT-3.5-turbo, GPT-4-turbo,\nand GPT-4o) demonstrated comparable predictive accuracy. This discrepancy\nemerged regardless of whether sentences appeared in fitting or unfitting\ncontexts. These findings indicate that, despite LLMs' demonstrated capacity to\nmodel human cognition at the object-level, they struggle at the meta-level,\nfailing to capture the variability in individual memory predictions. By\nidentifying this shortcoming, our study underscores the need for further\nrefinements in LLMs' self-monitoring abilities, which could enhance their\nutility in educational settings, personalized learning, and human-AI\ninteractions. Strengthening LLMs' metacognitive performance may reduce the\nreliance on human oversight, paving the way for more autonomous and seamless\nintegration of AI into tasks requiring deeper cognitive awareness."
                },
                "authors": [
                    {
                        "name": "Markus Huff"
                    },
                    {
                        "name": "Elanur Ulakçı"
                    }
                ],
                "author_detail": {
                    "name": "Elanur Ulakçı"
                },
                "author": "Elanur Ulakçı",
                "arxiv_comment": "31 pages, 2 figures. Note: The appendix is similar to\n  arXiv:2403.05152 because the same stimulus material was used",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.13392v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.13392v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.18442v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.18442v3",
                "updated": "2025-01-23T13:27:12Z",
                "updated_parsed": [
                    2025,
                    1,
                    23,
                    13,
                    27,
                    12,
                    3,
                    23,
                    0
                ],
                "published": "2024-12-24T14:02:44Z",
                "published_parsed": [
                    2024,
                    12,
                    24,
                    14,
                    2,
                    44,
                    1,
                    359,
                    0
                ],
                "title": "SoK: On the Offensive Potential of AI",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SoK: On the Offensive Potential of AI"
                },
                "summary": "Our society increasingly benefits from Artificial Intelligence (AI).\nUnfortunately, more and more evidence shows that AI is also used for offensive\npurposes. Prior works have revealed various examples of use cases in which the\ndeployment of AI can lead to violation of security and privacy objectives. No\nextant work, however, has been able to draw a holistic picture of the offensive\npotential of AI. In this SoK paper we seek to lay the ground for a systematic\nanalysis of the heterogeneous capabilities of offensive AI. In particular we\n(i) account for AI risks to both humans and systems while (ii) consolidating\nand distilling knowledge from academic literature, expert opinions, industrial\nvenues, as well as laypeople -- all of which being valuable sources of\ninformation on offensive AI.\n  To enable alignment of such diverse sources of knowledge, we devise a common\nset of criteria reflecting essential technological factors related to offensive\nAI. With the help of such criteria, we systematically analyze: 95 research\npapers; 38 InfoSec briefings (from, e.g., BlackHat); the responses of a user\nstudy (N=549) entailing individuals with diverse backgrounds and expertise; and\nthe opinion of 12 experts. Our contributions not only reveal concerning ways\n(some of which overlooked by prior work) in which AI can be offensively used\ntoday, but also represent a foothold to address this threat in the years to\ncome.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Our society increasingly benefits from Artificial Intelligence (AI).\nUnfortunately, more and more evidence shows that AI is also used for offensive\npurposes. Prior works have revealed various examples of use cases in which the\ndeployment of AI can lead to violation of security and privacy objectives. No\nextant work, however, has been able to draw a holistic picture of the offensive\npotential of AI. In this SoK paper we seek to lay the ground for a systematic\nanalysis of the heterogeneous capabilities of offensive AI. In particular we\n(i) account for AI risks to both humans and systems while (ii) consolidating\nand distilling knowledge from academic literature, expert opinions, industrial\nvenues, as well as laypeople -- all of which being valuable sources of\ninformation on offensive AI.\n  To enable alignment of such diverse sources of knowledge, we devise a common\nset of criteria reflecting essential technological factors related to offensive\nAI. With the help of such criteria, we systematically analyze: 95 research\npapers; 38 InfoSec briefings (from, e.g., BlackHat); the responses of a user\nstudy (N=549) entailing individuals with diverse backgrounds and expertise; and\nthe opinion of 12 experts. Our contributions not only reveal concerning ways\n(some of which overlooked by prior work) in which AI can be offensively used\ntoday, but also represent a foothold to address this threat in the years to\ncome."
                },
                "authors": [
                    {
                        "name": "Saskia Laura Schröer"
                    },
                    {
                        "name": "Giovanni Apruzzese"
                    },
                    {
                        "name": "Soheil Human"
                    },
                    {
                        "name": "Pavel Laskov"
                    },
                    {
                        "name": "Hyrum S. Anderson"
                    },
                    {
                        "name": "Edward W. N. Bernroider"
                    },
                    {
                        "name": "Aurore Fass"
                    },
                    {
                        "name": "Ben Nassi"
                    },
                    {
                        "name": "Vera Rimmer"
                    },
                    {
                        "name": "Fabio Roli"
                    },
                    {
                        "name": "Samer Salam"
                    },
                    {
                        "name": "Ashley Shen"
                    },
                    {
                        "name": "Ali Sunyaev"
                    },
                    {
                        "name": "Tim Wadwha-Brown"
                    },
                    {
                        "name": "Isabel Wagner"
                    },
                    {
                        "name": "Gang Wang"
                    }
                ],
                "author_detail": {
                    "name": "Gang Wang"
                },
                "author": "Gang Wang",
                "arxiv_comment": "Systemization of Knowledge (SoK) paper. Accepted to the 3rd IEEE\n  Conference on Secure and Trustworthy Machine Learning (SaTML'25)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.18442v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.18442v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.13641v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.13641v1",
                "updated": "2025-01-23T13:18:52Z",
                "updated_parsed": [
                    2025,
                    1,
                    23,
                    13,
                    18,
                    52,
                    3,
                    23,
                    0
                ],
                "published": "2025-01-23T13:18:52Z",
                "published_parsed": [
                    2025,
                    1,
                    23,
                    13,
                    18,
                    52,
                    3,
                    23,
                    0
                ],
                "title": "The Road to Learning Explainable Inverse Kinematic Models: Graph Neural\n  Networks as Inductive Bias for Symbolic Regression",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Road to Learning Explainable Inverse Kinematic Models: Graph Neural\n  Networks as Inductive Bias for Symbolic Regression"
                },
                "summary": "This paper shows how a Graph Neural Network (GNN) can be used to learn an\nInverse Kinematics (IK) based on an automatically generated dataset. The\ngenerated Inverse Kinematics is generalized to a family of manipulators with\nthe same Degree of Freedom (DOF), but varying link length configurations. The\nresults indicate a position error of less than 1.0 cm for 3 DOF and 4.5 cm for\n5 DOF, and orientation error of 2$^\\circ$ for 3 DOF and 8.2$^\\circ$ for 6 DOF,\nwhich allows the deployment to certain real world-problems. However,\nout-of-domain errors and lack of extrapolation can be observed in the resulting\nGNN. An extensive analysis of these errors indicates potential for enhancement\nin the future. Consequently, the generated GNNs are tailored to be used in\nfuture work as an inductive bias to generate analytical equations through\nsymbolic regression.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper shows how a Graph Neural Network (GNN) can be used to learn an\nInverse Kinematics (IK) based on an automatically generated dataset. The\ngenerated Inverse Kinematics is generalized to a family of manipulators with\nthe same Degree of Freedom (DOF), but varying link length configurations. The\nresults indicate a position error of less than 1.0 cm for 3 DOF and 4.5 cm for\n5 DOF, and orientation error of 2$^\\circ$ for 3 DOF and 8.2$^\\circ$ for 6 DOF,\nwhich allows the deployment to certain real world-problems. However,\nout-of-domain errors and lack of extrapolation can be observed in the resulting\nGNN. An extensive analysis of these errors indicates potential for enhancement\nin the future. Consequently, the generated GNNs are tailored to be used in\nfuture work as an inductive bias to generate analytical equations through\nsymbolic regression."
                },
                "authors": [
                    {
                        "name": "Pravin Pandey"
                    },
                    {
                        "name": "Julia Reuter"
                    },
                    {
                        "name": "Christoph Steup"
                    },
                    {
                        "name": "Sanaz Mostaghim"
                    }
                ],
                "author_detail": {
                    "name": "Sanaz Mostaghim"
                },
                "arxiv_affiliation": "Fraunhofer Institute for Transportation and Infrastructure Systems IVI, Dresden, Germany",
                "author": "Sanaz Mostaghim",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.13641v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.13641v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.04686v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.04686v2",
                "updated": "2025-01-23T13:16:39Z",
                "updated_parsed": [
                    2025,
                    1,
                    23,
                    13,
                    16,
                    39,
                    3,
                    23,
                    0
                ],
                "published": "2025-01-08T18:49:41Z",
                "published_parsed": [
                    2025,
                    1,
                    8,
                    18,
                    49,
                    41,
                    2,
                    8,
                    0
                ],
                "title": "URSA: Understanding and Verifying Chain-of-thought Reasoning in\n  Multimodal Mathematics",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "URSA: Understanding and Verifying Chain-of-thought Reasoning in\n  Multimodal Mathematics"
                },
                "summary": "Chain-of-Thought (CoT) reasoning is widely used to enhance the mathematical\nreasoning capabilities of large language models (LLMs). The introduction of\nprocess supervision for CoT trajectories has sparked discussions on improving\ntest-time scaling, thereby unlocking the System 2-style thinking capabilities\nof these models. However, in multimodal mathematical reasoning, the scarcity of\nhigh-quality CoT training data has hindered existing models from achieving both\ndeliberate reasoning and fine-grained verification. In this work, we propose a\nnovel framework that introduces System 2-style thinking to multimodal\nmathematical reasoning. We introduce a three-module CoT data synthesis process\nthat integrates CoT distillation, trajectory-format rewriting, and format\nunification. This process generates MMathCoT-1M, a high-quality CoT reasoning\ninstruction fine-tuning dataset. Furthermore, we implement a dual-view\ntrajectory labeling automation that targets both visual grounding fidelity and\ndeductive chain validity, resulting in the DualMath-1.1M dataset. The URSA-8B\nmodel, trained on MMathCoT-1M, achieves new state-of-the-art (SOTA) performance\namong similarly sized multimodal LLMs on six popular reasoning benchmarks.\nTraining URSA-8B further on the DualMath-1.1M dataset yields URSA-RM-8B, a\nverifier that enhances URSA-8B's test-time performance and surpasses strong\nclosed-source multimodal MLLMs like GPT-4o. The model weights, training data,\nand code have been open-sourced: https://github.com/URSA-MATH/URSA-MATH.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Chain-of-Thought (CoT) reasoning is widely used to enhance the mathematical\nreasoning capabilities of large language models (LLMs). The introduction of\nprocess supervision for CoT trajectories has sparked discussions on improving\ntest-time scaling, thereby unlocking the System 2-style thinking capabilities\nof these models. However, in multimodal mathematical reasoning, the scarcity of\nhigh-quality CoT training data has hindered existing models from achieving both\ndeliberate reasoning and fine-grained verification. In this work, we propose a\nnovel framework that introduces System 2-style thinking to multimodal\nmathematical reasoning. We introduce a three-module CoT data synthesis process\nthat integrates CoT distillation, trajectory-format rewriting, and format\nunification. This process generates MMathCoT-1M, a high-quality CoT reasoning\ninstruction fine-tuning dataset. Furthermore, we implement a dual-view\ntrajectory labeling automation that targets both visual grounding fidelity and\ndeductive chain validity, resulting in the DualMath-1.1M dataset. The URSA-8B\nmodel, trained on MMathCoT-1M, achieves new state-of-the-art (SOTA) performance\namong similarly sized multimodal LLMs on six popular reasoning benchmarks.\nTraining URSA-8B further on the DualMath-1.1M dataset yields URSA-RM-8B, a\nverifier that enhances URSA-8B's test-time performance and surpasses strong\nclosed-source multimodal MLLMs like GPT-4o. The model weights, training data,\nand code have been open-sourced: https://github.com/URSA-MATH/URSA-MATH."
                },
                "authors": [
                    {
                        "name": "Ruilin Luo"
                    },
                    {
                        "name": "Zhuofan Zheng"
                    },
                    {
                        "name": "Yifan Wang"
                    },
                    {
                        "name": "Yiyao Yu"
                    },
                    {
                        "name": "Xinzhe Ni"
                    },
                    {
                        "name": "Zicheng Lin"
                    },
                    {
                        "name": "Jin Zeng"
                    },
                    {
                        "name": "Yujiu Yang"
                    }
                ],
                "author_detail": {
                    "name": "Yujiu Yang"
                },
                "author": "Yujiu Yang",
                "arxiv_comment": "Add benchmarks and baselines. 27 pages, 11 tables, 17 figures.\n  Models, training data and code have been open-sourced. Project url:\n  https://ursa-math.github.io",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.04686v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.04686v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.13624v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.13624v1",
                "updated": "2025-01-23T12:45:20Z",
                "updated_parsed": [
                    2025,
                    1,
                    23,
                    12,
                    45,
                    20,
                    3,
                    23,
                    0
                ],
                "published": "2025-01-23T12:45:20Z",
                "published_parsed": [
                    2025,
                    1,
                    23,
                    12,
                    45,
                    20,
                    3,
                    23,
                    0
                ],
                "title": "QMamba: Post-Training Quantization for Vision State Space Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "QMamba: Post-Training Quantization for Vision State Space Models"
                },
                "summary": "State Space Models (SSMs), as key components of Mamaba, have gained\nincreasing attention for vision models recently, thanks to their efficient long\nsequence modeling capability. Given the computational cost of deploying SSMs on\nresource-limited edge devices, Post-Training Quantization (PTQ) is a technique\nwith the potential for efficient deployment of SSMs. In this work, we propose\nQMamba, one of the first PTQ frameworks to our knowledge, designed for vision\nSSMs based on the analysis of the activation distributions in SSMs. We reveal\nthat the distribution of discrete parameters exhibits long-tailed skewness and\nthe distribution of the hidden state sequence exhibits highly dynamic\nvariations. Correspondingly, we design Long-tailed Skewness Quantization (LtSQ)\nto quantize discrete parameters and Temporal Group Quantization (TGQ) to\nquantize hidden states, which reduces the quantization errors. Extensive\nexperiments demonstrate that QMamba outperforms advanced PTQ methods on vision\nmodels across multiple model sizes and architectures. Notably, QMamba surpasses\nexisting methods by 21.0% on ImageNet classification with 4-bit activations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "State Space Models (SSMs), as key components of Mamaba, have gained\nincreasing attention for vision models recently, thanks to their efficient long\nsequence modeling capability. Given the computational cost of deploying SSMs on\nresource-limited edge devices, Post-Training Quantization (PTQ) is a technique\nwith the potential for efficient deployment of SSMs. In this work, we propose\nQMamba, one of the first PTQ frameworks to our knowledge, designed for vision\nSSMs based on the analysis of the activation distributions in SSMs. We reveal\nthat the distribution of discrete parameters exhibits long-tailed skewness and\nthe distribution of the hidden state sequence exhibits highly dynamic\nvariations. Correspondingly, we design Long-tailed Skewness Quantization (LtSQ)\nto quantize discrete parameters and Temporal Group Quantization (TGQ) to\nquantize hidden states, which reduces the quantization errors. Extensive\nexperiments demonstrate that QMamba outperforms advanced PTQ methods on vision\nmodels across multiple model sizes and architectures. Notably, QMamba surpasses\nexisting methods by 21.0% on ImageNet classification with 4-bit activations."
                },
                "authors": [
                    {
                        "name": "Yinglong Li"
                    },
                    {
                        "name": "Xiaoyu Liu"
                    },
                    {
                        "name": "Jiacheng Li"
                    },
                    {
                        "name": "Ruikang Xu"
                    },
                    {
                        "name": "Yinda Chen"
                    },
                    {
                        "name": "Zhiwei Xiong"
                    }
                ],
                "author_detail": {
                    "name": "Zhiwei Xiong"
                },
                "author": "Zhiwei Xiong",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.13624v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.13624v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.11551v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.11551v2",
                "updated": "2025-01-23T12:42:49Z",
                "updated_parsed": [
                    2025,
                    1,
                    23,
                    12,
                    42,
                    49,
                    3,
                    23,
                    0
                ],
                "published": "2025-01-20T15:39:39Z",
                "published_parsed": [
                    2025,
                    1,
                    20,
                    15,
                    39,
                    39,
                    0,
                    20,
                    0
                ],
                "title": "PIKE-RAG: sPecIalized KnowledgE and Rationale Augmented Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PIKE-RAG: sPecIalized KnowledgE and Rationale Augmented Generation"
                },
                "summary": "Despite notable advancements in Retrieval-Augmented Generation (RAG) systems\nthat expand large language model (LLM) capabilities through external retrieval,\nthese systems often struggle to meet the complex and diverse needs of\nreal-world industrial applications. The reliance on retrieval alone proves\ninsufficient for extracting deep, domain-specific knowledge performing in\nlogical reasoning from specialized corpora. To address this, we introduce\nsPecIalized KnowledgE and Rationale Augmentation Generation (PIKE-RAG),\nfocusing on extracting, understanding, and applying specialized knowledge,\nwhile constructing coherent rationale to incrementally steer LLMs toward\naccurate responses. Recognizing the diverse challenges of industrial tasks, we\nintroduce a new paradigm that classifies tasks based on their complexity in\nknowledge extraction and application, allowing for a systematic evaluation of\nRAG systems' problem-solving capabilities. This strategic approach offers a\nroadmap for the phased development and enhancement of RAG systems, tailored to\nmeet the evolving demands of industrial applications. Furthermore, we propose\nknowledge atomizing and knowledge-aware task decomposition to effectively\nextract multifaceted knowledge from the data chunks and iteratively construct\nthe rationale based on original query and the accumulated knowledge,\nrespectively, showcasing exceptional performance across various benchmarks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Despite notable advancements in Retrieval-Augmented Generation (RAG) systems\nthat expand large language model (LLM) capabilities through external retrieval,\nthese systems often struggle to meet the complex and diverse needs of\nreal-world industrial applications. The reliance on retrieval alone proves\ninsufficient for extracting deep, domain-specific knowledge performing in\nlogical reasoning from specialized corpora. To address this, we introduce\nsPecIalized KnowledgE and Rationale Augmentation Generation (PIKE-RAG),\nfocusing on extracting, understanding, and applying specialized knowledge,\nwhile constructing coherent rationale to incrementally steer LLMs toward\naccurate responses. Recognizing the diverse challenges of industrial tasks, we\nintroduce a new paradigm that classifies tasks based on their complexity in\nknowledge extraction and application, allowing for a systematic evaluation of\nRAG systems' problem-solving capabilities. This strategic approach offers a\nroadmap for the phased development and enhancement of RAG systems, tailored to\nmeet the evolving demands of industrial applications. Furthermore, we propose\nknowledge atomizing and knowledge-aware task decomposition to effectively\nextract multifaceted knowledge from the data chunks and iteratively construct\nthe rationale based on original query and the accumulated knowledge,\nrespectively, showcasing exceptional performance across various benchmarks."
                },
                "authors": [
                    {
                        "name": "Jinyu Wang"
                    },
                    {
                        "name": "Jingjing Fu"
                    },
                    {
                        "name": "Rui Wang"
                    },
                    {
                        "name": "Lei Song"
                    },
                    {
                        "name": "Jiang Bian"
                    }
                ],
                "author_detail": {
                    "name": "Jiang Bian"
                },
                "author": "Jiang Bian",
                "arxiv_comment": "36 pages, 18 figures, technique report",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.11551v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.11551v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.12339v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.12339v2",
                "updated": "2025-01-23T12:15:42Z",
                "updated_parsed": [
                    2025,
                    1,
                    23,
                    12,
                    15,
                    42,
                    3,
                    23,
                    0
                ],
                "published": "2025-01-21T18:13:43Z",
                "published_parsed": [
                    2025,
                    1,
                    21,
                    18,
                    13,
                    43,
                    1,
                    21,
                    0
                ],
                "title": "Treefix: Enabling Execution with a Tree of Prefixes",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Treefix: Enabling Execution with a Tree of Prefixes"
                },
                "summary": "The ability to execute code is a prerequisite for various dynamic program\nanalyses. Learning-guided execution has been proposed as an approach to enable\nthe execution of arbitrary code snippets by letting a neural model predict\nlikely values for any missing variables. Although state-of-the-art\nlearning-guided execution approaches, such as LExecutor, can enable the\nexecution of a relative high amount of code, they are limited to predicting a\nrestricted set of possible values and do not use any feedback from previous\nexecutions to execute even more code. This paper presents Treefix, a novel\nlearning-guided execution approach that leverages LLMs to iteratively create\ncode prefixes that enable the execution of a given code snippet. The approach\naddresses the problem in a multi-step fashion, where each step uses feedback\nabout the code snippet and its execution to instruct an LLM to improve a\npreviously generated prefix. This process iteratively creates a tree of\nprefixes, a subset of which is returned to the user as prefixes that maximize\nthe number of executed lines in the code snippet. In our experiments with two\ndatasets of Python code snippets, Treefix achieves 25% and 7% more coverage\nrelative to the current state of the art in learning-guided execution, covering\na total of 84% and 82% of all lines in the code snippets.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The ability to execute code is a prerequisite for various dynamic program\nanalyses. Learning-guided execution has been proposed as an approach to enable\nthe execution of arbitrary code snippets by letting a neural model predict\nlikely values for any missing variables. Although state-of-the-art\nlearning-guided execution approaches, such as LExecutor, can enable the\nexecution of a relative high amount of code, they are limited to predicting a\nrestricted set of possible values and do not use any feedback from previous\nexecutions to execute even more code. This paper presents Treefix, a novel\nlearning-guided execution approach that leverages LLMs to iteratively create\ncode prefixes that enable the execution of a given code snippet. The approach\naddresses the problem in a multi-step fashion, where each step uses feedback\nabout the code snippet and its execution to instruct an LLM to improve a\npreviously generated prefix. This process iteratively creates a tree of\nprefixes, a subset of which is returned to the user as prefixes that maximize\nthe number of executed lines in the code snippet. In our experiments with two\ndatasets of Python code snippets, Treefix achieves 25% and 7% more coverage\nrelative to the current state of the art in learning-guided execution, covering\na total of 84% and 82% of all lines in the code snippets."
                },
                "authors": [
                    {
                        "name": "Beatriz Souza"
                    },
                    {
                        "name": "Michael Pradel"
                    }
                ],
                "author_detail": {
                    "name": "Michael Pradel"
                },
                "author": "Michael Pradel",
                "arxiv_comment": "Accepted in research track of the IEEE/ACM International Conference\n  on Software Engineering (ICSE) 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.12339v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.12339v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.13604v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.13604v1",
                "updated": "2025-01-23T12:12:59Z",
                "updated_parsed": [
                    2025,
                    1,
                    23,
                    12,
                    12,
                    59,
                    3,
                    23,
                    0
                ],
                "published": "2025-01-23T12:12:59Z",
                "published_parsed": [
                    2025,
                    1,
                    23,
                    12,
                    12,
                    59,
                    3,
                    23,
                    0
                ],
                "title": "FedPref: Federated Learning Across Heterogeneous Multi-objective\n  Preferences",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FedPref: Federated Learning Across Heterogeneous Multi-objective\n  Preferences"
                },
                "summary": "Federated Learning (FL) is a distributed machine learning strategy, developed\nfor settings where training data is owned by distributed devices and cannot be\nshared. FL circumvents this constraint by carrying out model training in\ndistribution. The parameters of these local models are shared intermittently\namong participants and aggregated to enhance model accuracy. This strategy has\nbeen rapidly adopted by the industry in efforts to overcome privacy and\nresource constraints in model training. However, the application of FL to\nreal-world settings brings additional challenges associated with heterogeneity\nbetween participants. Research into mitigating these difficulties in FL has\nlargely focused on only two types of heterogeneity: the unbalanced distribution\nof training data, and differences in client resources. Yet more types of\nheterogeneity are becoming relevant as the capability of FL expands to cover\nmore complex problems, from the tuning of LLMs to enabling machine learning on\nedge devices. In this work, we discuss a novel type of heterogeneity that is\nlikely to become increasingly relevant in future applications: this is\npreference heterogeneity, emerging when clients learn under multiple\nobjectives, with different importance assigned to each objective on different\nclients. In this work, we discuss the implications of this type of\nheterogeneity and propose FedPref, a first algorithm designed to facilitate\npersonalised FL in this setting. We demonstrate the effectiveness of the\nalgorithm across different problems, preference distributions and model\narchitectures. In addition, we introduce a new analytical point of view, based\non multi-objective metrics, for evaluating the performance of FL algorithms in\nthis setting beyond the traditional client-focused metrics. We perform a second\nexperimental analysis based in this view, and show that FedPref outperforms\ncompared algorithms.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Federated Learning (FL) is a distributed machine learning strategy, developed\nfor settings where training data is owned by distributed devices and cannot be\nshared. FL circumvents this constraint by carrying out model training in\ndistribution. The parameters of these local models are shared intermittently\namong participants and aggregated to enhance model accuracy. This strategy has\nbeen rapidly adopted by the industry in efforts to overcome privacy and\nresource constraints in model training. However, the application of FL to\nreal-world settings brings additional challenges associated with heterogeneity\nbetween participants. Research into mitigating these difficulties in FL has\nlargely focused on only two types of heterogeneity: the unbalanced distribution\nof training data, and differences in client resources. Yet more types of\nheterogeneity are becoming relevant as the capability of FL expands to cover\nmore complex problems, from the tuning of LLMs to enabling machine learning on\nedge devices. In this work, we discuss a novel type of heterogeneity that is\nlikely to become increasingly relevant in future applications: this is\npreference heterogeneity, emerging when clients learn under multiple\nobjectives, with different importance assigned to each objective on different\nclients. In this work, we discuss the implications of this type of\nheterogeneity and propose FedPref, a first algorithm designed to facilitate\npersonalised FL in this setting. We demonstrate the effectiveness of the\nalgorithm across different problems, preference distributions and model\narchitectures. In addition, we introduce a new analytical point of view, based\non multi-objective metrics, for evaluating the performance of FL algorithms in\nthis setting beyond the traditional client-focused metrics. We perform a second\nexperimental analysis based in this view, and show that FedPref outperforms\ncompared algorithms."
                },
                "authors": [
                    {
                        "name": "Maria Hartmann"
                    },
                    {
                        "name": "Grégoire Danoy"
                    },
                    {
                        "name": "Pascal Bouvry"
                    }
                ],
                "author_detail": {
                    "name": "Pascal Bouvry"
                },
                "author": "Pascal Bouvry",
                "arxiv_doi": "10.1145/3708984",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3708984",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2501.13604v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.13604v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "Accepted to ACM ToMPECS journal",
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.05200v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.05200v4",
                "updated": "2025-01-23T12:06:37Z",
                "updated_parsed": [
                    2025,
                    1,
                    23,
                    12,
                    6,
                    37,
                    3,
                    23,
                    0
                ],
                "published": "2024-08-09T17:44:45Z",
                "published_parsed": [
                    2024,
                    8,
                    9,
                    17,
                    44,
                    45,
                    4,
                    222,
                    0
                ],
                "title": "KIF: Knowledge Identification and Fusion for Language Model Continual\n  Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "KIF: Knowledge Identification and Fusion for Language Model Continual\n  Learning"
                },
                "summary": "Language model continual learning (CL) has recently attracted significant\ninterest for its ability to adapt large language models (LLMs) to dynamic\nreal-world scenarios without retraining. A major challenge in this domain is\ncatastrophic forgetting, where models lose previously acquired knowledge upon\nlearning new tasks. Existing approaches commonly utilize multiple\nparameter-efficient fine-tuning (PEFT) blocks to acquire task-specific\nknowledge, yet these methods are inefficient and fail to leverage potential\nknowledge transfer across tasks. In this paper, we introduce a novel CL\nframework for language models, named Knowledge Identification and Fusion (KIF),\nwhich boosts knowledge transfer without depending on memory replay. KIF\ninitially segregates the model into 'skill units' based on parameter\ndependencies, allowing for more precise control. Subsequently, it employs a\nnovel group-wise knowledge identification technique to ascertain the importance\ndistribution of skill units for a new task. By comparing this importance\ndistribution with those from previous tasks, we implement a fine-grained\nknowledge fusion strategy that retains task-specific knowledge, thereby\npreventing forgetting, and updates task-shared knowledge, which facilitates\nbi-directional knowledge transfer. As a result, KIF achieves an optimal balance\nbetween retaining prior knowledge and excelling in new tasks. KIF also\ndemonstrates strong generalizability, making it suitable for various base\nmodels and adaptable to PEFT methods like LoRA. Furthermore, it offers notable\nextensibility, supporting enhancements through integration with memory replay\ntechniques. Comprehensive experiments conducted on two CL benchmarks, involving\nmodels ranging from 220M to 7B parameters, affirm the effectiveness of KIF and\nits variants across different settings.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Language model continual learning (CL) has recently attracted significant\ninterest for its ability to adapt large language models (LLMs) to dynamic\nreal-world scenarios without retraining. A major challenge in this domain is\ncatastrophic forgetting, where models lose previously acquired knowledge upon\nlearning new tasks. Existing approaches commonly utilize multiple\nparameter-efficient fine-tuning (PEFT) blocks to acquire task-specific\nknowledge, yet these methods are inefficient and fail to leverage potential\nknowledge transfer across tasks. In this paper, we introduce a novel CL\nframework for language models, named Knowledge Identification and Fusion (KIF),\nwhich boosts knowledge transfer without depending on memory replay. KIF\ninitially segregates the model into 'skill units' based on parameter\ndependencies, allowing for more precise control. Subsequently, it employs a\nnovel group-wise knowledge identification technique to ascertain the importance\ndistribution of skill units for a new task. By comparing this importance\ndistribution with those from previous tasks, we implement a fine-grained\nknowledge fusion strategy that retains task-specific knowledge, thereby\npreventing forgetting, and updates task-shared knowledge, which facilitates\nbi-directional knowledge transfer. As a result, KIF achieves an optimal balance\nbetween retaining prior knowledge and excelling in new tasks. KIF also\ndemonstrates strong generalizability, making it suitable for various base\nmodels and adaptable to PEFT methods like LoRA. Furthermore, it offers notable\nextensibility, supporting enhancements through integration with memory replay\ntechniques. Comprehensive experiments conducted on two CL benchmarks, involving\nmodels ranging from 220M to 7B parameters, affirm the effectiveness of KIF and\nits variants across different settings."
                },
                "authors": [
                    {
                        "name": "Yujie Feng"
                    },
                    {
                        "name": "Xu Chu"
                    },
                    {
                        "name": "Yongxin Xu"
                    },
                    {
                        "name": "Zexin Lu"
                    },
                    {
                        "name": "Bo Liu"
                    },
                    {
                        "name": "Philip S. Yu"
                    },
                    {
                        "name": "Xiao-Ming Wu"
                    }
                ],
                "author_detail": {
                    "name": "Xiao-Ming Wu"
                },
                "author": "Xiao-Ming Wu",
                "arxiv_comment": "This version updates the model name from Task Skill Localization and\n  Consolidation (TaSL) to Knowledge Identification and Fusion (KIF). It is an\n  extension of the ACL 2024 paper titled Continual Dialog State Tracking via\n  Task Skill Localization and Consolidation",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.05200v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.05200v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.13594v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.13594v1",
                "updated": "2025-01-23T12:03:29Z",
                "updated_parsed": [
                    2025,
                    1,
                    23,
                    12,
                    3,
                    29,
                    3,
                    23,
                    0
                ],
                "published": "2025-01-23T12:03:29Z",
                "published_parsed": [
                    2025,
                    1,
                    23,
                    12,
                    3,
                    29,
                    3,
                    23,
                    0
                ],
                "title": "Text-to-SQL based on Large Language Models and Database Keyword Search",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Text-to-SQL based on Large Language Models and Database Keyword Search"
                },
                "summary": "Text-to-SQL prompt strategies based on Large Language Models (LLMs) achieve\nremarkable performance on well-known benchmarks. However, when applied to\nreal-world databases, their performance is significantly less than for these\nbenchmarks, especially for Natural Language (NL) questions requiring complex\nfilters and joins to be processed. This paper then proposes a strategy to\ncompile NL questions into SQL queries that incorporates a dynamic few-shot\nexamples strategy and leverages the services provided by a database keyword\nsearch (KwS) platform. The paper details how the precision and recall of the\nschema-linking process are improved with the help of the examples provided and\nthe keyword-matching service that the KwS platform offers. Then, it shows how\nthe KwS platform can be used to synthesize a view that captures the joins\nrequired to process an input NL question and thereby simplify the SQL query\ncompilation step. The paper includes experiments with a real-world relational\ndatabase to assess the performance of the proposed strategy. The experiments\nsuggest that the strategy achieves an accuracy on the real-world relational\ndatabase that surpasses state-of-the-art approaches. The paper concludes by\ndiscussing the results obtained.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Text-to-SQL prompt strategies based on Large Language Models (LLMs) achieve\nremarkable performance on well-known benchmarks. However, when applied to\nreal-world databases, their performance is significantly less than for these\nbenchmarks, especially for Natural Language (NL) questions requiring complex\nfilters and joins to be processed. This paper then proposes a strategy to\ncompile NL questions into SQL queries that incorporates a dynamic few-shot\nexamples strategy and leverages the services provided by a database keyword\nsearch (KwS) platform. The paper details how the precision and recall of the\nschema-linking process are improved with the help of the examples provided and\nthe keyword-matching service that the KwS platform offers. Then, it shows how\nthe KwS platform can be used to synthesize a view that captures the joins\nrequired to process an input NL question and thereby simplify the SQL query\ncompilation step. The paper includes experiments with a real-world relational\ndatabase to assess the performance of the proposed strategy. The experiments\nsuggest that the strategy achieves an accuracy on the real-world relational\ndatabase that surpasses state-of-the-art approaches. The paper concludes by\ndiscussing the results obtained."
                },
                "authors": [
                    {
                        "name": "Eduardo R. Nascimento"
                    },
                    {
                        "name": "Caio Viktor S. Avila"
                    },
                    {
                        "name": "Yenier T. Izquierdo"
                    },
                    {
                        "name": "Grettel M. García"
                    },
                    {
                        "name": "Lucas Feijó L. Andrade"
                    },
                    {
                        "name": "Michelle S. P. Facina"
                    },
                    {
                        "name": "Melissa Lemos"
                    },
                    {
                        "name": "Marco A. Casanova"
                    }
                ],
                "author_detail": {
                    "name": "Marco A. Casanova"
                },
                "arxiv_affiliation": "Departamento de Informática, PUC-Rio, Rio de Janeiro, Brazil",
                "author": "Marco A. Casanova",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.13594v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.13594v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "68T50",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "H.2.3; I.2.7",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.13587v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.13587v1",
                "updated": "2025-01-23T11:55:13Z",
                "updated_parsed": [
                    2025,
                    1,
                    23,
                    11,
                    55,
                    13,
                    3,
                    23,
                    0
                ],
                "published": "2025-01-23T11:55:13Z",
                "published_parsed": [
                    2025,
                    1,
                    23,
                    11,
                    55,
                    13,
                    3,
                    23,
                    0
                ],
                "title": "Contrastive Representation Learning Helps Cross-institutional Knowledge\n  Transfer: A Study in Pediatric Ventilation Management",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Contrastive Representation Learning Helps Cross-institutional Knowledge\n  Transfer: A Study in Pediatric Ventilation Management"
                },
                "summary": "Clinical machine learning deployment across institutions faces significant\nchallenges when patient populations and clinical practices differ\nsubstantially. We present a systematic framework for cross-institutional\nknowledge transfer in clinical time series, demonstrated through pediatric\nventilation management between a general pediatric intensive care unit (PICU)\nand a cardiac-focused unit. Using contrastive predictive coding (CPC) for\nrepresentation learning, we investigate how different data regimes and\nfine-tuning strategies affect knowledge transfer across institutional\nboundaries. Our results show that while direct model transfer performs poorly,\nCPC with appropriate fine-tuning enables effective knowledge sharing between\ninstitutions, with benefits particularly evident in limited data scenarios.\nAnalysis of transfer patterns reveals an important asymmetry: temporal\nprogression patterns transfer more readily than point-of-care decisions,\nsuggesting practical pathways for cross-institutional deployment. Through a\nsystematic evaluation of fine-tuning approaches and transfer patterns, our work\nprovides insights for developing more generalizable clinical decision support\nsystems while enabling smaller specialized units to leverage knowledge from\nlarger centers.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Clinical machine learning deployment across institutions faces significant\nchallenges when patient populations and clinical practices differ\nsubstantially. We present a systematic framework for cross-institutional\nknowledge transfer in clinical time series, demonstrated through pediatric\nventilation management between a general pediatric intensive care unit (PICU)\nand a cardiac-focused unit. Using contrastive predictive coding (CPC) for\nrepresentation learning, we investigate how different data regimes and\nfine-tuning strategies affect knowledge transfer across institutional\nboundaries. Our results show that while direct model transfer performs poorly,\nCPC with appropriate fine-tuning enables effective knowledge sharing between\ninstitutions, with benefits particularly evident in limited data scenarios.\nAnalysis of transfer patterns reveals an important asymmetry: temporal\nprogression patterns transfer more readily than point-of-care decisions,\nsuggesting practical pathways for cross-institutional deployment. Through a\nsystematic evaluation of fine-tuning approaches and transfer patterns, our work\nprovides insights for developing more generalizable clinical decision support\nsystems while enabling smaller specialized units to leverage knowledge from\nlarger centers."
                },
                "authors": [
                    {
                        "name": "Yuxuan"
                    },
                    {
                        "name": "Liu"
                    },
                    {
                        "name": "Jinpei Han"
                    },
                    {
                        "name": "Padmanabhan Ramnarayan"
                    },
                    {
                        "name": "A. Aldo Faisal"
                    }
                ],
                "author_detail": {
                    "name": "A. Aldo Faisal"
                },
                "arxiv_affiliation": "Edison",
                "author": "A. Aldo Faisal",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.13587v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.13587v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.13573v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.13573v1",
                "updated": "2025-01-23T11:23:25Z",
                "updated_parsed": [
                    2025,
                    1,
                    23,
                    11,
                    23,
                    25,
                    3,
                    23,
                    0
                ],
                "published": "2025-01-23T11:23:25Z",
                "published_parsed": [
                    2025,
                    1,
                    23,
                    11,
                    23,
                    25,
                    3,
                    23,
                    0
                ],
                "title": "Improving Contextual Faithfulness of Large Language Models via Retrieval\n  Heads-Induced Optimization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Improving Contextual Faithfulness of Large Language Models via Retrieval\n  Heads-Induced Optimization"
                },
                "summary": "Ensuring contextual faithfulness in retrieval-augmented large language models\n(LLMs) is crucial for building trustworthy information-seeking systems,\nparticularly in long-form question-answering (LFQA) scenarios. In this work, we\nidentify a salient correlation between LFQA faithfulness and retrieval heads, a\nset of attention heads responsible for retrieving contextual information.\nLeveraging this insight, we propose RHIO, a framework designed to teach LLMs to\nexplicitly discriminate between faithful and unfaithful generations. RHIO first\naugments unfaithful samples that simulate realistic model-intrinsic errors by\nselectively masking retrieval heads. Then, these samples are incorporated into\njoint training, enabling the model to distinguish unfaithful outputs from\nfaithful ones conditioned on control tokens. Furthermore, these control tokens\nare leveraged to self-induce contrastive outputs, amplifying their difference\nthrough contrastive decoding. Additionally, to facilitate the evaluation of\ncontextual faithfulness, we also introduce GroundBench, a comprehensive\nbenchmark compiled from five existing LFQA datasets. Extensive experimental\nresults on GroundBench demonstrate that RHIO significantly improves\nfaithfulness, even outperforming GPT-4o.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Ensuring contextual faithfulness in retrieval-augmented large language models\n(LLMs) is crucial for building trustworthy information-seeking systems,\nparticularly in long-form question-answering (LFQA) scenarios. In this work, we\nidentify a salient correlation between LFQA faithfulness and retrieval heads, a\nset of attention heads responsible for retrieving contextual information.\nLeveraging this insight, we propose RHIO, a framework designed to teach LLMs to\nexplicitly discriminate between faithful and unfaithful generations. RHIO first\naugments unfaithful samples that simulate realistic model-intrinsic errors by\nselectively masking retrieval heads. Then, these samples are incorporated into\njoint training, enabling the model to distinguish unfaithful outputs from\nfaithful ones conditioned on control tokens. Furthermore, these control tokens\nare leveraged to self-induce contrastive outputs, amplifying their difference\nthrough contrastive decoding. Additionally, to facilitate the evaluation of\ncontextual faithfulness, we also introduce GroundBench, a comprehensive\nbenchmark compiled from five existing LFQA datasets. Extensive experimental\nresults on GroundBench demonstrate that RHIO significantly improves\nfaithfulness, even outperforming GPT-4o."
                },
                "authors": [
                    {
                        "name": "Lei Huang"
                    },
                    {
                        "name": "Xiaocheng Feng"
                    },
                    {
                        "name": "Weitao Ma"
                    },
                    {
                        "name": "Yuchun Fan"
                    },
                    {
                        "name": "Xiachong Feng"
                    },
                    {
                        "name": "Yangfan Ye"
                    },
                    {
                        "name": "Weihong Zhong"
                    },
                    {
                        "name": "Yuxuan Gu"
                    },
                    {
                        "name": "Baoxin Wang"
                    },
                    {
                        "name": "Dayong Wu"
                    },
                    {
                        "name": "Guoping Hu"
                    },
                    {
                        "name": "Bing Qin"
                    }
                ],
                "author_detail": {
                    "name": "Bing Qin"
                },
                "author": "Bing Qin",
                "arxiv_comment": "Submitted to ARR October 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.13573v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.13573v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.00132v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.00132v3",
                "updated": "2025-01-23T11:22:20Z",
                "updated_parsed": [
                    2025,
                    1,
                    23,
                    11,
                    22,
                    20,
                    3,
                    23,
                    0
                ],
                "published": "2024-06-28T08:45:02Z",
                "published_parsed": [
                    2024,
                    6,
                    28,
                    8,
                    45,
                    2,
                    4,
                    180,
                    0
                ],
                "title": "ShortcutsBench: A Large-Scale Real-world Benchmark for API-based Agents",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ShortcutsBench: A Large-Scale Real-world Benchmark for API-based Agents"
                },
                "summary": "Recent advancements in integrating large language models (LLMs) with\napplication programming interfaces (APIs) have gained significant interest in\nboth academia and industry. Recent work demonstrates that these API-based\nagents exhibit relatively strong autonomy and planning capabilities. However,\ntheir ability to handle multi-dimensional difficulty levels, diverse task\ntypes, and real-world demands remains unknown. In this paper, we introduce\n\\textsc{ShortcutsBench}, a large-scale benchmark for the comprehensive\nevaluation of API-based agents in solving real-world complex tasks.\n\\textsc{ShortcutsBench} includes a wealth of real APIs from Apple Inc., refined\nuser queries, human-annotated high-quality action sequences, detailed parameter\nfilling values, and parameters requesting necessary input from the system or\nuser. We revealed how existing benchmarks~/~datasets struggle to accommodate\nthe advanced reasoning capabilities of existing more intelligent LLMs.\nMoreover, our extensive evaluation of agents built with $5$ leading open-source\n(size $\\geq$ 57B) and $5$ closed-source LLMs (e.g. Gemini-1.5-Pro and\nGPT-4o-mini) with varying intelligence level reveals significant limitations of\nexisting API-based agents in the whole process of handling complex queries\nrelated to API selection, parameter filling, and requesting necessary input\nfrom the system and the user. These findings highlight the great challenges\nthat API-based agents face in effectively fulfilling real and complex user\nqueries. All datasets, code, experimental logs, and results are available at\n\\url{https://github.com/EachSheep/ShortcutsBench}.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advancements in integrating large language models (LLMs) with\napplication programming interfaces (APIs) have gained significant interest in\nboth academia and industry. Recent work demonstrates that these API-based\nagents exhibit relatively strong autonomy and planning capabilities. However,\ntheir ability to handle multi-dimensional difficulty levels, diverse task\ntypes, and real-world demands remains unknown. In this paper, we introduce\n\\textsc{ShortcutsBench}, a large-scale benchmark for the comprehensive\nevaluation of API-based agents in solving real-world complex tasks.\n\\textsc{ShortcutsBench} includes a wealth of real APIs from Apple Inc., refined\nuser queries, human-annotated high-quality action sequences, detailed parameter\nfilling values, and parameters requesting necessary input from the system or\nuser. We revealed how existing benchmarks~/~datasets struggle to accommodate\nthe advanced reasoning capabilities of existing more intelligent LLMs.\nMoreover, our extensive evaluation of agents built with $5$ leading open-source\n(size $\\geq$ 57B) and $5$ closed-source LLMs (e.g. Gemini-1.5-Pro and\nGPT-4o-mini) with varying intelligence level reveals significant limitations of\nexisting API-based agents in the whole process of handling complex queries\nrelated to API selection, parameter filling, and requesting necessary input\nfrom the system and the user. These findings highlight the great challenges\nthat API-based agents face in effectively fulfilling real and complex user\nqueries. All datasets, code, experimental logs, and results are available at\n\\url{https://github.com/EachSheep/ShortcutsBench}."
                },
                "authors": [
                    {
                        "name": "Haiyang Shen"
                    },
                    {
                        "name": "Yue Li"
                    },
                    {
                        "name": "Desong Meng"
                    },
                    {
                        "name": "Dongqi Cai"
                    },
                    {
                        "name": "Sheng Qi"
                    },
                    {
                        "name": "Li Zhang"
                    },
                    {
                        "name": "Mengwei Xu"
                    },
                    {
                        "name": "Yun Ma"
                    }
                ],
                "author_detail": {
                    "name": "Yun Ma"
                },
                "author": "Yun Ma",
                "arxiv_comment": "ICLR'25: https://openreview.net/forum?id=kKILfPkhSz",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.00132v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.00132v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.12883v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.12883v2",
                "updated": "2025-01-23T11:12:59Z",
                "updated_parsed": [
                    2025,
                    1,
                    23,
                    11,
                    12,
                    59,
                    3,
                    23,
                    0
                ],
                "published": "2025-01-22T13:44:44Z",
                "published_parsed": [
                    2025,
                    1,
                    22,
                    13,
                    44,
                    44,
                    2,
                    22,
                    0
                ],
                "title": "Generative AI Misuse Potential in Cyber Security Education: A Case Study\n  of a UK Degree Program",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Generative AI Misuse Potential in Cyber Security Education: A Case Study\n  of a UK Degree Program"
                },
                "summary": "Recent advances in generative artificial intelligence (AI), such as ChatGPT,\nGoogle Gemini, and other large language models (LLMs), pose significant\nchallenges to upholding academic integrity in higher education. This paper\ninvestigates the susceptibility of a Master's-level cyber security degree\nprogram at a UK Russell Group university, accredited by a leading national\nbody, to LLM misuse. Through the application and extension of a quantitative\nassessment framework, we identify a high exposure to misuse, particularly in\nindependent project- and report-based assessments. Contributing factors,\nincluding block teaching and a predominantly international cohort, are\nhighlighted as potential amplifiers of these vulnerabilities. To address these\nchallenges, we discuss the adoption of LLM-resistant assessments, detection\ntools, and the importance of fostering an ethical learning environment. These\napproaches aim to uphold academic standards while preparing students for the\ncomplexities of real-world cyber security.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advances in generative artificial intelligence (AI), such as ChatGPT,\nGoogle Gemini, and other large language models (LLMs), pose significant\nchallenges to upholding academic integrity in higher education. This paper\ninvestigates the susceptibility of a Master's-level cyber security degree\nprogram at a UK Russell Group university, accredited by a leading national\nbody, to LLM misuse. Through the application and extension of a quantitative\nassessment framework, we identify a high exposure to misuse, particularly in\nindependent project- and report-based assessments. Contributing factors,\nincluding block teaching and a predominantly international cohort, are\nhighlighted as potential amplifiers of these vulnerabilities. To address these\nchallenges, we discuss the adoption of LLM-resistant assessments, detection\ntools, and the importance of fostering an ethical learning environment. These\napproaches aim to uphold academic standards while preparing students for the\ncomplexities of real-world cyber security."
                },
                "authors": [
                    {
                        "name": "Carlton Shepherd"
                    }
                ],
                "author_detail": {
                    "name": "Carlton Shepherd"
                },
                "author": "Carlton Shepherd",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.12883v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.12883v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.09916v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.09916v3",
                "updated": "2025-01-23T11:03:13Z",
                "updated_parsed": [
                    2025,
                    1,
                    23,
                    11,
                    3,
                    13,
                    3,
                    23,
                    0
                ],
                "published": "2024-08-19T11:44:40Z",
                "published_parsed": [
                    2024,
                    8,
                    19,
                    11,
                    44,
                    40,
                    0,
                    232,
                    0
                ],
                "title": "Attribution Analysis Meets Model Editing: Advancing Knowledge Correction\n  in Vision Language Models with VisEdit",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Attribution Analysis Meets Model Editing: Advancing Knowledge Correction\n  in Vision Language Models with VisEdit"
                },
                "summary": "Model editing aims to correct outdated or erroneous knowledge in large models\nwithout costly retraining. Recent research discovered that the mid-layer\nrepresentation of the subject's final token in a prompt has a strong influence\non factual predictions, and developed Large Language Model (LLM) editing\ntechniques based on this observation. However, for Vision-LLMs (VLLMs), how\nvisual representations impact the predictions from a decoder-only language\nmodel remains largely unexplored. To the best of our knowledge, model editing\nfor VLLMs has not been extensively studied in the literature. In this work, we\nemploy the contribution allocation and noise perturbation methods to measure\nthe contributions of visual representations for token predictions. Our\nattribution analysis shows that visual representations in mid-to-later layers\nthat are highly relevant to the prompt contribute significantly to predictions.\nBased on these insights, we propose VisEdit, a novel model editor for VLLMs\nthat effectively corrects knowledge by editing intermediate visual\nrepresentations in regions important to the edit prompt. We evaluated VisEdit\nusing multiple VLLM backbones and public VLLM editing benchmark datasets. The\nresults show the superiority of VisEdit over the strong baselines adapted from\nexisting state-of-the-art editors for LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Model editing aims to correct outdated or erroneous knowledge in large models\nwithout costly retraining. Recent research discovered that the mid-layer\nrepresentation of the subject's final token in a prompt has a strong influence\non factual predictions, and developed Large Language Model (LLM) editing\ntechniques based on this observation. However, for Vision-LLMs (VLLMs), how\nvisual representations impact the predictions from a decoder-only language\nmodel remains largely unexplored. To the best of our knowledge, model editing\nfor VLLMs has not been extensively studied in the literature. In this work, we\nemploy the contribution allocation and noise perturbation methods to measure\nthe contributions of visual representations for token predictions. Our\nattribution analysis shows that visual representations in mid-to-later layers\nthat are highly relevant to the prompt contribute significantly to predictions.\nBased on these insights, we propose VisEdit, a novel model editor for VLLMs\nthat effectively corrects knowledge by editing intermediate visual\nrepresentations in regions important to the edit prompt. We evaluated VisEdit\nusing multiple VLLM backbones and public VLLM editing benchmark datasets. The\nresults show the superiority of VisEdit over the strong baselines adapted from\nexisting state-of-the-art editors for LLMs."
                },
                "authors": [
                    {
                        "name": "Qizhou Chen"
                    },
                    {
                        "name": "Taolin Zhang"
                    },
                    {
                        "name": "Chengyu Wang"
                    },
                    {
                        "name": "Xiaofeng He"
                    },
                    {
                        "name": "Dakan Wang"
                    },
                    {
                        "name": "Tingting Liu"
                    }
                ],
                "author_detail": {
                    "name": "Tingting Liu"
                },
                "author": "Tingting Liu",
                "arxiv_comment": "Accepted to AAAI-2025 as an oral presentation",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.09916v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.09916v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2401.06769v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2401.06769v3",
                "updated": "2025-01-23T10:59:49Z",
                "updated_parsed": [
                    2025,
                    1,
                    23,
                    10,
                    59,
                    49,
                    3,
                    23,
                    0
                ],
                "published": "2024-01-12T18:59:02Z",
                "published_parsed": [
                    2024,
                    1,
                    12,
                    18,
                    59,
                    2,
                    4,
                    12,
                    0
                ],
                "title": "Machine Translation Models are Zero-Shot Detectors of Translation\n  Direction",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Machine Translation Models are Zero-Shot Detectors of Translation\n  Direction"
                },
                "summary": "Detecting the translation direction of parallel text has applications for\nmachine translation training and evaluation, but also has forensic applications\nsuch as resolving plagiarism or forgery allegations. In this work, we explore\nan unsupervised approach to translation direction detection based on the simple\nhypothesis that\n$p(\\text{translation}|\\text{original})>p(\\text{original}|\\text{translation})$,\nmotivated by the well-known simplification effect in translationese or\nmachine-translationese. In experiments with massively multilingual machine\ntranslation models across 20 translation directions, we confirm the\neffectiveness of the approach for high-resource language pairs, achieving\ndocument-level accuracies of 82--96% for NMT-produced translations, and 60--81%\nfor human translations, depending on the model used. Code and demo are\navailable at https://github.com/ZurichNLP/translation-direction-detection",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Detecting the translation direction of parallel text has applications for\nmachine translation training and evaluation, but also has forensic applications\nsuch as resolving plagiarism or forgery allegations. In this work, we explore\nan unsupervised approach to translation direction detection based on the simple\nhypothesis that\n$p(\\text{translation}|\\text{original})>p(\\text{original}|\\text{translation})$,\nmotivated by the well-known simplification effect in translationese or\nmachine-translationese. In experiments with massively multilingual machine\ntranslation models across 20 translation directions, we confirm the\neffectiveness of the approach for high-resource language pairs, achieving\ndocument-level accuracies of 82--96% for NMT-produced translations, and 60--81%\nfor human translations, depending on the model used. Code and demo are\navailable at https://github.com/ZurichNLP/translation-direction-detection"
                },
                "authors": [
                    {
                        "name": "Michelle Wastl"
                    },
                    {
                        "name": "Jannis Vamvas"
                    },
                    {
                        "name": "Rico Sennrich"
                    }
                ],
                "author_detail": {
                    "name": "Rico Sennrich"
                },
                "author": "Rico Sennrich",
                "arxiv_comment": "21 pages, 2 figures, 10 tables in main part, 12 tables in appendix.\n  Added to this version: alternative supervised system, performance on LLM\n  generated translations, additional running text in appendix, typo fixes",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2401.06769v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2401.06769v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.13545v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.13545v1",
                "updated": "2025-01-23T10:46:14Z",
                "updated_parsed": [
                    2025,
                    1,
                    23,
                    10,
                    46,
                    14,
                    3,
                    23,
                    0
                ],
                "published": "2025-01-23T10:46:14Z",
                "published_parsed": [
                    2025,
                    1,
                    23,
                    10,
                    46,
                    14,
                    3,
                    23,
                    0
                ],
                "title": "LLMs Can Plan Only If We Tell Them",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLMs Can Plan Only If We Tell Them"
                },
                "summary": "Large language models (LLMs) have demonstrated significant capabilities in\nnatural language processing and reasoning, yet their effectiveness in\nautonomous planning has been under debate. While existing studies have utilized\nLLMs with external feedback mechanisms or in controlled environments for\nplanning, these approaches often involve substantial computational and\ndevelopment resources due to the requirement for careful design and iterative\nbackprompting. Moreover, even the most advanced LLMs like GPT-4 struggle to\nmatch human performance on standard planning benchmarks, such as the\nBlocksworld, without additional support. This paper investigates whether LLMs\ncan independently generate long-horizon plans that rival human baselines. Our\nnovel enhancements to Algorithm-of-Thoughts (AoT), which we dub AoT+, help\nachieve state-of-the-art results in planning benchmarks out-competing prior\nmethods and human baselines all autonomously.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have demonstrated significant capabilities in\nnatural language processing and reasoning, yet their effectiveness in\nautonomous planning has been under debate. While existing studies have utilized\nLLMs with external feedback mechanisms or in controlled environments for\nplanning, these approaches often involve substantial computational and\ndevelopment resources due to the requirement for careful design and iterative\nbackprompting. Moreover, even the most advanced LLMs like GPT-4 struggle to\nmatch human performance on standard planning benchmarks, such as the\nBlocksworld, without additional support. This paper investigates whether LLMs\ncan independently generate long-horizon plans that rival human baselines. Our\nnovel enhancements to Algorithm-of-Thoughts (AoT), which we dub AoT+, help\nachieve state-of-the-art results in planning benchmarks out-competing prior\nmethods and human baselines all autonomously."
                },
                "authors": [
                    {
                        "name": "Bilgehan Sel"
                    },
                    {
                        "name": "Ruoxi Jia"
                    },
                    {
                        "name": "Ming Jin"
                    }
                ],
                "author_detail": {
                    "name": "Ming Jin"
                },
                "author": "Ming Jin",
                "arxiv_comment": "ICLR 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.13545v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.13545v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.13506v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.13506v1",
                "updated": "2025-01-23T09:37:13Z",
                "updated_parsed": [
                    2025,
                    1,
                    23,
                    9,
                    37,
                    13,
                    3,
                    23,
                    0
                ],
                "published": "2025-01-23T09:37:13Z",
                "published_parsed": [
                    2025,
                    1,
                    23,
                    9,
                    37,
                    13,
                    3,
                    23,
                    0
                ],
                "title": "Inverted finite elements approximation of the Neumann problem for second\n  order elliptic equations in exterior two-dimensional domains",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Inverted finite elements approximation of the Neumann problem for second\n  order elliptic equations in exterior two-dimensional domains"
                },
                "summary": "We use inverted finite elements method for approximating solutions of second\norder elliptic equations with non-constant coefficients varying to infinity in\nthe exterior of a 2D bounded obstacle, when a Neumann boundary condition is\nconsidered. After proposing an appropriate functional framework for the\ndeployment of the method, we analyse its convergence and detail its\nimplementation. Numerical tests performed after implementation confirm\nconvergence and high efficiency of the method.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We use inverted finite elements method for approximating solutions of second\norder elliptic equations with non-constant coefficients varying to infinity in\nthe exterior of a 2D bounded obstacle, when a Neumann boundary condition is\nconsidered. After proposing an appropriate functional framework for the\ndeployment of the method, we analyse its convergence and detail its\nimplementation. Numerical tests performed after implementation confirm\nconvergence and high efficiency of the method."
                },
                "authors": [
                    {
                        "name": "R Belbaki"
                    },
                    {
                        "name": "S K Bhowmik"
                    },
                    {
                        "name": "T Z Boulmezaoud"
                    },
                    {
                        "name": "N Kerdid"
                    },
                    {
                        "name": "S Mziou"
                    }
                ],
                "author_detail": {
                    "name": "S Mziou"
                },
                "arxiv_affiliation": "LAMMDA",
                "author": "S Mziou",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.13506v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.13506v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "math.NA",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "math.NA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.11433v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.11433v2",
                "updated": "2025-01-23T09:29:19Z",
                "updated_parsed": [
                    2025,
                    1,
                    23,
                    9,
                    29,
                    19,
                    3,
                    23,
                    0
                ],
                "published": "2025-01-20T12:12:09Z",
                "published_parsed": [
                    2025,
                    1,
                    20,
                    12,
                    12,
                    9,
                    0,
                    20,
                    0
                ],
                "title": "One Does Not Simply Meme Alone: Evaluating Co-Creativity Between LLMs\n  and Humans in the Generation of Humor",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "One Does Not Simply Meme Alone: Evaluating Co-Creativity Between LLMs\n  and Humans in the Generation of Humor"
                },
                "summary": "Collaboration has been shown to enhance creativity, leading to more\ninnovative and effective outcomes. While previous research has explored the\nabilities of Large Language Models (LLMs) to serve as co-creative partners in\ntasks like writing poetry or creating narratives, the collaborative potential\nof LLMs in humor-rich and culturally nuanced domains remains an open question.\nTo address this gap, we conducted a user study to explore the potential of LLMs\nin co-creating memes - a humor-driven and culturally specific form of creative\nexpression. We conducted a user study with three groups of 50 participants\neach: a human-only group creating memes without AI assistance, a human-AI\ncollaboration group interacting with a state-of-the-art LLM model, and an\nAI-only group where the LLM autonomously generated memes. We assessed the\nquality of the generated memes through crowdsourcing, with each meme rated on\ncreativity, humor, and shareability. Our results showed that LLM assistance\nincreased the number of ideas generated and reduced the effort participants\nfelt. However, it did not improve the quality of the memes when humans\ncollaborated with LLM. Interestingly, memes created entirely by AI performed\nbetter than both human-only and human-AI collaborative memes in all areas on\naverage. However, when looking at the top-performing memes, human-created ones\nwere better in humor, while human-AI collaborations stood out in creativity and\nshareability. These findings highlight the complexities of human-AI\ncollaboration in creative tasks. While AI can boost productivity and create\ncontent that appeals to a broad audience, human creativity remains crucial for\ncontent that connects on a deeper level.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Collaboration has been shown to enhance creativity, leading to more\ninnovative and effective outcomes. While previous research has explored the\nabilities of Large Language Models (LLMs) to serve as co-creative partners in\ntasks like writing poetry or creating narratives, the collaborative potential\nof LLMs in humor-rich and culturally nuanced domains remains an open question.\nTo address this gap, we conducted a user study to explore the potential of LLMs\nin co-creating memes - a humor-driven and culturally specific form of creative\nexpression. We conducted a user study with three groups of 50 participants\neach: a human-only group creating memes without AI assistance, a human-AI\ncollaboration group interacting with a state-of-the-art LLM model, and an\nAI-only group where the LLM autonomously generated memes. We assessed the\nquality of the generated memes through crowdsourcing, with each meme rated on\ncreativity, humor, and shareability. Our results showed that LLM assistance\nincreased the number of ideas generated and reduced the effort participants\nfelt. However, it did not improve the quality of the memes when humans\ncollaborated with LLM. Interestingly, memes created entirely by AI performed\nbetter than both human-only and human-AI collaborative memes in all areas on\naverage. However, when looking at the top-performing memes, human-created ones\nwere better in humor, while human-AI collaborations stood out in creativity and\nshareability. These findings highlight the complexities of human-AI\ncollaboration in creative tasks. While AI can boost productivity and create\ncontent that appeals to a broad audience, human creativity remains crucial for\ncontent that connects on a deeper level."
                },
                "authors": [
                    {
                        "name": "Zhikun Wu"
                    },
                    {
                        "name": "Thomas Weber"
                    },
                    {
                        "name": "Florian Müller"
                    }
                ],
                "author_detail": {
                    "name": "Florian Müller"
                },
                "arxiv_affiliation": "TU Darmstadt",
                "author": "Florian Müller",
                "arxiv_doi": "10.1145/3708359.3712094",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3708359.3712094",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2501.11433v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.11433v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "to appear in: 30th International Conference on Intelligent User\n  Interfaces IUI 25 March 2427 2025 Cagliari Italy",
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.13503v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.13503v1",
                "updated": "2025-01-23T09:28:04Z",
                "updated_parsed": [
                    2025,
                    1,
                    23,
                    9,
                    28,
                    4,
                    3,
                    23,
                    0
                ],
                "published": "2025-01-23T09:28:04Z",
                "published_parsed": [
                    2025,
                    1,
                    23,
                    9,
                    28,
                    4,
                    3,
                    23,
                    0
                ],
                "title": "Benchmark Study of Transient Stability during Power-Hardware-in-the-Loop\n  and Fault-Ride-Through capabilities of PV inverters",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Benchmark Study of Transient Stability during Power-Hardware-in-the-Loop\n  and Fault-Ride-Through capabilities of PV inverters"
                },
                "summary": "The deployment of PV inverters is rapidly expanding across Europe, where\nthese devices must increasingly comply with stringent grid requirements.This\nstudy presents a benchmark analysis of four PV inverter manufacturers, focusing\non their Fault Ride Through capabilities under varying grid strengths, voltage\ndips, and fault durations, parameters critical for grid operators during fault\nconditions.The findings highlight the influence of different inverter controls\non key metrics such as total harmonic distortion of current and voltage\nsignals, as well as system stability following grid faults.Additionally, the\nstudy evaluates transient stability using two distinct testing approaches.The\nfirst approach employs the current standard method, which is testing with an\nideal voltage source. The second utilizes a Power Hardware in the Loop\nmethodology with a benchmark CIGRE grid model.The results reveal that while\ntesting with an ideal voltage source is cost-effective and convenient in the\nshort term, it lacks the ability to capture the dynamic interactions and\nfeedback loops of physical grid components.This limitation can obscure critical\nreal world factors, potentially leading to unexpected inverter behavior and\noperational challenges in grids with high PV penetration.This study underscores\nthe importance of re-evaluating conventional testing methods and incorporating\nPower Hardware in the Loop structures to achieve test results that more closely\nalign with real-world conditions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The deployment of PV inverters is rapidly expanding across Europe, where\nthese devices must increasingly comply with stringent grid requirements.This\nstudy presents a benchmark analysis of four PV inverter manufacturers, focusing\non their Fault Ride Through capabilities under varying grid strengths, voltage\ndips, and fault durations, parameters critical for grid operators during fault\nconditions.The findings highlight the influence of different inverter controls\non key metrics such as total harmonic distortion of current and voltage\nsignals, as well as system stability following grid faults.Additionally, the\nstudy evaluates transient stability using two distinct testing approaches.The\nfirst approach employs the current standard method, which is testing with an\nideal voltage source. The second utilizes a Power Hardware in the Loop\nmethodology with a benchmark CIGRE grid model.The results reveal that while\ntesting with an ideal voltage source is cost-effective and convenient in the\nshort term, it lacks the ability to capture the dynamic interactions and\nfeedback loops of physical grid components.This limitation can obscure critical\nreal world factors, potentially leading to unexpected inverter behavior and\noperational challenges in grids with high PV penetration.This study underscores\nthe importance of re-evaluating conventional testing methods and incorporating\nPower Hardware in the Loop structures to achieve test results that more closely\nalign with real-world conditions."
                },
                "authors": [
                    {
                        "name": "Carina Lehmal"
                    },
                    {
                        "name": "Ziqian Zhang"
                    },
                    {
                        "name": "Philipp Hackl"
                    },
                    {
                        "name": "Robert Schürhuber"
                    }
                ],
                "author_detail": {
                    "name": "Robert Schürhuber"
                },
                "author": "Robert Schürhuber",
                "arxiv_comment": "7 pages, 9 figures, study of behaviour of different inverters during\n  different grid strength",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.13503v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.13503v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.13492v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.13492v1",
                "updated": "2025-01-23T09:14:15Z",
                "updated_parsed": [
                    2025,
                    1,
                    23,
                    9,
                    14,
                    15,
                    3,
                    23,
                    0
                ],
                "published": "2025-01-23T09:14:15Z",
                "published_parsed": [
                    2025,
                    1,
                    23,
                    9,
                    14,
                    15,
                    3,
                    23,
                    0
                ],
                "title": "Quantized Spike-driven Transformer",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Quantized Spike-driven Transformer"
                },
                "summary": "Spiking neural networks are emerging as a promising energy-efficient\nalternative to traditional artificial neural networks due to their spike-driven\nparadigm. However, recent research in the SNN domain has mainly focused on\nenhancing accuracy by designing large-scale Transformer structures, which\ntypically rely on substantial computational resources, limiting their\ndeployment on resource-constrained devices. To overcome this challenge, we\npropose a quantized spike-driven Transformer baseline (QSD-Transformer), which\nachieves reduced resource demands by utilizing a low bit-width parameter.\nRegrettably, the QSD-Transformer often suffers from severe performance\ndegradation. In this paper, we first conduct empirical analysis and find that\nthe bimodal distribution of quantized spike-driven self-attention (Q-SDSA)\nleads to spike information distortion (SID) during quantization, causing\nsignificant performance degradation. To mitigate this issue, we take\ninspiration from mutual information entropy and propose a bi-level optimization\nstrategy to rectify the information distribution in Q-SDSA. Specifically, at\nthe lower level, we introduce an information-enhanced LIF to rectify the\ninformation distribution in Q-SDSA. At the upper level, we propose a\nfine-grained distillation scheme for the QSD-Transformer to align the\ndistribution in Q-SDSA with that in the counterpart ANN. By integrating the\nbi-level optimization strategy, the QSD-Transformer can attain enhanced energy\nefficiency without sacrificing its high-performance advantage.For instance,\nwhen compared to the prior SNN benchmark on ImageNet, the QSD-Transformer\nachieves 80.3\\% top-1 accuracy, accompanied by significant reductions of\n6.0$\\times$ and 8.1$\\times$ in power consumption and model size, respectively.\nCode is available at https://github.com/bollossom/QSD-Transformer.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Spiking neural networks are emerging as a promising energy-efficient\nalternative to traditional artificial neural networks due to their spike-driven\nparadigm. However, recent research in the SNN domain has mainly focused on\nenhancing accuracy by designing large-scale Transformer structures, which\ntypically rely on substantial computational resources, limiting their\ndeployment on resource-constrained devices. To overcome this challenge, we\npropose a quantized spike-driven Transformer baseline (QSD-Transformer), which\nachieves reduced resource demands by utilizing a low bit-width parameter.\nRegrettably, the QSD-Transformer often suffers from severe performance\ndegradation. In this paper, we first conduct empirical analysis and find that\nthe bimodal distribution of quantized spike-driven self-attention (Q-SDSA)\nleads to spike information distortion (SID) during quantization, causing\nsignificant performance degradation. To mitigate this issue, we take\ninspiration from mutual information entropy and propose a bi-level optimization\nstrategy to rectify the information distribution in Q-SDSA. Specifically, at\nthe lower level, we introduce an information-enhanced LIF to rectify the\ninformation distribution in Q-SDSA. At the upper level, we propose a\nfine-grained distillation scheme for the QSD-Transformer to align the\ndistribution in Q-SDSA with that in the counterpart ANN. By integrating the\nbi-level optimization strategy, the QSD-Transformer can attain enhanced energy\nefficiency without sacrificing its high-performance advantage.For instance,\nwhen compared to the prior SNN benchmark on ImageNet, the QSD-Transformer\nachieves 80.3\\% top-1 accuracy, accompanied by significant reductions of\n6.0$\\times$ and 8.1$\\times$ in power consumption and model size, respectively.\nCode is available at https://github.com/bollossom/QSD-Transformer."
                },
                "authors": [
                    {
                        "name": "Xuerui Qiu"
                    },
                    {
                        "name": "Jieyuan Zhang"
                    },
                    {
                        "name": "Wenjie Wei"
                    },
                    {
                        "name": "Honglin Cao"
                    },
                    {
                        "name": "Junsheng Guo"
                    },
                    {
                        "name": "Rui-Jie Zhu"
                    },
                    {
                        "name": "Yimeng Shan"
                    },
                    {
                        "name": "Yang Yang"
                    },
                    {
                        "name": "Malu Zhang"
                    },
                    {
                        "name": "Haizhou Li"
                    }
                ],
                "author_detail": {
                    "name": "Haizhou Li"
                },
                "author": "Haizhou Li",
                "arxiv_comment": "Accepted by ICLR 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.13492v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.13492v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.13491v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.13491v1",
                "updated": "2025-01-23T09:14:07Z",
                "updated_parsed": [
                    2025,
                    1,
                    23,
                    9,
                    14,
                    7,
                    3,
                    23,
                    0
                ],
                "published": "2025-01-23T09:14:07Z",
                "published_parsed": [
                    2025,
                    1,
                    23,
                    9,
                    14,
                    7,
                    3,
                    23,
                    0
                ],
                "title": "RECALL: Library-Like Behavior In Language Models is Enhanced by\n  Self-Referencing Causal Cycles",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RECALL: Library-Like Behavior In Language Models is Enhanced by\n  Self-Referencing Causal Cycles"
                },
                "summary": "We introduce the concept of the self-referencing causal cycle (abbreviated\nRECALL) - a mechanism that enables large language models (LLMs) to bypass the\nlimitations of unidirectional causality, which underlies a phenomenon known as\nthe reversal curse. When an LLM is prompted with sequential data, it often\nfails to recall preceding context. For example, when we ask an LLM to recall\nthe line preceding \"O say does that star-spangled banner yet wave\" in the U.S.\nNational Anthem, it often fails to correctly return \"Gave proof through the\nnight that our flag was still there\" - this is due to the reversal curse. It\noccurs because language models such as ChatGPT and Llama generate text based on\npreceding tokens, requiring facts to be learned and reproduced in a consistent\ntoken order. While the reversal curse is often viewed as a limitation, we offer\nevidence of an alternative view: it is not always an obstacle in practice. We\nfind that RECALL is driven by what we designate as cycle tokens - sequences\nthat connect different parts of the training data, enabling recall of preceding\ntokens from succeeding ones. Through rigorous probabilistic formalization and\ncontrolled experiments, we demonstrate how the cycles they induce influence a\nmodel's ability to reproduce information. To facilitate reproducibility, we\nprovide our code and experimental details at\nhttps://anonymous.4open.science/r/remember-B0B8/.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce the concept of the self-referencing causal cycle (abbreviated\nRECALL) - a mechanism that enables large language models (LLMs) to bypass the\nlimitations of unidirectional causality, which underlies a phenomenon known as\nthe reversal curse. When an LLM is prompted with sequential data, it often\nfails to recall preceding context. For example, when we ask an LLM to recall\nthe line preceding \"O say does that star-spangled banner yet wave\" in the U.S.\nNational Anthem, it often fails to correctly return \"Gave proof through the\nnight that our flag was still there\" - this is due to the reversal curse. It\noccurs because language models such as ChatGPT and Llama generate text based on\npreceding tokens, requiring facts to be learned and reproduced in a consistent\ntoken order. While the reversal curse is often viewed as a limitation, we offer\nevidence of an alternative view: it is not always an obstacle in practice. We\nfind that RECALL is driven by what we designate as cycle tokens - sequences\nthat connect different parts of the training data, enabling recall of preceding\ntokens from succeeding ones. Through rigorous probabilistic formalization and\ncontrolled experiments, we demonstrate how the cycles they induce influence a\nmodel's ability to reproduce information. To facilitate reproducibility, we\nprovide our code and experimental details at\nhttps://anonymous.4open.science/r/remember-B0B8/."
                },
                "authors": [
                    {
                        "name": "Munachiso Nwadike"
                    },
                    {
                        "name": "Zangir Iklassov"
                    },
                    {
                        "name": "Toluwani Aremu"
                    },
                    {
                        "name": "Tatsuya Hiraoka"
                    },
                    {
                        "name": "Velibor Bojkovic"
                    },
                    {
                        "name": "Benjamin Heinzerling"
                    },
                    {
                        "name": "Hilal Alqaubeh"
                    },
                    {
                        "name": "Martin Takáč"
                    },
                    {
                        "name": "Kentaro Inui"
                    }
                ],
                "author_detail": {
                    "name": "Kentaro Inui"
                },
                "author": "Kentaro Inui",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.13491v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.13491v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.04454v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.04454v2",
                "updated": "2025-01-23T09:11:30Z",
                "updated_parsed": [
                    2025,
                    1,
                    23,
                    9,
                    11,
                    30,
                    3,
                    23,
                    0
                ],
                "published": "2024-10-06T11:41:39Z",
                "published_parsed": [
                    2024,
                    10,
                    6,
                    11,
                    41,
                    39,
                    6,
                    280,
                    0
                ],
                "title": "Inner-Probe: Discovering Copyright-related Data Generation in LLM\n  Architecture",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Inner-Probe: Discovering Copyright-related Data Generation in LLM\n  Architecture"
                },
                "summary": "Large Language Models (LLMs) utilize extensive knowledge databases and show\npowerful text generation ability. However, their reliance on high-quality\ncopyrighted datasets raises concerns about copyright infringements in generated\ntexts. Current research often employs prompt engineering or semantic\nclassifiers to identify copyrighted content, but these approaches have two\nsignificant limitations: (1) Challenging to identify which specific sub-dataset\n(e.g., works from particular authors) influences an LLM's output. (2) Treating\nthe entire training database as copyrighted, hence overlooking the inclusion of\nnon-copyrighted training data.\n  We propose InnerProbe, a lightweight framework designed to evaluate the\ninfluence of copyrighted sub-datasets on LLM-generated texts. Unlike\ntraditional methods relying solely on text, we discover that the results of\nmulti-head attention (MHA) during LLM output generation provide more effective\ninformation. Thus, InnerProbe performs sub-dataset contribution analysis using\na lightweight LSTM-based network trained on MHA results in a supervised manner.\nHarnessing such a prior, InnerProbe enables non-copyrighted text detection\nthrough a concatenated global projector trained with unsupervised contrastive\nlearning. InnerProbe demonstrates 3x improved efficiency compared to semantic\nmodel training in sub-dataset contribution analysis on Books3, achieves\n15.04%-58.7% higher accuracy over baselines on the Pile, and delivers a 0.104\nincrease in AUC for non-copyrighted data filtering.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) utilize extensive knowledge databases and show\npowerful text generation ability. However, their reliance on high-quality\ncopyrighted datasets raises concerns about copyright infringements in generated\ntexts. Current research often employs prompt engineering or semantic\nclassifiers to identify copyrighted content, but these approaches have two\nsignificant limitations: (1) Challenging to identify which specific sub-dataset\n(e.g., works from particular authors) influences an LLM's output. (2) Treating\nthe entire training database as copyrighted, hence overlooking the inclusion of\nnon-copyrighted training data.\n  We propose InnerProbe, a lightweight framework designed to evaluate the\ninfluence of copyrighted sub-datasets on LLM-generated texts. Unlike\ntraditional methods relying solely on text, we discover that the results of\nmulti-head attention (MHA) during LLM output generation provide more effective\ninformation. Thus, InnerProbe performs sub-dataset contribution analysis using\na lightweight LSTM-based network trained on MHA results in a supervised manner.\nHarnessing such a prior, InnerProbe enables non-copyrighted text detection\nthrough a concatenated global projector trained with unsupervised contrastive\nlearning. InnerProbe demonstrates 3x improved efficiency compared to semantic\nmodel training in sub-dataset contribution analysis on Books3, achieves\n15.04%-58.7% higher accuracy over baselines on the Pile, and delivers a 0.104\nincrease in AUC for non-copyrighted data filtering."
                },
                "authors": [
                    {
                        "name": "Qichao Ma"
                    },
                    {
                        "name": "Rui-Jie Zhu"
                    },
                    {
                        "name": "Peiye Liu"
                    },
                    {
                        "name": "Renye Yan"
                    },
                    {
                        "name": "Fahong Zhang"
                    },
                    {
                        "name": "Ling Liang"
                    },
                    {
                        "name": "Meng Li"
                    },
                    {
                        "name": "Zhaofei Yu"
                    },
                    {
                        "name": "Zongwei Wang"
                    },
                    {
                        "name": "Yimao Cai"
                    },
                    {
                        "name": "Tiejun Huang"
                    }
                ],
                "author_detail": {
                    "name": "Tiejun Huang"
                },
                "author": "Tiejun Huang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.04454v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.04454v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.13480v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.13480v1",
                "updated": "2025-01-23T08:53:12Z",
                "updated_parsed": [
                    2025,
                    1,
                    23,
                    8,
                    53,
                    12,
                    3,
                    23,
                    0
                ],
                "published": "2025-01-23T08:53:12Z",
                "published_parsed": [
                    2025,
                    1,
                    23,
                    8,
                    53,
                    12,
                    3,
                    23,
                    0
                ],
                "title": "Adaptive Testing for LLM-Based Applications: A Diversity-based Approach",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Adaptive Testing for LLM-Based Applications: A Diversity-based Approach"
                },
                "summary": "The recent surge of building software systems powered by Large Language\nModels (LLMs) has led to the development of various testing frameworks,\nprimarily focused on treating prompt templates as the unit of testing. Despite\nthe significant costs associated with test input execution and output\nassessment, the curation of optimized test suites is yet overlooked in these\ntools, which calls for tailored test selection or prioritization strategies. In\nthis paper, we show that diversity-based testing techniques, such as Adaptive\nRandom Testing (ART) with appropriate string distance metrics, can be\neffectively applied to the testing of prompt templates. Our proposed adaptive\ntesting approach adjusts the conventional ART process to this context by\nselecting new test inputs based on scores derived from existing test suite and\ntheir labelling results. Our results, obtained using various implementations\nthat explore several string-based distances, confirm that our approach enables\nthe discovery of failures with reduced testing budgets and promotes the\ngeneration of more varied outputs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The recent surge of building software systems powered by Large Language\nModels (LLMs) has led to the development of various testing frameworks,\nprimarily focused on treating prompt templates as the unit of testing. Despite\nthe significant costs associated with test input execution and output\nassessment, the curation of optimized test suites is yet overlooked in these\ntools, which calls for tailored test selection or prioritization strategies. In\nthis paper, we show that diversity-based testing techniques, such as Adaptive\nRandom Testing (ART) with appropriate string distance metrics, can be\neffectively applied to the testing of prompt templates. Our proposed adaptive\ntesting approach adjusts the conventional ART process to this context by\nselecting new test inputs based on scores derived from existing test suite and\ntheir labelling results. Our results, obtained using various implementations\nthat explore several string-based distances, confirm that our approach enables\nthe discovery of failures with reduced testing budgets and promotes the\ngeneration of more varied outputs."
                },
                "authors": [
                    {
                        "name": "Juyeon Yoon"
                    },
                    {
                        "name": "Robert Feldt"
                    },
                    {
                        "name": "Shin Yoo"
                    }
                ],
                "author_detail": {
                    "name": "Shin Yoo"
                },
                "author": "Shin Yoo",
                "arxiv_comment": "9 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.13480v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.13480v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.19825v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.19825v2",
                "updated": "2025-01-23T08:45:52Z",
                "updated_parsed": [
                    2025,
                    1,
                    23,
                    8,
                    45,
                    52,
                    3,
                    23,
                    0
                ],
                "published": "2024-07-29T09:21:52Z",
                "published_parsed": [
                    2024,
                    7,
                    29,
                    9,
                    21,
                    52,
                    0,
                    211,
                    0
                ],
                "title": "Concise Thoughts: Impact of Output Length on LLM Reasoning and Cost",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Concise Thoughts: Impact of Output Length on LLM Reasoning and Cost"
                },
                "summary": "Today's large language models (LLMs) can solve challenging question-answering\ntasks, and prompt engineering techniques, such as chain-of-thought (CoT), have\ngained attention for enhancing the explanation and correctness of outputs.\nHowever, many models and techniques tend to produce excessively verbose and\nlengthy answers, leading to issues with both conciseness and generation time.\nTo address this, this paper analyzes the impact of output lengths on LLM\ninference pipelines by introducing and proposing novel metrics to evaluate the\n\\textit{correct conciseness} of a model and related prompting techniques. Then,\nwe examine the impact of controlling output length through a refined prompt\nengineering strategy, Constrained-CoT (CCoT), which encourages the model to\nproduce more concise outputs. To better understand the effects of such a\nprompt, we also introduce two additional scores for analyzing the conciseness,\nmeasured in terms of redundancy and information flow in generated answers.\nExperiments on pretrained LLMs and multiple datasets demonstrate the benefits\nof the proposed metrics and the effectiveness of CCoT across different models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Today's large language models (LLMs) can solve challenging question-answering\ntasks, and prompt engineering techniques, such as chain-of-thought (CoT), have\ngained attention for enhancing the explanation and correctness of outputs.\nHowever, many models and techniques tend to produce excessively verbose and\nlengthy answers, leading to issues with both conciseness and generation time.\nTo address this, this paper analyzes the impact of output lengths on LLM\ninference pipelines by introducing and proposing novel metrics to evaluate the\n\\textit{correct conciseness} of a model and related prompting techniques. Then,\nwe examine the impact of controlling output length through a refined prompt\nengineering strategy, Constrained-CoT (CCoT), which encourages the model to\nproduce more concise outputs. To better understand the effects of such a\nprompt, we also introduce two additional scores for analyzing the conciseness,\nmeasured in terms of redundancy and information flow in generated answers.\nExperiments on pretrained LLMs and multiple datasets demonstrate the benefits\nof the proposed metrics and the effectiveness of CCoT across different models."
                },
                "authors": [
                    {
                        "name": "Sania Nayab"
                    },
                    {
                        "name": "Giulio Rossolini"
                    },
                    {
                        "name": "Marco Simoni"
                    },
                    {
                        "name": "Andrea Saracino"
                    },
                    {
                        "name": "Giorgio Buttazzo"
                    },
                    {
                        "name": "Nicolamaria Manes"
                    },
                    {
                        "name": "Fabrizio Giacomelli"
                    }
                ],
                "author_detail": {
                    "name": "Fabrizio Giacomelli"
                },
                "author": "Fabrizio Giacomelli",
                "arxiv_comment": "Preprint version, under review",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.19825v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.19825v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.09686v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.09686v3",
                "updated": "2025-01-23T08:44:44Z",
                "updated_parsed": [
                    2025,
                    1,
                    23,
                    8,
                    44,
                    44,
                    3,
                    23,
                    0
                ],
                "published": "2025-01-16T17:37:58Z",
                "published_parsed": [
                    2025,
                    1,
                    16,
                    17,
                    37,
                    58,
                    3,
                    16,
                    0
                ],
                "title": "Towards Large Reasoning Models: A Survey of Reinforced Reasoning with\n  Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Towards Large Reasoning Models: A Survey of Reinforced Reasoning with\n  Large Language Models"
                },
                "summary": "Language has long been conceived as an essential tool for human reasoning.\nThe breakthrough of Large Language Models (LLMs) has sparked significant\nresearch interest in leveraging these models to tackle complex reasoning tasks.\nResearchers have moved beyond simple autoregressive token generation by\nintroducing the concept of \"thought\" -- a sequence of tokens representing\nintermediate steps in the reasoning process. This innovative paradigm enables\nLLMs' to mimic complex human reasoning processes, such as tree search and\nreflective thinking. Recently, an emerging trend of learning to reason has\napplied reinforcement learning (RL) to train LLMs to master reasoning\nprocesses. This approach enables the automatic generation of high-quality\nreasoning trajectories through trial-and-error search algorithms, significantly\nexpanding LLMs' reasoning capacity by providing substantially more training\ndata. Furthermore, recent studies demonstrate that encouraging LLMs to \"think\"\nwith more tokens during test-time inference can further significantly boost\nreasoning accuracy. Therefore, the train-time and test-time scaling combined to\nshow a new research frontier -- a path toward Large Reasoning Model. The\nintroduction of OpenAI's o1 series marks a significant milestone in this\nresearch direction. In this survey, we present a comprehensive review of recent\nprogress in LLM reasoning. We begin by introducing the foundational background\nof LLMs and then explore the key technical components driving the development\nof large reasoning models, with a focus on automated data construction,\nlearning-to-reason techniques, and test-time scaling. We also analyze popular\nopen-source projects at building large reasoning models, and conclude with open\nchallenges and future research directions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Language has long been conceived as an essential tool for human reasoning.\nThe breakthrough of Large Language Models (LLMs) has sparked significant\nresearch interest in leveraging these models to tackle complex reasoning tasks.\nResearchers have moved beyond simple autoregressive token generation by\nintroducing the concept of \"thought\" -- a sequence of tokens representing\nintermediate steps in the reasoning process. This innovative paradigm enables\nLLMs' to mimic complex human reasoning processes, such as tree search and\nreflective thinking. Recently, an emerging trend of learning to reason has\napplied reinforcement learning (RL) to train LLMs to master reasoning\nprocesses. This approach enables the automatic generation of high-quality\nreasoning trajectories through trial-and-error search algorithms, significantly\nexpanding LLMs' reasoning capacity by providing substantially more training\ndata. Furthermore, recent studies demonstrate that encouraging LLMs to \"think\"\nwith more tokens during test-time inference can further significantly boost\nreasoning accuracy. Therefore, the train-time and test-time scaling combined to\nshow a new research frontier -- a path toward Large Reasoning Model. The\nintroduction of OpenAI's o1 series marks a significant milestone in this\nresearch direction. In this survey, we present a comprehensive review of recent\nprogress in LLM reasoning. We begin by introducing the foundational background\nof LLMs and then explore the key technical components driving the development\nof large reasoning models, with a focus on automated data construction,\nlearning-to-reason techniques, and test-time scaling. We also analyze popular\nopen-source projects at building large reasoning models, and conclude with open\nchallenges and future research directions."
                },
                "authors": [
                    {
                        "name": "Fengli Xu"
                    },
                    {
                        "name": "Qianyue Hao"
                    },
                    {
                        "name": "Zefang Zong"
                    },
                    {
                        "name": "Jingwei Wang"
                    },
                    {
                        "name": "Yunke Zhang"
                    },
                    {
                        "name": "Jingyi Wang"
                    },
                    {
                        "name": "Xiaochong Lan"
                    },
                    {
                        "name": "Jiahui Gong"
                    },
                    {
                        "name": "Tianjian Ouyang"
                    },
                    {
                        "name": "Fanjin Meng"
                    },
                    {
                        "name": "Chenyang Shao"
                    },
                    {
                        "name": "Yuwei Yan"
                    },
                    {
                        "name": "Qinglong Yang"
                    },
                    {
                        "name": "Yiwen Song"
                    },
                    {
                        "name": "Sijian Ren"
                    },
                    {
                        "name": "Xinyuan Hu"
                    },
                    {
                        "name": "Yu Li"
                    },
                    {
                        "name": "Jie Feng"
                    },
                    {
                        "name": "Chen Gao"
                    },
                    {
                        "name": "Yong Li"
                    }
                ],
                "author_detail": {
                    "name": "Yong Li"
                },
                "author": "Yong Li",
                "arxiv_comment": "36 pages, 5 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.09686v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.09686v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.14162v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.14162v4",
                "updated": "2025-01-23T08:41:05Z",
                "updated_parsed": [
                    2025,
                    1,
                    23,
                    8,
                    41,
                    5,
                    3,
                    23,
                    0
                ],
                "published": "2024-06-20T10:04:09Z",
                "published_parsed": [
                    2024,
                    6,
                    20,
                    10,
                    4,
                    9,
                    3,
                    172,
                    0
                ],
                "title": "DIRAS: Efficient LLM Annotation of Document Relevance in Retrieval\n  Augmented Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DIRAS: Efficient LLM Annotation of Document Relevance in Retrieval\n  Augmented Generation"
                },
                "summary": "Retrieval Augmented Generation (RAG) is widely employed to ground responses\nto queries on domain-specific documents. But do RAG implementations leave out\nimportant information when answering queries that need an integrated analysis\nof information (e.g., Tell me good news in the stock market today.)? To address\nthese concerns, RAG developers need to annotate information retrieval (IR) data\nfor their domain of interest, which is challenging because (1) domain-specific\nqueries usually need nuanced definitions of relevance beyond shallow semantic\nrelevance; and (2) human or GPT-4 annotation is costly and cannot cover all\n(query, document) pairs (i.e., annotation selection bias), thus harming the\neffectiveness in evaluating IR recall. To address these challenges, we propose\nDIRAS (Domain-specific Information Retrieval Annotation with Scalability), a\nmanual-annotation-free schema that fine-tunes open-sourced LLMs to consider\nnuanced relevance definition and annotate (partial) relevance labels with\ncalibrated relevance scores. Extensive evaluation shows that DIRAS enables\nsmaller (8B) LLMs to achieve GPT-4-level performance on annotating and ranking\nunseen (query, document) pairs, and is helpful for real-world RAG development.\nAll code, LLM generations, and human annotations can be found in\n\\url{https://github.com/EdisonNi-hku/DIRAS}.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Retrieval Augmented Generation (RAG) is widely employed to ground responses\nto queries on domain-specific documents. But do RAG implementations leave out\nimportant information when answering queries that need an integrated analysis\nof information (e.g., Tell me good news in the stock market today.)? To address\nthese concerns, RAG developers need to annotate information retrieval (IR) data\nfor their domain of interest, which is challenging because (1) domain-specific\nqueries usually need nuanced definitions of relevance beyond shallow semantic\nrelevance; and (2) human or GPT-4 annotation is costly and cannot cover all\n(query, document) pairs (i.e., annotation selection bias), thus harming the\neffectiveness in evaluating IR recall. To address these challenges, we propose\nDIRAS (Domain-specific Information Retrieval Annotation with Scalability), a\nmanual-annotation-free schema that fine-tunes open-sourced LLMs to consider\nnuanced relevance definition and annotate (partial) relevance labels with\ncalibrated relevance scores. Extensive evaluation shows that DIRAS enables\nsmaller (8B) LLMs to achieve GPT-4-level performance on annotating and ranking\nunseen (query, document) pairs, and is helpful for real-world RAG development.\nAll code, LLM generations, and human annotations can be found in\n\\url{https://github.com/EdisonNi-hku/DIRAS}."
                },
                "authors": [
                    {
                        "name": "Jingwei Ni"
                    },
                    {
                        "name": "Tobias Schimanski"
                    },
                    {
                        "name": "Meihong Lin"
                    },
                    {
                        "name": "Mrinmaya Sachan"
                    },
                    {
                        "name": "Elliott Ash"
                    },
                    {
                        "name": "Markus Leippold"
                    }
                ],
                "author_detail": {
                    "name": "Markus Leippold"
                },
                "author": "Markus Leippold",
                "arxiv_comment": "NAACL 2025 Long",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.14162v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.14162v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.13468v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.13468v1",
                "updated": "2025-01-23T08:33:10Z",
                "updated_parsed": [
                    2025,
                    1,
                    23,
                    8,
                    33,
                    10,
                    3,
                    23,
                    0
                ],
                "published": "2025-01-23T08:33:10Z",
                "published_parsed": [
                    2025,
                    1,
                    23,
                    8,
                    33,
                    10,
                    3,
                    23,
                    0
                ],
                "title": "Streaming Video Understanding and Multi-round Interaction with\n  Memory-enhanced Knowledge",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Streaming Video Understanding and Multi-round Interaction with\n  Memory-enhanced Knowledge"
                },
                "summary": "Recent advances in Large Language Models (LLMs) have enabled the development\nof Video-LLMs, advancing multimodal learning by bridging video data with\nlanguage tasks. However, current video understanding models struggle with\nprocessing long video sequences, supporting multi-turn dialogues, and adapting\nto real-world dynamic scenarios. To address these issues, we propose\nStreamChat, a training-free framework for streaming video reasoning and\nconversational interaction. $\\StreamChat$ leverages a novel hierarchical memory\nsystem to efficiently process and compress video features over extended\nsequences, enabling real-time, multi-turn dialogue. Our framework incorporates\na parallel system scheduling strategy that enhances processing speed and\nreduces latency, ensuring robust performance in real-world applications.\nFurthermore, we introduce StreamBench, a versatile benchmark that evaluates\nstreaming video understanding across diverse media types and interactive\nscenarios, including multi-turn interactions and complex reasoning tasks.\nExtensive evaluations on StreamBench and other public benchmarks demonstrate\nthat StreamChat significantly outperforms existing state-of-the-art models in\nterms of accuracy and response times, confirming its effectiveness for\nstreaming video understanding. Code is available at StreamChat:\nhttps://github.com/hmxiong/StreamChat.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advances in Large Language Models (LLMs) have enabled the development\nof Video-LLMs, advancing multimodal learning by bridging video data with\nlanguage tasks. However, current video understanding models struggle with\nprocessing long video sequences, supporting multi-turn dialogues, and adapting\nto real-world dynamic scenarios. To address these issues, we propose\nStreamChat, a training-free framework for streaming video reasoning and\nconversational interaction. $\\StreamChat$ leverages a novel hierarchical memory\nsystem to efficiently process and compress video features over extended\nsequences, enabling real-time, multi-turn dialogue. Our framework incorporates\na parallel system scheduling strategy that enhances processing speed and\nreduces latency, ensuring robust performance in real-world applications.\nFurthermore, we introduce StreamBench, a versatile benchmark that evaluates\nstreaming video understanding across diverse media types and interactive\nscenarios, including multi-turn interactions and complex reasoning tasks.\nExtensive evaluations on StreamBench and other public benchmarks demonstrate\nthat StreamChat significantly outperforms existing state-of-the-art models in\nterms of accuracy and response times, confirming its effectiveness for\nstreaming video understanding. Code is available at StreamChat:\nhttps://github.com/hmxiong/StreamChat."
                },
                "authors": [
                    {
                        "name": "Haomiao Xiong"
                    },
                    {
                        "name": "Zongxin Yang"
                    },
                    {
                        "name": "Jiazuo Yu"
                    },
                    {
                        "name": "Yunzhi Zhuge"
                    },
                    {
                        "name": "Lu Zhang"
                    },
                    {
                        "name": "Jiawen Zhu"
                    },
                    {
                        "name": "Huchuan Lu"
                    }
                ],
                "author_detail": {
                    "name": "Huchuan Lu"
                },
                "author": "Huchuan Lu",
                "arxiv_comment": "Accepted to ICLR 2025. Code is available at\n  https://github.com/hmxiong/StreamChat",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.13468v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.13468v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.13464v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.13464v1",
                "updated": "2025-01-23T08:26:45Z",
                "updated_parsed": [
                    2025,
                    1,
                    23,
                    8,
                    26,
                    45,
                    3,
                    23,
                    0
                ],
                "published": "2025-01-23T08:26:45Z",
                "published_parsed": [
                    2025,
                    1,
                    23,
                    8,
                    26,
                    45,
                    3,
                    23,
                    0
                ],
                "title": "Deep Multi-modal Neural Receiver for 6G Vehicular Communication",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Deep Multi-modal Neural Receiver for 6G Vehicular Communication"
                },
                "summary": "Deep Learning (DL) based neural receiver models are used to jointly optimize\nPHY of baseline receiver for cellular vehicle to everything (C-V2X) system in\nnext generation (6G) communication, however, there has been no exploration of\nhow varying training parameters affect the model's efficiency. Additionally, a\ncomprehensive evaluation of its performance on multi-modal data remains largely\nunexplored. To address this, we propose a neural receiver designed to optimize\nBit Error Rate (BER) for vehicle to network (V2N) uplink scenario in 6G\nnetwork. We train multiple neural receivers by changing its trainable\nparameters and use the best fit model as proposition for large scale\ndeployment. Our proposed neural receiver gets signal in frequency domain at the\nbase station (BS) as input and generates optimal log likelihood ratio (LLR) at\nthe output. It estimates the channel based on the received signal, equalizes\nand demodulates the higher order modulated signal. Later, to evaluate\nmulti-modality of the proposed model, we test it across diverse V2X data flows\n(e.g., image, video, gps, lidar cloud points and radar detection signal).\nResults from simulation clearly indicates that our proposed multi-modal neural\nreceiver outperforms state-of-the-art receiver architectures by achieving high\nperformance at low Signal to Noise Ratio (SNR).",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Deep Learning (DL) based neural receiver models are used to jointly optimize\nPHY of baseline receiver for cellular vehicle to everything (C-V2X) system in\nnext generation (6G) communication, however, there has been no exploration of\nhow varying training parameters affect the model's efficiency. Additionally, a\ncomprehensive evaluation of its performance on multi-modal data remains largely\nunexplored. To address this, we propose a neural receiver designed to optimize\nBit Error Rate (BER) for vehicle to network (V2N) uplink scenario in 6G\nnetwork. We train multiple neural receivers by changing its trainable\nparameters and use the best fit model as proposition for large scale\ndeployment. Our proposed neural receiver gets signal in frequency domain at the\nbase station (BS) as input and generates optimal log likelihood ratio (LLR) at\nthe output. It estimates the channel based on the received signal, equalizes\nand demodulates the higher order modulated signal. Later, to evaluate\nmulti-modality of the proposed model, we test it across diverse V2X data flows\n(e.g., image, video, gps, lidar cloud points and radar detection signal).\nResults from simulation clearly indicates that our proposed multi-modal neural\nreceiver outperforms state-of-the-art receiver architectures by achieving high\nperformance at low Signal to Noise Ratio (SNR)."
                },
                "authors": [
                    {
                        "name": "Osama Saleem"
                    },
                    {
                        "name": "Mohammed Alfaqawi"
                    },
                    {
                        "name": "Pierre Merdrignac"
                    },
                    {
                        "name": "Abdelaziz Bensrhair"
                    },
                    {
                        "name": "Soheyb Ribouh"
                    }
                ],
                "author_detail": {
                    "name": "Soheyb Ribouh"
                },
                "author": "Soheyb Ribouh",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.13464v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.13464v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SP",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.13461v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.13461v1",
                "updated": "2025-01-23T08:23:45Z",
                "updated_parsed": [
                    2025,
                    1,
                    23,
                    8,
                    23,
                    45,
                    3,
                    23,
                    0
                ],
                "published": "2025-01-23T08:23:45Z",
                "published_parsed": [
                    2025,
                    1,
                    23,
                    8,
                    23,
                    45,
                    3,
                    23,
                    0
                ],
                "title": "Knowledge-Informed Multi-Agent Trajectory Prediction at Signalized\n  Intersections for Infrastructure-to-Everything",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Knowledge-Informed Multi-Agent Trajectory Prediction at Signalized\n  Intersections for Infrastructure-to-Everything"
                },
                "summary": "Multi-agent trajectory prediction at signalized intersections is crucial for\ndeveloping efficient intelligent transportation systems and safe autonomous\ndriving systems. Due to the complexity of intersection scenarios and the\nlimitations of single-vehicle perception, the performance of vehicle-centric\nprediction methods has reached a plateau. Furthermore, most works underutilize\ncritical intersection information, including traffic signals, and behavior\npatterns induced by road structures. Therefore, we propose a multi-agent\ntrajectory prediction framework at signalized intersections dedicated to\nInfrastructure-to-Everything (I2XTraj). Our framework leverages dynamic graph\nattention to integrate knowledge from traffic signals and driving behaviors. A\ncontinuous signal-informed mechanism is proposed to adaptively process\nreal-time traffic signals from infrastructure devices. Additionally, leveraging\nthe prior knowledge of the intersection topology, we propose a driving strategy\nawareness mechanism to model the joint distribution of goal intentions and\nmaneuvers. To the best of our knowledge, I2XTraj represents the first\nmulti-agent trajectory prediction framework explicitly designed for\ninfrastructure deployment, supplying subscribable prediction services to all\nvehicles at intersections. I2XTraj demonstrates state-of-the-art performance on\nboth the Vehicle-to-Infrastructure dataset V2X-Seq and the aerial-view dataset\nSinD for signalized intersections. Quantitative evaluations show that our\napproach outperforms existing methods by more than 30% in both multi-agent and\nsingle-agent scenarios.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-agent trajectory prediction at signalized intersections is crucial for\ndeveloping efficient intelligent transportation systems and safe autonomous\ndriving systems. Due to the complexity of intersection scenarios and the\nlimitations of single-vehicle perception, the performance of vehicle-centric\nprediction methods has reached a plateau. Furthermore, most works underutilize\ncritical intersection information, including traffic signals, and behavior\npatterns induced by road structures. Therefore, we propose a multi-agent\ntrajectory prediction framework at signalized intersections dedicated to\nInfrastructure-to-Everything (I2XTraj). Our framework leverages dynamic graph\nattention to integrate knowledge from traffic signals and driving behaviors. A\ncontinuous signal-informed mechanism is proposed to adaptively process\nreal-time traffic signals from infrastructure devices. Additionally, leveraging\nthe prior knowledge of the intersection topology, we propose a driving strategy\nawareness mechanism to model the joint distribution of goal intentions and\nmaneuvers. To the best of our knowledge, I2XTraj represents the first\nmulti-agent trajectory prediction framework explicitly designed for\ninfrastructure deployment, supplying subscribable prediction services to all\nvehicles at intersections. I2XTraj demonstrates state-of-the-art performance on\nboth the Vehicle-to-Infrastructure dataset V2X-Seq and the aerial-view dataset\nSinD for signalized intersections. Quantitative evaluations show that our\napproach outperforms existing methods by more than 30% in both multi-agent and\nsingle-agent scenarios."
                },
                "authors": [
                    {
                        "name": "Huilin Yin"
                    },
                    {
                        "name": "Yangwenhui Xu"
                    },
                    {
                        "name": "Jiaxiang Li"
                    },
                    {
                        "name": "Hao Zhang"
                    },
                    {
                        "name": "Gerhard Rigoll"
                    }
                ],
                "author_detail": {
                    "name": "Gerhard Rigoll"
                },
                "author": "Gerhard Rigoll",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.13461v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.13461v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.13453v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.13453v1",
                "updated": "2025-01-23T08:09:54Z",
                "updated_parsed": [
                    2025,
                    1,
                    23,
                    8,
                    9,
                    54,
                    3,
                    23,
                    0
                ],
                "published": "2025-01-23T08:09:54Z",
                "published_parsed": [
                    2025,
                    1,
                    23,
                    8,
                    9,
                    54,
                    3,
                    23,
                    0
                ],
                "title": "Spurious Forgetting in Continual Learning of Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Spurious Forgetting in Continual Learning of Language Models"
                },
                "summary": "Recent advancements in large language models (LLMs) reveal a perplexing\nphenomenon in continual learning: despite extensive training, models experience\nsignificant performance declines, raising questions about task alignment and\nunderlying knowledge retention. This study first explores the concept of\n\"spurious forgetting\", proposing that such performance drops often reflect a\ndecline in task alignment rather than true knowledge loss. Through controlled\nexperiments with a synthesized dataset, we investigate the dynamics of model\nperformance during the initial training phases of new tasks, discovering that\nearly optimization steps can disrupt previously established task alignments.\nOur theoretical analysis connects these shifts to orthogonal updates in model\nweights, providing a robust framework for understanding this behavior.\nUltimately, we introduce a Freezing strategy that fix the bottom layers of the\nmodel, leading to substantial improvements in four continual learning\nscenarios. Our findings underscore the critical distinction between task\nalignment and knowledge retention, paving the way for more effective strategies\nin continual learning.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advancements in large language models (LLMs) reveal a perplexing\nphenomenon in continual learning: despite extensive training, models experience\nsignificant performance declines, raising questions about task alignment and\nunderlying knowledge retention. This study first explores the concept of\n\"spurious forgetting\", proposing that such performance drops often reflect a\ndecline in task alignment rather than true knowledge loss. Through controlled\nexperiments with a synthesized dataset, we investigate the dynamics of model\nperformance during the initial training phases of new tasks, discovering that\nearly optimization steps can disrupt previously established task alignments.\nOur theoretical analysis connects these shifts to orthogonal updates in model\nweights, providing a robust framework for understanding this behavior.\nUltimately, we introduce a Freezing strategy that fix the bottom layers of the\nmodel, leading to substantial improvements in four continual learning\nscenarios. Our findings underscore the critical distinction between task\nalignment and knowledge retention, paving the way for more effective strategies\nin continual learning."
                },
                "authors": [
                    {
                        "name": "Junhao Zheng"
                    },
                    {
                        "name": "Xidi Cai"
                    },
                    {
                        "name": "Shengjie Qiu"
                    },
                    {
                        "name": "Qianli Ma"
                    }
                ],
                "author_detail": {
                    "name": "Qianli Ma"
                },
                "author": "Qianli Ma",
                "arxiv_comment": "ICLR2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.13453v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.13453v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.11970v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.11970v2",
                "updated": "2025-01-23T08:07:41Z",
                "updated_parsed": [
                    2025,
                    1,
                    23,
                    8,
                    7,
                    41,
                    3,
                    23,
                    0
                ],
                "published": "2024-12-16T16:51:27Z",
                "published_parsed": [
                    2024,
                    12,
                    16,
                    16,
                    51,
                    27,
                    0,
                    351,
                    0
                ],
                "title": "DARWIN 1.5: Large Language Models as Materials Science Adapted Learners",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DARWIN 1.5: Large Language Models as Materials Science Adapted Learners"
                },
                "summary": "Materials discovery and design aim to find compositions and structures with\ndesirable properties over highly complex and diverse physical spaces.\nTraditional solutions, such as high-throughput simulations or machine learning,\noften rely on complex descriptors, which hinder generalizability and\ntransferability across different material systems. Moreover, These descriptors\nmay inadequately represent macro-scale material properties, which are\ninfluenced by structural imperfections and compositional variations in\nreal-world samples, thus limiting their practical applicability. To address\nthese challenges, we propose DARWIN 1.5, the largest open-source large language\nmodel tailored for materials science. By leveraging natural language as input,\nDARWIN eliminates the need for task-specific descriptors and enables a\nflexible, unified approach to material property prediction and discovery. Our\napproach integrates 6M material domain papers and 21 experimental datasets from\n49,256 materials across modalities while enabling cross-task knowledge\ntransfer. The enhanced model achieves up to 59.1% improvement in prediction\naccuracy over the base LLaMA-7B architecture and outperforms SOTA machine\nlearning approaches across 8 materials design tasks. These results establish\nLLMs as a promising foundation for developing versatile and scalable models in\nmaterials science.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Materials discovery and design aim to find compositions and structures with\ndesirable properties over highly complex and diverse physical spaces.\nTraditional solutions, such as high-throughput simulations or machine learning,\noften rely on complex descriptors, which hinder generalizability and\ntransferability across different material systems. Moreover, These descriptors\nmay inadequately represent macro-scale material properties, which are\ninfluenced by structural imperfections and compositional variations in\nreal-world samples, thus limiting their practical applicability. To address\nthese challenges, we propose DARWIN 1.5, the largest open-source large language\nmodel tailored for materials science. By leveraging natural language as input,\nDARWIN eliminates the need for task-specific descriptors and enables a\nflexible, unified approach to material property prediction and discovery. Our\napproach integrates 6M material domain papers and 21 experimental datasets from\n49,256 materials across modalities while enabling cross-task knowledge\ntransfer. The enhanced model achieves up to 59.1% improvement in prediction\naccuracy over the base LLaMA-7B architecture and outperforms SOTA machine\nlearning approaches across 8 materials design tasks. These results establish\nLLMs as a promising foundation for developing versatile and scalable models in\nmaterials science."
                },
                "authors": [
                    {
                        "name": "Tong Xie"
                    },
                    {
                        "name": "Yuwei Wan"
                    },
                    {
                        "name": "Yixuan Liu"
                    },
                    {
                        "name": "Yuchen Zeng"
                    },
                    {
                        "name": "Shaozhou Wang"
                    },
                    {
                        "name": "Wenjie Zhang"
                    },
                    {
                        "name": "Clara Grazian"
                    },
                    {
                        "name": "Chunyu Kit"
                    },
                    {
                        "name": "Wanli Ouyang"
                    },
                    {
                        "name": "Dongzhan Zhou"
                    },
                    {
                        "name": "Bram Hoex"
                    }
                ],
                "author_detail": {
                    "name": "Bram Hoex"
                },
                "author": "Bram Hoex",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.11970v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.11970v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.13449v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.13449v1",
                "updated": "2025-01-23T08:02:59Z",
                "updated_parsed": [
                    2025,
                    1,
                    23,
                    8,
                    2,
                    59,
                    3,
                    23,
                    0
                ],
                "published": "2025-01-23T08:02:59Z",
                "published_parsed": [
                    2025,
                    1,
                    23,
                    8,
                    2,
                    59,
                    3,
                    23,
                    0
                ],
                "title": "MultiDreamer3D: Multi-concept 3D Customization with Concept-Aware\n  Diffusion Guidance",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MultiDreamer3D: Multi-concept 3D Customization with Concept-Aware\n  Diffusion Guidance"
                },
                "summary": "While single-concept customization has been studied in 3D, multi-concept\ncustomization remains largely unexplored. To address this, we propose\nMultiDreamer3D that can generate coherent multi-concept 3D content in a\ndivide-and-conquer manner. First, we generate 3D bounding boxes using an\nLLM-based layout controller. Next, a selective point cloud generator creates\ncoarse point clouds for each concept. These point clouds are placed in the 3D\nbounding boxes and initialized into 3D Gaussian Splatting with concept labels,\nenabling precise identification of concept attributions in 2D projections.\nFinally, we refine 3D Gaussians via concept-aware interval score matching,\nguided by concept-aware diffusion. Our experimental results show that\nMultiDreamer3D not only ensures object presence and preserves the distinct\nidentities of each concept but also successfully handles complex cases such as\nproperty change or interaction. To the best of our knowledge, we are the first\nto address the multi-concept customization in 3D.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While single-concept customization has been studied in 3D, multi-concept\ncustomization remains largely unexplored. To address this, we propose\nMultiDreamer3D that can generate coherent multi-concept 3D content in a\ndivide-and-conquer manner. First, we generate 3D bounding boxes using an\nLLM-based layout controller. Next, a selective point cloud generator creates\ncoarse point clouds for each concept. These point clouds are placed in the 3D\nbounding boxes and initialized into 3D Gaussian Splatting with concept labels,\nenabling precise identification of concept attributions in 2D projections.\nFinally, we refine 3D Gaussians via concept-aware interval score matching,\nguided by concept-aware diffusion. Our experimental results show that\nMultiDreamer3D not only ensures object presence and preserves the distinct\nidentities of each concept but also successfully handles complex cases such as\nproperty change or interaction. To the best of our knowledge, we are the first\nto address the multi-concept customization in 3D."
                },
                "authors": [
                    {
                        "name": "Wooseok Song"
                    },
                    {
                        "name": "Seunggyu Chang"
                    },
                    {
                        "name": "Jaejun Yoo"
                    }
                ],
                "author_detail": {
                    "name": "Jaejun Yoo"
                },
                "author": "Jaejun Yoo",
                "arxiv_comment": "9 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.13449v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.13449v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.11885v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.11885v3",
                "updated": "2025-01-23T07:45:20Z",
                "updated_parsed": [
                    2025,
                    1,
                    23,
                    7,
                    45,
                    20,
                    3,
                    23,
                    0
                ],
                "published": "2025-01-21T04:40:43Z",
                "published_parsed": [
                    2025,
                    1,
                    21,
                    4,
                    40,
                    43,
                    1,
                    21,
                    0
                ],
                "title": "Med-R$^2$: Crafting Trustworthy LLM Physicians through Retrieval and\n  Reasoning of Evidence-Based Medicine",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Med-R$^2$: Crafting Trustworthy LLM Physicians through Retrieval and\n  Reasoning of Evidence-Based Medicine"
                },
                "summary": "In recent years, Large Language Models (LLMs) have exhibited remarkable\ncapabilities in clinical scenarios. However, despite their potential, existing\nworks face challenges when applying LLMs to medical settings. Strategies\nrelying on training with medical datasets are highly cost-intensive and may\nsuffer from outdated training data. Leveraging external knowledge bases is a\nsuitable alternative, yet it faces obstacles such as limited retrieval\nprecision and poor effectiveness in answer extraction. These issues\ncollectively prevent LLMs from demonstrating the expected level of proficiency\nin mastering medical expertise. To address these challenges, we introduce\nMed-R^2, a novel LLM physician framework that adheres to the Evidence-Based\nMedicine (EBM) process, efficiently integrating retrieval mechanisms as well as\nthe selection and reasoning processes of evidence, thereby enhancing the\nproblem-solving capabilities of LLMs in healthcare scenarios and fostering a\ntrustworthy LLM physician. Our comprehensive experiments indicate that Med-R^2\nachieves a 14.87\\% improvement over vanilla RAG methods and even a 3.59\\%\nenhancement compared to fine-tuning strategies, without incurring additional\ntraining costs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In recent years, Large Language Models (LLMs) have exhibited remarkable\ncapabilities in clinical scenarios. However, despite their potential, existing\nworks face challenges when applying LLMs to medical settings. Strategies\nrelying on training with medical datasets are highly cost-intensive and may\nsuffer from outdated training data. Leveraging external knowledge bases is a\nsuitable alternative, yet it faces obstacles such as limited retrieval\nprecision and poor effectiveness in answer extraction. These issues\ncollectively prevent LLMs from demonstrating the expected level of proficiency\nin mastering medical expertise. To address these challenges, we introduce\nMed-R^2, a novel LLM physician framework that adheres to the Evidence-Based\nMedicine (EBM) process, efficiently integrating retrieval mechanisms as well as\nthe selection and reasoning processes of evidence, thereby enhancing the\nproblem-solving capabilities of LLMs in healthcare scenarios and fostering a\ntrustworthy LLM physician. Our comprehensive experiments indicate that Med-R^2\nachieves a 14.87\\% improvement over vanilla RAG methods and even a 3.59\\%\nenhancement compared to fine-tuning strategies, without incurring additional\ntraining costs."
                },
                "authors": [
                    {
                        "name": "Keer Lu"
                    },
                    {
                        "name": "Zheng Liang"
                    },
                    {
                        "name": "Da Pan"
                    },
                    {
                        "name": "Shusen Zhang"
                    },
                    {
                        "name": "Xin Wu"
                    },
                    {
                        "name": "Weipeng Chen"
                    },
                    {
                        "name": "Zenan Zhou"
                    },
                    {
                        "name": "Guosheng Dong"
                    },
                    {
                        "name": "Bin Cui"
                    },
                    {
                        "name": "Wentao Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Wentao Zhang"
                },
                "author": "Wentao Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.11885v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.11885v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.09827v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.09827v3",
                "updated": "2025-01-23T07:25:28Z",
                "updated_parsed": [
                    2025,
                    1,
                    23,
                    7,
                    25,
                    28,
                    3,
                    23,
                    0
                ],
                "published": "2024-06-14T08:32:45Z",
                "published_parsed": [
                    2024,
                    6,
                    14,
                    8,
                    32,
                    45,
                    4,
                    166,
                    0
                ],
                "title": "A Training-free Sub-quadratic Cost Transformer Model Serving Framework\n  With Hierarchically Pruned Attention",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Training-free Sub-quadratic Cost Transformer Model Serving Framework\n  With Hierarchically Pruned Attention"
                },
                "summary": "In modern large language models (LLMs), increasing the context length is\ncrucial for improving comprehension and coherence in long-context, multi-modal,\nand retrieval-augmented language generation. While many recent transformer\nmodels attempt to extend their context length over a million tokens, they\nremain impractical due to the quadratic time and space complexities. Although\nrecent works on linear and sparse attention mechanisms can achieve this goal,\ntheir real-world applicability is often limited by the need to re-train from\nscratch and significantly worse performance. In response, we propose a novel\napproach, Hierarchically Pruned Attention (HiP), which reduces the time\ncomplexity of the attention mechanism to $O(T \\log T)$ and the space complexity\nto $O(T)$, where $T$ is the sequence length. We notice a pattern in the\nattention scores of pretrained LLMs where tokens close together tend to have\nsimilar scores, which we call ``attention locality''. Based on this\nobservation, we utilize a novel tree-search-like algorithm that estimates the\ntop-$k$ key tokens for a given query on the fly, which is mathematically\nguaranteed to have better performance than random attention pruning. In\naddition to improving the time complexity of the attention mechanism, we\nfurther optimize GPU memory usage by implementing KV cache offloading, which\nstores only $O(\\log T)$ tokens on the GPU while maintaining similar decoding\nthroughput. Experiments on benchmarks show that HiP, with its training-free\nnature, significantly reduces both prefill and decoding latencies, as well as\nmemory usage, while maintaining high-quality generation with minimal\ndegradation. HiP enables pretrained LLMs to scale up to millions of tokens on\ncommodity GPUs, potentially unlocking long-context LLM applications previously\ndeemed infeasible.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In modern large language models (LLMs), increasing the context length is\ncrucial for improving comprehension and coherence in long-context, multi-modal,\nand retrieval-augmented language generation. While many recent transformer\nmodels attempt to extend their context length over a million tokens, they\nremain impractical due to the quadratic time and space complexities. Although\nrecent works on linear and sparse attention mechanisms can achieve this goal,\ntheir real-world applicability is often limited by the need to re-train from\nscratch and significantly worse performance. In response, we propose a novel\napproach, Hierarchically Pruned Attention (HiP), which reduces the time\ncomplexity of the attention mechanism to $O(T \\log T)$ and the space complexity\nto $O(T)$, where $T$ is the sequence length. We notice a pattern in the\nattention scores of pretrained LLMs where tokens close together tend to have\nsimilar scores, which we call ``attention locality''. Based on this\nobservation, we utilize a novel tree-search-like algorithm that estimates the\ntop-$k$ key tokens for a given query on the fly, which is mathematically\nguaranteed to have better performance than random attention pruning. In\naddition to improving the time complexity of the attention mechanism, we\nfurther optimize GPU memory usage by implementing KV cache offloading, which\nstores only $O(\\log T)$ tokens on the GPU while maintaining similar decoding\nthroughput. Experiments on benchmarks show that HiP, with its training-free\nnature, significantly reduces both prefill and decoding latencies, as well as\nmemory usage, while maintaining high-quality generation with minimal\ndegradation. HiP enables pretrained LLMs to scale up to millions of tokens on\ncommodity GPUs, potentially unlocking long-context LLM applications previously\ndeemed infeasible."
                },
                "authors": [
                    {
                        "name": "Heejun Lee"
                    },
                    {
                        "name": "Geon Park"
                    },
                    {
                        "name": "Youngwan Lee"
                    },
                    {
                        "name": "Jaduk Suh"
                    },
                    {
                        "name": "Jina Kim"
                    },
                    {
                        "name": "Wonyoung Jeong"
                    },
                    {
                        "name": "Bumsik Kim"
                    },
                    {
                        "name": "Hyemin Lee"
                    },
                    {
                        "name": "Myeongjae Jeon"
                    },
                    {
                        "name": "Sung Ju Hwang"
                    }
                ],
                "author_detail": {
                    "name": "Sung Ju Hwang"
                },
                "author": "Sung Ju Hwang",
                "arxiv_comment": "44 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.09827v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.09827v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2402.11541v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2402.11541v4",
                "updated": "2025-01-23T07:21:35Z",
                "updated_parsed": [
                    2025,
                    1,
                    23,
                    7,
                    21,
                    35,
                    3,
                    23,
                    0
                ],
                "published": "2024-02-18T10:44:03Z",
                "published_parsed": [
                    2024,
                    2,
                    18,
                    10,
                    44,
                    3,
                    6,
                    49,
                    0
                ],
                "title": "Large Language Models Can Better Understand Knowledge Graphs Than We\n  Thought",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models Can Better Understand Knowledge Graphs Than We\n  Thought"
                },
                "summary": "When we integrate factual knowledge from knowledge graphs (KGs) into large\nlanguage models (LLMs) to enhance their performance, the cost of injection\nthrough training increases with the scale of the models. Consequently, there is\nsignificant interest in developing prompt strategies that effectively\nincorporate KG information into LLMs. However, the community has not yet\ncomprehensively understood how LLMs process and interpret KG information in\ndifferent input formats and organizations within prompts, and researchers often\nrely on trial and error. To address this gap, we design extensive experiments\nto empirically study LLMs' comprehension of different KG prompts. At the\nliteral level, we reveal LLMs' preferences for various input formats (from\nlinearized triples to fluent natural language text). At the attention\ndistribution level, we discuss the underlying mechanisms driving these\npreferences. We then investigate how the organization of structured knowledge\nimpacts LLMs and evaluate LLMs' robustness in processing and utilizing KG\ninformation in practical scenarios. Our experiments show that (1) linearized\ntriples are more effective than fluent NL text in helping LLMs understand KG\ninformation and answer fact-intensive questions; (2) Different LLMs exhibit\nvarying preferences for different organizational formats of triples; (3) LLMs\nwith larger scales are more susceptible to noisy, incomplete subgraphs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "When we integrate factual knowledge from knowledge graphs (KGs) into large\nlanguage models (LLMs) to enhance their performance, the cost of injection\nthrough training increases with the scale of the models. Consequently, there is\nsignificant interest in developing prompt strategies that effectively\nincorporate KG information into LLMs. However, the community has not yet\ncomprehensively understood how LLMs process and interpret KG information in\ndifferent input formats and organizations within prompts, and researchers often\nrely on trial and error. To address this gap, we design extensive experiments\nto empirically study LLMs' comprehension of different KG prompts. At the\nliteral level, we reveal LLMs' preferences for various input formats (from\nlinearized triples to fluent natural language text). At the attention\ndistribution level, we discuss the underlying mechanisms driving these\npreferences. We then investigate how the organization of structured knowledge\nimpacts LLMs and evaluate LLMs' robustness in processing and utilizing KG\ninformation in practical scenarios. Our experiments show that (1) linearized\ntriples are more effective than fluent NL text in helping LLMs understand KG\ninformation and answer fact-intensive questions; (2) Different LLMs exhibit\nvarying preferences for different organizational formats of triples; (3) LLMs\nwith larger scales are more susceptible to noisy, incomplete subgraphs."
                },
                "authors": [
                    {
                        "name": "Xinbang Dai"
                    },
                    {
                        "name": "Yuncheng Hua"
                    },
                    {
                        "name": "Tongtong Wu"
                    },
                    {
                        "name": "Yang Sheng"
                    },
                    {
                        "name": "Qiu Ji"
                    },
                    {
                        "name": "Guilin Qi"
                    }
                ],
                "author_detail": {
                    "name": "Guilin Qi"
                },
                "author": "Guilin Qi",
                "arxiv_comment": "24 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2402.11541v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2402.11541v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.4; I.2.7",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.14146v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.14146v3",
                "updated": "2025-01-23T07:06:15Z",
                "updated_parsed": [
                    2025,
                    1,
                    23,
                    7,
                    6,
                    15,
                    3,
                    23,
                    0
                ],
                "published": "2024-12-18T18:44:08Z",
                "published_parsed": [
                    2024,
                    12,
                    18,
                    18,
                    44,
                    8,
                    2,
                    353,
                    0
                ],
                "title": "ARTEMIS-DA: An Advanced Reasoning and Transformation Engine for\n  Multi-Step Insight Synthesis in Data Analytics",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ARTEMIS-DA: An Advanced Reasoning and Transformation Engine for\n  Multi-Step Insight Synthesis in Data Analytics"
                },
                "summary": "This paper presents the Advanced Reasoning and Transformation Engine for\nMulti-Step Insight Synthesis in Data Analytics (ARTEMIS-DA), a novel framework\ndesigned to augment Large Language Models (LLMs) for solving complex,\nmulti-step data analytics tasks. ARTEMIS-DA integrates three core components:\nthe Planner, which dissects complex user queries into structured, sequential\ninstructions encompassing data preprocessing, transformation, predictive\nmodeling, and visualization; the Coder, which dynamically generates and\nexecutes Python code to implement these instructions; and the Grapher, which\ninterprets generated visualizations to derive actionable insights. By\norchestrating the collaboration between these components, ARTEMIS-DA\neffectively manages sophisticated analytical workflows involving advanced\nreasoning, multi-step transformations, and synthesis across diverse data\nmodalities. The framework achieves state-of-the-art (SOTA) performance on\nbenchmarks such as WikiTableQuestions and TabFact, demonstrating its ability to\ntackle intricate analytical tasks with precision and adaptability. By combining\nthe reasoning capabilities of LLMs with automated code generation and execution\nand visual analysis, ARTEMIS-DA offers a robust, scalable solution for\nmulti-step insight synthesis, addressing a wide range of challenges in data\nanalytics.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper presents the Advanced Reasoning and Transformation Engine for\nMulti-Step Insight Synthesis in Data Analytics (ARTEMIS-DA), a novel framework\ndesigned to augment Large Language Models (LLMs) for solving complex,\nmulti-step data analytics tasks. ARTEMIS-DA integrates three core components:\nthe Planner, which dissects complex user queries into structured, sequential\ninstructions encompassing data preprocessing, transformation, predictive\nmodeling, and visualization; the Coder, which dynamically generates and\nexecutes Python code to implement these instructions; and the Grapher, which\ninterprets generated visualizations to derive actionable insights. By\norchestrating the collaboration between these components, ARTEMIS-DA\neffectively manages sophisticated analytical workflows involving advanced\nreasoning, multi-step transformations, and synthesis across diverse data\nmodalities. The framework achieves state-of-the-art (SOTA) performance on\nbenchmarks such as WikiTableQuestions and TabFact, demonstrating its ability to\ntackle intricate analytical tasks with precision and adaptability. By combining\nthe reasoning capabilities of LLMs with automated code generation and execution\nand visual analysis, ARTEMIS-DA offers a robust, scalable solution for\nmulti-step insight synthesis, addressing a wide range of challenges in data\nanalytics."
                },
                "authors": [
                    {
                        "name": "Atin Sakkeer Hussain"
                    }
                ],
                "author_detail": {
                    "name": "Atin Sakkeer Hussain"
                },
                "author": "Atin Sakkeer Hussain",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.14146v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.14146v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.16454v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.16454v2",
                "updated": "2025-01-23T06:50:48Z",
                "updated_parsed": [
                    2025,
                    1,
                    23,
                    6,
                    50,
                    48,
                    3,
                    23,
                    0
                ],
                "published": "2024-10-21T19:28:37Z",
                "published_parsed": [
                    2024,
                    10,
                    21,
                    19,
                    28,
                    37,
                    0,
                    295,
                    0
                ],
                "title": "Catastrophic Failure of LLM Unlearning via Quantization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Catastrophic Failure of LLM Unlearning via Quantization"
                },
                "summary": "Large language models (LLMs) have shown remarkable proficiency in generating\ntext, benefiting from extensive training on vast textual corpora. However, LLMs\nmay also acquire unwanted behaviors from the diverse and sensitive nature of\ntheir training data, which can include copyrighted and private content. Machine\nunlearning has been introduced as a viable solution to remove the influence of\nsuch problematic content without the need for costly and time-consuming\nretraining. This process aims to erase specific knowledge from LLMs while\npreserving as much model utility as possible. Despite the effectiveness of\ncurrent unlearning methods, little attention has been given to whether existing\nunlearning methods for LLMs truly achieve forgetting or merely hide the\nknowledge, which current unlearning benchmarks fail to detect. This paper\nreveals that applying quantization to models that have undergone unlearning can\nrestore the \"forgotten\" information. To thoroughly evaluate this phenomenon, we\nconduct comprehensive experiments using various quantization techniques across\nmultiple precision levels. We find that for unlearning methods with utility\nconstraints, the unlearned model retains an average of 21\\% of the intended\nforgotten knowledge in full precision, which significantly increases to 83\\%\nafter 4-bit quantization. ... Our code is available at:\n\\href{https://github.com/zzwjames/FailureLLMUnlearning}{https://github.com/zzwjames/FailureLLMUnlearning}.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have shown remarkable proficiency in generating\ntext, benefiting from extensive training on vast textual corpora. However, LLMs\nmay also acquire unwanted behaviors from the diverse and sensitive nature of\ntheir training data, which can include copyrighted and private content. Machine\nunlearning has been introduced as a viable solution to remove the influence of\nsuch problematic content without the need for costly and time-consuming\nretraining. This process aims to erase specific knowledge from LLMs while\npreserving as much model utility as possible. Despite the effectiveness of\ncurrent unlearning methods, little attention has been given to whether existing\nunlearning methods for LLMs truly achieve forgetting or merely hide the\nknowledge, which current unlearning benchmarks fail to detect. This paper\nreveals that applying quantization to models that have undergone unlearning can\nrestore the \"forgotten\" information. To thoroughly evaluate this phenomenon, we\nconduct comprehensive experiments using various quantization techniques across\nmultiple precision levels. We find that for unlearning methods with utility\nconstraints, the unlearned model retains an average of 21\\% of the intended\nforgotten knowledge in full precision, which significantly increases to 83\\%\nafter 4-bit quantization. ... Our code is available at:\n\\href{https://github.com/zzwjames/FailureLLMUnlearning}{https://github.com/zzwjames/FailureLLMUnlearning}."
                },
                "authors": [
                    {
                        "name": "Zhiwei Zhang"
                    },
                    {
                        "name": "Fali Wang"
                    },
                    {
                        "name": "Xiaomin Li"
                    },
                    {
                        "name": "Zongyu Wu"
                    },
                    {
                        "name": "Xianfeng Tang"
                    },
                    {
                        "name": "Hui Liu"
                    },
                    {
                        "name": "Qi He"
                    },
                    {
                        "name": "Wenpeng Yin"
                    },
                    {
                        "name": "Suhang Wang"
                    }
                ],
                "author_detail": {
                    "name": "Suhang Wang"
                },
                "author": "Suhang Wang",
                "arxiv_comment": "25 pages, 2 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.16454v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.16454v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.07523v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.07523v2",
                "updated": "2025-01-23T06:48:22Z",
                "updated_parsed": [
                    2025,
                    1,
                    23,
                    6,
                    48,
                    22,
                    3,
                    23,
                    0
                ],
                "published": "2025-01-13T17:50:30Z",
                "published_parsed": [
                    2025,
                    1,
                    13,
                    17,
                    50,
                    30,
                    0,
                    13,
                    0
                ],
                "title": "Parallel Key-Value Cache Fusion for Position Invariant RAG",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Parallel Key-Value Cache Fusion for Position Invariant RAG"
                },
                "summary": "Recent advancements in Large Language Models (LLMs) underscore the necessity\nof Retrieval Augmented Generation (RAG) to leverage external information.\nHowever, LLMs are sensitive to the position of relevant information within\ncontexts and tend to generate incorrect responses when such information is\nplaced in the middle, known as `Lost in the Middle' phenomenon. In this paper,\nwe introduce a framework that generates consistent outputs for decoder-only\nmodels, irrespective of the input context order. Experimental results for three\nopen domain question answering tasks demonstrate position invariance, where the\nmodel is not sensitive to input context order, and superior robustness to\nirrelevent passages compared to prevailing approaches for RAG pipelines.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advancements in Large Language Models (LLMs) underscore the necessity\nof Retrieval Augmented Generation (RAG) to leverage external information.\nHowever, LLMs are sensitive to the position of relevant information within\ncontexts and tend to generate incorrect responses when such information is\nplaced in the middle, known as `Lost in the Middle' phenomenon. In this paper,\nwe introduce a framework that generates consistent outputs for decoder-only\nmodels, irrespective of the input context order. Experimental results for three\nopen domain question answering tasks demonstrate position invariance, where the\nmodel is not sensitive to input context order, and superior robustness to\nirrelevent passages compared to prevailing approaches for RAG pipelines."
                },
                "authors": [
                    {
                        "name": "Philhoon Oh"
                    },
                    {
                        "name": "Jinwoo Shin"
                    },
                    {
                        "name": "James Thorne"
                    }
                ],
                "author_detail": {
                    "name": "James Thorne"
                },
                "author": "James Thorne",
                "arxiv_comment": "5 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.07523v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.07523v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.11478v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.11478v2",
                "updated": "2025-01-23T06:36:18Z",
                "updated_parsed": [
                    2025,
                    1,
                    23,
                    6,
                    36,
                    18,
                    3,
                    23,
                    0
                ],
                "published": "2025-01-20T13:20:41Z",
                "published_parsed": [
                    2025,
                    1,
                    20,
                    13,
                    20,
                    41,
                    0,
                    20,
                    0
                ],
                "title": "Each Graph is a New Language: Graph Learning with LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Each Graph is a New Language: Graph Learning with LLMs"
                },
                "summary": "Recent efforts leverage Large Language Models (LLMs) for modeling\ntext-attributed graph structures in node classification tasks. These approaches\ndescribe graph structures for LLMs to understand or aggregate LLM-generated\ntextual attribute embeddings through graph structure. However, these approaches\nface two main limitations in modeling graph structures with LLMs. (i) Graph\ndescriptions become verbose in describing high-order graph structure. (ii)\nTextual attributes alone do not contain adequate graph structure information.\nIt is challenging to model graph structure concisely and adequately with LLMs.\nLLMs lack built-in mechanisms to model graph structures directly. They also\nstruggle with complex long-range dependencies between high-order nodes and\ntarget nodes.\n  Inspired by the observation that LLMs pre-trained on one language can achieve\nexceptional performance on another with minimal additional training, we propose\n\\textbf{G}raph-\\textbf{D}efined \\textbf{L}anguage for \\textbf{L}arge\n\\textbf{L}anguage \\textbf{M}odel (GDL4LLM). This novel framework enables LLMs\nto transfer their powerful language understanding capabilities to\ngraph-structured data. GDL4LLM translates graphs into a graph language corpus\ninstead of graph descriptions and pre-trains LLMs on this corpus to adequately\nunderstand graph structures. During fine-tuning, this corpus describes the\nstructural information of target nodes concisely with only a few tokens. By\ntreating graphs as a new language, GDL4LLM enables LLMs to model graph\nstructures adequately and concisely for node classification tasks. Extensive\nexperiments on three real-world datasets demonstrate that GDL4LLM outperforms\ndescription-based and textual attribute embeddings-based baselines by\nefficiently modeling different orders of graph structure with LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent efforts leverage Large Language Models (LLMs) for modeling\ntext-attributed graph structures in node classification tasks. These approaches\ndescribe graph structures for LLMs to understand or aggregate LLM-generated\ntextual attribute embeddings through graph structure. However, these approaches\nface two main limitations in modeling graph structures with LLMs. (i) Graph\ndescriptions become verbose in describing high-order graph structure. (ii)\nTextual attributes alone do not contain adequate graph structure information.\nIt is challenging to model graph structure concisely and adequately with LLMs.\nLLMs lack built-in mechanisms to model graph structures directly. They also\nstruggle with complex long-range dependencies between high-order nodes and\ntarget nodes.\n  Inspired by the observation that LLMs pre-trained on one language can achieve\nexceptional performance on another with minimal additional training, we propose\n\\textbf{G}raph-\\textbf{D}efined \\textbf{L}anguage for \\textbf{L}arge\n\\textbf{L}anguage \\textbf{M}odel (GDL4LLM). This novel framework enables LLMs\nto transfer their powerful language understanding capabilities to\ngraph-structured data. GDL4LLM translates graphs into a graph language corpus\ninstead of graph descriptions and pre-trains LLMs on this corpus to adequately\nunderstand graph structures. During fine-tuning, this corpus describes the\nstructural information of target nodes concisely with only a few tokens. By\ntreating graphs as a new language, GDL4LLM enables LLMs to model graph\nstructures adequately and concisely for node classification tasks. Extensive\nexperiments on three real-world datasets demonstrate that GDL4LLM outperforms\ndescription-based and textual attribute embeddings-based baselines by\nefficiently modeling different orders of graph structure with LLMs."
                },
                "authors": [
                    {
                        "name": "Huachi Zhou"
                    },
                    {
                        "name": "Jiahe Du"
                    },
                    {
                        "name": "Chuang Zhou"
                    },
                    {
                        "name": "Chang Yang"
                    },
                    {
                        "name": "Yilin Xiao"
                    },
                    {
                        "name": "Yuxuan Xie"
                    },
                    {
                        "name": "Xiao Huang"
                    }
                ],
                "author_detail": {
                    "name": "Xiao Huang"
                },
                "author": "Xiao Huang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.11478v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.11478v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.13411v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.13411v1",
                "updated": "2025-01-23T06:33:05Z",
                "updated_parsed": [
                    2025,
                    1,
                    23,
                    6,
                    33,
                    5,
                    3,
                    23,
                    0
                ],
                "published": "2025-01-23T06:33:05Z",
                "published_parsed": [
                    2025,
                    1,
                    23,
                    6,
                    33,
                    5,
                    3,
                    23,
                    0
                ],
                "title": "VulnBot: Autonomous Penetration Testing for A Multi-Agent Collaborative\n  Framework",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "VulnBot: Autonomous Penetration Testing for A Multi-Agent Collaborative\n  Framework"
                },
                "summary": "Penetration testing is a vital practice for identifying and mitigating\nvulnerabilities in cybersecurity systems, but its manual execution is\nlabor-intensive and time-consuming. Existing large language model\n(LLM)-assisted or automated penetration testing approaches often suffer from\ninefficiencies, such as a lack of contextual understanding and excessive,\nunstructured data generation. This paper presents VulnBot, an automated\npenetration testing framework that leverages LLMs to simulate the collaborative\nworkflow of human penetration testing teams through a multi-agent system. To\naddress the inefficiencies and reliance on manual intervention in traditional\npenetration testing methods, VulnBot decomposes complex tasks into three\nspecialized phases: reconnaissance, scanning, and exploitation. These phases\nare guided by a penetration task graph (PTG) to ensure logical task execution.\nKey design features include role specialization, penetration path planning,\ninter-agent communication, and generative penetration behavior. Experimental\nresults demonstrate that VulnBot outperforms baseline models such as GPT-4 and\nLlama3 in automated penetration testing tasks, particularly showcasing its\npotential in fully autonomous testing on real-world machines.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Penetration testing is a vital practice for identifying and mitigating\nvulnerabilities in cybersecurity systems, but its manual execution is\nlabor-intensive and time-consuming. Existing large language model\n(LLM)-assisted or automated penetration testing approaches often suffer from\ninefficiencies, such as a lack of contextual understanding and excessive,\nunstructured data generation. This paper presents VulnBot, an automated\npenetration testing framework that leverages LLMs to simulate the collaborative\nworkflow of human penetration testing teams through a multi-agent system. To\naddress the inefficiencies and reliance on manual intervention in traditional\npenetration testing methods, VulnBot decomposes complex tasks into three\nspecialized phases: reconnaissance, scanning, and exploitation. These phases\nare guided by a penetration task graph (PTG) to ensure logical task execution.\nKey design features include role specialization, penetration path planning,\ninter-agent communication, and generative penetration behavior. Experimental\nresults demonstrate that VulnBot outperforms baseline models such as GPT-4 and\nLlama3 in automated penetration testing tasks, particularly showcasing its\npotential in fully autonomous testing on real-world machines."
                },
                "authors": [
                    {
                        "name": "He Kong"
                    },
                    {
                        "name": "Die Hu"
                    },
                    {
                        "name": "Jingguo Ge"
                    },
                    {
                        "name": "Liangxiong Li"
                    },
                    {
                        "name": "Tong Li"
                    },
                    {
                        "name": "Bingzhen Wu"
                    }
                ],
                "author_detail": {
                    "name": "Bingzhen Wu"
                },
                "author": "Bingzhen Wu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.13411v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.13411v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2402.09649v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2402.09649v2",
                "updated": "2025-01-23T06:30:10Z",
                "updated_parsed": [
                    2025,
                    1,
                    23,
                    6,
                    30,
                    10,
                    3,
                    23,
                    0
                ],
                "published": "2024-02-15T01:22:30Z",
                "published_parsed": [
                    2024,
                    2,
                    15,
                    1,
                    22,
                    30,
                    3,
                    46,
                    0
                ],
                "title": "ProtChatGPT: Towards Understanding Proteins with Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ProtChatGPT: Towards Understanding Proteins with Large Language Models"
                },
                "summary": "Protein research is crucial in various fundamental disciplines, but\nunderstanding their intricate structure-function relationships remains\nchallenging. Recent Large Language Models (LLMs) have made significant strides\nin comprehending task-specific knowledge, suggesting the potential for\nChatGPT-like systems specialized in protein to facilitate basic research. In\nthis work, we introduce ProtChatGPT, which aims at learning and understanding\nprotein structures via natural languages. ProtChatGPT enables users to upload\nproteins, ask questions, and engage in interactive conversations to produce\ncomprehensive answers. The system comprises protein encoders, a\nProtein-Language Pertaining Transformer (PLP-former), a projection adapter, and\nan LLM. The protein first undergoes protein encoders and PLP-former to produce\nprotein embeddings, which are then projected by the adapter to conform with the\nLLM. The LLM finally combines user questions with projected embeddings to\ngenerate informative answers. Experiments show that ProtChatGPT can produce\npromising responses to proteins and their corresponding questions. We hope that\nProtChatGPT could form the basis for further exploration and application in\nprotein research. Code and our pre-trained model will be publicly available.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Protein research is crucial in various fundamental disciplines, but\nunderstanding their intricate structure-function relationships remains\nchallenging. Recent Large Language Models (LLMs) have made significant strides\nin comprehending task-specific knowledge, suggesting the potential for\nChatGPT-like systems specialized in protein to facilitate basic research. In\nthis work, we introduce ProtChatGPT, which aims at learning and understanding\nprotein structures via natural languages. ProtChatGPT enables users to upload\nproteins, ask questions, and engage in interactive conversations to produce\ncomprehensive answers. The system comprises protein encoders, a\nProtein-Language Pertaining Transformer (PLP-former), a projection adapter, and\nan LLM. The protein first undergoes protein encoders and PLP-former to produce\nprotein embeddings, which are then projected by the adapter to conform with the\nLLM. The LLM finally combines user questions with projected embeddings to\ngenerate informative answers. Experiments show that ProtChatGPT can produce\npromising responses to proteins and their corresponding questions. We hope that\nProtChatGPT could form the basis for further exploration and application in\nprotein research. Code and our pre-trained model will be publicly available."
                },
                "authors": [
                    {
                        "name": "Chao Wang"
                    },
                    {
                        "name": "Hehe Fan"
                    },
                    {
                        "name": "Ruijie Quan"
                    },
                    {
                        "name": "Yi Yang"
                    }
                ],
                "author_detail": {
                    "name": "Yi Yang"
                },
                "author": "Yi Yang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2402.09649v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2402.09649v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "q-bio.BM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.13391v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.13391v1",
                "updated": "2025-01-23T05:24:18Z",
                "updated_parsed": [
                    2025,
                    1,
                    23,
                    5,
                    24,
                    18,
                    3,
                    23,
                    0
                ],
                "published": "2025-01-23T05:24:18Z",
                "published_parsed": [
                    2025,
                    1,
                    23,
                    5,
                    24,
                    18,
                    3,
                    23,
                    0
                ],
                "title": "Can Large Language Models Understand Preferences in Personalized\n  Recommendation?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Can Large Language Models Understand Preferences in Personalized\n  Recommendation?"
                },
                "summary": "Large Language Models (LLMs) excel in various tasks, including personalized\nrecommendations. Existing evaluation methods often focus on rating prediction,\nrelying on regression errors between actual and predicted ratings. However,\nuser rating bias and item quality, two influential factors behind rating\nscores, can obscure personal preferences in user-item pair data. To address\nthis, we introduce PerRecBench, disassociating the evaluation from these two\nfactors and assessing recommendation techniques on capturing the personal\npreferences in a grouped ranking manner. We find that the LLM-based\nrecommendation techniques that are generally good at rating prediction fail to\nidentify users' favored and disfavored items when the user rating bias and item\nquality are eliminated by grouping users. With PerRecBench and 19 LLMs, we find\nthat while larger models generally outperform smaller ones, they still struggle\nwith personalized recommendation. Our findings reveal the superiority of\npairwise and listwise ranking approaches over pointwise ranking, PerRecBench's\nlow correlation with traditional regression metrics, the importance of user\nprofiles, and the role of pretraining data distributions. We further explore\nthree supervised fine-tuning strategies, finding that merging weights from\nsingle-format training is promising but improving LLMs' understanding of user\npreferences remains an open research problem. Code and data are available at\nhttps://github.com/TamSiuhin/PerRecBench",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) excel in various tasks, including personalized\nrecommendations. Existing evaluation methods often focus on rating prediction,\nrelying on regression errors between actual and predicted ratings. However,\nuser rating bias and item quality, two influential factors behind rating\nscores, can obscure personal preferences in user-item pair data. To address\nthis, we introduce PerRecBench, disassociating the evaluation from these two\nfactors and assessing recommendation techniques on capturing the personal\npreferences in a grouped ranking manner. We find that the LLM-based\nrecommendation techniques that are generally good at rating prediction fail to\nidentify users' favored and disfavored items when the user rating bias and item\nquality are eliminated by grouping users. With PerRecBench and 19 LLMs, we find\nthat while larger models generally outperform smaller ones, they still struggle\nwith personalized recommendation. Our findings reveal the superiority of\npairwise and listwise ranking approaches over pointwise ranking, PerRecBench's\nlow correlation with traditional regression metrics, the importance of user\nprofiles, and the role of pretraining data distributions. We further explore\nthree supervised fine-tuning strategies, finding that merging weights from\nsingle-format training is promising but improving LLMs' understanding of user\npreferences remains an open research problem. Code and data are available at\nhttps://github.com/TamSiuhin/PerRecBench"
                },
                "authors": [
                    {
                        "name": "Zhaoxuan Tan"
                    },
                    {
                        "name": "Zinan Zeng"
                    },
                    {
                        "name": "Qingkai Zeng"
                    },
                    {
                        "name": "Zhenyu Wu"
                    },
                    {
                        "name": "Zheyuan Liu"
                    },
                    {
                        "name": "Fengran Mo"
                    },
                    {
                        "name": "Meng Jiang"
                    }
                ],
                "author_detail": {
                    "name": "Meng Jiang"
                },
                "author": "Meng Jiang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.13391v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.13391v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.13381v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.13381v1",
                "updated": "2025-01-23T04:50:03Z",
                "updated_parsed": [
                    2025,
                    1,
                    23,
                    4,
                    50,
                    3,
                    3,
                    23,
                    0
                ],
                "published": "2025-01-23T04:50:03Z",
                "published_parsed": [
                    2025,
                    1,
                    23,
                    4,
                    50,
                    3,
                    3,
                    23,
                    0
                ],
                "title": "Do as We Do, Not as You Think: the Conformity of Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Do as We Do, Not as You Think: the Conformity of Large Language Models"
                },
                "summary": "Recent advancements in large language models (LLMs) revolutionize the field\nof intelligent agents, enabling collaborative multi-agent systems capable of\ntackling complex problems across various domains. However, the potential of\nconformity within these systems, analogous to phenomena like conformity bias\nand groupthink in human group dynamics, remains largely unexplored, raising\nconcerns about their collective problem-solving capabilities and possible\nethical implications. This paper presents a comprehensive study on conformity\nin LLM-driven multi-agent systems, focusing on three aspects: the existence of\nconformity, the factors influencing conformity, and potential mitigation\nstrategies. In particular, we introduce BenchForm, a new conformity-oriented\nbenchmark, featuring reasoning-intensive tasks and five distinct interaction\nprotocols designed to probe LLMs' behavior in collaborative scenarios. Several\nrepresentative LLMs are evaluated on BenchForm, using metrics such as\nconformity rate and independence rate to quantify conformity's impact. Our\nanalysis delves into factors influencing conformity, including interaction time\nand majority size, and examines how the subject agent rationalizes its\nconforming behavior. Furthermore, we explore two strategies to mitigate\nconformity effects, i.e., developing enhanced personas and implementing a\nreflection mechanism. Several interesting findings regarding LLMs' conformity\nare derived from empirical results and case studies. We hope that these\ninsights can pave the way for more robust and ethically-aligned collaborative\nAI systems. Our benchmark and code are available at BenchForm.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advancements in large language models (LLMs) revolutionize the field\nof intelligent agents, enabling collaborative multi-agent systems capable of\ntackling complex problems across various domains. However, the potential of\nconformity within these systems, analogous to phenomena like conformity bias\nand groupthink in human group dynamics, remains largely unexplored, raising\nconcerns about their collective problem-solving capabilities and possible\nethical implications. This paper presents a comprehensive study on conformity\nin LLM-driven multi-agent systems, focusing on three aspects: the existence of\nconformity, the factors influencing conformity, and potential mitigation\nstrategies. In particular, we introduce BenchForm, a new conformity-oriented\nbenchmark, featuring reasoning-intensive tasks and five distinct interaction\nprotocols designed to probe LLMs' behavior in collaborative scenarios. Several\nrepresentative LLMs are evaluated on BenchForm, using metrics such as\nconformity rate and independence rate to quantify conformity's impact. Our\nanalysis delves into factors influencing conformity, including interaction time\nand majority size, and examines how the subject agent rationalizes its\nconforming behavior. Furthermore, we explore two strategies to mitigate\nconformity effects, i.e., developing enhanced personas and implementing a\nreflection mechanism. Several interesting findings regarding LLMs' conformity\nare derived from empirical results and case studies. We hope that these\ninsights can pave the way for more robust and ethically-aligned collaborative\nAI systems. Our benchmark and code are available at BenchForm."
                },
                "authors": [
                    {
                        "name": "Zhiyuan Weng"
                    },
                    {
                        "name": "Guikun Chen"
                    },
                    {
                        "name": "Wenguan Wang"
                    }
                ],
                "author_detail": {
                    "name": "Wenguan Wang"
                },
                "author": "Wenguan Wang",
                "arxiv_comment": "ICLR 2025. Code: https://github.com/Zhiyuan-Weng/BenchForm",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.13381v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.13381v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.05675v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.05675v3",
                "updated": "2025-01-23T04:23:49Z",
                "updated_parsed": [
                    2025,
                    1,
                    23,
                    4,
                    23,
                    49,
                    3,
                    23,
                    0
                ],
                "published": "2025-01-10T02:57:08Z",
                "published_parsed": [
                    2025,
                    1,
                    10,
                    2,
                    57,
                    8,
                    4,
                    10,
                    0
                ],
                "title": "Synergizing Large Language Models and Task-specific Models for Time\n  Series Anomaly Detection",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Synergizing Large Language Models and Task-specific Models for Time\n  Series Anomaly Detection"
                },
                "summary": "In anomaly detection, methods based on large language models (LLMs) can\nincorporate expert knowledge by reading professional document, while\ntask-specific small models excel at extracting normal data patterns and\ndetecting value fluctuations from training data of target applications.\nInspired by the human nervous system, where the brain stores expert knowledge\nand the peripheral nervous system and spinal cord handle specific tasks like\nwithdrawal and knee-jerk reflexes, we propose CoLLaTe, a framework designed to\nfacilitate collaboration between LLMs and task-specific models, leveraging the\nstrengths of both models for anomaly detection.\n  In particular, we first formulate the collaboration process and identify two\nkey challenges in the collaboration:\n  (1) the misalignment between the expression domains of the LLMs and\ntask-specific small models, and (2) error accumulation arising from the\npredictions of both models.\n  To address these challenges, we then introduce two key components in CoLLaTe:\na model alignment module and a collaborative loss function. Through theoretical\nanalysis and experimental validation, we demonstrate that these components\neffectively mitigate the identified challenges and achieve better performance\nthan both LLM-based and task-specific models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In anomaly detection, methods based on large language models (LLMs) can\nincorporate expert knowledge by reading professional document, while\ntask-specific small models excel at extracting normal data patterns and\ndetecting value fluctuations from training data of target applications.\nInspired by the human nervous system, where the brain stores expert knowledge\nand the peripheral nervous system and spinal cord handle specific tasks like\nwithdrawal and knee-jerk reflexes, we propose CoLLaTe, a framework designed to\nfacilitate collaboration between LLMs and task-specific models, leveraging the\nstrengths of both models for anomaly detection.\n  In particular, we first formulate the collaboration process and identify two\nkey challenges in the collaboration:\n  (1) the misalignment between the expression domains of the LLMs and\ntask-specific small models, and (2) error accumulation arising from the\npredictions of both models.\n  To address these challenges, we then introduce two key components in CoLLaTe:\na model alignment module and a collaborative loss function. Through theoretical\nanalysis and experimental validation, we demonstrate that these components\neffectively mitigate the identified challenges and achieve better performance\nthan both LLM-based and task-specific models."
                },
                "authors": [
                    {
                        "name": "Feiyi Chen"
                    },
                    {
                        "name": "Leilei Zhang"
                    },
                    {
                        "name": "Guansong Pang"
                    },
                    {
                        "name": "Roger Zimmermann"
                    },
                    {
                        "name": "Shuiguang Deng"
                    }
                ],
                "author_detail": {
                    "name": "Shuiguang Deng"
                },
                "author": "Shuiguang Deng",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.05675v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.05675v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.03518v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.03518v4",
                "updated": "2025-01-23T04:04:22Z",
                "updated_parsed": [
                    2025,
                    1,
                    23,
                    4,
                    4,
                    22,
                    3,
                    23,
                    0
                ],
                "published": "2024-07-03T21:34:26Z",
                "published_parsed": [
                    2024,
                    7,
                    3,
                    21,
                    34,
                    26,
                    2,
                    185,
                    0
                ],
                "title": "Improving LLM Abilities in Idiomatic Translation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Improving LLM Abilities in Idiomatic Translation"
                },
                "summary": "For large language models (LLMs) like NLLB and GPT, translating idioms\nremains a challenge. Our goal is to enhance translation fidelity by improving\nLLM processing of idiomatic language while preserving the original linguistic\nstyle. This has a significant social impact, as it preserves cultural nuances\nand ensures translated texts retain their intent and emotional resonance,\nfostering better cross-cultural communication. Previous work has utilized\nknowledge bases like IdiomKB by providing the LLM with the meaning of an idiom\nto use in translation. Although this method yielded better results than a\ndirect translation, it is still limited in its ability to preserve idiomatic\nwriting style across languages. In this research, we expand upon the knowledge\nbase to find corresponding idioms in the target language. Our research performs\ntranslations using two methods: The first method employs the\nSentenceTransformers model to semantically generate cosine similarity scores\nbetween the meanings of the original and target language idioms, selecting the\nbest idiom (Cosine Similarity method). The second method uses an LLM to find a\ncorresponding idiom in the target language for use in the translation\n(LLM-generated idiom method). As a baseline, we performed a direct translation\nwithout providing additional information. Human evaluations on the English ->\nChinese, and Chinese -> English show the Cosine Similarity Lookup method\nout-performed others in all GPT4o translations. To further build upon IdiomKB,\nwe developed a low-resource Urdu dataset containing Urdu idioms and their\ntranslations. Despite dataset limitations, the Cosine Similarity Lookup method\nshows promise, potentially overcoming language barriers and enabling the\nexploration of diverse literary works in Chinese and Urdu.(LoResLM @ COLING\nPreprint)",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "For large language models (LLMs) like NLLB and GPT, translating idioms\nremains a challenge. Our goal is to enhance translation fidelity by improving\nLLM processing of idiomatic language while preserving the original linguistic\nstyle. This has a significant social impact, as it preserves cultural nuances\nand ensures translated texts retain their intent and emotional resonance,\nfostering better cross-cultural communication. Previous work has utilized\nknowledge bases like IdiomKB by providing the LLM with the meaning of an idiom\nto use in translation. Although this method yielded better results than a\ndirect translation, it is still limited in its ability to preserve idiomatic\nwriting style across languages. In this research, we expand upon the knowledge\nbase to find corresponding idioms in the target language. Our research performs\ntranslations using two methods: The first method employs the\nSentenceTransformers model to semantically generate cosine similarity scores\nbetween the meanings of the original and target language idioms, selecting the\nbest idiom (Cosine Similarity method). The second method uses an LLM to find a\ncorresponding idiom in the target language for use in the translation\n(LLM-generated idiom method). As a baseline, we performed a direct translation\nwithout providing additional information. Human evaluations on the English ->\nChinese, and Chinese -> English show the Cosine Similarity Lookup method\nout-performed others in all GPT4o translations. To further build upon IdiomKB,\nwe developed a low-resource Urdu dataset containing Urdu idioms and their\ntranslations. Despite dataset limitations, the Cosine Similarity Lookup method\nshows promise, potentially overcoming language barriers and enabling the\nexploration of diverse literary works in Chinese and Urdu.(LoResLM @ COLING\nPreprint)"
                },
                "authors": [
                    {
                        "name": "Sundesh Donthi"
                    },
                    {
                        "name": "Maximilian Spencer"
                    },
                    {
                        "name": "Om Patel"
                    },
                    {
                        "name": "Joon Doh"
                    },
                    {
                        "name": "Eid Rodan"
                    },
                    {
                        "name": "Kevin Zhu"
                    },
                    {
                        "name": "Sean O'Brien"
                    }
                ],
                "author_detail": {
                    "name": "Sean O'Brien"
                },
                "author": "Sean O'Brien",
                "arxiv_comment": "Preprint for LoResLM Workshop at COLING 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.03518v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.03518v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.13363v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.13363v1",
                "updated": "2025-01-23T04:04:22Z",
                "updated_parsed": [
                    2025,
                    1,
                    23,
                    4,
                    4,
                    22,
                    3,
                    23,
                    0
                ],
                "published": "2025-01-23T04:04:22Z",
                "published_parsed": [
                    2025,
                    1,
                    23,
                    4,
                    4,
                    22,
                    3,
                    23,
                    0
                ],
                "title": "False Sense of Security on Protected Wi-Fi Networks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "False Sense of Security on Protected Wi-Fi Networks"
                },
                "summary": "The Wi-Fi technology (IEEE 802.11) was introduced in 1997. With the\nincreasing use and deployment of such networks, their security has also\nattracted considerable attention. Current Wi-Fi networks use WPA2 (Wi-Fi\nProtected Access 2) for security (authentication and encryption) between access\npoints and clients. According to the IEEE 802.11i-2004 standard, wireless\nnetworks secured with WPA2-PSK (Pre-Shared Key) are required to be protected\nwith a passphrase between 8 to 63 ASCII characters. However, a poorly chosen\npassphrase significantly reduces the effectiveness of both WPA2 and\nWPA3-Personal Transition Mode. The objective of this paper is to empirically\nevaluate password choices in the wild and evaluate weakness in current common\npractices. We collected a total of 3,352 password hashes from Wi-Fi access\npoints and determine the passphrases that were protecting them. We then analyze\nthese passwords to investigate the impact of user's behavior and preference for\nconvenience on passphrase strength in secured private Wi-Fi networks in\nSingapore. We characterized the predictability of passphrases that use the\nminimum required length of 8 numeric or alphanumeric characters, and/or symbols\nstipulated in wireless security standards, and the usage of default passwords,\nand found that 16 percent of the passwords show such behavior. Our results also\nindicate the prevalence of the use of default passwords by hardware\nmanufacturers. We correlate our results with our findings and recommend methods\nthat will improve the overall security and future of our Wi-Fi networks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Wi-Fi technology (IEEE 802.11) was introduced in 1997. With the\nincreasing use and deployment of such networks, their security has also\nattracted considerable attention. Current Wi-Fi networks use WPA2 (Wi-Fi\nProtected Access 2) for security (authentication and encryption) between access\npoints and clients. According to the IEEE 802.11i-2004 standard, wireless\nnetworks secured with WPA2-PSK (Pre-Shared Key) are required to be protected\nwith a passphrase between 8 to 63 ASCII characters. However, a poorly chosen\npassphrase significantly reduces the effectiveness of both WPA2 and\nWPA3-Personal Transition Mode. The objective of this paper is to empirically\nevaluate password choices in the wild and evaluate weakness in current common\npractices. We collected a total of 3,352 password hashes from Wi-Fi access\npoints and determine the passphrases that were protecting them. We then analyze\nthese passwords to investigate the impact of user's behavior and preference for\nconvenience on passphrase strength in secured private Wi-Fi networks in\nSingapore. We characterized the predictability of passphrases that use the\nminimum required length of 8 numeric or alphanumeric characters, and/or symbols\nstipulated in wireless security standards, and the usage of default passwords,\nand found that 16 percent of the passwords show such behavior. Our results also\nindicate the prevalence of the use of default passwords by hardware\nmanufacturers. We correlate our results with our findings and recommend methods\nthat will improve the overall security and future of our Wi-Fi networks."
                },
                "authors": [
                    {
                        "name": "Yong Zhi Lim"
                    },
                    {
                        "name": "Hazmei Bin Abdul Rahman"
                    },
                    {
                        "name": "Biplab Sikdar"
                    }
                ],
                "author_detail": {
                    "name": "Biplab Sikdar"
                },
                "author": "Biplab Sikdar",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.13363v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.13363v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.13344v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.13344v1",
                "updated": "2025-01-23T03:05:13Z",
                "updated_parsed": [
                    2025,
                    1,
                    23,
                    3,
                    5,
                    13,
                    3,
                    23,
                    0
                ],
                "published": "2025-01-23T03:05:13Z",
                "published_parsed": [
                    2025,
                    1,
                    23,
                    3,
                    5,
                    13,
                    3,
                    23,
                    0
                ],
                "title": "Full-Stack Optimized Large Language Models for Lifelong Sequential\n  Behavior Comprehension in Recommendation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Full-Stack Optimized Large Language Models for Lifelong Sequential\n  Behavior Comprehension in Recommendation"
                },
                "summary": "In this paper, we address the lifelong sequential behavior incomprehension\nproblem in large language models (LLMs) for recommendation, where LLMs struggle\nto extract useful information from long user behavior sequences, even within\ntheir context limits. To tackle this, we propose ReLLaX (Retrieval-enhanced\nLarge Language models Plus), a framework offering optimization across data,\nprompt, and parameter levels. At the data level, we introduce Semantic User\nBehavior Retrieval (SUBR) to reduce sequence heterogeneity, making it easier\nfor LLMs to extract key information. For prompt-level enhancement, we employ\nSoft Prompt Augmentation (SPA) to inject collaborative knowledge, aligning item\nrepresentations with recommendation tasks and improving LLMs's exploration of\nitem relationships. Finally, at the parameter level, we propose Component\nFully-interactive LoRA (CFLoRA), which enhances LoRA's expressiveness by\nenabling interactions between its components, allowing better capture of\nsequential information. Moreover, we present new perspectives to compare\ncurrent LoRA-based LLM4Rec methods, i.e. from both a composite and a decomposed\nview. We theoretically demonstrate that the ways they employ LoRA for\nrecommendation are degraded versions of our CFLoRA, with different constraints\non atom component interactions. Extensive experiments on three public datasets\ndemonstrate ReLLaX's superiority over existing baselines and its ability to\nmitigate lifelong sequential behavior incomprehension effectively.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this paper, we address the lifelong sequential behavior incomprehension\nproblem in large language models (LLMs) for recommendation, where LLMs struggle\nto extract useful information from long user behavior sequences, even within\ntheir context limits. To tackle this, we propose ReLLaX (Retrieval-enhanced\nLarge Language models Plus), a framework offering optimization across data,\nprompt, and parameter levels. At the data level, we introduce Semantic User\nBehavior Retrieval (SUBR) to reduce sequence heterogeneity, making it easier\nfor LLMs to extract key information. For prompt-level enhancement, we employ\nSoft Prompt Augmentation (SPA) to inject collaborative knowledge, aligning item\nrepresentations with recommendation tasks and improving LLMs's exploration of\nitem relationships. Finally, at the parameter level, we propose Component\nFully-interactive LoRA (CFLoRA), which enhances LoRA's expressiveness by\nenabling interactions between its components, allowing better capture of\nsequential information. Moreover, we present new perspectives to compare\ncurrent LoRA-based LLM4Rec methods, i.e. from both a composite and a decomposed\nview. We theoretically demonstrate that the ways they employ LoRA for\nrecommendation are degraded versions of our CFLoRA, with different constraints\non atom component interactions. Extensive experiments on three public datasets\ndemonstrate ReLLaX's superiority over existing baselines and its ability to\nmitigate lifelong sequential behavior incomprehension effectively."
                },
                "authors": [
                    {
                        "name": "Rong Shan"
                    },
                    {
                        "name": "Jiachen Zhu"
                    },
                    {
                        "name": "Jianghao Lin"
                    },
                    {
                        "name": "Chenxu Zhu"
                    },
                    {
                        "name": "Bo Chen"
                    },
                    {
                        "name": "Ruiming Tang"
                    },
                    {
                        "name": "Yong Yu"
                    },
                    {
                        "name": "Weinan Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Weinan Zhang"
                },
                "author": "Weinan Zhang",
                "arxiv_comment": "Under Review",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.13344v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.13344v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.12174v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.12174v2",
                "updated": "2025-01-23T02:28:28Z",
                "updated_parsed": [
                    2025,
                    1,
                    23,
                    2,
                    28,
                    28,
                    3,
                    23,
                    0
                ],
                "published": "2025-01-21T14:32:50Z",
                "published_parsed": [
                    2025,
                    1,
                    21,
                    14,
                    32,
                    50,
                    1,
                    21,
                    0
                ],
                "title": "BiMarker: Enhancing Text Watermark Detection for Large Language Models\n  with Bipolar Watermarks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "BiMarker: Enhancing Text Watermark Detection for Large Language Models\n  with Bipolar Watermarks"
                },
                "summary": "The rapid proliferation of Large Language Models (LLMs) has raised concerns\nabout misuse and the challenges of distinguishing AI-generated text from\nhuman-written content. Existing watermarking techniques, such as \\kgw, still\nface limitations under low watermark strength, stringent false-positive\nrequirements, and low-entropy scenarios. Our analysis reveals that current\ndetection methods rely on coarse estimates of non-watermarked text, which\nconstrains watermark detectability. We propose the Bipolar Watermark\n(BiMarker), a novel approach that divides generated text into positive and\nnegative poles, leveraging the difference in green token counts for detection.\nThis differential mechanism significantly enhances the detectability of\nwatermarked text. Theoretical analysis and experimental results demonstrate\nBiMarker's effectiveness and compatibility with existing optimization\ntechniques, offering a new optimization dimension for watermarking in\nLLM-generated content.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rapid proliferation of Large Language Models (LLMs) has raised concerns\nabout misuse and the challenges of distinguishing AI-generated text from\nhuman-written content. Existing watermarking techniques, such as \\kgw, still\nface limitations under low watermark strength, stringent false-positive\nrequirements, and low-entropy scenarios. Our analysis reveals that current\ndetection methods rely on coarse estimates of non-watermarked text, which\nconstrains watermark detectability. We propose the Bipolar Watermark\n(BiMarker), a novel approach that divides generated text into positive and\nnegative poles, leveraging the difference in green token counts for detection.\nThis differential mechanism significantly enhances the detectability of\nwatermarked text. Theoretical analysis and experimental results demonstrate\nBiMarker's effectiveness and compatibility with existing optimization\ntechniques, offering a new optimization dimension for watermarking in\nLLM-generated content."
                },
                "authors": [
                    {
                        "name": "Zhuang Li"
                    }
                ],
                "author_detail": {
                    "name": "Zhuang Li"
                },
                "author": "Zhuang Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.12174v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.12174v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.13333v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.13333v1",
                "updated": "2025-01-23T02:25:44Z",
                "updated_parsed": [
                    2025,
                    1,
                    23,
                    2,
                    25,
                    44,
                    3,
                    23,
                    0
                ],
                "published": "2025-01-23T02:25:44Z",
                "published_parsed": [
                    2025,
                    1,
                    23,
                    2,
                    25,
                    44,
                    3,
                    23,
                    0
                ],
                "title": "AgentRec: Agent Recommendation Using Sentence Embeddings Aligned to\n  Human Feedback",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AgentRec: Agent Recommendation Using Sentence Embeddings Aligned to\n  Human Feedback"
                },
                "summary": "Multi-agent systems must decide which agent is the most appropriate for a\ngiven task. We propose a novel architecture for recommending which LLM agent\nout of many should perform a task given a natural language prompt by extending\nthe Sentence-BERT (SBERT) encoder model. On test data, we are able to achieve a\ntop-1 accuracy of 92.2% with each classification taking less than 300\nmilliseconds. In contrast to traditional classification methods, our\narchitecture is computationally cheap, adaptive to new classes, interpretable,\nand controllable with arbitrary metrics through reinforcement learning. By\nencoding natural language prompts into sentence embeddings, our model captures\nthe semantic content relevant to recommending an agent. The distance between\nsentence embeddings that belong to the same agent is then minimized through\nfine-tuning and aligned to human values through reinforcement learning from\nhuman feedback. This allows the classification of natural language prompts\nbased on their nearest neighbors by measuring the cosine similarity between\nembeddings. This work is made possible through the generation of a synthetic\ndataset for agent recommendation, which we have open-sourced to the public\nalong with the code for AgentRec recommendation system at\nhttps://github.com/joshprk/agentrec.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-agent systems must decide which agent is the most appropriate for a\ngiven task. We propose a novel architecture for recommending which LLM agent\nout of many should perform a task given a natural language prompt by extending\nthe Sentence-BERT (SBERT) encoder model. On test data, we are able to achieve a\ntop-1 accuracy of 92.2% with each classification taking less than 300\nmilliseconds. In contrast to traditional classification methods, our\narchitecture is computationally cheap, adaptive to new classes, interpretable,\nand controllable with arbitrary metrics through reinforcement learning. By\nencoding natural language prompts into sentence embeddings, our model captures\nthe semantic content relevant to recommending an agent. The distance between\nsentence embeddings that belong to the same agent is then minimized through\nfine-tuning and aligned to human values through reinforcement learning from\nhuman feedback. This allows the classification of natural language prompts\nbased on their nearest neighbors by measuring the cosine similarity between\nembeddings. This work is made possible through the generation of a synthetic\ndataset for agent recommendation, which we have open-sourced to the public\nalong with the code for AgentRec recommendation system at\nhttps://github.com/joshprk/agentrec."
                },
                "authors": [
                    {
                        "name": "Joshua Park"
                    },
                    {
                        "name": "Yongfeng Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Yongfeng Zhang"
                },
                "author": "Yongfeng Zhang",
                "arxiv_comment": "10 pages, 8 figures, preprint",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.13333v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.13333v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.13331v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.13331v1",
                "updated": "2025-01-23T02:20:08Z",
                "updated_parsed": [
                    2025,
                    1,
                    23,
                    2,
                    20,
                    8,
                    3,
                    23,
                    0
                ],
                "published": "2025-01-23T02:20:08Z",
                "published_parsed": [
                    2025,
                    1,
                    23,
                    2,
                    20,
                    8,
                    3,
                    23,
                    0
                ],
                "title": "Qrazor: Reliable and effortless 4-bit llm quantization by significant\n  data razoring",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Qrazor: Reliable and effortless 4-bit llm quantization by significant\n  data razoring"
                },
                "summary": "Large-scale language models (LLMs) have demonstrated outstanding performance\nin language processing tasks, yet their deployment is often hindered by high\nmemory demands and computational complexity. Although low-bit quantization\ntechniques, such as 4-bit quantization, present a potential solution, they\nfrequently lead to significant accuracy degradation or require substantial\neffort for such aggressive quantization approaches. To overcome these\nchallenges, we introduce QRazor, a reliable and effortless quantization scheme\ndesigned to enable 4-bit quantization for weights, activations, and KV cache in\ntransformer-based LLMs. The scheme involves two main stages: quantization and\ncompression. During the quantization stage, weights, activations, and KV cache\nvalues are quantized with wider 8 or 16-bit integers as a basis to achieve\nnearly identical accuracy to the original full-precision LLM models, using the\nabsolute max scaling. Subsequently, all data are compressed to 4-bit using our\nproposed significant data razoring (SDR) technique, which retains only the four\nmost salient bits while discarding the others. Furthermore, we present an\ninteger-based arithmetic unit dedicated to QRazor, enabling direct\nlow-precision arithmetic operations without decompressing the SDR data. Despite\nthe reduced quantization effort, QRazor achieves LLM accuracies better or\ncomparable to state-of-the-art 4-bit methods. By also validating the hardware\nefficiency, our decompression-free arithmetic unit achieves 61.2% and 57.8%\nreduction in area and power consumption, respectively.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large-scale language models (LLMs) have demonstrated outstanding performance\nin language processing tasks, yet their deployment is often hindered by high\nmemory demands and computational complexity. Although low-bit quantization\ntechniques, such as 4-bit quantization, present a potential solution, they\nfrequently lead to significant accuracy degradation or require substantial\neffort for such aggressive quantization approaches. To overcome these\nchallenges, we introduce QRazor, a reliable and effortless quantization scheme\ndesigned to enable 4-bit quantization for weights, activations, and KV cache in\ntransformer-based LLMs. The scheme involves two main stages: quantization and\ncompression. During the quantization stage, weights, activations, and KV cache\nvalues are quantized with wider 8 or 16-bit integers as a basis to achieve\nnearly identical accuracy to the original full-precision LLM models, using the\nabsolute max scaling. Subsequently, all data are compressed to 4-bit using our\nproposed significant data razoring (SDR) technique, which retains only the four\nmost salient bits while discarding the others. Furthermore, we present an\ninteger-based arithmetic unit dedicated to QRazor, enabling direct\nlow-precision arithmetic operations without decompressing the SDR data. Despite\nthe reduced quantization effort, QRazor achieves LLM accuracies better or\ncomparable to state-of-the-art 4-bit methods. By also validating the hardware\nefficiency, our decompression-free arithmetic unit achieves 61.2% and 57.8%\nreduction in area and power consumption, respectively."
                },
                "authors": [
                    {
                        "name": "Dongyoung Lee"
                    },
                    {
                        "name": "Seungkyu Choi"
                    },
                    {
                        "name": "Ik Joon Chang"
                    }
                ],
                "author_detail": {
                    "name": "Ik Joon Chang"
                },
                "author": "Ik Joon Chang",
                "arxiv_comment": "19 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.13331v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.13331v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.10245v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.10245v3",
                "updated": "2025-01-23T02:10:45Z",
                "updated_parsed": [
                    2025,
                    1,
                    23,
                    2,
                    10,
                    45,
                    3,
                    23,
                    0
                ],
                "published": "2024-09-16T12:55:14Z",
                "published_parsed": [
                    2024,
                    9,
                    16,
                    12,
                    55,
                    14,
                    0,
                    260,
                    0
                ],
                "title": "From Text to Emoji: How PEFT-Driven Personality Manipulation Unleashes\n  the Emoji Potential in LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "From Text to Emoji: How PEFT-Driven Personality Manipulation Unleashes\n  the Emoji Potential in LLMs"
                },
                "summary": "The manipulation of the personality traits of large language models (LLMs)\nhas emerged as a key area of research. Methods like prompt-based In-Context\nKnowledge Editing (IKE) and gradient-based Model Editor Networks (MEND) have\nbeen explored but show irregularity and variability; IKE depends on the prompt,\nleading to variability and sensitivity, while MEND yields inconsistent and\ngibberish outputs. To address this, we employed Opinion QA Based\nParameter-Efficient Fine-Tuning (PEFT), specifically Quantized Low-Rank\nAdaptation (QLoRA), to manipulate the Big Five personality traits: Openness,\nConscientiousness, Extraversion, Agreeableness, and Neuroticism. After PEFT,\nmodels such as Mistral-7B-Instruct and LLaMA-2-7B-chat began generating emojis,\neven though no emojis were present in the PEFT data. For instance,\nLLaMA-2-7B-chat generated emojis in 99.5% of extraversion-related test\ninstances, while Mistral-7B-Instruct did so in 92.5% of openness-related test\ninstances. ICL Explainability analysis indicated that the LLMs used emojis\nintentionally to express these traits. Mechanistic Interpretability analysis\nshowed that this latent behaviour of LLMs could be traced to specific neurons\nthat became activated or amplified after PEFT. This paper provides a number of\nnovel contributions. First, introducing an Opinion QA dataset for PEFT-driven\npersonality manipulation; second, developing metric models to benchmark LLM\npersonality traits; third, demonstrating PEFT's superiority over IKE in\npersonality manipulation; and finally, analysing and validating emoji usage\nthrough explainability methods such as Mechanistic Interpretability and\nIn-context learning Explainability methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The manipulation of the personality traits of large language models (LLMs)\nhas emerged as a key area of research. Methods like prompt-based In-Context\nKnowledge Editing (IKE) and gradient-based Model Editor Networks (MEND) have\nbeen explored but show irregularity and variability; IKE depends on the prompt,\nleading to variability and sensitivity, while MEND yields inconsistent and\ngibberish outputs. To address this, we employed Opinion QA Based\nParameter-Efficient Fine-Tuning (PEFT), specifically Quantized Low-Rank\nAdaptation (QLoRA), to manipulate the Big Five personality traits: Openness,\nConscientiousness, Extraversion, Agreeableness, and Neuroticism. After PEFT,\nmodels such as Mistral-7B-Instruct and LLaMA-2-7B-chat began generating emojis,\neven though no emojis were present in the PEFT data. For instance,\nLLaMA-2-7B-chat generated emojis in 99.5% of extraversion-related test\ninstances, while Mistral-7B-Instruct did so in 92.5% of openness-related test\ninstances. ICL Explainability analysis indicated that the LLMs used emojis\nintentionally to express these traits. Mechanistic Interpretability analysis\nshowed that this latent behaviour of LLMs could be traced to specific neurons\nthat became activated or amplified after PEFT. This paper provides a number of\nnovel contributions. First, introducing an Opinion QA dataset for PEFT-driven\npersonality manipulation; second, developing metric models to benchmark LLM\npersonality traits; third, demonstrating PEFT's superiority over IKE in\npersonality manipulation; and finally, analysing and validating emoji usage\nthrough explainability methods such as Mechanistic Interpretability and\nIn-context learning Explainability methods."
                },
                "authors": [
                    {
                        "name": "Navya Jain"
                    },
                    {
                        "name": "Zekun Wu"
                    },
                    {
                        "name": "Cristian Munoz"
                    },
                    {
                        "name": "Airlie Hilliard"
                    },
                    {
                        "name": "Xin Guan"
                    },
                    {
                        "name": "Adriano Koshiyama"
                    },
                    {
                        "name": "Emre Kazim"
                    },
                    {
                        "name": "Philip Treleaven"
                    }
                ],
                "author_detail": {
                    "name": "Philip Treleaven"
                },
                "author": "Philip Treleaven",
                "arxiv_comment": "Findings paper of NAACL 2025 and NeurIPS 2024 Workshop on Behavioral\n  Machine Learning",
                "arxiv_journal_ref": "Findings paper of NAACL 2025 and NeurIPS 2024 Workshop on\n  Behavioral Machine Learning",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.10245v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.10245v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.13318v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.13318v1",
                "updated": "2025-01-23T02:02:38Z",
                "updated_parsed": [
                    2025,
                    1,
                    23,
                    2,
                    2,
                    38,
                    3,
                    23,
                    0
                ],
                "published": "2025-01-23T02:02:38Z",
                "published_parsed": [
                    2025,
                    1,
                    23,
                    2,
                    2,
                    38,
                    3,
                    23,
                    0
                ],
                "title": "SplitLLM: Hierarchical Split Learning for Large Language Model over\n  Wireless Network",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SplitLLM: Hierarchical Split Learning for Large Language Model over\n  Wireless Network"
                },
                "summary": "Fine-tuning a large language model (LLM) using the local data of edge users\ncan enable personalized services and applications. For privacy protection, the\nprevalent solution adopts distributed learning for fine-tuning and integrates\nlow-rank adaptation (LoRA) to reduce users' computational load. However, as the\nnumber of users increases, numerous users simultaneously communicate with the\nserver, and multiple server-side models concurrently execute on the server,\nleading to significant communication congestion and memory pressure. In this\npaper, we propose a split learning (SL) scheme for fine-tuning LLM in wireless\nnetworks, which involves one cloud server, a small number of edge servers, and\nmultiple users. Specifically, the pre-trained model and LoRA adapters are\ndivided into three parts and deployed across the cloud, edge, and user sides.\nThe training process follows the sequence of user, edge, and cloud, with\nforward and backward propagation achieved by transmitting activation and\ngradient. In each round, all edge servers and an equivalent number of users\ntrain in parallel, and only the LoRA adapters are updated. At the end of each\nround, all edge-side and user-side LoRA adapters are uploaded to the cloud for\naggregation. Extensive simulation demonstrates that the proposed scheme can\nreduce peak memory usage up to 74% compared to the state-of-the-art benchmarks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Fine-tuning a large language model (LLM) using the local data of edge users\ncan enable personalized services and applications. For privacy protection, the\nprevalent solution adopts distributed learning for fine-tuning and integrates\nlow-rank adaptation (LoRA) to reduce users' computational load. However, as the\nnumber of users increases, numerous users simultaneously communicate with the\nserver, and multiple server-side models concurrently execute on the server,\nleading to significant communication congestion and memory pressure. In this\npaper, we propose a split learning (SL) scheme for fine-tuning LLM in wireless\nnetworks, which involves one cloud server, a small number of edge servers, and\nmultiple users. Specifically, the pre-trained model and LoRA adapters are\ndivided into three parts and deployed across the cloud, edge, and user sides.\nThe training process follows the sequence of user, edge, and cloud, with\nforward and backward propagation achieved by transmitting activation and\ngradient. In each round, all edge servers and an equivalent number of users\ntrain in parallel, and only the LoRA adapters are updated. At the end of each\nround, all edge-side and user-side LoRA adapters are uploaded to the cloud for\naggregation. Extensive simulation demonstrates that the proposed scheme can\nreduce peak memory usage up to 74% compared to the state-of-the-art benchmarks."
                },
                "authors": [
                    {
                        "name": "Songge Zhang"
                    },
                    {
                        "name": "Guoliang Cheng"
                    },
                    {
                        "name": "Zuguang Li"
                    },
                    {
                        "name": "Wen Wu"
                    }
                ],
                "author_detail": {
                    "name": "Wen Wu"
                },
                "author": "Wen Wu",
                "arxiv_comment": "6 pages with 2 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.13318v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.13318v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.07739v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.07739v2",
                "updated": "2025-01-23T02:02:34Z",
                "updated_parsed": [
                    2025,
                    1,
                    23,
                    2,
                    2,
                    34,
                    3,
                    23,
                    0
                ],
                "published": "2024-10-10T09:16:05Z",
                "published_parsed": [
                    2024,
                    10,
                    10,
                    9,
                    16,
                    5,
                    3,
                    284,
                    0
                ],
                "title": "SLIM: Let LLM Learn More and Forget Less with Soft LoRA and Identity\n  Mixture",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SLIM: Let LLM Learn More and Forget Less with Soft LoRA and Identity\n  Mixture"
                },
                "summary": "Although many efforts have been made, it is still a challenge to balance the\ntraining budget, downstream performance, and the general capabilities of the\nLLMs in many applications. Training the whole model for downstream tasks is\nexpensive, and could easily result in catastrophic forgetting. By introducing\nparameter-efficient fine-tuning (PEFT), the training cost could be reduced, but\nit still suffers from forgetting, and limits the learning on the downstream\ntasks. To efficiently fine-tune the LLMs with less limitation to their\ndownstream performance while mitigating the forgetting of general capabilities,\nwe propose a novel mixture of expert (MoE) framework based on Soft LoRA and\nIdentity Mixture (SLIM), that allows dynamic routing between LoRA adapters and\nskipping connection, enables the suppression of forgetting. We adopt\nweight-yielding with sliding clustering for better out-of-domain distinguish to\nenhance the routing. We also propose to convert the mixture of low-rank\nadapters to the model merging formulation and introduce fast dynamic merging of\nLoRA adapters to keep the general capabilities of the base model. Extensive\nexperiments demonstrate that the proposed SLIM is comparable to the\nstate-of-the-art PEFT approaches on the downstream tasks while achieving the\nleading performance in mitigating catastrophic forgetting.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Although many efforts have been made, it is still a challenge to balance the\ntraining budget, downstream performance, and the general capabilities of the\nLLMs in many applications. Training the whole model for downstream tasks is\nexpensive, and could easily result in catastrophic forgetting. By introducing\nparameter-efficient fine-tuning (PEFT), the training cost could be reduced, but\nit still suffers from forgetting, and limits the learning on the downstream\ntasks. To efficiently fine-tune the LLMs with less limitation to their\ndownstream performance while mitigating the forgetting of general capabilities,\nwe propose a novel mixture of expert (MoE) framework based on Soft LoRA and\nIdentity Mixture (SLIM), that allows dynamic routing between LoRA adapters and\nskipping connection, enables the suppression of forgetting. We adopt\nweight-yielding with sliding clustering for better out-of-domain distinguish to\nenhance the routing. We also propose to convert the mixture of low-rank\nadapters to the model merging formulation and introduce fast dynamic merging of\nLoRA adapters to keep the general capabilities of the base model. Extensive\nexperiments demonstrate that the proposed SLIM is comparable to the\nstate-of-the-art PEFT approaches on the downstream tasks while achieving the\nleading performance in mitigating catastrophic forgetting."
                },
                "authors": [
                    {
                        "name": "Jiayi Han"
                    },
                    {
                        "name": "Liang Du"
                    },
                    {
                        "name": "Hongwei Du"
                    },
                    {
                        "name": "Xiangguo Zhou"
                    },
                    {
                        "name": "Yiwen Wu"
                    },
                    {
                        "name": "Weibo Zheng"
                    },
                    {
                        "name": "Donghong Han"
                    }
                ],
                "author_detail": {
                    "name": "Donghong Han"
                },
                "author": "Donghong Han",
                "arxiv_comment": "13 pages, 7 figures, 4 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.07739v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.07739v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.05208v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.05208v2",
                "updated": "2025-01-23T01:29:42Z",
                "updated_parsed": [
                    2025,
                    1,
                    23,
                    1,
                    29,
                    42,
                    3,
                    23,
                    0
                ],
                "published": "2024-12-06T17:36:28Z",
                "published_parsed": [
                    2024,
                    12,
                    6,
                    17,
                    36,
                    28,
                    4,
                    341,
                    0
                ],
                "title": "A Survey of Large Language Model-Based Generative AI for Text-to-SQL:\n  Benchmarks, Applications, Use Cases, and Challenges",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Survey of Large Language Model-Based Generative AI for Text-to-SQL:\n  Benchmarks, Applications, Use Cases, and Challenges"
                },
                "summary": "Text-to-SQL systems facilitate smooth interaction with databases by\ntranslating natural language queries into Structured Query Language (SQL),\nbridging the gap between non-technical users and complex database management\nsystems. This survey provides a comprehensive overview of the evolution of\nAI-driven text-to-SQL systems, highlighting their foundational components,\nadvancements in large language model (LLM) architectures, and the critical role\nof datasets such as Spider, WikiSQL, and CoSQL in driving progress. We examine\nthe applications of text-to-SQL in domains like healthcare, education, and\nfinance, emphasizing their transformative potential for improving data\naccessibility. Additionally, we analyze persistent challenges, including domain\ngeneralization, query optimization, support for multi-turn conversational\ninteractions, and the limited availability of datasets tailored for NoSQL\ndatabases and dynamic real-world scenarios. To address these challenges, we\noutline future research directions, such as extending text-to-SQL capabilities\nto support NoSQL databases, designing datasets for dynamic multi-turn\ninteractions, and optimizing systems for real-world scalability and robustness.\nBy surveying current advancements and identifying key gaps, this paper aims to\nguide the next generation of research and applications in LLM-based text-to-SQL\nsystems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Text-to-SQL systems facilitate smooth interaction with databases by\ntranslating natural language queries into Structured Query Language (SQL),\nbridging the gap between non-technical users and complex database management\nsystems. This survey provides a comprehensive overview of the evolution of\nAI-driven text-to-SQL systems, highlighting their foundational components,\nadvancements in large language model (LLM) architectures, and the critical role\nof datasets such as Spider, WikiSQL, and CoSQL in driving progress. We examine\nthe applications of text-to-SQL in domains like healthcare, education, and\nfinance, emphasizing their transformative potential for improving data\naccessibility. Additionally, we analyze persistent challenges, including domain\ngeneralization, query optimization, support for multi-turn conversational\ninteractions, and the limited availability of datasets tailored for NoSQL\ndatabases and dynamic real-world scenarios. To address these challenges, we\noutline future research directions, such as extending text-to-SQL capabilities\nto support NoSQL databases, designing datasets for dynamic multi-turn\ninteractions, and optimizing systems for real-world scalability and robustness.\nBy surveying current advancements and identifying key gaps, this paper aims to\nguide the next generation of research and applications in LLM-based text-to-SQL\nsystems."
                },
                "authors": [
                    {
                        "name": "Aditi Singh"
                    },
                    {
                        "name": "Akash Shetty"
                    },
                    {
                        "name": "Abul Ehtesham"
                    },
                    {
                        "name": "Saket Kumar"
                    },
                    {
                        "name": "Tala Talaei Khoei"
                    }
                ],
                "author_detail": {
                    "name": "Tala Talaei Khoei"
                },
                "author": "Tala Talaei Khoei",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.05208v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.05208v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.13306v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.13306v1",
                "updated": "2025-01-23T01:27:46Z",
                "updated_parsed": [
                    2025,
                    1,
                    23,
                    1,
                    27,
                    46,
                    3,
                    23,
                    0
                ],
                "published": "2025-01-23T01:27:46Z",
                "published_parsed": [
                    2025,
                    1,
                    23,
                    1,
                    27,
                    46,
                    3,
                    23,
                    0
                ],
                "title": "OSUM: Advancing Open Speech Understanding Models with Limited Resources\n  in Academia",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "OSUM: Advancing Open Speech Understanding Models with Limited Resources\n  in Academia"
                },
                "summary": "Large Language Models (LLMs) have made significant progress in various\ndownstream tasks, inspiring the development of Speech Understanding Language\nModels (SULMs) to enable comprehensive speech-based interactions. However, most\nadvanced SULMs are developed by the industry, leveraging large-scale datasets\nand computational resources that are not readily available to the academic\ncommunity. Moreover, the lack of transparency in training details creates\nadditional barriers to further innovation. In this study, we present OSUM, an\nOpen Speech Understanding Model designed to explore the potential of training\nSLUMs under constrained academic resources. The OSUM model combines a Whisper\nencoder with a Qwen2 LLM and supports a wide range of speech tasks, including\nspeech recognition (ASR), speech recognition with timestamps (SRWT), vocal\nevent detection (VED), speech emotion recognition (SER), speaking style\nrecognition (SSR), speaker gender classification (SGC), speaker age prediction\n(SAP), and speech-to-text chat (STTC). By employing an ASR+X training strategy,\nOSUM achieves efficient and stable multi-task training by simultaneously\noptimizing ASR alongside target tasks. Beyond delivering strong performance,\nOSUM emphasizes transparency by providing openly available data preparation and\ntraining methodologies, offering valuable insights and practical guidance for\nthe academic community. By doing so, we aim to accelerate research and\ninnovation in advanced SULM technologies.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have made significant progress in various\ndownstream tasks, inspiring the development of Speech Understanding Language\nModels (SULMs) to enable comprehensive speech-based interactions. However, most\nadvanced SULMs are developed by the industry, leveraging large-scale datasets\nand computational resources that are not readily available to the academic\ncommunity. Moreover, the lack of transparency in training details creates\nadditional barriers to further innovation. In this study, we present OSUM, an\nOpen Speech Understanding Model designed to explore the potential of training\nSLUMs under constrained academic resources. The OSUM model combines a Whisper\nencoder with a Qwen2 LLM and supports a wide range of speech tasks, including\nspeech recognition (ASR), speech recognition with timestamps (SRWT), vocal\nevent detection (VED), speech emotion recognition (SER), speaking style\nrecognition (SSR), speaker gender classification (SGC), speaker age prediction\n(SAP), and speech-to-text chat (STTC). By employing an ASR+X training strategy,\nOSUM achieves efficient and stable multi-task training by simultaneously\noptimizing ASR alongside target tasks. Beyond delivering strong performance,\nOSUM emphasizes transparency by providing openly available data preparation and\ntraining methodologies, offering valuable insights and practical guidance for\nthe academic community. By doing so, we aim to accelerate research and\ninnovation in advanced SULM technologies."
                },
                "authors": [
                    {
                        "name": "Xuelong Geng"
                    },
                    {
                        "name": "Kun Wei"
                    },
                    {
                        "name": "Qijie Shao"
                    },
                    {
                        "name": "Shuiyun Liu"
                    },
                    {
                        "name": "Zhennan Lin"
                    },
                    {
                        "name": "Zhixian Zhao"
                    },
                    {
                        "name": "Guojian Li"
                    },
                    {
                        "name": "Wenjie Tian"
                    },
                    {
                        "name": "Peikun Chen"
                    },
                    {
                        "name": "Yangze Li"
                    },
                    {
                        "name": "Pengcheng Guo"
                    },
                    {
                        "name": "Mingchen Shao"
                    },
                    {
                        "name": "Shuiyuan Wang"
                    },
                    {
                        "name": "Yuang Cao"
                    },
                    {
                        "name": "Chengyou Wang"
                    },
                    {
                        "name": "Tianyi Xu"
                    },
                    {
                        "name": "Yuhang Dai"
                    },
                    {
                        "name": "Xinfa Zhu"
                    },
                    {
                        "name": "Yue Li"
                    },
                    {
                        "name": "Li Zhang"
                    },
                    {
                        "name": "Lei Xie"
                    }
                ],
                "author_detail": {
                    "name": "Lei Xie"
                },
                "author": "Lei Xie",
                "arxiv_comment": "OSUM Technical Report",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.13306v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.13306v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SD",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2311.10652v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2311.10652v4",
                "updated": "2025-01-23T01:24:28Z",
                "updated_parsed": [
                    2025,
                    1,
                    23,
                    1,
                    24,
                    28,
                    3,
                    23,
                    0
                ],
                "published": "2023-11-17T17:14:32Z",
                "published_parsed": [
                    2023,
                    11,
                    17,
                    17,
                    14,
                    32,
                    4,
                    321,
                    0
                ],
                "title": "What Lies Beneath? Exploring the Impact of Underlying AI Model Updates\n  in AI-Infused Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "What Lies Beneath? Exploring the Impact of Underlying AI Model Updates\n  in AI-Infused Systems"
                },
                "summary": "AI models are constantly evolving, with new versions released frequently.\nHuman-AI interaction guidelines encourage notifying users about changes in\nmodel capabilities, ideally supported by thorough benchmarking. However, as AI\nsystems integrate into domain-specific workflows, exhaustive benchmarking can\nbecome impractical, often resulting in silent or minimally communicated\nupdates. This raises critical questions: Can users notice these updates? What\ncues do they rely on to distinguish between models? How do such changes affect\ntheir behavior and task performance? We address these questions through two\nstudies in the context of facial recognition for historical photo\nidentification: an online experiment examining users' ability to detect model\nupdates, followed by a diary study exploring perceptions in a real-world\ndeployment. Our findings highlight challenges in noticing AI model updates,\ntheir impact on downstream user behavior and performance, and how they lead\nusers to develop divergent folk theories. Drawing on these insights, we discuss\nstrategies for effectively communicating model updates in AI-infused systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AI models are constantly evolving, with new versions released frequently.\nHuman-AI interaction guidelines encourage notifying users about changes in\nmodel capabilities, ideally supported by thorough benchmarking. However, as AI\nsystems integrate into domain-specific workflows, exhaustive benchmarking can\nbecome impractical, often resulting in silent or minimally communicated\nupdates. This raises critical questions: Can users notice these updates? What\ncues do they rely on to distinguish between models? How do such changes affect\ntheir behavior and task performance? We address these questions through two\nstudies in the context of facial recognition for historical photo\nidentification: an online experiment examining users' ability to detect model\nupdates, followed by a diary study exploring perceptions in a real-world\ndeployment. Our findings highlight challenges in noticing AI model updates,\ntheir impact on downstream user behavior and performance, and how they lead\nusers to develop divergent folk theories. Drawing on these insights, we discuss\nstrategies for effectively communicating model updates in AI-infused systems."
                },
                "authors": [
                    {
                        "name": "Vikram Mohanty"
                    },
                    {
                        "name": "Jude Lim"
                    },
                    {
                        "name": "Kurt Luther"
                    }
                ],
                "author_detail": {
                    "name": "Kurt Luther"
                },
                "author": "Kurt Luther",
                "arxiv_doi": "10.1145/3706598.3713751",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3706598.3713751",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2311.10652v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2311.10652v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "Accepted to ACM CHI 2025 Conference on Human Factors in Computing\n  Systems",
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.14876v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.14876v2",
                "updated": "2025-01-23T01:08:19Z",
                "updated_parsed": [
                    2025,
                    1,
                    23,
                    1,
                    8,
                    19,
                    3,
                    23,
                    0
                ],
                "published": "2024-04-02T01:42:32Z",
                "published_parsed": [
                    2024,
                    4,
                    2,
                    1,
                    42,
                    32,
                    1,
                    93,
                    0
                ],
                "title": "Precise and Robust Sidewalk Detection: Leveraging Ensemble Learning to\n  Surpass LLM Limitations in Urban Environments",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Precise and Robust Sidewalk Detection: Leveraging Ensemble Learning to\n  Surpass LLM Limitations in Urban Environments"
                },
                "summary": "This study aims to compare the effectiveness of a robust ensemble model with\nthe state-of-the-art ONE-PEACE Large Language Model (LLM) for accurate\ndetection of sidewalks. Accurate sidewalk detection is crucial in improving\nroad safety and urban planning. The study evaluated the model's performance on\nCityscapes, Ade20k, and the Boston Dataset. The results showed that the\nensemble model performed better than the individual models, achieving mean\nIntersection Over Union (mIOU) scores of 93.1\\%, 90.3\\%, and 90.6\\% on these\ndatasets under ideal conditions. Additionally, the ensemble model maintained a\nconsistent level of performance even in challenging conditions such as\nSalt-and-Pepper and Speckle noise, with only a gradual decrease in efficiency\nobserved. On the other hand, the ONE-PEACE LLM performed slightly better than\nthe ensemble model in ideal scenarios but experienced a significant decline in\nperformance under noisy conditions. These findings demonstrate the robustness\nand reliability of the ensemble model, making it a valuable asset for improving\nurban infrastructure related to road safety and curb space management. This\nstudy contributes positively to the broader context of urban health and\nmobility.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This study aims to compare the effectiveness of a robust ensemble model with\nthe state-of-the-art ONE-PEACE Large Language Model (LLM) for accurate\ndetection of sidewalks. Accurate sidewalk detection is crucial in improving\nroad safety and urban planning. The study evaluated the model's performance on\nCityscapes, Ade20k, and the Boston Dataset. The results showed that the\nensemble model performed better than the individual models, achieving mean\nIntersection Over Union (mIOU) scores of 93.1\\%, 90.3\\%, and 90.6\\% on these\ndatasets under ideal conditions. Additionally, the ensemble model maintained a\nconsistent level of performance even in challenging conditions such as\nSalt-and-Pepper and Speckle noise, with only a gradual decrease in efficiency\nobserved. On the other hand, the ONE-PEACE LLM performed slightly better than\nthe ensemble model in ideal scenarios but experienced a significant decline in\nperformance under noisy conditions. These findings demonstrate the robustness\nand reliability of the ensemble model, making it a valuable asset for improving\nurban infrastructure related to road safety and curb space management. This\nstudy contributes positively to the broader context of urban health and\nmobility."
                },
                "authors": [
                    {
                        "name": "Ibne Farabi Shihab"
                    },
                    {
                        "name": "Sudesh Ramesh Bhagat"
                    },
                    {
                        "name": "Anuj Sharma"
                    }
                ],
                "author_detail": {
                    "name": "Anuj Sharma"
                },
                "author": "Anuj Sharma",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.14876v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.14876v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.13302v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.13302v1",
                "updated": "2025-01-23T01:04:00Z",
                "updated_parsed": [
                    2025,
                    1,
                    23,
                    1,
                    4,
                    0,
                    3,
                    23,
                    0
                ],
                "published": "2025-01-23T01:04:00Z",
                "published_parsed": [
                    2025,
                    1,
                    23,
                    1,
                    4,
                    0,
                    3,
                    23,
                    0
                ],
                "title": "Watching the AI Watchdogs: A Fairness and Robustness Analysis of AI\n  Safety Moderation Classifiers",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Watching the AI Watchdogs: A Fairness and Robustness Analysis of AI\n  Safety Moderation Classifiers"
                },
                "summary": "AI Safety Moderation (ASM) classifiers are designed to moderate content on\nsocial media platforms and to serve as guardrails that prevent Large Language\nModels (LLMs) from being fine-tuned on unsafe inputs. Owing to their potential\nfor disparate impact, it is crucial to ensure that these classifiers: (1) do\nnot unfairly classify content belonging to users from minority groups as unsafe\ncompared to those from majority groups and (2) that their behavior remains\nrobust and consistent across similar inputs. In this work, we thus examine the\nfairness and robustness of four widely-used, closed-source ASM classifiers:\nOpenAI Moderation API, Perspective API, Google Cloud Natural Language (GCNL)\nAPI, and Clarifai API. We assess fairness using metrics such as demographic\nparity and conditional statistical parity, comparing their performance against\nASM models and a fair-only baseline. Additionally, we analyze robustness by\ntesting the classifiers' sensitivity to small and natural input perturbations.\nOur findings reveal potential fairness and robustness gaps, highlighting the\nneed to mitigate these issues in future versions of these models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AI Safety Moderation (ASM) classifiers are designed to moderate content on\nsocial media platforms and to serve as guardrails that prevent Large Language\nModels (LLMs) from being fine-tuned on unsafe inputs. Owing to their potential\nfor disparate impact, it is crucial to ensure that these classifiers: (1) do\nnot unfairly classify content belonging to users from minority groups as unsafe\ncompared to those from majority groups and (2) that their behavior remains\nrobust and consistent across similar inputs. In this work, we thus examine the\nfairness and robustness of four widely-used, closed-source ASM classifiers:\nOpenAI Moderation API, Perspective API, Google Cloud Natural Language (GCNL)\nAPI, and Clarifai API. We assess fairness using metrics such as demographic\nparity and conditional statistical parity, comparing their performance against\nASM models and a fair-only baseline. Additionally, we analyze robustness by\ntesting the classifiers' sensitivity to small and natural input perturbations.\nOur findings reveal potential fairness and robustness gaps, highlighting the\nneed to mitigate these issues in future versions of these models."
                },
                "authors": [
                    {
                        "name": "Akshit Achara"
                    },
                    {
                        "name": "Anshuman Chhabra"
                    }
                ],
                "author_detail": {
                    "name": "Anshuman Chhabra"
                },
                "author": "Anshuman Chhabra",
                "arxiv_comment": "Accepted to NAACL 2025 Main Conference",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.13302v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.13302v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.04466v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.04466v3",
                "updated": "2025-01-23T01:01:31Z",
                "updated_parsed": [
                    2025,
                    1,
                    23,
                    1,
                    1,
                    31,
                    3,
                    23,
                    0
                ],
                "published": "2024-10-06T12:42:04Z",
                "published_parsed": [
                    2024,
                    10,
                    6,
                    12,
                    42,
                    4,
                    6,
                    280,
                    0
                ],
                "title": "Large Language Model Inference Acceleration: A Comprehensive Hardware\n  Perspective",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Model Inference Acceleration: A Comprehensive Hardware\n  Perspective"
                },
                "summary": "Large Language Models (LLMs) have demonstrated remarkable capabilities across\nvarious fields, from natural language understanding to text generation.\nCompared to non-generative LLMs like BERT and DeBERTa, generative LLMs like GPT\nseries and Llama series are currently the main focus due to their superior\nalgorithmic performance. The advancements in generative LLMs are closely\nintertwined with the development of hardware capabilities. Various hardware\nplatforms exhibit distinct hardware characteristics, which can help improve LLM\ninference performance. Therefore, this paper comprehensively surveys efficient\ngenerative LLM inference on different hardware platforms. First, we provide an\noverview of the algorithm architecture of mainstream generative LLMs and delve\ninto the inference process. Then, we summarize different optimization methods\nfor different platforms such as CPU, GPU, FPGA, ASIC, and PIM/NDP, and provide\ninference results for generative LLMs. Furthermore, we perform a qualitative\nand quantitative comparison of inference performance with batch sizes 1 and 8\non different hardware platforms by considering hardware power consumption,\nabsolute inference speed (tokens/s), and energy efficiency (tokens/J). We\ncompare the performance of the same optimization methods across different\nhardware platforms, the performance across different hardware platforms, and\nthe performance of different methods on the same hardware platform. This\nprovides a systematic and comprehensive summary of existing inference\nacceleration work by integrating software optimization methods and hardware\nplatforms. We point out that three trends (multimodality, inference-time\ncompute, and higher inference energy efficiency) are promising to redefine the\ncapabilities of edge artificial intelligence systems. Our project is available\nat https://dai.sjtu.edu.cn/project.html.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have demonstrated remarkable capabilities across\nvarious fields, from natural language understanding to text generation.\nCompared to non-generative LLMs like BERT and DeBERTa, generative LLMs like GPT\nseries and Llama series are currently the main focus due to their superior\nalgorithmic performance. The advancements in generative LLMs are closely\nintertwined with the development of hardware capabilities. Various hardware\nplatforms exhibit distinct hardware characteristics, which can help improve LLM\ninference performance. Therefore, this paper comprehensively surveys efficient\ngenerative LLM inference on different hardware platforms. First, we provide an\noverview of the algorithm architecture of mainstream generative LLMs and delve\ninto the inference process. Then, we summarize different optimization methods\nfor different platforms such as CPU, GPU, FPGA, ASIC, and PIM/NDP, and provide\ninference results for generative LLMs. Furthermore, we perform a qualitative\nand quantitative comparison of inference performance with batch sizes 1 and 8\non different hardware platforms by considering hardware power consumption,\nabsolute inference speed (tokens/s), and energy efficiency (tokens/J). We\ncompare the performance of the same optimization methods across different\nhardware platforms, the performance across different hardware platforms, and\nthe performance of different methods on the same hardware platform. This\nprovides a systematic and comprehensive summary of existing inference\nacceleration work by integrating software optimization methods and hardware\nplatforms. We point out that three trends (multimodality, inference-time\ncompute, and higher inference energy efficiency) are promising to redefine the\ncapabilities of edge artificial intelligence systems. Our project is available\nat https://dai.sjtu.edu.cn/project.html."
                },
                "authors": [
                    {
                        "name": "Jinhao Li"
                    },
                    {
                        "name": "Jiaming Xu"
                    },
                    {
                        "name": "Shan Huang"
                    },
                    {
                        "name": "Yonghua Chen"
                    },
                    {
                        "name": "Wen Li"
                    },
                    {
                        "name": "Jun Liu"
                    },
                    {
                        "name": "Yaoxiu Lian"
                    },
                    {
                        "name": "Jiayi Pan"
                    },
                    {
                        "name": "Li Ding"
                    },
                    {
                        "name": "Hao Zhou"
                    },
                    {
                        "name": "Yu Wang"
                    },
                    {
                        "name": "Guohao Dai"
                    }
                ],
                "author_detail": {
                    "name": "Guohao Dai"
                },
                "author": "Guohao Dai",
                "arxiv_comment": "51 pages, 19 figures. Update the discussion about the future trends\n  of LLM",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.04466v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.04466v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.13299v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.13299v1",
                "updated": "2025-01-23T01:01:05Z",
                "updated_parsed": [
                    2025,
                    1,
                    23,
                    1,
                    1,
                    5,
                    3,
                    23,
                    0
                ],
                "published": "2025-01-23T01:01:05Z",
                "published_parsed": [
                    2025,
                    1,
                    23,
                    1,
                    1,
                    5,
                    3,
                    23,
                    0
                ],
                "title": "Hypothesis Generation for Materials Discovery and Design Using\n  Goal-Driven and Constraint-Guided LLM Agents",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Hypothesis Generation for Materials Discovery and Design Using\n  Goal-Driven and Constraint-Guided LLM Agents"
                },
                "summary": "Materials discovery and design are essential for advancing technology across\nvarious industries by enabling the development of application-specific\nmaterials. Recent research has leveraged Large Language Models (LLMs) to\naccelerate this process. We explore the potential of LLMs to generate viable\nhypotheses that, once validated, can expedite materials discovery.\nCollaborating with materials science experts, we curated a novel dataset from\nrecent journal publications, featuring real-world goals, constraints, and\nmethods for designing real-world applications. Using this dataset, we test\nLLM-based agents that generate hypotheses for achieving given goals under\nspecific constraints. To assess the relevance and quality of these hypotheses,\nwe propose a novel scalable evaluation metric that emulates the process a\nmaterials scientist would use to evaluate a hypothesis critically. Our curated\ndataset, proposed method, and evaluation framework aim to advance future\nresearch in accelerating materials discovery and design with LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Materials discovery and design are essential for advancing technology across\nvarious industries by enabling the development of application-specific\nmaterials. Recent research has leveraged Large Language Models (LLMs) to\naccelerate this process. We explore the potential of LLMs to generate viable\nhypotheses that, once validated, can expedite materials discovery.\nCollaborating with materials science experts, we curated a novel dataset from\nrecent journal publications, featuring real-world goals, constraints, and\nmethods for designing real-world applications. Using this dataset, we test\nLLM-based agents that generate hypotheses for achieving given goals under\nspecific constraints. To assess the relevance and quality of these hypotheses,\nwe propose a novel scalable evaluation metric that emulates the process a\nmaterials scientist would use to evaluate a hypothesis critically. Our curated\ndataset, proposed method, and evaluation framework aim to advance future\nresearch in accelerating materials discovery and design with LLMs."
                },
                "authors": [
                    {
                        "name": "Shrinidhi Kumbhar"
                    },
                    {
                        "name": "Venkatesh Mishra"
                    },
                    {
                        "name": "Kevin Coutinho"
                    },
                    {
                        "name": "Divij Handa"
                    },
                    {
                        "name": "Ashif Iquebal"
                    },
                    {
                        "name": "Chitta Baral"
                    }
                ],
                "author_detail": {
                    "name": "Chitta Baral"
                },
                "author": "Chitta Baral",
                "arxiv_comment": "Accepted in NAACL 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.13299v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.13299v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            }
        ]
    }
]