[
    {
        "keyword": "kv cache",
        "arxiv_results": [
            {
                "id": "http://arxiv.org/abs/2402.14576v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2402.14576v3",
                "updated": "2024-10-30T16:06:21Z",
                "updated_parsed": [
                    2024,
                    10,
                    30,
                    16,
                    6,
                    21,
                    2,
                    304,
                    0
                ],
                "published": "2024-02-08T17:17:46Z",
                "published_parsed": [
                    2024,
                    2,
                    8,
                    17,
                    17,
                    46,
                    3,
                    39,
                    0
                ],
                "title": "Attention-Enhanced Prioritized Proximal Policy Optimization for Adaptive\n  Edge Caching",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Attention-Enhanced Prioritized Proximal Policy Optimization for Adaptive\n  Edge Caching"
                },
                "summary": "This paper tackles the growing issue of excessive data transmission in\nnetworks. With increasing traffic, backhaul links and core networks are under\nsignificant traffic, leading to the investigation of caching solutions at edge\nrouters. Many existing studies utilize Markov Decision Processes (MDP) to\ntackle caching problems, often assuming decision points at fixed intervals;\nhowever, real-world environments are characterized by random request arrivals.\nAdditionally, critical file attributes such as lifetime, size, and priority\nsignificantly impact the effectiveness of caching policies, yet existing\nresearch fails to integrate all these attributes in policy design. In this\nwork, we model the caching problem using a Semi-Markov Decision Process (SMDP)\nto better capture the continuous-time nature of real-world applications,\nenabling caching decisions to be triggered by random file requests. We then\nintroduce a Proximal Policy Optimization (PPO)--based caching strategy that\nfully considers file attributes like lifetime, size, and priority. Simulations\nshow that our method outperforms a recent Deep Reinforcement Learning-based\ntechnique. To further advance our research, we improved the convergence rate of\nPPO by prioritizing transitions within the replay buffer through an attention\nmechanism. This mechanism evaluates the similarity between the current state\nand all stored transitions, assigning higher priorities to transitions that\nexhibit greater similarity.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper tackles the growing issue of excessive data transmission in\nnetworks. With increasing traffic, backhaul links and core networks are under\nsignificant traffic, leading to the investigation of caching solutions at edge\nrouters. Many existing studies utilize Markov Decision Processes (MDP) to\ntackle caching problems, often assuming decision points at fixed intervals;\nhowever, real-world environments are characterized by random request arrivals.\nAdditionally, critical file attributes such as lifetime, size, and priority\nsignificantly impact the effectiveness of caching policies, yet existing\nresearch fails to integrate all these attributes in policy design. In this\nwork, we model the caching problem using a Semi-Markov Decision Process (SMDP)\nto better capture the continuous-time nature of real-world applications,\nenabling caching decisions to be triggered by random file requests. We then\nintroduce a Proximal Policy Optimization (PPO)--based caching strategy that\nfully considers file attributes like lifetime, size, and priority. Simulations\nshow that our method outperforms a recent Deep Reinforcement Learning-based\ntechnique. To further advance our research, we improved the convergence rate of\nPPO by prioritizing transitions within the replay buffer through an attention\nmechanism. This mechanism evaluates the similarity between the current state\nand all stored transitions, assigning higher priorities to transitions that\nexhibit greater similarity."
                },
                "authors": [
                    {
                        "name": "Farnaz Niknia"
                    },
                    {
                        "name": "Ping Wang"
                    },
                    {
                        "name": "Zixu Wang"
                    },
                    {
                        "name": "Aakash Agarwal"
                    },
                    {
                        "name": "Adib S. Rezaei"
                    }
                ],
                "author_detail": {
                    "name": "Adib S. Rezaei"
                },
                "author": "Adib S. Rezaei",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2402.14576v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2402.14576v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.23079v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.23079v1",
                "updated": "2024-10-30T14:53:37Z",
                "updated_parsed": [
                    2024,
                    10,
                    30,
                    14,
                    53,
                    37,
                    2,
                    304,
                    0
                ],
                "published": "2024-10-30T14:53:37Z",
                "published_parsed": [
                    2024,
                    10,
                    30,
                    14,
                    53,
                    37,
                    2,
                    304,
                    0
                ],
                "title": "BUZZ: Beehive-structured Sparse KV Cache with Segmented Heavy Hitters\n  for Efficient LLM Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "BUZZ: Beehive-structured Sparse KV Cache with Segmented Heavy Hitters\n  for Efficient LLM Inference"
                },
                "summary": "Large language models (LLMs) are essential in natural language processing but\noften struggle with inference speed and computational efficiency, limiting\nreal-time deployment. The key-value (KV) cache mechanism reduces computational\noverhead in transformer models, but challenges in maintaining contextual\nunderstanding remain. In this paper, we propose BUZZ, a novel KV caching\nalgorithm that leverages structured contextual information to minimize cache\nmemory usage while enhancing inference speed. BUZZ employs a beehive-structured\nsparse cache, incorporating a sliding window to capture recent information and\ndynamically segmenting historical tokens into chunks to prioritize important\ntokens in local neighborhoods. We evaluate BUZZ on four real-world datasets:\nCNN/Daily Mail, XSUM, Wikitext, and 10-QA. Our results demonstrate that BUZZ\n(1) reduces cache memory usage by $\\textbf{2.5}\\times$ in LLM inference while\nmaintaining over 99% accuracy in long-text summarization, and (2) surpasses\nstate-of-the-art performance in multi-document question answering by\n$\\textbf{7.69%}$ under the same memory limit, where full cache methods\nencounter out-of-memory issues. Additionally, BUZZ achieves significant\ninference speedup with a $\\log{n}$ time complexity. The code is available at\nhttps://github.com/JunqiZhao888/buzz-llm.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) are essential in natural language processing but\noften struggle with inference speed and computational efficiency, limiting\nreal-time deployment. The key-value (KV) cache mechanism reduces computational\noverhead in transformer models, but challenges in maintaining contextual\nunderstanding remain. In this paper, we propose BUZZ, a novel KV caching\nalgorithm that leverages structured contextual information to minimize cache\nmemory usage while enhancing inference speed. BUZZ employs a beehive-structured\nsparse cache, incorporating a sliding window to capture recent information and\ndynamically segmenting historical tokens into chunks to prioritize important\ntokens in local neighborhoods. We evaluate BUZZ on four real-world datasets:\nCNN/Daily Mail, XSUM, Wikitext, and 10-QA. Our results demonstrate that BUZZ\n(1) reduces cache memory usage by $\\textbf{2.5}\\times$ in LLM inference while\nmaintaining over 99% accuracy in long-text summarization, and (2) surpasses\nstate-of-the-art performance in multi-document question answering by\n$\\textbf{7.69%}$ under the same memory limit, where full cache methods\nencounter out-of-memory issues. Additionally, BUZZ achieves significant\ninference speedup with a $\\log{n}$ time complexity. The code is available at\nhttps://github.com/JunqiZhao888/buzz-llm."
                },
                "authors": [
                    {
                        "name": "Junqi Zhao"
                    },
                    {
                        "name": "Zhijin Fang"
                    },
                    {
                        "name": "Shu Li"
                    },
                    {
                        "name": "Shaohui Yang"
                    },
                    {
                        "name": "Shichao He"
                    }
                ],
                "author_detail": {
                    "name": "Shichao He"
                },
                "author": "Shichao He",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.23079v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.23079v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.17808v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.17808v2",
                "updated": "2024-10-30T03:31:09Z",
                "updated_parsed": [
                    2024,
                    10,
                    30,
                    3,
                    31,
                    9,
                    2,
                    304,
                    0
                ],
                "published": "2024-06-24T03:59:17Z",
                "published_parsed": [
                    2024,
                    6,
                    24,
                    3,
                    59,
                    17,
                    0,
                    176,
                    0
                ],
                "title": "Training-Free Exponential Context Extension via Cascading KV Cache",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Training-Free Exponential Context Extension via Cascading KV Cache"
                },
                "summary": "The transformer's context window is vital for tasks such as few-shot learning\nand conditional generation as it preserves previous tokens for active memory.\nHowever, as the context lengths increase, the computational costs grow\nquadratically, hindering the deployment of large language models (LLMs) in\nreal-world, long sequence scenarios. Although some recent key-value caching (KV\nCache) methods offer linear inference complexity, they naively manage the\nstored context, prematurely evicting tokens and losing valuable information.\nMoreover, they lack an optimized prefill/prompt stage strategy, resulting in\nhigher latency than even quadratic attention for realistic context sizes. In\nresponse, we introduce a novel mechanism that leverages cascading sub-cache\nbuffers to selectively retain the most relevant tokens, enabling the model to\nmaintain longer context histories without increasing the cache size. Our\napproach outperforms linear caching baselines across key benchmarks, including\nstreaming perplexity, question answering, book summarization, and passkey\nretrieval, where it retains better retrieval accuracy at 1M tokens after four\ndoublings of the cache size of 65K. Additionally, our method reduces prefill\nstage latency by a factor of 6.8 when compared to flash attention on 1M tokens.\nThese innovations not only enhance the computational efficiency of LLMs but\nalso pave the way for their effective deployment in resource-constrained\nenvironments, enabling large-scale, real-time applications with significantly\nreduced latency.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The transformer's context window is vital for tasks such as few-shot learning\nand conditional generation as it preserves previous tokens for active memory.\nHowever, as the context lengths increase, the computational costs grow\nquadratically, hindering the deployment of large language models (LLMs) in\nreal-world, long sequence scenarios. Although some recent key-value caching (KV\nCache) methods offer linear inference complexity, they naively manage the\nstored context, prematurely evicting tokens and losing valuable information.\nMoreover, they lack an optimized prefill/prompt stage strategy, resulting in\nhigher latency than even quadratic attention for realistic context sizes. In\nresponse, we introduce a novel mechanism that leverages cascading sub-cache\nbuffers to selectively retain the most relevant tokens, enabling the model to\nmaintain longer context histories without increasing the cache size. Our\napproach outperforms linear caching baselines across key benchmarks, including\nstreaming perplexity, question answering, book summarization, and passkey\nretrieval, where it retains better retrieval accuracy at 1M tokens after four\ndoublings of the cache size of 65K. Additionally, our method reduces prefill\nstage latency by a factor of 6.8 when compared to flash attention on 1M tokens.\nThese innovations not only enhance the computational efficiency of LLMs but\nalso pave the way for their effective deployment in resource-constrained\nenvironments, enabling large-scale, real-time applications with significantly\nreduced latency."
                },
                "authors": [
                    {
                        "name": "Jeffrey Willette"
                    },
                    {
                        "name": "Heejun Lee"
                    },
                    {
                        "name": "Youngwan Lee"
                    },
                    {
                        "name": "Myeongjae Jeon"
                    },
                    {
                        "name": "Sung Ju Hwang"
                    }
                ],
                "author_detail": {
                    "name": "Sung Ju Hwang"
                },
                "author": "Sung Ju Hwang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.17808v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.17808v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.22649v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.22649v1",
                "updated": "2024-10-30T02:36:55Z",
                "updated_parsed": [
                    2024,
                    10,
                    30,
                    2,
                    36,
                    55,
                    2,
                    304,
                    0
                ],
                "published": "2024-10-30T02:36:55Z",
                "published_parsed": [
                    2024,
                    10,
                    30,
                    2,
                    36,
                    55,
                    2,
                    304,
                    0
                ],
                "title": "WaveRoRA: Wavelet Rotary Route Attention for Multivariate Time Series\n  Forecasting",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "WaveRoRA: Wavelet Rotary Route Attention for Multivariate Time Series\n  Forecasting"
                },
                "summary": "In recent years, Transformer-based models (Transformers) have achieved\nsignificant success in multivariate time series forecasting (MTSF). However,\nprevious works focus on extracting features either from the time domain or the\nfrequency domain, which inadequately captures the trends and periodic\ncharacteristics. To address this issue, we propose a wavelet learning framework\nto model complex temporal dependencies of the time series data. The wavelet\ndomain integrates both time and frequency information, allowing for the\nanalysis of local characteristics of signals at different scales. Additionally,\nthe Softmax self-attention mechanism used by Transformers has quadratic\ncomplexity, which leads to excessive computational costs when capturing\nlong-term dependencies. Therefore, we propose a novel attention mechanism:\nRotary Route Attention (RoRA). Unlike Softmax attention, RoRA utilizes rotary\nposition embeddings to inject relative positional information to sequence\ntokens and introduces a small number of routing tokens $r$ to aggregate\ninformation from the $KV$ matrices and redistribute it to the $Q$ matrix,\noffering linear complexity. We further propose WaveRoRA, which leverages RoRA\nto capture inter-series dependencies in the wavelet domain. We conduct\nextensive experiments on eight real-world datasets. The results indicate that\nWaveRoRA outperforms existing state-of-the-art models while maintaining lower\ncomputational costs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In recent years, Transformer-based models (Transformers) have achieved\nsignificant success in multivariate time series forecasting (MTSF). However,\nprevious works focus on extracting features either from the time domain or the\nfrequency domain, which inadequately captures the trends and periodic\ncharacteristics. To address this issue, we propose a wavelet learning framework\nto model complex temporal dependencies of the time series data. The wavelet\ndomain integrates both time and frequency information, allowing for the\nanalysis of local characteristics of signals at different scales. Additionally,\nthe Softmax self-attention mechanism used by Transformers has quadratic\ncomplexity, which leads to excessive computational costs when capturing\nlong-term dependencies. Therefore, we propose a novel attention mechanism:\nRotary Route Attention (RoRA). Unlike Softmax attention, RoRA utilizes rotary\nposition embeddings to inject relative positional information to sequence\ntokens and introduces a small number of routing tokens $r$ to aggregate\ninformation from the $KV$ matrices and redistribute it to the $Q$ matrix,\noffering linear complexity. We further propose WaveRoRA, which leverages RoRA\nto capture inter-series dependencies in the wavelet domain. We conduct\nextensive experiments on eight real-world datasets. The results indicate that\nWaveRoRA outperforms existing state-of-the-art models while maintaining lower\ncomputational costs."
                },
                "authors": [
                    {
                        "name": "Aobo Liang"
                    },
                    {
                        "name": "Yan Sun"
                    }
                ],
                "author_detail": {
                    "name": "Yan Sun"
                },
                "author": "Yan Sun",
                "arxiv_comment": "The code is coming soon! For sure",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.22649v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.22649v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2310.01801v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2310.01801v4",
                "updated": "2024-10-29T18:26:09Z",
                "updated_parsed": [
                    2024,
                    10,
                    29,
                    18,
                    26,
                    9,
                    1,
                    303,
                    0
                ],
                "published": "2023-10-03T05:17:08Z",
                "published_parsed": [
                    2023,
                    10,
                    3,
                    5,
                    17,
                    8,
                    1,
                    276,
                    0
                ],
                "title": "Model Tells You What to Discard: Adaptive KV Cache Compression for LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Model Tells You What to Discard: Adaptive KV Cache Compression for LLMs"
                },
                "summary": "In this study, we introduce adaptive KV cache compression, a plug-and-play\nmethod that reduces the memory footprint of generative inference for Large\nLanguage Models (LLMs). Different from the conventional KV cache that retains\nkey and value vectors for all context tokens, we conduct targeted profiling to\ndiscern the intrinsic structure of attention modules. Based on the recognized\nstructure, we then construct the KV cache in an adaptive manner: evicting\nlong-range contexts on attention heads emphasizing local contexts, discarding\nnon-special tokens on attention heads centered on special tokens, and only\nemploying the standard KV cache for attention heads that broadly attend to all\ntokens. Moreover, with the lightweight attention profiling used to guide the\nconstruction of the adaptive KV cache, FastGen can be deployed without\nresource-intensive fine-tuning or re-training. In our experiments across\nvarious asks, FastGen demonstrates substantial reduction on GPU memory\nconsumption with negligible generation quality loss. We will release our code\nand the compatible CUDA kernel for reproducibility.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this study, we introduce adaptive KV cache compression, a plug-and-play\nmethod that reduces the memory footprint of generative inference for Large\nLanguage Models (LLMs). Different from the conventional KV cache that retains\nkey and value vectors for all context tokens, we conduct targeted profiling to\ndiscern the intrinsic structure of attention modules. Based on the recognized\nstructure, we then construct the KV cache in an adaptive manner: evicting\nlong-range contexts on attention heads emphasizing local contexts, discarding\nnon-special tokens on attention heads centered on special tokens, and only\nemploying the standard KV cache for attention heads that broadly attend to all\ntokens. Moreover, with the lightweight attention profiling used to guide the\nconstruction of the adaptive KV cache, FastGen can be deployed without\nresource-intensive fine-tuning or re-training. In our experiments across\nvarious asks, FastGen demonstrates substantial reduction on GPU memory\nconsumption with negligible generation quality loss. We will release our code\nand the compatible CUDA kernel for reproducibility."
                },
                "authors": [
                    {
                        "name": "Suyu Ge"
                    },
                    {
                        "name": "Yunan Zhang"
                    },
                    {
                        "name": "Liyuan Liu"
                    },
                    {
                        "name": "Minjia Zhang"
                    },
                    {
                        "name": "Jiawei Han"
                    },
                    {
                        "name": "Jianfeng Gao"
                    }
                ],
                "author_detail": {
                    "name": "Jianfeng Gao"
                },
                "author": "Jianfeng Gao",
                "arxiv_comment": "ICLR 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2310.01801v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2310.01801v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.19274v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.19274v2",
                "updated": "2024-10-29T17:33:19Z",
                "updated_parsed": [
                    2024,
                    10,
                    29,
                    17,
                    33,
                    19,
                    1,
                    303,
                    0
                ],
                "published": "2024-10-25T03:01:19Z",
                "published_parsed": [
                    2024,
                    10,
                    25,
                    3,
                    1,
                    19,
                    4,
                    299,
                    0
                ],
                "title": "Ripple: Accelerating LLM Inference on Smartphones with Correlation-Aware\n  Neuron Management",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Ripple: Accelerating LLM Inference on Smartphones with Correlation-Aware\n  Neuron Management"
                },
                "summary": "Large Language Models (LLMs) have achieved remarkable success across various\ndomains, yet deploying them on mobile devices remains an arduous challenge due\nto their extensive computational and memory demands. While lightweight LLMs\nhave been developed to fit mobile environments, they suffer from degraded model\naccuracy. In contrast, sparsity-based techniques minimize DRAM usage by\nselectively transferring only relevant neurons to DRAM while retaining the full\nmodel in external storage, such as flash. However, such approaches are\ncritically limited by numerous I/O operations, particularly on smartphones with\nsevere IOPS constraints.\n  In this paper, we propose Ripple, a novel approach that accelerates LLM\ninference on smartphones by optimizing neuron placement in flash memory. Ripple\nleverages the concept of Neuron Co-Activation, where neurons frequently\nactivated together are linked to facilitate continuous read access and optimize\ndata transfer efficiency. Our approach incorporates a two-stage solution: an\noffline stage that reorganizes neuron placement based on co-activation\npatterns, and an online stage that employs tailored data access and caching\nstrategies to align well with hardware characteristics. Evaluations conducted\non a variety of smartphones and LLMs demonstrate that Ripple achieves up to\n5.93x improvements in I/O latency compared to the state-of-the-art. As the\nfirst solution to optimize storage placement under sparsity, Ripple explores a\nnew optimization space at the intersection of sparsity-driven algorithm and\nstorage-level system co-design in LLM inference.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have achieved remarkable success across various\ndomains, yet deploying them on mobile devices remains an arduous challenge due\nto their extensive computational and memory demands. While lightweight LLMs\nhave been developed to fit mobile environments, they suffer from degraded model\naccuracy. In contrast, sparsity-based techniques minimize DRAM usage by\nselectively transferring only relevant neurons to DRAM while retaining the full\nmodel in external storage, such as flash. However, such approaches are\ncritically limited by numerous I/O operations, particularly on smartphones with\nsevere IOPS constraints.\n  In this paper, we propose Ripple, a novel approach that accelerates LLM\ninference on smartphones by optimizing neuron placement in flash memory. Ripple\nleverages the concept of Neuron Co-Activation, where neurons frequently\nactivated together are linked to facilitate continuous read access and optimize\ndata transfer efficiency. Our approach incorporates a two-stage solution: an\noffline stage that reorganizes neuron placement based on co-activation\npatterns, and an online stage that employs tailored data access and caching\nstrategies to align well with hardware characteristics. Evaluations conducted\non a variety of smartphones and LLMs demonstrate that Ripple achieves up to\n5.93x improvements in I/O latency compared to the state-of-the-art. As the\nfirst solution to optimize storage placement under sparsity, Ripple explores a\nnew optimization space at the intersection of sparsity-driven algorithm and\nstorage-level system co-design in LLM inference."
                },
                "authors": [
                    {
                        "name": "Tuowei Wang"
                    },
                    {
                        "name": "Ruwen Fan"
                    },
                    {
                        "name": "Minxing Huang"
                    },
                    {
                        "name": "Zixu Hao"
                    },
                    {
                        "name": "Kun Li"
                    },
                    {
                        "name": "Ting Cao"
                    },
                    {
                        "name": "Youyou Lu"
                    },
                    {
                        "name": "Yaoxue Zhang"
                    },
                    {
                        "name": "Ju Ren"
                    }
                ],
                "author_detail": {
                    "name": "Ju Ren"
                },
                "author": "Ju Ren",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.19274v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.19274v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.OS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.21142v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.21142v2",
                "updated": "2024-10-29T16:55:23Z",
                "updated_parsed": [
                    2024,
                    10,
                    29,
                    16,
                    55,
                    23,
                    1,
                    303,
                    0
                ],
                "published": "2024-10-28T15:43:33Z",
                "published_parsed": [
                    2024,
                    10,
                    28,
                    15,
                    43,
                    33,
                    0,
                    302,
                    0
                ],
                "title": "Modeling and Monitoring of Indoor Populations using Sparse Positioning\n  Data (Extension)",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Modeling and Monitoring of Indoor Populations using Sparse Positioning\n  Data (Extension)"
                },
                "summary": "In large venues like shopping malls and airports, knowledge on the indoor\npopulations fuels applications such as business analytics, venue management,\nand safety control. In this work, we provide means of modeling populations in\npartitions of indoor space offline and of monitoring indoor populations\ncontinuously, by using indoor positioning data. However, the low-sampling rates\nof indoor positioning render the data temporally and spatially sparse, which in\nturn renders the offline capture of indoor populations challenging. It is even\nmore challenging to continuously monitor indoor populations, as positioning\ndata may be missing or not ready yet at the current moment. To address these\nchallenges, we first enable probabilistic modeling of populations in indoor\nspace partitions as Normal distributions. Based on that, we propose two\nlearning-based estimators for on-the-fly prediction of population\ndistributions. Leveraging the prediction-based schemes, we provide a unified\ncontinuous query processing framework for a type of query that enables\ncontinuous monitoring of populated partitions. The framework encompasses\ncaching and result validity mechanisms to reduce cost and maintain monitoring\neffectiveness. Extensive experiments on two real data sets show that the\nproposed estimators are able to outperform the state-of-the-art alternatives\nand that the query processing framework is effective and efficient.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In large venues like shopping malls and airports, knowledge on the indoor\npopulations fuels applications such as business analytics, venue management,\nand safety control. In this work, we provide means of modeling populations in\npartitions of indoor space offline and of monitoring indoor populations\ncontinuously, by using indoor positioning data. However, the low-sampling rates\nof indoor positioning render the data temporally and spatially sparse, which in\nturn renders the offline capture of indoor populations challenging. It is even\nmore challenging to continuously monitor indoor populations, as positioning\ndata may be missing or not ready yet at the current moment. To address these\nchallenges, we first enable probabilistic modeling of populations in indoor\nspace partitions as Normal distributions. Based on that, we propose two\nlearning-based estimators for on-the-fly prediction of population\ndistributions. Leveraging the prediction-based schemes, we provide a unified\ncontinuous query processing framework for a type of query that enables\ncontinuous monitoring of populated partitions. The framework encompasses\ncaching and result validity mechanisms to reduce cost and maintain monitoring\neffectiveness. Extensive experiments on two real data sets show that the\nproposed estimators are able to outperform the state-of-the-art alternatives\nand that the query processing framework is effective and efficient."
                },
                "authors": [
                    {
                        "name": "Xiao Li"
                    },
                    {
                        "name": "Huan Li"
                    },
                    {
                        "name": "Hua Lu"
                    },
                    {
                        "name": "Christian S. Jensen"
                    }
                ],
                "author_detail": {
                    "name": "Christian S. Jensen"
                },
                "author": "Christian S. Jensen",
                "arxiv_comment": "Accepted at TKDE",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.21142v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.21142v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.22134v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.22134v1",
                "updated": "2024-10-29T15:31:27Z",
                "updated_parsed": [
                    2024,
                    10,
                    29,
                    15,
                    31,
                    27,
                    1,
                    303,
                    0
                ],
                "published": "2024-10-29T15:31:27Z",
                "published_parsed": [
                    2024,
                    10,
                    29,
                    15,
                    31,
                    27,
                    1,
                    303,
                    0
                ],
                "title": "ProMoE: Fast MoE-based LLM Serving using Proactive Caching",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ProMoE: Fast MoE-based LLM Serving using Proactive Caching"
                },
                "summary": "The promising applications of large language models are often constrained by\nthe limited GPU memory capacity available on edge devices. Mixture-of-Experts\n(MoE) models help mitigate this issue by activating only a subset of the\nmodel's parameters during computation, allowing the unused parameters to be\noffloaded to host memory and reducing overall GPU memory demand. However,\nexisting cache-based offloading solutions handle cache misses reactively and\nsignificantly impact system performance. In this paper, we propose ProMoE, a\nnovel proactive caching system that leverages intermediate model results to\npredict subsequent parameter usage. By proactively fetching experts in advance,\nProMoE removes the loading time from the critical path and diminishes the\nperformance overhead of offloading. Our evaluations demonstrate that ProMoE\nachieves an average speedup of 2.13x and 2.84x in the prefill and decode stages\nrespectively, compared to existing offloading solutions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The promising applications of large language models are often constrained by\nthe limited GPU memory capacity available on edge devices. Mixture-of-Experts\n(MoE) models help mitigate this issue by activating only a subset of the\nmodel's parameters during computation, allowing the unused parameters to be\noffloaded to host memory and reducing overall GPU memory demand. However,\nexisting cache-based offloading solutions handle cache misses reactively and\nsignificantly impact system performance. In this paper, we propose ProMoE, a\nnovel proactive caching system that leverages intermediate model results to\npredict subsequent parameter usage. By proactively fetching experts in advance,\nProMoE removes the loading time from the critical path and diminishes the\nperformance overhead of offloading. Our evaluations demonstrate that ProMoE\nachieves an average speedup of 2.13x and 2.84x in the prefill and decode stages\nrespectively, compared to existing offloading solutions."
                },
                "authors": [
                    {
                        "name": "Xiaoniu Song"
                    },
                    {
                        "name": "Zihang Zhong"
                    },
                    {
                        "name": "Rong Chen"
                    }
                ],
                "author_detail": {
                    "name": "Rong Chen"
                },
                "author": "Rong Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.22134v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.22134v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.22118v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.22118v1",
                "updated": "2024-10-29T15:19:13Z",
                "updated_parsed": [
                    2024,
                    10,
                    29,
                    15,
                    19,
                    13,
                    1,
                    303,
                    0
                ],
                "published": "2024-10-29T15:19:13Z",
                "published_parsed": [
                    2024,
                    10,
                    29,
                    15,
                    19,
                    13,
                    1,
                    303,
                    0
                ],
                "title": "The Impact of Inference Acceleration Strategies on Bias of LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Impact of Inference Acceleration Strategies on Bias of LLMs"
                },
                "summary": "Last few years have seen unprecedented advances in capabilities of Large\nLanguage Models (LLMs). These advancements promise to deeply benefit a vast\narray of application domains. However, due to their immense size, performing\ninference with LLMs is both costly and slow. Consequently, a plethora of recent\nwork has proposed strategies to enhance inference efficiency, e.g.,\nquantization, pruning, and caching. These acceleration strategies reduce the\ninference cost and latency, often by several factors, while maintaining much of\nthe predictive performance measured via common benchmarks. In this work, we\nexplore another critical aspect of LLM performance: demographic bias in model\ngenerations due to inference acceleration optimizations. Using a wide range of\nmetrics, we probe bias in model outputs from a number of angles. Analysis of\noutputs before and after inference acceleration shows significant change in\nbias. Worryingly, these bias effects are complex and unpredictable. A\ncombination of an acceleration strategy and bias type may show little bias\nchange in one model but may lead to a large effect in another. Our results\nhighlight a need for in-depth and case-by-case evaluation of model bias after\nit has been modified to accelerate inference.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Last few years have seen unprecedented advances in capabilities of Large\nLanguage Models (LLMs). These advancements promise to deeply benefit a vast\narray of application domains. However, due to their immense size, performing\ninference with LLMs is both costly and slow. Consequently, a plethora of recent\nwork has proposed strategies to enhance inference efficiency, e.g.,\nquantization, pruning, and caching. These acceleration strategies reduce the\ninference cost and latency, often by several factors, while maintaining much of\nthe predictive performance measured via common benchmarks. In this work, we\nexplore another critical aspect of LLM performance: demographic bias in model\ngenerations due to inference acceleration optimizations. Using a wide range of\nmetrics, we probe bias in model outputs from a number of angles. Analysis of\noutputs before and after inference acceleration shows significant change in\nbias. Worryingly, these bias effects are complex and unpredictable. A\ncombination of an acceleration strategy and bias type may show little bias\nchange in one model but may lead to a large effect in another. Our results\nhighlight a need for in-depth and case-by-case evaluation of model bias after\nit has been modified to accelerate inference."
                },
                "authors": [
                    {
                        "name": "Elisabeth Kirsten"
                    },
                    {
                        "name": "Ivan Habernal"
                    },
                    {
                        "name": "Vedant Nanda"
                    },
                    {
                        "name": "Muhammad Bilal Zafar"
                    }
                ],
                "author_detail": {
                    "name": "Muhammad Bilal Zafar"
                },
                "author": "Muhammad Bilal Zafar",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.22118v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.22118v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.09526v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.09526v2",
                "updated": "2024-10-29T13:04:42Z",
                "updated_parsed": [
                    2024,
                    10,
                    29,
                    13,
                    4,
                    42,
                    1,
                    303,
                    0
                ],
                "published": "2024-04-15T07:45:04Z",
                "published_parsed": [
                    2024,
                    4,
                    15,
                    7,
                    45,
                    4,
                    0,
                    106,
                    0
                ],
                "title": "LoongServe: Efficiently Serving Long-Context Large Language Models with\n  Elastic Sequence Parallelism",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LoongServe: Efficiently Serving Long-Context Large Language Models with\n  Elastic Sequence Parallelism"
                },
                "summary": "The context window of large language models (LLMs) is rapidly increasing,\nleading to a huge variance in resource usage between different requests as well\nas between different phases of the same request. Restricted by static\nparallelism strategies, existing LLM serving systems cannot efficiently utilize\nthe underlying resources to serve variable-length requests in different phases.\nTo address this problem, we propose a new parallelism paradigm, elastic\nsequence parallelism (ESP), to elastically adapt to the variance between\ndifferent requests and phases. Based on ESP, we design and build LoongServe, an\nLLM serving system that (1) improves computation efficiency by elastically\nadjusting the degree of parallelism in real-time, (2) improves communication\nefficiency by reducing key-value cache migration overhead and overlapping\npartial decoding communication with computation, and (3) improves GPU memory\nefficiency by reducing key-value cache fragmentation across instances. Our\nevaluation under diverse real-world datasets shows that LoongServe improves the\nmaximum throughput by up to 3.85$\\times$ compared to the chunked prefill and\n5.81$\\times$ compared to the prefill-decoding disaggregation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The context window of large language models (LLMs) is rapidly increasing,\nleading to a huge variance in resource usage between different requests as well\nas between different phases of the same request. Restricted by static\nparallelism strategies, existing LLM serving systems cannot efficiently utilize\nthe underlying resources to serve variable-length requests in different phases.\nTo address this problem, we propose a new parallelism paradigm, elastic\nsequence parallelism (ESP), to elastically adapt to the variance between\ndifferent requests and phases. Based on ESP, we design and build LoongServe, an\nLLM serving system that (1) improves computation efficiency by elastically\nadjusting the degree of parallelism in real-time, (2) improves communication\nefficiency by reducing key-value cache migration overhead and overlapping\npartial decoding communication with computation, and (3) improves GPU memory\nefficiency by reducing key-value cache fragmentation across instances. Our\nevaluation under diverse real-world datasets shows that LoongServe improves the\nmaximum throughput by up to 3.85$\\times$ compared to the chunked prefill and\n5.81$\\times$ compared to the prefill-decoding disaggregation."
                },
                "authors": [
                    {
                        "name": "Bingyang Wu"
                    },
                    {
                        "name": "Shengyu Liu"
                    },
                    {
                        "name": "Yinmin Zhong"
                    },
                    {
                        "name": "Peng Sun"
                    },
                    {
                        "name": "Xuanzhe Liu"
                    },
                    {
                        "name": "Xin Jin"
                    }
                ],
                "author_detail": {
                    "name": "Xin Jin"
                },
                "author": "Xin Jin",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.09526v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.09526v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2312.05821v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2312.05821v4",
                "updated": "2024-10-29T12:28:58Z",
                "updated_parsed": [
                    2024,
                    10,
                    29,
                    12,
                    28,
                    58,
                    1,
                    303,
                    0
                ],
                "published": "2023-12-10T08:41:24Z",
                "published_parsed": [
                    2023,
                    12,
                    10,
                    8,
                    41,
                    24,
                    6,
                    344,
                    0
                ],
                "title": "ASVD: Activation-aware Singular Value Decomposition for Compressing\n  Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ASVD: Activation-aware Singular Value Decomposition for Compressing\n  Large Language Models"
                },
                "summary": "In this paper, we introduce a new post-training compression paradigm for\nLarge Language Models (LLMs) to facilitate their wider adoption. We delve into\nLLM weight low-rank decomposition, and find that the challenges of this task\nstem from the distribution variance in the LLM activations and the sensitivity\ndifference among various kinds of layers. To address these issues, we propose a\ntraining-free approach called Activation-aware Singular Value Decomposition\n(ASVD). Specifically, ASVD manages activation outliers by transforming the\nweight matrix based on the activation distribution. This transformation allows\nthe outliers in the activation matrix to be absorbed into the transformed\nweight matrix, thereby enhancing decomposition accuracy. Additionally, we\npropose an efficient iterative calibration process to optimize layer-specific\ndecomposition by addressing the varying sensitivity of different LLM layers. In\nthis way, ASVD can compress a network by 10%-30%. Based on the success of the\nlow-rank decomposition of projection matrices in the self-attention module, we\nfurther introduce ASVD to compress the KV cache. By reducing the channel\ndimension of KV activations, memory requirements for KV cache can be largely\nreduced. ASVD can further achieve 50% KV cache reductions without performance\ndrop in a training-free manner. Code is anonymously available in supplementary\nmaterials.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this paper, we introduce a new post-training compression paradigm for\nLarge Language Models (LLMs) to facilitate their wider adoption. We delve into\nLLM weight low-rank decomposition, and find that the challenges of this task\nstem from the distribution variance in the LLM activations and the sensitivity\ndifference among various kinds of layers. To address these issues, we propose a\ntraining-free approach called Activation-aware Singular Value Decomposition\n(ASVD). Specifically, ASVD manages activation outliers by transforming the\nweight matrix based on the activation distribution. This transformation allows\nthe outliers in the activation matrix to be absorbed into the transformed\nweight matrix, thereby enhancing decomposition accuracy. Additionally, we\npropose an efficient iterative calibration process to optimize layer-specific\ndecomposition by addressing the varying sensitivity of different LLM layers. In\nthis way, ASVD can compress a network by 10%-30%. Based on the success of the\nlow-rank decomposition of projection matrices in the self-attention module, we\nfurther introduce ASVD to compress the KV cache. By reducing the channel\ndimension of KV activations, memory requirements for KV cache can be largely\nreduced. ASVD can further achieve 50% KV cache reductions without performance\ndrop in a training-free manner. Code is anonymously available in supplementary\nmaterials."
                },
                "authors": [
                    {
                        "name": "Zhihang Yuan"
                    },
                    {
                        "name": "Yuzhang Shang"
                    },
                    {
                        "name": "Yue Song"
                    },
                    {
                        "name": "Qiang Wu"
                    },
                    {
                        "name": "Yan Yan"
                    },
                    {
                        "name": "Guangyu Sun"
                    }
                ],
                "author_detail": {
                    "name": "Guangyu Sun"
                },
                "author": "Guangyu Sun",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2312.05821v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2312.05821v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.18627v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.18627v2",
                "updated": "2024-10-29T12:03:14Z",
                "updated_parsed": [
                    2024,
                    10,
                    29,
                    12,
                    3,
                    14,
                    1,
                    303,
                    0
                ],
                "published": "2024-10-24T10:36:16Z",
                "published_parsed": [
                    2024,
                    10,
                    24,
                    10,
                    36,
                    16,
                    3,
                    298,
                    0
                ],
                "title": "Dynamic Content Caching with Waiting Costs via Restless Multi-Armed\n  Bandits",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Dynamic Content Caching with Waiting Costs via Restless Multi-Armed\n  Bandits"
                },
                "summary": "We consider a system with a local cache connected to a backend server and an\nend user population. A set of contents are stored at the the server where they\ncontinuously get updated. The local cache keeps copies, potentially stale, of a\nsubset of the contents. The users make content requests to the local cache\nwhich either can serve the local version if available or can fetch a fresh\nversion or can wait for additional requests before fetching and serving a fresh\nversion. Serving a stale version of a content incurs an age-of-version(AoV)\ndependent ageing cost, fetching it from the server incurs a fetching cost, and\nmaking a request wait incurs a per unit time waiting cost. We focus on the\noptimal actions subject to the cache capacity constraint at each decision\nepoch, aiming at minimizing the long term average cost. We pose the problem as\na Restless Multi-armed Bandit(RMAB) Problem and propose a Whittle index based\npolicy which is known to be asymptotically optimal. We explicitly characterize\nthe Whittle indices. We numerically evaluate the proposed policy and also\ncompare it to a greedy policy. We show that it is close to the optimal policy\nand substantially outperforms the greedy policy.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We consider a system with a local cache connected to a backend server and an\nend user population. A set of contents are stored at the the server where they\ncontinuously get updated. The local cache keeps copies, potentially stale, of a\nsubset of the contents. The users make content requests to the local cache\nwhich either can serve the local version if available or can fetch a fresh\nversion or can wait for additional requests before fetching and serving a fresh\nversion. Serving a stale version of a content incurs an age-of-version(AoV)\ndependent ageing cost, fetching it from the server incurs a fetching cost, and\nmaking a request wait incurs a per unit time waiting cost. We focus on the\noptimal actions subject to the cache capacity constraint at each decision\nepoch, aiming at minimizing the long term average cost. We pose the problem as\na Restless Multi-armed Bandit(RMAB) Problem and propose a Whittle index based\npolicy which is known to be asymptotically optimal. We explicitly characterize\nthe Whittle indices. We numerically evaluate the proposed policy and also\ncompare it to a greedy policy. We show that it is close to the optimal policy\nand substantially outperforms the greedy policy."
                },
                "authors": [
                    {
                        "name": "Ankita Koley"
                    },
                    {
                        "name": "Chandramani Singh"
                    }
                ],
                "author_detail": {
                    "name": "Chandramani Singh"
                },
                "author": "Chandramani Singh",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.18627v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.18627v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.00456v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.00456v2",
                "updated": "2024-10-29T11:09:12Z",
                "updated_parsed": [
                    2024,
                    10,
                    29,
                    11,
                    9,
                    12,
                    1,
                    303,
                    0
                ],
                "published": "2024-03-30T19:20:06Z",
                "published_parsed": [
                    2024,
                    3,
                    30,
                    19,
                    20,
                    6,
                    5,
                    90,
                    0
                ],
                "title": "QuaRot: Outlier-Free 4-Bit Inference in Rotated LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "QuaRot: Outlier-Free 4-Bit Inference in Rotated LLMs"
                },
                "summary": "We introduce QuaRot, a new Quantization scheme based on Rotations, which is\nable to quantize LLMs end-to-end, including all weights, activations, and KV\ncache in 4 bits. QuaRot rotates LLMs in a way that removes outliers from the\nhidden state without changing the output, making quantization easier. This\ncomputational invariance is applied to the hidden state (residual) of the LLM,\nas well as to the activations of the feed-forward components, aspects of the\nattention mechanism, and to the KV cache. The result is a quantized model where\nall matrix multiplications are performed in 4 bits, without any channels\nidentified for retention in higher precision. Our 4-bit quantized LLaMa2-70B\nmodel has losses of at most 0.47 WikiText-2 perplexity and retains 99% of the\nzero-shot performance. We also show that QuaRot can provide lossless 6 and 8\nbit LLaMa2 models without any calibration data using round-to-nearest\nquantization. Code is available at: https://github.com/spcl/QuaRot.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce QuaRot, a new Quantization scheme based on Rotations, which is\nable to quantize LLMs end-to-end, including all weights, activations, and KV\ncache in 4 bits. QuaRot rotates LLMs in a way that removes outliers from the\nhidden state without changing the output, making quantization easier. This\ncomputational invariance is applied to the hidden state (residual) of the LLM,\nas well as to the activations of the feed-forward components, aspects of the\nattention mechanism, and to the KV cache. The result is a quantized model where\nall matrix multiplications are performed in 4 bits, without any channels\nidentified for retention in higher precision. Our 4-bit quantized LLaMa2-70B\nmodel has losses of at most 0.47 WikiText-2 perplexity and retains 99% of the\nzero-shot performance. We also show that QuaRot can provide lossless 6 and 8\nbit LLaMa2 models without any calibration data using round-to-nearest\nquantization. Code is available at: https://github.com/spcl/QuaRot."
                },
                "authors": [
                    {
                        "name": "Saleh Ashkboos"
                    },
                    {
                        "name": "Amirkeivan Mohtashami"
                    },
                    {
                        "name": "Maximilian L. Croci"
                    },
                    {
                        "name": "Bo Li"
                    },
                    {
                        "name": "Pashmina Cameron"
                    },
                    {
                        "name": "Martin Jaggi"
                    },
                    {
                        "name": "Dan Alistarh"
                    },
                    {
                        "name": "Torsten Hoefler"
                    },
                    {
                        "name": "James Hensman"
                    }
                ],
                "author_detail": {
                    "name": "James Hensman"
                },
                "author": "James Hensman",
                "arxiv_comment": "21 pages, 7 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.00456v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.00456v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.11430v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.11430v3",
                "updated": "2024-10-29T10:52:35Z",
                "updated_parsed": [
                    2024,
                    10,
                    29,
                    10,
                    52,
                    35,
                    1,
                    303,
                    0
                ],
                "published": "2024-06-17T11:35:16Z",
                "published_parsed": [
                    2024,
                    6,
                    17,
                    11,
                    35,
                    16,
                    0,
                    169,
                    0
                ],
                "title": "A Simple and Effective $L_2$ Norm-Based Strategy for KV Cache\n  Compression",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Simple and Effective $L_2$ Norm-Based Strategy for KV Cache\n  Compression"
                },
                "summary": "The deployment of large language models (LLMs) is often hindered by the\nextensive memory requirements of the Key-Value (KV) cache, especially as\ncontext lengths increase. Existing approaches to reduce the KV cache size\ninvolve either fine-tuning the model to learn a compression strategy or\nleveraging attention scores to reduce the sequence length. We analyse the\nattention distributions in decoder-only Transformers-based models and observe\nthat attention allocation patterns stay consistent across most layers.\nSurprisingly, we find a clear correlation between the $L_2$ and the attention\nscores over cached KV pairs, where a low $L_2$ of a key embedding usually leads\nto a high attention score during decoding. This finding indicates that the\ninfluence of a KV pair is potentially determined by the key embedding itself\nbefore being queried. Based on this observation, we compress the KV cache based\non the $L_2$ of key embeddings. Our experimental results show that this simple\nstrategy can reduce the KV cache size by 50% on language modelling and\nneedle-in-a-haystack tasks and 90% on passkey retrieval tasks without losing\naccuracy. Moreover, without relying on the attention scores, this approach\nremains compatible with FlashAttention, enabling broader applicability.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The deployment of large language models (LLMs) is often hindered by the\nextensive memory requirements of the Key-Value (KV) cache, especially as\ncontext lengths increase. Existing approaches to reduce the KV cache size\ninvolve either fine-tuning the model to learn a compression strategy or\nleveraging attention scores to reduce the sequence length. We analyse the\nattention distributions in decoder-only Transformers-based models and observe\nthat attention allocation patterns stay consistent across most layers.\nSurprisingly, we find a clear correlation between the $L_2$ and the attention\nscores over cached KV pairs, where a low $L_2$ of a key embedding usually leads\nto a high attention score during decoding. This finding indicates that the\ninfluence of a KV pair is potentially determined by the key embedding itself\nbefore being queried. Based on this observation, we compress the KV cache based\non the $L_2$ of key embeddings. Our experimental results show that this simple\nstrategy can reduce the KV cache size by 50% on language modelling and\nneedle-in-a-haystack tasks and 90% on passkey retrieval tasks without losing\naccuracy. Moreover, without relying on the attention scores, this approach\nremains compatible with FlashAttention, enabling broader applicability."
                },
                "authors": [
                    {
                        "name": "Alessio Devoto"
                    },
                    {
                        "name": "Yu Zhao"
                    },
                    {
                        "name": "Simone Scardapane"
                    },
                    {
                        "name": "Pasquale Minervini"
                    }
                ],
                "author_detail": {
                    "name": "Pasquale Minervini"
                },
                "author": "Pasquale Minervini",
                "arxiv_comment": "This is an extended version of a paper published in the proceedings\n  of the 2024 Conference on Empirical Methods in Natural Language Processing\n  (EMNLP 2024); this version was presented at the 4th NeurIPS Workshop on\n  Efficient Natural Language and Speech Processing (ENLSP-IV)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.11430v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.11430v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.02369v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.02369v3",
                "updated": "2024-10-29T04:21:30Z",
                "updated_parsed": [
                    2024,
                    10,
                    29,
                    4,
                    21,
                    30,
                    1,
                    303,
                    0
                ],
                "published": "2024-10-03T10:33:49Z",
                "published_parsed": [
                    2024,
                    10,
                    3,
                    10,
                    33,
                    49,
                    3,
                    277,
                    0
                ],
                "title": "Unleashing the Potential of the Diffusion Model in Few-shot Semantic\n  Segmentation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Unleashing the Potential of the Diffusion Model in Few-shot Semantic\n  Segmentation"
                },
                "summary": "The Diffusion Model has not only garnered noteworthy achievements in the\nrealm of image generation but has also demonstrated its potential as an\neffective pretraining method utilizing unlabeled data. Drawing from the\nextensive potential unveiled by the Diffusion Model in both semantic\ncorrespondence and open vocabulary segmentation, our work initiates an\ninvestigation into employing the Latent Diffusion Model for Few-shot Semantic\nSegmentation. Recently, inspired by the in-context learning ability of large\nlanguage models, Few-shot Semantic Segmentation has evolved into In-context\nSegmentation tasks, morphing into a crucial element in assessing generalist\nsegmentation models. In this context, we concentrate on Few-shot Semantic\nSegmentation, establishing a solid foundation for the future development of a\nDiffusion-based generalist model for segmentation. Our initial focus lies in\nunderstanding how to facilitate interaction between the query image and the\nsupport image, resulting in the proposal of a KV fusion method within the\nself-attention framework. Subsequently, we delve deeper into optimizing the\ninfusion of information from the support mask and simultaneously re-evaluating\nhow to provide reasonable supervision from the query mask. Based on our\nanalysis, we establish a simple and effective framework named DiffewS,\nmaximally retaining the original Latent Diffusion Model's generative framework\nand effectively utilizing the pre-training prior. Experimental results\ndemonstrate that our method significantly outperforms the previous SOTA models\nin multiple settings.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Diffusion Model has not only garnered noteworthy achievements in the\nrealm of image generation but has also demonstrated its potential as an\neffective pretraining method utilizing unlabeled data. Drawing from the\nextensive potential unveiled by the Diffusion Model in both semantic\ncorrespondence and open vocabulary segmentation, our work initiates an\ninvestigation into employing the Latent Diffusion Model for Few-shot Semantic\nSegmentation. Recently, inspired by the in-context learning ability of large\nlanguage models, Few-shot Semantic Segmentation has evolved into In-context\nSegmentation tasks, morphing into a crucial element in assessing generalist\nsegmentation models. In this context, we concentrate on Few-shot Semantic\nSegmentation, establishing a solid foundation for the future development of a\nDiffusion-based generalist model for segmentation. Our initial focus lies in\nunderstanding how to facilitate interaction between the query image and the\nsupport image, resulting in the proposal of a KV fusion method within the\nself-attention framework. Subsequently, we delve deeper into optimizing the\ninfusion of information from the support mask and simultaneously re-evaluating\nhow to provide reasonable supervision from the query mask. Based on our\nanalysis, we establish a simple and effective framework named DiffewS,\nmaximally retaining the original Latent Diffusion Model's generative framework\nand effectively utilizing the pre-training prior. Experimental results\ndemonstrate that our method significantly outperforms the previous SOTA models\nin multiple settings."
                },
                "authors": [
                    {
                        "name": "Muzhi Zhu"
                    },
                    {
                        "name": "Yang Liu"
                    },
                    {
                        "name": "Zekai Luo"
                    },
                    {
                        "name": "Chenchen Jing"
                    },
                    {
                        "name": "Hao Chen"
                    },
                    {
                        "name": "Guangkai Xu"
                    },
                    {
                        "name": "Xinlong Wang"
                    },
                    {
                        "name": "Chunhua Shen"
                    }
                ],
                "author_detail": {
                    "name": "Chunhua Shen"
                },
                "author": "Chunhua Shen",
                "arxiv_comment": "Accepted to Proc. Annual Conference on Neural Information Processing\n  Systems (NeurIPS) 2024. Webpage: https://github.com/aim-uofa/DiffewS",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.02369v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.02369v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.19291v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.19291v3",
                "updated": "2024-10-29T02:52:24Z",
                "updated_parsed": [
                    2024,
                    10,
                    29,
                    2,
                    52,
                    24,
                    1,
                    303,
                    0
                ],
                "published": "2024-07-27T16:20:21Z",
                "published_parsed": [
                    2024,
                    7,
                    27,
                    16,
                    20,
                    21,
                    5,
                    209,
                    0
                ],
                "title": "Symmetric Locality: Definition and Initial Results",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Symmetric Locality: Definition and Initial Results"
                },
                "summary": "In this short paper, we characterize symmetric locality. In designing\nalgorithms, compilers, and systems, data movement is a common bottleneck in\nhigh-performance computation, in which we improve cache and memory performance.\nWe study a special type of data reuse in the form of repeated traversals, or\nre-traversals, which are based on the symmetric group. The cyclic and sawtooth\ntraces are previously known results in symmetric locality, and in this work, we\nwould like to generalize this result for any re-traversal. Then, we also\nprovide an abstract framework for applications in compiler design and machine\nlearning models to improve the memory performance of certain programs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this short paper, we characterize symmetric locality. In designing\nalgorithms, compilers, and systems, data movement is a common bottleneck in\nhigh-performance computation, in which we improve cache and memory performance.\nWe study a special type of data reuse in the form of repeated traversals, or\nre-traversals, which are based on the symmetric group. The cyclic and sawtooth\ntraces are previously known results in symmetric locality, and in this work, we\nwould like to generalize this result for any re-traversal. Then, we also\nprovide an abstract framework for applications in compiler design and machine\nlearning models to improve the memory performance of certain programs."
                },
                "authors": [
                    {
                        "name": "Giordan Escalona"
                    },
                    {
                        "name": "Dylan McKellips"
                    },
                    {
                        "name": "Chen Ding"
                    }
                ],
                "author_detail": {
                    "name": "Chen Ding"
                },
                "author": "Chen Ding",
                "arxiv_comment": "6 pages, 2nd ver",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.19291v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.19291v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.19258v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.19258v2",
                "updated": "2024-10-28T19:32:23Z",
                "updated_parsed": [
                    2024,
                    10,
                    28,
                    19,
                    32,
                    23,
                    0,
                    302,
                    0
                ],
                "published": "2024-10-25T02:22:00Z",
                "published_parsed": [
                    2024,
                    10,
                    25,
                    2,
                    22,
                    0,
                    4,
                    299,
                    0
                ],
                "title": "Not All Heads Matter: A Head-Level KV Cache Compression Method with\n  Integrated Retrieval and Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Not All Heads Matter: A Head-Level KV Cache Compression Method with\n  Integrated Retrieval and Reasoning"
                },
                "summary": "Key-Value (KV) caching is a common technique to enhance the computational\nefficiency of Large Language Models (LLMs), but its memory overhead grows\nrapidly with input length. Prior work has shown that not all tokens are equally\nimportant for text generation, proposing layer-level KV cache compression to\nselectively retain key information. Recognizing the distinct roles of attention\nheads in generation, we propose HeadKV, a head-level KV cache compression\nmethod, and HeadKV-R2, which leverages a novel contextual reasoning ability\nestimation for compression. Our approach operates at the level of individual\nheads, estimating their importance for contextual QA tasks that require both\nretrieval and reasoning capabilities. Extensive experiments across diverse\nbenchmarks (LongBench, LooGLE), model architectures (e.g., Llama-3-8B-Instruct,\nMistral-7B-Instruct), and long-context abilities tests demonstrate that our\nhead-level KV cache compression significantly outperforms strong baselines,\nparticularly in low-resource settings (KV size = 64 & 128). Notably, our method\nretains just 1.5% of the KV cache while achieving 97% of the performance of the\nfull KV cache on the contextual question answering benchmark.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Key-Value (KV) caching is a common technique to enhance the computational\nefficiency of Large Language Models (LLMs), but its memory overhead grows\nrapidly with input length. Prior work has shown that not all tokens are equally\nimportant for text generation, proposing layer-level KV cache compression to\nselectively retain key information. Recognizing the distinct roles of attention\nheads in generation, we propose HeadKV, a head-level KV cache compression\nmethod, and HeadKV-R2, which leverages a novel contextual reasoning ability\nestimation for compression. Our approach operates at the level of individual\nheads, estimating their importance for contextual QA tasks that require both\nretrieval and reasoning capabilities. Extensive experiments across diverse\nbenchmarks (LongBench, LooGLE), model architectures (e.g., Llama-3-8B-Instruct,\nMistral-7B-Instruct), and long-context abilities tests demonstrate that our\nhead-level KV cache compression significantly outperforms strong baselines,\nparticularly in low-resource settings (KV size = 64 & 128). Notably, our method\nretains just 1.5% of the KV cache while achieving 97% of the performance of the\nfull KV cache on the contextual question answering benchmark."
                },
                "authors": [
                    {
                        "name": "Yu Fu"
                    },
                    {
                        "name": "Zefan Cai"
                    },
                    {
                        "name": "Abedelkadir Asi"
                    },
                    {
                        "name": "Wayne Xiong"
                    },
                    {
                        "name": "Yue Dong"
                    },
                    {
                        "name": "Wen Xiao"
                    }
                ],
                "author_detail": {
                    "name": "Wen Xiao"
                },
                "author": "Wen Xiao",
                "arxiv_comment": "18pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.19258v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.19258v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.21465v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.21465v1",
                "updated": "2024-10-28T19:08:12Z",
                "updated_parsed": [
                    2024,
                    10,
                    28,
                    19,
                    8,
                    12,
                    0,
                    302,
                    0
                ],
                "published": "2024-10-28T19:08:12Z",
                "published_parsed": [
                    2024,
                    10,
                    28,
                    19,
                    8,
                    12,
                    0,
                    302,
                    0
                ],
                "title": "ShadowKV: KV Cache in Shadows for High-Throughput Long-Context LLM\n  Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ShadowKV: KV Cache in Shadows for High-Throughput Long-Context LLM\n  Inference"
                },
                "summary": "With the widespread deployment of long-context large language models (LLMs),\nthere has been a growing demand for efficient support of high-throughput\ninference. However, as the key-value (KV) cache expands with the sequence\nlength, the increasing memory footprint and the need to access it for each\ntoken generation both result in low throughput when serving long-context LLMs.\nWhile various dynamic sparse attention methods have been proposed to speed up\ninference while maintaining generation quality, they either fail to\nsufficiently reduce GPU memory consumption or introduce significant decoding\nlatency by offloading the KV cache to the CPU. We present ShadowKV, a\nhigh-throughput long-context LLM inference system that stores the low-rank key\ncache and offloads the value cache to reduce the memory footprint for larger\nbatch sizes and longer sequences. To minimize decoding latency, ShadowKV\nemploys an accurate KV selection strategy that reconstructs minimal sparse KV\npairs on-the-fly. By evaluating ShadowKV on a broad range of benchmarks,\nincluding RULER, LongBench, and Needle In A Haystack, and models like\nLlama-3.1-8B, Llama-3-8B-1M, GLM-4-9B-1M, Yi-9B-200K, Phi-3-Mini-128K, and\nQwen2-7B-128K, we demonstrate that it can support up to 6$\\times$ larger batch\nsizes and boost throughput by up to 3.04$\\times$ on an A100 GPU without\nsacrificing accuracy, even surpassing the performance achievable with infinite\nbatch size under the assumption of infinite GPU memory. The code is available\nat https://github.com/bytedance/ShadowKV.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the widespread deployment of long-context large language models (LLMs),\nthere has been a growing demand for efficient support of high-throughput\ninference. However, as the key-value (KV) cache expands with the sequence\nlength, the increasing memory footprint and the need to access it for each\ntoken generation both result in low throughput when serving long-context LLMs.\nWhile various dynamic sparse attention methods have been proposed to speed up\ninference while maintaining generation quality, they either fail to\nsufficiently reduce GPU memory consumption or introduce significant decoding\nlatency by offloading the KV cache to the CPU. We present ShadowKV, a\nhigh-throughput long-context LLM inference system that stores the low-rank key\ncache and offloads the value cache to reduce the memory footprint for larger\nbatch sizes and longer sequences. To minimize decoding latency, ShadowKV\nemploys an accurate KV selection strategy that reconstructs minimal sparse KV\npairs on-the-fly. By evaluating ShadowKV on a broad range of benchmarks,\nincluding RULER, LongBench, and Needle In A Haystack, and models like\nLlama-3.1-8B, Llama-3-8B-1M, GLM-4-9B-1M, Yi-9B-200K, Phi-3-Mini-128K, and\nQwen2-7B-128K, we demonstrate that it can support up to 6$\\times$ larger batch\nsizes and boost throughput by up to 3.04$\\times$ on an A100 GPU without\nsacrificing accuracy, even surpassing the performance achievable with infinite\nbatch size under the assumption of infinite GPU memory. The code is available\nat https://github.com/bytedance/ShadowKV."
                },
                "authors": [
                    {
                        "name": "Hanshi Sun"
                    },
                    {
                        "name": "Li-Wen Chang"
                    },
                    {
                        "name": "Wenlei Bao"
                    },
                    {
                        "name": "Size Zheng"
                    },
                    {
                        "name": "Ningxin Zheng"
                    },
                    {
                        "name": "Xin Liu"
                    },
                    {
                        "name": "Harry Dong"
                    },
                    {
                        "name": "Yuejie Chi"
                    },
                    {
                        "name": "Beidi Chen"
                    }
                ],
                "author_detail": {
                    "name": "Beidi Chen"
                },
                "author": "Beidi Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.21465v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.21465v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.21266v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.21266v1",
                "updated": "2024-10-28T17:57:40Z",
                "updated_parsed": [
                    2024,
                    10,
                    28,
                    17,
                    57,
                    40,
                    0,
                    302,
                    0
                ],
                "published": "2024-10-28T17:57:40Z",
                "published_parsed": [
                    2024,
                    10,
                    28,
                    17,
                    57,
                    40,
                    0,
                    302,
                    0
                ],
                "title": "Online Weighted Paging with Unknown Weights",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Online Weighted Paging with Unknown Weights"
                },
                "summary": "Online paging is a fundamental problem in the field of online algorithms, in\nwhich one maintains a cache of $k$ slots as requests for fetching pages arrive\nonline. In the weighted variant of this problem, each page has its own fetching\ncost; a substantial line of work on this problem culminated in an (optimal)\n$O(\\log k)$-competitive randomized algorithm, due to Bansal, Buchbinder and\nNaor (FOCS'07).\n  Existing work for weighted paging assumes that page weights are known in\nadvance, which is not always the case in practice. For example, in multi-level\ncaching architectures, the expected cost of fetching a memory block is a\nfunction of its probability of being in a mid-level cache rather than the main\nmemory. This complex property cannot be predicted in advance; over time,\nhowever, one may glean information about page weights through sampling their\nfetching cost multiple times.\n  We present the first algorithm for online weighted paging that does not know\npage weights in advance, but rather learns from weight samples. In terms of\ntechniques, this requires providing (integral) samples to a fractional solver,\nrequiring a delicate interface between this solver and the randomized rounding\nscheme; we believe that our work can inspire online algorithms to other\nproblems that involve cost sampling.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Online paging is a fundamental problem in the field of online algorithms, in\nwhich one maintains a cache of $k$ slots as requests for fetching pages arrive\nonline. In the weighted variant of this problem, each page has its own fetching\ncost; a substantial line of work on this problem culminated in an (optimal)\n$O(\\log k)$-competitive randomized algorithm, due to Bansal, Buchbinder and\nNaor (FOCS'07).\n  Existing work for weighted paging assumes that page weights are known in\nadvance, which is not always the case in practice. For example, in multi-level\ncaching architectures, the expected cost of fetching a memory block is a\nfunction of its probability of being in a mid-level cache rather than the main\nmemory. This complex property cannot be predicted in advance; over time,\nhowever, one may glean information about page weights through sampling their\nfetching cost multiple times.\n  We present the first algorithm for online weighted paging that does not know\npage weights in advance, but rather learns from weight samples. In terms of\ntechniques, this requires providing (integral) samples to a fractional solver,\nrequiring a delicate interface between this solver and the randomized rounding\nscheme; we believe that our work can inspire online algorithms to other\nproblems that involve cost sampling."
                },
                "authors": [
                    {
                        "name": "Orin Levy"
                    },
                    {
                        "name": "Noam Touitou"
                    },
                    {
                        "name": "Aviv Rosenberg"
                    }
                ],
                "author_detail": {
                    "name": "Aviv Rosenberg"
                },
                "author": "Aviv Rosenberg",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.21266v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.21266v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.08141v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.08141v2",
                "updated": "2024-10-28T16:42:11Z",
                "updated_parsed": [
                    2024,
                    10,
                    28,
                    16,
                    42,
                    11,
                    0,
                    302,
                    0
                ],
                "published": "2024-09-12T15:34:23Z",
                "published_parsed": [
                    2024,
                    9,
                    12,
                    15,
                    34,
                    23,
                    3,
                    256,
                    0
                ],
                "title": "Rethinking Programmed I/O for Fast Devices, Cheap Cores, and Coherent\n  Interconnects",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Rethinking Programmed I/O for Fast Devices, Cheap Cores, and Coherent\n  Interconnects"
                },
                "summary": "Conventional wisdom holds that an efficient interface between an OS running\non a CPU and a high-bandwidth I/O device should be based on Direct Memory\nAccess (DMA), descriptor rings, and interrupts: DMA offloads transfers from the\nCPU, descriptor rings provide buffering and queuing, and interrupts facilitate\nasynchronous interaction between cores and device with a lightweight\nnotification mechanism. In this paper we question this wisdom in the light of\nmodern hardware and workloads, particularly in cloud servers. Like others\nbefore us, we argue that the assumptions that led to this model are obsolete,\nand in many use-cases use of Programmed I/O (PIO), where the CPU explicitly\ntransfers data and control information to and from a device via loads and\nstores, actually results in a more efficient system. However, unlike others to\ndate, we push this idea further and show, in a real implementation, the gains\nin average and tail latency for fine-grained communication achievable using an\nopen cache-coherence protocol which exposes cache transitions to a smart\ndevice. We show this using three use-cases: fine-grained RPC-style invocation\nof functions on an accelerator, offloading of operators in a streaming dataflow\nengine, and a network interface targeting for serverless functions, comparing\nour use of coherence with both traditional DMA-style interaction and a\nhighly-optimized implementation using PIO over PCI Express (PCIe).",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Conventional wisdom holds that an efficient interface between an OS running\non a CPU and a high-bandwidth I/O device should be based on Direct Memory\nAccess (DMA), descriptor rings, and interrupts: DMA offloads transfers from the\nCPU, descriptor rings provide buffering and queuing, and interrupts facilitate\nasynchronous interaction between cores and device with a lightweight\nnotification mechanism. In this paper we question this wisdom in the light of\nmodern hardware and workloads, particularly in cloud servers. Like others\nbefore us, we argue that the assumptions that led to this model are obsolete,\nand in many use-cases use of Programmed I/O (PIO), where the CPU explicitly\ntransfers data and control information to and from a device via loads and\nstores, actually results in a more efficient system. However, unlike others to\ndate, we push this idea further and show, in a real implementation, the gains\nin average and tail latency for fine-grained communication achievable using an\nopen cache-coherence protocol which exposes cache transitions to a smart\ndevice. We show this using three use-cases: fine-grained RPC-style invocation\nof functions on an accelerator, offloading of operators in a streaming dataflow\nengine, and a network interface targeting for serverless functions, comparing\nour use of coherence with both traditional DMA-style interaction and a\nhighly-optimized implementation using PIO over PCI Express (PCIe)."
                },
                "authors": [
                    {
                        "name": "Anastasiia Ruzhanskaia"
                    },
                    {
                        "name": "Pengcheng Xu"
                    },
                    {
                        "name": "David Cock"
                    },
                    {
                        "name": "Timothy Roscoe"
                    }
                ],
                "author_detail": {
                    "name": "Timothy Roscoe"
                },
                "author": "Timothy Roscoe",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.08141v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.08141v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.OS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.16179v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.16179v2",
                "updated": "2024-10-28T14:44:22Z",
                "updated_parsed": [
                    2024,
                    10,
                    28,
                    14,
                    44,
                    22,
                    0,
                    302,
                    0
                ],
                "published": "2024-10-21T16:44:51Z",
                "published_parsed": [
                    2024,
                    10,
                    21,
                    16,
                    44,
                    51,
                    0,
                    295,
                    0
                ],
                "title": "MagicPIG: LSH Sampling for Efficient LLM Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MagicPIG: LSH Sampling for Efficient LLM Generation"
                },
                "summary": "Large language models (LLMs) with long context windows have gained\nsignificant attention. However, the KV cache, stored to avoid re-computation,\nbecomes a bottleneck. Various dynamic sparse or TopK-based attention\napproximation methods have been proposed to leverage the common insight that\nattention is sparse. In this paper, we first show that TopK attention itself\nsuffers from quality degradation in certain downstream tasks because attention\nis not always as sparse as expected. Rather than selecting the keys and values\nwith the highest attention scores, sampling with theoretical guarantees can\nprovide a better estimation for attention output. To make the sampling-based\napproximation practical in LLM generation, we propose MagicPIG, a heterogeneous\nsystem based on Locality Sensitive Hashing (LSH). MagicPIG significantly\nreduces the workload of attention computation while preserving high accuracy\nfor diverse tasks. MagicPIG stores the LSH hash tables and runs the attention\ncomputation on the CPU, which allows it to serve longer contexts and larger\nbatch sizes with high approximation accuracy. MagicPIG can improve decoding\nthroughput by $1.9\\sim3.9\\times$ across various GPU hardware and achieve 110ms\ndecoding latency on a single RTX 4090 for Llama-3.1-8B-Instruct model with a\ncontext of 96k tokens. The code is available at\n\\url{https://github.com/Infini-AI-Lab/MagicPIG}.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) with long context windows have gained\nsignificant attention. However, the KV cache, stored to avoid re-computation,\nbecomes a bottleneck. Various dynamic sparse or TopK-based attention\napproximation methods have been proposed to leverage the common insight that\nattention is sparse. In this paper, we first show that TopK attention itself\nsuffers from quality degradation in certain downstream tasks because attention\nis not always as sparse as expected. Rather than selecting the keys and values\nwith the highest attention scores, sampling with theoretical guarantees can\nprovide a better estimation for attention output. To make the sampling-based\napproximation practical in LLM generation, we propose MagicPIG, a heterogeneous\nsystem based on Locality Sensitive Hashing (LSH). MagicPIG significantly\nreduces the workload of attention computation while preserving high accuracy\nfor diverse tasks. MagicPIG stores the LSH hash tables and runs the attention\ncomputation on the CPU, which allows it to serve longer contexts and larger\nbatch sizes with high approximation accuracy. MagicPIG can improve decoding\nthroughput by $1.9\\sim3.9\\times$ across various GPU hardware and achieve 110ms\ndecoding latency on a single RTX 4090 for Llama-3.1-8B-Instruct model with a\ncontext of 96k tokens. The code is available at\n\\url{https://github.com/Infini-AI-Lab/MagicPIG}."
                },
                "authors": [
                    {
                        "name": "Zhuoming Chen"
                    },
                    {
                        "name": "Ranajoy Sadhukhan"
                    },
                    {
                        "name": "Zihao Ye"
                    },
                    {
                        "name": "Yang Zhou"
                    },
                    {
                        "name": "Jianyu Zhang"
                    },
                    {
                        "name": "Niklas Nolte"
                    },
                    {
                        "name": "Yuandong Tian"
                    },
                    {
                        "name": "Matthijs Douze"
                    },
                    {
                        "name": "Leon Bottou"
                    },
                    {
                        "name": "Zhihao Jia"
                    },
                    {
                        "name": "Beidi Chen"
                    }
                ],
                "author_detail": {
                    "name": "Beidi Chen"
                },
                "author": "Beidi Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.16179v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.16179v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.21073v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.21073v1",
                "updated": "2024-10-28T14:35:12Z",
                "updated_parsed": [
                    2024,
                    10,
                    28,
                    14,
                    35,
                    12,
                    0,
                    302,
                    0
                ],
                "published": "2024-10-28T14:35:12Z",
                "published_parsed": [
                    2024,
                    10,
                    28,
                    14,
                    35,
                    12,
                    0,
                    302,
                    0
                ],
                "title": "Skip2-LoRA: A Lightweight On-device DNN Fine-tuning Method for Low-cost\n  Edge Devices",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Skip2-LoRA: A Lightweight On-device DNN Fine-tuning Method for Low-cost\n  Edge Devices"
                },
                "summary": "This paper proposes Skip2-LoRA as a lightweight fine-tuning method for deep\nneural networks to address the gap between pre-trained and deployed models. In\nour approach, trainable LoRA (low-rank adaptation) adapters are inserted\nbetween the last layer and every other layer to enhance the network expressive\npower while keeping the backward computation cost low. This architecture is\nwell-suited to cache intermediate computation results of the forward pass and\nthen can skip the forward computation of seen samples as training epochs\nprogress. We implemented the combination of the proposed architecture and\ncache, denoted as Skip2-LoRA, and tested it on a $15 single board computer. Our\nresults show that Skip2-LoRA reduces the fine-tuning time by 90.0% on average\ncompared to the counterpart that has the same number of trainable parameters\nwhile preserving the accuracy, while taking only a few seconds on the\nmicrocontroller board.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper proposes Skip2-LoRA as a lightweight fine-tuning method for deep\nneural networks to address the gap between pre-trained and deployed models. In\nour approach, trainable LoRA (low-rank adaptation) adapters are inserted\nbetween the last layer and every other layer to enhance the network expressive\npower while keeping the backward computation cost low. This architecture is\nwell-suited to cache intermediate computation results of the forward pass and\nthen can skip the forward computation of seen samples as training epochs\nprogress. We implemented the combination of the proposed architecture and\ncache, denoted as Skip2-LoRA, and tested it on a $15 single board computer. Our\nresults show that Skip2-LoRA reduces the fine-tuning time by 90.0% on average\ncompared to the counterpart that has the same number of trainable parameters\nwhile preserving the accuracy, while taking only a few seconds on the\nmicrocontroller board."
                },
                "authors": [
                    {
                        "name": "Hiroki Matsutani"
                    },
                    {
                        "name": "Masaaki Kondo"
                    },
                    {
                        "name": "Kazuki Sunaga"
                    },
                    {
                        "name": "Radu Marculescu"
                    }
                ],
                "author_detail": {
                    "name": "Radu Marculescu"
                },
                "author": "Radu Marculescu",
                "arxiv_comment": "ASP-DAC 2025 (accepted)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.21073v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.21073v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.21035v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.21035v1",
                "updated": "2024-10-28T13:56:30Z",
                "updated_parsed": [
                    2024,
                    10,
                    28,
                    13,
                    56,
                    30,
                    0,
                    302,
                    0
                ],
                "published": "2024-10-28T13:56:30Z",
                "published_parsed": [
                    2024,
                    10,
                    28,
                    13,
                    56,
                    30,
                    0,
                    302,
                    0
                ],
                "title": "Beyond Autoregression: Fast LLMs via Self-Distillation Through Time",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Beyond Autoregression: Fast LLMs via Self-Distillation Through Time"
                },
                "summary": "Autoregressive (AR) Large Language Models (LLMs) have demonstrated\nsignificant success across numerous tasks. However, the AR modeling paradigm\npresents certain limitations; for instance, contemporary autoregressive LLMs\nare trained to generate one token at a time, which can result in noticeable\nlatency. Recent advances have indicated that search and repeated sampling can\nenhance performance in various applications, such as theorem proving, code\ngeneration, and alignment, by utilizing greater computational resources during\ninference. In this study, we demonstrate that diffusion language models are\ncapable of generating at least 32 tokens simultaneously, while exceeding the\nperformance of AR models in text quality and on the LAMBADA natural language\nunderstanding benchmark. This outcome is achieved through a novel distillation\nmethod for discrete diffusion models, which reduces the number of inference\nsteps by a factor of 32-64. Practically, our models, even without caching, can\ngenerate tokens at a rate that is up to 8 times faster than AR models employing\nKV caching, and we anticipate further improvements with the inclusion of\ncaching. Moreover, we demonstrate the efficacy of our approach for diffusion\nlanguage models with up to 860M parameters.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Autoregressive (AR) Large Language Models (LLMs) have demonstrated\nsignificant success across numerous tasks. However, the AR modeling paradigm\npresents certain limitations; for instance, contemporary autoregressive LLMs\nare trained to generate one token at a time, which can result in noticeable\nlatency. Recent advances have indicated that search and repeated sampling can\nenhance performance in various applications, such as theorem proving, code\ngeneration, and alignment, by utilizing greater computational resources during\ninference. In this study, we demonstrate that diffusion language models are\ncapable of generating at least 32 tokens simultaneously, while exceeding the\nperformance of AR models in text quality and on the LAMBADA natural language\nunderstanding benchmark. This outcome is achieved through a novel distillation\nmethod for discrete diffusion models, which reduces the number of inference\nsteps by a factor of 32-64. Practically, our models, even without caching, can\ngenerate tokens at a rate that is up to 8 times faster than AR models employing\nKV caching, and we anticipate further improvements with the inclusion of\ncaching. Moreover, we demonstrate the efficacy of our approach for diffusion\nlanguage models with up to 860M parameters."
                },
                "authors": [
                    {
                        "name": "Justin Deschenaux"
                    },
                    {
                        "name": "Caglar Gulcehre"
                    }
                ],
                "author_detail": {
                    "name": "Caglar Gulcehre"
                },
                "author": "Caglar Gulcehre",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.21035v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.21035v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.20790v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.20790v1",
                "updated": "2024-10-28T07:13:25Z",
                "updated_parsed": [
                    2024,
                    10,
                    28,
                    7,
                    13,
                    25,
                    0,
                    302,
                    0
                ],
                "published": "2024-10-28T07:13:25Z",
                "published_parsed": [
                    2024,
                    10,
                    28,
                    7,
                    13,
                    25,
                    0,
                    302,
                    0
                ],
                "title": "SparseTem: Boosting the Efficiency of CNN-Based Video Encoders by\n  Exploiting Temporal Continuity",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SparseTem: Boosting the Efficiency of CNN-Based Video Encoders by\n  Exploiting Temporal Continuity"
                },
                "summary": "Deep learning models have become pivotal in the field of video processing and\nis increasingly critical in practical applications such as autonomous driving\nand object detection. Although Vision Transformers (ViTs) have demonstrated\ntheir power, Convolutional Neural Networks (CNNs) remain a highly efficient and\nhigh-performance choice for feature extraction and encoding. However, the\nintensive computational demands of convolution operations hinder its broader\nadoption as a video encoder. Given the inherent temporal continuity in video\nframes, changes between consecutive frames are minimal, allowing for the\nskipping of redundant computations. This technique, which we term as Diff\nComputation, presents two primary challenges. First, Diff Computation requires\nto cache intermediate feature maps to ensure the correctness of non-linear\ncomputations, leading to significant memory consumption. Second, the imbalance\nof sparsity among layers, introduced by Diff Computation, incurs accuracy\ndegradation. To address these issues, we propose a memory-efficient scheduling\nmethod to eliminate memory overhead and an online adjustment mechanism to\nminimize accuracy degradation. We integrate these techniques into our\nframework, SparseTem, to seamlessly support various CNN-based video encoders.\nSparseTem achieves speedup of 1.79x for EfficientDet and 4.72x for CRNN, with\nminimal accuracy drop and no additional memory overhead. Extensive experimental\nresults demonstrate that SparseTem sets a new state-of-the-art by effectively\nutilizing temporal continuity to accelerate CNN-based video encoders.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Deep learning models have become pivotal in the field of video processing and\nis increasingly critical in practical applications such as autonomous driving\nand object detection. Although Vision Transformers (ViTs) have demonstrated\ntheir power, Convolutional Neural Networks (CNNs) remain a highly efficient and\nhigh-performance choice for feature extraction and encoding. However, the\nintensive computational demands of convolution operations hinder its broader\nadoption as a video encoder. Given the inherent temporal continuity in video\nframes, changes between consecutive frames are minimal, allowing for the\nskipping of redundant computations. This technique, which we term as Diff\nComputation, presents two primary challenges. First, Diff Computation requires\nto cache intermediate feature maps to ensure the correctness of non-linear\ncomputations, leading to significant memory consumption. Second, the imbalance\nof sparsity among layers, introduced by Diff Computation, incurs accuracy\ndegradation. To address these issues, we propose a memory-efficient scheduling\nmethod to eliminate memory overhead and an online adjustment mechanism to\nminimize accuracy degradation. We integrate these techniques into our\nframework, SparseTem, to seamlessly support various CNN-based video encoders.\nSparseTem achieves speedup of 1.79x for EfficientDet and 4.72x for CRNN, with\nminimal accuracy drop and no additional memory overhead. Extensive experimental\nresults demonstrate that SparseTem sets a new state-of-the-art by effectively\nutilizing temporal continuity to accelerate CNN-based video encoders."
                },
                "authors": [
                    {
                        "name": "Kunyun Wang"
                    },
                    {
                        "name": "Jieru Zhao"
                    },
                    {
                        "name": "Shuo Yang"
                    },
                    {
                        "name": "Wenchao Ding"
                    },
                    {
                        "name": "Minyi Guo"
                    }
                ],
                "author_detail": {
                    "name": "Minyi Guo"
                },
                "author": "Minyi Guo",
                "arxiv_comment": "9 pages, 13 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.20790v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.20790v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.01847v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.01847v3",
                "updated": "2024-10-27T14:40:08Z",
                "updated_parsed": [
                    2024,
                    10,
                    27,
                    14,
                    40,
                    8,
                    6,
                    301,
                    0
                ],
                "published": "2024-04-02T11:12:42Z",
                "published_parsed": [
                    2024,
                    4,
                    2,
                    11,
                    12,
                    42,
                    1,
                    93,
                    0
                ],
                "title": "Accelerating Transformer Pre-training with 2:4 Sparsity",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Accelerating Transformer Pre-training with 2:4 Sparsity"
                },
                "summary": "Training large transformers is slow, but recent innovations on GPU\narchitecture give us an advantage. NVIDIA Ampere GPUs can execute a\nfine-grained 2:4 sparse matrix multiplication twice as fast as its dense\nequivalent. In the light of this property, we comprehensively investigate the\nfeasibility of accelerating feed-forward networks (FFNs) of transformers in\npre-training. First, we define a ``flip rate'' to monitor the stability of a\n2:4 training process. Utilizing this metric, we propose three techniques to\npreserve accuracy: to modify the sparse-refined straight-through estimator by\napplying the masked decay term on gradients, to determine a feasible decay\nfactor in warm-up stage, and to enhance the model's quality by a dense\nfine-tuning procedure near the end of pre-training. Besides, we devise two\ntechniques to practically accelerate training: to calculate transposable 2:4\nmasks by convolution, and to accelerate gated activation functions by reducing\nGPU L2 cache miss. Experiments show that our 2:4 sparse training algorithm\nachieves similar convergence to dense training algorithms on several\ntransformer pre-training tasks, while actual acceleration can be observed on\ndifferent shapes of transformer block apparently. Our toolkit is available at\nhttps://github.com/huyz2023/2by4-pretrain.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Training large transformers is slow, but recent innovations on GPU\narchitecture give us an advantage. NVIDIA Ampere GPUs can execute a\nfine-grained 2:4 sparse matrix multiplication twice as fast as its dense\nequivalent. In the light of this property, we comprehensively investigate the\nfeasibility of accelerating feed-forward networks (FFNs) of transformers in\npre-training. First, we define a ``flip rate'' to monitor the stability of a\n2:4 training process. Utilizing this metric, we propose three techniques to\npreserve accuracy: to modify the sparse-refined straight-through estimator by\napplying the masked decay term on gradients, to determine a feasible decay\nfactor in warm-up stage, and to enhance the model's quality by a dense\nfine-tuning procedure near the end of pre-training. Besides, we devise two\ntechniques to practically accelerate training: to calculate transposable 2:4\nmasks by convolution, and to accelerate gated activation functions by reducing\nGPU L2 cache miss. Experiments show that our 2:4 sparse training algorithm\nachieves similar convergence to dense training algorithms on several\ntransformer pre-training tasks, while actual acceleration can be observed on\ndifferent shapes of transformer block apparently. Our toolkit is available at\nhttps://github.com/huyz2023/2by4-pretrain."
                },
                "authors": [
                    {
                        "name": "Yuezhou Hu"
                    },
                    {
                        "name": "Kang Zhao"
                    },
                    {
                        "name": "Weiyu Huang"
                    },
                    {
                        "name": "Jianfei Chen"
                    },
                    {
                        "name": "Jun Zhu"
                    }
                ],
                "author_detail": {
                    "name": "Jun Zhu"
                },
                "author": "Jun Zhu",
                "arxiv_journal_ref": "Proceedings of the 41st International Conference on Machine\n  Learning (2024), in Proceedings of Machine Learning Research 235:19531-19543",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.01847v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.01847v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.20337v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.20337v1",
                "updated": "2024-10-27T04:31:35Z",
                "updated_parsed": [
                    2024,
                    10,
                    27,
                    4,
                    31,
                    35,
                    6,
                    301,
                    0
                ],
                "published": "2024-10-27T04:31:35Z",
                "published_parsed": [
                    2024,
                    10,
                    27,
                    4,
                    31,
                    35,
                    6,
                    301,
                    0
                ],
                "title": "On the I/O Complexity of the CYK Algorithm and of a Family of Related DP\n  Algorithms",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "On the I/O Complexity of the CYK Algorithm and of a Family of Related DP\n  Algorithms"
                },
                "summary": "Asymptotically tight lower bounds are derived for the Input/Output (I/O)\ncomplexity of a class of dynamic programming algorithms including matrix chain\nmultiplication, optimal polygon triangulation, and the construction of optimal\nbinary search trees. Assuming no recomputation of intermediate values, we\nestablish an $\\Omega\\left(\\frac{n^3}{\\sqrt{M}B}\\right)$ I/O lower bound, where\n$n$ denotes the size of the input and $M$ denotes the size of the available\nfast memory (cache). When recomputation is allowed, we show the same bound\nholds for $M < cn$, where $c$ is a positive constant. In the case where $M \\ge\n2n$, we show an $\\Omega\\left(n/B\\right)$ I/O lower bound. We also discuss\nalgorithms for which the number of executed I/O operations matches\nasymptotically each of the presented lower bounds, which are thus\nasymptotically tight.\n  Additionally, we refine our general method to obtain a lower bound for the\nI/O complexity of the Cocke-Younger-Kasami algorithm, where the size of the\ngrammar impacts the I/O complexity. An upper bound with asymptotically matching\nperformance in many cases is also provided.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Asymptotically tight lower bounds are derived for the Input/Output (I/O)\ncomplexity of a class of dynamic programming algorithms including matrix chain\nmultiplication, optimal polygon triangulation, and the construction of optimal\nbinary search trees. Assuming no recomputation of intermediate values, we\nestablish an $\\Omega\\left(\\frac{n^3}{\\sqrt{M}B}\\right)$ I/O lower bound, where\n$n$ denotes the size of the input and $M$ denotes the size of the available\nfast memory (cache). When recomputation is allowed, we show the same bound\nholds for $M < cn$, where $c$ is a positive constant. In the case where $M \\ge\n2n$, we show an $\\Omega\\left(n/B\\right)$ I/O lower bound. We also discuss\nalgorithms for which the number of executed I/O operations matches\nasymptotically each of the presented lower bounds, which are thus\nasymptotically tight.\n  Additionally, we refine our general method to obtain a lower bound for the\nI/O complexity of the Cocke-Younger-Kasami algorithm, where the size of the\ngrammar impacts the I/O complexity. An upper bound with asymptotically matching\nperformance in many cases is also provided."
                },
                "authors": [
                    {
                        "name": "Lorenzo De Stefani"
                    },
                    {
                        "name": "Vedant Gupta"
                    }
                ],
                "author_detail": {
                    "name": "Vedant Gupta"
                },
                "author": "Vedant Gupta",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.20337v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.20337v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "F.2.0",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2402.04216v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2402.04216v3",
                "updated": "2024-10-26T22:19:04Z",
                "updated_parsed": [
                    2024,
                    10,
                    26,
                    22,
                    19,
                    4,
                    5,
                    300,
                    0
                ],
                "published": "2024-02-06T18:17:02Z",
                "published_parsed": [
                    2024,
                    2,
                    6,
                    18,
                    17,
                    2,
                    1,
                    37,
                    0
                ],
                "title": "Resource-Aware Hierarchical Federated Learning in Wireless Video Caching\n  Networks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Resource-Aware Hierarchical Federated Learning in Wireless Video Caching\n  Networks"
                },
                "summary": "Backhaul traffic congestion caused by the video traffic of a few popular\nfiles can be alleviated by storing the to-be-requested content at various\nlevels in wireless video caching networks. Typically, content service providers\n(CSPs) own the content, and the users request their preferred content from the\nCSPs using their (wireless) internet service providers (ISPs). As these parties\ndo not reveal their private information and business secrets, traditional\ntechniques may not be readily used to predict the dynamic changes in users'\nfuture demands. Motivated by this, we propose a novel resource-aware\nhierarchical federated learning (RawHFL) solution for predicting user's future\ncontent requests. A practical data acquisition technique is used that allows\nthe user to update its local training dataset based on its requested content.\nBesides, since networking and other computational resources are limited,\nconsidering that only a subset of the users participate in the model training,\nwe derive the convergence bound of the proposed algorithm. Based on this bound,\nwe minimize a weighted utility function for jointly configuring the\ncontrollable parameters to train the RawHFL energy efficiently under practical\nresource constraints. Our extensive simulation results validate the proposed\nalgorithm's superiority, in terms of test accuracy and energy cost, over\nexisting baselines.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Backhaul traffic congestion caused by the video traffic of a few popular\nfiles can be alleviated by storing the to-be-requested content at various\nlevels in wireless video caching networks. Typically, content service providers\n(CSPs) own the content, and the users request their preferred content from the\nCSPs using their (wireless) internet service providers (ISPs). As these parties\ndo not reveal their private information and business secrets, traditional\ntechniques may not be readily used to predict the dynamic changes in users'\nfuture demands. Motivated by this, we propose a novel resource-aware\nhierarchical federated learning (RawHFL) solution for predicting user's future\ncontent requests. A practical data acquisition technique is used that allows\nthe user to update its local training dataset based on its requested content.\nBesides, since networking and other computational resources are limited,\nconsidering that only a subset of the users participate in the model training,\nwe derive the convergence bound of the proposed algorithm. Based on this bound,\nwe minimize a weighted utility function for jointly configuring the\ncontrollable parameters to train the RawHFL energy efficiently under practical\nresource constraints. Our extensive simulation results validate the proposed\nalgorithm's superiority, in terms of test accuracy and energy cost, over\nexisting baselines."
                },
                "authors": [
                    {
                        "name": "Md Ferdous Pervej"
                    },
                    {
                        "name": "Andreas F. Molisch"
                    }
                ],
                "author_detail": {
                    "name": "Andreas F. Molisch"
                },
                "author": "Andreas F. Molisch",
                "arxiv_comment": "Under review for possible publication in IEEE Transactions on\n  Wireless Communications",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2402.04216v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2402.04216v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.20149v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.20149v1",
                "updated": "2024-10-26T11:20:02Z",
                "updated_parsed": [
                    2024,
                    10,
                    26,
                    11,
                    20,
                    2,
                    5,
                    300,
                    0
                ],
                "published": "2024-10-26T11:20:02Z",
                "published_parsed": [
                    2024,
                    10,
                    26,
                    11,
                    20,
                    2,
                    5,
                    300,
                    0
                ],
                "title": "AdaNeg: Adaptive Negative Proxy Guided OOD Detection with\n  Vision-Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AdaNeg: Adaptive Negative Proxy Guided OOD Detection with\n  Vision-Language Models"
                },
                "summary": "Recent research has shown that pre-trained vision-language models are\neffective at identifying out-of-distribution (OOD) samples by using negative\nlabels as guidance. However, employing consistent negative labels across\ndifferent OOD datasets often results in semantic misalignments, as these text\nlabels may not accurately reflect the actual space of OOD images. To overcome\nthis issue, we introduce \\textit{adaptive negative proxies}, which are\ndynamically generated during testing by exploring actual OOD images, to align\nmore closely with the underlying OOD label space and enhance the efficacy of\nnegative proxy guidance. Specifically, our approach utilizes a feature memory\nbank to selectively cache discriminative features from test images,\nrepresenting the targeted OOD distribution. This facilitates the creation of\nproxies that can better align with specific OOD datasets. While task-adaptive\nproxies average features to reflect the unique characteristics of each dataset,\nthe sample-adaptive proxies weight features based on their similarity to\nindividual test samples, exploring detailed sample-level nuances. The final\nscore for identifying OOD samples integrates static negative labels with our\nproposed adaptive proxies, effectively combining textual and visual knowledge\nfor enhanced performance. Our method is training-free and annotation-free, and\nit maintains fast testing speed. Extensive experiments across various\nbenchmarks demonstrate the effectiveness of our approach, abbreviated as\nAdaNeg. Notably, on the large-scale ImageNet benchmark, our AdaNeg\nsignificantly outperforms existing methods, with a 2.45\\% increase in AUROC and\na 6.48\\% reduction in FPR95. Codes are available at\n\\url{https://github.com/YBZh/OpenOOD-VLM}.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent research has shown that pre-trained vision-language models are\neffective at identifying out-of-distribution (OOD) samples by using negative\nlabels as guidance. However, employing consistent negative labels across\ndifferent OOD datasets often results in semantic misalignments, as these text\nlabels may not accurately reflect the actual space of OOD images. To overcome\nthis issue, we introduce \\textit{adaptive negative proxies}, which are\ndynamically generated during testing by exploring actual OOD images, to align\nmore closely with the underlying OOD label space and enhance the efficacy of\nnegative proxy guidance. Specifically, our approach utilizes a feature memory\nbank to selectively cache discriminative features from test images,\nrepresenting the targeted OOD distribution. This facilitates the creation of\nproxies that can better align with specific OOD datasets. While task-adaptive\nproxies average features to reflect the unique characteristics of each dataset,\nthe sample-adaptive proxies weight features based on their similarity to\nindividual test samples, exploring detailed sample-level nuances. The final\nscore for identifying OOD samples integrates static negative labels with our\nproposed adaptive proxies, effectively combining textual and visual knowledge\nfor enhanced performance. Our method is training-free and annotation-free, and\nit maintains fast testing speed. Extensive experiments across various\nbenchmarks demonstrate the effectiveness of our approach, abbreviated as\nAdaNeg. Notably, on the large-scale ImageNet benchmark, our AdaNeg\nsignificantly outperforms existing methods, with a 2.45\\% increase in AUROC and\na 6.48\\% reduction in FPR95. Codes are available at\n\\url{https://github.com/YBZh/OpenOOD-VLM}."
                },
                "authors": [
                    {
                        "name": "Yabin Zhang"
                    },
                    {
                        "name": "Lei Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Lei Zhang"
                },
                "author": "Lei Zhang",
                "arxiv_comment": "NIPS 2024 Camera Ready, Codes are available at\n  \\url{https://github.com/YBZh/OpenOOD-VLM}",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.20149v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.20149v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.20004v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.20004v1",
                "updated": "2024-10-25T23:17:56Z",
                "updated_parsed": [
                    2024,
                    10,
                    25,
                    23,
                    17,
                    56,
                    4,
                    299,
                    0
                ],
                "published": "2024-10-25T23:17:56Z",
                "published_parsed": [
                    2024,
                    10,
                    25,
                    23,
                    17,
                    56,
                    4,
                    299,
                    0
                ],
                "title": "Lightweight, Secure and Stateful Serverless Computing with PSL",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Lightweight, Secure and Stateful Serverless Computing with PSL"
                },
                "summary": "We present PSL, a lightweight, secure and stateful Function-as-a-Serivce\n(FaaS) framework for Trusted Execution Environments (TEEs). The framework\nprovides rich programming language support on heterogeneous TEE hardware for\nstatically compiled binaries and/or WebAssembly (WASM) bytecodes, with a\nfamiliar Key-Value Store (KVS) interface to secure, performant,\nnetwork-embedded storage. It achieves near-native execution speeds by utilizing\nthe dynamic memory mapping capabilities of Intel SGX2 to create an in-enclave\nWASM runtime with Just-In-Time (JIT) compilation. PSL is designed to\nefficiently operate within an asynchronous environment with a distributed\ntamper-proof confidential storage system, assuming minority failures. The\nsystem exchanges eventually-consistent state updates across nodes while\nutilizing release-consistent locking mechanisms to enhance transactional\ncapabilities. The execution of PSL is up to 3.7x faster than the\nstate-of-the-art SGX WASM runtime. PSL reaches 95k ops/s with YCSB 100% read\nworkload and 89k ops/s with 50% read/write workload. We demonstrate the\nscalability and adaptivity of PSL through a case study of secure and\ndistributed training of deep neural networks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present PSL, a lightweight, secure and stateful Function-as-a-Serivce\n(FaaS) framework for Trusted Execution Environments (TEEs). The framework\nprovides rich programming language support on heterogeneous TEE hardware for\nstatically compiled binaries and/or WebAssembly (WASM) bytecodes, with a\nfamiliar Key-Value Store (KVS) interface to secure, performant,\nnetwork-embedded storage. It achieves near-native execution speeds by utilizing\nthe dynamic memory mapping capabilities of Intel SGX2 to create an in-enclave\nWASM runtime with Just-In-Time (JIT) compilation. PSL is designed to\nefficiently operate within an asynchronous environment with a distributed\ntamper-proof confidential storage system, assuming minority failures. The\nsystem exchanges eventually-consistent state updates across nodes while\nutilizing release-consistent locking mechanisms to enhance transactional\ncapabilities. The execution of PSL is up to 3.7x faster than the\nstate-of-the-art SGX WASM runtime. PSL reaches 95k ops/s with YCSB 100% read\nworkload and 89k ops/s with 50% read/write workload. We demonstrate the\nscalability and adaptivity of PSL through a case study of secure and\ndistributed training of deep neural networks."
                },
                "authors": [
                    {
                        "name": "Alexander Thomas"
                    },
                    {
                        "name": "Shubham Mishra"
                    },
                    {
                        "name": "Kaiyuan Chen"
                    },
                    {
                        "name": "John Kubiatowicz"
                    }
                ],
                "author_detail": {
                    "name": "John Kubiatowicz"
                },
                "author": "John Kubiatowicz",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.20004v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.20004v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.05317v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.05317v2",
                "updated": "2024-10-25T21:09:59Z",
                "updated_parsed": [
                    2024,
                    10,
                    25,
                    21,
                    9,
                    59,
                    4,
                    299,
                    0
                ],
                "published": "2024-06-08T01:35:11Z",
                "published_parsed": [
                    2024,
                    6,
                    8,
                    1,
                    35,
                    11,
                    5,
                    160,
                    0
                ],
                "title": "LoCoCo: Dropping In Convolutions for Long Context Compression",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LoCoCo: Dropping In Convolutions for Long Context Compression"
                },
                "summary": "This paper tackles the memory hurdle of processing long context sequences in\nLarge Language Models (LLMs), by presenting a novel approach, Dropping In\nConvolutions for Long Context Compression (LoCoCo). LoCoCo employs only a\nfixed-size Key-Value (KV) cache, and can enhance efficiency in both inference\nand fine-tuning stages. Diverging from prior methods that selectively drop KV\npairs based on heuristics, LoCoCo leverages a data-driven adaptive fusion\ntechnique, blending previous KV pairs with incoming tokens to minimize the loss\nof contextual information and ensure accurate attention modeling. This token\nintegration is achieved through injecting one-dimensional convolutional kernels\nthat dynamically calculate mixing weights for each KV cache slot. Designed for\nbroad compatibility with existing LLM frameworks, LoCoCo allows for\nstraightforward \"drop-in\" integration without needing architectural\nmodifications, while incurring minimal tuning overhead. Experiments demonstrate\nthat LoCoCo maintains consistently outstanding performance across various\ncontext lengths and can achieve a high context compression rate during both\ninference and fine-tuning phases. During inference, we successfully compressed\nup to 3482 tokens into a 128-size KV cache, while retaining comparable\nperformance to the full sequence - an accuracy improvement of up to 0.2791\ncompared to baselines at the same cache size. During post-training tuning, we\nalso effectively extended the context length from 4K to 32K using a KV cache of\nfixed size 512, achieving performance similar to fine-tuning with entire\nsequences.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper tackles the memory hurdle of processing long context sequences in\nLarge Language Models (LLMs), by presenting a novel approach, Dropping In\nConvolutions for Long Context Compression (LoCoCo). LoCoCo employs only a\nfixed-size Key-Value (KV) cache, and can enhance efficiency in both inference\nand fine-tuning stages. Diverging from prior methods that selectively drop KV\npairs based on heuristics, LoCoCo leverages a data-driven adaptive fusion\ntechnique, blending previous KV pairs with incoming tokens to minimize the loss\nof contextual information and ensure accurate attention modeling. This token\nintegration is achieved through injecting one-dimensional convolutional kernels\nthat dynamically calculate mixing weights for each KV cache slot. Designed for\nbroad compatibility with existing LLM frameworks, LoCoCo allows for\nstraightforward \"drop-in\" integration without needing architectural\nmodifications, while incurring minimal tuning overhead. Experiments demonstrate\nthat LoCoCo maintains consistently outstanding performance across various\ncontext lengths and can achieve a high context compression rate during both\ninference and fine-tuning phases. During inference, we successfully compressed\nup to 3482 tokens into a 128-size KV cache, while retaining comparable\nperformance to the full sequence - an accuracy improvement of up to 0.2791\ncompared to baselines at the same cache size. During post-training tuning, we\nalso effectively extended the context length from 4K to 32K using a KV cache of\nfixed size 512, achieving performance similar to fine-tuning with entire\nsequences."
                },
                "authors": [
                    {
                        "name": "Ruisi Cai"
                    },
                    {
                        "name": "Yuandong Tian"
                    },
                    {
                        "name": "Zhangyang Wang"
                    },
                    {
                        "name": "Beidi Chen"
                    }
                ],
                "author_detail": {
                    "name": "Beidi Chen"
                },
                "author": "Beidi Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.05317v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.05317v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.03766v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.03766v2",
                "updated": "2024-10-25T19:45:33Z",
                "updated_parsed": [
                    2024,
                    10,
                    25,
                    19,
                    45,
                    33,
                    4,
                    299,
                    0
                ],
                "published": "2024-10-02T15:22:08Z",
                "published_parsed": [
                    2024,
                    10,
                    2,
                    15,
                    22,
                    8,
                    2,
                    276,
                    0
                ],
                "title": "FutureFill: Fast Generation from Convolutional Sequence Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FutureFill: Fast Generation from Convolutional Sequence Models"
                },
                "summary": "We address the challenge of efficient auto-regressive generation in sequence\nprediction models by introducing FutureFill - a method for fast generation that\napplies to any sequence prediction algorithm based on convolutional operators.\nOur approach reduces the generation time requirement from quadratic to\nquasilinear relative to the context length. Additionally, FutureFill requires a\nprefill cache sized only by the number of tokens generated, which is smaller\nthan the cache requirements for standard convolutional and attention-based\nmodels. We validate our theoretical findings with experimental evidence\ndemonstrating correctness and efficiency gains in a synthetic generation task.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We address the challenge of efficient auto-regressive generation in sequence\nprediction models by introducing FutureFill - a method for fast generation that\napplies to any sequence prediction algorithm based on convolutional operators.\nOur approach reduces the generation time requirement from quadratic to\nquasilinear relative to the context length. Additionally, FutureFill requires a\nprefill cache sized only by the number of tokens generated, which is smaller\nthan the cache requirements for standard convolutional and attention-based\nmodels. We validate our theoretical findings with experimental evidence\ndemonstrating correctness and efficiency gains in a synthetic generation task."
                },
                "authors": [
                    {
                        "name": "Naman Agarwal"
                    },
                    {
                        "name": "Xinyi Chen"
                    },
                    {
                        "name": "Evan Dogariu"
                    },
                    {
                        "name": "Vlad Feinberg"
                    },
                    {
                        "name": "Daniel Suo"
                    },
                    {
                        "name": "Peter Bartlett"
                    },
                    {
                        "name": "Elad Hazan"
                    }
                ],
                "author_detail": {
                    "name": "Elad Hazan"
                },
                "author": "Elad Hazan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.03766v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.03766v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.19937v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.19937v1",
                "updated": "2024-10-25T19:18:22Z",
                "updated_parsed": [
                    2024,
                    10,
                    25,
                    19,
                    18,
                    22,
                    4,
                    299,
                    0
                ],
                "published": "2024-10-25T19:18:22Z",
                "published_parsed": [
                    2024,
                    10,
                    25,
                    19,
                    18,
                    22,
                    4,
                    299,
                    0
                ],
                "title": "RobustKV: Defending Large Language Models against Jailbreak Attacks via\n  KV Eviction",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RobustKV: Defending Large Language Models against Jailbreak Attacks via\n  KV Eviction"
                },
                "summary": "Jailbreak attacks circumvent LLMs' built-in safeguards by concealing harmful\nqueries within jailbreak prompts. While existing defenses primarily focus on\nmitigating the effects of jailbreak prompts, they often prove inadequate as\njailbreak prompts can take arbitrary, adaptive forms. This paper presents\nRobustKV, a novel defense that adopts a fundamentally different approach by\nselectively removing critical tokens of harmful queries from key-value (KV)\ncaches. Intuitively, for a jailbreak prompt to be effective, its tokens must\nachieve sufficient `importance' (as measured by attention scores), which\ninevitably lowers the importance of tokens in the concealed harmful query.\nThus, by strategically evicting the KVs of the lowest-ranked tokens, RobustKV\ndiminishes the presence of the harmful query in the KV cache, thus preventing\nthe LLM from generating malicious responses. Extensive evaluation using\nbenchmark datasets and models demonstrates that RobustKV effectively counters\nstate-of-the-art jailbreak attacks while maintaining the LLM's general\nperformance on benign queries. Moreover, RobustKV creates an intriguing\nevasiveness dilemma for adversaries, forcing them to balance between evading\nRobustKV and bypassing the LLM's built-in safeguards. This trade-off\ncontributes to RobustKV's robustness against adaptive attacks. (warning: this\npaper contains potentially harmful content generated by LLMs.)",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Jailbreak attacks circumvent LLMs' built-in safeguards by concealing harmful\nqueries within jailbreak prompts. While existing defenses primarily focus on\nmitigating the effects of jailbreak prompts, they often prove inadequate as\njailbreak prompts can take arbitrary, adaptive forms. This paper presents\nRobustKV, a novel defense that adopts a fundamentally different approach by\nselectively removing critical tokens of harmful queries from key-value (KV)\ncaches. Intuitively, for a jailbreak prompt to be effective, its tokens must\nachieve sufficient `importance' (as measured by attention scores), which\ninevitably lowers the importance of tokens in the concealed harmful query.\nThus, by strategically evicting the KVs of the lowest-ranked tokens, RobustKV\ndiminishes the presence of the harmful query in the KV cache, thus preventing\nthe LLM from generating malicious responses. Extensive evaluation using\nbenchmark datasets and models demonstrates that RobustKV effectively counters\nstate-of-the-art jailbreak attacks while maintaining the LLM's general\nperformance on benign queries. Moreover, RobustKV creates an intriguing\nevasiveness dilemma for adversaries, forcing them to balance between evading\nRobustKV and bypassing the LLM's built-in safeguards. This trade-off\ncontributes to RobustKV's robustness against adaptive attacks. (warning: this\npaper contains potentially harmful content generated by LLMs.)"
                },
                "authors": [
                    {
                        "name": "Tanqiu Jiang"
                    },
                    {
                        "name": "Zian Wang"
                    },
                    {
                        "name": "Jiacheng Liang"
                    },
                    {
                        "name": "Changjiang Li"
                    },
                    {
                        "name": "Yuhui Wang"
                    },
                    {
                        "name": "Ting Wang"
                    }
                ],
                "author_detail": {
                    "name": "Ting Wang"
                },
                "author": "Ting Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.19937v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.19937v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.18248v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.18248v2",
                "updated": "2024-10-25T19:18:00Z",
                "updated_parsed": [
                    2024,
                    10,
                    25,
                    19,
                    18,
                    0,
                    4,
                    299,
                    0
                ],
                "published": "2024-10-23T19:53:30Z",
                "published_parsed": [
                    2024,
                    10,
                    23,
                    19,
                    53,
                    30,
                    2,
                    297,
                    0
                ],
                "title": "Fast Inference for Augmented Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Fast Inference for Augmented Large Language Models"
                },
                "summary": "Augmented Large Language Models (LLMs) enhance the capabilities of standalone\nLLMs by integrating external data sources through API calls. In interactive LLM\napplications, efficient scheduling is crucial for maintaining low request\ncompletion times, directly impacting user engagement. However, these\naugmentations introduce scheduling challenges due to the need to manage limited\nmemory for cached information (KV caches). As a result, traditional size-based\nscheduling algorithms, such as Shortest Job First (SJF), become less effective\nat minimizing completion times. Existing work focuses only on handling requests\nduring API calls by preserving, discarding, or swapping memory without\nconsidering how to schedule requests with API calls. In this paper, we propose\nLAMPS, a novel LLM inference framework for augmented LLMs. LAMPS minimizes\nrequest completion time through a unified scheduling approach that considers\nthe total length of requests and their handling strategies during API calls.\nRecognizing that LLM inference is memory-bound, our approach ranks requests\nbased on their consumption of memory over time, which depends on both the\noutput sizes and how a request is managed during its API calls. To implement\nour scheduling, LAMPS predicts the strategy that minimizes memory waste of a\nrequest during its API calls, aligning with but improving upon existing\napproaches. We also propose starvation prevention techniques and optimizations\nto mitigate the overhead of our scheduling. We implement LAMPS on top of vLLM\nand evaluate its performance against baseline LLM inference systems,\ndemonstrating improvements in end-to-end latency by 27%-85% and reductions in\nTTFT by 4%-96% compared to the existing augmented-LLM system, with even greater\ngains over vLLM.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Augmented Large Language Models (LLMs) enhance the capabilities of standalone\nLLMs by integrating external data sources through API calls. In interactive LLM\napplications, efficient scheduling is crucial for maintaining low request\ncompletion times, directly impacting user engagement. However, these\naugmentations introduce scheduling challenges due to the need to manage limited\nmemory for cached information (KV caches). As a result, traditional size-based\nscheduling algorithms, such as Shortest Job First (SJF), become less effective\nat minimizing completion times. Existing work focuses only on handling requests\nduring API calls by preserving, discarding, or swapping memory without\nconsidering how to schedule requests with API calls. In this paper, we propose\nLAMPS, a novel LLM inference framework for augmented LLMs. LAMPS minimizes\nrequest completion time through a unified scheduling approach that considers\nthe total length of requests and their handling strategies during API calls.\nRecognizing that LLM inference is memory-bound, our approach ranks requests\nbased on their consumption of memory over time, which depends on both the\noutput sizes and how a request is managed during its API calls. To implement\nour scheduling, LAMPS predicts the strategy that minimizes memory waste of a\nrequest during its API calls, aligning with but improving upon existing\napproaches. We also propose starvation prevention techniques and optimizations\nto mitigate the overhead of our scheduling. We implement LAMPS on top of vLLM\nand evaluate its performance against baseline LLM inference systems,\ndemonstrating improvements in end-to-end latency by 27%-85% and reductions in\nTTFT by 4%-96% compared to the existing augmented-LLM system, with even greater\ngains over vLLM."
                },
                "authors": [
                    {
                        "name": "Rana Shahout"
                    },
                    {
                        "name": "Cong Liang"
                    },
                    {
                        "name": "Shiji Xin"
                    },
                    {
                        "name": "Qianru Lao"
                    },
                    {
                        "name": "Yong Cui"
                    },
                    {
                        "name": "Minlan Yu"
                    },
                    {
                        "name": "Michael Mitzenmacher"
                    }
                ],
                "author_detail": {
                    "name": "Michael Mitzenmacher"
                },
                "author": "Michael Mitzenmacher",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.18248v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.18248v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2401.18079v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2401.18079v5",
                "updated": "2024-10-25T18:29:43Z",
                "updated_parsed": [
                    2024,
                    10,
                    25,
                    18,
                    29,
                    43,
                    4,
                    299,
                    0
                ],
                "published": "2024-01-31T18:58:14Z",
                "published_parsed": [
                    2024,
                    1,
                    31,
                    18,
                    58,
                    14,
                    2,
                    31,
                    0
                ],
                "title": "KVQuant: Towards 10 Million Context Length LLM Inference with KV Cache\n  Quantization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "KVQuant: Towards 10 Million Context Length LLM Inference with KV Cache\n  Quantization"
                },
                "summary": "LLMs are seeing growing use for applications which require large context\nwindows, and with these large context windows KV cache activations surface as\nthe dominant contributor to memory consumption during inference. Quantization\nis a promising approach for compressing KV cache activations; however, existing\nsolutions fail to represent activations accurately in sub-4-bit precision. Our\nwork, KVQuant, facilitates low precision KV cache quantization by incorporating\nseveral novel methods: (i) Per-Channel Key Quantization, where we adjust the\ndimension along which we quantize the Key activations to better match the\ndistribution; (ii) Pre-RoPE Key Quantization, where we quantize Key activations\nbefore the rotary positional embedding to mitigate its impact on quantization;\n(iii) Non-Uniform KV Cache Quantization, where we derive per-layer\nsensitivity-weighted non-uniform datatypes that better represent the\ndistributions; and (iv) Per-Vector Dense-and-Sparse Quantization, where we\nisolate outliers separately for each vector to minimize skews in quantization\nranges. By applying our method to the LLaMA, Llama-2, Llama-3, and Mistral\nmodels, we achieve < 0.1 perplexity degradation with 3-bit quantization on both\nWikitext-2 and C4, outperforming existing approaches. Our method enables\nserving LLaMA-7B with a context length of up to 1 million on a single A100-80GB\nGPU and up to 10 million on an 8-GPU system. We develop custom CUDA kernels for\nKVQuant, showing that we can achieve up to ~1.7x speedups, compared to baseline\nfp16 matrix-vector multiplications, for the LLaMA-7B model.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLMs are seeing growing use for applications which require large context\nwindows, and with these large context windows KV cache activations surface as\nthe dominant contributor to memory consumption during inference. Quantization\nis a promising approach for compressing KV cache activations; however, existing\nsolutions fail to represent activations accurately in sub-4-bit precision. Our\nwork, KVQuant, facilitates low precision KV cache quantization by incorporating\nseveral novel methods: (i) Per-Channel Key Quantization, where we adjust the\ndimension along which we quantize the Key activations to better match the\ndistribution; (ii) Pre-RoPE Key Quantization, where we quantize Key activations\nbefore the rotary positional embedding to mitigate its impact on quantization;\n(iii) Non-Uniform KV Cache Quantization, where we derive per-layer\nsensitivity-weighted non-uniform datatypes that better represent the\ndistributions; and (iv) Per-Vector Dense-and-Sparse Quantization, where we\nisolate outliers separately for each vector to minimize skews in quantization\nranges. By applying our method to the LLaMA, Llama-2, Llama-3, and Mistral\nmodels, we achieve < 0.1 perplexity degradation with 3-bit quantization on both\nWikitext-2 and C4, outperforming existing approaches. Our method enables\nserving LLaMA-7B with a context length of up to 1 million on a single A100-80GB\nGPU and up to 10 million on an 8-GPU system. We develop custom CUDA kernels for\nKVQuant, showing that we can achieve up to ~1.7x speedups, compared to baseline\nfp16 matrix-vector multiplications, for the LLaMA-7B model."
                },
                "authors": [
                    {
                        "name": "Coleman Hooper"
                    },
                    {
                        "name": "Sehoon Kim"
                    },
                    {
                        "name": "Hiva Mohammadzadeh"
                    },
                    {
                        "name": "Michael W. Mahoney"
                    },
                    {
                        "name": "Yakun Sophia Shao"
                    },
                    {
                        "name": "Kurt Keutzer"
                    },
                    {
                        "name": "Amir Gholami"
                    }
                ],
                "author_detail": {
                    "name": "Amir Gholami"
                },
                "author": "Amir Gholami",
                "arxiv_comment": "NeurIPS 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2401.18079v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2401.18079v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.19355v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.19355v1",
                "updated": "2024-10-25T07:24:38Z",
                "updated_parsed": [
                    2024,
                    10,
                    25,
                    7,
                    24,
                    38,
                    4,
                    299,
                    0
                ],
                "published": "2024-10-25T07:24:38Z",
                "published_parsed": [
                    2024,
                    10,
                    25,
                    7,
                    24,
                    38,
                    4,
                    299,
                    0
                ],
                "title": "FasterCache: Training-Free Video Diffusion Model Acceleration with High\n  Quality",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FasterCache: Training-Free Video Diffusion Model Acceleration with High\n  Quality"
                },
                "summary": "In this paper, we present \\textbf{\\textit{FasterCache}}, a novel\ntraining-free strategy designed to accelerate the inference of video diffusion\nmodels with high-quality generation. By analyzing existing cache-based methods,\nwe observe that \\textit{directly reusing adjacent-step features degrades video\nquality due to the loss of subtle variations}. We further perform a pioneering\ninvestigation of the acceleration potential of classifier-free guidance (CFG)\nand reveal significant redundancy between conditional and unconditional\nfeatures within the same timestep. Capitalizing on these observations, we\nintroduce FasterCache to substantially accelerate diffusion-based video\ngeneration. Our key contributions include a dynamic feature reuse strategy that\npreserves both feature distinction and temporal continuity, and CFG-Cache which\noptimizes the reuse of conditional and unconditional outputs to further enhance\ninference speed without compromising video quality. We empirically evaluate\nFasterCache on recent video diffusion models. Experimental results show that\nFasterCache can significantly accelerate video generation (\\eg 1.67$\\times$\nspeedup on Vchitect-2.0) while keeping video quality comparable to the\nbaseline, and consistently outperform existing methods in both inference speed\nand video quality.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this paper, we present \\textbf{\\textit{FasterCache}}, a novel\ntraining-free strategy designed to accelerate the inference of video diffusion\nmodels with high-quality generation. By analyzing existing cache-based methods,\nwe observe that \\textit{directly reusing adjacent-step features degrades video\nquality due to the loss of subtle variations}. We further perform a pioneering\ninvestigation of the acceleration potential of classifier-free guidance (CFG)\nand reveal significant redundancy between conditional and unconditional\nfeatures within the same timestep. Capitalizing on these observations, we\nintroduce FasterCache to substantially accelerate diffusion-based video\ngeneration. Our key contributions include a dynamic feature reuse strategy that\npreserves both feature distinction and temporal continuity, and CFG-Cache which\noptimizes the reuse of conditional and unconditional outputs to further enhance\ninference speed without compromising video quality. We empirically evaluate\nFasterCache on recent video diffusion models. Experimental results show that\nFasterCache can significantly accelerate video generation (\\eg 1.67$\\times$\nspeedup on Vchitect-2.0) while keeping video quality comparable to the\nbaseline, and consistently outperform existing methods in both inference speed\nand video quality."
                },
                "authors": [
                    {
                        "name": "Zhengyao Lv"
                    },
                    {
                        "name": "Chenyang Si"
                    },
                    {
                        "name": "Junhao Song"
                    },
                    {
                        "name": "Zhenyu Yang"
                    },
                    {
                        "name": "Yu Qiao"
                    },
                    {
                        "name": "Ziwei Liu"
                    },
                    {
                        "name": "Kwan-Yee K. Wong"
                    }
                ],
                "author_detail": {
                    "name": "Kwan-Yee K. Wong"
                },
                "author": "Kwan-Yee K. Wong",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.19355v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.19355v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.19123v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.19123v1",
                "updated": "2024-10-24T19:48:51Z",
                "updated_parsed": [
                    2024,
                    10,
                    24,
                    19,
                    48,
                    51,
                    3,
                    298,
                    0
                ],
                "published": "2024-10-24T19:48:51Z",
                "published_parsed": [
                    2024,
                    10,
                    24,
                    19,
                    48,
                    51,
                    3,
                    298,
                    0
                ],
                "title": "Read-ME: Refactorizing LLMs as Router-Decoupled Mixture of Experts with\n  System Co-Design",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Read-ME: Refactorizing LLMs as Router-Decoupled Mixture of Experts with\n  System Co-Design"
                },
                "summary": "The proliferation of large language models (LLMs) has led to the adoption of\nMixture-of-Experts (MoE) architectures that dynamically leverage specialized\nsubnetworks for improved efficiency and performance. Despite their benefits,\nMoE models face significant challenges during inference, including inefficient\nmemory management and suboptimal batching, due to misaligned design choices\nbetween the model architecture and the system policies. Furthermore, the\nconventional approach of training MoEs from scratch is increasingly prohibitive\nin terms of cost. In this paper, we propose a novel framework Read-ME that\ntransforms pre-trained dense LLMs into smaller MoE models (in contrast to\n\"upcycling\" generalist MoEs), avoiding the high costs of ground-up training.\nOur approach employs activation sparsity to extract experts. To compose\nexperts, we examine the widely-adopted layer-wise router design and show its\nredundancy, and thus we introduce the pre-gating router decoupled from the MoE\nbackbone that facilitates system-friendly pre-computing and lookahead\nscheduling, enhancing expert-aware batching and caching. Our codesign therefore\naddresses critical gaps on both the algorithmic and system fronts, establishing\na scalable and efficient alternative for LLM inference in resource-constrained\nsettings. Read-ME outperforms other popular open-source dense models of similar\nscales, achieving improvements of up to 10.1% on MMLU, and improving mean\nend-to-end latency up to 6.1%. Codes are available at:\nhttps://github.com/VITA-Group/READ-ME.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The proliferation of large language models (LLMs) has led to the adoption of\nMixture-of-Experts (MoE) architectures that dynamically leverage specialized\nsubnetworks for improved efficiency and performance. Despite their benefits,\nMoE models face significant challenges during inference, including inefficient\nmemory management and suboptimal batching, due to misaligned design choices\nbetween the model architecture and the system policies. Furthermore, the\nconventional approach of training MoEs from scratch is increasingly prohibitive\nin terms of cost. In this paper, we propose a novel framework Read-ME that\ntransforms pre-trained dense LLMs into smaller MoE models (in contrast to\n\"upcycling\" generalist MoEs), avoiding the high costs of ground-up training.\nOur approach employs activation sparsity to extract experts. To compose\nexperts, we examine the widely-adopted layer-wise router design and show its\nredundancy, and thus we introduce the pre-gating router decoupled from the MoE\nbackbone that facilitates system-friendly pre-computing and lookahead\nscheduling, enhancing expert-aware batching and caching. Our codesign therefore\naddresses critical gaps on both the algorithmic and system fronts, establishing\na scalable and efficient alternative for LLM inference in resource-constrained\nsettings. Read-ME outperforms other popular open-source dense models of similar\nscales, achieving improvements of up to 10.1% on MMLU, and improving mean\nend-to-end latency up to 6.1%. Codes are available at:\nhttps://github.com/VITA-Group/READ-ME."
                },
                "authors": [
                    {
                        "name": "Ruisi Cai"
                    },
                    {
                        "name": "Yeonju Ro"
                    },
                    {
                        "name": "Geon-Woo Kim"
                    },
                    {
                        "name": "Peihao Wang"
                    },
                    {
                        "name": "Babak Ehteshami Bejnordi"
                    },
                    {
                        "name": "Aditya Akella"
                    },
                    {
                        "name": "Zhangyang Wang"
                    }
                ],
                "author_detail": {
                    "name": "Zhangyang Wang"
                },
                "author": "Zhangyang Wang",
                "arxiv_comment": "38th Conference on Neural Information Processing Systems (NeurIPS\n  2024)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.19123v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.19123v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.15420v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.15420v2",
                "updated": "2024-10-24T16:40:10Z",
                "updated_parsed": [
                    2024,
                    10,
                    24,
                    16,
                    40,
                    10,
                    3,
                    298,
                    0
                ],
                "published": "2024-04-23T18:10:42Z",
                "published_parsed": [
                    2024,
                    4,
                    23,
                    18,
                    10,
                    42,
                    1,
                    114,
                    0
                ],
                "title": "XC-Cache: Cross-Attending to Cached Context for Efficient LLM Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "XC-Cache: Cross-Attending to Cached Context for Efficient LLM Inference"
                },
                "summary": "In-context learning (ICL) approaches typically leverage prompting to\ncondition decoder-only language model generation on reference information.\nJust-in-time processing of a context is inefficient due to the quadratic cost\nof self-attention operations, and caching is desirable. However, caching\ntransformer states can easily require almost as much space as the model\nparameters. When the right context isn't known in advance, caching ICL can be\nchallenging. This work addresses these limitations by introducing models that,\ninspired by the encoder-decoder architecture, use cross-attention to condition\ngeneration on reference text without the prompt. More precisely, we leverage\npre-trained decoder-only models and only train a small number of added layers.\nWe use Question-Answering (QA) as a testbed to evaluate the ability of our\nmodels to perform conditional generation and observe that they outperform ICL,\nare comparable to fine-tuned prompted LLMs, and drastically reduce the space\nfootprint relative to standard KV caching by two orders of magnitude.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In-context learning (ICL) approaches typically leverage prompting to\ncondition decoder-only language model generation on reference information.\nJust-in-time processing of a context is inefficient due to the quadratic cost\nof self-attention operations, and caching is desirable. However, caching\ntransformer states can easily require almost as much space as the model\nparameters. When the right context isn't known in advance, caching ICL can be\nchallenging. This work addresses these limitations by introducing models that,\ninspired by the encoder-decoder architecture, use cross-attention to condition\ngeneration on reference text without the prompt. More precisely, we leverage\npre-trained decoder-only models and only train a small number of added layers.\nWe use Question-Answering (QA) as a testbed to evaluate the ability of our\nmodels to perform conditional generation and observe that they outperform ICL,\nare comparable to fine-tuned prompted LLMs, and drastically reduce the space\nfootprint relative to standard KV caching by two orders of magnitude."
                },
                "authors": [
                    {
                        "name": "João Monteiro"
                    },
                    {
                        "name": "Étienne Marcotte"
                    },
                    {
                        "name": "Pierre-André Noël"
                    },
                    {
                        "name": "Valentina Zantedeschi"
                    },
                    {
                        "name": "David Vázquez"
                    },
                    {
                        "name": "Nicolas Chapados"
                    },
                    {
                        "name": "Christopher Pal"
                    },
                    {
                        "name": "Perouz Taslakian"
                    }
                ],
                "author_detail": {
                    "name": "Perouz Taslakian"
                },
                "author": "Perouz Taslakian",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.15420v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.15420v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.18517v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.18517v1",
                "updated": "2024-10-24T08:06:41Z",
                "updated_parsed": [
                    2024,
                    10,
                    24,
                    8,
                    6,
                    41,
                    3,
                    298,
                    0
                ],
                "published": "2024-10-24T08:06:41Z",
                "published_parsed": [
                    2024,
                    10,
                    24,
                    8,
                    6,
                    41,
                    3,
                    298,
                    0
                ],
                "title": "KVSharer: Efficient Inference via Layer-Wise Dissimilar KV Cache Sharing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "KVSharer: Efficient Inference via Layer-Wise Dissimilar KV Cache Sharing"
                },
                "summary": "The development of large language models (LLMs) has significantly expanded\nmodel sizes, resulting in substantial GPU memory requirements during inference.\nThe key and value storage of the attention map in the KV (key-value) cache\naccounts for more than 80\\% of this memory consumption. Nowadays, most existing\nKV cache compression methods focus on intra-layer compression within a single\nTransformer layer but few works consider layer-wise compression. In this paper,\nwe propose a plug-and-play method called \\textit{KVSharer}, which shares the KV\ncache between layers to achieve layer-wise compression. Rather than intuitively\nsharing based on higher similarity, we discover a counterintuitive phenomenon:\nsharing dissimilar KV caches better preserves the model performance.\nExperiments show that \\textit{KVSharer} can reduce KV cache computation by\n30\\%, thereby lowering memory consumption without significantly impacting model\nperformance and it can also achieve at least 1.3 times generation acceleration.\nAdditionally, we verify that \\textit{KVSharer} is compatible with existing\nintra-layer KV cache compression methods, and combining both can further save\nmemory.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The development of large language models (LLMs) has significantly expanded\nmodel sizes, resulting in substantial GPU memory requirements during inference.\nThe key and value storage of the attention map in the KV (key-value) cache\naccounts for more than 80\\% of this memory consumption. Nowadays, most existing\nKV cache compression methods focus on intra-layer compression within a single\nTransformer layer but few works consider layer-wise compression. In this paper,\nwe propose a plug-and-play method called \\textit{KVSharer}, which shares the KV\ncache between layers to achieve layer-wise compression. Rather than intuitively\nsharing based on higher similarity, we discover a counterintuitive phenomenon:\nsharing dissimilar KV caches better preserves the model performance.\nExperiments show that \\textit{KVSharer} can reduce KV cache computation by\n30\\%, thereby lowering memory consumption without significantly impacting model\nperformance and it can also achieve at least 1.3 times generation acceleration.\nAdditionally, we verify that \\textit{KVSharer} is compatible with existing\nintra-layer KV cache compression methods, and combining both can further save\nmemory."
                },
                "authors": [
                    {
                        "name": "Yifei Yang"
                    },
                    {
                        "name": "Zouying Cao"
                    },
                    {
                        "name": "Qiguang Chen"
                    },
                    {
                        "name": "Libo Qin"
                    },
                    {
                        "name": "Dongjie Yang"
                    },
                    {
                        "name": "Hai Zhao"
                    },
                    {
                        "name": "Zhi Chen"
                    }
                ],
                "author_detail": {
                    "name": "Zhi Chen"
                },
                "author": "Zhi Chen",
                "arxiv_comment": "Under Review by ICLR2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.18517v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.18517v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.18441v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.18441v1",
                "updated": "2024-10-24T05:29:20Z",
                "updated_parsed": [
                    2024,
                    10,
                    24,
                    5,
                    29,
                    20,
                    3,
                    298,
                    0
                ],
                "published": "2024-10-24T05:29:20Z",
                "published_parsed": [
                    2024,
                    10,
                    24,
                    5,
                    29,
                    20,
                    3,
                    298,
                    0
                ],
                "title": "The Nature of Mathematical Modeling and Probabilistic Optimization\n  Engineering in Generative AI",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Nature of Mathematical Modeling and Probabilistic Optimization\n  Engineering in Generative AI"
                },
                "summary": "In this paper, we give an in-depth analysis on the mathematical problem\nformulations and the probabilistic optimization explorations for some of the\nkey components in Transformer model [33] in the field of generative AI. We\nexplore and discuss some potential further enhancement for current state of the\nart methods for some key underlying technologies of generative AI models from\nalgorithmic and probabilistic optimization perspective. In particular, we\npresent an optimal solution for sub-word encoding (SWE) based on similar\ninitial settings as that of byte-pair encoding (BPE) algorithm in [9] with\nsimilar objectives as that of WordPiece approach in [28, 31] to maximize the\nlikelihood of the training data. We also present cross entropy optimization\nmethod to optimize hyperparameters for word2vec model [17]. In addition, we\npropose a factored combination of rotary positional encoding (RoPE) [32] and\nattention with linear biases (ALiBi) [23] with a harmonic series. We also\npresent a probabilistic FlashAttention [6, 7] (PrFlashAttention) method with a\nprobability distribution over block distances in the matrix to decide which\nblock is likely to participate in a given round of attention computation while\nmaintaining the lower triangle shape of the tensor for autoregressive language\nmodels by re-shaping the tensors. Finally, we present staircase adaptive\nquantization (SAQ) of key-value (KV) cache for multi-query attention (MQA)\nbased on the framework presented in [16] to have gradual quantization\ndegradation while achieving reasonable model quality and cost savings.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this paper, we give an in-depth analysis on the mathematical problem\nformulations and the probabilistic optimization explorations for some of the\nkey components in Transformer model [33] in the field of generative AI. We\nexplore and discuss some potential further enhancement for current state of the\nart methods for some key underlying technologies of generative AI models from\nalgorithmic and probabilistic optimization perspective. In particular, we\npresent an optimal solution for sub-word encoding (SWE) based on similar\ninitial settings as that of byte-pair encoding (BPE) algorithm in [9] with\nsimilar objectives as that of WordPiece approach in [28, 31] to maximize the\nlikelihood of the training data. We also present cross entropy optimization\nmethod to optimize hyperparameters for word2vec model [17]. In addition, we\npropose a factored combination of rotary positional encoding (RoPE) [32] and\nattention with linear biases (ALiBi) [23] with a harmonic series. We also\npresent a probabilistic FlashAttention [6, 7] (PrFlashAttention) method with a\nprobability distribution over block distances in the matrix to decide which\nblock is likely to participate in a given round of attention computation while\nmaintaining the lower triangle shape of the tensor for autoregressive language\nmodels by re-shaping the tensors. Finally, we present staircase adaptive\nquantization (SAQ) of key-value (KV) cache for multi-query attention (MQA)\nbased on the framework presented in [16] to have gradual quantization\ndegradation while achieving reasonable model quality and cost savings."
                },
                "authors": [
                    {
                        "name": "Fulu Li"
                    }
                ],
                "author_detail": {
                    "name": "Fulu Li"
                },
                "author": "Fulu Li",
                "arxiv_comment": "19 pages, 3 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.18441v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.18441v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.18002v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.18002v1",
                "updated": "2024-10-23T16:25:22Z",
                "updated_parsed": [
                    2024,
                    10,
                    23,
                    16,
                    25,
                    22,
                    2,
                    297,
                    0
                ],
                "published": "2024-10-23T16:25:22Z",
                "published_parsed": [
                    2024,
                    10,
                    23,
                    16,
                    25,
                    22,
                    2,
                    297,
                    0
                ],
                "title": "Digital Network Twins for Next-generation Wireless: Creation,\n  Optimization, and Challenges",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Digital Network Twins for Next-generation Wireless: Creation,\n  Optimization, and Challenges"
                },
                "summary": "Digital network twins (DNTs), by representing a physical network using a\nvirtual model, offer significant benefits such as streamlined network\ndevelopment, enhanced productivity, and cost reduction for next-generation\n(nextG) communication infrastructure. Existing works mainly describe the\ndeployment of DNT technologies in various service sections.The full life cycle\nof DNTs for telecommunication has not yet been comprehensively studied,\nparticularly in the aspects of fine-grained creation, real-time adaptation,\nresource-efficient deployment, and security protection. This article presents\nan in-depth overview of DNTs, exploring their concrete integration into\nnetworks and communication, covering the fundamental designs, the emergent\napplications, and critical challenges in multiple dimensions. We also include\ntwo detailed case studies to illustrate how DNTs can be applied in real-world\nscenarios such as wireless traffic forecasting and edge caching. Additionally,\na forward-looking vision of the research opportunities in tackling the\nchallenges of DNTs is provided, aiming to fully maximize the benefits of DNTs\nin nextG networks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Digital network twins (DNTs), by representing a physical network using a\nvirtual model, offer significant benefits such as streamlined network\ndevelopment, enhanced productivity, and cost reduction for next-generation\n(nextG) communication infrastructure. Existing works mainly describe the\ndeployment of DNT technologies in various service sections.The full life cycle\nof DNTs for telecommunication has not yet been comprehensively studied,\nparticularly in the aspects of fine-grained creation, real-time adaptation,\nresource-efficient deployment, and security protection. This article presents\nan in-depth overview of DNTs, exploring their concrete integration into\nnetworks and communication, covering the fundamental designs, the emergent\napplications, and critical challenges in multiple dimensions. We also include\ntwo detailed case studies to illustrate how DNTs can be applied in real-world\nscenarios such as wireless traffic forecasting and edge caching. Additionally,\na forward-looking vision of the research opportunities in tackling the\nchallenges of DNTs is provided, aiming to fully maximize the benefits of DNTs\nin nextG networks."
                },
                "authors": [
                    {
                        "name": "Yuchen Liu"
                    },
                    {
                        "name": "Zhiyuan Peng"
                    },
                    {
                        "name": "Zifan Zhang"
                    },
                    {
                        "name": "Hanzhi Yu"
                    },
                    {
                        "name": "Mingzhe Chen"
                    }
                ],
                "author_detail": {
                    "name": "Mingzhe Chen"
                },
                "author": "Mingzhe Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.18002v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.18002v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2310.08437v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2310.08437v2",
                "updated": "2024-10-23T15:44:09Z",
                "updated_parsed": [
                    2024,
                    10,
                    23,
                    15,
                    44,
                    9,
                    2,
                    297,
                    0
                ],
                "published": "2023-10-12T16:01:46Z",
                "published_parsed": [
                    2023,
                    10,
                    12,
                    16,
                    1,
                    46,
                    3,
                    285,
                    0
                ],
                "title": "Cold Start Latency in Serverless Computing: A Systematic Review,\n  Taxonomy, and Future Directions",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cold Start Latency in Serverless Computing: A Systematic Review,\n  Taxonomy, and Future Directions"
                },
                "summary": "Recently, academics and the corporate sector have paid attention to\nserverless computing, which enables dynamic scalability and an economic model.\nIn serverless computing, users only pay for the time they actually use\nresources, enabling zero scaling to optimise cost and resource utilisation.\nHowever, this approach also introduces the serverless cold start problem.\nResearchers have developed various solutions to address the cold start problem,\nyet it remains an unresolved research area. In this article, we propose a\nsystematic literature review on clod start latency in serverless computing.\nFurthermore, we create a detailed taxonomy of approaches to cold start latency,\nwhich we use to investigate existing techniques for reducing the cold start\ntime and frequency. We have classified the current studies on cold start\nlatency into several categories such as caching and application-level\noptimisation-based solutions, as well as Artificial Intelligence (AI)/Machine\nLearning (ML)-based solutions. Moreover, we have analyzed the impact of cold\nstart latency on quality of service, explored current cold start latency\nmitigation methods, datasets, and implementation platforms, and classified them\ninto categories based on their common characteristics and features. Finally, we\noutline the open challenges and highlight the possible future directions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recently, academics and the corporate sector have paid attention to\nserverless computing, which enables dynamic scalability and an economic model.\nIn serverless computing, users only pay for the time they actually use\nresources, enabling zero scaling to optimise cost and resource utilisation.\nHowever, this approach also introduces the serverless cold start problem.\nResearchers have developed various solutions to address the cold start problem,\nyet it remains an unresolved research area. In this article, we propose a\nsystematic literature review on clod start latency in serverless computing.\nFurthermore, we create a detailed taxonomy of approaches to cold start latency,\nwhich we use to investigate existing techniques for reducing the cold start\ntime and frequency. We have classified the current studies on cold start\nlatency into several categories such as caching and application-level\noptimisation-based solutions, as well as Artificial Intelligence (AI)/Machine\nLearning (ML)-based solutions. Moreover, we have analyzed the impact of cold\nstart latency on quality of service, explored current cold start latency\nmitigation methods, datasets, and implementation platforms, and classified them\ninto categories based on their common characteristics and features. Finally, we\noutline the open challenges and highlight the possible future directions."
                },
                "authors": [
                    {
                        "name": "Muhammed Golec"
                    },
                    {
                        "name": "Guneet Kaur Walia"
                    },
                    {
                        "name": "Mohit Kumar"
                    },
                    {
                        "name": "Felix Cuadrado"
                    },
                    {
                        "name": "Sukhpal Singh Gill"
                    },
                    {
                        "name": "Steve Uhlig"
                    }
                ],
                "author_detail": {
                    "name": "Steve Uhlig"
                },
                "author": "Steve Uhlig",
                "arxiv_doi": "10.1145/3700875",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3700875",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2310.08437v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2310.08437v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "Preprint Version Accepted for Publication in ACM Computing Survey,\n  2024",
                "arxiv_journal_ref": "ACM Computing Surveys 2024",
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.17954v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.17954v1",
                "updated": "2024-10-23T15:24:54Z",
                "updated_parsed": [
                    2024,
                    10,
                    23,
                    15,
                    24,
                    54,
                    2,
                    297,
                    0
                ],
                "published": "2024-10-23T15:24:54Z",
                "published_parsed": [
                    2024,
                    10,
                    23,
                    15,
                    24,
                    54,
                    2,
                    297,
                    0
                ],
                "title": "ExpertFlow: Optimized Expert Activation and Token Allocation for\n  Efficient Mixture-of-Experts Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ExpertFlow: Optimized Expert Activation and Token Allocation for\n  Efficient Mixture-of-Experts Inference"
                },
                "summary": "Sparse Mixture of Experts (MoE) models, while outperforming dense Large\nLanguage Models (LLMs) in terms of performance, face significant deployment\nchallenges during inference due to their high memory demands. Existing\noffloading techniques, which involve swapping activated and idle experts\nbetween the GPU and CPU, often suffer from rigid expert caching mechanisms.\nThese mechanisms fail to adapt to dynamic routing, leading to inefficient cache\nutilization, or incur prohibitive costs for prediction training. To tackle\nthese inference-specific challenges, we introduce ExpertFlow, a comprehensive\nsystem specifically designed to enhance inference efficiency by accommodating\nflexible routing and enabling efficient expert scheduling between CPU and GPU.\nThis reduces overhead and boosts system performance. Central to our approach is\na predictive routing path-based offloading mechanism that utilizes a\nlightweight predictor to accurately forecast routing paths before computation\nbegins. This proactive strategy allows for real-time error correction in expert\ncaching, significantly increasing cache hit ratios and reducing the frequency\nof expert transfers, thereby minimizing I/O overhead. Additionally, we\nimplement a dynamic token scheduling strategy that optimizes MoE inference by\nrearranging input tokens across different batches. This method not only reduces\nthe number of activated experts per batch but also improves computational\nefficiency. Our extensive experiments demonstrate that ExpertFlow achieves up\nto 93.72\\% GPU memory savings and enhances inference speed by 2 to 10 times\ncompared to baseline methods, highlighting its effectiveness and utility as a\nrobust solution for resource-constrained inference scenarios.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Sparse Mixture of Experts (MoE) models, while outperforming dense Large\nLanguage Models (LLMs) in terms of performance, face significant deployment\nchallenges during inference due to their high memory demands. Existing\noffloading techniques, which involve swapping activated and idle experts\nbetween the GPU and CPU, often suffer from rigid expert caching mechanisms.\nThese mechanisms fail to adapt to dynamic routing, leading to inefficient cache\nutilization, or incur prohibitive costs for prediction training. To tackle\nthese inference-specific challenges, we introduce ExpertFlow, a comprehensive\nsystem specifically designed to enhance inference efficiency by accommodating\nflexible routing and enabling efficient expert scheduling between CPU and GPU.\nThis reduces overhead and boosts system performance. Central to our approach is\na predictive routing path-based offloading mechanism that utilizes a\nlightweight predictor to accurately forecast routing paths before computation\nbegins. This proactive strategy allows for real-time error correction in expert\ncaching, significantly increasing cache hit ratios and reducing the frequency\nof expert transfers, thereby minimizing I/O overhead. Additionally, we\nimplement a dynamic token scheduling strategy that optimizes MoE inference by\nrearranging input tokens across different batches. This method not only reduces\nthe number of activated experts per batch but also improves computational\nefficiency. Our extensive experiments demonstrate that ExpertFlow achieves up\nto 93.72\\% GPU memory savings and enhances inference speed by 2 to 10 times\ncompared to baseline methods, highlighting its effectiveness and utility as a\nrobust solution for resource-constrained inference scenarios."
                },
                "authors": [
                    {
                        "name": "Xin He"
                    },
                    {
                        "name": "Shunkang Zhang"
                    },
                    {
                        "name": "Yuxin Wang"
                    },
                    {
                        "name": "Haiyan Yin"
                    },
                    {
                        "name": "Zihao Zeng"
                    },
                    {
                        "name": "Shaohuai Shi"
                    },
                    {
                        "name": "Zhenheng Tang"
                    },
                    {
                        "name": "Xiaowen Chu"
                    },
                    {
                        "name": "Ivor Tsang"
                    },
                    {
                        "name": "Ong Yew Soon"
                    }
                ],
                "author_detail": {
                    "name": "Ong Yew Soon"
                },
                "author": "Ong Yew Soon",
                "arxiv_comment": "Mixture-of-Experts, Inference, Offloading",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.17954v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.17954v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.17897v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.17897v1",
                "updated": "2024-10-23T14:15:07Z",
                "updated_parsed": [
                    2024,
                    10,
                    23,
                    14,
                    15,
                    7,
                    2,
                    297,
                    0
                ],
                "published": "2024-10-23T14:15:07Z",
                "published_parsed": [
                    2024,
                    10,
                    23,
                    14,
                    15,
                    7,
                    2,
                    297,
                    0
                ],
                "title": "Value Residual Learning For Alleviating Attention Concentration In\n  Transformers",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Value Residual Learning For Alleviating Attention Concentration In\n  Transformers"
                },
                "summary": "Transformers can capture long-range dependencies using self-attention,\nallowing tokens to attend to all others directly. However, stacking multiple\nattention layers leads to attention concentration. One natural way to address\nthis issue is to use cross-layer attention, allowing information from earlier\nlayers to be directly accessible to later layers. However, this approach is\ncomputationally expensive. To address this problem, we propose Transformer with\nresidual value (ResFormer) which approximates cross-layer attention through\nadding a residual connection from the values of the the first layer to all\nsubsequent layers. Based on this method, one variant is the Transformer with\nsingle layer value (SVFormer), where all layers share the same value embedding\nfrom first layer, reducing the KV cache by nearly 50%. Comprehensive empirical\nevidence demonstrates that ResFormer mitigates attention concentration problem\nin deeper layers and enhances representation across most layers, outperforming\nthe vanilla Transformer, DenseFormer, and NeuTRENO in training error as well as\ndownstream tasks. SVFormer trains significantly faster than the vanilla\nTransformer and performs better than other methods like GQA and CLA, with\nperformance influenced by sequence length and cumulative learning rate.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Transformers can capture long-range dependencies using self-attention,\nallowing tokens to attend to all others directly. However, stacking multiple\nattention layers leads to attention concentration. One natural way to address\nthis issue is to use cross-layer attention, allowing information from earlier\nlayers to be directly accessible to later layers. However, this approach is\ncomputationally expensive. To address this problem, we propose Transformer with\nresidual value (ResFormer) which approximates cross-layer attention through\nadding a residual connection from the values of the the first layer to all\nsubsequent layers. Based on this method, one variant is the Transformer with\nsingle layer value (SVFormer), where all layers share the same value embedding\nfrom first layer, reducing the KV cache by nearly 50%. Comprehensive empirical\nevidence demonstrates that ResFormer mitigates attention concentration problem\nin deeper layers and enhances representation across most layers, outperforming\nthe vanilla Transformer, DenseFormer, and NeuTRENO in training error as well as\ndownstream tasks. SVFormer trains significantly faster than the vanilla\nTransformer and performs better than other methods like GQA and CLA, with\nperformance influenced by sequence length and cumulative learning rate."
                },
                "authors": [
                    {
                        "name": "Zhanchao Zhou"
                    },
                    {
                        "name": "Tianyi Wu"
                    },
                    {
                        "name": "Zhiyun Jiang"
                    },
                    {
                        "name": "Zhenzhong Lan"
                    }
                ],
                "author_detail": {
                    "name": "Zhenzhong Lan"
                },
                "author": "Zhenzhong Lan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.17897v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.17897v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.05118v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.05118v3",
                "updated": "2024-10-23T10:39:15Z",
                "updated_parsed": [
                    2024,
                    10,
                    23,
                    10,
                    39,
                    15,
                    2,
                    297,
                    0
                ],
                "published": "2024-05-08T15:16:02Z",
                "published_parsed": [
                    2024,
                    5,
                    8,
                    15,
                    16,
                    2,
                    2,
                    129,
                    0
                ],
                "title": "Full Version: (De/Re)-Composition of Data-Parallel Computations via\n  Multi-Dimensional Homomorphisms",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Full Version: (De/Re)-Composition of Data-Parallel Computations via\n  Multi-Dimensional Homomorphisms"
                },
                "summary": "We formally introduce a systematic (de/re)-composition approach, based on the\nalgebraic formalism of \"Multi-Dimensional Homomorphisms (MDHs)\". Our approach\nis designed as general enough to be applicable to a wide range of data-parallel\ncomputations and for various kinds of target parallel architectures. To\nefficiently target the deep and complex memory and core hierarchies of\ncontemporary architectures, we exploit our introduced (de/re)-composition\napproach for a correct-by-construction, parametrized cache blocking and\nparallelization strategy. We show that our approach is powerful enough to\nexpress, in the same formalism, the (de/re)-composition strategies of different\nclasses of state-of-the-art approaches (scheduling-based, polyhedral, etc), and\nwe demonstrate that the parameters of our strategies enable systematically\ngenerating code that can be fully automatically optimized (auto-tuned) for the\nparticular target architecture and characteristics of the input and output data\n(e.g., their sizes and memory layouts). Particularly, our experiments confirm\nthat via auto-tuning, we achieve higher performance than state-of-the-art\napproaches, including hand-optimized solutions provided by vendors (such as\nNVIDIA cuBLAS/cuDNN and Intel oneMKL/oneDNN), on real-world data sets and for a\nvariety of data-parallel computations, including: linear algebra routines,\nstencil and quantum chemistry computations, data mining algorithms, and\ncomputations that recently gained high attention due to their relevance for\ndeep learning.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We formally introduce a systematic (de/re)-composition approach, based on the\nalgebraic formalism of \"Multi-Dimensional Homomorphisms (MDHs)\". Our approach\nis designed as general enough to be applicable to a wide range of data-parallel\ncomputations and for various kinds of target parallel architectures. To\nefficiently target the deep and complex memory and core hierarchies of\ncontemporary architectures, we exploit our introduced (de/re)-composition\napproach for a correct-by-construction, parametrized cache blocking and\nparallelization strategy. We show that our approach is powerful enough to\nexpress, in the same formalism, the (de/re)-composition strategies of different\nclasses of state-of-the-art approaches (scheduling-based, polyhedral, etc), and\nwe demonstrate that the parameters of our strategies enable systematically\ngenerating code that can be fully automatically optimized (auto-tuned) for the\nparticular target architecture and characteristics of the input and output data\n(e.g., their sizes and memory layouts). Particularly, our experiments confirm\nthat via auto-tuning, we achieve higher performance than state-of-the-art\napproaches, including hand-optimized solutions provided by vendors (such as\nNVIDIA cuBLAS/cuDNN and Intel oneMKL/oneDNN), on real-world data sets and for a\nvariety of data-parallel computations, including: linear algebra routines,\nstencil and quantum chemistry computations, data mining algorithms, and\ncomputations that recently gained high attention due to their relevance for\ndeep learning."
                },
                "authors": [
                    {
                        "name": "Ari Rasch"
                    }
                ],
                "author_detail": {
                    "name": "Ari Rasch"
                },
                "author": "Ari Rasch",
                "arxiv_doi": "10.1145/3665643",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3665643",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2405.05118v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.05118v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "A short version of this paper is published at ACM TOPLAS and\n  presented at PLDI'24",
                "arxiv_journal_ref": "ACM Trans. Program. Lang. Syst. (May 2024)",
                "arxiv_primary_category": {
                    "term": "cs.PL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.PL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.17635v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.17635v1",
                "updated": "2024-10-23T07:53:29Z",
                "updated_parsed": [
                    2024,
                    10,
                    23,
                    7,
                    53,
                    29,
                    2,
                    297,
                    0
                ],
                "published": "2024-10-23T07:53:29Z",
                "published_parsed": [
                    2024,
                    10,
                    23,
                    7,
                    53,
                    29,
                    2,
                    297,
                    0
                ],
                "title": "Markov Chain of Thought for Efficient Mathematical Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Markov Chain of Thought for Efficient Mathematical Reasoning"
                },
                "summary": "Chain of Thought (CoT) of multi-step benefits from the logical structure of\nthe reasoning steps and task-specific actions, significantly enhancing the\nmathematical reasoning capabilities of large language models. As the prevalence\nof long CoT, the number of reasoning steps exceeds manageable token limits and\nleads to higher computational demands. Inspired by the fundamental logic of\nhuman cognition, ``derive, then reduce'', we conceptualize the standard\nmulti-step CoT as a novel Markov Chain of Thought (MCoT). In this study, we\nconsider the mathematical reasoning task, defining each reasoning step as text\naccompanied by a Python code snippet. To facilitate a longer reasoning path,\nself-correction is enabled through interactions with the code interpreter. Our\nMCoT aims to compress previous reasoning steps into a simplified question,\nenabling efficient next-step inference without relying on a lengthy KV cache.\nIn our experiments, we curate the \\texttt{MCoTInstruct} dataset, and the\nempirical results indicate that MCoT not only significantly enhances efficiency\nbut also maintains comparable accuracy. While much remains to be explored, this\nwork paves the way for exploring the long CoT reasoning abilities of LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Chain of Thought (CoT) of multi-step benefits from the logical structure of\nthe reasoning steps and task-specific actions, significantly enhancing the\nmathematical reasoning capabilities of large language models. As the prevalence\nof long CoT, the number of reasoning steps exceeds manageable token limits and\nleads to higher computational demands. Inspired by the fundamental logic of\nhuman cognition, ``derive, then reduce'', we conceptualize the standard\nmulti-step CoT as a novel Markov Chain of Thought (MCoT). In this study, we\nconsider the mathematical reasoning task, defining each reasoning step as text\naccompanied by a Python code snippet. To facilitate a longer reasoning path,\nself-correction is enabled through interactions with the code interpreter. Our\nMCoT aims to compress previous reasoning steps into a simplified question,\nenabling efficient next-step inference without relying on a lengthy KV cache.\nIn our experiments, we curate the \\texttt{MCoTInstruct} dataset, and the\nempirical results indicate that MCoT not only significantly enhances efficiency\nbut also maintains comparable accuracy. While much remains to be explored, this\nwork paves the way for exploring the long CoT reasoning abilities of LLMs."
                },
                "authors": [
                    {
                        "name": "Wen Yang"
                    },
                    {
                        "name": "Kai Fan"
                    },
                    {
                        "name": "Minpeng Liao"
                    }
                ],
                "author_detail": {
                    "name": "Minpeng Liao"
                },
                "author": "Minpeng Liao",
                "arxiv_comment": "Work in progress",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.17635v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.17635v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.04870v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.04870v5",
                "updated": "2024-10-23T05:55:31Z",
                "updated_parsed": [
                    2024,
                    10,
                    23,
                    5,
                    55,
                    31,
                    2,
                    297,
                    0
                ],
                "published": "2024-08-09T05:20:05Z",
                "published_parsed": [
                    2024,
                    8,
                    9,
                    5,
                    20,
                    5,
                    4,
                    222,
                    0
                ],
                "title": "ConfusedPilot: Confused Deputy Risks in RAG-based LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ConfusedPilot: Confused Deputy Risks in RAG-based LLMs"
                },
                "summary": "Retrieval augmented generation (RAG) is a process where a large language\nmodel (LLM) retrieves useful information from a database and then generates the\nresponses. It is becoming popular in enterprise settings for daily business\noperations. For example, Copilot for Microsoft 365 has accumulated millions of\nbusinesses. However, the security implications of adopting such RAG-based\nsystems are unclear.\n  In this paper, we introduce ConfusedPilot, a class of security\nvulnerabilities of RAG systems that confuse Copilot and cause integrity and\nconfidentiality violations in its responses. First, we investigate a\nvulnerability that embeds malicious text in the modified prompt in RAG,\ncorrupting the responses generated by the LLM. Second, we demonstrate a\nvulnerability that leaks secret data, which leverages the caching mechanism\nduring retrieval. Third, we investigate how both vulnerabilities can be\nexploited to propagate misinformation within the enterprise and ultimately\nimpact its operations, such as sales and manufacturing. We also discuss the\nroot cause of these attacks by investigating the architecture of a RAG-based\nsystem. This study highlights the security vulnerabilities in today's RAG-based\nsystems and proposes design guidelines to secure future RAG-based systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Retrieval augmented generation (RAG) is a process where a large language\nmodel (LLM) retrieves useful information from a database and then generates the\nresponses. It is becoming popular in enterprise settings for daily business\noperations. For example, Copilot for Microsoft 365 has accumulated millions of\nbusinesses. However, the security implications of adopting such RAG-based\nsystems are unclear.\n  In this paper, we introduce ConfusedPilot, a class of security\nvulnerabilities of RAG systems that confuse Copilot and cause integrity and\nconfidentiality violations in its responses. First, we investigate a\nvulnerability that embeds malicious text in the modified prompt in RAG,\ncorrupting the responses generated by the LLM. Second, we demonstrate a\nvulnerability that leaks secret data, which leverages the caching mechanism\nduring retrieval. Third, we investigate how both vulnerabilities can be\nexploited to propagate misinformation within the enterprise and ultimately\nimpact its operations, such as sales and manufacturing. We also discuss the\nroot cause of these attacks by investigating the architecture of a RAG-based\nsystem. This study highlights the security vulnerabilities in today's RAG-based\nsystems and proposes design guidelines to secure future RAG-based systems."
                },
                "authors": [
                    {
                        "name": "Ayush RoyChowdhury"
                    },
                    {
                        "name": "Mulong Luo"
                    },
                    {
                        "name": "Prateek Sahu"
                    },
                    {
                        "name": "Sarbartha Banerjee"
                    },
                    {
                        "name": "Mohit Tiwari"
                    }
                ],
                "author_detail": {
                    "name": "Mohit Tiwari"
                },
                "author": "Mohit Tiwari",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.04870v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.04870v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.14740v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.14740v2",
                "updated": "2024-10-23T01:08:59Z",
                "updated_parsed": [
                    2024,
                    10,
                    23,
                    1,
                    8,
                    59,
                    2,
                    297,
                    0
                ],
                "published": "2024-10-17T08:33:39Z",
                "published_parsed": [
                    2024,
                    10,
                    17,
                    8,
                    33,
                    39,
                    3,
                    291,
                    0
                ],
                "title": "Harnessing Your DRAM and SSD for Sustainable and Accessible LLM\n  Inference with Mixed-Precision and Multi-level Caching",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Harnessing Your DRAM and SSD for Sustainable and Accessible LLM\n  Inference with Mixed-Precision and Multi-level Caching"
                },
                "summary": "Although Large Language Models (LLMs) have demonstrated remarkable\ncapabilities, their massive parameter counts and associated extensive computing\nmake LLMs' deployment the main part of carbon emission from nowadays AI\napplications. Compared to modern GPUs like H$100$, it would be significantly\ncarbon-sustainable if we could leverage old-fashioned GPUs such as M$40$ (as\nshown in Figure 1, M$40$ only has one third carbon emission of H$100$'s) for\nLLM servings. However, the limited High Bandwidth Memory (HBM) available on\nsuch GPU often cannot support the loading of LLMs due to the gigantic model\nsize and intermediate activation data, making their serving challenging. For\ninstance, a LLaMA2 model with $70$B parameters typically requires $128$GB for\ninference, which substantially surpasses $24$GB HBM in a $3090$ GPU and remains\ninfeasible even considering the additional $64$GB DRAM. To address this\nchallenge, this paper proposes a mixed-precision with a model modularization\nalgorithm to enable LLM inference on outdated hardware with resource\nconstraints. (The precision denotes the numerical precision like FP16, INT8,\nINT4) and multi-level caching (M2Cache).)\n  Specifically, our M2Cache first modulizes neurons in LLM and creates their\nimportance ranking. Then, it adopts a dynamic sparse mixed-precision\nquantization mechanism in weight space to reduce computational demands and\ncommunication overhead at each decoding step. It collectively lowers the\noperational carbon emissions associated with LLM inference. Moreover, M2Cache\nintroduces a three-level cache management system with HBM, DRAM, and SSDs that\ncomplements the dynamic sparse mixed-precision inference. To enhance\ncommunication efficiency, M2Cache maintains a neuron-level mixed-precision LRU\ncache in HBM, a larger layer-aware cache in DRAM, and a full model in SSD.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Although Large Language Models (LLMs) have demonstrated remarkable\ncapabilities, their massive parameter counts and associated extensive computing\nmake LLMs' deployment the main part of carbon emission from nowadays AI\napplications. Compared to modern GPUs like H$100$, it would be significantly\ncarbon-sustainable if we could leverage old-fashioned GPUs such as M$40$ (as\nshown in Figure 1, M$40$ only has one third carbon emission of H$100$'s) for\nLLM servings. However, the limited High Bandwidth Memory (HBM) available on\nsuch GPU often cannot support the loading of LLMs due to the gigantic model\nsize and intermediate activation data, making their serving challenging. For\ninstance, a LLaMA2 model with $70$B parameters typically requires $128$GB for\ninference, which substantially surpasses $24$GB HBM in a $3090$ GPU and remains\ninfeasible even considering the additional $64$GB DRAM. To address this\nchallenge, this paper proposes a mixed-precision with a model modularization\nalgorithm to enable LLM inference on outdated hardware with resource\nconstraints. (The precision denotes the numerical precision like FP16, INT8,\nINT4) and multi-level caching (M2Cache).)\n  Specifically, our M2Cache first modulizes neurons in LLM and creates their\nimportance ranking. Then, it adopts a dynamic sparse mixed-precision\nquantization mechanism in weight space to reduce computational demands and\ncommunication overhead at each decoding step. It collectively lowers the\noperational carbon emissions associated with LLM inference. Moreover, M2Cache\nintroduces a three-level cache management system with HBM, DRAM, and SSDs that\ncomplements the dynamic sparse mixed-precision inference. To enhance\ncommunication efficiency, M2Cache maintains a neuron-level mixed-precision LRU\ncache in HBM, a larger layer-aware cache in DRAM, and a full model in SSD."
                },
                "authors": [
                    {
                        "name": "Jie Peng"
                    },
                    {
                        "name": "Zhang Cao"
                    },
                    {
                        "name": "Huaizhi Qu"
                    },
                    {
                        "name": "Zhengyu Zhang"
                    },
                    {
                        "name": "Chang Guo"
                    },
                    {
                        "name": "Yanyong Zhang"
                    },
                    {
                        "name": "Zhichao Cao"
                    },
                    {
                        "name": "Tianlong Chen"
                    }
                ],
                "author_detail": {
                    "name": "Tianlong Chen"
                },
                "author": "Tianlong Chen",
                "arxiv_comment": "24 pages, 13 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.14740v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.14740v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.11724v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.11724v2",
                "updated": "2024-10-22T19:07:08Z",
                "updated_parsed": [
                    2024,
                    10,
                    22,
                    19,
                    7,
                    8,
                    1,
                    296,
                    0
                ],
                "published": "2024-05-20T01:57:34Z",
                "published_parsed": [
                    2024,
                    5,
                    20,
                    1,
                    57,
                    34,
                    0,
                    141,
                    0
                ],
                "title": "Token-wise Influential Training Data Retrieval for Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Token-wise Influential Training Data Retrieval for Large Language Models"
                },
                "summary": "Given a Large Language Model (LLM) generation, how can we identify which\ntraining data led to this generation? In this paper, we proposed RapidIn, a\nscalable framework adapting to LLMs for estimating the influence of each\ntraining data. The proposed framework consists of two stages: caching and\nretrieval. First, we compress the gradient vectors by over 200,000x, allowing\nthem to be cached on disk or in GPU/CPU memory. Then, given a generation,\nRapidIn efficiently traverses the cached gradients to estimate the influence\nwithin minutes, achieving over a 6,326x speedup. Moreover, RapidIn supports\nmulti-GPU parallelization to substantially accelerate caching and retrieval.\nOur empirical result confirms the efficiency and effectiveness of RapidIn.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Given a Large Language Model (LLM) generation, how can we identify which\ntraining data led to this generation? In this paper, we proposed RapidIn, a\nscalable framework adapting to LLMs for estimating the influence of each\ntraining data. The proposed framework consists of two stages: caching and\nretrieval. First, we compress the gradient vectors by over 200,000x, allowing\nthem to be cached on disk or in GPU/CPU memory. Then, given a generation,\nRapidIn efficiently traverses the cached gradients to estimate the influence\nwithin minutes, achieving over a 6,326x speedup. Moreover, RapidIn supports\nmulti-GPU parallelization to substantially accelerate caching and retrieval.\nOur empirical result confirms the efficiency and effectiveness of RapidIn."
                },
                "authors": [
                    {
                        "name": "Huawei Lin"
                    },
                    {
                        "name": "Jikai Long"
                    },
                    {
                        "name": "Zhaozhuo Xu"
                    },
                    {
                        "name": "Weijie Zhao"
                    }
                ],
                "author_detail": {
                    "name": "Weijie Zhao"
                },
                "author": "Weijie Zhao",
                "arxiv_comment": "Accepted to ACL 2024. Keywords: Influence Function, Influence\n  Estimation, Training Data Attribution",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.11724v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.11724v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.18400v6",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.18400v6",
                "updated": "2024-10-30T21:22:54Z",
                "updated_parsed": [
                    2024,
                    10,
                    30,
                    21,
                    22,
                    54,
                    2,
                    304,
                    0
                ],
                "published": "2024-05-28T17:40:48Z",
                "published_parsed": [
                    2024,
                    5,
                    28,
                    17,
                    40,
                    48,
                    1,
                    149,
                    0
                ],
                "title": "Superposed Decoding: Multiple Generations from a Single Autoregressive\n  Inference Pass",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Superposed Decoding: Multiple Generations from a Single Autoregressive\n  Inference Pass"
                },
                "summary": "Many applications today provide users with multiple auto-complete drafts as\nthey type, including GitHub's code completion, Gmail's smart compose, and\nApple's messaging auto-suggestions. Under the hood, language models support\nthis by running an autoregressive inference pass to provide a draft.\nConsequently, providing $k$ drafts to the user requires running an expensive\nlanguage model $k$ times. To alleviate the computation cost of running $k$\ninference passes, we propose Superposed Decoding, a new decoding algorithm that\ngenerates $k$ drafts at the computation cost of one autoregressive inference\npass. We achieve this by feeding a superposition of the most recent token\nembeddings from the $k$ drafts as input to the next decoding step of the\nlanguage model. At every inference step we combine the $k$ drafts with the\ntop-$k$ tokens to get $k^2$ new drafts and cache the $k$ most likely options,\nusing an n-gram interpolation with minimal compute overhead to filter out\nincoherent generations. Our experiments show that $k$ drafts from Superposed\nDecoding are at least as coherent and factual as Nucleus Sampling and Greedy\nDecoding respectively, while being at least $2.44\\times$ faster for $k\\ge3$. In\na compute-normalized setting, user evaluations demonstrably favor text\ngenerated by Superposed Decoding over Nucleus Sampling. Superposed Decoding can\nalso be combined with other decoding strategies, resulting in universal\ncoverage gains when scaling inference time compute. Code and more examples\nopen-sourced at https://github.com/RAIVNLab/SuperposedDecoding.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Many applications today provide users with multiple auto-complete drafts as\nthey type, including GitHub's code completion, Gmail's smart compose, and\nApple's messaging auto-suggestions. Under the hood, language models support\nthis by running an autoregressive inference pass to provide a draft.\nConsequently, providing $k$ drafts to the user requires running an expensive\nlanguage model $k$ times. To alleviate the computation cost of running $k$\ninference passes, we propose Superposed Decoding, a new decoding algorithm that\ngenerates $k$ drafts at the computation cost of one autoregressive inference\npass. We achieve this by feeding a superposition of the most recent token\nembeddings from the $k$ drafts as input to the next decoding step of the\nlanguage model. At every inference step we combine the $k$ drafts with the\ntop-$k$ tokens to get $k^2$ new drafts and cache the $k$ most likely options,\nusing an n-gram interpolation with minimal compute overhead to filter out\nincoherent generations. Our experiments show that $k$ drafts from Superposed\nDecoding are at least as coherent and factual as Nucleus Sampling and Greedy\nDecoding respectively, while being at least $2.44\\times$ faster for $k\\ge3$. In\na compute-normalized setting, user evaluations demonstrably favor text\ngenerated by Superposed Decoding over Nucleus Sampling. Superposed Decoding can\nalso be combined with other decoding strategies, resulting in universal\ncoverage gains when scaling inference time compute. Code and more examples\nopen-sourced at https://github.com/RAIVNLab/SuperposedDecoding."
                },
                "authors": [
                    {
                        "name": "Ethan Shen"
                    },
                    {
                        "name": "Alan Fan"
                    },
                    {
                        "name": "Sarah M. Pratt"
                    },
                    {
                        "name": "Jae Sung Park"
                    },
                    {
                        "name": "Matthew Wallingford"
                    },
                    {
                        "name": "Sham M. Kakade"
                    },
                    {
                        "name": "Ari Holtzman"
                    },
                    {
                        "name": "Ranjay Krishna"
                    },
                    {
                        "name": "Ali Farhadi"
                    },
                    {
                        "name": "Aditya Kusupati"
                    }
                ],
                "author_detail": {
                    "name": "Aditya Kusupati"
                },
                "author": "Aditya Kusupati",
                "arxiv_comment": "23 pages, 16 figures, accepted at NeurIPS 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.18400v6",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.18400v6",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.16218v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.16218v1",
                "updated": "2024-10-21T17:23:03Z",
                "updated_parsed": [
                    2024,
                    10,
                    21,
                    17,
                    23,
                    3,
                    0,
                    295,
                    0
                ],
                "published": "2024-10-21T17:23:03Z",
                "published_parsed": [
                    2024,
                    10,
                    21,
                    17,
                    23,
                    3,
                    0,
                    295,
                    0
                ],
                "title": "3 kV Monolithic Bidirectional GaN HEMT on Sapphire",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "3 kV Monolithic Bidirectional GaN HEMT on Sapphire"
                },
                "summary": "More than 3 kV breakdown voltage was demonstrated in monolithic bidirectional\nGaN HEMTs for the first time having potential applications in 1200V or\n1700V-class novel power converters. The on resistance of the fabricated\ntransistors was ~20 ohm.mm or ~11 mili ohm.cm^2. Breakdown voltage was\noptimized by utilizing two field plates in either side of the transistor and\noptimizing their geometry. Shorter first field plate lengths (less than 2\nmicron) resulted in higher breakdown voltage and the possible reason for this\nwas discussed. The transistors had a steep subthreshold swing of 92 mV / dec.\nThe on/off ratio was greater than 10^5 and it was limited by the tool capacity.\nThe fabricated 3 kV transistor was benchmarked against the state-of-the-art\nmonolithic bidirectional GaN HEMTs in the performance matrices of breakdown\nvoltage and on resistance, that showed crucial progress.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "More than 3 kV breakdown voltage was demonstrated in monolithic bidirectional\nGaN HEMTs for the first time having potential applications in 1200V or\n1700V-class novel power converters. The on resistance of the fabricated\ntransistors was ~20 ohm.mm or ~11 mili ohm.cm^2. Breakdown voltage was\noptimized by utilizing two field plates in either side of the transistor and\noptimizing their geometry. Shorter first field plate lengths (less than 2\nmicron) resulted in higher breakdown voltage and the possible reason for this\nwas discussed. The transistors had a steep subthreshold swing of 92 mV / dec.\nThe on/off ratio was greater than 10^5 and it was limited by the tool capacity.\nThe fabricated 3 kV transistor was benchmarked against the state-of-the-art\nmonolithic bidirectional GaN HEMTs in the performance matrices of breakdown\nvoltage and on resistance, that showed crucial progress."
                },
                "authors": [
                    {
                        "name": "Md Tahmidul Alam"
                    },
                    {
                        "name": "Swarnav Mukhopadhyay"
                    },
                    {
                        "name": "Md Mobinul Haque"
                    },
                    {
                        "name": "Shubhra S. Pasayat"
                    },
                    {
                        "name": "Chirag Gupta"
                    }
                ],
                "author_detail": {
                    "name": "Chirag Gupta"
                },
                "author": "Chirag Gupta",
                "arxiv_comment": "4 pages, 7 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.16218v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.16218v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.app-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.app-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.13761v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.13761v2",
                "updated": "2024-10-21T15:59:18Z",
                "updated_parsed": [
                    2024,
                    10,
                    21,
                    15,
                    59,
                    18,
                    0,
                    295,
                    0
                ],
                "published": "2024-09-16T18:46:24Z",
                "published_parsed": [
                    2024,
                    9,
                    16,
                    18,
                    46,
                    24,
                    0,
                    260,
                    0
                ],
                "title": "Do Large Language Models Need a Content Delivery Network?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Do Large Language Models Need a Content Delivery Network?"
                },
                "summary": "As the use of large language models (LLMs) expands rapidly, so does the range\nof knowledge needed to supplement various LLM queries. Thus, enabling flexible\nand efficient injection of new knowledge in LLM inference is critical. Three\nhigh-level options exist: (i) embedding the knowledge in LLM's weights (i.e.,\nfine-tuning), (ii) including the knowledge as a part of LLM's text input (i.e.,\nin-context learning), or (iii) injecting the KV caches of the new knowledge to\nLLM during prefill. This paper argues that, although fine-tuning and in-context\nlearning are popular, using KV caches as the medium of knowledge could\nsimultaneously enable more modular management of knowledge injection and more\nefficient LLM serving with low cost and fast response. To realize these\nbenefits, we envision a Knowledge Delivery Network (KDN), a new system\ncomponent in LLM services that dynamically optimizes the storage, transfer, and\ncomposition of KV cache across LLM engines and other compute and storage\nresources. We believe that, just like content delivery networks (CDNs), such as\nAkamai, enabled the success of the Internet ecosystem through their efficient\ndata delivery, KDNs will be critical to the success of LLM applications through\ntheir efficient knowledge delivery. We have open-sourced a KDN prototype at\nhttps://github.com/LMCache/LMCache.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As the use of large language models (LLMs) expands rapidly, so does the range\nof knowledge needed to supplement various LLM queries. Thus, enabling flexible\nand efficient injection of new knowledge in LLM inference is critical. Three\nhigh-level options exist: (i) embedding the knowledge in LLM's weights (i.e.,\nfine-tuning), (ii) including the knowledge as a part of LLM's text input (i.e.,\nin-context learning), or (iii) injecting the KV caches of the new knowledge to\nLLM during prefill. This paper argues that, although fine-tuning and in-context\nlearning are popular, using KV caches as the medium of knowledge could\nsimultaneously enable more modular management of knowledge injection and more\nefficient LLM serving with low cost and fast response. To realize these\nbenefits, we envision a Knowledge Delivery Network (KDN), a new system\ncomponent in LLM services that dynamically optimizes the storage, transfer, and\ncomposition of KV cache across LLM engines and other compute and storage\nresources. We believe that, just like content delivery networks (CDNs), such as\nAkamai, enabled the success of the Internet ecosystem through their efficient\ndata delivery, KDNs will be critical to the success of LLM applications through\ntheir efficient knowledge delivery. We have open-sourced a KDN prototype at\nhttps://github.com/LMCache/LMCache."
                },
                "authors": [
                    {
                        "name": "Yihua Cheng"
                    },
                    {
                        "name": "Kuntai Du"
                    },
                    {
                        "name": "Jiayi Yao"
                    },
                    {
                        "name": "Junchen Jiang"
                    }
                ],
                "author_detail": {
                    "name": "Junchen Jiang"
                },
                "author": "Junchen Jiang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.13761v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.13761v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.15908v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.15908v1",
                "updated": "2024-10-21T11:29:49Z",
                "updated_parsed": [
                    2024,
                    10,
                    21,
                    11,
                    29,
                    49,
                    0,
                    295,
                    0
                ],
                "published": "2024-10-21T11:29:49Z",
                "published_parsed": [
                    2024,
                    10,
                    21,
                    11,
                    29,
                    49,
                    0,
                    295,
                    0
                ],
                "title": "Formalising CXL Cache Coherence",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Formalising CXL Cache Coherence"
                },
                "summary": "We report our experience formally modelling and verifying CXL.cache, the\ninter-device cache coherence protocol of the Compute Express Link standard. We\nhave used the Isabelle proof assistant to create a formal model for CXL.cache\nbased on the prose English specification. This led to us identifying and\nproposing fixes to several problems we identified as unclear, ambiguous or\ninaccurate, some of which could lead to incoherence if left unfixed. Nearly all\nour issues and proposed fixes have been confirmed and tentatively accepted by\nthe CXL consortium for adoption, save for one which is still under discussion.\nTo validate the faithfulness of our model we performed scenario verification of\nessential restrictions such as \"Snoop-pushes-GO\", and produced a fully\nmechanised proof of a coherence property of the model. The considerable size of\nthis proof, comprising tens of thousands of lemmas, prompted us to develop new\nproof automation tools, which we have made available for other Isabelle users\nworking with similarly cumbersome proofs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We report our experience formally modelling and verifying CXL.cache, the\ninter-device cache coherence protocol of the Compute Express Link standard. We\nhave used the Isabelle proof assistant to create a formal model for CXL.cache\nbased on the prose English specification. This led to us identifying and\nproposing fixes to several problems we identified as unclear, ambiguous or\ninaccurate, some of which could lead to incoherence if left unfixed. Nearly all\nour issues and proposed fixes have been confirmed and tentatively accepted by\nthe CXL consortium for adoption, save for one which is still under discussion.\nTo validate the faithfulness of our model we performed scenario verification of\nessential restrictions such as \"Snoop-pushes-GO\", and produced a fully\nmechanised proof of a coherence property of the model. The considerable size of\nthis proof, comprising tens of thousands of lemmas, prompted us to develop new\nproof automation tools, which we have made available for other Isabelle users\nworking with similarly cumbersome proofs."
                },
                "authors": [
                    {
                        "name": "Chengsong Tan"
                    },
                    {
                        "name": "Alastair F. Donaldson"
                    },
                    {
                        "name": "John Wickerson"
                    }
                ],
                "author_detail": {
                    "name": "John Wickerson"
                },
                "author": "John Wickerson",
                "arxiv_comment": "12 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.15908v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.15908v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.14142v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.14142v2",
                "updated": "2024-10-21T07:24:53Z",
                "updated_parsed": [
                    2024,
                    10,
                    21,
                    7,
                    24,
                    53,
                    0,
                    295,
                    0
                ],
                "published": "2024-10-18T03:30:25Z",
                "published_parsed": [
                    2024,
                    10,
                    18,
                    3,
                    30,
                    25,
                    4,
                    292,
                    0
                ],
                "title": "Secure Collaborative Computation Offloading and Resource Allocation in\n  Cache-Assisted Ultra-Dense IoT Networks With Multi-Slope Channels",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Secure Collaborative Computation Offloading and Resource Allocation in\n  Cache-Assisted Ultra-Dense IoT Networks With Multi-Slope Channels"
                },
                "summary": "Cache-assisted ultra-dense mobile edge computing (MEC) networks are a\npromising solution for meeting the increasing demands of numerous\nInternet-of-Things mobile devices (IMDs). To address the complex interferences\ncaused by small base stations (SBSs) deployed densely in such networks, this\npaper explores the combination of orthogonal frequency division multiple access\n(OFDMA), non-orthogonal multiple access (NOMA), and base station (BS)\nclustering. Additionally, security measures are introduced to protect IMDs'\ntasks offloaded to BSs from potential eavesdropping and malicious attacks. As\nfor such a network framework, a computation offloading scheme is proposed to\nminimize IMDs' energy consumption while considering constraints such as delay,\npower, computing resources, and security costs, optimizing channel selections,\ntask execution decisions, device associations, power controls, security service\nassignments, and computing resource allocations. To solve the formulated\nproblem efficiently, we develop a further improved hierarchical adaptive search\n(FIHAS) algorithm, giving some insights into its parallel implementation,\ncomputation complexity, and convergence. Simulation results demonstrate that\nthe proposed algorithms can achieve lower total energy consumption and delay\ncompared to other algorithms when strict latency and cost constraints are\nimposed.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cache-assisted ultra-dense mobile edge computing (MEC) networks are a\npromising solution for meeting the increasing demands of numerous\nInternet-of-Things mobile devices (IMDs). To address the complex interferences\ncaused by small base stations (SBSs) deployed densely in such networks, this\npaper explores the combination of orthogonal frequency division multiple access\n(OFDMA), non-orthogonal multiple access (NOMA), and base station (BS)\nclustering. Additionally, security measures are introduced to protect IMDs'\ntasks offloaded to BSs from potential eavesdropping and malicious attacks. As\nfor such a network framework, a computation offloading scheme is proposed to\nminimize IMDs' energy consumption while considering constraints such as delay,\npower, computing resources, and security costs, optimizing channel selections,\ntask execution decisions, device associations, power controls, security service\nassignments, and computing resource allocations. To solve the formulated\nproblem efficiently, we develop a further improved hierarchical adaptive search\n(FIHAS) algorithm, giving some insights into its parallel implementation,\ncomputation complexity, and convergence. Simulation results demonstrate that\nthe proposed algorithms can achieve lower total energy consumption and delay\ncompared to other algorithms when strict latency and cost constraints are\nimposed."
                },
                "authors": [
                    {
                        "name": "Tianqing Zhou"
                    },
                    {
                        "name": "Bobo Wang"
                    },
                    {
                        "name": "Dong Qin"
                    },
                    {
                        "name": "Xuefang Nie"
                    },
                    {
                        "name": "Nan Jiang"
                    },
                    {
                        "name": "Chunguo Li"
                    }
                ],
                "author_detail": {
                    "name": "Chunguo Li"
                },
                "author": "Chunguo Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.14142v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.14142v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IT",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.15704v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.15704v1",
                "updated": "2024-10-21T07:20:41Z",
                "updated_parsed": [
                    2024,
                    10,
                    21,
                    7,
                    20,
                    41,
                    0,
                    295,
                    0
                ],
                "published": "2024-10-21T07:20:41Z",
                "published_parsed": [
                    2024,
                    10,
                    21,
                    7,
                    20,
                    41,
                    0,
                    295,
                    0
                ],
                "title": "Residual vector quantization for KV cache compression in large language\n  model",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Residual vector quantization for KV cache compression in large language\n  model"
                },
                "summary": "KV cache compression methods have mainly relied on scalar quantization\ntechniques to reduce the memory requirements during decoding. In this work, we\napply residual vector quantization, which has been widely used for high\nfidelity audio compression, to compress KV cache in large language models\n(LLM). We adapt the standard recipe with minimal changes to compress the output\nof any key or value projection matrix in a pretrained LLM: we scale the vector\nby its standard deviation, divide channels into groups and then quantize each\ngroup with the same residual vector quantizer. We learn the codebook using\nexponential moving average and there are no other learnable parameters\nincluding the input and output projections normally used in a vector\nquantization set up. We find that a residual depth of 8 recovers most of the\nperformance of the unquantized model. We also find that grouping non-contiguous\nchannels together works better than grouping contiguous channels for\ncompressing key matrix and the method further benefits from a light weight\nfinetuning of LLM together with the quantization. Overall, the proposed\ntechnique is competitive with existing quantization methods while being much\nsimpler and results in 5.5x compression compared to half precision.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "KV cache compression methods have mainly relied on scalar quantization\ntechniques to reduce the memory requirements during decoding. In this work, we\napply residual vector quantization, which has been widely used for high\nfidelity audio compression, to compress KV cache in large language models\n(LLM). We adapt the standard recipe with minimal changes to compress the output\nof any key or value projection matrix in a pretrained LLM: we scale the vector\nby its standard deviation, divide channels into groups and then quantize each\ngroup with the same residual vector quantizer. We learn the codebook using\nexponential moving average and there are no other learnable parameters\nincluding the input and output projections normally used in a vector\nquantization set up. We find that a residual depth of 8 recovers most of the\nperformance of the unquantized model. We also find that grouping non-contiguous\nchannels together works better than grouping contiguous channels for\ncompressing key matrix and the method further benefits from a light weight\nfinetuning of LLM together with the quantization. Overall, the proposed\ntechnique is competitive with existing quantization methods while being much\nsimpler and results in 5.5x compression compared to half precision."
                },
                "authors": [
                    {
                        "name": "Ankur Kumar"
                    }
                ],
                "author_detail": {
                    "name": "Ankur Kumar"
                },
                "author": "Ankur Kumar",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.15704v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.15704v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.16546v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.16546v2",
                "updated": "2024-10-21T05:06:01Z",
                "updated_parsed": [
                    2024,
                    10,
                    21,
                    5,
                    6,
                    1,
                    0,
                    295,
                    0
                ],
                "published": "2024-09-25T01:39:02Z",
                "published_parsed": [
                    2024,
                    9,
                    25,
                    1,
                    39,
                    2,
                    2,
                    269,
                    0
                ],
                "title": "AlignedKV: Reducing Memory Access of KV-Cache with Precision-Aligned\n  Quantization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AlignedKV: Reducing Memory Access of KV-Cache with Precision-Aligned\n  Quantization"
                },
                "summary": "Model quantization has become a crucial technique to address the issues of\nlarge memory consumption and long inference times associated with LLMs.\nMixed-precision quantization, which distinguishes between important and\nunimportant parameters, stands out among numerous quantization schemes as it\nachieves a balance between precision and compression rate. However, existing\napproaches can only identify important parameters through qualitative analysis\nand manual experiments without quantitatively analyzing how their importance is\ndetermined. We propose a new criterion, so-called 'precision alignment', to\nbuild a quantitative framework to holistically evaluate the importance of\nparameters in mixed-precision quantization. Our observations on floating point\naddition under various real-world scenarios suggest that two addends should\nhave identical precision, otherwise the information in the higher-precision\nnumber will be wasted. Such an observation offers an essential principle to\ndetermine the precision of each parameter in matrix multiplication operation.\nAs the first step towards applying the above discovery to large model\ninference, we develop a dynamic KV-Cache quantization technique to effectively\nreduce memory access latency. Different from existing quantization approaches\nthat focus on memory saving, this work directly aims to accelerate LLM\ninference through quantifying floating numbers. The proposed technique attains\na 25% saving of memory access and delivers up to 1.3x speedup in the\ncomputation of attention in the decoding phase of LLM, with almost no loss of\nprecision.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Model quantization has become a crucial technique to address the issues of\nlarge memory consumption and long inference times associated with LLMs.\nMixed-precision quantization, which distinguishes between important and\nunimportant parameters, stands out among numerous quantization schemes as it\nachieves a balance between precision and compression rate. However, existing\napproaches can only identify important parameters through qualitative analysis\nand manual experiments without quantitatively analyzing how their importance is\ndetermined. We propose a new criterion, so-called 'precision alignment', to\nbuild a quantitative framework to holistically evaluate the importance of\nparameters in mixed-precision quantization. Our observations on floating point\naddition under various real-world scenarios suggest that two addends should\nhave identical precision, otherwise the information in the higher-precision\nnumber will be wasted. Such an observation offers an essential principle to\ndetermine the precision of each parameter in matrix multiplication operation.\nAs the first step towards applying the above discovery to large model\ninference, we develop a dynamic KV-Cache quantization technique to effectively\nreduce memory access latency. Different from existing quantization approaches\nthat focus on memory saving, this work directly aims to accelerate LLM\ninference through quantifying floating numbers. The proposed technique attains\na 25% saving of memory access and delivers up to 1.3x speedup in the\ncomputation of attention in the decoding phase of LLM, with almost no loss of\nprecision."
                },
                "authors": [
                    {
                        "name": "Yifan Tan"
                    },
                    {
                        "name": "Haoze Wang"
                    },
                    {
                        "name": "Chao Yan"
                    },
                    {
                        "name": "Yangdong Deng"
                    }
                ],
                "author_detail": {
                    "name": "Yangdong Deng"
                },
                "author": "Yangdong Deng",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.16546v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.16546v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.09202v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.09202v2",
                "updated": "2024-10-21T02:35:08Z",
                "updated_parsed": [
                    2024,
                    10,
                    21,
                    2,
                    35,
                    8,
                    0,
                    295,
                    0
                ],
                "published": "2024-09-13T21:31:45Z",
                "published_parsed": [
                    2024,
                    9,
                    13,
                    21,
                    31,
                    45,
                    4,
                    257,
                    0
                ],
                "title": "WarmSwap: Sharing Dependencies for Accelerating Cold Starts in\n  Serverless Functions",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "WarmSwap: Sharing Dependencies for Accelerating Cold Starts in\n  Serverless Functions"
                },
                "summary": "This work presents WarmSwap, a novel provider-side cold-start optimization\nfor serverless computing. This optimization reduces cold-start time when\nbooting and loading dependencies at runtime inside a function container.\nPrevious approaches to the optimization of cold starts tend to fall into two\ncategories: optimizing the infrastructure of serverless computing to benefit\nall serverless functions; or function-specific tuning for individual serverless\nfunctions. In contrast, WarmSwap offers a broad middle ground, which optimizes\nentire categories of serverless functions. WarmSwap eliminates the need to\ninitialize middleware or software dependencies when launching a new serverless\ncontainer, by migrating a pre-initialized live dependency image to the new\nfunction instance. WarmSwap respects the provider's cache constraints, as a\nsingle pre-warmed dependency image in the cache is shared among all serverless\nfunctions requiring that software dependency image. WarmSwap has been tested on\nseven representative functions from FunctionBench. In those tests, WarmSwap\naccelerates dependency loading for serverless functions with large dependency\nrequirements by a factor ranging from 2.2 to 3.2. Simulation experiments using\nAzure traces indicate that WarmSwap can save 88\\% of optimization space when\nsharing a dependency image among ten different functions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This work presents WarmSwap, a novel provider-side cold-start optimization\nfor serverless computing. This optimization reduces cold-start time when\nbooting and loading dependencies at runtime inside a function container.\nPrevious approaches to the optimization of cold starts tend to fall into two\ncategories: optimizing the infrastructure of serverless computing to benefit\nall serverless functions; or function-specific tuning for individual serverless\nfunctions. In contrast, WarmSwap offers a broad middle ground, which optimizes\nentire categories of serverless functions. WarmSwap eliminates the need to\ninitialize middleware or software dependencies when launching a new serverless\ncontainer, by migrating a pre-initialized live dependency image to the new\nfunction instance. WarmSwap respects the provider's cache constraints, as a\nsingle pre-warmed dependency image in the cache is shared among all serverless\nfunctions requiring that software dependency image. WarmSwap has been tested on\nseven representative functions from FunctionBench. In those tests, WarmSwap\naccelerates dependency loading for serverless functions with large dependency\nrequirements by a factor ranging from 2.2 to 3.2. Simulation experiments using\nAzure traces indicate that WarmSwap can save 88\\% of optimization space when\nsharing a dependency image among ten different functions."
                },
                "authors": [
                    {
                        "name": "Rui Li"
                    },
                    {
                        "name": "Devesh Tiwari"
                    },
                    {
                        "name": "Gene Cooperman"
                    }
                ],
                "author_detail": {
                    "name": "Gene Cooperman"
                },
                "author": "Gene Cooperman",
                "arxiv_comment": "15 pages, 7 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.09202v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.09202v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.04053v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.04053v2",
                "updated": "2024-10-20T13:37:46Z",
                "updated_parsed": [
                    2024,
                    10,
                    20,
                    13,
                    37,
                    46,
                    6,
                    294,
                    0
                ],
                "published": "2024-07-04T16:51:17Z",
                "published_parsed": [
                    2024,
                    7,
                    4,
                    16,
                    51,
                    17,
                    3,
                    186,
                    0
                ],
                "title": "Edge AI: A Taxonomy, Systematic Review and Future Directions",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Edge AI: A Taxonomy, Systematic Review and Future Directions"
                },
                "summary": "Edge Artificial Intelligence (AI) incorporates a network of interconnected\nsystems and devices that receive, cache, process, and analyze data in close\ncommunication with the location where the data is captured with AI technology.\nRecent advancements in AI efficiency, the widespread use of Internet of Things\n(IoT) devices, and the emergence of edge computing have unlocked the enormous\nscope of Edge AI. Edge AI aims to optimize data processing efficiency and\nvelocity while ensuring data confidentiality and integrity. Despite being a\nrelatively new field of research from 2014 to the present, it has shown\nsignificant and rapid development over the last five years. This article\npresents a systematic literature review for Edge AI to discuss the existing\nresearch, recent advancements, and future research directions. We created a\ncollaborative edge AI learning system for cloud and edge computing analysis,\nincluding an in-depth study of the architectures that facilitate this\nmechanism. The taxonomy for Edge AI facilitates the classification and\nconfiguration of Edge AI systems while examining its potential influence across\nmany fields through compassing infrastructure, cloud computing, fog computing,\nservices, use cases, ML and deep learning, and resource management. This study\nhighlights the significance of Edge AI in processing real-time data at the edge\nof the network. Additionally, it emphasizes the research challenges encountered\nby Edge AI systems, including constraints on resources, vulnerabilities to\nsecurity threats, and problems with scalability. Finally, this study highlights\nthe potential future research directions that aim to address the current\nlimitations of Edge AI by providing innovative solutions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Edge Artificial Intelligence (AI) incorporates a network of interconnected\nsystems and devices that receive, cache, process, and analyze data in close\ncommunication with the location where the data is captured with AI technology.\nRecent advancements in AI efficiency, the widespread use of Internet of Things\n(IoT) devices, and the emergence of edge computing have unlocked the enormous\nscope of Edge AI. Edge AI aims to optimize data processing efficiency and\nvelocity while ensuring data confidentiality and integrity. Despite being a\nrelatively new field of research from 2014 to the present, it has shown\nsignificant and rapid development over the last five years. This article\npresents a systematic literature review for Edge AI to discuss the existing\nresearch, recent advancements, and future research directions. We created a\ncollaborative edge AI learning system for cloud and edge computing analysis,\nincluding an in-depth study of the architectures that facilitate this\nmechanism. The taxonomy for Edge AI facilitates the classification and\nconfiguration of Edge AI systems while examining its potential influence across\nmany fields through compassing infrastructure, cloud computing, fog computing,\nservices, use cases, ML and deep learning, and resource management. This study\nhighlights the significance of Edge AI in processing real-time data at the edge\nof the network. Additionally, it emphasizes the research challenges encountered\nby Edge AI systems, including constraints on resources, vulnerabilities to\nsecurity threats, and problems with scalability. Finally, this study highlights\nthe potential future research directions that aim to address the current\nlimitations of Edge AI by providing innovative solutions."
                },
                "authors": [
                    {
                        "name": "Sukhpal Singh Gill"
                    },
                    {
                        "name": "Muhammed Golec"
                    },
                    {
                        "name": "Jianmin Hu"
                    },
                    {
                        "name": "Minxian Xu"
                    },
                    {
                        "name": "Junhui Du"
                    },
                    {
                        "name": "Huaming Wu"
                    },
                    {
                        "name": "Guneet Kaur Walia"
                    },
                    {
                        "name": "Subramaniam Subramanian Murugesan"
                    },
                    {
                        "name": "Babar Ali"
                    },
                    {
                        "name": "Mohit Kumar"
                    },
                    {
                        "name": "Kejiang Ye"
                    },
                    {
                        "name": "Prabal Verma"
                    },
                    {
                        "name": "Surendra Kumar"
                    },
                    {
                        "name": "Felix Cuadrado"
                    },
                    {
                        "name": "Steve Uhlig"
                    }
                ],
                "author_detail": {
                    "name": "Steve Uhlig"
                },
                "author": "Steve Uhlig",
                "arxiv_doi": "10.1007/s10586-024-04686-y",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1007/s10586-024-04686-y",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2407.04053v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.04053v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "Preprint Version Accepted for Publication in Springer Cluster\n  Computing, 2024",
                "arxiv_journal_ref": "Springer Cluster Computing, Volume 28, article number 18, pages\n  11953 - 11981, (2025)",
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.15344v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.15344v1",
                "updated": "2024-10-20T09:37:07Z",
                "updated_parsed": [
                    2024,
                    10,
                    20,
                    9,
                    37,
                    7,
                    6,
                    294,
                    0
                ],
                "published": "2024-10-20T09:37:07Z",
                "published_parsed": [
                    2024,
                    10,
                    20,
                    9,
                    37,
                    7,
                    6,
                    294,
                    0
                ],
                "title": "LLC Intra-set Write Balancing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLC Intra-set Write Balancing"
                },
                "summary": "The increasing use of Non-Volatile Memory (NVM) in computer architecture has\nbrought about new challenges, one of which is the write endurance problem.\nFrequent writes to a particular cache cell in NVM can lead to degradation of\nthe memory cell and reduce its lifespan. To solve this problem, we propose a\nsample-based blocking technique for the Last Level Cache (LLC). Our approach\ninvolves defining a threshold value and sampling a subset of cache sets. If the\nnumber of writes to a way in a sampled set exceeds the threshold, the way is\nblocked, and writes are redirected to other ways. We also maintain a history\nstructure to record the number of writes in a set and a PC-Table to use for\nblocking in unsampled sets. Based on blocking on sampled sets, variance of\nvalues stored in history is used to determine whether blocking had a positive\nimpact or not, and on this basis, value corresponding to instruction pointer is\nincremented or decremented. This value is later used for blocking in unsampled\nsets. Our results show that our approach significantly balances write traffic\nto the cache and improves the overall lifespan of the memory cells while having\nbetter performance to the base-line system. Our approach can also be applied to\nother cache hierarchies and NVM technologies to mitigate the problem of write\nendurance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The increasing use of Non-Volatile Memory (NVM) in computer architecture has\nbrought about new challenges, one of which is the write endurance problem.\nFrequent writes to a particular cache cell in NVM can lead to degradation of\nthe memory cell and reduce its lifespan. To solve this problem, we propose a\nsample-based blocking technique for the Last Level Cache (LLC). Our approach\ninvolves defining a threshold value and sampling a subset of cache sets. If the\nnumber of writes to a way in a sampled set exceeds the threshold, the way is\nblocked, and writes are redirected to other ways. We also maintain a history\nstructure to record the number of writes in a set and a PC-Table to use for\nblocking in unsampled sets. Based on blocking on sampled sets, variance of\nvalues stored in history is used to determine whether blocking had a positive\nimpact or not, and on this basis, value corresponding to instruction pointer is\nincremented or decremented. This value is later used for blocking in unsampled\nsets. Our results show that our approach significantly balances write traffic\nto the cache and improves the overall lifespan of the memory cells while having\nbetter performance to the base-line system. Our approach can also be applied to\nother cache hierarchies and NVM technologies to mitigate the problem of write\nendurance."
                },
                "authors": [
                    {
                        "name": "Keshav Krishna"
                    },
                    {
                        "name": "Ayush Verma"
                    }
                ],
                "author_detail": {
                    "name": "Ayush Verma"
                },
                "author": "Ayush Verma",
                "arxiv_comment": "11 pages, 7 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.15344v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.15344v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.15332v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.15332v1",
                "updated": "2024-10-20T08:42:29Z",
                "updated_parsed": [
                    2024,
                    10,
                    20,
                    8,
                    42,
                    29,
                    6,
                    294,
                    0
                ],
                "published": "2024-10-20T08:42:29Z",
                "published_parsed": [
                    2024,
                    10,
                    20,
                    8,
                    42,
                    29,
                    6,
                    294,
                    0
                ],
                "title": "EPIC: Efficient Position-Independent Context Caching for Serving Large\n  Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "EPIC: Efficient Position-Independent Context Caching for Serving Large\n  Language Models"
                },
                "summary": "Large Language Models (LLMs) are critical for a wide range of applications,\nbut serving them efficiently becomes increasingly challenging as inputs become\nmore complex. Context caching improves serving performance by exploiting\ninter-request dependency and reusing key-value (KV) cache across requests, thus\nimproving time-to-first-token (TTFT). However, existing prefix-based context\ncaching requires exact token prefix matches, limiting cache reuse in few-shot\nlearning, multi-document QA, or retrieval-augmented generation, where prefixes\nmay vary. In this paper, we present EPIC, an LLM serving system that introduces\nposition-independent context caching (PIC), enabling modular KV cache reuse\nregardless of token chunk position (or prefix). EPIC features two key designs:\nAttnLink, which leverages static attention sparsity to minimize recomputation\nfor accuracy recovery, and KVSplit, a customizable chunking method that\npreserves semantic coherence. Our experiments demonstrate that Epic delivers up\nto 8x improvements in TTFT and 7x throughput over existing systems, with\nnegligible or no accuracy loss. By addressing the limitations of traditional\ncaching approaches, Epic enables more scalable and efficient LLM inference.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) are critical for a wide range of applications,\nbut serving them efficiently becomes increasingly challenging as inputs become\nmore complex. Context caching improves serving performance by exploiting\ninter-request dependency and reusing key-value (KV) cache across requests, thus\nimproving time-to-first-token (TTFT). However, existing prefix-based context\ncaching requires exact token prefix matches, limiting cache reuse in few-shot\nlearning, multi-document QA, or retrieval-augmented generation, where prefixes\nmay vary. In this paper, we present EPIC, an LLM serving system that introduces\nposition-independent context caching (PIC), enabling modular KV cache reuse\nregardless of token chunk position (or prefix). EPIC features two key designs:\nAttnLink, which leverages static attention sparsity to minimize recomputation\nfor accuracy recovery, and KVSplit, a customizable chunking method that\npreserves semantic coherence. Our experiments demonstrate that Epic delivers up\nto 8x improvements in TTFT and 7x throughput over existing systems, with\nnegligible or no accuracy loss. By addressing the limitations of traditional\ncaching approaches, Epic enables more scalable and efficient LLM inference."
                },
                "authors": [
                    {
                        "name": "Junhao Hu"
                    },
                    {
                        "name": "Wenrui Huang"
                    },
                    {
                        "name": "Haoyi Wang"
                    },
                    {
                        "name": "Weidong Wang"
                    },
                    {
                        "name": "Tiancheng Hu"
                    },
                    {
                        "name": "Qin Zhang"
                    },
                    {
                        "name": "Hao Feng"
                    },
                    {
                        "name": "Xusheng Chen"
                    },
                    {
                        "name": "Yizhou Shan"
                    },
                    {
                        "name": "Tao Xie"
                    }
                ],
                "author_detail": {
                    "name": "Tao Xie"
                },
                "author": "Tao Xie",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.15332v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.15332v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.15252v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.15252v1",
                "updated": "2024-10-20T02:17:35Z",
                "updated_parsed": [
                    2024,
                    10,
                    20,
                    2,
                    17,
                    35,
                    6,
                    294,
                    0
                ],
                "published": "2024-10-20T02:17:35Z",
                "published_parsed": [
                    2024,
                    10,
                    20,
                    2,
                    17,
                    35,
                    6,
                    294,
                    0
                ],
                "title": "Lossless KV Cache Compression to 2%",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Lossless KV Cache Compression to 2%"
                },
                "summary": "Large language models have revolutionized data processing in numerous\ndomains, with their ability to handle extended context reasoning receiving\nnotable recognition. To speed up inference, maintaining a key-value (KV) cache\nmemory is essential. Nonetheless, the growing demands for KV cache memory\ncreate significant hurdles for efficient implementation. This work introduces a\nnovel architecture, Cross-Layer Latent Attention (CLLA), aimed at compressing\nthe KV cache to less than 2% of its original size while maintaining comparable\nperformance levels. CLLA integrates multiple aspects of KV cache compression,\nincluding attention head/dimension reduction, layer sharing, and quantization\ntechniques, into a cohesive framework. Our extensive experiments demonstrate\nthat CLLA achieves lossless performance on most tasks while utilizing minimal\nKV cache, marking a significant advancement in practical KV cache compression.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models have revolutionized data processing in numerous\ndomains, with their ability to handle extended context reasoning receiving\nnotable recognition. To speed up inference, maintaining a key-value (KV) cache\nmemory is essential. Nonetheless, the growing demands for KV cache memory\ncreate significant hurdles for efficient implementation. This work introduces a\nnovel architecture, Cross-Layer Latent Attention (CLLA), aimed at compressing\nthe KV cache to less than 2% of its original size while maintaining comparable\nperformance levels. CLLA integrates multiple aspects of KV cache compression,\nincluding attention head/dimension reduction, layer sharing, and quantization\ntechniques, into a cohesive framework. Our extensive experiments demonstrate\nthat CLLA achieves lossless performance on most tasks while utilizing minimal\nKV cache, marking a significant advancement in practical KV cache compression."
                },
                "authors": [
                    {
                        "name": "Zhen Yang"
                    },
                    {
                        "name": "J. N. Han"
                    },
                    {
                        "name": "Kan Wu"
                    },
                    {
                        "name": "Ruobing Xie"
                    },
                    {
                        "name": "An Wang"
                    },
                    {
                        "name": "Xingwu Sun"
                    },
                    {
                        "name": "Zhanhui Kang"
                    }
                ],
                "author_detail": {
                    "name": "Zhanhui Kang"
                },
                "author": "Zhanhui Kang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.15252v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.15252v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2206.05579v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2206.05579v4",
                "updated": "2024-10-19T12:15:50Z",
                "updated_parsed": [
                    2024,
                    10,
                    19,
                    12,
                    15,
                    50,
                    5,
                    293,
                    0
                ],
                "published": "2022-06-11T17:52:10Z",
                "published_parsed": [
                    2022,
                    6,
                    11,
                    17,
                    52,
                    10,
                    5,
                    162,
                    0
                ],
                "title": "Online Paging with Heterogeneous Cache Slots",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Online Paging with Heterogeneous Cache Slots"
                },
                "summary": "It is natural to generalize the online $k$-Server problem by allowing each\nrequest to specify not only a point $p$, but also a subset $S$ of servers that\nmay serve it. For uniform metrics, the problem is equivalent to a\ngeneralization of Paging in which each request specifies not only a page $p$,\nbut also a subset $S$ of cache slots, and is satisfied by having a copy of $p$\nin some slot in $S$. We call this problem Slot-Heterogenous Paging.\n  We parameterize the problem by specifying a family $\\mathcal S \\subseteq\n2^{[k]}$ of requestable slot sets, and we establish bounds on the competitive\nratio as a function of the cache size $k$ and family $\\mathcal S$:\n  - If all request sets are allowed ($\\mathcal\nS=2^{[k]}\\setminus\\{\\emptyset\\}$), the optimal deterministic and randomized\ncompetitive ratios are exponentially worse than for standard \\Paging ($\\mathcal\nS=\\{[k]\\}$).\n  - As a function of $|\\mathcal S|$ and $k$, the optimal deterministic ratio is\npolynomial: at most $O(k^2|\\mathcal S|)$ and at least $\\Omega(\\sqrt{|\\mathcal\nS|})$.\n  - For any laminar family $\\mathcal S$ of height $h$, the optimal ratios are\n$O(hk)$ (deterministic) and $O(h^2\\log k)$ (randomized).\n  - The special case of laminar $\\mathcal S$ that we call All-or-One Paging\nextends standard Paging by allowing each request to specify a specific slot to\nput the requested page in. The optimal deterministic ratio for weighted\nAll-or-One Paging is $\\Theta(k)$. Offline All-or-One Paging is NP-hard.\n  Some results for the laminar case are shown via a reduction to the\ngeneralization of Paging in which each request specifies a set $\\mathcal P of\npages, and is satisfied by fetching any page from $\\mathcal P into the cache.\nThe optimal ratios for the latter problem (with laminar family of height $h$)\nare at most $hk$ (deterministic) and $h\\,H_k$ (randomized).",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "It is natural to generalize the online $k$-Server problem by allowing each\nrequest to specify not only a point $p$, but also a subset $S$ of servers that\nmay serve it. For uniform metrics, the problem is equivalent to a\ngeneralization of Paging in which each request specifies not only a page $p$,\nbut also a subset $S$ of cache slots, and is satisfied by having a copy of $p$\nin some slot in $S$. We call this problem Slot-Heterogenous Paging.\n  We parameterize the problem by specifying a family $\\mathcal S \\subseteq\n2^{[k]}$ of requestable slot sets, and we establish bounds on the competitive\nratio as a function of the cache size $k$ and family $\\mathcal S$:\n  - If all request sets are allowed ($\\mathcal\nS=2^{[k]}\\setminus\\{\\emptyset\\}$), the optimal deterministic and randomized\ncompetitive ratios are exponentially worse than for standard \\Paging ($\\mathcal\nS=\\{[k]\\}$).\n  - As a function of $|\\mathcal S|$ and $k$, the optimal deterministic ratio is\npolynomial: at most $O(k^2|\\mathcal S|)$ and at least $\\Omega(\\sqrt{|\\mathcal\nS|})$.\n  - For any laminar family $\\mathcal S$ of height $h$, the optimal ratios are\n$O(hk)$ (deterministic) and $O(h^2\\log k)$ (randomized).\n  - The special case of laminar $\\mathcal S$ that we call All-or-One Paging\nextends standard Paging by allowing each request to specify a specific slot to\nput the requested page in. The optimal deterministic ratio for weighted\nAll-or-One Paging is $\\Theta(k)$. Offline All-or-One Paging is NP-hard.\n  Some results for the laminar case are shown via a reduction to the\ngeneralization of Paging in which each request specifies a set $\\mathcal P of\npages, and is satisfied by fetching any page from $\\mathcal P into the cache.\nThe optimal ratios for the latter problem (with laminar family of height $h$)\nare at most $hk$ (deterministic) and $h\\,H_k$ (randomized)."
                },
                "authors": [
                    {
                        "name": "Marek Chrobak"
                    },
                    {
                        "name": "Samuel Haney"
                    },
                    {
                        "name": "Mehraneh Liaee"
                    },
                    {
                        "name": "Debmalya Panigrahi"
                    },
                    {
                        "name": "Rajmohan Rajaraman"
                    },
                    {
                        "name": "Ravi Sundaram"
                    },
                    {
                        "name": "Neal E. Young"
                    }
                ],
                "author_detail": {
                    "name": "Neal E. Young"
                },
                "author": "Neal E. Young",
                "arxiv_doi": "10.1007/s00453-024-01270-z",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1007/s00453-024-01270-z",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2206.05579v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2206.05579v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "conference and journal versions appear in STACS 2023 and Algorithmica\n  (2004)",
                "arxiv_journal_ref": "Algorithmica (2004)",
                "arxiv_primary_category": {
                    "term": "cs.DS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "F.2.0; F.1.2; C.0",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.12876v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.12876v2",
                "updated": "2024-10-19T08:45:11Z",
                "updated_parsed": [
                    2024,
                    10,
                    19,
                    8,
                    45,
                    11,
                    5,
                    293,
                    0
                ],
                "published": "2024-10-15T05:01:19Z",
                "published_parsed": [
                    2024,
                    10,
                    15,
                    5,
                    1,
                    19,
                    1,
                    289,
                    0
                ],
                "title": "In-context KV-Cache Eviction for LLMs via Attention-Gate",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In-context KV-Cache Eviction for LLMs via Attention-Gate"
                },
                "summary": "The KV-Cache technique has become the standard for the inference of large\nlanguage models (LLMs). It caches states of self-attention to avoid\nrecomputation. Yet, it is widely criticized that KV-Cache can become a\nbottleneck of the LLM inference system, especially when confronted with\nultra-large models and long-context queries. A natural remedy is to discard the\nKV-Cache for less important tokens, with StreamingLLM as an example, but the\nused static eviction strategies cannot flexibly adapt to varying contexts.\nRemedies like H2O leverage accumulative attention scores to perform dynamic\neviction but suffer from the attention bias issue in capturing contextual\ninformation. This paper bridges this gap by devising a parameterized KV-Cache\neviction mechanism, dubbed as Attention-Gate, which accepts the whole context\nas input and yields eviction flags for each token to realize in-context\neviction. The subsequent self-attention module proceeds according to the flags\nand only the KV states for the remaining tokens need to be cached. The\nAttention-Gates can vary among different heads and layers and be trivially\nplugged into pre-trained LLMs, tuned by cost-effective continual pre-training\nor supervised fine-tuning objectives to acquire what to discard. The\ncomputational and memory overhead introduced by Attention-Gates is minimal. Our\nmethod is validated across multiple tasks, demonstrating both efficiency and\nadaptability. After a highly efficient continual pre-training, it achieves\nhigher average accuracy and evicts more tokens compared to traditional\ntraining-free methods. In supervised fine-tuning, it not only evicts many\ntokens but also outperforms LoRA-finetuned LLMs on some datasets, such as RTE,\nwhere it improves accuracy by 13.9% while evicting 62.8% of tokens, showing\nthat effective eviction of redundant tokens can even enhance performance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The KV-Cache technique has become the standard for the inference of large\nlanguage models (LLMs). It caches states of self-attention to avoid\nrecomputation. Yet, it is widely criticized that KV-Cache can become a\nbottleneck of the LLM inference system, especially when confronted with\nultra-large models and long-context queries. A natural remedy is to discard the\nKV-Cache for less important tokens, with StreamingLLM as an example, but the\nused static eviction strategies cannot flexibly adapt to varying contexts.\nRemedies like H2O leverage accumulative attention scores to perform dynamic\neviction but suffer from the attention bias issue in capturing contextual\ninformation. This paper bridges this gap by devising a parameterized KV-Cache\neviction mechanism, dubbed as Attention-Gate, which accepts the whole context\nas input and yields eviction flags for each token to realize in-context\neviction. The subsequent self-attention module proceeds according to the flags\nand only the KV states for the remaining tokens need to be cached. The\nAttention-Gates can vary among different heads and layers and be trivially\nplugged into pre-trained LLMs, tuned by cost-effective continual pre-training\nor supervised fine-tuning objectives to acquire what to discard. The\ncomputational and memory overhead introduced by Attention-Gates is minimal. Our\nmethod is validated across multiple tasks, demonstrating both efficiency and\nadaptability. After a highly efficient continual pre-training, it achieves\nhigher average accuracy and evicts more tokens compared to traditional\ntraining-free methods. In supervised fine-tuning, it not only evicts many\ntokens but also outperforms LoRA-finetuned LLMs on some datasets, such as RTE,\nwhere it improves accuracy by 13.9% while evicting 62.8% of tokens, showing\nthat effective eviction of redundant tokens can even enhance performance."
                },
                "authors": [
                    {
                        "name": "Zihao Zeng"
                    },
                    {
                        "name": "Bokai Lin"
                    },
                    {
                        "name": "Tianqi Hou"
                    },
                    {
                        "name": "Hao Zhang"
                    },
                    {
                        "name": "Zhijie Deng"
                    }
                ],
                "author_detail": {
                    "name": "Zhijie Deng"
                },
                "author": "Zhijie Deng",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.12876v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.12876v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.10593v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.10593v3",
                "updated": "2024-10-18T19:30:35Z",
                "updated_parsed": [
                    2024,
                    10,
                    18,
                    19,
                    30,
                    35,
                    4,
                    292,
                    0
                ],
                "published": "2024-09-16T17:36:50Z",
                "published_parsed": [
                    2024,
                    9,
                    16,
                    17,
                    36,
                    50,
                    0,
                    260,
                    0
                ],
                "title": "CSKV: Training-Efficient Channel Shrinking for KV Cache in Long-Context\n  Scenarios",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CSKV: Training-Efficient Channel Shrinking for KV Cache in Long-Context\n  Scenarios"
                },
                "summary": "Large Language Models (LLMs) have been widely adopted to process long-context\ntasks. However, the large memory overhead of the key-value (KV) cache poses\nsignificant challenges in long-context scenarios. Existing training-free KV\ncache compression methods typically focus on quantization and token pruning,\nwhich have compression limits, and excessive sparsity can lead to severe\nperformance degradation. Other methods design new architectures with less KV\noverhead but require significant training overhead. To address the above two\ndrawbacks, we further explore the redundancy in the channel dimension and apply\nan architecture-level design with minor training costs. Therefore, we introduce\nCSKV, a training-efficient Channel Shrinking technique for KV cache\ncompression: (1) We first analyze the singular value distribution of the KV\ncache, revealing significant redundancy and compression potential along the\nchannel dimension. Based on this observation, we propose using low-rank\ndecomposition for key and value layers and storing the low-dimension features.\n(2) To preserve model performance, we introduce a bi-branch KV cache, including\na window-based full-precision KV cache and a low-precision compressed KV cache.\n(3) To reduce the training costs, we minimize the layer-wise reconstruction\nloss for the compressed KV cache instead of retraining the entire LLMs.\nExtensive experiments show that CSKV can reduce the memory overhead of the KV\ncache by 80% while maintaining the model's long-context capability. Moreover,\nwe show that our method can be seamlessly combined with quantization to further\nreduce the memory overhead, achieving a compression ratio of up to 95%. Code is\navailable at https://github.com/wln20/CSKV.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have been widely adopted to process long-context\ntasks. However, the large memory overhead of the key-value (KV) cache poses\nsignificant challenges in long-context scenarios. Existing training-free KV\ncache compression methods typically focus on quantization and token pruning,\nwhich have compression limits, and excessive sparsity can lead to severe\nperformance degradation. Other methods design new architectures with less KV\noverhead but require significant training overhead. To address the above two\ndrawbacks, we further explore the redundancy in the channel dimension and apply\nan architecture-level design with minor training costs. Therefore, we introduce\nCSKV, a training-efficient Channel Shrinking technique for KV cache\ncompression: (1) We first analyze the singular value distribution of the KV\ncache, revealing significant redundancy and compression potential along the\nchannel dimension. Based on this observation, we propose using low-rank\ndecomposition for key and value layers and storing the low-dimension features.\n(2) To preserve model performance, we introduce a bi-branch KV cache, including\na window-based full-precision KV cache and a low-precision compressed KV cache.\n(3) To reduce the training costs, we minimize the layer-wise reconstruction\nloss for the compressed KV cache instead of retraining the entire LLMs.\nExtensive experiments show that CSKV can reduce the memory overhead of the KV\ncache by 80% while maintaining the model's long-context capability. Moreover,\nwe show that our method can be seamlessly combined with quantization to further\nreduce the memory overhead, achieving a compression ratio of up to 95%. Code is\navailable at https://github.com/wln20/CSKV."
                },
                "authors": [
                    {
                        "name": "Luning Wang"
                    },
                    {
                        "name": "Shiyao Li"
                    },
                    {
                        "name": "Xuefei Ning"
                    },
                    {
                        "name": "Zhihang Yuan"
                    },
                    {
                        "name": "Shengen Yan"
                    },
                    {
                        "name": "Guohao Dai"
                    },
                    {
                        "name": "Yu Wang"
                    }
                ],
                "author_detail": {
                    "name": "Yu Wang"
                },
                "author": "Yu Wang",
                "arxiv_comment": "4th NeurIPS Efficient Natural Language and Speech Processing Workshop\n  (ENLSP-IV 2024)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.10593v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.10593v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.14346v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.14346v2",
                "updated": "2024-10-18T13:59:54Z",
                "updated_parsed": [
                    2024,
                    10,
                    18,
                    13,
                    59,
                    54,
                    4,
                    292,
                    0
                ],
                "published": "2024-07-19T14:28:53Z",
                "published_parsed": [
                    2024,
                    7,
                    19,
                    14,
                    28,
                    53,
                    4,
                    201,
                    0
                ],
                "title": "Improving Retrieval in Sponsored Search by Leveraging Query Context\n  Signals",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Improving Retrieval in Sponsored Search by Leveraging Query Context\n  Signals"
                },
                "summary": "Accurately retrieving relevant bid keywords for user queries is critical in\nSponsored Search but remains challenging, particularly for short, ambiguous\nqueries. Existing dense and generative retrieval models often fail to capture\nnuanced user intent in these cases. To address this, we propose an approach to\nenhance query understanding by augmenting queries with rich contextual signals\nderived from web search results and large language models, stored in an online\ncache. Specifically, we use web search titles and snippets to ground queries in\nreal-world information and utilize GPT-4 to generate query rewrites and\nexplanations that clarify user intent. These signals are efficiently integrated\nthrough a Fusion-in-Decoder based Unity architecture, enabling both dense and\ngenerative retrieval with serving costs on par with traditional context-free\nmodels. To address scenarios where context is unavailable in the cache, we\nintroduce context glancing, a curriculum learning strategy that improves model\nrobustness and performance even without contextual signals during inference.\nExtensive offline experiments demonstrate that our context-aware approach\nsubstantially outperforms context-free models. Furthermore, online A/B testing\non a prominent search engine across 160+ countries shows significant\nimprovements in user engagement and revenue.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Accurately retrieving relevant bid keywords for user queries is critical in\nSponsored Search but remains challenging, particularly for short, ambiguous\nqueries. Existing dense and generative retrieval models often fail to capture\nnuanced user intent in these cases. To address this, we propose an approach to\nenhance query understanding by augmenting queries with rich contextual signals\nderived from web search results and large language models, stored in an online\ncache. Specifically, we use web search titles and snippets to ground queries in\nreal-world information and utilize GPT-4 to generate query rewrites and\nexplanations that clarify user intent. These signals are efficiently integrated\nthrough a Fusion-in-Decoder based Unity architecture, enabling both dense and\ngenerative retrieval with serving costs on par with traditional context-free\nmodels. To address scenarios where context is unavailable in the cache, we\nintroduce context glancing, a curriculum learning strategy that improves model\nrobustness and performance even without contextual signals during inference.\nExtensive offline experiments demonstrate that our context-aware approach\nsubstantially outperforms context-free models. Furthermore, online A/B testing\non a prominent search engine across 160+ countries shows significant\nimprovements in user engagement and revenue."
                },
                "authors": [
                    {
                        "name": "Akash Kumar Mohankumar"
                    },
                    {
                        "name": "Gururaj K"
                    },
                    {
                        "name": "Gagan Madan"
                    },
                    {
                        "name": "Amit Singh"
                    }
                ],
                "author_detail": {
                    "name": "Amit Singh"
                },
                "author": "Amit Singh",
                "arxiv_comment": "Accepted to EMNLP 2024 Industry Track. 10 pages, 10 tables, 1 figure",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.14346v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.14346v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.14442v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.14442v1",
                "updated": "2024-10-18T13:01:14Z",
                "updated_parsed": [
                    2024,
                    10,
                    18,
                    13,
                    1,
                    14,
                    4,
                    292,
                    0
                ],
                "published": "2024-10-18T13:01:14Z",
                "published_parsed": [
                    2024,
                    10,
                    18,
                    13,
                    1,
                    14,
                    4,
                    292,
                    0
                ],
                "title": "A Systematic Study of Cross-Layer KV Sharing for Efficient LLM Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Systematic Study of Cross-Layer KV Sharing for Efficient LLM Inference"
                },
                "summary": "Recently, sharing key-value (KV) cache across layers has been found effective\nin efficient inference of large language models (LLMs). To systematically\ninvestigate different techniques of cross-layer KV sharing, we propose a\nunified framework that covers several recent methods and their novel variants.\nWe conduct comprehensive experiments on all the configurations of the\nframework, evaluating their generation throughput and performance in language\nmodeling and downstream tasks. We find that when reducing the size of the KV\ncache by 2x, most configurations can achieve competitive performance to and\nhigher throughput than standard transformers, but when further reducing the\nsize of the KV cache, pairing queries of all layers with KVs of upper layers\ncan better maintain performance, although it also introduces additional\ntraining cost and prefilling latency. We hope that this work will help users\nchoose the appropriate approach according to their requirements and facilitate\nresearch on the acceleration of LLM inference.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recently, sharing key-value (KV) cache across layers has been found effective\nin efficient inference of large language models (LLMs). To systematically\ninvestigate different techniques of cross-layer KV sharing, we propose a\nunified framework that covers several recent methods and their novel variants.\nWe conduct comprehensive experiments on all the configurations of the\nframework, evaluating their generation throughput and performance in language\nmodeling and downstream tasks. We find that when reducing the size of the KV\ncache by 2x, most configurations can achieve competitive performance to and\nhigher throughput than standard transformers, but when further reducing the\nsize of the KV cache, pairing queries of all layers with KVs of upper layers\ncan better maintain performance, although it also introduces additional\ntraining cost and prefilling latency. We hope that this work will help users\nchoose the appropriate approach according to their requirements and facilitate\nresearch on the acceleration of LLM inference."
                },
                "authors": [
                    {
                        "name": "You Wu"
                    },
                    {
                        "name": "Haoyi Wu"
                    },
                    {
                        "name": "Kewei Tu"
                    }
                ],
                "author_detail": {
                    "name": "Kewei Tu"
                },
                "author": "Kewei Tu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.14442v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.14442v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.10859v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.10859v2",
                "updated": "2024-10-18T10:02:03Z",
                "updated_parsed": [
                    2024,
                    10,
                    18,
                    10,
                    2,
                    3,
                    4,
                    292,
                    0
                ],
                "published": "2024-10-07T13:46:06Z",
                "published_parsed": [
                    2024,
                    10,
                    7,
                    13,
                    46,
                    6,
                    0,
                    281,
                    0
                ],
                "title": "FAME: Towards Factual Multi-Task Model Editing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FAME: Towards Factual Multi-Task Model Editing"
                },
                "summary": "Large language models (LLMs) embed extensive knowledge and utilize it to\nperform exceptionally well across various tasks. Nevertheless, outdated\nknowledge or factual errors within LLMs can lead to misleading or incorrect\nresponses, causing significant issues in practical applications. To rectify the\nfatal flaw without the necessity for costly model retraining, various model\nediting approaches have been proposed to correct inaccurate knowledge within\nLLMs in a cost-efficient way. To evaluate these model editing methods, previous\nwork introduced a series of datasets. However, most of the previous datasets\nonly contain fabricated data in a single format, which diverges from real-world\nmodel editing scenarios, raising doubts about their usability in practice. To\nfacilitate the application of model editing in real-world scenarios, we propose\nthe challenge of practicality. To resolve such challenges and effectively\nenhance the capabilities of LLMs, we present FAME, an factual, comprehensive,\nand multi-task dataset, which is designed to enhance the practicality of model\nediting. We then propose SKEME, a model editing method that uses a novel\ncaching mechanism to ensure synchronization with the real world. The\nexperiments demonstrate that SKEME performs excellently across various tasks\nand scenarios, confirming its practicality.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) embed extensive knowledge and utilize it to\nperform exceptionally well across various tasks. Nevertheless, outdated\nknowledge or factual errors within LLMs can lead to misleading or incorrect\nresponses, causing significant issues in practical applications. To rectify the\nfatal flaw without the necessity for costly model retraining, various model\nediting approaches have been proposed to correct inaccurate knowledge within\nLLMs in a cost-efficient way. To evaluate these model editing methods, previous\nwork introduced a series of datasets. However, most of the previous datasets\nonly contain fabricated data in a single format, which diverges from real-world\nmodel editing scenarios, raising doubts about their usability in practice. To\nfacilitate the application of model editing in real-world scenarios, we propose\nthe challenge of practicality. To resolve such challenges and effectively\nenhance the capabilities of LLMs, we present FAME, an factual, comprehensive,\nand multi-task dataset, which is designed to enhance the practicality of model\nediting. We then propose SKEME, a model editing method that uses a novel\ncaching mechanism to ensure synchronization with the real world. The\nexperiments demonstrate that SKEME performs excellently across various tasks\nand scenarios, confirming its practicality."
                },
                "authors": [
                    {
                        "name": "Li Zeng"
                    },
                    {
                        "name": "Yingyu Shan"
                    },
                    {
                        "name": "Zeming Liu"
                    },
                    {
                        "name": "Jiashu Yao"
                    },
                    {
                        "name": "Yuhang Guo"
                    }
                ],
                "author_detail": {
                    "name": "Yuhang Guo"
                },
                "author": "Yuhang Guo",
                "arxiv_comment": "9 pages, 3 figures. This paper has been accepted by EMNLP 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.10859v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.10859v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.14003v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.14003v1",
                "updated": "2024-10-17T20:11:34Z",
                "updated_parsed": [
                    2024,
                    10,
                    17,
                    20,
                    11,
                    34,
                    3,
                    291,
                    0
                ],
                "published": "2024-10-17T20:11:34Z",
                "published_parsed": [
                    2024,
                    10,
                    17,
                    20,
                    11,
                    34,
                    3,
                    291,
                    0
                ],
                "title": "Per-Bank Bandwidth Regulation of Shared Last-Level Cache for Real-Time\n  Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Per-Bank Bandwidth Regulation of Shared Last-Level Cache for Real-Time\n  Systems"
                },
                "summary": "Modern commercial-off-the-shelf (COTS) multicore processors have advanced\nmemory hierarchies that enhance memory-level parallelism (MLP), which is\ncrucial for high performance. To support high MLP, shared last-level caches\n(LLCs) are divided into multiple banks, allowing parallel access. However,\nuneven distribution of cache requests from the cores, especially when requests\nfrom multiple cores are concentrated on a single bank, can result in\nsignificant contention affecting all cores that access the cache. Such cache\nbank contention can even be maliciously induced -- known as cache bank-aware\ndenial-of-service (DoS) attacks -- in order to jeopardize the system's timing\npredictability.\n  In this paper, we propose a per-bank bandwidth regulation approach for\nmulti-banked shared LLC based multicore real-time systems. By regulating\nbandwidth on a per-bank basis, the approach aims to prevent unnecessary\nthrottling of cache accesses to non-contended banks, thus improving overall\nperformance (throughput) without compromising isolation benefits of throttling.\nWe implement our approach on a RISC-V system-on-chip (SoC) platform using\nFireSim and evaluate extensively using both synthetic and real-world workloads.\nOur evaluation results show that the proposed per-bank regulation approach\neffectively protects real-time tasks from co-running cache bank-aware DoS\nattacks, and offers up to a 3.66$\\times$ performance improvement for the\nthrottled benign best-effort tasks compared to prior bank-oblivious bandwidth\nthrottling approaches.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Modern commercial-off-the-shelf (COTS) multicore processors have advanced\nmemory hierarchies that enhance memory-level parallelism (MLP), which is\ncrucial for high performance. To support high MLP, shared last-level caches\n(LLCs) are divided into multiple banks, allowing parallel access. However,\nuneven distribution of cache requests from the cores, especially when requests\nfrom multiple cores are concentrated on a single bank, can result in\nsignificant contention affecting all cores that access the cache. Such cache\nbank contention can even be maliciously induced -- known as cache bank-aware\ndenial-of-service (DoS) attacks -- in order to jeopardize the system's timing\npredictability.\n  In this paper, we propose a per-bank bandwidth regulation approach for\nmulti-banked shared LLC based multicore real-time systems. By regulating\nbandwidth on a per-bank basis, the approach aims to prevent unnecessary\nthrottling of cache accesses to non-contended banks, thus improving overall\nperformance (throughput) without compromising isolation benefits of throttling.\nWe implement our approach on a RISC-V system-on-chip (SoC) platform using\nFireSim and evaluate extensively using both synthetic and real-world workloads.\nOur evaluation results show that the proposed per-bank regulation approach\neffectively protects real-time tasks from co-running cache bank-aware DoS\nattacks, and offers up to a 3.66$\\times$ performance improvement for the\nthrottled benign best-effort tasks compared to prior bank-oblivious bandwidth\nthrottling approaches."
                },
                "authors": [
                    {
                        "name": "Connor Sullivan"
                    },
                    {
                        "name": "Alex Manley"
                    },
                    {
                        "name": "Mohammad Alian"
                    },
                    {
                        "name": "Heechul Yun"
                    }
                ],
                "author_detail": {
                    "name": "Heechul Yun"
                },
                "author": "Heechul Yun",
                "arxiv_comment": "RTSS 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.14003v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.14003v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.13846v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.13846v1",
                "updated": "2024-10-17T17:58:14Z",
                "updated_parsed": [
                    2024,
                    10,
                    17,
                    17,
                    58,
                    14,
                    3,
                    291,
                    0
                ],
                "published": "2024-10-17T17:58:14Z",
                "published_parsed": [
                    2024,
                    10,
                    17,
                    17,
                    58,
                    14,
                    3,
                    291,
                    0
                ],
                "title": "SimLayerKV: A Simple Framework for Layer-Level KV Cache Reduction",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SimLayerKV: A Simple Framework for Layer-Level KV Cache Reduction"
                },
                "summary": "Recent advancements in large language models (LLMs) have extended their\ncapabilities to handle long contexts. However, increasing the number of model\nlayers and the length of input sequences significantly escalates the memory\nrequired to store key-value (KV) cache, posing challenges for efficient\ninference. To mitigate this issue, we present SimLayerKV, a simple yet\neffective method that reduces inter-layer KV cache redundancies by selectively\ndropping cache in identified lazy layers. Our approach is based on the\nobservation that certain layers in long-context LLMs exhibit \"lazy\" behavior,\ncontributing less to modeling long-range dependencies compared to non-lazy\nlayers. By analyzing attention weight patterns, we find that the behavior of\nthese lazy layers is consistent across tokens during generation for a given\ninput. This insight motivates our SimLayerKV, which identifies lazy layers and\nreduces their KV cache accordingly. SimLayerKV is training-free, generalizable,\nand can be implemented with only seven lines of code. We conduct extensive\nexperiments on three representative LLMs, e.g., LLaMA2-7B, LLaMA3-8B, and\nMistral-7B across 16 tasks from the LongBench benchmark. The results\ndemonstrate that SimLayerKV achieves a KV cache compression ratio of 5$\\times$\nwith only a 1.2% performance drop when combined with 4-bit quantization. Our\ncode is available at https://github.com/sail-sg/SimLayerKV.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advancements in large language models (LLMs) have extended their\ncapabilities to handle long contexts. However, increasing the number of model\nlayers and the length of input sequences significantly escalates the memory\nrequired to store key-value (KV) cache, posing challenges for efficient\ninference. To mitigate this issue, we present SimLayerKV, a simple yet\neffective method that reduces inter-layer KV cache redundancies by selectively\ndropping cache in identified lazy layers. Our approach is based on the\nobservation that certain layers in long-context LLMs exhibit \"lazy\" behavior,\ncontributing less to modeling long-range dependencies compared to non-lazy\nlayers. By analyzing attention weight patterns, we find that the behavior of\nthese lazy layers is consistent across tokens during generation for a given\ninput. This insight motivates our SimLayerKV, which identifies lazy layers and\nreduces their KV cache accordingly. SimLayerKV is training-free, generalizable,\nand can be implemented with only seven lines of code. We conduct extensive\nexperiments on three representative LLMs, e.g., LLaMA2-7B, LLaMA3-8B, and\nMistral-7B across 16 tasks from the LongBench benchmark. The results\ndemonstrate that SimLayerKV achieves a KV cache compression ratio of 5$\\times$\nwith only a 1.2% performance drop when combined with 4-bit quantization. Our\ncode is available at https://github.com/sail-sg/SimLayerKV."
                },
                "authors": [
                    {
                        "name": "Xuan Zhang"
                    },
                    {
                        "name": "Cunxiao Du"
                    },
                    {
                        "name": "Chao Du"
                    },
                    {
                        "name": "Tianyu Pang"
                    },
                    {
                        "name": "Wei Gao"
                    },
                    {
                        "name": "Min Lin"
                    }
                ],
                "author_detail": {
                    "name": "Min Lin"
                },
                "author": "Min Lin",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.13846v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.13846v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.15355v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.15355v4",
                "updated": "2024-10-17T15:27:30Z",
                "updated_parsed": [
                    2024,
                    10,
                    17,
                    15,
                    27,
                    30,
                    3,
                    291,
                    0
                ],
                "published": "2024-09-14T02:34:26Z",
                "published_parsed": [
                    2024,
                    9,
                    14,
                    2,
                    34,
                    26,
                    5,
                    258,
                    0
                ],
                "title": "Block-Attention for Efficient RAG",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Block-Attention for Efficient RAG"
                },
                "summary": "We introduce Block-Attention, an attention mechanism designed to address the\nincreased inference latency and cost in Retrieval-Augmented Generation (RAG)\nscenarios. Traditional approaches often encode the entire context. Instead,\nBlock-Attention divides retrieved documents into discrete blocks, with each\nblock independently calculating key-value (KV) states except for the final\nblock. In RAG scenarios, by defining each passage as a block, Block-Attention\nenables us to reuse the KV states of passages that have been seen before,\nthereby significantly reducing the latency and the computation overhead during\ninference. The implementation of Block-Attention involves block segmentation,\nposition re-encoding, and fine-tuning the LLM to adapt to the Block-Attention\nmechanism. Experiments on four RAG benchmarks demonstrate that after block\nfine-tuning, the Block-Attention model achieves performance comparable to\nself-attention models (68.4\\% vs 67.9\\% on Llama3) or even superior performance\n(62.8\\% vs 59.6\\% on Mistral). Notably, Block-Attention significantly reduces\nthe time to first token (TTFT) and floating point operations (FLOPs) to a very\nlow level. It only takes 45 ms to output the first token for an input sequence\nwith a total length of 32K. Compared to the self-attention models, the time\nconsumption and corresponding FLOPs are reduced by 98.7\\% and 99.8\\%,\nrespectively.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce Block-Attention, an attention mechanism designed to address the\nincreased inference latency and cost in Retrieval-Augmented Generation (RAG)\nscenarios. Traditional approaches often encode the entire context. Instead,\nBlock-Attention divides retrieved documents into discrete blocks, with each\nblock independently calculating key-value (KV) states except for the final\nblock. In RAG scenarios, by defining each passage as a block, Block-Attention\nenables us to reuse the KV states of passages that have been seen before,\nthereby significantly reducing the latency and the computation overhead during\ninference. The implementation of Block-Attention involves block segmentation,\nposition re-encoding, and fine-tuning the LLM to adapt to the Block-Attention\nmechanism. Experiments on four RAG benchmarks demonstrate that after block\nfine-tuning, the Block-Attention model achieves performance comparable to\nself-attention models (68.4\\% vs 67.9\\% on Llama3) or even superior performance\n(62.8\\% vs 59.6\\% on Mistral). Notably, Block-Attention significantly reduces\nthe time to first token (TTFT) and floating point operations (FLOPs) to a very\nlow level. It only takes 45 ms to output the first token for an input sequence\nwith a total length of 32K. Compared to the self-attention models, the time\nconsumption and corresponding FLOPs are reduced by 98.7\\% and 99.8\\%,\nrespectively."
                },
                "authors": [
                    {
                        "name": "East Sun"
                    },
                    {
                        "name": "Yan Wang"
                    },
                    {
                        "name": "Lan Tian"
                    }
                ],
                "author_detail": {
                    "name": "Lan Tian"
                },
                "author": "Lan Tian",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.15355v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.15355v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.07979v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.07979v2",
                "updated": "2024-10-17T08:54:37Z",
                "updated_parsed": [
                    2024,
                    10,
                    17,
                    8,
                    54,
                    37,
                    3,
                    291,
                    0
                ],
                "published": "2024-04-11T17:57:22Z",
                "published_parsed": [
                    2024,
                    4,
                    11,
                    17,
                    57,
                    22,
                    3,
                    102,
                    0
                ],
                "title": "LLoCO: Learning Long Contexts Offline",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLoCO: Learning Long Contexts Offline"
                },
                "summary": "Processing long contexts remains a challenge for large language models (LLMs)\ndue to the quadratic computational and memory overhead of the self-attention\nmechanism and the substantial KV cache sizes during generation. We propose\nLLoCO, a novel approach to address this problem by learning contexts offline\nthrough context compression and in-domain parameter-efficient finetuning with\nLoRA. Our method enables an LLM to create a concise representation of the\noriginal context and efficiently retrieve relevant information to answer\nquestions accurately. Our approach extends the effective context window of a 4k\ntoken LLaMA2-7B model to handle up to 128k tokens. We evaluate our approach on\nseveral long-context question-answering datasets, demonstrating that LLoCO\nsignificantly outperforms in-context learning while using $30\\times$ fewer\ntokens during inference. LLoCO achieves up to $7.62\\times$ speed-up during\ninference and $11.52\\times$ higher throughput during finetuning, substantially\nreduces the cost of long document question answering. This makes it a promising\nsolution for efficient long context processing. Our code is publicly available\non https://github.com/jeffreysijuntan/lloco.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Processing long contexts remains a challenge for large language models (LLMs)\ndue to the quadratic computational and memory overhead of the self-attention\nmechanism and the substantial KV cache sizes during generation. We propose\nLLoCO, a novel approach to address this problem by learning contexts offline\nthrough context compression and in-domain parameter-efficient finetuning with\nLoRA. Our method enables an LLM to create a concise representation of the\noriginal context and efficiently retrieve relevant information to answer\nquestions accurately. Our approach extends the effective context window of a 4k\ntoken LLaMA2-7B model to handle up to 128k tokens. We evaluate our approach on\nseveral long-context question-answering datasets, demonstrating that LLoCO\nsignificantly outperforms in-context learning while using $30\\times$ fewer\ntokens during inference. LLoCO achieves up to $7.62\\times$ speed-up during\ninference and $11.52\\times$ higher throughput during finetuning, substantially\nreduces the cost of long document question answering. This makes it a promising\nsolution for efficient long context processing. Our code is publicly available\non https://github.com/jeffreysijuntan/lloco."
                },
                "authors": [
                    {
                        "name": "Sijun Tan"
                    },
                    {
                        "name": "Xiuyu Li"
                    },
                    {
                        "name": "Shishir Patil"
                    },
                    {
                        "name": "Ziyang Wu"
                    },
                    {
                        "name": "Tianjun Zhang"
                    },
                    {
                        "name": "Kurt Keutzer"
                    },
                    {
                        "name": "Joseph E. Gonzalez"
                    },
                    {
                        "name": "Raluca Ada Popa"
                    }
                ],
                "author_detail": {
                    "name": "Raluca Ada Popa"
                },
                "author": "Raluca Ada Popa",
                "arxiv_comment": "EMNLP 2024. The first two authors contributed equally to this work",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.07979v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.07979v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.18126v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.18126v1",
                "updated": "2024-10-17T04:37:43Z",
                "updated_parsed": [
                    2024,
                    10,
                    17,
                    4,
                    37,
                    43,
                    3,
                    291,
                    0
                ],
                "published": "2024-10-17T04:37:43Z",
                "published_parsed": [
                    2024,
                    10,
                    17,
                    4,
                    37,
                    43,
                    3,
                    291,
                    0
                ],
                "title": "Leveraging Hardware Performance Counters for Predicting Workload\n  Interference in Vector Supercomputers",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Leveraging Hardware Performance Counters for Predicting Workload\n  Interference in Vector Supercomputers"
                },
                "summary": "In the rapidly evolving domain of high-performance computing (HPC),\nheterogeneous architectures such as the SX-Aurora TSUBASA (SX-AT) system\narchitecture, which integrate diverse processor types, present both\nopportunities and challenges for optimizing resource utilization. This paper\ninvestigates workload interference within an SX-AT system, with a specific\nfocus on resource contention between Vector Hosts (VHs) and Vector Engines\n(VEs). Through comprehensive empirical analysis, the study identifies key\nfactors contributing to performance degradation, such as cache and memory\nbandwidth contention, when jobs with varying computational demands share\nresources. To address these issues, we develop a predictive model that\nleverages hardware performance counters (HCs) and machine learning (ML)\nalgorithms to classify and predict workload interference. Our results\ndemonstrate that the model accurately forecasts performance degradation,\noffering valuable insights for future research on optimizing job scheduling and\nresource allocation. This approach highlights the importance of adaptive\nresource management strategies in maintaining system efficiency and provides a\nfoundation for future enhancements in heterogeneous supercomputing\nenvironments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In the rapidly evolving domain of high-performance computing (HPC),\nheterogeneous architectures such as the SX-Aurora TSUBASA (SX-AT) system\narchitecture, which integrate diverse processor types, present both\nopportunities and challenges for optimizing resource utilization. This paper\ninvestigates workload interference within an SX-AT system, with a specific\nfocus on resource contention between Vector Hosts (VHs) and Vector Engines\n(VEs). Through comprehensive empirical analysis, the study identifies key\nfactors contributing to performance degradation, such as cache and memory\nbandwidth contention, when jobs with varying computational demands share\nresources. To address these issues, we develop a predictive model that\nleverages hardware performance counters (HCs) and machine learning (ML)\nalgorithms to classify and predict workload interference. Our results\ndemonstrate that the model accurately forecasts performance degradation,\noffering valuable insights for future research on optimizing job scheduling and\nresource allocation. This approach highlights the importance of adaptive\nresource management strategies in maintaining system efficiency and provides a\nfoundation for future enhancements in heterogeneous supercomputing\nenvironments."
                },
                "authors": [
                    {
                        "name": "Shubham"
                    },
                    {
                        "name": "Keichi Takahashi"
                    },
                    {
                        "name": "Hiroyuki Takizawa"
                    }
                ],
                "author_detail": {
                    "name": "Hiroyuki Takizawa"
                },
                "author": "Hiroyuki Takizawa",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.18126v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.18126v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.13212v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.13212v1",
                "updated": "2024-10-17T04:35:57Z",
                "updated_parsed": [
                    2024,
                    10,
                    17,
                    4,
                    35,
                    57,
                    3,
                    291,
                    0
                ],
                "published": "2024-10-17T04:35:57Z",
                "published_parsed": [
                    2024,
                    10,
                    17,
                    4,
                    35,
                    57,
                    3,
                    291,
                    0
                ],
                "title": "AsymKV: Enabling 1-Bit Quantization of KV Cache with Layer-Wise\n  Asymmetric Quantization Configurations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AsymKV: Enabling 1-Bit Quantization of KV Cache with Layer-Wise\n  Asymmetric Quantization Configurations"
                },
                "summary": "Large language models have shown exceptional capabilities in a wide range of\ntasks, such as text generation and video generation, among others. However, due\nto their massive parameter count, these models often require substantial\nstorage space, imposing significant constraints on the machines deploying LLMs.\nTo overcome this limitation, one research direction proposes to compress the\nmodels using integer replacements for floating-point numbers, in a process\nknown as Quantization. Some recent studies suggest quantizing the key and value\ncache (KV Cache) of LLMs, and designing quantization techniques that treat the\nkey and value matrices equivalently.\n  This work delves deeper into the asymmetric structural roles of KV Cache, a\nphenomenon where the transformer's output loss is more sensitive to the\nquantization of key matrices. We conduct a systematic examination of the\nattention output error resulting from key and value quantization. The\nphenomenon inspires us to propose an asymmetric quantization strategy. Our\napproach allows for 1-bit quantization of the KV cache by implementing distinct\nconfigurations for key and value matrices. We carry out experiments across a\nvariety of datasets, demonstrating that our proposed model allows for the\nquantization of up to 75% decoder layers with 1 bit, while simultaneously\nmaintaining performance levels comparable to those of the models with floating\nparameters.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models have shown exceptional capabilities in a wide range of\ntasks, such as text generation and video generation, among others. However, due\nto their massive parameter count, these models often require substantial\nstorage space, imposing significant constraints on the machines deploying LLMs.\nTo overcome this limitation, one research direction proposes to compress the\nmodels using integer replacements for floating-point numbers, in a process\nknown as Quantization. Some recent studies suggest quantizing the key and value\ncache (KV Cache) of LLMs, and designing quantization techniques that treat the\nkey and value matrices equivalently.\n  This work delves deeper into the asymmetric structural roles of KV Cache, a\nphenomenon where the transformer's output loss is more sensitive to the\nquantization of key matrices. We conduct a systematic examination of the\nattention output error resulting from key and value quantization. The\nphenomenon inspires us to propose an asymmetric quantization strategy. Our\napproach allows for 1-bit quantization of the KV cache by implementing distinct\nconfigurations for key and value matrices. We carry out experiments across a\nvariety of datasets, demonstrating that our proposed model allows for the\nquantization of up to 75% decoder layers with 1 bit, while simultaneously\nmaintaining performance levels comparable to those of the models with floating\nparameters."
                },
                "authors": [
                    {
                        "name": "Qian Tao"
                    },
                    {
                        "name": "Wenyuan Yu"
                    },
                    {
                        "name": "Jingren Zhou"
                    }
                ],
                "author_detail": {
                    "name": "Jingren Zhou"
                },
                "author": "Jingren Zhou",
                "arxiv_comment": "12 pages, 4 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.13212v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.13212v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2401.08895v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2401.08895v3",
                "updated": "2024-10-16T17:54:15Z",
                "updated_parsed": [
                    2024,
                    10,
                    16,
                    17,
                    54,
                    15,
                    2,
                    290,
                    0
                ],
                "published": "2024-01-17T00:36:58Z",
                "published_parsed": [
                    2024,
                    1,
                    17,
                    0,
                    36,
                    58,
                    2,
                    17,
                    0
                ],
                "title": "cedar: Optimized and Unified Machine Learning Input Data Pipelines",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "cedar: Optimized and Unified Machine Learning Input Data Pipelines"
                },
                "summary": "The input data pipeline is an essential component of each machine learning\n(ML) training job. It is responsible for reading massive amounts of training\ndata, processing batches of samples using complex transformations, and loading\nthem onto training nodes at low latency and high throughput. Performant input\ndata systems are becoming increasingly critical, driven by skyrocketing data\nvolumes and training throughput demands. Unfortunately, current input data\nsystems cannot fully leverage key performance optimizations, resulting in\nhugely inefficient infrastructures that require significant resources - or\nworse - underutilize expensive accelerators.\n  To address these demands, we present cedar, an optimized and unified\nprogramming framework for ML input data pipelines. cedar allows users to define\ninput data pipelines using composable operators that support arbitrary ML\nframeworks and libraries. cedar introduces an extensible optimizer that\nsystematically applies a complex combination of optimizations (e.g.,\noffloading, caching, prefetching, fusion, and reordering). It orchestrates\nprocessing across a customizable set of local and distributed compute resources\nin order to improve processing performance and efficiency, all without user\ninput. Across eight pipelines, cedar improves performance by up to 1.87x to\n10.65x compared to state-of-the-art input data systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The input data pipeline is an essential component of each machine learning\n(ML) training job. It is responsible for reading massive amounts of training\ndata, processing batches of samples using complex transformations, and loading\nthem onto training nodes at low latency and high throughput. Performant input\ndata systems are becoming increasingly critical, driven by skyrocketing data\nvolumes and training throughput demands. Unfortunately, current input data\nsystems cannot fully leverage key performance optimizations, resulting in\nhugely inefficient infrastructures that require significant resources - or\nworse - underutilize expensive accelerators.\n  To address these demands, we present cedar, an optimized and unified\nprogramming framework for ML input data pipelines. cedar allows users to define\ninput data pipelines using composable operators that support arbitrary ML\nframeworks and libraries. cedar introduces an extensible optimizer that\nsystematically applies a complex combination of optimizations (e.g.,\noffloading, caching, prefetching, fusion, and reordering). It orchestrates\nprocessing across a customizable set of local and distributed compute resources\nin order to improve processing performance and efficiency, all without user\ninput. Across eight pipelines, cedar improves performance by up to 1.87x to\n10.65x compared to state-of-the-art input data systems."
                },
                "authors": [
                    {
                        "name": "Mark Zhao"
                    },
                    {
                        "name": "Emanuel Adamiak"
                    },
                    {
                        "name": "Christos Kozyrakis"
                    }
                ],
                "author_detail": {
                    "name": "Christos Kozyrakis"
                },
                "author": "Christos Kozyrakis",
                "arxiv_comment": "Accepted to PVLDB Volume 18",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2401.08895v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2401.08895v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.12749v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.12749v1",
                "updated": "2024-10-16T17:10:48Z",
                "updated_parsed": [
                    2024,
                    10,
                    16,
                    17,
                    10,
                    48,
                    2,
                    290,
                    0
                ],
                "published": "2024-10-16T17:10:48Z",
                "published_parsed": [
                    2024,
                    10,
                    16,
                    17,
                    10,
                    48,
                    2,
                    290,
                    0
                ],
                "title": "Toleo: Scaling Freshness to Tera-scale Memory using CXL and PIM",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Toleo: Scaling Freshness to Tera-scale Memory using CXL and PIM"
                },
                "summary": "Trusted hardware's freshness guarantee ensures that an adversary cannot\nreplay an old value in response to a memory read request. They rely on\nmaintaining a version number for each cache block and ensuring their integrity\nusing a Merkle tree. However, these existing solutions protect only a small\namount of main memory (few MBs), as the extraneous memory accesses to the\nMerkle tree increase prohibitively with the protected memory size. We present\nToleo, which uses trusted smart memory connected through a secure CXL IDE\nnetwork to safely store version numbers. Toleo eliminates the need for an\nunscalable Merkle tree to protect the integrity of version numbers by instead\nusing smart memory as the root of trust. Additionally, Toleo ensures version\nconfidentiality which enables stealth versions that reduce the version storage\noverhead in half.\n  Furthermore, in the absence of Merkle tree imposed constraints, we\neffectively exploit version locality at page granularity to compress version\nnumber by a factor of 240. These space optimizations make it feasible for one\n168 GB Toleo smart memory device to provide freshness to a 28 TB CXL-expanded\nmain memory pool in a rack server for a negligible performance overhead. We\nanalyze the benefits of Toleo using several privacy-sensitive genomics, graph,\ngenerative AI, and database workloads.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Trusted hardware's freshness guarantee ensures that an adversary cannot\nreplay an old value in response to a memory read request. They rely on\nmaintaining a version number for each cache block and ensuring their integrity\nusing a Merkle tree. However, these existing solutions protect only a small\namount of main memory (few MBs), as the extraneous memory accesses to the\nMerkle tree increase prohibitively with the protected memory size. We present\nToleo, which uses trusted smart memory connected through a secure CXL IDE\nnetwork to safely store version numbers. Toleo eliminates the need for an\nunscalable Merkle tree to protect the integrity of version numbers by instead\nusing smart memory as the root of trust. Additionally, Toleo ensures version\nconfidentiality which enables stealth versions that reduce the version storage\noverhead in half.\n  Furthermore, in the absence of Merkle tree imposed constraints, we\neffectively exploit version locality at page granularity to compress version\nnumber by a factor of 240. These space optimizations make it feasible for one\n168 GB Toleo smart memory device to provide freshness to a 28 TB CXL-expanded\nmain memory pool in a rack server for a negligible performance overhead. We\nanalyze the benefits of Toleo using several privacy-sensitive genomics, graph,\ngenerative AI, and database workloads."
                },
                "authors": [
                    {
                        "name": "Juechu Dong"
                    },
                    {
                        "name": "Jonah Rosenblum"
                    },
                    {
                        "name": "Satish Narayanasamy"
                    }
                ],
                "author_detail": {
                    "name": "Satish Narayanasamy"
                },
                "author": "Satish Narayanasamy",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.12749v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.12749v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.12605v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.12605v1",
                "updated": "2024-10-16T14:24:16Z",
                "updated_parsed": [
                    2024,
                    10,
                    16,
                    14,
                    24,
                    16,
                    2,
                    290,
                    0
                ],
                "published": "2024-10-16T14:24:16Z",
                "published_parsed": [
                    2024,
                    10,
                    16,
                    14,
                    24,
                    16,
                    2,
                    290,
                    0
                ],
                "title": "Advancing Web Browser Forensics: Critical Evaluation of Emerging Tools\n  and Techniques",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Advancing Web Browser Forensics: Critical Evaluation of Emerging Tools\n  and Techniques"
                },
                "summary": "As the use of web browsers continues to grow, the potential for cybercrime\nand web-related criminal activities also increases. Digital forensic\ninvestigators must understand how different browsers function and the critical\nareas to consider during web forensic analysis. Web forensics, a subfield of\ndigital forensics, involves collecting and analyzing browser artifacts, such as\nbrowser history, search keywords, and downloads, which serve as potential\nevidence. While existing research has provided valuable insights, many studies\nfocus on individual browsing modes or limited forensic scenarios, leaving gaps\nin understanding the full scope of data retention and recovery across different\nmodes and browsers. This paper addresses these gaps by defining four browsing\nscenarios and critically analyzing browser artifacts across normal, private,\nand portable modes using various forensic tools. We define four browsing\nscenarios to perform a comprehensive evaluation of popular browsers -- Google\nChrome, Mozilla Firefox, Brave, Tor, and Microsoft Edge -- by monitoring\nchanges in key data storage areas such as cache files, cookies, browsing\nhistory, and local storage across different browsing modes. Overall, this paper\ncontributes to a deeper understanding of browser forensic analysis and\nidentifies key areas for enhancing privacy protection and forensic\nmethodologies.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As the use of web browsers continues to grow, the potential for cybercrime\nand web-related criminal activities also increases. Digital forensic\ninvestigators must understand how different browsers function and the critical\nareas to consider during web forensic analysis. Web forensics, a subfield of\ndigital forensics, involves collecting and analyzing browser artifacts, such as\nbrowser history, search keywords, and downloads, which serve as potential\nevidence. While existing research has provided valuable insights, many studies\nfocus on individual browsing modes or limited forensic scenarios, leaving gaps\nin understanding the full scope of data retention and recovery across different\nmodes and browsers. This paper addresses these gaps by defining four browsing\nscenarios and critically analyzing browser artifacts across normal, private,\nand portable modes using various forensic tools. We define four browsing\nscenarios to perform a comprehensive evaluation of popular browsers -- Google\nChrome, Mozilla Firefox, Brave, Tor, and Microsoft Edge -- by monitoring\nchanges in key data storage areas such as cache files, cookies, browsing\nhistory, and local storage across different browsing modes. Overall, this paper\ncontributes to a deeper understanding of browser forensic analysis and\nidentifies key areas for enhancing privacy protection and forensic\nmethodologies."
                },
                "authors": [
                    {
                        "name": "Rishal Ravikesh Chand"
                    },
                    {
                        "name": "Neeraj Anand Sharma"
                    },
                    {
                        "name": "Muhammad Ashad Kabir"
                    }
                ],
                "author_detail": {
                    "name": "Muhammad Ashad Kabir"
                },
                "author": "Muhammad Ashad Kabir",
                "arxiv_comment": "34 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.12605v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.12605v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.12513v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.12513v1",
                "updated": "2024-10-16T12:45:35Z",
                "updated_parsed": [
                    2024,
                    10,
                    16,
                    12,
                    45,
                    35,
                    2,
                    290,
                    0
                ],
                "published": "2024-10-16T12:45:35Z",
                "published_parsed": [
                    2024,
                    10,
                    16,
                    12,
                    45,
                    35,
                    2,
                    290,
                    0
                ],
                "title": "FiRST: Finetuning Router-Selective Transformers for Input-Adaptive\n  Latency Reduction",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FiRST: Finetuning Router-Selective Transformers for Input-Adaptive\n  Latency Reduction"
                },
                "summary": "Auto-regressive Large Language Models (LLMs) demonstrate remarkable\nperformance across domanins such as vision and language processing. However,\ndue to sequential processing through a stack of transformer layers,\nautoregressive decoding faces significant computation/latency challenges,\nparticularly in resource constrained environments like mobile and edge devices.\nExisting approaches in literature that aim to improve latency via skipping\nlayers have two distinct flavors - 1) Early exit 2) Input-agnostic heuristics\nwhere tokens exit at pre-determined layers irrespective of input sequence. Both\nthe above strategies have limitations - the former cannot be applied to handle\nKV Caching necessary for speed-ups in modern framework and the latter does not\ncapture the variation in layer importance across tasks or more generally,\nacross input sequences. To address both limitations, we propose FIRST, an\nalgorithm that reduces inference latency by using layer-specific routers to\nselect a subset of transformer layers adaptively for each input sequence - the\nprompt (during prefill stage) decides which layers will be skipped during\ndecoding. FIRST preserves compatibility with KV caching enabling faster\ninference while being quality-aware. FIRST is model-agnostic and can be easily\nenabled on any pre-trained LLM. We further improve performance by incorporating\nLoRA adapters for fine-tuning on external datasets, enhancing task-specific\naccuracy while maintaining latency benefits. Our approach reveals that input\nadaptivity is critical - indeed, different task-specific middle layers play a\ncrucial role in evolving hidden representations depending on task. Extensive\nexperiments show that FIRST significantly reduces latency while retaining\ncompetitive performance (as compared to baselines), making our approach an\nefficient solution for LLM deployment in low-resource environments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Auto-regressive Large Language Models (LLMs) demonstrate remarkable\nperformance across domanins such as vision and language processing. However,\ndue to sequential processing through a stack of transformer layers,\nautoregressive decoding faces significant computation/latency challenges,\nparticularly in resource constrained environments like mobile and edge devices.\nExisting approaches in literature that aim to improve latency via skipping\nlayers have two distinct flavors - 1) Early exit 2) Input-agnostic heuristics\nwhere tokens exit at pre-determined layers irrespective of input sequence. Both\nthe above strategies have limitations - the former cannot be applied to handle\nKV Caching necessary for speed-ups in modern framework and the latter does not\ncapture the variation in layer importance across tasks or more generally,\nacross input sequences. To address both limitations, we propose FIRST, an\nalgorithm that reduces inference latency by using layer-specific routers to\nselect a subset of transformer layers adaptively for each input sequence - the\nprompt (during prefill stage) decides which layers will be skipped during\ndecoding. FIRST preserves compatibility with KV caching enabling faster\ninference while being quality-aware. FIRST is model-agnostic and can be easily\nenabled on any pre-trained LLM. We further improve performance by incorporating\nLoRA adapters for fine-tuning on external datasets, enhancing task-specific\naccuracy while maintaining latency benefits. Our approach reveals that input\nadaptivity is critical - indeed, different task-specific middle layers play a\ncrucial role in evolving hidden representations depending on task. Extensive\nexperiments show that FIRST significantly reduces latency while retaining\ncompetitive performance (as compared to baselines), making our approach an\nefficient solution for LLM deployment in low-resource environments."
                },
                "authors": [
                    {
                        "name": "Akriti Jain"
                    },
                    {
                        "name": "Saransh Sharma"
                    },
                    {
                        "name": "Koyel Mukherjee"
                    },
                    {
                        "name": "Soumyabrata Pal"
                    }
                ],
                "author_detail": {
                    "name": "Soumyabrata Pal"
                },
                "author": "Soumyabrata Pal",
                "arxiv_comment": "17 pages, 6 figures, Submitted to ICLR 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.12513v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.12513v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.12423v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.12423v1",
                "updated": "2024-10-16T10:06:22Z",
                "updated_parsed": [
                    2024,
                    10,
                    16,
                    10,
                    6,
                    22,
                    2,
                    290,
                    0
                ],
                "published": "2024-10-16T10:06:22Z",
                "published_parsed": [
                    2024,
                    10,
                    16,
                    10,
                    6,
                    22,
                    2,
                    290,
                    0
                ],
                "title": "An O(m+n)-Space Spatiotemporal Denoising Filter with Cache-Like Memories\n  for Dynamic Vision Sensors",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "An O(m+n)-Space Spatiotemporal Denoising Filter with Cache-Like Memories\n  for Dynamic Vision Sensors"
                },
                "summary": "Dynamic vision sensor (DVS) is novel neuromorphic imaging device that\ngenerates asynchronous events. Despite the high temporal resolution and high\ndynamic range features, DVS is faced with background noise problem.\nSpatiotemporal filter is an effective and hardware-friendly solution for DVS\ndenoising but previous designs have large memory overhead or degraded\nperformance issues. In this paper, we present a lightweight and real-time\nspatiotemporal denoising filter with set-associative cache-like memories, which\nhas low space complexity of \\text{O(m+n)} for DVS of $m\\times n$ resolution. A\ntwo-stage pipeline for memory access with read cancellation feature is proposed\nto reduce power consumption. Further the bitwidth redundancy for event storage\nis exploited to minimize the memory footprint. We implemented our design on\nFPGA and experimental results show that it achieves state-of-the-art\nperformance compared with previous spatiotemporal filters while maintaining low\nresource utilization and low power consumption of about 125mW to 210mW at\n100MHz clock frequency.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Dynamic vision sensor (DVS) is novel neuromorphic imaging device that\ngenerates asynchronous events. Despite the high temporal resolution and high\ndynamic range features, DVS is faced with background noise problem.\nSpatiotemporal filter is an effective and hardware-friendly solution for DVS\ndenoising but previous designs have large memory overhead or degraded\nperformance issues. In this paper, we present a lightweight and real-time\nspatiotemporal denoising filter with set-associative cache-like memories, which\nhas low space complexity of \\text{O(m+n)} for DVS of $m\\times n$ resolution. A\ntwo-stage pipeline for memory access with read cancellation feature is proposed\nto reduce power consumption. Further the bitwidth redundancy for event storage\nis exploited to minimize the memory footprint. We implemented our design on\nFPGA and experimental results show that it achieves state-of-the-art\nperformance compared with previous spatiotemporal filters while maintaining low\nresource utilization and low power consumption of about 125mW to 210mW at\n100MHz clock frequency."
                },
                "authors": [
                    {
                        "name": "Qinghang Zhao"
                    },
                    {
                        "name": "Jiaqi Wang"
                    },
                    {
                        "name": "Yixi Ji"
                    },
                    {
                        "name": "Jinjian Wu"
                    },
                    {
                        "name": "Guangming Shi"
                    }
                ],
                "author_detail": {
                    "name": "Guangming Shi"
                },
                "author": "Guangming Shi",
                "arxiv_doi": "10.1145/3676536.3676710",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3676536.3676710",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2410.12423v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.12423v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.14731v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.14731v1",
                "updated": "2024-10-16T08:34:51Z",
                "updated_parsed": [
                    2024,
                    10,
                    16,
                    8,
                    34,
                    51,
                    2,
                    290,
                    0
                ],
                "published": "2024-10-16T08:34:51Z",
                "published_parsed": [
                    2024,
                    10,
                    16,
                    8,
                    34,
                    51,
                    2,
                    290,
                    0
                ],
                "title": "MatryoshkaKV: Adaptive KV Compression via Trainable Orthogonal\n  Projection",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MatryoshkaKV: Adaptive KV Compression via Trainable Orthogonal\n  Projection"
                },
                "summary": "KV cache has become a de facto technique for the inference of large language\nmodels (LLMs), where tensors of shape (layer number, head number, sequence\nlength, feature dimension) are introduced to cache historical information for\nself-attention. As the size of the model and data grows, the KV cache can\nquickly become a bottleneck within the system in both storage and memory\ntransfer. To address this, prior studies usually focus on the first three axes\nof the cache tensors for compression. This paper supplements them, focusing on\nthe feature dimension axis, by utilizing low-rank projection matrices to\ntransform the cache features into spaces with reduced dimensions. We begin by\ninvestigating the canonical orthogonal projection method for data compression\nthrough principal component analysis (PCA). We observe the issue with PCA\nprojection where significant performance degradation is observed at low\ncompression rates. To bridge the gap, we propose to directly tune the\northogonal projection matrices with a distillation objective using an elaborate\nMatryoshka training strategy. After training, we adaptively search for the\noptimal compression rates for various layers and heads given varying\ncompression budgets. Compared to previous works, our method can easily embrace\npre-trained LLMs and hold a smooth tradeoff between performance and compression\nrate. We empirically witness the high data efficiency of our training procedure\nand find that our method can sustain over 90% performance with an average KV\ncache compression rate of 60% (and up to 75% in certain extreme scenarios) for\npopular LLMs like LLaMA2-7B-base and Mistral-7B-v0.3-base.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "KV cache has become a de facto technique for the inference of large language\nmodels (LLMs), where tensors of shape (layer number, head number, sequence\nlength, feature dimension) are introduced to cache historical information for\nself-attention. As the size of the model and data grows, the KV cache can\nquickly become a bottleneck within the system in both storage and memory\ntransfer. To address this, prior studies usually focus on the first three axes\nof the cache tensors for compression. This paper supplements them, focusing on\nthe feature dimension axis, by utilizing low-rank projection matrices to\ntransform the cache features into spaces with reduced dimensions. We begin by\ninvestigating the canonical orthogonal projection method for data compression\nthrough principal component analysis (PCA). We observe the issue with PCA\nprojection where significant performance degradation is observed at low\ncompression rates. To bridge the gap, we propose to directly tune the\northogonal projection matrices with a distillation objective using an elaborate\nMatryoshka training strategy. After training, we adaptively search for the\noptimal compression rates for various layers and heads given varying\ncompression budgets. Compared to previous works, our method can easily embrace\npre-trained LLMs and hold a smooth tradeoff between performance and compression\nrate. We empirically witness the high data efficiency of our training procedure\nand find that our method can sustain over 90% performance with an average KV\ncache compression rate of 60% (and up to 75% in certain extreme scenarios) for\npopular LLMs like LLaMA2-7B-base and Mistral-7B-v0.3-base."
                },
                "authors": [
                    {
                        "name": "Bokai Lin"
                    },
                    {
                        "name": "Zihao Zeng"
                    },
                    {
                        "name": "Zipeng Xiao"
                    },
                    {
                        "name": "Siqi Kou"
                    },
                    {
                        "name": "Tianqi Hou"
                    },
                    {
                        "name": "Xiaofeng Gao"
                    },
                    {
                        "name": "Hao Zhang"
                    },
                    {
                        "name": "Zhijie Deng"
                    }
                ],
                "author_detail": {
                    "name": "Zhijie Deng"
                },
                "author": "Zhijie Deng",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.14731v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.14731v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.12168v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.12168v1",
                "updated": "2024-10-16T02:16:53Z",
                "updated_parsed": [
                    2024,
                    10,
                    16,
                    2,
                    16,
                    53,
                    2,
                    290,
                    0
                ],
                "published": "2024-10-16T02:16:53Z",
                "published_parsed": [
                    2024,
                    10,
                    16,
                    2,
                    16,
                    53,
                    2,
                    290,
                    0
                ],
                "title": "COMET: Towards Partical W4A4KV4 LLMs Serving",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "COMET: Towards Partical W4A4KV4 LLMs Serving"
                },
                "summary": "Quantization is a widely-used compression technology to reduce the overhead\nof serving large language models (LLMs) on terminal devices and in cloud data\ncenters. However, prevalent quantization methods, such as 8-bit\nweight-activation or 4-bit weight-only quantization, achieve limited\nperformance improvements due to poor support for low-precision (e.g., 4-bit)\nactivation. This work, for the first time, realizes practical W4A4KV4 serving\nfor LLMs, fully utilizing the INT4 tensor cores on modern GPUs and reducing the\nmemory bottleneck caused by the KV cache. Specifically, we propose a novel\nfine-grained mixed-precision quantization algorithm (FMPQ) that compresses most\nactivations into 4-bit with negligible accuracy loss. To support\nmixed-precision matrix multiplication for W4A4 and W4A8, we develop a highly\noptimized W4Ax kernel. Our approach introduces a novel mixed-precision data\nlayout to facilitate access and fast dequantization for activation and weight\ntensors, utilizing the GPU's software pipeline to hide the overhead of data\nloading and conversion. Additionally, we propose fine-grained streaming\nmultiprocessor (SM) scheduling to achieve load balance across different SMs. We\nintegrate the optimized W4Ax kernel into our inference framework, COMET, and\nprovide efficient management to support popular LLMs such as LLaMA-3-70B.\nExtensive evaluations demonstrate that, when running LLaMA family models on a\nsingle A100-80G-SMX4, COMET achieves a kernel-level speedup of\n\\textbf{$2.88\\times$} over cuBLAS and a \\textbf{$2.02 \\times$} throughput\nimprovement compared to TensorRT-LLM from an end-to-end framework perspective.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Quantization is a widely-used compression technology to reduce the overhead\nof serving large language models (LLMs) on terminal devices and in cloud data\ncenters. However, prevalent quantization methods, such as 8-bit\nweight-activation or 4-bit weight-only quantization, achieve limited\nperformance improvements due to poor support for low-precision (e.g., 4-bit)\nactivation. This work, for the first time, realizes practical W4A4KV4 serving\nfor LLMs, fully utilizing the INT4 tensor cores on modern GPUs and reducing the\nmemory bottleneck caused by the KV cache. Specifically, we propose a novel\nfine-grained mixed-precision quantization algorithm (FMPQ) that compresses most\nactivations into 4-bit with negligible accuracy loss. To support\nmixed-precision matrix multiplication for W4A4 and W4A8, we develop a highly\noptimized W4Ax kernel. Our approach introduces a novel mixed-precision data\nlayout to facilitate access and fast dequantization for activation and weight\ntensors, utilizing the GPU's software pipeline to hide the overhead of data\nloading and conversion. Additionally, we propose fine-grained streaming\nmultiprocessor (SM) scheduling to achieve load balance across different SMs. We\nintegrate the optimized W4Ax kernel into our inference framework, COMET, and\nprovide efficient management to support popular LLMs such as LLaMA-3-70B.\nExtensive evaluations demonstrate that, when running LLaMA family models on a\nsingle A100-80G-SMX4, COMET achieves a kernel-level speedup of\n\\textbf{$2.88\\times$} over cuBLAS and a \\textbf{$2.02 \\times$} throughput\nimprovement compared to TensorRT-LLM from an end-to-end framework perspective."
                },
                "authors": [
                    {
                        "name": "Lian Liu"
                    },
                    {
                        "name": "Haimeng Ren"
                    },
                    {
                        "name": "Long Cheng"
                    },
                    {
                        "name": "Zhaohui Xu"
                    },
                    {
                        "name": "Yudong Pan"
                    },
                    {
                        "name": "Mengdi Wang"
                    },
                    {
                        "name": "Xiaowei Li"
                    },
                    {
                        "name": "Yinhe Han"
                    },
                    {
                        "name": "Ying Wang"
                    }
                ],
                "author_detail": {
                    "name": "Ying Wang"
                },
                "author": "Ying Wang",
                "arxiv_comment": "14 pages, 12 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.12168v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.12168v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.02536v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.02536v2",
                "updated": "2024-10-15T15:58:07Z",
                "updated_parsed": [
                    2024,
                    10,
                    15,
                    15,
                    58,
                    7,
                    1,
                    289,
                    0
                ],
                "published": "2024-06-04T17:55:38Z",
                "published_parsed": [
                    2024,
                    6,
                    4,
                    17,
                    55,
                    38,
                    1,
                    156,
                    0
                ],
                "title": "Mitigate Position Bias in Large Language Models via Scaling a Single\n  Dimension",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mitigate Position Bias in Large Language Models via Scaling a Single\n  Dimension"
                },
                "summary": "Large Language Models (LLMs) are increasingly applied in various real-world\nscenarios due to their excellent generalization capabilities and robust\ngenerative abilities. However, they exhibit position bias, also known as \"lost\nin the middle\", a phenomenon that is especially pronounced in long-context\nscenarios, which indicates the placement of the key information in different\npositions of a prompt can significantly affect accuracy. This paper first\nexplores the micro-level manifestations of position bias, concluding that\nattention weights are a micro-level expression of position bias. It further\nidentifies that, in addition to position embeddings, causal attention mask also\ncontributes to position bias by creating position-specific hidden states. Based\non these insights, we propose a method to mitigate position bias by scaling\nthis positional hidden states. Experiments on the NaturalQuestions\nMulti-document QA, KV retrieval, LongBench and timeline reorder tasks, using\nvarious models including RoPE models, context windowextended models, and Alibi\nmodels, demonstrate the effectiveness and generalizability of our approach. Our\nmethod can improve performance by up to 15.2% by modifying just one dimension\nof hidden states. Our code is available at https://aka.ms/PositionalHidden.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) are increasingly applied in various real-world\nscenarios due to their excellent generalization capabilities and robust\ngenerative abilities. However, they exhibit position bias, also known as \"lost\nin the middle\", a phenomenon that is especially pronounced in long-context\nscenarios, which indicates the placement of the key information in different\npositions of a prompt can significantly affect accuracy. This paper first\nexplores the micro-level manifestations of position bias, concluding that\nattention weights are a micro-level expression of position bias. It further\nidentifies that, in addition to position embeddings, causal attention mask also\ncontributes to position bias by creating position-specific hidden states. Based\non these insights, we propose a method to mitigate position bias by scaling\nthis positional hidden states. Experiments on the NaturalQuestions\nMulti-document QA, KV retrieval, LongBench and timeline reorder tasks, using\nvarious models including RoPE models, context windowextended models, and Alibi\nmodels, demonstrate the effectiveness and generalizability of our approach. Our\nmethod can improve performance by up to 15.2% by modifying just one dimension\nof hidden states. Our code is available at https://aka.ms/PositionalHidden."
                },
                "authors": [
                    {
                        "name": "Yijiong Yu"
                    },
                    {
                        "name": "Huiqiang Jiang"
                    },
                    {
                        "name": "Xufang Luo"
                    },
                    {
                        "name": "Qianhui Wu"
                    },
                    {
                        "name": "Chin-Yew Lin"
                    },
                    {
                        "name": "Dongsheng Li"
                    },
                    {
                        "name": "Yuqing Yang"
                    },
                    {
                        "name": "Yongfeng Huang"
                    },
                    {
                        "name": "Lili Qiu"
                    }
                ],
                "author_detail": {
                    "name": "Lili Qiu"
                },
                "author": "Lili Qiu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.02536v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.02536v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.11417v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.11417v1",
                "updated": "2024-10-15T09:07:25Z",
                "updated_parsed": [
                    2024,
                    10,
                    15,
                    9,
                    7,
                    25,
                    1,
                    289,
                    0
                ],
                "published": "2024-10-15T09:07:25Z",
                "published_parsed": [
                    2024,
                    10,
                    15,
                    9,
                    7,
                    25,
                    1,
                    289,
                    0
                ],
                "title": "VidCompress: Memory-Enhanced Temporal Compression for Video\n  Understanding in Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "VidCompress: Memory-Enhanced Temporal Compression for Video\n  Understanding in Large Language Models"
                },
                "summary": "Video-based multimodal large language models (Video-LLMs) possess significant\npotential for video understanding tasks. However, most Video-LLMs treat videos\nas a sequential set of individual frames, which results in insufficient\ntemporal-spatial interaction that hinders fine-grained comprehension and\ndifficulty in processing longer videos due to limited visual token capacity. To\naddress these challenges, we propose VidCompress, a novel Video-LLM featuring\nmemory-enhanced temporal compression. VidCompress employs a dual-compressor\napproach: a memory-enhanced compressor captures both short-term and long-term\ntemporal relationships in videos and compresses the visual tokens using a\nmultiscale transformer with a memory-cache mechanism, while a text-perceived\ncompressor generates condensed visual tokens by utilizing Q-Former and\nintegrating temporal contexts into query embeddings with cross attention.\nExperiments on several VideoQA datasets and comprehensive benchmarks\ndemonstrate that VidCompress efficiently models complex temporal-spatial\nrelations and significantly outperforms existing Video-LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Video-based multimodal large language models (Video-LLMs) possess significant\npotential for video understanding tasks. However, most Video-LLMs treat videos\nas a sequential set of individual frames, which results in insufficient\ntemporal-spatial interaction that hinders fine-grained comprehension and\ndifficulty in processing longer videos due to limited visual token capacity. To\naddress these challenges, we propose VidCompress, a novel Video-LLM featuring\nmemory-enhanced temporal compression. VidCompress employs a dual-compressor\napproach: a memory-enhanced compressor captures both short-term and long-term\ntemporal relationships in videos and compresses the visual tokens using a\nmultiscale transformer with a memory-cache mechanism, while a text-perceived\ncompressor generates condensed visual tokens by utilizing Q-Former and\nintegrating temporal contexts into query embeddings with cross attention.\nExperiments on several VideoQA datasets and comprehensive benchmarks\ndemonstrate that VidCompress efficiently models complex temporal-spatial\nrelations and significantly outperforms existing Video-LLMs."
                },
                "authors": [
                    {
                        "name": "Xiaohan Lan"
                    },
                    {
                        "name": "Yitian Yuan"
                    },
                    {
                        "name": "Zequn Jie"
                    },
                    {
                        "name": "Lin Ma"
                    }
                ],
                "author_detail": {
                    "name": "Lin Ma"
                },
                "author": "Lin Ma",
                "arxiv_comment": "9 pages, 4 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.11417v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.11417v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.09297v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.09297v3",
                "updated": "2024-10-15T08:45:18Z",
                "updated_parsed": [
                    2024,
                    10,
                    15,
                    8,
                    45,
                    18,
                    1,
                    289,
                    0
                ],
                "published": "2024-06-13T16:33:44Z",
                "published_parsed": [
                    2024,
                    6,
                    13,
                    16,
                    33,
                    44,
                    3,
                    165,
                    0
                ],
                "title": "MLKV: Multi-Layer Key-Value Heads for Memory Efficient Transformer\n  Decoding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MLKV: Multi-Layer Key-Value Heads for Memory Efficient Transformer\n  Decoding"
                },
                "summary": "Auto-regressive inference of transformers benefit greatly from Key-Value (KV)\ncaching, but can lead to major memory bottlenecks as model size, batch size,\nand sequence length grow at scale. We introduce Multi-Layer Key-Value (MLKV)\nsharing, a novel approach extending KV sharing across transformer layers to\nreduce memory usage beyond what was possible with Multi-Query Attention (MQA)\nand Grouped-Query Attention (GQA). Evaluations on various NLP benchmarks and\ninference metrics using uptrained Pythia-160M variants demonstrate that MLKV\nsignificantly reduces memory usage with minimal performance loss, reducing KV\ncache size down to a factor of 6x compared to MQA. These results highlight\nMLKV's potential for efficient deployment of transformer models at scale. We\nprovide code at https://github.com/zaydzuhri/pythia-mlkv",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Auto-regressive inference of transformers benefit greatly from Key-Value (KV)\ncaching, but can lead to major memory bottlenecks as model size, batch size,\nand sequence length grow at scale. We introduce Multi-Layer Key-Value (MLKV)\nsharing, a novel approach extending KV sharing across transformer layers to\nreduce memory usage beyond what was possible with Multi-Query Attention (MQA)\nand Grouped-Query Attention (GQA). Evaluations on various NLP benchmarks and\ninference metrics using uptrained Pythia-160M variants demonstrate that MLKV\nsignificantly reduces memory usage with minimal performance loss, reducing KV\ncache size down to a factor of 6x compared to MQA. These results highlight\nMLKV's potential for efficient deployment of transformer models at scale. We\nprovide code at https://github.com/zaydzuhri/pythia-mlkv"
                },
                "authors": [
                    {
                        "name": "Zayd Muhammad Kawakibi Zuhri"
                    },
                    {
                        "name": "Muhammad Farid Adilazuarda"
                    },
                    {
                        "name": "Ayu Purwarianti"
                    },
                    {
                        "name": "Alham Fikri Aji"
                    }
                ],
                "author_detail": {
                    "name": "Alham Fikri Aji"
                },
                "author": "Alham Fikri Aji",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.09297v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.09297v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.09827v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.09827v2",
                "updated": "2024-10-15T06:09:35Z",
                "updated_parsed": [
                    2024,
                    10,
                    15,
                    6,
                    9,
                    35,
                    1,
                    289,
                    0
                ],
                "published": "2024-06-14T08:32:45Z",
                "published_parsed": [
                    2024,
                    6,
                    14,
                    8,
                    32,
                    45,
                    4,
                    166,
                    0
                ],
                "title": "A Training-free Sub-quadratic Cost Transformer Model Serving Framework\n  With Hierarchically Pruned Attention",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Training-free Sub-quadratic Cost Transformer Model Serving Framework\n  With Hierarchically Pruned Attention"
                },
                "summary": "In modern large language models (LLMs), increasing the context length is\ncrucial for improving comprehension and coherence in long-context, multi-modal,\nand retrieval-augmented language generation. While many recent transformer\nmodels attempt to extend their context length over a million tokens, they\nremain impractical due to the quadratic time and space complexities. Although\nrecent works on linear and sparse attention mechanisms can achieve this goal,\ntheir real-world applicability is often limited by the need to re-train from\nscratch and significantly worse performance. In response, we propose a novel\napproach, Hierarchically Pruned Attention (HiP), which reduces the time\ncomplexity of the attention mechanism to $O(T \\log T)$ and the space complexity\nto $O(T)$, where $T$ is the sequence length. We notice a pattern in the\nattention scores of pretrained LLMs where tokens close together tend to have\nsimilar scores, which we call ``attention locality''. Based on this\nobservation, we utilize a novel tree-search-like algorithm that estimates the\ntop-$k$ key tokens for a given query on the fly, which is mathematically\nguaranteed to have better performance than random attention pruning. In\naddition to improving the time complexity of the attention mechanism, we\nfurther optimize GPU memory usage by implementing KV cache offloading, which\nstores only $O(\\log T)$ tokens on the GPU while maintaining similar decoding\nthroughput. Experiments on benchmarks show that HiP, with its training-free\nnature, significantly reduces both prefill and decoding latencies, as well as\nmemory usage, while maintaining high-quality generation with minimal\ndegradation. HiP enables pretrained LLMs to scale up to millions of tokens on\ncommodity GPUs, potentially unlocking long-context LLM applications previously\ndeemed infeasible.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In modern large language models (LLMs), increasing the context length is\ncrucial for improving comprehension and coherence in long-context, multi-modal,\nand retrieval-augmented language generation. While many recent transformer\nmodels attempt to extend their context length over a million tokens, they\nremain impractical due to the quadratic time and space complexities. Although\nrecent works on linear and sparse attention mechanisms can achieve this goal,\ntheir real-world applicability is often limited by the need to re-train from\nscratch and significantly worse performance. In response, we propose a novel\napproach, Hierarchically Pruned Attention (HiP), which reduces the time\ncomplexity of the attention mechanism to $O(T \\log T)$ and the space complexity\nto $O(T)$, where $T$ is the sequence length. We notice a pattern in the\nattention scores of pretrained LLMs where tokens close together tend to have\nsimilar scores, which we call ``attention locality''. Based on this\nobservation, we utilize a novel tree-search-like algorithm that estimates the\ntop-$k$ key tokens for a given query on the fly, which is mathematically\nguaranteed to have better performance than random attention pruning. In\naddition to improving the time complexity of the attention mechanism, we\nfurther optimize GPU memory usage by implementing KV cache offloading, which\nstores only $O(\\log T)$ tokens on the GPU while maintaining similar decoding\nthroughput. Experiments on benchmarks show that HiP, with its training-free\nnature, significantly reduces both prefill and decoding latencies, as well as\nmemory usage, while maintaining high-quality generation with minimal\ndegradation. HiP enables pretrained LLMs to scale up to millions of tokens on\ncommodity GPUs, potentially unlocking long-context LLM applications previously\ndeemed infeasible."
                },
                "authors": [
                    {
                        "name": "Heejun Lee"
                    },
                    {
                        "name": "Geon Park"
                    },
                    {
                        "name": "Youngwan Lee"
                    },
                    {
                        "name": "Jaduk Suh"
                    },
                    {
                        "name": "Jina Kim"
                    },
                    {
                        "name": "Wonyoung Jeong"
                    },
                    {
                        "name": "Bumsik Kim"
                    },
                    {
                        "name": "Hyemin Lee"
                    },
                    {
                        "name": "Myeongjae Jeon"
                    },
                    {
                        "name": "Sung Ju Hwang"
                    }
                ],
                "author_detail": {
                    "name": "Sung Ju Hwang"
                },
                "author": "Sung Ju Hwang",
                "arxiv_comment": "44 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.09827v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.09827v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.11305v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.11305v1",
                "updated": "2024-10-15T05:57:51Z",
                "updated_parsed": [
                    2024,
                    10,
                    15,
                    5,
                    57,
                    51,
                    1,
                    289,
                    0
                ],
                "published": "2024-10-15T05:57:51Z",
                "published_parsed": [
                    2024,
                    10,
                    15,
                    5,
                    57,
                    51,
                    1,
                    289,
                    0
                ],
                "title": "QSpec: Speculative Decoding with Complementary Quantization Schemes",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "QSpec: Speculative Decoding with Complementary Quantization Schemes"
                },
                "summary": "Quantization has been substantially adopted to accelerate inference and\nreduce memory consumption of large language models (LLMs). While\nactivation-weight joint quantization speeds up the inference process through\nlow-precision kernels, we demonstrate that it suffers severe performance\ndegradation on multi-step reasoning tasks, rendering it ineffective. We propose\na novel quantization paradigm called QSPEC, which seamlessly integrates two\ncomplementary quantization schemes for speculative decoding. Leveraging nearly\ncost-free execution switching, QSPEC drafts tokens with low-precision, fast\nactivation-weight quantization, and verifies them with high-precision\nweight-only quantization, effectively combining the strengths of both\nquantization schemes. Compared to high-precision quantization methods, QSPEC\nempirically boosts token generation throughput by up to 1.80x without any\nquality compromise, distinguishing it from other low-precision quantization\napproaches. This enhancement is also consistent across various serving tasks,\nmodel sizes, quantization methods, and batch sizes. Unlike existing speculative\ndecoding techniques, our approach reuses weights and the KV cache, avoiding\nadditional memory overhead. Furthermore, QSPEC offers a plug-and-play advantage\nwithout requiring any training. We believe that QSPEC demonstrates unique\nstrengths for future deployment of high-fidelity quantization schemes,\nparticularly in memory-constrained scenarios (e.g., edge devices).",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Quantization has been substantially adopted to accelerate inference and\nreduce memory consumption of large language models (LLMs). While\nactivation-weight joint quantization speeds up the inference process through\nlow-precision kernels, we demonstrate that it suffers severe performance\ndegradation on multi-step reasoning tasks, rendering it ineffective. We propose\na novel quantization paradigm called QSPEC, which seamlessly integrates two\ncomplementary quantization schemes for speculative decoding. Leveraging nearly\ncost-free execution switching, QSPEC drafts tokens with low-precision, fast\nactivation-weight quantization, and verifies them with high-precision\nweight-only quantization, effectively combining the strengths of both\nquantization schemes. Compared to high-precision quantization methods, QSPEC\nempirically boosts token generation throughput by up to 1.80x without any\nquality compromise, distinguishing it from other low-precision quantization\napproaches. This enhancement is also consistent across various serving tasks,\nmodel sizes, quantization methods, and batch sizes. Unlike existing speculative\ndecoding techniques, our approach reuses weights and the KV cache, avoiding\nadditional memory overhead. Furthermore, QSPEC offers a plug-and-play advantage\nwithout requiring any training. We believe that QSPEC demonstrates unique\nstrengths for future deployment of high-fidelity quantization schemes,\nparticularly in memory-constrained scenarios (e.g., edge devices)."
                },
                "authors": [
                    {
                        "name": "Juntao Zhao"
                    },
                    {
                        "name": "Wenhao Lu"
                    },
                    {
                        "name": "Sheng Wang"
                    },
                    {
                        "name": "Lingpeng Kong"
                    },
                    {
                        "name": "Chuan Wu"
                    }
                ],
                "author_detail": {
                    "name": "Chuan Wu"
                },
                "author": "Chuan Wu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.11305v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.11305v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.00080v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.00080v3",
                "updated": "2024-10-15T05:34:07Z",
                "updated_parsed": [
                    2024,
                    10,
                    15,
                    5,
                    34,
                    7,
                    1,
                    289,
                    0
                ],
                "published": "2024-04-30T16:35:08Z",
                "published_parsed": [
                    2024,
                    4,
                    30,
                    16,
                    35,
                    8,
                    1,
                    121,
                    0
                ],
                "title": "Recommenadation aided Caching using Combinatorial Multi-armed Bandits",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recommenadation aided Caching using Combinatorial Multi-armed Bandits"
                },
                "summary": "We study content caching with recommendations in a wireless network where the\nusers are connected through a base station equipped with a finite-capacity\ncache. We assume a fixed set of contents with unknown user preferences and\ncontent popularities. The base station can cache a subset of the contents and\ncan also recommend subsets of the contents to different users in order to\nencourage them to request the recommended contents. Recommendations, depending\non their acceptability, can thus be used to increase cache hits. We first\nassume that the users' recommendation acceptabilities are known and formulate\nthe cache hit optimization problem as a combinatorial multi-armed bandit\n(CMAB). We propose a UCB-based algorithm to decide which contents to cache and\nrecommend and provide an upper bound on the regret of this algorithm.\nSubsequently, we consider a more general scenario where the users'\nrecommendation acceptabilities are also unknown and propose another UCB-based\nalgorithm that learns these as well. We numerically demonstrate the performance\nof our algorithms and compare these to state-of-the-art algorithms.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We study content caching with recommendations in a wireless network where the\nusers are connected through a base station equipped with a finite-capacity\ncache. We assume a fixed set of contents with unknown user preferences and\ncontent popularities. The base station can cache a subset of the contents and\ncan also recommend subsets of the contents to different users in order to\nencourage them to request the recommended contents. Recommendations, depending\non their acceptability, can thus be used to increase cache hits. We first\nassume that the users' recommendation acceptabilities are known and formulate\nthe cache hit optimization problem as a combinatorial multi-armed bandit\n(CMAB). We propose a UCB-based algorithm to decide which contents to cache and\nrecommend and provide an upper bound on the regret of this algorithm.\nSubsequently, we consider a more general scenario where the users'\nrecommendation acceptabilities are also unknown and propose another UCB-based\nalgorithm that learns these as well. We numerically demonstrate the performance\nof our algorithms and compare these to state-of-the-art algorithms."
                },
                "authors": [
                    {
                        "name": "Pavamana K J"
                    },
                    {
                        "name": "Chandramani Kishore Singh"
                    }
                ],
                "author_detail": {
                    "name": "Chandramani Kishore Singh"
                },
                "author": "Chandramani Kishore Singh",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.00080v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.00080v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.11260v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.11260v1",
                "updated": "2024-10-15T04:35:49Z",
                "updated_parsed": [
                    2024,
                    10,
                    15,
                    4,
                    35,
                    49,
                    1,
                    289,
                    0
                ],
                "published": "2024-10-15T04:35:49Z",
                "published_parsed": [
                    2024,
                    10,
                    15,
                    4,
                    35,
                    49,
                    1,
                    289,
                    0
                ],
                "title": "A Zoned Storage Optimized Flash Cache on ZNS SSDs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Zoned Storage Optimized Flash Cache on ZNS SSDs"
                },
                "summary": "Zoned Namespace SSDs (ZNS) are introduced recently to mitigate the block\ninterface penalties of flash-based SSDs. It is a good opportunity for flash\ncache to address cache throughput and write amplification (WA) issues by fully\ncontrolling data allocation and garbage collection via zone-based interfaces.\nHowever, there are several critical challenges that need to be addressed\nincluding zone-interface compatibility, data management of large zone size, and\na better tradeoff between throughput, cache hit ratio, and WA.\n  In this paper, we present Z-CacheLib, a zoned storage optimized flash cache\non ZNS SSDs. In Z-CacheLib, we propose: 1) a new zStorage Engine for ZNS SSDs\nwith low mapping and operational overhead, and 2) a novel zCache Engine with\ncross-layer optimizations to resolve the throughput regression and WA issues of\ngarbage collection, which consists of delayed data eviction with virtual\nover-provisioning (vOP), a top-down eviction policy (zLRU) optimized from LRU,\nand a bottom-up drop mechanism (zDrop) for low WA. Our evaluation shows that\nZ-CacheLib can achieve up to 2X throughput, 5% improvement hit ratio, and\nalmost no WA compared to CacheLib with compatible regular SSDs, demonstrating\nbenefits of using ZNS SSDs for cache. Moreover, Z-CacheLib can achieve up to 6X\nthroughput and 92% WA reduction compared with F2FS-based scheme.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Zoned Namespace SSDs (ZNS) are introduced recently to mitigate the block\ninterface penalties of flash-based SSDs. It is a good opportunity for flash\ncache to address cache throughput and write amplification (WA) issues by fully\ncontrolling data allocation and garbage collection via zone-based interfaces.\nHowever, there are several critical challenges that need to be addressed\nincluding zone-interface compatibility, data management of large zone size, and\na better tradeoff between throughput, cache hit ratio, and WA.\n  In this paper, we present Z-CacheLib, a zoned storage optimized flash cache\non ZNS SSDs. In Z-CacheLib, we propose: 1) a new zStorage Engine for ZNS SSDs\nwith low mapping and operational overhead, and 2) a novel zCache Engine with\ncross-layer optimizations to resolve the throughput regression and WA issues of\ngarbage collection, which consists of delayed data eviction with virtual\nover-provisioning (vOP), a top-down eviction policy (zLRU) optimized from LRU,\nand a bottom-up drop mechanism (zDrop) for low WA. Our evaluation shows that\nZ-CacheLib can achieve up to 2X throughput, 5% improvement hit ratio, and\nalmost no WA compared to CacheLib with compatible regular SSDs, demonstrating\nbenefits of using ZNS SSDs for cache. Moreover, Z-CacheLib can achieve up to 6X\nthroughput and 92% WA reduction compared with F2FS-based scheme."
                },
                "authors": [
                    {
                        "name": "Chongzhuo Yang"
                    },
                    {
                        "name": "Chang Guo"
                    },
                    {
                        "name": "Ming Zhao"
                    },
                    {
                        "name": "Zhichao Cao"
                    }
                ],
                "author_detail": {
                    "name": "Zhichao Cao"
                },
                "author": "Zhichao Cao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.11260v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.11260v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.PF",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.03058v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.03058v4",
                "updated": "2024-10-14T19:12:48Z",
                "updated_parsed": [
                    2024,
                    10,
                    14,
                    19,
                    12,
                    48,
                    0,
                    288,
                    0
                ],
                "published": "2024-05-05T21:41:43Z",
                "published_parsed": [
                    2024,
                    5,
                    5,
                    21,
                    41,
                    43,
                    6,
                    126,
                    0
                ],
                "title": "Enhancing High-Level Synthesis with Automated Pragma Insertion and Code\n  Transformation Framework",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Enhancing High-Level Synthesis with Automated Pragma Insertion and Code\n  Transformation Framework"
                },
                "summary": "High-level synthesis, source-to-source compilers, and various Design Space\nExploration techniques for pragma insertion have significantly improved the\nQuality of Results of generated designs. These tools offer benefits such as\nreduced development time and enhanced performance. However, achieving\nhigh-quality results often requires additional manual code transformations and\ntiling selections, which are typically performed separately or as\npre-processing steps. Although DSE techniques enable code transformation\nupfront, the vastness of the search space often limits the exploration of all\npossible code transformations, making it challenging to determine which\ntransformations are necessary. Additionally, ensuring correctness remains\nchallenging, especially for complex transformations and optimizations.\n  To tackle this obstacle, we first propose a comprehensive framework\nleveraging HLS compilers. Our system streamlines code transformation, pragma\ninsertion, and tiles size selection for on-chip data caching through a unified\noptimization problem, aiming to enhance parallelization, particularly\nbeneficial for computation-bound kernels. Them employing a novel Non-Linear\nProgramming (NLP) approach, we simultaneously ascertain transformations,\npragmas, and tile sizes, focusing on regular loop-based kernels. Our evaluation\ndemonstrates that our framework adeptly identifies the appropriate\ntransformations, including scenarios where no transformation is necessary, and\ninserts pragmas to achieve a favorable Quality of Results.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "High-level synthesis, source-to-source compilers, and various Design Space\nExploration techniques for pragma insertion have significantly improved the\nQuality of Results of generated designs. These tools offer benefits such as\nreduced development time and enhanced performance. However, achieving\nhigh-quality results often requires additional manual code transformations and\ntiling selections, which are typically performed separately or as\npre-processing steps. Although DSE techniques enable code transformation\nupfront, the vastness of the search space often limits the exploration of all\npossible code transformations, making it challenging to determine which\ntransformations are necessary. Additionally, ensuring correctness remains\nchallenging, especially for complex transformations and optimizations.\n  To tackle this obstacle, we first propose a comprehensive framework\nleveraging HLS compilers. Our system streamlines code transformation, pragma\ninsertion, and tiles size selection for on-chip data caching through a unified\noptimization problem, aiming to enhance parallelization, particularly\nbeneficial for computation-bound kernels. Them employing a novel Non-Linear\nProgramming (NLP) approach, we simultaneously ascertain transformations,\npragmas, and tile sizes, focusing on regular loop-based kernels. Our evaluation\ndemonstrates that our framework adeptly identifies the appropriate\ntransformations, including scenarios where no transformation is necessary, and\ninserts pragmas to achieve a favorable Quality of Results."
                },
                "authors": [
                    {
                        "name": "Stéphane Pouget"
                    },
                    {
                        "name": "Louis-Noël Pouchet"
                    },
                    {
                        "name": "Jason Cong"
                    }
                ],
                "author_detail": {
                    "name": "Jason Cong"
                },
                "author": "Jason Cong",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.03058v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.03058v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.10819v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.10819v1",
                "updated": "2024-10-14T17:59:58Z",
                "updated_parsed": [
                    2024,
                    10,
                    14,
                    17,
                    59,
                    58,
                    0,
                    288,
                    0
                ],
                "published": "2024-10-14T17:59:58Z",
                "published_parsed": [
                    2024,
                    10,
                    14,
                    17,
                    59,
                    58,
                    0,
                    288,
                    0
                ],
                "title": "DuoAttention: Efficient Long-Context LLM Inference with Retrieval and\n  Streaming Heads",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DuoAttention: Efficient Long-Context LLM Inference with Retrieval and\n  Streaming Heads"
                },
                "summary": "Deploying long-context large language models (LLMs) is essential but poses\nsignificant computational and memory challenges. Caching all Key and Value (KV)\nstates across all attention heads consumes substantial memory. Existing KV\ncache pruning methods either damage the long-context capabilities of LLMs or\noffer only limited efficiency improvements. In this paper, we identify that\nonly a fraction of attention heads, a.k.a, Retrieval Heads, are critical for\nprocessing long contexts and require full attention across all tokens. In\ncontrast, all other heads, which primarily focus on recent tokens and attention\nsinks--referred to as Streaming Heads--do not require full attention. Based on\nthis insight, we introduce DuoAttention, a framework that only applies a full\nKV cache to retrieval heads while using a light-weight, constant-length KV\ncache for streaming heads, which reduces both LLM's decoding and pre-filling\nmemory and latency without compromising its long-context abilities.\nDuoAttention uses a lightweight, optimization-based algorithm with synthetic\ndata to identify retrieval heads accurately. Our method significantly reduces\nlong-context inference memory by up to 2.55x for MHA and 1.67x for GQA models\nwhile speeding up decoding by up to 2.18x and 1.50x and accelerating\npre-filling by up to 1.73x and 1.63x for MHA and GQA models, respectively, with\nminimal accuracy loss compared to full attention. Notably, combined with\nquantization, DuoAttention enables Llama-3-8B decoding with 3.3 million context\nlength on a single A100 GPU. Code is provided in\nhttps://github.com/mit-han-lab/duo-attention.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Deploying long-context large language models (LLMs) is essential but poses\nsignificant computational and memory challenges. Caching all Key and Value (KV)\nstates across all attention heads consumes substantial memory. Existing KV\ncache pruning methods either damage the long-context capabilities of LLMs or\noffer only limited efficiency improvements. In this paper, we identify that\nonly a fraction of attention heads, a.k.a, Retrieval Heads, are critical for\nprocessing long contexts and require full attention across all tokens. In\ncontrast, all other heads, which primarily focus on recent tokens and attention\nsinks--referred to as Streaming Heads--do not require full attention. Based on\nthis insight, we introduce DuoAttention, a framework that only applies a full\nKV cache to retrieval heads while using a light-weight, constant-length KV\ncache for streaming heads, which reduces both LLM's decoding and pre-filling\nmemory and latency without compromising its long-context abilities.\nDuoAttention uses a lightweight, optimization-based algorithm with synthetic\ndata to identify retrieval heads accurately. Our method significantly reduces\nlong-context inference memory by up to 2.55x for MHA and 1.67x for GQA models\nwhile speeding up decoding by up to 2.18x and 1.50x and accelerating\npre-filling by up to 1.73x and 1.63x for MHA and GQA models, respectively, with\nminimal accuracy loss compared to full attention. Notably, combined with\nquantization, DuoAttention enables Llama-3-8B decoding with 3.3 million context\nlength on a single A100 GPU. Code is provided in\nhttps://github.com/mit-han-lab/duo-attention."
                },
                "authors": [
                    {
                        "name": "Guangxuan Xiao"
                    },
                    {
                        "name": "Jiaming Tang"
                    },
                    {
                        "name": "Jingwei Zuo"
                    },
                    {
                        "name": "Junxian Guo"
                    },
                    {
                        "name": "Shang Yang"
                    },
                    {
                        "name": "Haotian Tang"
                    },
                    {
                        "name": "Yao Fu"
                    },
                    {
                        "name": "Song Han"
                    }
                ],
                "author_detail": {
                    "name": "Song Han"
                },
                "author": "Song Han",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.10819v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.10819v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.10781v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.10781v1",
                "updated": "2024-10-14T17:50:28Z",
                "updated_parsed": [
                    2024,
                    10,
                    14,
                    17,
                    50,
                    28,
                    0,
                    288,
                    0
                ],
                "published": "2024-10-14T17:50:28Z",
                "published_parsed": [
                    2024,
                    10,
                    14,
                    17,
                    50,
                    28,
                    0,
                    288,
                    0
                ],
                "title": "When Attention Sink Emerges in Language Models: An Empirical View",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "When Attention Sink Emerges in Language Models: An Empirical View"
                },
                "summary": "Language Models (LMs) assign significant attention to the first token, even\nif it is not semantically important, which is known as attention sink. This\nphenomenon has been widely adopted in applications such as streaming/long\ncontext generation, KV cache optimization, inference acceleration, model\nquantization, and others. Despite its widespread use, a deep understanding of\nattention sink in LMs is still lacking. In this work, we first demonstrate that\nattention sinks exist universally in LMs with various inputs, even in small\nmodels. Furthermore, attention sink is observed to emerge during the LM\npre-training, motivating us to investigate how optimization, data distribution,\nloss function, and model architecture in LM pre-training influence its\nemergence. We highlight that attention sink emerges after effective\noptimization on sufficient training data. The sink position is highly\ncorrelated with the loss function and data distribution. Most importantly, we\nfind that attention sink acts more like key biases, storing extra attention\nscores, which could be non-informative and not contribute to the value\ncomputation. We also observe that this phenomenon (at least partially) stems\nfrom tokens' inner dependence on attention scores as a result of softmax\nnormalization. After relaxing such dependence by replacing softmax attention\nwith other attention operations, such as sigmoid attention without\nnormalization, attention sinks do not emerge in LMs up to 1B parameters. The\ncode is available at https://github.com/sail-sg/Attention-Sink.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Language Models (LMs) assign significant attention to the first token, even\nif it is not semantically important, which is known as attention sink. This\nphenomenon has been widely adopted in applications such as streaming/long\ncontext generation, KV cache optimization, inference acceleration, model\nquantization, and others. Despite its widespread use, a deep understanding of\nattention sink in LMs is still lacking. In this work, we first demonstrate that\nattention sinks exist universally in LMs with various inputs, even in small\nmodels. Furthermore, attention sink is observed to emerge during the LM\npre-training, motivating us to investigate how optimization, data distribution,\nloss function, and model architecture in LM pre-training influence its\nemergence. We highlight that attention sink emerges after effective\noptimization on sufficient training data. The sink position is highly\ncorrelated with the loss function and data distribution. Most importantly, we\nfind that attention sink acts more like key biases, storing extra attention\nscores, which could be non-informative and not contribute to the value\ncomputation. We also observe that this phenomenon (at least partially) stems\nfrom tokens' inner dependence on attention scores as a result of softmax\nnormalization. After relaxing such dependence by replacing softmax attention\nwith other attention operations, such as sigmoid attention without\nnormalization, attention sinks do not emerge in LMs up to 1B parameters. The\ncode is available at https://github.com/sail-sg/Attention-Sink."
                },
                "authors": [
                    {
                        "name": "Xiangming Gu"
                    },
                    {
                        "name": "Tianyu Pang"
                    },
                    {
                        "name": "Chao Du"
                    },
                    {
                        "name": "Qian Liu"
                    },
                    {
                        "name": "Fengzhuo Zhang"
                    },
                    {
                        "name": "Cunxiao Du"
                    },
                    {
                        "name": "Ye Wang"
                    },
                    {
                        "name": "Min Lin"
                    }
                ],
                "author_detail": {
                    "name": "Min Lin"
                },
                "author": "Min Lin",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.10781v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.10781v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.10511v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.10511v1",
                "updated": "2024-10-14T13:49:06Z",
                "updated_parsed": [
                    2024,
                    10,
                    14,
                    13,
                    49,
                    6,
                    0,
                    288,
                    0
                ],
                "published": "2024-10-14T13:49:06Z",
                "published_parsed": [
                    2024,
                    10,
                    14,
                    13,
                    49,
                    6,
                    0,
                    288,
                    0
                ],
                "title": "Customize Your Visual Autoregressive Recipe with Set Autoregressive\n  Modeling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Customize Your Visual Autoregressive Recipe with Set Autoregressive\n  Modeling"
                },
                "summary": "We introduce a new paradigm for AutoRegressive (AR) image generation, termed\nSet AutoRegressive Modeling (SAR). SAR generalizes the conventional AR to the\nnext-set setting, i.e., splitting the sequence into arbitrary sets containing\nmultiple tokens, rather than outputting each token in a fixed raster order. To\naccommodate SAR, we develop a straightforward architecture termed Fully Masked\nTransformer. We reveal that existing AR variants correspond to specific design\nchoices of sequence order and output intervals within the SAR framework, with\nAR and Masked AR (MAR) as two extreme instances. Notably, SAR facilitates a\nseamless transition from AR to MAR, where intermediate states allow for\ntraining a causal model that benefits from both few-step inference and KV cache\nacceleration, thus leveraging the advantages of both AR and MAR. On the\nImageNet benchmark, we carefully explore the properties of SAR by analyzing the\nimpact of sequence order and output intervals on performance, as well as the\ngeneralization ability regarding inference order and steps. We further validate\nthe potential of SAR by training a 900M text-to-image model capable of\nsynthesizing photo-realistic images with any resolution. We hope our work may\ninspire more exploration and application of AR-based modeling across diverse\nmodalities.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce a new paradigm for AutoRegressive (AR) image generation, termed\nSet AutoRegressive Modeling (SAR). SAR generalizes the conventional AR to the\nnext-set setting, i.e., splitting the sequence into arbitrary sets containing\nmultiple tokens, rather than outputting each token in a fixed raster order. To\naccommodate SAR, we develop a straightforward architecture termed Fully Masked\nTransformer. We reveal that existing AR variants correspond to specific design\nchoices of sequence order and output intervals within the SAR framework, with\nAR and Masked AR (MAR) as two extreme instances. Notably, SAR facilitates a\nseamless transition from AR to MAR, where intermediate states allow for\ntraining a causal model that benefits from both few-step inference and KV cache\nacceleration, thus leveraging the advantages of both AR and MAR. On the\nImageNet benchmark, we carefully explore the properties of SAR by analyzing the\nimpact of sequence order and output intervals on performance, as well as the\ngeneralization ability regarding inference order and steps. We further validate\nthe potential of SAR by training a 900M text-to-image model capable of\nsynthesizing photo-realistic images with any resolution. We hope our work may\ninspire more exploration and application of AR-based modeling across diverse\nmodalities."
                },
                "authors": [
                    {
                        "name": "Wenze Liu"
                    },
                    {
                        "name": "Le Zhuo"
                    },
                    {
                        "name": "Yi Xin"
                    },
                    {
                        "name": "Sheng Xia"
                    },
                    {
                        "name": "Peng Gao"
                    },
                    {
                        "name": "Xiangyu Yue"
                    }
                ],
                "author_detail": {
                    "name": "Xiangyu Yue"
                },
                "author": "Xiangyu Yue",
                "arxiv_comment": "19 pages, 17 figures, 8 tables, github repo:\n  https://github.com/poppuppy/SAR",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.10511v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.10511v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.05317v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.05317v2",
                "updated": "2024-10-14T09:35:35Z",
                "updated_parsed": [
                    2024,
                    10,
                    14,
                    9,
                    35,
                    35,
                    0,
                    288,
                    0
                ],
                "published": "2024-10-05T03:47:06Z",
                "published_parsed": [
                    2024,
                    10,
                    5,
                    3,
                    47,
                    6,
                    5,
                    279,
                    0
                ],
                "title": "Accelerating Diffusion Transformers with Token-wise Feature Caching",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Accelerating Diffusion Transformers with Token-wise Feature Caching"
                },
                "summary": "Diffusion transformers have shown significant effectiveness in both image and\nvideo synthesis at the expense of huge computation costs. To address this\nproblem, feature caching methods have been introduced to accelerate diffusion\ntransformers by caching the features in previous timesteps and reusing them in\nthe following timesteps. However, previous caching methods ignore that\ndifferent tokens exhibit different sensitivities to feature caching, and\nfeature caching on some tokens may lead to 10$\\times$ more destruction to the\noverall generation quality compared with other tokens. In this paper, we\nintroduce token-wise feature caching, allowing us to adaptively select the most\nsuitable tokens for caching, and further enable us to apply different caching\nratios to neural layers in different types and depths. Extensive experiments on\nPixArt-$\\alpha$, OpenSora, and DiT demonstrate our effectiveness in both image\nand video generation with no requirements for training. For instance,\n2.36$\\times$ and 1.93$\\times$ acceleration are achieved on OpenSora and\nPixArt-$\\alpha$ with almost no drop in generation quality.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion transformers have shown significant effectiveness in both image and\nvideo synthesis at the expense of huge computation costs. To address this\nproblem, feature caching methods have been introduced to accelerate diffusion\ntransformers by caching the features in previous timesteps and reusing them in\nthe following timesteps. However, previous caching methods ignore that\ndifferent tokens exhibit different sensitivities to feature caching, and\nfeature caching on some tokens may lead to 10$\\times$ more destruction to the\noverall generation quality compared with other tokens. In this paper, we\nintroduce token-wise feature caching, allowing us to adaptively select the most\nsuitable tokens for caching, and further enable us to apply different caching\nratios to neural layers in different types and depths. Extensive experiments on\nPixArt-$\\alpha$, OpenSora, and DiT demonstrate our effectiveness in both image\nand video generation with no requirements for training. For instance,\n2.36$\\times$ and 1.93$\\times$ acceleration are achieved on OpenSora and\nPixArt-$\\alpha$ with almost no drop in generation quality."
                },
                "authors": [
                    {
                        "name": "Chang Zou"
                    },
                    {
                        "name": "Xuyang Liu"
                    },
                    {
                        "name": "Ting Liu"
                    },
                    {
                        "name": "Siteng Huang"
                    },
                    {
                        "name": "Linfeng Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Linfeng Zhang"
                },
                "author": "Linfeng Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.05317v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.05317v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.13378v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.13378v2",
                "updated": "2024-10-14T07:58:39Z",
                "updated_parsed": [
                    2024,
                    10,
                    14,
                    7,
                    58,
                    39,
                    0,
                    288,
                    0
                ],
                "published": "2024-05-22T06:19:43Z",
                "published_parsed": [
                    2024,
                    5,
                    22,
                    6,
                    19,
                    43,
                    2,
                    143,
                    0
                ],
                "title": "FedCache 2.0: Federated Edge Learning with Knowledge Caching and Dataset\n  Distillation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FedCache 2.0: Federated Edge Learning with Knowledge Caching and Dataset\n  Distillation"
                },
                "summary": "Federated Edge Learning (FEL) has emerged as a promising approach for\nenabling edge devices to collaboratively train machine learning models while\npreserving data privacy. Despite its advantages, practical FEL deployment faces\nsignificant challenges related to device constraints and device-server\ninteractions, necessitating heterogeneous, user-adaptive model training with\nlimited and uncertain communication. In this paper, we introduce FedCache 2.0,\na novel personalized FEL architecture that simultaneously addresses these\nchallenges. FedCache 2.0 incorporates the benefits of both dataset distillation\nand knowledge cache-driven federated learning by storing and organizing\ndistilled data as knowledge in the server-side knowledge cache. Moreover, a\ndevice-centric cache sampling strategy is introduced to tailor transferred\nknowledge for individual devices within controlled communication bandwidth.\nExtensive experiments on five datasets covering image recognition, audio\nunderstanding, and mobile sensor data mining tasks demonstrate that (1)\nFedCache 2.0 significantly outperforms state-of-the-art methods regardless of\nmodel structures, data distributions, and modalities. (2) FedCache 2.0 can\ntrain splendid personalized on-device models with at least $\\times$28.6\nimprovement in communication efficiency.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Federated Edge Learning (FEL) has emerged as a promising approach for\nenabling edge devices to collaboratively train machine learning models while\npreserving data privacy. Despite its advantages, practical FEL deployment faces\nsignificant challenges related to device constraints and device-server\ninteractions, necessitating heterogeneous, user-adaptive model training with\nlimited and uncertain communication. In this paper, we introduce FedCache 2.0,\na novel personalized FEL architecture that simultaneously addresses these\nchallenges. FedCache 2.0 incorporates the benefits of both dataset distillation\nand knowledge cache-driven federated learning by storing and organizing\ndistilled data as knowledge in the server-side knowledge cache. Moreover, a\ndevice-centric cache sampling strategy is introduced to tailor transferred\nknowledge for individual devices within controlled communication bandwidth.\nExtensive experiments on five datasets covering image recognition, audio\nunderstanding, and mobile sensor data mining tasks demonstrate that (1)\nFedCache 2.0 significantly outperforms state-of-the-art methods regardless of\nmodel structures, data distributions, and modalities. (2) FedCache 2.0 can\ntrain splendid personalized on-device models with at least $\\times$28.6\nimprovement in communication efficiency."
                },
                "authors": [
                    {
                        "name": "Quyang Pan"
                    },
                    {
                        "name": "Sheng Sun"
                    },
                    {
                        "name": "Zhiyuan Wu"
                    },
                    {
                        "name": "Yuwei Wang"
                    },
                    {
                        "name": "Min Liu"
                    },
                    {
                        "name": "Bo Gao"
                    },
                    {
                        "name": "Jingyuan Wang"
                    }
                ],
                "author_detail": {
                    "name": "Jingyuan Wang"
                },
                "author": "Jingyuan Wang",
                "arxiv_comment": "17 pages, 7 figures, 14 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.13378v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.13378v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.10157v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.10157v1",
                "updated": "2024-10-14T04:49:22Z",
                "updated_parsed": [
                    2024,
                    10,
                    14,
                    4,
                    49,
                    22,
                    0,
                    288,
                    0
                ],
                "published": "2024-10-14T04:49:22Z",
                "published_parsed": [
                    2024,
                    10,
                    14,
                    4,
                    49,
                    22,
                    0,
                    288,
                    0
                ],
                "title": "Caching Content Placement and Beamforming Co-design for IRS-Aided MIMO\n  Systems with Imperfect CSI",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Caching Content Placement and Beamforming Co-design for IRS-Aided MIMO\n  Systems with Imperfect CSI"
                },
                "summary": "When offloading links encounter deep fading and obstruction, edge caching\ncannot fully enhance wireless network performance and improve the QoS of edge\nnodes, as it fails to effectively reduce backhaul burden. The emerging\ntechnology of intelligent reflecting surfaces (IRS) compensates for this\ndisadvantage by creating a smart and reconfigurable wireless environment.\nSubsequently, we jointly design content placement and active/passive\nbeamforming to minimize network costs under imperfect channel state information\n(CSI) in the IRS-oriented edge caching system. This minimization problem is\ndecomposed into two subproblems. The content placement subproblem is addressed\nby applying KKT optimality conditions. We then develop the alternating\noptimization method to resolve precoder and reflection beamforming.\nSpecifically, we reduce transmission power by first fixing the phase shift,\nreducing the problem to a convex one relative to the precoder, which is solved\nthrough convex optimization. Next, we fix the precoder and resolve the\nresulting reflection beamforming problem using the penalty convex-concave\nprocedure (CCP) method. Results demonstrate that our proposed method\noutperforms uniform caching and random phase approaches in reducing\ntransmission power and saving network costs. Eventually, the proposed approach\noffers potential improvements in the caching optimization and transmission\nrobustness of wireless communication with imperfect CSI.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "When offloading links encounter deep fading and obstruction, edge caching\ncannot fully enhance wireless network performance and improve the QoS of edge\nnodes, as it fails to effectively reduce backhaul burden. The emerging\ntechnology of intelligent reflecting surfaces (IRS) compensates for this\ndisadvantage by creating a smart and reconfigurable wireless environment.\nSubsequently, we jointly design content placement and active/passive\nbeamforming to minimize network costs under imperfect channel state information\n(CSI) in the IRS-oriented edge caching system. This minimization problem is\ndecomposed into two subproblems. The content placement subproblem is addressed\nby applying KKT optimality conditions. We then develop the alternating\noptimization method to resolve precoder and reflection beamforming.\nSpecifically, we reduce transmission power by first fixing the phase shift,\nreducing the problem to a convex one relative to the precoder, which is solved\nthrough convex optimization. Next, we fix the precoder and resolve the\nresulting reflection beamforming problem using the penalty convex-concave\nprocedure (CCP) method. Results demonstrate that our proposed method\noutperforms uniform caching and random phase approaches in reducing\ntransmission power and saving network costs. Eventually, the proposed approach\noffers potential improvements in the caching optimization and transmission\nrobustness of wireless communication with imperfect CSI."
                },
                "authors": [
                    {
                        "name": "Meng Gao"
                    },
                    {
                        "name": "Yang Wang"
                    },
                    {
                        "name": "Huafu Li"
                    },
                    {
                        "name": "Junqi Guo"
                    }
                ],
                "author_detail": {
                    "name": "Junqi Guo"
                },
                "author": "Junqi Guo",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.10157v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.10157v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SP",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.10149v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.10149v1",
                "updated": "2024-10-14T04:30:38Z",
                "updated_parsed": [
                    2024,
                    10,
                    14,
                    4,
                    30,
                    38,
                    0,
                    288,
                    0
                ],
                "published": "2024-10-14T04:30:38Z",
                "published_parsed": [
                    2024,
                    10,
                    14,
                    4,
                    30,
                    38,
                    0,
                    288,
                    0
                ],
                "title": "Fast and Accurate Neural Rendering Using Semi-Gradients",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Fast and Accurate Neural Rendering Using Semi-Gradients"
                },
                "summary": "We propose a simple yet effective neural network-based framework for global\nillumination rendering. Recently, rendering techniques that learn neural\nradiance caches by minimizing the difference (i.e., residual) between the left\nand right sides of the rendering equation have been suggested. Due to their\nease of implementation and the advantage of excluding path integral\ncalculations, these techniques have been applied to various fields, such as\nfree-viewpoint rendering, differentiable rendering, and real-time rendering.\nHowever, issues of slow training and occasionally darkened renders have been\nnoted. We identify the cause of these issues as the bias and high variance\npresent in the gradient estimates of the existing residual-based objective\nfunction. To address this, we introduce a new objective function that maintains\nthe same global optimum as before but allows for unbiased and low-variance\ngradient estimates, enabling faster and more accurate training of neural\nnetworks. In conclusion, this method is simply implemented by ignoring the\npartial derivatives of the right-hand side, and theoretical and experimental\nanalyses demonstrate the effectiveness of the proposed loss.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We propose a simple yet effective neural network-based framework for global\nillumination rendering. Recently, rendering techniques that learn neural\nradiance caches by minimizing the difference (i.e., residual) between the left\nand right sides of the rendering equation have been suggested. Due to their\nease of implementation and the advantage of excluding path integral\ncalculations, these techniques have been applied to various fields, such as\nfree-viewpoint rendering, differentiable rendering, and real-time rendering.\nHowever, issues of slow training and occasionally darkened renders have been\nnoted. We identify the cause of these issues as the bias and high variance\npresent in the gradient estimates of the existing residual-based objective\nfunction. To address this, we introduce a new objective function that maintains\nthe same global optimum as before but allows for unbiased and low-variance\ngradient estimates, enabling faster and more accurate training of neural\nnetworks. In conclusion, this method is simply implemented by ignoring the\npartial derivatives of the right-hand side, and theoretical and experimental\nanalyses demonstrate the effectiveness of the proposed loss."
                },
                "authors": [
                    {
                        "name": "In-Young Cho"
                    },
                    {
                        "name": "Jaewoong Cho"
                    }
                ],
                "author_detail": {
                    "name": "Jaewoong Cho"
                },
                "author": "Jaewoong Cho",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.10149v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.10149v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.GR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.10071v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.10071v1",
                "updated": "2024-10-14T01:25:56Z",
                "updated_parsed": [
                    2024,
                    10,
                    14,
                    1,
                    25,
                    56,
                    0,
                    288,
                    0
                ],
                "published": "2024-10-14T01:25:56Z",
                "published_parsed": [
                    2024,
                    10,
                    14,
                    1,
                    25,
                    56,
                    0,
                    288,
                    0
                ],
                "title": "Content Caching-Assisted Vehicular Edge Computing Using Multi-Agent\n  Graph Attention Reinforcement Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Content Caching-Assisted Vehicular Edge Computing Using Multi-Agent\n  Graph Attention Reinforcement Learning"
                },
                "summary": "In order to avoid repeated task offloading and realize the reuse of popular\ntask computing results, we construct a novel content caching-assisted vehicular\nedge computing (VEC) framework. In the face of irregular network topology and\nunknown environmental dynamics, we further propose a multi-agent graph\nattention reinforcement learning (MGARL) based edge caching scheme, which\nutilizes the graph attention convolution kernel to integrate the neighboring\nnodes' features of each agent and further enhance the cooperation among agents.\nOur simulation results show that our proposed scheme is capable of improving\nthe utilization of caching resources while reducing the long-term task\ncomputing latency compared to the baselines.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In order to avoid repeated task offloading and realize the reuse of popular\ntask computing results, we construct a novel content caching-assisted vehicular\nedge computing (VEC) framework. In the face of irregular network topology and\nunknown environmental dynamics, we further propose a multi-agent graph\nattention reinforcement learning (MGARL) based edge caching scheme, which\nutilizes the graph attention convolution kernel to integrate the neighboring\nnodes' features of each agent and further enhance the cooperation among agents.\nOur simulation results show that our proposed scheme is capable of improving\nthe utilization of caching resources while reducing the long-term task\ncomputing latency compared to the baselines."
                },
                "authors": [
                    {
                        "name": "Jinjin Shen"
                    },
                    {
                        "name": "Yan Lin"
                    },
                    {
                        "name": "Yijin Zhang"
                    },
                    {
                        "name": "Weibin Zhang"
                    },
                    {
                        "name": "Feng Shu"
                    },
                    {
                        "name": "Jun Li"
                    }
                ],
                "author_detail": {
                    "name": "Jun Li"
                },
                "author": "Jun Li",
                "arxiv_comment": "6 pages, 5 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.10071v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.10071v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.MA",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.ET",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.09533v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.09533v1",
                "updated": "2024-10-12T13:45:26Z",
                "updated_parsed": [
                    2024,
                    10,
                    12,
                    13,
                    45,
                    26,
                    5,
                    286,
                    0
                ],
                "published": "2024-10-12T13:45:26Z",
                "published_parsed": [
                    2024,
                    10,
                    12,
                    13,
                    45,
                    26,
                    5,
                    286,
                    0
                ],
                "title": "Leveraging Semantic Cues from Foundation Vision Models for Enhanced\n  Local Feature Correspondence",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Leveraging Semantic Cues from Foundation Vision Models for Enhanced\n  Local Feature Correspondence"
                },
                "summary": "Visual correspondence is a crucial step in key computer vision tasks,\nincluding camera localization, image registration, and structure from motion.\nThe most effective techniques for matching keypoints currently involve using\nlearned sparse or dense matchers, which need pairs of images. These neural\nnetworks have a good general understanding of features from both images, but\nthey often struggle to match points from different semantic areas. This paper\npresents a new method that uses semantic cues from foundation vision model\nfeatures (like DINOv2) to enhance local feature matching by incorporating\nsemantic reasoning into existing descriptors. Therefore, the learned\ndescriptors do not require image pairs at inference time, allowing feature\ncaching and fast matching using similarity search, unlike learned matchers. We\npresent adapted versions of six existing descriptors, with an average increase\nin performance of 29% in camera localization, with comparable accuracy to\nexisting matchers as LightGlue and LoFTR in two existing benchmarks. Both code\nand trained models are available at\nhttps://www.verlab.dcc.ufmg.br/descriptors/reasoning_accv24",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Visual correspondence is a crucial step in key computer vision tasks,\nincluding camera localization, image registration, and structure from motion.\nThe most effective techniques for matching keypoints currently involve using\nlearned sparse or dense matchers, which need pairs of images. These neural\nnetworks have a good general understanding of features from both images, but\nthey often struggle to match points from different semantic areas. This paper\npresents a new method that uses semantic cues from foundation vision model\nfeatures (like DINOv2) to enhance local feature matching by incorporating\nsemantic reasoning into existing descriptors. Therefore, the learned\ndescriptors do not require image pairs at inference time, allowing feature\ncaching and fast matching using similarity search, unlike learned matchers. We\npresent adapted versions of six existing descriptors, with an average increase\nin performance of 29% in camera localization, with comparable accuracy to\nexisting matchers as LightGlue and LoFTR in two existing benchmarks. Both code\nand trained models are available at\nhttps://www.verlab.dcc.ufmg.br/descriptors/reasoning_accv24"
                },
                "authors": [
                    {
                        "name": "Felipe Cadar"
                    },
                    {
                        "name": "Guilherme Potje"
                    },
                    {
                        "name": "Renato Martins"
                    },
                    {
                        "name": "Cédric Demonceaux"
                    },
                    {
                        "name": "Erickson R. Nascimento"
                    }
                ],
                "author_detail": {
                    "name": "Erickson R. Nascimento"
                },
                "author": "Erickson R. Nascimento",
                "arxiv_comment": "Accepted in ACCV 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.09533v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.09533v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.09479v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.09479v1",
                "updated": "2024-10-12T10:38:39Z",
                "updated_parsed": [
                    2024,
                    10,
                    12,
                    10,
                    38,
                    39,
                    5,
                    286,
                    0
                ],
                "published": "2024-10-12T10:38:39Z",
                "published_parsed": [
                    2024,
                    10,
                    12,
                    10,
                    38,
                    39,
                    5,
                    286,
                    0
                ],
                "title": "Viscoelastic Effects on the Hydrodynamics of an Active Compound Particle",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Viscoelastic Effects on the Hydrodynamics of an Active Compound Particle"
                },
                "summary": "Understanding the hydrodynamics of microswimmers in viscoelastic fluids and\nconfined environments is crucial for interpreting their behaviour in natural\nsettings and designing synthetic microswimmers for practical applications like\ncargo transport. In this study, we explore the hydrodynamics of a concentric\nactive compound particle - a model microswimmer (a squirmer) positioned at the\ncentre of a viscoelastic fluid droplet (a model cargo) suspended in another\nviscoelastic medium. We consider the Oldroyd-B constitutive model to\ncharacterize the fluids and employ a perturbative approach in the Deborah\nnumber to analyze viscoelastic effects analytically, assuming a small Capillary\nnumber so that the droplet remains spherical and does not deform. We examine\nthree cases: (i) a squirmer confined within a viscoelastic fluid droplet\nsuspended in a Newtonian fluid, (ii) a squirmer confined within a Newtonian\nfluid droplet suspended in a viscoelastic fluid, and (iii) a squirmer confined\nwithin a viscoelastic fluid droplet suspended in another viscoelastic fluid.\nOur findings reveal that the swimming speeds of the squirmer and the droplet\nare determined by the complex interplay of viscoelasticity, the size ratio of\nthe droplet to the squirmer (confinement strength), and the viscosity ratio of\nthe surrounding fluid to the droplet fluid. A critical aspect of this\ninteraction is the positioning of stagnation points within the fluid flow,\nwhich governs the distribution of polymeric stress. This distribution, in turn,\nplays a crucial role in determining the influence of viscoelasticity on the\nsquirmer's dynamics. Our analysis suggests that viscoelastic effects can either\nenhance or hinder the swimming speed of the squirmer when confined in a\ndroplet, depending on the specific configuration of the system.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Understanding the hydrodynamics of microswimmers in viscoelastic fluids and\nconfined environments is crucial for interpreting their behaviour in natural\nsettings and designing synthetic microswimmers for practical applications like\ncargo transport. In this study, we explore the hydrodynamics of a concentric\nactive compound particle - a model microswimmer (a squirmer) positioned at the\ncentre of a viscoelastic fluid droplet (a model cargo) suspended in another\nviscoelastic medium. We consider the Oldroyd-B constitutive model to\ncharacterize the fluids and employ a perturbative approach in the Deborah\nnumber to analyze viscoelastic effects analytically, assuming a small Capillary\nnumber so that the droplet remains spherical and does not deform. We examine\nthree cases: (i) a squirmer confined within a viscoelastic fluid droplet\nsuspended in a Newtonian fluid, (ii) a squirmer confined within a Newtonian\nfluid droplet suspended in a viscoelastic fluid, and (iii) a squirmer confined\nwithin a viscoelastic fluid droplet suspended in another viscoelastic fluid.\nOur findings reveal that the swimming speeds of the squirmer and the droplet\nare determined by the complex interplay of viscoelasticity, the size ratio of\nthe droplet to the squirmer (confinement strength), and the viscosity ratio of\nthe surrounding fluid to the droplet fluid. A critical aspect of this\ninteraction is the positioning of stagnation points within the fluid flow,\nwhich governs the distribution of polymeric stress. This distribution, in turn,\nplays a crucial role in determining the influence of viscoelasticity on the\nsquirmer's dynamics. Our analysis suggests that viscoelastic effects can either\nenhance or hinder the swimming speed of the squirmer when confined in a\ndroplet, depending on the specific configuration of the system."
                },
                "authors": [
                    {
                        "name": "KVS Chaithanya"
                    },
                    {
                        "name": "Sumesh P. Thampi"
                    }
                ],
                "author_detail": {
                    "name": "Sumesh P. Thampi"
                },
                "author": "Sumesh P. Thampi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.09479v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.09479v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.flu-dyn",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.flu-dyn",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cond-mat.soft",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.09397v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.09397v1",
                "updated": "2024-10-12T07:01:30Z",
                "updated_parsed": [
                    2024,
                    10,
                    12,
                    7,
                    1,
                    30,
                    5,
                    286,
                    0
                ],
                "published": "2024-10-12T07:01:30Z",
                "published_parsed": [
                    2024,
                    10,
                    12,
                    7,
                    1,
                    30,
                    5,
                    286,
                    0
                ],
                "title": "Fine-grained Attention I/O Complexity: Comprehensive Analysis for\n  Backward Passes",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Fine-grained Attention I/O Complexity: Comprehensive Analysis for\n  Backward Passes"
                },
                "summary": "Large Language Models (LLMs) have demonstrated remarkable capabilities in\nprocessing long-context information. However, the quadratic complexity of\nattention computation with respect to sequence length poses significant\ncomputational challenges, and I/O aware algorithms have been proposed. This\npaper presents a comprehensive analysis of the I/O complexity for attention\nmechanisms, focusing on backward passes by categorizing into small and large\ncache scenarios. Using the red-blue pebble game framework, we establish tight\nbounds on I/O complexity across all cache sizes. We confirm that the de facto\nstandard I/O aware algorithm FlashAttention is optimal for both forward and\nbackward passes for the large cache size scenario. For small cache sizes, we\nprovide an algorithm that improves over existing methods and achieves the tight\nbounds. Additionally, we extend our analysis to sparse attention, a mainstream\nspeeding-up approach, deriving fine-grained lower bounds for both forward and\nbackward passes and both small and large caches. Our findings complete the\ntheoretical foundation for I/O complexity in attention mechanisms, offering\ninsights for designing efficient algorithms of LLM training and inference.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have demonstrated remarkable capabilities in\nprocessing long-context information. However, the quadratic complexity of\nattention computation with respect to sequence length poses significant\ncomputational challenges, and I/O aware algorithms have been proposed. This\npaper presents a comprehensive analysis of the I/O complexity for attention\nmechanisms, focusing on backward passes by categorizing into small and large\ncache scenarios. Using the red-blue pebble game framework, we establish tight\nbounds on I/O complexity across all cache sizes. We confirm that the de facto\nstandard I/O aware algorithm FlashAttention is optimal for both forward and\nbackward passes for the large cache size scenario. For small cache sizes, we\nprovide an algorithm that improves over existing methods and achieves the tight\nbounds. Additionally, we extend our analysis to sparse attention, a mainstream\nspeeding-up approach, deriving fine-grained lower bounds for both forward and\nbackward passes and both small and large caches. Our findings complete the\ntheoretical foundation for I/O complexity in attention mechanisms, offering\ninsights for designing efficient algorithms of LLM training and inference."
                },
                "authors": [
                    {
                        "name": "Xiaoyu Li"
                    },
                    {
                        "name": "Yingyu Liang"
                    },
                    {
                        "name": "Zhenmei Shi"
                    },
                    {
                        "name": "Zhao Song"
                    },
                    {
                        "name": "Yufa Zhou"
                    }
                ],
                "author_detail": {
                    "name": "Yufa Zhou"
                },
                "author": "Yufa Zhou",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.09397v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.09397v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.14360v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.14360v3",
                "updated": "2024-10-12T02:11:14Z",
                "updated_parsed": [
                    2024,
                    10,
                    12,
                    2,
                    11,
                    14,
                    5,
                    286,
                    0
                ],
                "published": "2024-09-22T08:30:43Z",
                "published_parsed": [
                    2024,
                    9,
                    22,
                    8,
                    30,
                    43,
                    6,
                    266,
                    0
                ],
                "title": "In-place Switch: Reprogramming based SLC Cache Design for Hybrid 3D SSDs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In-place Switch: Reprogramming based SLC Cache Design for Hybrid 3D SSDs"
                },
                "summary": "Recently, 3D SSDs are widely adopted in PCs, data centers, and cloud storage\nsystems. To increase capacity, high bit-density cells, such as Triple-Level\nCell (TLC), are utilized within 3D SSDs. However, due to the inferior\nperformance of TLC, a portion of TLCs is configured to operate as Single-Level\nCell (SLC) to provide high performance, with host data initially directed to\nthe SLCs. In SLC/TLC hybrid 3D SSDs, a portion of the TLC space is designated\nas an SLC cache to achieve high SSD performance by writing host data at the SLC\nspeed. Given the limited size of the SLC cache, block reclamation is necessary\nto free up the SLC cache during idle periods. However, our preliminary studies\nindicate that the SLC cache can lead to a performance cliff if filled rapidly\nand cause significant write amplification when data migration occurs during\nidle times.\n  In this work, we propose leveraging a reprogram operation to address these\nchallenges. Specifically, when the SLC cache is full or during idle periods, a\nreprogram operation is performed to switch used SLC pages to TLC pages in place\n(termed In-place Switch, IPS). Subsequently, other free TLC space is allocated\nas the new SLC cache. IPS can continuously provide sufficient SLC cache within\nSSDs, significantly improving write performance and reducing write\namplification. Experimental results demonstrate that IPS can reduce write\nlatency and write amplification by up to 0.75 times and 0.53 times,\nrespectively, compared to state-of-the-art SLC cache technologies.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recently, 3D SSDs are widely adopted in PCs, data centers, and cloud storage\nsystems. To increase capacity, high bit-density cells, such as Triple-Level\nCell (TLC), are utilized within 3D SSDs. However, due to the inferior\nperformance of TLC, a portion of TLCs is configured to operate as Single-Level\nCell (SLC) to provide high performance, with host data initially directed to\nthe SLCs. In SLC/TLC hybrid 3D SSDs, a portion of the TLC space is designated\nas an SLC cache to achieve high SSD performance by writing host data at the SLC\nspeed. Given the limited size of the SLC cache, block reclamation is necessary\nto free up the SLC cache during idle periods. However, our preliminary studies\nindicate that the SLC cache can lead to a performance cliff if filled rapidly\nand cause significant write amplification when data migration occurs during\nidle times.\n  In this work, we propose leveraging a reprogram operation to address these\nchallenges. Specifically, when the SLC cache is full or during idle periods, a\nreprogram operation is performed to switch used SLC pages to TLC pages in place\n(termed In-place Switch, IPS). Subsequently, other free TLC space is allocated\nas the new SLC cache. IPS can continuously provide sufficient SLC cache within\nSSDs, significantly improving write performance and reducing write\namplification. Experimental results demonstrate that IPS can reduce write\nlatency and write amplification by up to 0.75 times and 0.53 times,\nrespectively, compared to state-of-the-art SLC cache technologies."
                },
                "authors": [
                    {
                        "name": "Xufeng Yang"
                    },
                    {
                        "name": "Zhengjian Cong"
                    },
                    {
                        "name": "Congming Gao"
                    }
                ],
                "author_detail": {
                    "name": "Congming Gao"
                },
                "author": "Congming Gao",
                "arxiv_comment": "This paper has been submitted to NAS'24 (The 17th International\n  Conference on Networking, Architecture and Storage)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.14360v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.14360v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.09237v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.09237v1",
                "updated": "2024-10-11T20:23:00Z",
                "updated_parsed": [
                    2024,
                    10,
                    11,
                    20,
                    23,
                    0,
                    4,
                    285,
                    0
                ],
                "published": "2024-10-11T20:23:00Z",
                "published_parsed": [
                    2024,
                    10,
                    11,
                    20,
                    23,
                    0,
                    4,
                    285,
                    0
                ],
                "title": "Foundation Model-Powered 3D Few-Shot Class Incremental Learning via\n  Training-free Adaptor",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Foundation Model-Powered 3D Few-Shot Class Incremental Learning via\n  Training-free Adaptor"
                },
                "summary": "Recent advances in deep learning for processing point clouds hold increased\ninterest in Few-Shot Class Incremental Learning (FSCIL) for 3D computer vision.\nThis paper introduces a new method to tackle the Few-Shot Continual Incremental\nLearning (FSCIL) problem in 3D point cloud environments. We leverage a\nfoundational 3D model trained extensively on point cloud data. Drawing from\nrecent improvements in foundation models, known for their ability to work well\nacross different tasks, we propose a novel strategy that does not require\nadditional training to adapt to new tasks. Our approach uses a dual cache\nsystem: first, it uses previous test samples based on how confident the model\nwas in its predictions to prevent forgetting, and second, it includes a small\nnumber of new task samples to prevent overfitting. This dynamic adaptation\nensures strong performance across different learning tasks without needing lots\nof fine-tuning. We tested our approach on datasets like ModelNet, ShapeNet,\nScanObjectNN, and CO3D, showing that it outperforms other FSCIL methods and\ndemonstrating its effectiveness and versatility. The code is available at\n\\url{https://github.com/ahmadisahar/ACCV_FCIL3D}.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advances in deep learning for processing point clouds hold increased\ninterest in Few-Shot Class Incremental Learning (FSCIL) for 3D computer vision.\nThis paper introduces a new method to tackle the Few-Shot Continual Incremental\nLearning (FSCIL) problem in 3D point cloud environments. We leverage a\nfoundational 3D model trained extensively on point cloud data. Drawing from\nrecent improvements in foundation models, known for their ability to work well\nacross different tasks, we propose a novel strategy that does not require\nadditional training to adapt to new tasks. Our approach uses a dual cache\nsystem: first, it uses previous test samples based on how confident the model\nwas in its predictions to prevent forgetting, and second, it includes a small\nnumber of new task samples to prevent overfitting. This dynamic adaptation\nensures strong performance across different learning tasks without needing lots\nof fine-tuning. We tested our approach on datasets like ModelNet, ShapeNet,\nScanObjectNN, and CO3D, showing that it outperforms other FSCIL methods and\ndemonstrating its effectiveness and versatility. The code is available at\n\\url{https://github.com/ahmadisahar/ACCV_FCIL3D}."
                },
                "authors": [
                    {
                        "name": "Sahar Ahmadi"
                    },
                    {
                        "name": "Ali Cheraghian"
                    },
                    {
                        "name": "Morteza Saberi"
                    },
                    {
                        "name": "Md. Towsif Abir"
                    },
                    {
                        "name": "Hamidreza Dastmalchi"
                    },
                    {
                        "name": "Farookh Hussain"
                    },
                    {
                        "name": "Shafin Rahman"
                    }
                ],
                "author_detail": {
                    "name": "Shafin Rahman"
                },
                "author": "Shafin Rahman",
                "arxiv_comment": "ACCV 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.09237v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.09237v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            }
        ]
    },
    {
        "keyword": "LLM Inference",
        "arxiv_results": [
            {
                "id": "http://arxiv.org/abs/2406.13236v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.13236v2",
                "updated": "2024-10-30T17:59:08Z",
                "updated_parsed": [
                    2024,
                    10,
                    30,
                    17,
                    59,
                    8,
                    2,
                    304,
                    0
                ],
                "published": "2024-06-19T05:53:27Z",
                "published_parsed": [
                    2024,
                    6,
                    19,
                    5,
                    53,
                    27,
                    2,
                    171,
                    0
                ],
                "title": "Data Contamination Can Cross Language Barriers",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Data Contamination Can Cross Language Barriers"
                },
                "summary": "The opacity in developing large language models (LLMs) is raising growing\nconcerns about the potential contamination of public benchmarks in the\npre-training data. Existing contamination detection methods are typically based\non the text overlap between training and evaluation data, which can be too\nsuperficial to reflect deeper forms of contamination. In this paper, we first\npresent a cross-lingual form of contamination that inflates LLMs' performance\nwhile evading current detection methods, deliberately injected by overfitting\nLLMs on the translated versions of benchmark test sets. Then, we propose\ngeneralization-based approaches to unmask such deeply concealed contamination.\nSpecifically, we examine the LLM's performance change after modifying the\noriginal benchmark by replacing the false answer choices with correct ones from\nother questions. Contaminated models can hardly generalize to such easier\nsituations, where the false choices can be \\emph{not even wrong}, as all\nchoices are correct in their memorization. Experimental results demonstrate\nthat cross-lingual contamination can easily fool existing detection methods,\nbut not ours. In addition, we discuss the potential utilization of\ncross-lingual contamination in interpreting LLMs' working mechanisms and in\npost-training LLMs for enhanced multilingual capabilities. The code and dataset\nwe use can be obtained from \\url{https://github.com/ShangDataLab/Deep-Contam}.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The opacity in developing large language models (LLMs) is raising growing\nconcerns about the potential contamination of public benchmarks in the\npre-training data. Existing contamination detection methods are typically based\non the text overlap between training and evaluation data, which can be too\nsuperficial to reflect deeper forms of contamination. In this paper, we first\npresent a cross-lingual form of contamination that inflates LLMs' performance\nwhile evading current detection methods, deliberately injected by overfitting\nLLMs on the translated versions of benchmark test sets. Then, we propose\ngeneralization-based approaches to unmask such deeply concealed contamination.\nSpecifically, we examine the LLM's performance change after modifying the\noriginal benchmark by replacing the false answer choices with correct ones from\nother questions. Contaminated models can hardly generalize to such easier\nsituations, where the false choices can be \\emph{not even wrong}, as all\nchoices are correct in their memorization. Experimental results demonstrate\nthat cross-lingual contamination can easily fool existing detection methods,\nbut not ours. In addition, we discuss the potential utilization of\ncross-lingual contamination in interpreting LLMs' working mechanisms and in\npost-training LLMs for enhanced multilingual capabilities. The code and dataset\nwe use can be obtained from \\url{https://github.com/ShangDataLab/Deep-Contam}."
                },
                "authors": [
                    {
                        "name": "Feng Yao"
                    },
                    {
                        "name": "Yufan Zhuang"
                    },
                    {
                        "name": "Zihao Sun"
                    },
                    {
                        "name": "Sunan Xu"
                    },
                    {
                        "name": "Animesh Kumar"
                    },
                    {
                        "name": "Jingbo Shang"
                    }
                ],
                "author_detail": {
                    "name": "Jingbo Shang"
                },
                "author": "Jingbo Shang",
                "arxiv_comment": "EMNLP 2024 Main camera-ready version",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.13236v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.13236v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.20533v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.20533v2",
                "updated": "2024-10-30T17:56:22Z",
                "updated_parsed": [
                    2024,
                    10,
                    30,
                    17,
                    56,
                    22,
                    2,
                    304,
                    0
                ],
                "published": "2024-10-27T17:55:27Z",
                "published_parsed": [
                    2024,
                    10,
                    27,
                    17,
                    55,
                    27,
                    6,
                    301,
                    0
                ],
                "title": "Guiding Through Complexity: What Makes Good Supervision for Hard\n  Reasoning Tasks?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Guiding Through Complexity: What Makes Good Supervision for Hard\n  Reasoning Tasks?"
                },
                "summary": "How can \"weak teacher models\" such as average human annotators or existing AI\nsystems, effectively supervise LLMs to improve performance on hard reasoning\ntasks, especially those that challenge and requires expertise or daily practice\nfrom the teacher models? In this paper, we seek for empirical answers to this\nquestion by investigating various data-driven strategies that offer supervision\ndata at different quality levels upon tasks of varying complexity. Two\nintuitive strategies emerge for teacher models to provide supervision during\nalignment training: 1) using lower-quality supervision from complete tasks that\nmatch the difficulty of the target reasoning tasks, and 2) leveraging\nhigher-quality supervision from easier subtasks that are less challenging.\nInterestingly, we find that even when the outcome error rate for hard task\nsupervision is high (e.g., 90\\%), training on such data can outperform\nperfectly correct supervision on easier subtasks on multiple hard math\nbenchmarks. We further identify a more critical factor influencing training\nperformance: step-wise error rates, which indicate the severity of errors in\nsolutions. Specifically, training on hard task supervision with the same\noutcome error rates but disparate step-wise error rates can lead to a 30\\%\naccuracy gap on MATH benchmark. Our results also reveal that supplementing hard\ntask supervision with the corresponding subtask supervision can yield notable\nperformance improvements than simply combining rephrased hard full task\nsupervision, suggesting new avenues for data augmentation. Data and code are\nreleased at \\url{https://github.com/hexuan21/Weak-to-Strong}.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "How can \"weak teacher models\" such as average human annotators or existing AI\nsystems, effectively supervise LLMs to improve performance on hard reasoning\ntasks, especially those that challenge and requires expertise or daily practice\nfrom the teacher models? In this paper, we seek for empirical answers to this\nquestion by investigating various data-driven strategies that offer supervision\ndata at different quality levels upon tasks of varying complexity. Two\nintuitive strategies emerge for teacher models to provide supervision during\nalignment training: 1) using lower-quality supervision from complete tasks that\nmatch the difficulty of the target reasoning tasks, and 2) leveraging\nhigher-quality supervision from easier subtasks that are less challenging.\nInterestingly, we find that even when the outcome error rate for hard task\nsupervision is high (e.g., 90\\%), training on such data can outperform\nperfectly correct supervision on easier subtasks on multiple hard math\nbenchmarks. We further identify a more critical factor influencing training\nperformance: step-wise error rates, which indicate the severity of errors in\nsolutions. Specifically, training on hard task supervision with the same\noutcome error rates but disparate step-wise error rates can lead to a 30\\%\naccuracy gap on MATH benchmark. Our results also reveal that supplementing hard\ntask supervision with the corresponding subtask supervision can yield notable\nperformance improvements than simply combining rephrased hard full task\nsupervision, suggesting new avenues for data augmentation. Data and code are\nreleased at \\url{https://github.com/hexuan21/Weak-to-Strong}."
                },
                "authors": [
                    {
                        "name": "Xuan He"
                    },
                    {
                        "name": "Da Yin"
                    },
                    {
                        "name": "Nanyun Peng"
                    }
                ],
                "author_detail": {
                    "name": "Nanyun Peng"
                },
                "author": "Nanyun Peng",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.20533v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.20533v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.23277v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.23277v1",
                "updated": "2024-10-30T17:55:52Z",
                "updated_parsed": [
                    2024,
                    10,
                    30,
                    17,
                    55,
                    52,
                    2,
                    304,
                    0
                ],
                "published": "2024-10-30T17:55:52Z",
                "published_parsed": [
                    2024,
                    10,
                    30,
                    17,
                    55,
                    52,
                    2,
                    304,
                    0
                ],
                "title": "SlowFast-VGen: Slow-Fast Learning for Action-Driven Long Video\n  Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SlowFast-VGen: Slow-Fast Learning for Action-Driven Long Video\n  Generation"
                },
                "summary": "Human beings are endowed with a complementary learning system, which bridges\nthe slow learning of general world dynamics with fast storage of episodic\nmemory from a new experience. Previous video generation models, however,\nprimarily focus on slow learning by pre-training on vast amounts of data,\noverlooking the fast learning phase crucial for episodic memory storage. This\noversight leads to inconsistencies across temporally distant frames when\ngenerating longer videos, as these frames fall beyond the model's context\nwindow. To this end, we introduce SlowFast-VGen, a novel dual-speed learning\nsystem for action-driven long video generation. Our approach incorporates a\nmasked conditional video diffusion model for the slow learning of world\ndynamics, alongside an inference-time fast learning strategy based on a\ntemporal LoRA module. Specifically, the fast learning process updates its\ntemporal LoRA parameters based on local inputs and outputs, thereby efficiently\nstoring episodic memory in its parameters. We further propose a slow-fast\nlearning loop algorithm that seamlessly integrates the inner fast learning loop\ninto the outer slow learning loop, enabling the recall of prior multi-episode\nexperiences for context-aware skill learning. To facilitate the slow learning\nof an approximate world model, we collect a large-scale dataset of 200k videos\nwith language action annotations, covering a wide range of scenarios. Extensive\nexperiments show that SlowFast-VGen outperforms baselines across various\nmetrics for action-driven video generation, achieving an FVD score of 514\ncompared to 782, and maintaining consistency in longer videos, with an average\nof 0.37 scene cuts versus 0.89. The slow-fast learning loop algorithm\nsignificantly enhances performances on long-horizon planning tasks as well.\nProject Website: https://slowfast-vgen.github.io",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Human beings are endowed with a complementary learning system, which bridges\nthe slow learning of general world dynamics with fast storage of episodic\nmemory from a new experience. Previous video generation models, however,\nprimarily focus on slow learning by pre-training on vast amounts of data,\noverlooking the fast learning phase crucial for episodic memory storage. This\noversight leads to inconsistencies across temporally distant frames when\ngenerating longer videos, as these frames fall beyond the model's context\nwindow. To this end, we introduce SlowFast-VGen, a novel dual-speed learning\nsystem for action-driven long video generation. Our approach incorporates a\nmasked conditional video diffusion model for the slow learning of world\ndynamics, alongside an inference-time fast learning strategy based on a\ntemporal LoRA module. Specifically, the fast learning process updates its\ntemporal LoRA parameters based on local inputs and outputs, thereby efficiently\nstoring episodic memory in its parameters. We further propose a slow-fast\nlearning loop algorithm that seamlessly integrates the inner fast learning loop\ninto the outer slow learning loop, enabling the recall of prior multi-episode\nexperiences for context-aware skill learning. To facilitate the slow learning\nof an approximate world model, we collect a large-scale dataset of 200k videos\nwith language action annotations, covering a wide range of scenarios. Extensive\nexperiments show that SlowFast-VGen outperforms baselines across various\nmetrics for action-driven video generation, achieving an FVD score of 514\ncompared to 782, and maintaining consistency in longer videos, with an average\nof 0.37 scene cuts versus 0.89. The slow-fast learning loop algorithm\nsignificantly enhances performances on long-horizon planning tasks as well.\nProject Website: https://slowfast-vgen.github.io"
                },
                "authors": [
                    {
                        "name": "Yining Hong"
                    },
                    {
                        "name": "Beide Liu"
                    },
                    {
                        "name": "Maxine Wu"
                    },
                    {
                        "name": "Yuanhao Zhai"
                    },
                    {
                        "name": "Kai-Wei Chang"
                    },
                    {
                        "name": "Lingjie Li"
                    },
                    {
                        "name": "Kevin Lin"
                    },
                    {
                        "name": "Chung-Ching Lin"
                    },
                    {
                        "name": "Jianfeng Wang"
                    },
                    {
                        "name": "Zhengyuan Yang"
                    },
                    {
                        "name": "Yingnian Wu"
                    },
                    {
                        "name": "Lijuan Wang"
                    }
                ],
                "author_detail": {
                    "name": "Lijuan Wang"
                },
                "author": "Lijuan Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.23277v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.23277v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.23274v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.23274v1",
                "updated": "2024-10-30T17:54:56Z",
                "updated_parsed": [
                    2024,
                    10,
                    30,
                    17,
                    54,
                    56,
                    2,
                    304,
                    0
                ],
                "published": "2024-10-30T17:54:56Z",
                "published_parsed": [
                    2024,
                    10,
                    30,
                    17,
                    54,
                    56,
                    2,
                    304,
                    0
                ],
                "title": "Multi-student Diffusion Distillation for Better One-step Generators",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-student Diffusion Distillation for Better One-step Generators"
                },
                "summary": "Diffusion models achieve high-quality sample generation at the cost of a\nlengthy multistep inference procedure. To overcome this, diffusion distillation\ntechniques produce student generators capable of matching or surpassing the\nteacher in a single step. However, the student model's inference speed is\nlimited by the size of the teacher architecture, preventing real-time\ngeneration for computationally heavy applications. In this work, we introduce\nMulti-Student Distillation (MSD), a framework to distill a conditional teacher\ndiffusion model into multiple single-step generators. Each student generator is\nresponsible for a subset of the conditioning data, thereby obtaining higher\ngeneration quality for the same capacity. MSD trains multiple distilled\nstudents, allowing smaller sizes and, therefore, faster inference. Also, MSD\noffers a lightweight quality boost over single-student distillation with the\nsame architecture. We demonstrate MSD is effective by training multiple\nsame-sized or smaller students on single-step distillation using distribution\nmatching and adversarial distillation techniques. With smaller students, MSD\ngets competitive results with faster inference for single-step generation.\nUsing 4 same-sized students, MSD sets a new state-of-the-art for one-step image\ngeneration: FID 1.20 on ImageNet-64x64 and 8.20 on zero-shot COCO2014.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion models achieve high-quality sample generation at the cost of a\nlengthy multistep inference procedure. To overcome this, diffusion distillation\ntechniques produce student generators capable of matching or surpassing the\nteacher in a single step. However, the student model's inference speed is\nlimited by the size of the teacher architecture, preventing real-time\ngeneration for computationally heavy applications. In this work, we introduce\nMulti-Student Distillation (MSD), a framework to distill a conditional teacher\ndiffusion model into multiple single-step generators. Each student generator is\nresponsible for a subset of the conditioning data, thereby obtaining higher\ngeneration quality for the same capacity. MSD trains multiple distilled\nstudents, allowing smaller sizes and, therefore, faster inference. Also, MSD\noffers a lightweight quality boost over single-student distillation with the\nsame architecture. We demonstrate MSD is effective by training multiple\nsame-sized or smaller students on single-step distillation using distribution\nmatching and adversarial distillation techniques. With smaller students, MSD\ngets competitive results with faster inference for single-step generation.\nUsing 4 same-sized students, MSD sets a new state-of-the-art for one-step image\ngeneration: FID 1.20 on ImageNet-64x64 and 8.20 on zero-shot COCO2014."
                },
                "authors": [
                    {
                        "name": "Yanke Song"
                    },
                    {
                        "name": "Jonathan Lorraine"
                    },
                    {
                        "name": "Weili Nie"
                    },
                    {
                        "name": "Karsten Kreis"
                    },
                    {
                        "name": "James Lucas"
                    }
                ],
                "author_detail": {
                    "name": "James Lucas"
                },
                "author": "James Lucas",
                "arxiv_comment": "Project page: https://research.nvidia.com/labs/toronto-ai/MSD/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.23274v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.23274v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.22217v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.22217v2",
                "updated": "2024-10-30T17:51:26Z",
                "updated_parsed": [
                    2024,
                    10,
                    30,
                    17,
                    51,
                    26,
                    2,
                    304,
                    0
                ],
                "published": "2024-10-29T16:48:22Z",
                "published_parsed": [
                    2024,
                    10,
                    29,
                    16,
                    48,
                    22,
                    1,
                    303,
                    0
                ],
                "title": "Towards Unifying Understanding and Generation in the Era of Vision\n  Foundation Models: A Survey from the Autoregression Perspective",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Towards Unifying Understanding and Generation in the Era of Vision\n  Foundation Models: A Survey from the Autoregression Perspective"
                },
                "summary": "Autoregression in large language models (LLMs) has shown impressive\nscalability by unifying all language tasks into the next token prediction\nparadigm. Recently, there is a growing interest in extending this success to\nvision foundation models. In this survey, we review the recent advances and\ndiscuss future directions for autoregressive vision foundation models. First,\nwe present the trend for next generation of vision foundation models, i.e.,\nunifying both understanding and generation in vision tasks. We then analyze the\nlimitations of existing vision foundation models, and present a formal\ndefinition of autoregression with its advantages. Later, we categorize\nautoregressive vision foundation models from their vision tokenizers and\nautoregression backbones. Finally, we discuss several promising research\nchallenges and directions. To the best of our knowledge, this is the first\nsurvey to comprehensively summarize autoregressive vision foundation models\nunder the trend of unifying understanding and generation. A collection of\nrelated resources is available at https://github.com/EmmaSRH/ARVFM.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Autoregression in large language models (LLMs) has shown impressive\nscalability by unifying all language tasks into the next token prediction\nparadigm. Recently, there is a growing interest in extending this success to\nvision foundation models. In this survey, we review the recent advances and\ndiscuss future directions for autoregressive vision foundation models. First,\nwe present the trend for next generation of vision foundation models, i.e.,\nunifying both understanding and generation in vision tasks. We then analyze the\nlimitations of existing vision foundation models, and present a formal\ndefinition of autoregression with its advantages. Later, we categorize\nautoregressive vision foundation models from their vision tokenizers and\nautoregression backbones. Finally, we discuss several promising research\nchallenges and directions. To the best of our knowledge, this is the first\nsurvey to comprehensively summarize autoregressive vision foundation models\nunder the trend of unifying understanding and generation. A collection of\nrelated resources is available at https://github.com/EmmaSRH/ARVFM."
                },
                "authors": [
                    {
                        "name": "Shenghao Xie"
                    },
                    {
                        "name": "Wenqiang Zu"
                    },
                    {
                        "name": "Mingyang Zhao"
                    },
                    {
                        "name": "Duo Su"
                    },
                    {
                        "name": "Shilong Liu"
                    },
                    {
                        "name": "Ruohua Shi"
                    },
                    {
                        "name": "Guoqi Li"
                    },
                    {
                        "name": "Shanghang Zhang"
                    },
                    {
                        "name": "Lei Ma"
                    }
                ],
                "author_detail": {
                    "name": "Lei Ma"
                },
                "author": "Lei Ma",
                "arxiv_comment": "17 pages, 1 table, 2 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.22217v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.22217v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.23252v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.23252v1",
                "updated": "2024-10-30T17:35:44Z",
                "updated_parsed": [
                    2024,
                    10,
                    30,
                    17,
                    35,
                    44,
                    2,
                    304,
                    0
                ],
                "published": "2024-10-30T17:35:44Z",
                "published_parsed": [
                    2024,
                    10,
                    30,
                    17,
                    35,
                    44,
                    2,
                    304,
                    0
                ],
                "title": "Evaluating Cultural and Social Awareness of LLM Web Agents",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Evaluating Cultural and Social Awareness of LLM Web Agents"
                },
                "summary": "As large language models (LLMs) expand into performing as agents for\nreal-world applications beyond traditional NLP tasks, evaluating their\nrobustness becomes increasingly important. However, existing benchmarks often\noverlook critical dimensions like cultural and social awareness. To address\nthese, we introduce CASA, a benchmark designed to assess LLM agents'\nsensitivity to cultural and social norms across two web-based tasks: online\nshopping and social discussion forums. Our approach evaluates LLM agents'\nability to detect and appropriately respond to norm-violating user queries and\nobservations. Furthermore, we propose a comprehensive evaluation framework that\nmeasures awareness coverage, helpfulness in managing user queries, and the\nviolation rate when facing misleading web content. Experiments show that\ncurrent LLMs perform significantly better in non-agent than in web-based agent\nenvironments, with agents achieving less than 10% awareness coverage and over\n40% violation rates. To improve performance, we explore two methods: prompting\nand fine-tuning, and find that combining both methods can offer complementary\nadvantages -- fine-tuning on culture-specific datasets significantly enhances\nthe agents' ability to generalize across different regions, while prompting\nboosts the agents' ability to navigate complex tasks. These findings highlight\nthe importance of constantly benchmarking LLM agents' cultural and social\nawareness during the development cycle.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As large language models (LLMs) expand into performing as agents for\nreal-world applications beyond traditional NLP tasks, evaluating their\nrobustness becomes increasingly important. However, existing benchmarks often\noverlook critical dimensions like cultural and social awareness. To address\nthese, we introduce CASA, a benchmark designed to assess LLM agents'\nsensitivity to cultural and social norms across two web-based tasks: online\nshopping and social discussion forums. Our approach evaluates LLM agents'\nability to detect and appropriately respond to norm-violating user queries and\nobservations. Furthermore, we propose a comprehensive evaluation framework that\nmeasures awareness coverage, helpfulness in managing user queries, and the\nviolation rate when facing misleading web content. Experiments show that\ncurrent LLMs perform significantly better in non-agent than in web-based agent\nenvironments, with agents achieving less than 10% awareness coverage and over\n40% violation rates. To improve performance, we explore two methods: prompting\nand fine-tuning, and find that combining both methods can offer complementary\nadvantages -- fine-tuning on culture-specific datasets significantly enhances\nthe agents' ability to generalize across different regions, while prompting\nboosts the agents' ability to navigate complex tasks. These findings highlight\nthe importance of constantly benchmarking LLM agents' cultural and social\nawareness during the development cycle."
                },
                "authors": [
                    {
                        "name": "Haoyi Qiu"
                    },
                    {
                        "name": "Alexander R. Fabbri"
                    },
                    {
                        "name": "Divyansh Agarwal"
                    },
                    {
                        "name": "Kung-Hsiang Huang"
                    },
                    {
                        "name": "Sarah Tan"
                    },
                    {
                        "name": "Nanyun Peng"
                    },
                    {
                        "name": "Chien-Sheng Wu"
                    }
                ],
                "author_detail": {
                    "name": "Chien-Sheng Wu"
                },
                "author": "Chien-Sheng Wu",
                "arxiv_comment": "Work in progress",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.23252v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.23252v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.23242v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.23242v1",
                "updated": "2024-10-30T17:28:28Z",
                "updated_parsed": [
                    2024,
                    10,
                    30,
                    17,
                    28,
                    28,
                    2,
                    304,
                    0
                ],
                "published": "2024-10-30T17:28:28Z",
                "published_parsed": [
                    2024,
                    10,
                    30,
                    17,
                    28,
                    28,
                    2,
                    304,
                    0
                ],
                "title": "A little less conversation, a little more action, please: Investigating\n  the physical common-sense of LLMs in a 3D embodied environment",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A little less conversation, a little more action, please: Investigating\n  the physical common-sense of LLMs in a 3D embodied environment"
                },
                "summary": "As general-purpose tools, Large Language Models (LLMs) must often reason\nabout everyday physical environments. In a question-and-answer capacity,\nunderstanding the interactions of physical objects may be necessary to give\nappropriate responses. Moreover, LLMs are increasingly used as reasoning\nengines in agentic systems, designing and controlling their action sequences.\nThe vast majority of research has tackled this issue using static benchmarks,\ncomprised of text or image-based questions about the physical world. However,\nthese benchmarks do not capture the complexity and nuance of real-life physical\nprocesses. Here we advocate for a second, relatively unexplored, approach:\n'embodying' the LLMs by granting them control of an agent within a 3D\nenvironment. We present the first embodied and cognitively meaningful\nevaluation of physical common-sense reasoning in LLMs. Our framework allows\ndirect comparison of LLMs with other embodied agents, such as those based on\nDeep Reinforcement Learning, and human and non-human animals. We employ the\nAnimal-AI (AAI) environment, a simulated 3D virtual laboratory, to study\nphysical common-sense reasoning in LLMs. For this, we use the AAI Testbed, a\nsuite of experiments that replicate laboratory studies with non-human animals,\nto study physical reasoning capabilities including distance estimation,\ntracking out-of-sight objects, and tool use. We demonstrate that\nstate-of-the-art multi-modal models with no finetuning can complete this style\nof task, allowing meaningful comparison to the entrants of the 2019 Animal-AI\nOlympics competition and to human children. Our results show that LLMs are\ncurrently outperformed by human children on these tasks. We argue that this\napproach allows the study of physical reasoning using ecologically valid\nexperiments drawn directly from cognitive science, improving the predictability\nand reliability of LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As general-purpose tools, Large Language Models (LLMs) must often reason\nabout everyday physical environments. In a question-and-answer capacity,\nunderstanding the interactions of physical objects may be necessary to give\nappropriate responses. Moreover, LLMs are increasingly used as reasoning\nengines in agentic systems, designing and controlling their action sequences.\nThe vast majority of research has tackled this issue using static benchmarks,\ncomprised of text or image-based questions about the physical world. However,\nthese benchmarks do not capture the complexity and nuance of real-life physical\nprocesses. Here we advocate for a second, relatively unexplored, approach:\n'embodying' the LLMs by granting them control of an agent within a 3D\nenvironment. We present the first embodied and cognitively meaningful\nevaluation of physical common-sense reasoning in LLMs. Our framework allows\ndirect comparison of LLMs with other embodied agents, such as those based on\nDeep Reinforcement Learning, and human and non-human animals. We employ the\nAnimal-AI (AAI) environment, a simulated 3D virtual laboratory, to study\nphysical common-sense reasoning in LLMs. For this, we use the AAI Testbed, a\nsuite of experiments that replicate laboratory studies with non-human animals,\nto study physical reasoning capabilities including distance estimation,\ntracking out-of-sight objects, and tool use. We demonstrate that\nstate-of-the-art multi-modal models with no finetuning can complete this style\nof task, allowing meaningful comparison to the entrants of the 2019 Animal-AI\nOlympics competition and to human children. Our results show that LLMs are\ncurrently outperformed by human children on these tasks. We argue that this\napproach allows the study of physical reasoning using ecologically valid\nexperiments drawn directly from cognitive science, improving the predictability\nand reliability of LLMs."
                },
                "authors": [
                    {
                        "name": "Matteo G. Mecattaf"
                    },
                    {
                        "name": "Ben Slater"
                    },
                    {
                        "name": "Marko Tešić"
                    },
                    {
                        "name": "Jonathan Prunty"
                    },
                    {
                        "name": "Konstantinos Voudouris"
                    },
                    {
                        "name": "Lucy G. Cheke"
                    }
                ],
                "author_detail": {
                    "name": "Lucy G. Cheke"
                },
                "author": "Lucy G. Cheke",
                "arxiv_comment": "25 pages, 4 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.23242v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.23242v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.04836v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.04836v2",
                "updated": "2024-10-30T17:26:29Z",
                "updated_parsed": [
                    2024,
                    10,
                    30,
                    17,
                    26,
                    29,
                    2,
                    304,
                    0
                ],
                "published": "2024-09-07T14:20:17Z",
                "published_parsed": [
                    2024,
                    9,
                    7,
                    14,
                    20,
                    17,
                    5,
                    251,
                    0
                ],
                "title": "Spatial Interference Detection in Treatment Effect Model",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Spatial Interference Detection in Treatment Effect Model"
                },
                "summary": "Modeling the interference effect is an important issue in the field of causal\ninference. Existing studies rely on explicit and often homogeneous assumptions\nregarding interference structures. In this paper, we introduce a low-rank and\nsparse treatment effect model that leverages data-driven techniques to identify\nthe locations of interference effects. A profiling algorithm is proposed to\nestimate the model coefficients, and based on these estimates, global test and\nlocal detection methods are established to detect the existence of interference\nand the interference neighbor locations for each unit. We derive the\nnon-asymptotic bound of the estimation error, and establish theoretical\nguarantees for the global test and the accuracy of the detection method in\nterms of Jaccard index. Simulations and real data examples are provided to\ndemonstrate the usefulness of the proposed method.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Modeling the interference effect is an important issue in the field of causal\ninference. Existing studies rely on explicit and often homogeneous assumptions\nregarding interference structures. In this paper, we introduce a low-rank and\nsparse treatment effect model that leverages data-driven techniques to identify\nthe locations of interference effects. A profiling algorithm is proposed to\nestimate the model coefficients, and based on these estimates, global test and\nlocal detection methods are established to detect the existence of interference\nand the interference neighbor locations for each unit. We derive the\nnon-asymptotic bound of the estimation error, and establish theoretical\nguarantees for the global test and the accuracy of the detection method in\nterms of Jaccard index. Simulations and real data examples are provided to\ndemonstrate the usefulness of the proposed method."
                },
                "authors": [
                    {
                        "name": "Wei Zhang"
                    },
                    {
                        "name": "Ying Yang"
                    },
                    {
                        "name": "Fang Yao"
                    }
                ],
                "author_detail": {
                    "name": "Fang Yao"
                },
                "author": "Fang Yao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.04836v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.04836v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ME",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.23238v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.23238v1",
                "updated": "2024-10-30T17:25:57Z",
                "updated_parsed": [
                    2024,
                    10,
                    30,
                    17,
                    25,
                    57,
                    2,
                    304,
                    0
                ],
                "published": "2024-10-30T17:25:57Z",
                "published_parsed": [
                    2024,
                    10,
                    30,
                    17,
                    25,
                    57,
                    2,
                    304,
                    0
                ],
                "title": "Full-waveform earthquake source inversion using simulation-based\n  inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Full-waveform earthquake source inversion using simulation-based\n  inference"
                },
                "summary": "This paper presents a novel framework for full-waveform seismic source\ninversion using simulation-based inference (SBI). Traditional probabilistic\napproaches often rely on simplifying assumptions about data errors, which we\nshow can lead to inaccurate uncertainty quantification. SBI addresses this\nlimitation by building an empirical probabilistic model of the data errors\nusing machine learning models, known as neural density estimators, which can\nthen be integrated into the Bayesian inference framework. We apply the SBI\nframework to point-source moment tensor inversions as well as joint moment\ntensor and time-location inversions. We construct a range of synthetic examples\nto explore the quality of the SBI solutions, as well as to compare the SBI\nresults with standard Gaussian likelihood-based Bayesian inversions. We then\ndemonstrate that under real seismic noise, common Gaussian likelihood\nassumptions for treating full-waveform data yield overconfident posterior\ndistributions that underestimate the moment tensor component uncertainties by\nup to a factor of 3. We contrast this with SBI, which produces well-calibrated\nposteriors that generally agree with the true seismic source parameters, and\noffers an order-of-magnitude reduction in the number of simulations required to\nperform inference compared to standard Monte Carlo techniques. Finally, we\napply our methodology to a pair of moderate magnitude earthquakes in the North\nAtlantic. We utilise seismic waveforms recorded by the recent UPFLOW ocean\nbottom seismometer array as well as by regional land stations in the Azores,\ncomparing full moment tensor and source-time location posteriors between SBI\nand a Gaussian likelihood approach. We find that our adaptation of SBI can be\ndirectly applied to real earthquake sources to efficiently produce high quality\nposterior distributions that significantly improve upon Gaussian likelihood\napproaches.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper presents a novel framework for full-waveform seismic source\ninversion using simulation-based inference (SBI). Traditional probabilistic\napproaches often rely on simplifying assumptions about data errors, which we\nshow can lead to inaccurate uncertainty quantification. SBI addresses this\nlimitation by building an empirical probabilistic model of the data errors\nusing machine learning models, known as neural density estimators, which can\nthen be integrated into the Bayesian inference framework. We apply the SBI\nframework to point-source moment tensor inversions as well as joint moment\ntensor and time-location inversions. We construct a range of synthetic examples\nto explore the quality of the SBI solutions, as well as to compare the SBI\nresults with standard Gaussian likelihood-based Bayesian inversions. We then\ndemonstrate that under real seismic noise, common Gaussian likelihood\nassumptions for treating full-waveform data yield overconfident posterior\ndistributions that underestimate the moment tensor component uncertainties by\nup to a factor of 3. We contrast this with SBI, which produces well-calibrated\nposteriors that generally agree with the true seismic source parameters, and\noffers an order-of-magnitude reduction in the number of simulations required to\nperform inference compared to standard Monte Carlo techniques. Finally, we\napply our methodology to a pair of moderate magnitude earthquakes in the North\nAtlantic. We utilise seismic waveforms recorded by the recent UPFLOW ocean\nbottom seismometer array as well as by regional land stations in the Azores,\ncomparing full moment tensor and source-time location posteriors between SBI\nand a Gaussian likelihood approach. We find that our adaptation of SBI can be\ndirectly applied to real earthquake sources to efficiently produce high quality\nposterior distributions that significantly improve upon Gaussian likelihood\napproaches."
                },
                "authors": [
                    {
                        "name": "A. A. Saoulis"
                    },
                    {
                        "name": "D. Piras"
                    },
                    {
                        "name": "A. Spurio Mancini"
                    },
                    {
                        "name": "B. Joachimi"
                    },
                    {
                        "name": "A. M. G. Ferreira"
                    }
                ],
                "author_detail": {
                    "name": "A. M. G. Ferreira"
                },
                "author": "A. M. G. Ferreira",
                "arxiv_comment": "22 + 11 pages, 11 + 11 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.23238v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.23238v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.geo-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.geo-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "physics.data-an",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.23234v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.23234v1",
                "updated": "2024-10-30T17:22:45Z",
                "updated_parsed": [
                    2024,
                    10,
                    30,
                    17,
                    22,
                    45,
                    2,
                    304,
                    0
                ],
                "published": "2024-10-30T17:22:45Z",
                "published_parsed": [
                    2024,
                    10,
                    30,
                    17,
                    22,
                    45,
                    2,
                    304,
                    0
                ],
                "title": "EMOTION: Expressive Motion Sequence Generation for Humanoid Robots with\n  In-Context Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "EMOTION: Expressive Motion Sequence Generation for Humanoid Robots with\n  In-Context Learning"
                },
                "summary": "This paper introduces a framework, called EMOTION, for generating expressive\nmotion sequences in humanoid robots, enhancing their ability to engage in\nhumanlike non-verbal communication. Non-verbal cues such as facial expressions,\ngestures, and body movements play a crucial role in effective interpersonal\ninteractions. Despite the advancements in robotic behaviors, existing methods\noften fall short in mimicking the diversity and subtlety of human non-verbal\ncommunication. To address this gap, our approach leverages the in-context\nlearning capability of large language models (LLMs) to dynamically generate\nsocially appropriate gesture motion sequences for human-robot interaction. We\nuse this framework to generate 10 different expressive gestures and conduct\nonline user studies comparing the naturalness and understandability of the\nmotions generated by EMOTION and its human-feedback version, EMOTION++, against\nthose by human operators. The results demonstrate that our approach either\nmatches or surpasses human performance in generating understandable and natural\nrobot motions under certain scenarios. We also provide design implications for\nfuture research to consider a set of variables when generating expressive\nrobotic gestures.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper introduces a framework, called EMOTION, for generating expressive\nmotion sequences in humanoid robots, enhancing their ability to engage in\nhumanlike non-verbal communication. Non-verbal cues such as facial expressions,\ngestures, and body movements play a crucial role in effective interpersonal\ninteractions. Despite the advancements in robotic behaviors, existing methods\noften fall short in mimicking the diversity and subtlety of human non-verbal\ncommunication. To address this gap, our approach leverages the in-context\nlearning capability of large language models (LLMs) to dynamically generate\nsocially appropriate gesture motion sequences for human-robot interaction. We\nuse this framework to generate 10 different expressive gestures and conduct\nonline user studies comparing the naturalness and understandability of the\nmotions generated by EMOTION and its human-feedback version, EMOTION++, against\nthose by human operators. The results demonstrate that our approach either\nmatches or surpasses human performance in generating understandable and natural\nrobot motions under certain scenarios. We also provide design implications for\nfuture research to consider a set of variables when generating expressive\nrobotic gestures."
                },
                "authors": [
                    {
                        "name": "Peide Huang"
                    },
                    {
                        "name": "Yuhan Hu"
                    },
                    {
                        "name": "Nataliya Nechyporenko"
                    },
                    {
                        "name": "Daehwa Kim"
                    },
                    {
                        "name": "Walter Talbott"
                    },
                    {
                        "name": "Jian Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Jian Zhang"
                },
                "author": "Jian Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.23234v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.23234v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2306.06755v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2306.06755v4",
                "updated": "2024-10-30T17:22:41Z",
                "updated_parsed": [
                    2024,
                    10,
                    30,
                    17,
                    22,
                    41,
                    2,
                    304,
                    0
                ],
                "published": "2023-06-11T19:47:52Z",
                "published_parsed": [
                    2023,
                    6,
                    11,
                    19,
                    47,
                    52,
                    6,
                    162,
                    0
                ],
                "title": "CoTran: An LLM-based Code Translator using Reinforcement Learning with\n  Feedback from Compiler and Symbolic Execution",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CoTran: An LLM-based Code Translator using Reinforcement Learning with\n  Feedback from Compiler and Symbolic Execution"
                },
                "summary": "In this paper, we present an LLM-based code translation method and an\nassociated tool called CoTran, that translates whole-programs from one\nhigh-level programming language to another. Existing LLM-based code translation\nmethods lack training to ensure that the translated code reliably compiles or\nbears substantial functional equivalence to the input code. In our work, we\nfine-tune an LLM using reinforcement learning, incorporating compiler feedback,\nand symbolic execution (symexec)-based testing feedback to assess functional\nequivalence between the input and output programs. The idea is to guide an LLM\nduring fine-tuning, via compiler and symexec-based testing feedback, by letting\nit know how far it is from producing perfect translations. We conduct extensive\nexperiments comparing CoTran with 14 other code translation tools, including\nhuman-written transpilers, LLM-based translation tools, and ChatGPT. Using a\nbenchmark of over \\num{57000} code pairs in Java and Python, we demonstrate\nthat CoTran outperforms the other tools on relevant metrics such as compilation\naccuracy (CompAcc) and functional equivalence accuracy (FEqAcc). For example,\nin Python-to-Java translation, CoTran achieves 48.68% FEqAcc and 76.98%\nCompAcc, whereas the nearest competing tool (PLBART-base) gets 38.26% and\n75.77% respectively. Additionally, CoTran, built on top of CodeT5, improves\nFEqAcc by +14.89% and CompAcc by +8.14% for Python-to-Java (resp., +12.94% and\n+4.30% for Java-to-Python).",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this paper, we present an LLM-based code translation method and an\nassociated tool called CoTran, that translates whole-programs from one\nhigh-level programming language to another. Existing LLM-based code translation\nmethods lack training to ensure that the translated code reliably compiles or\nbears substantial functional equivalence to the input code. In our work, we\nfine-tune an LLM using reinforcement learning, incorporating compiler feedback,\nand symbolic execution (symexec)-based testing feedback to assess functional\nequivalence between the input and output programs. The idea is to guide an LLM\nduring fine-tuning, via compiler and symexec-based testing feedback, by letting\nit know how far it is from producing perfect translations. We conduct extensive\nexperiments comparing CoTran with 14 other code translation tools, including\nhuman-written transpilers, LLM-based translation tools, and ChatGPT. Using a\nbenchmark of over \\num{57000} code pairs in Java and Python, we demonstrate\nthat CoTran outperforms the other tools on relevant metrics such as compilation\naccuracy (CompAcc) and functional equivalence accuracy (FEqAcc). For example,\nin Python-to-Java translation, CoTran achieves 48.68% FEqAcc and 76.98%\nCompAcc, whereas the nearest competing tool (PLBART-base) gets 38.26% and\n75.77% respectively. Additionally, CoTran, built on top of CodeT5, improves\nFEqAcc by +14.89% and CompAcc by +8.14% for Python-to-Java (resp., +12.94% and\n+4.30% for Java-to-Python)."
                },
                "authors": [
                    {
                        "name": "Prithwish Jana"
                    },
                    {
                        "name": "Piyush Jha"
                    },
                    {
                        "name": "Haoyang Ju"
                    },
                    {
                        "name": "Gautham Kishore"
                    },
                    {
                        "name": "Aryan Mahajan"
                    },
                    {
                        "name": "Vijay Ganesh"
                    }
                ],
                "author_detail": {
                    "name": "Vijay Ganesh"
                },
                "author": "Vijay Ganesh",
                "arxiv_doi": "10.3233/FAIA240968",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.3233/FAIA240968",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2306.06755v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2306.06755v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "The paper has been published at the 27th European Conference on\n  Artificial Intelligence (ECAI-2024) and is available at\n  https://ebooks.iospress.nl/doi/10.3233/FAIA240968. This arXiv version is the\n  full version that includes the supplementary material (Appendix)",
                "arxiv_primary_category": {
                    "term": "cs.PL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.PL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.7; I.2.5; D.2",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.23230v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.23230v2",
                "updated": "2024-10-31T04:20:22Z",
                "updated_parsed": [
                    2024,
                    10,
                    31,
                    4,
                    20,
                    22,
                    3,
                    305,
                    0
                ],
                "published": "2024-10-30T17:18:53Z",
                "published_parsed": [
                    2024,
                    10,
                    30,
                    17,
                    18,
                    53,
                    2,
                    304,
                    0
                ],
                "title": "Aligning Audio-Visual Joint Representations with an Agentic Workflow",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Aligning Audio-Visual Joint Representations with an Agentic Workflow"
                },
                "summary": "Visual content and accompanied audio signals naturally formulate a joint\nrepresentation to improve audio-visual (AV) related applications. While studies\ndevelop various AV representation learning frameworks, the importance of AV\ndata alignment is usually undermined for achieving high-quality representation.\nWe observe that an audio signal may contain background noise interference.\nAlso, non-synchronization may appear between audio and video streams. These\nnon-strict data alignment limits representation quality and downgrade\napplication performance. In this paper, we propose to improve AV joint\nrepresentations from a data-centric perspective by aligning audio signals to\nvisual data. Our alignment is conducted in an agentic workflow controlled by an\nLLM-based assistant named AVAgent. For each input AV data pair, our AVAgent\nuses a multi-modal LLM to convert audio and visual data into language\ndescriptions separately (i.e., tool use). Then, AVAgent reasons whether this\npaired data is aligned well and plans to edit the audio signal if needed (i.e.,\nplanning). The audio editing is executed by predefined actions that filter\nnoise or augment data. Moreover, we use a VLM to evaluate how modified audio\nsignals match the visual content and provide feedback to AVAgent (i.e.,\nreflection). The tool use, planning, and reflection steps operate cyclically to\nbecome an agentic workflow where audio signals are gradually aligned to visual\ncontent. To this end, existing methods can directly leverage the aligned AV\ndata via our agentic workflow to improve AV joint representations. The\nexperimental results comprehensively demonstrate the state-of-the-art\nperformance of the proposed approach against previous baselines in diverse\ndownstream tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Visual content and accompanied audio signals naturally formulate a joint\nrepresentation to improve audio-visual (AV) related applications. While studies\ndevelop various AV representation learning frameworks, the importance of AV\ndata alignment is usually undermined for achieving high-quality representation.\nWe observe that an audio signal may contain background noise interference.\nAlso, non-synchronization may appear between audio and video streams. These\nnon-strict data alignment limits representation quality and downgrade\napplication performance. In this paper, we propose to improve AV joint\nrepresentations from a data-centric perspective by aligning audio signals to\nvisual data. Our alignment is conducted in an agentic workflow controlled by an\nLLM-based assistant named AVAgent. For each input AV data pair, our AVAgent\nuses a multi-modal LLM to convert audio and visual data into language\ndescriptions separately (i.e., tool use). Then, AVAgent reasons whether this\npaired data is aligned well and plans to edit the audio signal if needed (i.e.,\nplanning). The audio editing is executed by predefined actions that filter\nnoise or augment data. Moreover, we use a VLM to evaluate how modified audio\nsignals match the visual content and provide feedback to AVAgent (i.e.,\nreflection). The tool use, planning, and reflection steps operate cyclically to\nbecome an agentic workflow where audio signals are gradually aligned to visual\ncontent. To this end, existing methods can directly leverage the aligned AV\ndata via our agentic workflow to improve AV joint representations. The\nexperimental results comprehensively demonstrate the state-of-the-art\nperformance of the proposed approach against previous baselines in diverse\ndownstream tasks."
                },
                "authors": [
                    {
                        "name": "Shentong Mo"
                    },
                    {
                        "name": "Yibing Song"
                    }
                ],
                "author_detail": {
                    "name": "Yibing Song"
                },
                "author": "Yibing Song",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.23230v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.23230v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.02551v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.02551v2",
                "updated": "2024-10-30T17:16:44Z",
                "updated_parsed": [
                    2024,
                    10,
                    30,
                    17,
                    16,
                    44,
                    2,
                    304,
                    0
                ],
                "published": "2024-07-02T16:19:25Z",
                "published_parsed": [
                    2024,
                    7,
                    2,
                    16,
                    19,
                    25,
                    1,
                    184,
                    0
                ],
                "title": "Breach By A Thousand Leaks: Unsafe Information Leakage in `Safe' AI\n  Responses",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Breach By A Thousand Leaks: Unsafe Information Leakage in `Safe' AI\n  Responses"
                },
                "summary": "Vulnerability of Frontier language models to misuse and jailbreaks has\nprompted the development of safety measures like filters and alignment training\nin an effort to ensure safety through robustness to adversarially crafted\nprompts. We assert that robustness is fundamentally insufficient for ensuring\nsafety goals, and current defenses and evaluation methods fail to account for\nrisks of dual-intent queries and their composition for malicious goals. To\nquantify these risks, we introduce a new safety evaluation framework based on\nimpermissible information leakage of model outputs and demonstrate how our\nproposed question-decomposition attack can extract dangerous knowledge from a\ncensored LLM more effectively than traditional jailbreaking. Underlying our\nproposed evaluation method is a novel information-theoretic threat model of\ninferential adversaries, distinguished from security adversaries, such as\njailbreaks, in that success is measured by inferring impermissible knowledge\nfrom victim outputs as opposed to forcing explicitly impermissible outputs from\nthe victim. Through our information-theoretic framework, we show that to ensure\nsafety against inferential adversaries, defense mechanisms must ensure\ninformation censorship, bounding the leakage of impermissible information.\nHowever, we prove that such defenses inevitably incur a safety-utility\ntrade-off.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Vulnerability of Frontier language models to misuse and jailbreaks has\nprompted the development of safety measures like filters and alignment training\nin an effort to ensure safety through robustness to adversarially crafted\nprompts. We assert that robustness is fundamentally insufficient for ensuring\nsafety goals, and current defenses and evaluation methods fail to account for\nrisks of dual-intent queries and their composition for malicious goals. To\nquantify these risks, we introduce a new safety evaluation framework based on\nimpermissible information leakage of model outputs and demonstrate how our\nproposed question-decomposition attack can extract dangerous knowledge from a\ncensored LLM more effectively than traditional jailbreaking. Underlying our\nproposed evaluation method is a novel information-theoretic threat model of\ninferential adversaries, distinguished from security adversaries, such as\njailbreaks, in that success is measured by inferring impermissible knowledge\nfrom victim outputs as opposed to forcing explicitly impermissible outputs from\nthe victim. Through our information-theoretic framework, we show that to ensure\nsafety against inferential adversaries, defense mechanisms must ensure\ninformation censorship, bounding the leakage of impermissible information.\nHowever, we prove that such defenses inevitably incur a safety-utility\ntrade-off."
                },
                "authors": [
                    {
                        "name": "David Glukhov"
                    },
                    {
                        "name": "Ziwen Han"
                    },
                    {
                        "name": "Ilia Shumailov"
                    },
                    {
                        "name": "Vardan Papyan"
                    },
                    {
                        "name": "Nicolas Papernot"
                    }
                ],
                "author_detail": {
                    "name": "Nicolas Papernot"
                },
                "author": "Nicolas Papernot",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.02551v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.02551v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.23223v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.23223v1",
                "updated": "2024-10-30T17:13:02Z",
                "updated_parsed": [
                    2024,
                    10,
                    30,
                    17,
                    13,
                    2,
                    2,
                    304,
                    0
                ],
                "published": "2024-10-30T17:13:02Z",
                "published_parsed": [
                    2024,
                    10,
                    30,
                    17,
                    13,
                    2,
                    2,
                    304,
                    0
                ],
                "title": "COMAL: A Convergent Meta-Algorithm for Aligning LLMs with General\n  Preferences",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "COMAL: A Convergent Meta-Algorithm for Aligning LLMs with General\n  Preferences"
                },
                "summary": "Many alignment methods, including reinforcement learning from human feedback\n(RLHF), rely on the Bradley-Terry reward assumption, which is insufficient to\ncapture the full range of general human preferences. To achieve robust\nalignment with general preferences, we model the alignment problem as a\ntwo-player zero-sum game, where the Nash equilibrium policy guarantees a 50%\nwin rate against any competing policy. However, previous algorithms for finding\nthe Nash policy either diverge or converge to a Nash policy in a modified game,\neven in a simple synthetic setting, thereby failing to maintain the 50% win\nrate guarantee against all other policies. We propose a meta-algorithm,\nConvergent Meta Alignment Algorithm (COMAL), for language model alignment with\ngeneral preferences, inspired by convergent algorithms in game theory.\nTheoretically, we prove that our meta-algorithm converges to an exact Nash\npolicy in the last iterate. Additionally, our meta-algorithm is simple and can\nbe integrated with many existing methods designed for RLHF and preference\noptimization with minimal changes. Experimental results demonstrate the\neffectiveness of the proposed framework when combined with existing preference\npolicy optimization methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Many alignment methods, including reinforcement learning from human feedback\n(RLHF), rely on the Bradley-Terry reward assumption, which is insufficient to\ncapture the full range of general human preferences. To achieve robust\nalignment with general preferences, we model the alignment problem as a\ntwo-player zero-sum game, where the Nash equilibrium policy guarantees a 50%\nwin rate against any competing policy. However, previous algorithms for finding\nthe Nash policy either diverge or converge to a Nash policy in a modified game,\neven in a simple synthetic setting, thereby failing to maintain the 50% win\nrate guarantee against all other policies. We propose a meta-algorithm,\nConvergent Meta Alignment Algorithm (COMAL), for language model alignment with\ngeneral preferences, inspired by convergent algorithms in game theory.\nTheoretically, we prove that our meta-algorithm converges to an exact Nash\npolicy in the last iterate. Additionally, our meta-algorithm is simple and can\nbe integrated with many existing methods designed for RLHF and preference\noptimization with minimal changes. Experimental results demonstrate the\neffectiveness of the proposed framework when combined with existing preference\npolicy optimization methods."
                },
                "authors": [
                    {
                        "name": "Yixin Liu"
                    },
                    {
                        "name": "Argyris Oikonomou"
                    },
                    {
                        "name": "Weiqiang Zheng"
                    },
                    {
                        "name": "Yang Cai"
                    },
                    {
                        "name": "Arman Cohan"
                    }
                ],
                "author_detail": {
                    "name": "Arman Cohan"
                },
                "author": "Arman Cohan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.23223v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.23223v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.GT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.16745v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.16745v2",
                "updated": "2024-10-30T17:10:52Z",
                "updated_parsed": [
                    2024,
                    10,
                    30,
                    17,
                    10,
                    52,
                    2,
                    304,
                    0
                ],
                "published": "2024-06-24T15:53:11Z",
                "published_parsed": [
                    2024,
                    6,
                    24,
                    15,
                    53,
                    11,
                    0,
                    176,
                    0
                ],
                "title": "Bandits with Preference Feedback: A Stackelberg Game Perspective",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Bandits with Preference Feedback: A Stackelberg Game Perspective"
                },
                "summary": "Bandits with preference feedback present a powerful tool for optimizing\nunknown target functions when only pairwise comparisons are allowed instead of\ndirect value queries. This model allows for incorporating human feedback into\nonline inference and optimization and has been employed in systems for\nfine-tuning large language models. The problem is well understood in simplified\nsettings with linear target functions or over finite small domains that limit\npractical interest. Taking the next step, we consider infinite domains and\nnonlinear (kernelized) rewards. In this setting, selecting a pair of actions is\nquite challenging and requires balancing exploration and exploitation at two\nlevels: within the pair, and along the iterations of the algorithm. We propose\nMAXMINLCB, which emulates this trade-off as a zero-sum Stackelberg game, and\nchooses action pairs that are informative and yield favorable rewards.\nMAXMINLCB consistently outperforms existing algorithms and satisfies an\nanytime-valid rate-optimal regret guarantee. This is due to our novel\npreference-based confidence sequences for kernelized logistic estimators.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Bandits with preference feedback present a powerful tool for optimizing\nunknown target functions when only pairwise comparisons are allowed instead of\ndirect value queries. This model allows for incorporating human feedback into\nonline inference and optimization and has been employed in systems for\nfine-tuning large language models. The problem is well understood in simplified\nsettings with linear target functions or over finite small domains that limit\npractical interest. Taking the next step, we consider infinite domains and\nnonlinear (kernelized) rewards. In this setting, selecting a pair of actions is\nquite challenging and requires balancing exploration and exploitation at two\nlevels: within the pair, and along the iterations of the algorithm. We propose\nMAXMINLCB, which emulates this trade-off as a zero-sum Stackelberg game, and\nchooses action pairs that are informative and yield favorable rewards.\nMAXMINLCB consistently outperforms existing algorithms and satisfies an\nanytime-valid rate-optimal regret guarantee. This is due to our novel\npreference-based confidence sequences for kernelized logistic estimators."
                },
                "authors": [
                    {
                        "name": "Barna Pásztor"
                    },
                    {
                        "name": "Parnian Kassraie"
                    },
                    {
                        "name": "Andreas Krause"
                    }
                ],
                "author_detail": {
                    "name": "Andreas Krause"
                },
                "author": "Andreas Krause",
                "arxiv_comment": "In Proceedings of the 38th Conference on Neural Information\n  Processing Systems (NeurIPS), 30 pages, 8 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.16745v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.16745v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.GT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.23214v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.23214v2",
                "updated": "2024-10-31T01:34:16Z",
                "updated_parsed": [
                    2024,
                    10,
                    31,
                    1,
                    34,
                    16,
                    3,
                    305,
                    0
                ],
                "published": "2024-10-30T17:02:54Z",
                "published_parsed": [
                    2024,
                    10,
                    30,
                    17,
                    2,
                    54,
                    2,
                    304,
                    0
                ],
                "title": "Grounding by Trying: LLMs with Reinforcement Learning-Enhanced Retrieval",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Grounding by Trying: LLMs with Reinforcement Learning-Enhanced Retrieval"
                },
                "summary": "The hallucinations of large language models (LLMs) are increasingly mitigated\nby allowing LLMs to search for information and to ground their answers in real\nsources. Unfortunately, LLMs often struggle with posing the right search\nqueries, especially when dealing with complex or otherwise indirect topics.\nObserving that LLMs can learn to search for relevant facts by $\\textit{trying}$\ndifferent queries and learning to up-weight queries that successfully produce\nrelevant results, we introduce $\\underline{Le}$arning to $\\underline{Re}$trieve\nby $\\underline{T}$rying (LeReT), a reinforcement learning framework that\nexplores search queries and uses preference-based optimization to improve their\nquality. LeReT can improve the absolute retrieval accuracy by up to 29% and the\ndownstream generator evaluations by 17%. The simplicity and flexibility of\nLeReT allows it to be applied to arbitrary off-the-shelf retrievers and makes\nit a promising technique for improving general LLM pipelines. Project website:\nhttp://sherylhsu.com/LeReT/.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The hallucinations of large language models (LLMs) are increasingly mitigated\nby allowing LLMs to search for information and to ground their answers in real\nsources. Unfortunately, LLMs often struggle with posing the right search\nqueries, especially when dealing with complex or otherwise indirect topics.\nObserving that LLMs can learn to search for relevant facts by $\\textit{trying}$\ndifferent queries and learning to up-weight queries that successfully produce\nrelevant results, we introduce $\\underline{Le}$arning to $\\underline{Re}$trieve\nby $\\underline{T}$rying (LeReT), a reinforcement learning framework that\nexplores search queries and uses preference-based optimization to improve their\nquality. LeReT can improve the absolute retrieval accuracy by up to 29% and the\ndownstream generator evaluations by 17%. The simplicity and flexibility of\nLeReT allows it to be applied to arbitrary off-the-shelf retrievers and makes\nit a promising technique for improving general LLM pipelines. Project website:\nhttp://sherylhsu.com/LeReT/."
                },
                "authors": [
                    {
                        "name": "Sheryl Hsu"
                    },
                    {
                        "name": "Omar Khattab"
                    },
                    {
                        "name": "Chelsea Finn"
                    },
                    {
                        "name": "Archit Sharma"
                    }
                ],
                "author_detail": {
                    "name": "Archit Sharma"
                },
                "author": "Archit Sharma",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.23214v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.23214v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.16493v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.16493v2",
                "updated": "2024-10-30T16:58:25Z",
                "updated_parsed": [
                    2024,
                    10,
                    30,
                    16,
                    58,
                    25,
                    2,
                    304,
                    0
                ],
                "published": "2024-05-26T09:11:46Z",
                "published_parsed": [
                    2024,
                    5,
                    26,
                    9,
                    11,
                    46,
                    6,
                    147,
                    0
                ],
                "title": "Flow Snapshot Neurons in Action: Deep Neural Networks Generalize to\n  Biological Motion Perception",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Flow Snapshot Neurons in Action: Deep Neural Networks Generalize to\n  Biological Motion Perception"
                },
                "summary": "Biological motion perception (BMP) refers to humans' ability to perceive and\nrecognize the actions of living beings solely from their motion patterns,\nsometimes as minimal as those depicted on point-light displays. While humans\nexcel at these tasks without any prior training, current AI models struggle\nwith poor generalization performance. To close this research gap, we propose\nthe Motion Perceiver (MP). MP solely relies on patch-level optical flows from\nvideo clips as inputs. During training, it learns prototypical flow snapshots\nthrough a competitive binding mechanism and integrates invariant motion\nrepresentations to predict action labels for the given video. During inference,\nwe evaluate the generalization ability of all AI models and humans on 62,656\nvideo stimuli spanning 24 BMP conditions using point-light displays in\nneuroscience. Remarkably, MP outperforms all existing AI models with a maximum\nimprovement of 29% in top-1 action recognition accuracy on these conditions.\nMoreover, we benchmark all AI models in point-light displays of two standard\nvideo datasets in computer vision. MP also demonstrates superior performance in\nthese cases. More interestingly, via psychophysics experiments, we found that\nMP recognizes biological movements in a way that aligns with human behaviors.\nOur data and code are available at\nhttps://github.com/ZhangLab-DeepNeuroCogLab/MotionPerceiver.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Biological motion perception (BMP) refers to humans' ability to perceive and\nrecognize the actions of living beings solely from their motion patterns,\nsometimes as minimal as those depicted on point-light displays. While humans\nexcel at these tasks without any prior training, current AI models struggle\nwith poor generalization performance. To close this research gap, we propose\nthe Motion Perceiver (MP). MP solely relies on patch-level optical flows from\nvideo clips as inputs. During training, it learns prototypical flow snapshots\nthrough a competitive binding mechanism and integrates invariant motion\nrepresentations to predict action labels for the given video. During inference,\nwe evaluate the generalization ability of all AI models and humans on 62,656\nvideo stimuli spanning 24 BMP conditions using point-light displays in\nneuroscience. Remarkably, MP outperforms all existing AI models with a maximum\nimprovement of 29% in top-1 action recognition accuracy on these conditions.\nMoreover, we benchmark all AI models in point-light displays of two standard\nvideo datasets in computer vision. MP also demonstrates superior performance in\nthese cases. More interestingly, via psychophysics experiments, we found that\nMP recognizes biological movements in a way that aligns with human behaviors.\nOur data and code are available at\nhttps://github.com/ZhangLab-DeepNeuroCogLab/MotionPerceiver."
                },
                "authors": [
                    {
                        "name": "Shuangpeng Han"
                    },
                    {
                        "name": "Ziyu Wang"
                    },
                    {
                        "name": "Mengmi Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Mengmi Zhang"
                },
                "author": "Mengmi Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.16493v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.16493v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.05148v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.05148v3",
                "updated": "2024-10-30T16:52:42Z",
                "updated_parsed": [
                    2024,
                    10,
                    30,
                    16,
                    52,
                    42,
                    2,
                    304,
                    0
                ],
                "published": "2024-08-09T16:07:37Z",
                "published_parsed": [
                    2024,
                    8,
                    9,
                    16,
                    7,
                    37,
                    4,
                    222,
                    0
                ],
                "title": "Impacts of floating-point non-associativity on reproducibility for HPC\n  and deep learning applications",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Impacts of floating-point non-associativity on reproducibility for HPC\n  and deep learning applications"
                },
                "summary": "Run to run variability in parallel programs caused by floating-point\nnon-associativity has been known to significantly affect reproducibility in\niterative algorithms, due to accumulating errors. Non-reproducibility can\ncritically affect the efficiency and effectiveness of correctness testing for\nstochastic programs. Recently, the sensitivity of deep learning training and\ninference pipelines to floating-point non-associativity has been found to\nsometimes be extreme. It can prevent certification for commercial applications,\naccurate assessment of robustness and sensitivity, and bug detection. New\napproaches in scientific computing applications have coupled deep learning\nmodels with high-performance computing, leading to an aggravation of debugging\nand testing challenges. Here we perform an investigation of the statistical\nproperties of floating-point non-associativity within modern parallel\nprogramming models, and analyze performance and productivity impacts of\nreplacing atomic operations with deterministic alternatives on GPUs. We examine\nthe recently-added deterministic options in PyTorch within the context of GPU\ndeployment for deep learning, uncovering and quantifying the impacts of input\nparameters triggering run to run variability and reporting on the reliability\nand completeness of the documentation. Finally, we evaluate the strategy of\nexploiting automatic determinism that could be provided by deterministic\nhardware, using the Groq accelerator for inference portions of the deep\nlearning pipeline. We demonstrate the benefits that a hardware-based strategy\ncan provide within reproducibility and correctness efforts.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Run to run variability in parallel programs caused by floating-point\nnon-associativity has been known to significantly affect reproducibility in\niterative algorithms, due to accumulating errors. Non-reproducibility can\ncritically affect the efficiency and effectiveness of correctness testing for\nstochastic programs. Recently, the sensitivity of deep learning training and\ninference pipelines to floating-point non-associativity has been found to\nsometimes be extreme. It can prevent certification for commercial applications,\naccurate assessment of robustness and sensitivity, and bug detection. New\napproaches in scientific computing applications have coupled deep learning\nmodels with high-performance computing, leading to an aggravation of debugging\nand testing challenges. Here we perform an investigation of the statistical\nproperties of floating-point non-associativity within modern parallel\nprogramming models, and analyze performance and productivity impacts of\nreplacing atomic operations with deterministic alternatives on GPUs. We examine\nthe recently-added deterministic options in PyTorch within the context of GPU\ndeployment for deep learning, uncovering and quantifying the impacts of input\nparameters triggering run to run variability and reporting on the reliability\nand completeness of the documentation. Finally, we evaluate the strategy of\nexploiting automatic determinism that could be provided by deterministic\nhardware, using the Groq accelerator for inference portions of the deep\nlearning pipeline. We demonstrate the benefits that a hardware-based strategy\ncan provide within reproducibility and correctness efforts."
                },
                "authors": [
                    {
                        "name": "Sanjif Shanmugavelu"
                    },
                    {
                        "name": "Mathieu Taillefumier"
                    },
                    {
                        "name": "Christopher Culver"
                    },
                    {
                        "name": "Oscar Hernandez"
                    },
                    {
                        "name": "Mark Coletti"
                    },
                    {
                        "name": "Ada Sedova"
                    }
                ],
                "author_detail": {
                    "name": "Ada Sedova"
                },
                "author": "Ada Sedova",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.05148v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.05148v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.10372v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.10372v3",
                "updated": "2024-10-30T16:45:15Z",
                "updated_parsed": [
                    2024,
                    10,
                    30,
                    16,
                    45,
                    15,
                    2,
                    304,
                    0
                ],
                "published": "2024-09-16T15:15:51Z",
                "published_parsed": [
                    2024,
                    9,
                    16,
                    15,
                    15,
                    51,
                    0,
                    260,
                    0
                ],
                "title": "Instigating Cooperation among LLM Agents Using Adaptive Information\n  Modulation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Instigating Cooperation among LLM Agents Using Adaptive Information\n  Modulation"
                },
                "summary": "This paper introduces a novel framework combining LLM agents as proxies for\nhuman strategic behavior with reinforcement learning (RL) to engage these\nagents in evolving strategic interactions within team environments. Our\napproach extends traditional agent-based simulations by using strategic LLM\nagents (SLA) and introducing dynamic and adaptive governance through a\npro-social promoting RL agent (PPA) that modulates information access across\nagents in a network, optimizing social welfare and promoting pro-social\nbehavior. Through validation in iterative games, including the prisoner\ndilemma, we demonstrate that SLA agents exhibit nuanced strategic adaptations.\nThe PPA agent effectively learns to adjust information transparency, resulting\nin enhanced cooperation rates. This framework offers significant insights into\nAI-mediated social dynamics, contributing to the deployment of AI in real-world\nteam settings.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper introduces a novel framework combining LLM agents as proxies for\nhuman strategic behavior with reinforcement learning (RL) to engage these\nagents in evolving strategic interactions within team environments. Our\napproach extends traditional agent-based simulations by using strategic LLM\nagents (SLA) and introducing dynamic and adaptive governance through a\npro-social promoting RL agent (PPA) that modulates information access across\nagents in a network, optimizing social welfare and promoting pro-social\nbehavior. Through validation in iterative games, including the prisoner\ndilemma, we demonstrate that SLA agents exhibit nuanced strategic adaptations.\nThe PPA agent effectively learns to adjust information transparency, resulting\nin enhanced cooperation rates. This framework offers significant insights into\nAI-mediated social dynamics, contributing to the deployment of AI in real-world\nteam settings."
                },
                "authors": [
                    {
                        "name": "Qiliang Chen"
                    },
                    {
                        "name": "Sepehr Ilami"
                    },
                    {
                        "name": "Nunzio Lore"
                    },
                    {
                        "name": "Babak Heydari"
                    }
                ],
                "author_detail": {
                    "name": "Babak Heydari"
                },
                "author": "Babak Heydari",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.10372v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.10372v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.GT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.14421v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.14421v3",
                "updated": "2024-10-30T16:40:11Z",
                "updated_parsed": [
                    2024,
                    10,
                    30,
                    16,
                    40,
                    11,
                    2,
                    304,
                    0
                ],
                "published": "2024-07-19T15:43:41Z",
                "published_parsed": [
                    2024,
                    7,
                    19,
                    15,
                    43,
                    41,
                    4,
                    201,
                    0
                ],
                "title": "Short-period Heartbeat Binaries from TESS Full-Frame Images",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Short-period Heartbeat Binaries from TESS Full-Frame Images"
                },
                "summary": "We identify $240$ short-period ($P \\lesssim 10$ days) binary systems in the\nTESS data, $180$ of which are heartbeat binaries (HB). The sample is mostly a\nmix of A and B-type stars and primarily includes eclipsing systems, where over\n$30\\%$ of the sources with primary and secondary eclipses show a secular change\nin their inter-eclipse timings and relative eclipse depths over a multi-year\ntimescale, likely due to orbital precession. The orbital parameters of the\npopulation are estimated by fitting a heartbeat model to their phase curves and\nGaia magnitudes, where the model accounts for ellipsoidal variability, Doppler\nbeaming, reflection effects, and eclipses. We construct the sample's\nperiod-eccentricity distribution and find an eccentricity cutoff (where $e\n\\rightarrow 0$) at a period $1.7$ days. Additionally, we measure the periastron\nadvance rate for the $12$ of the precessing sources and find that they all\nexhibit prograde apsidal precession, which is as high as $9^{\\circ}$ yr$^{-1}$\nfor one of the systems. Using the inferred stellar parameters, we estimate the\ngeneral relativistic precession rate of the argument of periastron for the\npopulation and expect over $30$ systems to show a precession in excess of\n$0.3^{\\circ}$ yr$^{-1}$",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We identify $240$ short-period ($P \\lesssim 10$ days) binary systems in the\nTESS data, $180$ of which are heartbeat binaries (HB). The sample is mostly a\nmix of A and B-type stars and primarily includes eclipsing systems, where over\n$30\\%$ of the sources with primary and secondary eclipses show a secular change\nin their inter-eclipse timings and relative eclipse depths over a multi-year\ntimescale, likely due to orbital precession. The orbital parameters of the\npopulation are estimated by fitting a heartbeat model to their phase curves and\nGaia magnitudes, where the model accounts for ellipsoidal variability, Doppler\nbeaming, reflection effects, and eclipses. We construct the sample's\nperiod-eccentricity distribution and find an eccentricity cutoff (where $e\n\\rightarrow 0$) at a period $1.7$ days. Additionally, we measure the periastron\nadvance rate for the $12$ of the precessing sources and find that they all\nexhibit prograde apsidal precession, which is as high as $9^{\\circ}$ yr$^{-1}$\nfor one of the systems. Using the inferred stellar parameters, we estimate the\ngeneral relativistic precession rate of the argument of periastron for the\npopulation and expect over $30$ systems to show a precession in excess of\n$0.3^{\\circ}$ yr$^{-1}$"
                },
                "authors": [
                    {
                        "name": "Siddhant Solanki"
                    },
                    {
                        "name": "Agnieszka M. Cieplak"
                    },
                    {
                        "name": "Jeremy Schnittman"
                    },
                    {
                        "name": "John G. Baker"
                    },
                    {
                        "name": "Thomas Barclay"
                    },
                    {
                        "name": "Richard K. Barry"
                    },
                    {
                        "name": "Veselin Kostov"
                    },
                    {
                        "name": "Ethan Kruse"
                    },
                    {
                        "name": "Greg Olmschenk"
                    },
                    {
                        "name": "Brian P. Powell"
                    },
                    {
                        "name": "Stela Ishitani Silva"
                    },
                    {
                        "name": "Guillermo Torres"
                    }
                ],
                "author_detail": {
                    "name": "Guillermo Torres"
                },
                "author": "Guillermo Torres",
                "arxiv_comment": "Accepted for publication in ApJ",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.14421v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.14421v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.SR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.SR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.23182v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.23182v1",
                "updated": "2024-10-30T16:38:09Z",
                "updated_parsed": [
                    2024,
                    10,
                    30,
                    16,
                    38,
                    9,
                    2,
                    304,
                    0
                ],
                "published": "2024-10-30T16:38:09Z",
                "published_parsed": [
                    2024,
                    10,
                    30,
                    16,
                    38,
                    9,
                    2,
                    304,
                    0
                ],
                "title": "ProTransformer: Robustify Transformers via Plug-and-Play Paradigm",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ProTransformer: Robustify Transformers via Plug-and-Play Paradigm"
                },
                "summary": "Transformer-based architectures have dominated various areas of machine\nlearning in recent years. In this paper, we introduce a novel robust attention\nmechanism designed to enhance the resilience of transformer-based\narchitectures. Crucially, this technique can be integrated into existing\ntransformers as a plug-and-play layer, improving their robustness without the\nneed for additional training or fine-tuning. Through comprehensive experiments\nand ablation studies, we demonstrate that our ProTransformer significantly\nenhances the robustness of transformer models across a variety of prediction\ntasks, attack mechanisms, backbone architectures, and data domains. Notably,\nwithout further fine-tuning, the ProTransformer consistently improves the\nperformance of vanilla transformers by 19.5%, 28.3%, 16.1%, and 11.4% for BERT,\nALBERT, DistilBERT, and RoBERTa, respectively, under the classical TextFooler\nattack. Furthermore, ProTransformer shows promising resilience in large\nlanguage models (LLMs) against prompting-based attacks, improving the\nperformance of T5 and LLaMA by 24.8% and 17.8%, respectively, and enhancing\nVicuna by an average of 10.4% against the Jailbreaking attack. Beyond the\nlanguage domain, ProTransformer also demonstrates outstanding robustness in\nboth vision and graph domains.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Transformer-based architectures have dominated various areas of machine\nlearning in recent years. In this paper, we introduce a novel robust attention\nmechanism designed to enhance the resilience of transformer-based\narchitectures. Crucially, this technique can be integrated into existing\ntransformers as a plug-and-play layer, improving their robustness without the\nneed for additional training or fine-tuning. Through comprehensive experiments\nand ablation studies, we demonstrate that our ProTransformer significantly\nenhances the robustness of transformer models across a variety of prediction\ntasks, attack mechanisms, backbone architectures, and data domains. Notably,\nwithout further fine-tuning, the ProTransformer consistently improves the\nperformance of vanilla transformers by 19.5%, 28.3%, 16.1%, and 11.4% for BERT,\nALBERT, DistilBERT, and RoBERTa, respectively, under the classical TextFooler\nattack. Furthermore, ProTransformer shows promising resilience in large\nlanguage models (LLMs) against prompting-based attacks, improving the\nperformance of T5 and LLaMA by 24.8% and 17.8%, respectively, and enhancing\nVicuna by an average of 10.4% against the Jailbreaking attack. Beyond the\nlanguage domain, ProTransformer also demonstrates outstanding robustness in\nboth vision and graph domains."
                },
                "authors": [
                    {
                        "name": "Zhichao Hou"
                    },
                    {
                        "name": "Weizhi Gao"
                    },
                    {
                        "name": "Yuchen Shen"
                    },
                    {
                        "name": "Feiyi Wang"
                    },
                    {
                        "name": "Xiaorui Liu"
                    }
                ],
                "author_detail": {
                    "name": "Xiaorui Liu"
                },
                "author": "Xiaorui Liu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.23182v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.23182v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.23180v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.23180v1",
                "updated": "2024-10-30T16:37:04Z",
                "updated_parsed": [
                    2024,
                    10,
                    30,
                    16,
                    37,
                    4,
                    2,
                    304,
                    0
                ],
                "published": "2024-10-30T16:37:04Z",
                "published_parsed": [
                    2024,
                    10,
                    30,
                    16,
                    37,
                    4,
                    2,
                    304,
                    0
                ],
                "title": "ReasoningRec: Bridging Personalized Recommendations and\n  Human-Interpretable Explanations through LLM Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ReasoningRec: Bridging Personalized Recommendations and\n  Human-Interpretable Explanations through LLM Reasoning"
                },
                "summary": "This paper presents ReasoningRec, a reasoning-based recommendation framework\nthat leverages Large Language Models (LLMs) to bridge the gap between\nrecommendations and human-interpretable explanations. In contrast to\nconventional recommendation systems that rely on implicit user-item\ninteractions, ReasoningRec employs LLMs to model users and items, focusing on\npreferences, aversions, and explanatory reasoning. The framework utilizes a\nlarger LLM to generate synthetic explanations for user preferences,\nsubsequently used to fine-tune a smaller LLM for enhanced recommendation\naccuracy and human-interpretable explanation. Our experimental study\ninvestigates the impact of reasoning and contextual information on personalized\nrecommendations, revealing that the quality of contextual and personalized data\nsignificantly influences the LLM's capacity to generate plausible explanations.\nEmpirical evaluations demonstrate that ReasoningRec surpasses state-of-the-art\nmethods by up to 12.5\\% in recommendation prediction while concurrently\nproviding human-intelligible explanations. The code is available here:\nhttps://github.com/millenniumbismay/reasoningrec.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper presents ReasoningRec, a reasoning-based recommendation framework\nthat leverages Large Language Models (LLMs) to bridge the gap between\nrecommendations and human-interpretable explanations. In contrast to\nconventional recommendation systems that rely on implicit user-item\ninteractions, ReasoningRec employs LLMs to model users and items, focusing on\npreferences, aversions, and explanatory reasoning. The framework utilizes a\nlarger LLM to generate synthetic explanations for user preferences,\nsubsequently used to fine-tune a smaller LLM for enhanced recommendation\naccuracy and human-interpretable explanation. Our experimental study\ninvestigates the impact of reasoning and contextual information on personalized\nrecommendations, revealing that the quality of contextual and personalized data\nsignificantly influences the LLM's capacity to generate plausible explanations.\nEmpirical evaluations demonstrate that ReasoningRec surpasses state-of-the-art\nmethods by up to 12.5\\% in recommendation prediction while concurrently\nproviding human-intelligible explanations. The code is available here:\nhttps://github.com/millenniumbismay/reasoningrec."
                },
                "authors": [
                    {
                        "name": "Millennium Bismay"
                    },
                    {
                        "name": "Xiangjue Dong"
                    },
                    {
                        "name": "James Caverlee"
                    }
                ],
                "author_detail": {
                    "name": "James Caverlee"
                },
                "author": "James Caverlee",
                "arxiv_comment": "Large Language Model, Recommendation, Human-Interpretable Reasoning,\n  Personalization",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.23180v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.23180v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.14670v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.14670v2",
                "updated": "2024-10-30T16:33:48Z",
                "updated_parsed": [
                    2024,
                    10,
                    30,
                    16,
                    33,
                    48,
                    2,
                    304,
                    0
                ],
                "published": "2024-06-20T18:47:43Z",
                "published_parsed": [
                    2024,
                    6,
                    20,
                    18,
                    47,
                    43,
                    3,
                    172,
                    0
                ],
                "title": "Exploring Design Choices for Building Language-Specific LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Exploring Design Choices for Building Language-Specific LLMs"
                },
                "summary": "Despite rapid progress in large language models (LLMs), their performance on\na vast majority of languages remains unsatisfactory. In this paper, we study\nbuilding language-specific LLMs by adapting monolingual and multilingual LLMs.\nWe conduct systematic experiments on how design choices (base model selection,\nvocabulary extension, and continued pretraining) impact the adapted LLM, both\nin terms of efficiency (how many tokens are needed to encode the same amount of\ninformation) and end task performance. We find that (1) the initial performance\nof LLM does not always correlate with the final performance after the\nadaptation. Adapting an English-centric models can yield better results than\nadapting multilingual models despite their worse initial performance on\nlow-resource languages. (2) Efficiency can easily improved with simple\nvocabulary extension and continued pretraining in most LLMs we study, and (3)\nThe optimal adaptation method (choice of the base model, new vocabulary size,\ntraining data, initialization strategy) is highly language-dependent, and the\nsimplest embedding initialization works well across various experimental\nsettings. Together, our work lays foundations on efficiently building\nlanguage-specific LLMs by adapting existing LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Despite rapid progress in large language models (LLMs), their performance on\na vast majority of languages remains unsatisfactory. In this paper, we study\nbuilding language-specific LLMs by adapting monolingual and multilingual LLMs.\nWe conduct systematic experiments on how design choices (base model selection,\nvocabulary extension, and continued pretraining) impact the adapted LLM, both\nin terms of efficiency (how many tokens are needed to encode the same amount of\ninformation) and end task performance. We find that (1) the initial performance\nof LLM does not always correlate with the final performance after the\nadaptation. Adapting an English-centric models can yield better results than\nadapting multilingual models despite their worse initial performance on\nlow-resource languages. (2) Efficiency can easily improved with simple\nvocabulary extension and continued pretraining in most LLMs we study, and (3)\nThe optimal adaptation method (choice of the base model, new vocabulary size,\ntraining data, initialization strategy) is highly language-dependent, and the\nsimplest embedding initialization works well across various experimental\nsettings. Together, our work lays foundations on efficiently building\nlanguage-specific LLMs by adapting existing LLMs."
                },
                "authors": [
                    {
                        "name": "Atula Tejaswi"
                    },
                    {
                        "name": "Nilesh Gupta"
                    },
                    {
                        "name": "Eunsol Choi"
                    }
                ],
                "author_detail": {
                    "name": "Eunsol Choi"
                },
                "author": "Eunsol Choi",
                "arxiv_comment": "Accepted to EMNLP 2024 Findings",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.14670v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.14670v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.22309v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.22309v2",
                "updated": "2024-10-30T16:30:30Z",
                "updated_parsed": [
                    2024,
                    10,
                    30,
                    16,
                    30,
                    30,
                    2,
                    304,
                    0
                ],
                "published": "2024-10-29T17:53:10Z",
                "published_parsed": [
                    2024,
                    10,
                    29,
                    17,
                    53,
                    10,
                    1,
                    303,
                    0
                ],
                "title": "GPT-4o reads the mind in the eyes",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GPT-4o reads the mind in the eyes"
                },
                "summary": "Large Language Models (LLMs) are capable of reproducing human-like\ninferences, including inferences about emotions and mental states, from text.\nWhether this capability extends beyond text to other modalities remains\nunclear. Humans possess a sophisticated ability to read the mind in the eyes of\nother people. Here we tested whether this ability is also present in GPT-4o, a\nmultimodal LLM. Using two versions of a widely used theory of mind test, the\nReading the Mind in Eyes Test and the Multiracial Reading the Mind in the Eyes\nTest, we found that GPT-4o outperformed humans in interpreting mental states\nfrom upright faces but underperformed humans when faces were inverted. While\nhumans in our sample showed no difference between White and Non-white faces,\nGPT-4o's accuracy was higher for White than for Non-white faces. GPT-4o's\nerrors were not random but revealed a highly consistent, yet incorrect,\nprocessing of mental-state information across trials, with an\norientation-dependent error structure that qualitatively differed from that of\nhumans for inverted faces but not for upright faces. These findings highlight\nhow advanced mental state inference abilities and human-like face processing\nsignatures, such as inversion effects, coexist in GPT-4o alongside substantial\ndifferences in information processing compared to humans.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) are capable of reproducing human-like\ninferences, including inferences about emotions and mental states, from text.\nWhether this capability extends beyond text to other modalities remains\nunclear. Humans possess a sophisticated ability to read the mind in the eyes of\nother people. Here we tested whether this ability is also present in GPT-4o, a\nmultimodal LLM. Using two versions of a widely used theory of mind test, the\nReading the Mind in Eyes Test and the Multiracial Reading the Mind in the Eyes\nTest, we found that GPT-4o outperformed humans in interpreting mental states\nfrom upright faces but underperformed humans when faces were inverted. While\nhumans in our sample showed no difference between White and Non-white faces,\nGPT-4o's accuracy was higher for White than for Non-white faces. GPT-4o's\nerrors were not random but revealed a highly consistent, yet incorrect,\nprocessing of mental-state information across trials, with an\norientation-dependent error structure that qualitatively differed from that of\nhumans for inverted faces but not for upright faces. These findings highlight\nhow advanced mental state inference abilities and human-like face processing\nsignatures, such as inversion effects, coexist in GPT-4o alongside substantial\ndifferences in information processing compared to humans."
                },
                "authors": [
                    {
                        "name": "James W. A. Strachan"
                    },
                    {
                        "name": "Oriana Pansardi"
                    },
                    {
                        "name": "Eugenio Scaliti"
                    },
                    {
                        "name": "Marco Celotto"
                    },
                    {
                        "name": "Krati Saxena"
                    },
                    {
                        "name": "Chunzhi Yi"
                    },
                    {
                        "name": "Fabio Manzi"
                    },
                    {
                        "name": "Alessandro Rufo"
                    },
                    {
                        "name": "Guido Manzi"
                    },
                    {
                        "name": "Michael S. A. Graziano"
                    },
                    {
                        "name": "Stefano Panzeri"
                    },
                    {
                        "name": "Cristina Becchio"
                    }
                ],
                "author_detail": {
                    "name": "Cristina Becchio"
                },
                "author": "Cristina Becchio",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.22309v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.22309v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.23170v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.23170v1",
                "updated": "2024-10-30T16:20:48Z",
                "updated_parsed": [
                    2024,
                    10,
                    30,
                    16,
                    20,
                    48,
                    2,
                    304,
                    0
                ],
                "published": "2024-10-30T16:20:48Z",
                "published_parsed": [
                    2024,
                    10,
                    30,
                    16,
                    20,
                    48,
                    2,
                    304,
                    0
                ],
                "title": "Functional Gradient Flows for Constrained Sampling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Functional Gradient Flows for Constrained Sampling"
                },
                "summary": "Recently, through a unified gradient flow perspective of Markov chain Monte\nCarlo (MCMC) and variational inference (VI), particle-based variational\ninference methods (ParVIs) have been proposed that tend to combine the best of\nboth worlds. While typical ParVIs such as Stein Variational Gradient Descent\n(SVGD) approximate the gradient flow within a reproducing kernel Hilbert space\n(RKHS), many attempts have been made recently to replace RKHS with more\nexpressive function spaces, such as neural networks. While successful, these\nmethods are mainly designed for sampling from unconstrained domains. In this\npaper, we offer a general solution to constrained sampling by introducing a\nboundary condition for the gradient flow which would confine the particles\nwithin the specific domain. This allows us to propose a new functional gradient\nParVI method for constrained sampling, called constrained functional gradient\nflow (CFG), with provable continuous-time convergence in total variation (TV).\nWe also present novel numerical strategies to handle the boundary integral term\narising from the domain constraints. Our theory and experiments demonstrate the\neffectiveness of the proposed framework.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recently, through a unified gradient flow perspective of Markov chain Monte\nCarlo (MCMC) and variational inference (VI), particle-based variational\ninference methods (ParVIs) have been proposed that tend to combine the best of\nboth worlds. While typical ParVIs such as Stein Variational Gradient Descent\n(SVGD) approximate the gradient flow within a reproducing kernel Hilbert space\n(RKHS), many attempts have been made recently to replace RKHS with more\nexpressive function spaces, such as neural networks. While successful, these\nmethods are mainly designed for sampling from unconstrained domains. In this\npaper, we offer a general solution to constrained sampling by introducing a\nboundary condition for the gradient flow which would confine the particles\nwithin the specific domain. This allows us to propose a new functional gradient\nParVI method for constrained sampling, called constrained functional gradient\nflow (CFG), with provable continuous-time convergence in total variation (TV).\nWe also present novel numerical strategies to handle the boundary integral term\narising from the domain constraints. Our theory and experiments demonstrate the\neffectiveness of the proposed framework."
                },
                "authors": [
                    {
                        "name": "Shiyue Zhang"
                    },
                    {
                        "name": "Longlin Yu"
                    },
                    {
                        "name": "Ziheng Cheng"
                    },
                    {
                        "name": "Cheng Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Cheng Zhang"
                },
                "author": "Cheng Zhang",
                "arxiv_comment": "NeurIPS 2024 camera-ready (30 pages, 26 figures)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.23170v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.23170v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ML",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.23167v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.23167v1",
                "updated": "2024-10-30T16:18:53Z",
                "updated_parsed": [
                    2024,
                    10,
                    30,
                    16,
                    18,
                    53,
                    2,
                    304,
                    0
                ],
                "published": "2024-10-30T16:18:53Z",
                "published_parsed": [
                    2024,
                    10,
                    30,
                    16,
                    18,
                    53,
                    2,
                    304,
                    0
                ],
                "title": "Massive star evolution models incorporating $α$-enhanced\n  composition -- I. BPASS Single star models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Massive star evolution models incorporating $α$-enhanced\n  composition -- I. BPASS Single star models"
                },
                "summary": "Stellar evolution modelling is fundamental to many areas of astrophysics\nincluding stellar populations in both nearby and distant galaxies. It is\nheavily influenced by chemical composition. Observations of distant galaxies\nand nucleosynthesis calculations show that $\\alpha$-process elements are\nenriched faster than iron group elements. We present a dense grid of\nsingle-star models calculated using the BPASS stellar evolution code and\ncovering masses ($0.1\\le\\mathrm{M/M}_\\odot\\le316$), metallicity mass fractions\n($10^{-5} \\le Z \\le 0.04$) and $\\alpha$-to-iron abundance ratios\n($-0.2\\le[\\alpha/\\mathrm{Fe}]\\le+0.6$). By comparing Solar-scaled models to\nones enriched in $\\alpha$-process elements, we find that stellar radii, surface\ntemperatures, Main Sequence lifetimes, supernova progenitor properties and\nsupernova rates are all sensitive to changes in [$\\alpha$/Fe]. Lifetimes of\nlow-mass stars differ by up to 0.4 dex, while surface temperatures of massive\nstars at the end of the Main Sequence also differ by around 0.4 dex. Inferred\nsupernova rates when [Fe/H] is unknown can be highly uncertain. Models with\ndifferent [$\\alpha$/Fe] but comparable iron abundances show smaller variations,\nindicating that while iron primarily defines the course of evolution;\n$\\alpha$-enhancement nonetheless has an impact of up to 0.1 dex on stellar\nproperties. Such changes are small for individual stars, but have a large\ncumulative effect when considering an entire stellar population as demonstrated\nby isochrone fitting to nearby clusters. Changes in radii and lifetimes have\nfurther consequences for a stellar population including binary stars, as they\ninfluence the timing, nature and occurrence rate of mass transfer events.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Stellar evolution modelling is fundamental to many areas of astrophysics\nincluding stellar populations in both nearby and distant galaxies. It is\nheavily influenced by chemical composition. Observations of distant galaxies\nand nucleosynthesis calculations show that $\\alpha$-process elements are\nenriched faster than iron group elements. We present a dense grid of\nsingle-star models calculated using the BPASS stellar evolution code and\ncovering masses ($0.1\\le\\mathrm{M/M}_\\odot\\le316$), metallicity mass fractions\n($10^{-5} \\le Z \\le 0.04$) and $\\alpha$-to-iron abundance ratios\n($-0.2\\le[\\alpha/\\mathrm{Fe}]\\le+0.6$). By comparing Solar-scaled models to\nones enriched in $\\alpha$-process elements, we find that stellar radii, surface\ntemperatures, Main Sequence lifetimes, supernova progenitor properties and\nsupernova rates are all sensitive to changes in [$\\alpha$/Fe]. Lifetimes of\nlow-mass stars differ by up to 0.4 dex, while surface temperatures of massive\nstars at the end of the Main Sequence also differ by around 0.4 dex. Inferred\nsupernova rates when [Fe/H] is unknown can be highly uncertain. Models with\ndifferent [$\\alpha$/Fe] but comparable iron abundances show smaller variations,\nindicating that while iron primarily defines the course of evolution;\n$\\alpha$-enhancement nonetheless has an impact of up to 0.1 dex on stellar\nproperties. Such changes are small for individual stars, but have a large\ncumulative effect when considering an entire stellar population as demonstrated\nby isochrone fitting to nearby clusters. Changes in radii and lifetimes have\nfurther consequences for a stellar population including binary stars, as they\ninfluence the timing, nature and occurrence rate of mass transfer events."
                },
                "authors": [
                    {
                        "name": "Conor M Byrne"
                    },
                    {
                        "name": "Jan J Eldridge"
                    },
                    {
                        "name": "Elizabeth R Stanway"
                    }
                ],
                "author_detail": {
                    "name": "Elizabeth R Stanway"
                },
                "author": "Elizabeth R Stanway",
                "arxiv_comment": "Submitted to MNRAS on 01 August 2024. 18 pages, 12 figures, 1\n  appendix with 6 supplementary figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.23167v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.23167v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.GA",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.GA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "astro-ph.SR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.23166v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.23166v1",
                "updated": "2024-10-30T16:18:22Z",
                "updated_parsed": [
                    2024,
                    10,
                    30,
                    16,
                    18,
                    22,
                    2,
                    304,
                    0
                ],
                "published": "2024-10-30T16:18:22Z",
                "published_parsed": [
                    2024,
                    10,
                    30,
                    16,
                    18,
                    22,
                    2,
                    304,
                    0
                ],
                "title": "SciPIP: An LLM-based Scientific Paper Idea Proposer",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SciPIP: An LLM-based Scientific Paper Idea Proposer"
                },
                "summary": "The exponential growth of knowledge and the increasing complexity of\ninterdisciplinary research pose significant challenges for researchers,\nincluding information overload and difficulties in exploring novel ideas. The\nadvancements in large language models (LLMs), such as GPT-4, have shown great\npotential in enhancing idea proposals, but how to effectively utilize large\nmodels for reasonable idea proposal has not been thoroughly explored. This\npaper proposes a scientific paper idea proposer (SciPIP). Based on a\nuser-provided research background, SciPIP retrieves helpful papers from a\nliterature database while leveraging the capabilities of LLMs to generate more\nnovel and feasible ideas. To this end, 1) we construct a literature retrieval\ndatabase, extracting lots of papers' multi-dimension information for fast\naccess. Then, a literature retrieval method based on semantics, entity, and\ncitation co-occurrences is proposed to search relevant literature from multiple\naspects based on the user-provided background. 2) After literature retrieval,\nwe introduce dual-path idea proposal strategies, where one path infers\nsolutions from the retrieved literature and the other path generates original\nideas through model brainstorming. We then combine the two to achieve a good\nbalance between feasibility and originality. Through extensive experiments on\nthe natural language processing (NLP) field, we demonstrate that SciPIP can\nretrieve citations similar to those of existing top conference papers and\ngenerate many ideas consistent with them. Additionally, we evaluate the\noriginality of other ideas generated by SciPIP using large language models,\nfurther validating the effectiveness of our proposed method. The code and the\ndatabase are released at https://github.com/cheerss/SciPIP.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The exponential growth of knowledge and the increasing complexity of\ninterdisciplinary research pose significant challenges for researchers,\nincluding information overload and difficulties in exploring novel ideas. The\nadvancements in large language models (LLMs), such as GPT-4, have shown great\npotential in enhancing idea proposals, but how to effectively utilize large\nmodels for reasonable idea proposal has not been thoroughly explored. This\npaper proposes a scientific paper idea proposer (SciPIP). Based on a\nuser-provided research background, SciPIP retrieves helpful papers from a\nliterature database while leveraging the capabilities of LLMs to generate more\nnovel and feasible ideas. To this end, 1) we construct a literature retrieval\ndatabase, extracting lots of papers' multi-dimension information for fast\naccess. Then, a literature retrieval method based on semantics, entity, and\ncitation co-occurrences is proposed to search relevant literature from multiple\naspects based on the user-provided background. 2) After literature retrieval,\nwe introduce dual-path idea proposal strategies, where one path infers\nsolutions from the retrieved literature and the other path generates original\nideas through model brainstorming. We then combine the two to achieve a good\nbalance between feasibility and originality. Through extensive experiments on\nthe natural language processing (NLP) field, we demonstrate that SciPIP can\nretrieve citations similar to those of existing top conference papers and\ngenerate many ideas consistent with them. Additionally, we evaluate the\noriginality of other ideas generated by SciPIP using large language models,\nfurther validating the effectiveness of our proposed method. The code and the\ndatabase are released at https://github.com/cheerss/SciPIP."
                },
                "authors": [
                    {
                        "name": "Wenxiao Wang"
                    },
                    {
                        "name": "Lihui Gu"
                    },
                    {
                        "name": "Liye Zhang"
                    },
                    {
                        "name": "Yunxiang Luo"
                    },
                    {
                        "name": "Yi Dai"
                    },
                    {
                        "name": "Chen Shen"
                    },
                    {
                        "name": "Liang Xie"
                    },
                    {
                        "name": "Binbin Lin"
                    },
                    {
                        "name": "Xiaofei He"
                    },
                    {
                        "name": "Jieping Ye"
                    }
                ],
                "author_detail": {
                    "name": "Jieping Ye"
                },
                "author": "Jieping Ye",
                "arxiv_comment": "25 pages, 5 figures, 19 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.23166v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.23166v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.23162v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.23162v1",
                "updated": "2024-10-30T16:16:11Z",
                "updated_parsed": [
                    2024,
                    10,
                    30,
                    16,
                    16,
                    11,
                    2,
                    304,
                    0
                ],
                "published": "2024-10-30T16:16:11Z",
                "published_parsed": [
                    2024,
                    10,
                    30,
                    16,
                    16,
                    11,
                    2,
                    304,
                    0
                ],
                "title": "Kinetic Inductance and Jitter Dependence of the Intrinsic Photon Number\n  Resolution in Superconducting Nanowire Single-Photon Detectors",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Kinetic Inductance and Jitter Dependence of the Intrinsic Photon Number\n  Resolution in Superconducting Nanowire Single-Photon Detectors"
                },
                "summary": "The ability to resolve photon numbers is crucial in quantum information\nscience and technology, driving the development of detectors with intrinsic\nphoton-number resolving (PNR) capabilities. Although transition edge sensors\nrepresent the state-of-the-art in PNR performance, superconducting nanowire\nsingle-photon detectors (SNSPDs) offer superior efficiency, speed, noise\nreduction, and timing precision. Directly inferring photon numbers, however,\nhas only recently become feasible due to advances in readout technology.\nDespite this, photon-number discrimination remains constrained by the\nnanowire's electrical properties and readout jitter. In this work, we employ\nwaveguide-integrated SNSPDs and time-resolved measurements to explore how the\nnanowire kinetic inductance and system jitter affect PNR capabilities. By\nanalyzing the latency time of the photon detection, we can resolve changes in\nthe rising edge of the detection pulse. We find that lower jitter as well as\nincreased kinetic inductance enhances the pulse separation for different photon\nnumbers and improves the PNR capability. Enhancing the kinetic inductance from\n165 nH to 872 nH improves PNR quality by 12%, 31% and 23% over the first three\nphoton numbers, though at the cost of reducing the detector's count rate from\n165 Mcps to 19 Mcps. Our findings highlight the trade-off between PNR\nresolution and detector speed.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The ability to resolve photon numbers is crucial in quantum information\nscience and technology, driving the development of detectors with intrinsic\nphoton-number resolving (PNR) capabilities. Although transition edge sensors\nrepresent the state-of-the-art in PNR performance, superconducting nanowire\nsingle-photon detectors (SNSPDs) offer superior efficiency, speed, noise\nreduction, and timing precision. Directly inferring photon numbers, however,\nhas only recently become feasible due to advances in readout technology.\nDespite this, photon-number discrimination remains constrained by the\nnanowire's electrical properties and readout jitter. In this work, we employ\nwaveguide-integrated SNSPDs and time-resolved measurements to explore how the\nnanowire kinetic inductance and system jitter affect PNR capabilities. By\nanalyzing the latency time of the photon detection, we can resolve changes in\nthe rising edge of the detection pulse. We find that lower jitter as well as\nincreased kinetic inductance enhances the pulse separation for different photon\nnumbers and improves the PNR capability. Enhancing the kinetic inductance from\n165 nH to 872 nH improves PNR quality by 12%, 31% and 23% over the first three\nphoton numbers, though at the cost of reducing the detector's count rate from\n165 Mcps to 19 Mcps. Our findings highlight the trade-off between PNR\nresolution and detector speed."
                },
                "authors": [
                    {
                        "name": "Roland Jaha"
                    },
                    {
                        "name": "Connor A. Graham-Scott"
                    },
                    {
                        "name": "Adrian S. Abazi"
                    },
                    {
                        "name": "Wolfram Pernice"
                    },
                    {
                        "name": "Carsten Schuck"
                    },
                    {
                        "name": "Simone Ferrari"
                    }
                ],
                "author_detail": {
                    "name": "Simone Ferrari"
                },
                "author": "Simone Ferrari",
                "arxiv_comment": "21 pages, 7 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.23162v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.23162v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "quant-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "quant-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "physics.optics",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.19581v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.19581v2",
                "updated": "2024-10-30T16:12:36Z",
                "updated_parsed": [
                    2024,
                    10,
                    30,
                    16,
                    12,
                    36,
                    2,
                    304,
                    0
                ],
                "published": "2024-05-30T00:17:44Z",
                "published_parsed": [
                    2024,
                    5,
                    30,
                    0,
                    17,
                    44,
                    3,
                    151,
                    0
                ],
                "title": "Source Code Foundation Models are Transferable Binary Analysis Knowledge\n  Bases",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Source Code Foundation Models are Transferable Binary Analysis Knowledge\n  Bases"
                },
                "summary": "Human-Oriented Binary Reverse Engineering (HOBRE) lies at the intersection of\nbinary and source code, aiming to lift binary code to human-readable content\nrelevant to source code, thereby bridging the binary-source semantic gap.\nRecent advancements in uni-modal code model pre-training, particularly in\ngenerative Source Code Foundation Models (SCFMs) and binary understanding\nmodels, have laid the groundwork for transfer learning applicable to HOBRE.\nHowever, existing approaches for HOBRE rely heavily on uni-modal models like\nSCFMs for supervised fine-tuning or general LLMs for prompting, resulting in\nsub-optimal performance. Inspired by recent progress in large multi-modal\nmodels, we propose that it is possible to harness the strengths of uni-modal\ncode models from both sides to bridge the semantic gap effectively. In this\npaper, we introduce a novel probe-and-recover framework that incorporates a\nbinary-source encoder-decoder model and black-box LLMs for binary analysis. Our\napproach leverages the pre-trained knowledge within SCFMs to synthesize\nrelevant, symbol-rich code fragments as context. This additional context\nenables black-box LLMs to enhance recovery accuracy. We demonstrate significant\nimprovements in zero-shot binary summarization and binary function name\nrecovery, with a 10.3% relative gain in CHRF and a 16.7% relative gain in a\nGPT4-based metric for summarization, as well as a 6.7% and 7.4% absolute\nincrease in token-level precision and recall for name recovery, respectively.\nThese results highlight the effectiveness of our approach in automating and\nimproving binary code analysis.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Human-Oriented Binary Reverse Engineering (HOBRE) lies at the intersection of\nbinary and source code, aiming to lift binary code to human-readable content\nrelevant to source code, thereby bridging the binary-source semantic gap.\nRecent advancements in uni-modal code model pre-training, particularly in\ngenerative Source Code Foundation Models (SCFMs) and binary understanding\nmodels, have laid the groundwork for transfer learning applicable to HOBRE.\nHowever, existing approaches for HOBRE rely heavily on uni-modal models like\nSCFMs for supervised fine-tuning or general LLMs for prompting, resulting in\nsub-optimal performance. Inspired by recent progress in large multi-modal\nmodels, we propose that it is possible to harness the strengths of uni-modal\ncode models from both sides to bridge the semantic gap effectively. In this\npaper, we introduce a novel probe-and-recover framework that incorporates a\nbinary-source encoder-decoder model and black-box LLMs for binary analysis. Our\napproach leverages the pre-trained knowledge within SCFMs to synthesize\nrelevant, symbol-rich code fragments as context. This additional context\nenables black-box LLMs to enhance recovery accuracy. We demonstrate significant\nimprovements in zero-shot binary summarization and binary function name\nrecovery, with a 10.3% relative gain in CHRF and a 16.7% relative gain in a\nGPT4-based metric for summarization, as well as a 6.7% and 7.4% absolute\nincrease in token-level precision and recall for name recovery, respectively.\nThese results highlight the effectiveness of our approach in automating and\nimproving binary code analysis."
                },
                "authors": [
                    {
                        "name": "Zian Su"
                    },
                    {
                        "name": "Xiangzhe Xu"
                    },
                    {
                        "name": "Ziyang Huang"
                    },
                    {
                        "name": "Kaiyuan Zhang"
                    },
                    {
                        "name": "Xiangyu Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Xiangyu Zhang"
                },
                "author": "Xiangyu Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.19581v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.19581v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.18975v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.18975v2",
                "updated": "2024-10-30T16:10:33Z",
                "updated_parsed": [
                    2024,
                    10,
                    30,
                    16,
                    10,
                    33,
                    2,
                    304,
                    0
                ],
                "published": "2024-10-24T17:59:31Z",
                "published_parsed": [
                    2024,
                    10,
                    24,
                    17,
                    59,
                    31,
                    3,
                    298,
                    0
                ],
                "title": "Unbounded: A Generative Infinite Game of Character Life Simulation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Unbounded: A Generative Infinite Game of Character Life Simulation"
                },
                "summary": "We introduce the concept of a generative infinite game, a video game that\ntranscends the traditional boundaries of finite, hard-coded systems by using\ngenerative models. Inspired by James P. Carse's distinction between finite and\ninfinite games, we leverage recent advances in generative AI to create\nUnbounded: a game of character life simulation that is fully encapsulated in\ngenerative models. Specifically, Unbounded draws inspiration from sandbox life\nsimulations and allows you to interact with your autonomous virtual character\nin a virtual world by feeding, playing with and guiding it - with open-ended\nmechanics generated by an LLM, some of which can be emergent. In order to\ndevelop Unbounded, we propose technical innovations in both the LLM and visual\ngeneration domains. Specifically, we present: (1) a specialized, distilled\nlarge language model (LLM) that dynamically generates game mechanics,\nnarratives, and character interactions in real-time, and (2) a new dynamic\nregional image prompt Adapter (IP-Adapter) for vision models that ensures\nconsistent yet flexible visual generation of a character across multiple\nenvironments. We evaluate our system through both qualitative and quantitative\nanalysis, showing significant improvements in character life simulation, user\ninstruction following, narrative coherence, and visual consistency for both\ncharacters and the environments compared to traditional related approaches.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce the concept of a generative infinite game, a video game that\ntranscends the traditional boundaries of finite, hard-coded systems by using\ngenerative models. Inspired by James P. Carse's distinction between finite and\ninfinite games, we leverage recent advances in generative AI to create\nUnbounded: a game of character life simulation that is fully encapsulated in\ngenerative models. Specifically, Unbounded draws inspiration from sandbox life\nsimulations and allows you to interact with your autonomous virtual character\nin a virtual world by feeding, playing with and guiding it - with open-ended\nmechanics generated by an LLM, some of which can be emergent. In order to\ndevelop Unbounded, we propose technical innovations in both the LLM and visual\ngeneration domains. Specifically, we present: (1) a specialized, distilled\nlarge language model (LLM) that dynamically generates game mechanics,\nnarratives, and character interactions in real-time, and (2) a new dynamic\nregional image prompt Adapter (IP-Adapter) for vision models that ensures\nconsistent yet flexible visual generation of a character across multiple\nenvironments. We evaluate our system through both qualitative and quantitative\nanalysis, showing significant improvements in character life simulation, user\ninstruction following, narrative coherence, and visual consistency for both\ncharacters and the environments compared to traditional related approaches."
                },
                "authors": [
                    {
                        "name": "Jialu Li"
                    },
                    {
                        "name": "Yuanzhen Li"
                    },
                    {
                        "name": "Neal Wadhwa"
                    },
                    {
                        "name": "Yael Pritch"
                    },
                    {
                        "name": "David E. Jacobs"
                    },
                    {
                        "name": "Michael Rubinstein"
                    },
                    {
                        "name": "Mohit Bansal"
                    },
                    {
                        "name": "Nataniel Ruiz"
                    }
                ],
                "author_detail": {
                    "name": "Nataniel Ruiz"
                },
                "author": "Nataniel Ruiz",
                "arxiv_comment": "Project page: https://generative-infinite-game.github.io/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.18975v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.18975v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.GR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.23138v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.23138v1",
                "updated": "2024-10-30T15:53:51Z",
                "updated_parsed": [
                    2024,
                    10,
                    30,
                    15,
                    53,
                    51,
                    2,
                    304,
                    0
                ],
                "published": "2024-10-30T15:53:51Z",
                "published_parsed": [
                    2024,
                    10,
                    30,
                    15,
                    53,
                    51,
                    2,
                    304,
                    0
                ],
                "title": "Simulation and Data Assimilation in an Idealized Coupled\n  Atmosphere-Ocean-Sea Ice Floe Model with Cloud Effects",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Simulation and Data Assimilation in an Idealized Coupled\n  Atmosphere-Ocean-Sea Ice Floe Model with Cloud Effects"
                },
                "summary": "Sea ice plays a crucial role in the climate system, particularly in the\nMarginal Ice Zone (MIZ), a transitional area consisting of fragmented ice\nbetween the open ocean and consolidated pack ice. As the MIZ expands,\nunderstanding its dynamics becomes essential for predicting climate change\nimpacts. However, the role of clouds in these processes has been largely\noverlooked. This paper addresses that gap by developing an idealized coupled\natmosphere-ocean-ice model incorporating cloud and precipitation effects,\ntackling both forward (simulation) and inverse (data assimilation) problems.\nSea ice dynamics are modeled using the discrete element method, which simulates\nfloes driven by atmospheric and oceanic forces. The ocean is represented by a\ntwo-layer quasi-geostrophic (QG) model, capturing mesoscale eddies and\nice-ocean drag. The atmosphere is modeled using a two-layer saturated\nprecipitating QG system, accounting for variable evaporation over sea surfaces\nand ice. Cloud cover affects radiation, influencing ice melting. The idealized\ncoupled modeling framework allows us to study the interactions between\natmosphere, ocean, and sea ice floes. Specifically, it focuses on how clouds\nand precipitation affect energy balance, melting, and freezing processes. It\nalso serves as a testbed for data assimilation, which allows the recovery of\nunobserved floe trajectories and ocean fields in cloud-induced uncertainties.\nNumerical results show that appropriate reduced-order models help improve data\nassimilation efficiency with partial observations, allowing the skillful\ninference of missing floe trajectories and lower atmospheric winds. These\nresults imply the potential of integrating idealized models with data\nassimilation to improve our understanding of Arctic dynamics and predictions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Sea ice plays a crucial role in the climate system, particularly in the\nMarginal Ice Zone (MIZ), a transitional area consisting of fragmented ice\nbetween the open ocean and consolidated pack ice. As the MIZ expands,\nunderstanding its dynamics becomes essential for predicting climate change\nimpacts. However, the role of clouds in these processes has been largely\noverlooked. This paper addresses that gap by developing an idealized coupled\natmosphere-ocean-ice model incorporating cloud and precipitation effects,\ntackling both forward (simulation) and inverse (data assimilation) problems.\nSea ice dynamics are modeled using the discrete element method, which simulates\nfloes driven by atmospheric and oceanic forces. The ocean is represented by a\ntwo-layer quasi-geostrophic (QG) model, capturing mesoscale eddies and\nice-ocean drag. The atmosphere is modeled using a two-layer saturated\nprecipitating QG system, accounting for variable evaporation over sea surfaces\nand ice. Cloud cover affects radiation, influencing ice melting. The idealized\ncoupled modeling framework allows us to study the interactions between\natmosphere, ocean, and sea ice floes. Specifically, it focuses on how clouds\nand precipitation affect energy balance, melting, and freezing processes. It\nalso serves as a testbed for data assimilation, which allows the recovery of\nunobserved floe trajectories and ocean fields in cloud-induced uncertainties.\nNumerical results show that appropriate reduced-order models help improve data\nassimilation efficiency with partial observations, allowing the skillful\ninference of missing floe trajectories and lower atmospheric winds. These\nresults imply the potential of integrating idealized models with data\nassimilation to improve our understanding of Arctic dynamics and predictions."
                },
                "authors": [
                    {
                        "name": "Changhong Mou"
                    },
                    {
                        "name": "Samuel N. Stechmann"
                    },
                    {
                        "name": "Nan Chen"
                    }
                ],
                "author_detail": {
                    "name": "Nan Chen"
                },
                "author": "Nan Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.23138v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.23138v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.geo-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.geo-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "physics.comp-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.03656v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.03656v3",
                "updated": "2024-10-30T15:51:06Z",
                "updated_parsed": [
                    2024,
                    10,
                    30,
                    15,
                    51,
                    6,
                    2,
                    304,
                    0
                ],
                "published": "2024-07-04T05:54:19Z",
                "published_parsed": [
                    2024,
                    7,
                    4,
                    5,
                    54,
                    19,
                    3,
                    186,
                    0
                ],
                "title": "WildDESED: An LLM-Powered Dataset for Wild Domestic Environment Sound\n  Event Detection System",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "WildDESED: An LLM-Powered Dataset for Wild Domestic Environment Sound\n  Event Detection System"
                },
                "summary": "This work aims to advance sound event detection (SED) research by presenting\na new large language model (LLM)-powered dataset namely wild domestic\nenvironment sound event detection (WildDESED). It is crafted as an extension to\nthe original DESED dataset to reflect diverse acoustic variability and complex\nnoises in home settings. We leveraged LLMs to generate eight different domestic\nscenarios based on target sound categories of the DESED dataset. Then we\nenriched the scenarios with a carefully tailored mixture of noises selected\nfrom AudioSet and ensured no overlap with target sound. We consider widely\npopular convolutional neural recurrent network to study WildDESED dataset,\nwhich depicts its challenging nature. We then apply curriculum learning by\ngradually increasing noise complexity to enhance the model's generalization\ncapabilities across various noise levels. Our results with this approach show\nimprovements within the noisy environment, validating the effectiveness on the\nWildDESED dataset promoting noise-robust SED advancements.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This work aims to advance sound event detection (SED) research by presenting\na new large language model (LLM)-powered dataset namely wild domestic\nenvironment sound event detection (WildDESED). It is crafted as an extension to\nthe original DESED dataset to reflect diverse acoustic variability and complex\nnoises in home settings. We leveraged LLMs to generate eight different domestic\nscenarios based on target sound categories of the DESED dataset. Then we\nenriched the scenarios with a carefully tailored mixture of noises selected\nfrom AudioSet and ensured no overlap with target sound. We consider widely\npopular convolutional neural recurrent network to study WildDESED dataset,\nwhich depicts its challenging nature. We then apply curriculum learning by\ngradually increasing noise complexity to enhance the model's generalization\ncapabilities across various noise levels. Our results with this approach show\nimprovements within the noisy environment, validating the effectiveness on the\nWildDESED dataset promoting noise-robust SED advancements."
                },
                "authors": [
                    {
                        "name": "Yang Xiao"
                    },
                    {
                        "name": "Rohan Kumar Das"
                    }
                ],
                "author_detail": {
                    "name": "Rohan Kumar Das"
                },
                "author": "Rohan Kumar Das",
                "arxiv_comment": "DCASE WS 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.03656v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.03656v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.AS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.23136v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.23136v1",
                "updated": "2024-10-30T15:48:36Z",
                "updated_parsed": [
                    2024,
                    10,
                    30,
                    15,
                    48,
                    36,
                    2,
                    304,
                    0
                ],
                "published": "2024-10-30T15:48:36Z",
                "published_parsed": [
                    2024,
                    10,
                    30,
                    15,
                    48,
                    36,
                    2,
                    304,
                    0
                ],
                "title": "Real-Time Personalization for LLM-based Recommendation with Customized\n  In-Context Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Real-Time Personalization for LLM-based Recommendation with Customized\n  In-Context Learning"
                },
                "summary": "Frequently updating Large Language Model (LLM)-based recommender systems to\nadapt to new user interests -- as done for traditional ones -- is impractical\ndue to high training costs, even with acceleration methods. This work explores\nadapting to dynamic user interests without any model updates by leveraging\nIn-Context Learning (ICL), which allows LLMs to learn new tasks from few-shot\nexamples provided in the input. Using new-interest examples as the ICL few-shot\nexamples, LLMs may learn real-time interest directly, avoiding the need for\nmodel updates. However, existing LLM-based recommenders often lose the\nin-context learning ability during recommendation tuning, while the original\nLLM's in-context learning lacks recommendation-specific focus. To address this,\nwe propose RecICL, which customizes recommendation-specific in-context learning\nfor real-time recommendations. RecICL organizes training examples in an\nin-context learning format, ensuring that in-context learning ability is\npreserved and aligned with the recommendation task during tuning.\n  Extensive experiments demonstrate RecICL's effectiveness in delivering\nreal-time recommendations without requiring model updates. Our code is\navailable at https://github.com/ym689/rec_icl.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Frequently updating Large Language Model (LLM)-based recommender systems to\nadapt to new user interests -- as done for traditional ones -- is impractical\ndue to high training costs, even with acceleration methods. This work explores\nadapting to dynamic user interests without any model updates by leveraging\nIn-Context Learning (ICL), which allows LLMs to learn new tasks from few-shot\nexamples provided in the input. Using new-interest examples as the ICL few-shot\nexamples, LLMs may learn real-time interest directly, avoiding the need for\nmodel updates. However, existing LLM-based recommenders often lose the\nin-context learning ability during recommendation tuning, while the original\nLLM's in-context learning lacks recommendation-specific focus. To address this,\nwe propose RecICL, which customizes recommendation-specific in-context learning\nfor real-time recommendations. RecICL organizes training examples in an\nin-context learning format, ensuring that in-context learning ability is\npreserved and aligned with the recommendation task during tuning.\n  Extensive experiments demonstrate RecICL's effectiveness in delivering\nreal-time recommendations without requiring model updates. Our code is\navailable at https://github.com/ym689/rec_icl."
                },
                "authors": [
                    {
                        "name": "Keqin Bao"
                    },
                    {
                        "name": "Ming Yan"
                    },
                    {
                        "name": "Yang Zhang"
                    },
                    {
                        "name": "Jizhi Zhang"
                    },
                    {
                        "name": "Wenjie Wang"
                    },
                    {
                        "name": "Fuli Feng"
                    },
                    {
                        "name": "Xiangnan He"
                    }
                ],
                "author_detail": {
                    "name": "Xiangnan He"
                },
                "author": "Xiangnan He",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.23136v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.23136v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.23123v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.23123v1",
                "updated": "2024-10-30T15:31:54Z",
                "updated_parsed": [
                    2024,
                    10,
                    30,
                    15,
                    31,
                    54,
                    2,
                    304,
                    0
                ],
                "published": "2024-10-30T15:31:54Z",
                "published_parsed": [
                    2024,
                    10,
                    30,
                    15,
                    31,
                    54,
                    2,
                    304,
                    0
                ],
                "title": "On Memorization of Large Language Models in Logical Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "On Memorization of Large Language Models in Logical Reasoning"
                },
                "summary": "Large language models (LLMs) achieve good performance on challenging\nreasoning benchmarks, yet could also make basic reasoning mistakes. This\ncontrasting behavior is puzzling when it comes to understanding the mechanisms\nbehind LLMs' reasoning capabilities. One hypothesis is that the increasingly\nhigh and nearly saturated performance on common reasoning benchmarks could be\ndue to the memorization of similar problems. In this paper, we systematically\ninvestigate this hypothesis with a quantitative measurement of memorization in\nreasoning tasks, using a dynamically generated logical reasoning benchmark\nbased on Knights and Knaves (K&K) puzzles. We found that LLMs could interpolate\nthe training puzzles (achieving near-perfect accuracy) after fine-tuning, yet\nfail when those puzzles are slightly perturbed, suggesting that the models\nheavily rely on memorization to solve those training puzzles. On the other\nhand, we show that while fine-tuning leads to heavy memorization, it also\nconsistently improves generalization performance. In-depth analyses with\nperturbation tests, cross difficulty-level transferability, probing model\ninternals, and fine-tuning with wrong answers suggest that the LLMs learn to\nreason on K&K puzzles despite training data memorization. This phenomenon\nindicates that LLMs exhibit a complex interplay between memorization and\ngenuine reasoning abilities. Finally, our analysis with per-sample memorization\nscore sheds light on how LLMs switch between reasoning and memorization in\nsolving logical puzzles. Our code and data are available at\nhttps://memkklogic.github.io.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) achieve good performance on challenging\nreasoning benchmarks, yet could also make basic reasoning mistakes. This\ncontrasting behavior is puzzling when it comes to understanding the mechanisms\nbehind LLMs' reasoning capabilities. One hypothesis is that the increasingly\nhigh and nearly saturated performance on common reasoning benchmarks could be\ndue to the memorization of similar problems. In this paper, we systematically\ninvestigate this hypothesis with a quantitative measurement of memorization in\nreasoning tasks, using a dynamically generated logical reasoning benchmark\nbased on Knights and Knaves (K&K) puzzles. We found that LLMs could interpolate\nthe training puzzles (achieving near-perfect accuracy) after fine-tuning, yet\nfail when those puzzles are slightly perturbed, suggesting that the models\nheavily rely on memorization to solve those training puzzles. On the other\nhand, we show that while fine-tuning leads to heavy memorization, it also\nconsistently improves generalization performance. In-depth analyses with\nperturbation tests, cross difficulty-level transferability, probing model\ninternals, and fine-tuning with wrong answers suggest that the LLMs learn to\nreason on K&K puzzles despite training data memorization. This phenomenon\nindicates that LLMs exhibit a complex interplay between memorization and\ngenuine reasoning abilities. Finally, our analysis with per-sample memorization\nscore sheds light on how LLMs switch between reasoning and memorization in\nsolving logical puzzles. Our code and data are available at\nhttps://memkklogic.github.io."
                },
                "authors": [
                    {
                        "name": "Chulin Xie"
                    },
                    {
                        "name": "Yangsibo Huang"
                    },
                    {
                        "name": "Chiyuan Zhang"
                    },
                    {
                        "name": "Da Yu"
                    },
                    {
                        "name": "Xinyun Chen"
                    },
                    {
                        "name": "Bill Yuchen Lin"
                    },
                    {
                        "name": "Bo Li"
                    },
                    {
                        "name": "Badih Ghazi"
                    },
                    {
                        "name": "Ravi Kumar"
                    }
                ],
                "author_detail": {
                    "name": "Ravi Kumar"
                },
                "author": "Ravi Kumar",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.23123v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.23123v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.18952v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.18952v2",
                "updated": "2024-10-30T15:28:02Z",
                "updated_parsed": [
                    2024,
                    10,
                    30,
                    15,
                    28,
                    2,
                    2,
                    304,
                    0
                ],
                "published": "2024-10-24T17:52:31Z",
                "published_parsed": [
                    2024,
                    10,
                    24,
                    17,
                    52,
                    31,
                    3,
                    298,
                    0
                ],
                "title": "Dynamic Vocabulary Pruning in Early-Exit LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Dynamic Vocabulary Pruning in Early-Exit LLMs"
                },
                "summary": "Increasing the size of large language models (LLMs) has been shown to lead to\nbetter performance. However, this comes at the cost of slower and more\nexpensive inference. Early-exiting is a promising approach for improving the\nefficiency of LLM inference by enabling next token prediction at intermediate\nlayers. Yet, the large vocabulary size in modern LLMs makes the confidence\nestimation required for exit decisions computationally expensive, diminishing\nthe efficiency gains. To address this, we propose dynamically pruning the\nvocabulary at test time for each token. Specifically, the vocabulary is pruned\nat one of the initial layers, and the smaller vocabulary is then used\nthroughout the rest of the forward pass. Our experiments demonstrate that such\npost-hoc dynamic vocabulary pruning improves the efficiency of confidence\nestimation in early-exit LLMs while maintaining competitive performance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Increasing the size of large language models (LLMs) has been shown to lead to\nbetter performance. However, this comes at the cost of slower and more\nexpensive inference. Early-exiting is a promising approach for improving the\nefficiency of LLM inference by enabling next token prediction at intermediate\nlayers. Yet, the large vocabulary size in modern LLMs makes the confidence\nestimation required for exit decisions computationally expensive, diminishing\nthe efficiency gains. To address this, we propose dynamically pruning the\nvocabulary at test time for each token. Specifically, the vocabulary is pruned\nat one of the initial layers, and the smaller vocabulary is then used\nthroughout the rest of the forward pass. Our experiments demonstrate that such\npost-hoc dynamic vocabulary pruning improves the efficiency of confidence\nestimation in early-exit LLMs while maintaining competitive performance."
                },
                "authors": [
                    {
                        "name": "Jort Vincenti"
                    },
                    {
                        "name": "Karim Abdel Sadek"
                    },
                    {
                        "name": "Joan Velja"
                    },
                    {
                        "name": "Matteo Nulli"
                    },
                    {
                        "name": "Metod Jazbec"
                    }
                ],
                "author_detail": {
                    "name": "Metod Jazbec"
                },
                "author": "Metod Jazbec",
                "arxiv_journal_ref": "NeurIPS 2024 ENLSP Workshop",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.18952v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.18952v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.23118v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.23118v1",
                "updated": "2024-10-30T15:27:55Z",
                "updated_parsed": [
                    2024,
                    10,
                    30,
                    15,
                    27,
                    55,
                    2,
                    304,
                    0
                ],
                "published": "2024-10-30T15:27:55Z",
                "published_parsed": [
                    2024,
                    10,
                    30,
                    15,
                    27,
                    55,
                    2,
                    304,
                    0
                ],
                "title": "Teaching a Language Model to Distinguish Between Similar Details using a\n  Small Adversarial Training Set",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Teaching a Language Model to Distinguish Between Similar Details using a\n  Small Adversarial Training Set"
                },
                "summary": "Language models can achieve high accuracy on natural language tasks such as\nNLI, but performance suffers on manually created adversarial examples. We\ninvestigate the performance of a language model trained on the Stanford Natural\nLanguage Inference (SNLI) corpus on a manually created adversarial test set. We\nthen improve the model's performance by fine tuning the model on a small,\nmanually created adversarial training set, designed to help the language model\nto learn to differentiate between similar words and phrases in the data. We\nshow an increase in accuracy on the adversarial test set (+ 13%) while still\nmaintaining good performance on the original NLI task. We also show an increase\nin accuracy from 91.2% to 92.9% on the most similar contradictions in the SNLI\ntest set (as judged by cosine similarity).",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Language models can achieve high accuracy on natural language tasks such as\nNLI, but performance suffers on manually created adversarial examples. We\ninvestigate the performance of a language model trained on the Stanford Natural\nLanguage Inference (SNLI) corpus on a manually created adversarial test set. We\nthen improve the model's performance by fine tuning the model on a small,\nmanually created adversarial training set, designed to help the language model\nto learn to differentiate between similar words and phrases in the data. We\nshow an increase in accuracy on the adversarial test set (+ 13%) while still\nmaintaining good performance on the original NLI task. We also show an increase\nin accuracy from 91.2% to 92.9% on the most similar contradictions in the SNLI\ntest set (as judged by cosine similarity)."
                },
                "authors": [
                    {
                        "name": "Chris Achard"
                    }
                ],
                "author_detail": {
                    "name": "Chris Achard"
                },
                "author": "Chris Achard",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.23118v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.23118v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.23111v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.23111v1",
                "updated": "2024-10-30T15:23:44Z",
                "updated_parsed": [
                    2024,
                    10,
                    30,
                    15,
                    23,
                    44,
                    2,
                    304,
                    0
                ],
                "published": "2024-10-30T15:23:44Z",
                "published_parsed": [
                    2024,
                    10,
                    30,
                    15,
                    23,
                    44,
                    2,
                    304,
                    0
                ],
                "title": "Why Gradient Subspace? Identifying and Mitigating LoRA's Bottlenecks in\n  Federated Fine-Tuning of Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Why Gradient Subspace? Identifying and Mitigating LoRA's Bottlenecks in\n  Federated Fine-Tuning of Large Language Models"
                },
                "summary": "Large Language Models (LLMs) have demonstrated remarkable capabilities across\nvarious domains, particularly in task generalization for both text and vision\ndata. While fine-tuning these models can significantly enhance their\nperformance on specific downstream tasks, it often requires high-quality data\nthat cannot be shared due to privacy concerns. Federated Learning (FL) offers a\npromising solution for collaborative training without direct data sharing.\nHowever, many parameter-efficient fine-tuning strategies for LLMs in FL,\nparticularly those based on Low-Rank Adaptation (LoRA), face limitations. In\nthis paper, we critically analyze the convergence and performance guarantees of\npopular FL frameworks utilizing LoRA, highlighting its suboptimal nature due to\nconstrained subspace learning of low-rank matrices. This limitation hinders\neffective fine-tuning of LLMs in federated settings. Through rigorous\nanalytical and empirical evaluations, we demonstrate that direct weight\naveraging outperforms LoRA-based strategies, leading to superior performance\nfor fine-tuned models. Our comprehensive comparison exposes inefficiencies in\nLoRA approaches and underscores the advantages of full-rank weight aggregation.\nWe extend our analysis to low-rank gradient-based optimizers, such as GaLore,\nused during local training steps. Our findings show that GaLore is a more\neffective alternative, outperforming federated LoRA methods like FlexLoRA and\nFFA-LoRA across both text and image modalities. While privacy remains paramount\nin FL discourse, our focus is on assessing performance outcomes of federated\nfine-tuned models and evaluating various FL frameworks from both theoretical\nand empirical perspectives. Our findings advocate reassessing the reliance on\nLoRA within FL contexts, paving the way for more efficient training\nmethodologies.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have demonstrated remarkable capabilities across\nvarious domains, particularly in task generalization for both text and vision\ndata. While fine-tuning these models can significantly enhance their\nperformance on specific downstream tasks, it often requires high-quality data\nthat cannot be shared due to privacy concerns. Federated Learning (FL) offers a\npromising solution for collaborative training without direct data sharing.\nHowever, many parameter-efficient fine-tuning strategies for LLMs in FL,\nparticularly those based on Low-Rank Adaptation (LoRA), face limitations. In\nthis paper, we critically analyze the convergence and performance guarantees of\npopular FL frameworks utilizing LoRA, highlighting its suboptimal nature due to\nconstrained subspace learning of low-rank matrices. This limitation hinders\neffective fine-tuning of LLMs in federated settings. Through rigorous\nanalytical and empirical evaluations, we demonstrate that direct weight\naveraging outperforms LoRA-based strategies, leading to superior performance\nfor fine-tuned models. Our comprehensive comparison exposes inefficiencies in\nLoRA approaches and underscores the advantages of full-rank weight aggregation.\nWe extend our analysis to low-rank gradient-based optimizers, such as GaLore,\nused during local training steps. Our findings show that GaLore is a more\neffective alternative, outperforming federated LoRA methods like FlexLoRA and\nFFA-LoRA across both text and image modalities. While privacy remains paramount\nin FL discourse, our focus is on assessing performance outcomes of federated\nfine-tuned models and evaluating various FL frameworks from both theoretical\nand empirical perspectives. Our findings advocate reassessing the reliance on\nLoRA within FL contexts, paving the way for more efficient training\nmethodologies."
                },
                "authors": [
                    {
                        "name": "Navyansh Mahla"
                    },
                    {
                        "name": "Ganesh Ramakrishnan"
                    }
                ],
                "author_detail": {
                    "name": "Ganesh Ramakrishnan"
                },
                "author": "Ganesh Ramakrishnan",
                "arxiv_comment": "24 pages, 10 figures, pre-print",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.23111v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.23111v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2310.08975v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2310.08975v3",
                "updated": "2024-10-30T15:22:12Z",
                "updated_parsed": [
                    2024,
                    10,
                    30,
                    15,
                    22,
                    12,
                    2,
                    304,
                    0
                ],
                "published": "2023-10-13T09:45:14Z",
                "published_parsed": [
                    2023,
                    10,
                    13,
                    9,
                    45,
                    14,
                    4,
                    286,
                    0
                ],
                "title": "ChatKBQA: A Generate-then-Retrieve Framework for Knowledge Base Question\n  Answering with Fine-tuned Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ChatKBQA: A Generate-then-Retrieve Framework for Knowledge Base Question\n  Answering with Fine-tuned Large Language Models"
                },
                "summary": "Knowledge Base Question Answering (KBQA) aims to answer natural language\nquestions over large-scale knowledge bases (KBs), which can be summarized into\ntwo crucial steps: knowledge retrieval and semantic parsing. However, three\ncore challenges remain: inefficient knowledge retrieval, mistakes of retrieval\nadversely impacting semantic parsing, and the complexity of previous KBQA\nmethods. To tackle these challenges, we introduce ChatKBQA, a novel and simple\ngenerate-then-retrieve KBQA framework, which proposes first generating the\nlogical form with fine-tuned LLMs, then retrieving and replacing entities and\nrelations with an unsupervised retrieval method, to improve both generation and\nretrieval more directly. Experimental results show that ChatKBQA achieves new\nstate-of-the-art performance on standard KBQA datasets, WebQSP, and CWQ. This\nwork can also be regarded as a new paradigm for combining LLMs with knowledge\ngraphs (KGs) for interpretable and knowledge-required question answering. Our\ncode is publicly available.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Knowledge Base Question Answering (KBQA) aims to answer natural language\nquestions over large-scale knowledge bases (KBs), which can be summarized into\ntwo crucial steps: knowledge retrieval and semantic parsing. However, three\ncore challenges remain: inefficient knowledge retrieval, mistakes of retrieval\nadversely impacting semantic parsing, and the complexity of previous KBQA\nmethods. To tackle these challenges, we introduce ChatKBQA, a novel and simple\ngenerate-then-retrieve KBQA framework, which proposes first generating the\nlogical form with fine-tuned LLMs, then retrieving and replacing entities and\nrelations with an unsupervised retrieval method, to improve both generation and\nretrieval more directly. Experimental results show that ChatKBQA achieves new\nstate-of-the-art performance on standard KBQA datasets, WebQSP, and CWQ. This\nwork can also be regarded as a new paradigm for combining LLMs with knowledge\ngraphs (KGs) for interpretable and knowledge-required question answering. Our\ncode is publicly available."
                },
                "authors": [
                    {
                        "name": "Haoran Luo"
                    },
                    {
                        "name": "Haihong E"
                    },
                    {
                        "name": "Zichen Tang"
                    },
                    {
                        "name": "Shiyao Peng"
                    },
                    {
                        "name": "Yikai Guo"
                    },
                    {
                        "name": "Wentai Zhang"
                    },
                    {
                        "name": "Chenghao Ma"
                    },
                    {
                        "name": "Guanting Dong"
                    },
                    {
                        "name": "Meina Song"
                    },
                    {
                        "name": "Wei Lin"
                    },
                    {
                        "name": "Yifan Zhu"
                    },
                    {
                        "name": "Luu Anh Tuan"
                    }
                ],
                "author_detail": {
                    "name": "Luu Anh Tuan"
                },
                "author": "Luu Anh Tuan",
                "arxiv_doi": "10.18653/v1/2024.findings-acl.122",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.18653/v1/2024.findings-acl.122",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2310.08975v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2310.08975v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "Accepted by Findings of ACL 2024",
                "arxiv_journal_ref": "ACL 2024",
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.23099v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.23099v1",
                "updated": "2024-10-30T15:11:58Z",
                "updated_parsed": [
                    2024,
                    10,
                    30,
                    15,
                    11,
                    58,
                    2,
                    304,
                    0
                ],
                "published": "2024-10-30T15:11:58Z",
                "published_parsed": [
                    2024,
                    10,
                    30,
                    15,
                    11,
                    58,
                    2,
                    304,
                    0
                ],
                "title": "Comparative Analysis of Demonstration Selection Algorithms for LLM\n  In-Context Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Comparative Analysis of Demonstration Selection Algorithms for LLM\n  In-Context Learning"
                },
                "summary": "In-context learning can help Large Language Models (LLMs) to adapt new tasks\nwithout additional training. However, this performance heavily depends on the\nquality of the demonstrations, driving research into effective demonstration\nselection algorithms to optimize this process. These algorithms assist users in\nselecting the best $k$ input-label pairs (demonstration examples) based on a\ngiven test input, enabling LLMs to in-context learn the relationship between\nthe provided examples and the test inputs. Despite all the proposed\ndemonstration selection algorithms, their efficiency and effectiveness remain\nunclear. This lack of clarity make it difficult to apply these algorithms in\nreal-world scenarios and poses challenges for future research aimed at\ndeveloping improved methods. This paper revisits six proposed algorithms,\nevaluating them on five datasets from both efficiency and effectiveness\nperspectives. Our experiments reveal significant variations in algorithm\nperformance across different tasks, with some methods struggling to outperform\nrandom selection in certain scenarios. We also find that increasing the number\nof demonstrations does not always lead to better performance, and that there\nare often trade-offs between accuracy and computational efficiency. Our code is\navailable at https://github.com/Tizzzzy/Demonstration_Selection_Overview.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In-context learning can help Large Language Models (LLMs) to adapt new tasks\nwithout additional training. However, this performance heavily depends on the\nquality of the demonstrations, driving research into effective demonstration\nselection algorithms to optimize this process. These algorithms assist users in\nselecting the best $k$ input-label pairs (demonstration examples) based on a\ngiven test input, enabling LLMs to in-context learn the relationship between\nthe provided examples and the test inputs. Despite all the proposed\ndemonstration selection algorithms, their efficiency and effectiveness remain\nunclear. This lack of clarity make it difficult to apply these algorithms in\nreal-world scenarios and poses challenges for future research aimed at\ndeveloping improved methods. This paper revisits six proposed algorithms,\nevaluating them on five datasets from both efficiency and effectiveness\nperspectives. Our experiments reveal significant variations in algorithm\nperformance across different tasks, with some methods struggling to outperform\nrandom selection in certain scenarios. We also find that increasing the number\nof demonstrations does not always lead to better performance, and that there\nare often trade-offs between accuracy and computational efficiency. Our code is\navailable at https://github.com/Tizzzzy/Demonstration_Selection_Overview."
                },
                "authors": [
                    {
                        "name": "Dong Shu"
                    },
                    {
                        "name": "Mengnan Du"
                    }
                ],
                "author_detail": {
                    "name": "Mengnan Du"
                },
                "author": "Mengnan Du",
                "arxiv_comment": "6 pages, 4 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.23099v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.23099v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.23090v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.23090v1",
                "updated": "2024-10-30T15:06:32Z",
                "updated_parsed": [
                    2024,
                    10,
                    30,
                    15,
                    6,
                    32,
                    2,
                    304,
                    0
                ],
                "published": "2024-10-30T15:06:32Z",
                "published_parsed": [
                    2024,
                    10,
                    30,
                    15,
                    6,
                    32,
                    2,
                    304,
                    0
                ],
                "title": "CORAL: Benchmarking Multi-turn Conversational Retrieval-Augmentation\n  Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CORAL: Benchmarking Multi-turn Conversational Retrieval-Augmentation\n  Generation"
                },
                "summary": "Retrieval-Augmented Generation (RAG) has become a powerful paradigm for\nenhancing large language models (LLMs) through external knowledge retrieval.\nDespite its widespread attention, existing academic research predominantly\nfocuses on single-turn RAG, leaving a significant gap in addressing the\ncomplexities of multi-turn conversations found in real-world applications. To\nbridge this gap, we introduce CORAL, a large-scale benchmark designed to assess\nRAG systems in realistic multi-turn conversational settings. CORAL includes\ndiverse information-seeking conversations automatically derived from Wikipedia\nand tackles key challenges such as open-domain coverage, knowledge intensity,\nfree-form responses, and topic shifts. It supports three core tasks of\nconversational RAG: passage retrieval, response generation, and citation\nlabeling. We propose a unified framework to standardize various conversational\nRAG methods and conduct a comprehensive evaluation of these methods on CORAL,\ndemonstrating substantial opportunities for improving existing approaches.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Retrieval-Augmented Generation (RAG) has become a powerful paradigm for\nenhancing large language models (LLMs) through external knowledge retrieval.\nDespite its widespread attention, existing academic research predominantly\nfocuses on single-turn RAG, leaving a significant gap in addressing the\ncomplexities of multi-turn conversations found in real-world applications. To\nbridge this gap, we introduce CORAL, a large-scale benchmark designed to assess\nRAG systems in realistic multi-turn conversational settings. CORAL includes\ndiverse information-seeking conversations automatically derived from Wikipedia\nand tackles key challenges such as open-domain coverage, knowledge intensity,\nfree-form responses, and topic shifts. It supports three core tasks of\nconversational RAG: passage retrieval, response generation, and citation\nlabeling. We propose a unified framework to standardize various conversational\nRAG methods and conduct a comprehensive evaluation of these methods on CORAL,\ndemonstrating substantial opportunities for improving existing approaches."
                },
                "authors": [
                    {
                        "name": "Yiruo Cheng"
                    },
                    {
                        "name": "Kelong Mao"
                    },
                    {
                        "name": "Ziliang Zhao"
                    },
                    {
                        "name": "Guanting Dong"
                    },
                    {
                        "name": "Hongjin Qian"
                    },
                    {
                        "name": "Yongkang Wu"
                    },
                    {
                        "name": "Tetsuya Sakai"
                    },
                    {
                        "name": "Ji-Rong Wen"
                    },
                    {
                        "name": "Zhicheng Dou"
                    }
                ],
                "author_detail": {
                    "name": "Zhicheng Dou"
                },
                "author": "Zhicheng Dou",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.23090v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.23090v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.23089v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.23089v1",
                "updated": "2024-10-30T15:05:17Z",
                "updated_parsed": [
                    2024,
                    10,
                    30,
                    15,
                    5,
                    17,
                    2,
                    304,
                    0
                ],
                "published": "2024-10-30T15:05:17Z",
                "published_parsed": [
                    2024,
                    10,
                    30,
                    15,
                    5,
                    17,
                    2,
                    304,
                    0
                ],
                "title": "PIP-MM: Pre-Integrating Prompt Information into Visual Encoding via\n  Existing MLLM Structures",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PIP-MM: Pre-Integrating Prompt Information into Visual Encoding via\n  Existing MLLM Structures"
                },
                "summary": "The Multimodal Large Language Models (MLLMs) have activated the\ncapabilitiesof Large Language Models (LLMs) in solving visual-language tasks by\nintegratingvisual information. The prevailing approach in existing MLLMs\ninvolvesemploying an image encoder to extract visual features, converting\nthesefeatures into visual tokens via an adapter, and then integrating them with\ntheprompt into the LLM. However, because the process of image encoding\nisprompt-agnostic, the extracted visual features only provide a\ncoarsedescription of the image, impossible to focus on the requirements of\ntheprompt. On one hand, it is easy for image features to lack information\naboutthe prompt-specified objects, resulting in unsatisfactory responses. On\ntheother hand, the visual features contain a large amount of\nirrelevantinformation, which not only increases the burden on memory but also\nworsens thegeneration effectiveness. To address the aforementioned issues, we\npropose\\textbf{PIP-MM}, a framework that\n\\textbf{P}re-\\textbf{I}ntegrates\\textbf{P}rompt information into the visual\nencoding process using existingmodules of MLLMs. Specifically, We utilize the\nfrozen LLM in the MLLM tovectorize the input prompt, which summarizes the\nrequirements of the prompt.Then, we input the prompt vector into our trained\nMulti-Layer Perceptron (MLP)to align with the visual input requirements, and\nsubsequently replace the classembedding in the image encoder. Since our model\nonly requires adding atrainable MLP, it can be applied to any MLLM. To validate\nthe effectiveness ofPIP-MM, we conducted experiments on multiple benchmarks.\nAutomated evaluationmetrics and manual assessments demonstrate the strong\nperformance of PIP-MM.Particularly noteworthy is that our model maintains\nexcellent generationresults even when half of the visual tokens are reduced.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Multimodal Large Language Models (MLLMs) have activated the\ncapabilitiesof Large Language Models (LLMs) in solving visual-language tasks by\nintegratingvisual information. The prevailing approach in existing MLLMs\ninvolvesemploying an image encoder to extract visual features, converting\nthesefeatures into visual tokens via an adapter, and then integrating them with\ntheprompt into the LLM. However, because the process of image encoding\nisprompt-agnostic, the extracted visual features only provide a\ncoarsedescription of the image, impossible to focus on the requirements of\ntheprompt. On one hand, it is easy for image features to lack information\naboutthe prompt-specified objects, resulting in unsatisfactory responses. On\ntheother hand, the visual features contain a large amount of\nirrelevantinformation, which not only increases the burden on memory but also\nworsens thegeneration effectiveness. To address the aforementioned issues, we\npropose\\textbf{PIP-MM}, a framework that\n\\textbf{P}re-\\textbf{I}ntegrates\\textbf{P}rompt information into the visual\nencoding process using existingmodules of MLLMs. Specifically, We utilize the\nfrozen LLM in the MLLM tovectorize the input prompt, which summarizes the\nrequirements of the prompt.Then, we input the prompt vector into our trained\nMulti-Layer Perceptron (MLP)to align with the visual input requirements, and\nsubsequently replace the classembedding in the image encoder. Since our model\nonly requires adding atrainable MLP, it can be applied to any MLLM. To validate\nthe effectiveness ofPIP-MM, we conducted experiments on multiple benchmarks.\nAutomated evaluationmetrics and manual assessments demonstrate the strong\nperformance of PIP-MM.Particularly noteworthy is that our model maintains\nexcellent generationresults even when half of the visual tokens are reduced."
                },
                "authors": [
                    {
                        "name": "Tianxiang Wu"
                    },
                    {
                        "name": "Minxin Nie"
                    },
                    {
                        "name": "Ziqiang Cao"
                    }
                ],
                "author_detail": {
                    "name": "Ziqiang Cao"
                },
                "author": "Ziqiang Cao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.23089v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.23089v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.23082v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.23082v1",
                "updated": "2024-10-30T14:55:13Z",
                "updated_parsed": [
                    2024,
                    10,
                    30,
                    14,
                    55,
                    13,
                    2,
                    304,
                    0
                ],
                "published": "2024-10-30T14:55:13Z",
                "published_parsed": [
                    2024,
                    10,
                    30,
                    14,
                    55,
                    13,
                    2,
                    304,
                    0
                ],
                "title": "An Event-Based Digital Compute-In-Memory Accelerator with Flexible\n  Operand Resolution and Layer-Wise Weight/Output Stationarity",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "An Event-Based Digital Compute-In-Memory Accelerator with Flexible\n  Operand Resolution and Layer-Wise Weight/Output Stationarity"
                },
                "summary": "Compute-in-memory (CIM) accelerators for spiking neural networks (SNNs) are\npromising solutions to enable $\\mu$s-level inference latency and ultra-low\nenergy in edge vision applications. Yet, their current lack of flexibility at\nboth the circuit and system levels prevents their deployment in a wide range of\nreal-life scenarios. In this work, we propose a novel digital CIM macro that\nsupports arbitrary operand resolution and shape, with a unified CIM storage for\nweights and membrane potentials. These circuit-level techniques enable a hybrid\nweight- and output-stationary dataflow at the system level to maximize operand\nreuse, thereby minimizing costly on- and off-chip data movements during the SNN\nexecution. Measurement results of a fabricated FlexSpIM prototype in 40-nm CMOS\ndemonstrate a 2$\\times$ increase in bit-normalized energy efficiency compared\nto prior fixed-precision digital CIM-SNNs, while providing resolution\nreconfiguration with bitwise granularity. Our approach can save up to 90%\nenergy in large-scale systems, while reaching a state-of-the-art classification\naccuracy of 95.8% on the IBM DVS gesture dataset.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Compute-in-memory (CIM) accelerators for spiking neural networks (SNNs) are\npromising solutions to enable $\\mu$s-level inference latency and ultra-low\nenergy in edge vision applications. Yet, their current lack of flexibility at\nboth the circuit and system levels prevents their deployment in a wide range of\nreal-life scenarios. In this work, we propose a novel digital CIM macro that\nsupports arbitrary operand resolution and shape, with a unified CIM storage for\nweights and membrane potentials. These circuit-level techniques enable a hybrid\nweight- and output-stationary dataflow at the system level to maximize operand\nreuse, thereby minimizing costly on- and off-chip data movements during the SNN\nexecution. Measurement results of a fabricated FlexSpIM prototype in 40-nm CMOS\ndemonstrate a 2$\\times$ increase in bit-normalized energy efficiency compared\nto prior fixed-precision digital CIM-SNNs, while providing resolution\nreconfiguration with bitwise granularity. Our approach can save up to 90%\nenergy in large-scale systems, while reaching a state-of-the-art classification\naccuracy of 95.8% on the IBM DVS gesture dataset."
                },
                "authors": [
                    {
                        "name": "Nicolas Chauvaux"
                    },
                    {
                        "name": "Adrian Kneip"
                    },
                    {
                        "name": "Christoph Posch"
                    },
                    {
                        "name": "Kofi Makinwa"
                    },
                    {
                        "name": "Charlotte Frenkel"
                    }
                ],
                "author_detail": {
                    "name": "Charlotte Frenkel"
                },
                "author": "Charlotte Frenkel",
                "arxiv_comment": "5 pages, 7 figures, submitted to IEEE ISCAS 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.23082v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.23082v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "B.2.0; B.3.0; B.6.0; B.7.0; C.3",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.13147v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.13147v4",
                "updated": "2024-10-30T14:54:25Z",
                "updated_parsed": [
                    2024,
                    10,
                    30,
                    14,
                    54,
                    25,
                    2,
                    304,
                    0
                ],
                "published": "2024-10-17T02:04:57Z",
                "published_parsed": [
                    2024,
                    10,
                    17,
                    2,
                    4,
                    57,
                    3,
                    291,
                    0
                ],
                "title": "Utilizing Large Language Models in an iterative paradigm with Domain\n  feedback for Zero-shot Molecule optimization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Utilizing Large Language Models in an iterative paradigm with Domain\n  feedback for Zero-shot Molecule optimization"
                },
                "summary": "Molecule optimization is a critical task in drug discovery to optimize\ndesired properties of a given molecule through chemical modification. Despite\nLarge Language Models (LLMs) holding the potential to efficiently simulate this\ntask by using natural language to direct the optimization, straightforwardly\nutilizing shows limited performance. In this work, we facilitate utilizing LLMs\nin an iterative paradigm by proposing a simple yet highly effective domain\nfeedback provider, namely $\\text{Re}^3$DF. In detail, $\\text{Re}^3$DF harnesses\nan external toolkit, RDKit, to handle the molecule hallucination, if the\nmodified molecule is chemically invalid. Otherwise, its desired properties are\ncomputed and compared to the original one, establishing reliable domain\nfeedback with correct direction and distance towards the objective, followed by\na retrieved example, to explicitly guide the LLM to refine the modified\nmolecule. We conduct experiments across both single- and multi-property\nobjectives with 2 thresholds, where $\\text{Re}^3$DF shows significant\nimprovements. Particularly, for 20 single-property objectives, $\\text{Re}^3$DF\nenhances Hit ratio by 16.95% and 20.76% under loose and strict thresholds,\nrespectively. For 32 multi-property objectives, $\\text{Re}^3$DF enhances Hit\nratio by 6.04% and 5.25%.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Molecule optimization is a critical task in drug discovery to optimize\ndesired properties of a given molecule through chemical modification. Despite\nLarge Language Models (LLMs) holding the potential to efficiently simulate this\ntask by using natural language to direct the optimization, straightforwardly\nutilizing shows limited performance. In this work, we facilitate utilizing LLMs\nin an iterative paradigm by proposing a simple yet highly effective domain\nfeedback provider, namely $\\text{Re}^3$DF. In detail, $\\text{Re}^3$DF harnesses\nan external toolkit, RDKit, to handle the molecule hallucination, if the\nmodified molecule is chemically invalid. Otherwise, its desired properties are\ncomputed and compared to the original one, establishing reliable domain\nfeedback with correct direction and distance towards the objective, followed by\na retrieved example, to explicitly guide the LLM to refine the modified\nmolecule. We conduct experiments across both single- and multi-property\nobjectives with 2 thresholds, where $\\text{Re}^3$DF shows significant\nimprovements. Particularly, for 20 single-property objectives, $\\text{Re}^3$DF\nenhances Hit ratio by 16.95% and 20.76% under loose and strict thresholds,\nrespectively. For 32 multi-property objectives, $\\text{Re}^3$DF enhances Hit\nratio by 6.04% and 5.25%."
                },
                "authors": [
                    {
                        "name": "Khiem Le"
                    },
                    {
                        "name": "Nitesh V. Chawla"
                    }
                ],
                "author_detail": {
                    "name": "Nitesh V. Chawla"
                },
                "author": "Nitesh V. Chawla",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.13147v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.13147v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.23079v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.23079v1",
                "updated": "2024-10-30T14:53:37Z",
                "updated_parsed": [
                    2024,
                    10,
                    30,
                    14,
                    53,
                    37,
                    2,
                    304,
                    0
                ],
                "published": "2024-10-30T14:53:37Z",
                "published_parsed": [
                    2024,
                    10,
                    30,
                    14,
                    53,
                    37,
                    2,
                    304,
                    0
                ],
                "title": "BUZZ: Beehive-structured Sparse KV Cache with Segmented Heavy Hitters\n  for Efficient LLM Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "BUZZ: Beehive-structured Sparse KV Cache with Segmented Heavy Hitters\n  for Efficient LLM Inference"
                },
                "summary": "Large language models (LLMs) are essential in natural language processing but\noften struggle with inference speed and computational efficiency, limiting\nreal-time deployment. The key-value (KV) cache mechanism reduces computational\noverhead in transformer models, but challenges in maintaining contextual\nunderstanding remain. In this paper, we propose BUZZ, a novel KV caching\nalgorithm that leverages structured contextual information to minimize cache\nmemory usage while enhancing inference speed. BUZZ employs a beehive-structured\nsparse cache, incorporating a sliding window to capture recent information and\ndynamically segmenting historical tokens into chunks to prioritize important\ntokens in local neighborhoods. We evaluate BUZZ on four real-world datasets:\nCNN/Daily Mail, XSUM, Wikitext, and 10-QA. Our results demonstrate that BUZZ\n(1) reduces cache memory usage by $\\textbf{2.5}\\times$ in LLM inference while\nmaintaining over 99% accuracy in long-text summarization, and (2) surpasses\nstate-of-the-art performance in multi-document question answering by\n$\\textbf{7.69%}$ under the same memory limit, where full cache methods\nencounter out-of-memory issues. Additionally, BUZZ achieves significant\ninference speedup with a $\\log{n}$ time complexity. The code is available at\nhttps://github.com/JunqiZhao888/buzz-llm.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) are essential in natural language processing but\noften struggle with inference speed and computational efficiency, limiting\nreal-time deployment. The key-value (KV) cache mechanism reduces computational\noverhead in transformer models, but challenges in maintaining contextual\nunderstanding remain. In this paper, we propose BUZZ, a novel KV caching\nalgorithm that leverages structured contextual information to minimize cache\nmemory usage while enhancing inference speed. BUZZ employs a beehive-structured\nsparse cache, incorporating a sliding window to capture recent information and\ndynamically segmenting historical tokens into chunks to prioritize important\ntokens in local neighborhoods. We evaluate BUZZ on four real-world datasets:\nCNN/Daily Mail, XSUM, Wikitext, and 10-QA. Our results demonstrate that BUZZ\n(1) reduces cache memory usage by $\\textbf{2.5}\\times$ in LLM inference while\nmaintaining over 99% accuracy in long-text summarization, and (2) surpasses\nstate-of-the-art performance in multi-document question answering by\n$\\textbf{7.69%}$ under the same memory limit, where full cache methods\nencounter out-of-memory issues. Additionally, BUZZ achieves significant\ninference speedup with a $\\log{n}$ time complexity. The code is available at\nhttps://github.com/JunqiZhao888/buzz-llm."
                },
                "authors": [
                    {
                        "name": "Junqi Zhao"
                    },
                    {
                        "name": "Zhijin Fang"
                    },
                    {
                        "name": "Shu Li"
                    },
                    {
                        "name": "Shaohui Yang"
                    },
                    {
                        "name": "Shichao He"
                    }
                ],
                "author_detail": {
                    "name": "Shichao He"
                },
                "author": "Shichao He",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.23079v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.23079v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.02490v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.02490v2",
                "updated": "2024-10-30T14:53:22Z",
                "updated_parsed": [
                    2024,
                    10,
                    30,
                    14,
                    53,
                    22,
                    2,
                    304,
                    0
                ],
                "published": "2024-07-02T17:59:56Z",
                "published_parsed": [
                    2024,
                    7,
                    2,
                    17,
                    59,
                    56,
                    1,
                    184,
                    0
                ],
                "title": "MInference 1.0: Accelerating Pre-filling for Long-Context LLMs via\n  Dynamic Sparse Attention",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MInference 1.0: Accelerating Pre-filling for Long-Context LLMs via\n  Dynamic Sparse Attention"
                },
                "summary": "The computational challenges of Large Language Model (LLM) inference remain a\nsignificant barrier to their widespread deployment, especially as prompt\nlengths continue to increase. Due to the quadratic complexity of the attention\ncomputation, it takes 30 minutes for an 8B LLM to process a prompt of 1M tokens\n(i.e., the pre-filling stage) on a single A100 GPU. Existing methods for\nspeeding up prefilling often fail to maintain acceptable accuracy or efficiency\nwhen applied to long-context LLMs. To address this gap, we introduce MInference\n(Milliontokens Inference), a sparse calculation method designed to accelerate\npre-filling of long-sequence processing. Specifically, we identify three unique\npatterns in long-context attention matrices-the A-shape, Vertical-Slash, and\nBlock-Sparsethat can be leveraged for efficient sparse computation on GPUs. We\ndetermine the optimal pattern for each attention head offline and dynamically\nbuild sparse indices based on the assigned pattern during inference. With the\npattern and sparse indices, we perform efficient sparse attention calculations\nvia our optimized GPU kernels to significantly reduce the latency in the\npre-filling stage of long-context LLMs. Our proposed technique can be directly\napplied to existing LLMs without any modifications to the pre-training setup or\nadditional fine-tuning. By evaluating on a wide range of downstream tasks,\nincluding InfiniteBench, RULER, PG-19, and Needle In A Haystack, and models\nincluding LLaMA-3-1M, GLM4-1M, Yi-200K, Phi-3-128K, and Qwen2-128K, we\ndemonstrate that MInference effectively reduces inference latency by up to 10x\nfor pre-filling on an A100, while maintaining accuracy. Our code is available\nat https://aka.ms/MInference.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The computational challenges of Large Language Model (LLM) inference remain a\nsignificant barrier to their widespread deployment, especially as prompt\nlengths continue to increase. Due to the quadratic complexity of the attention\ncomputation, it takes 30 minutes for an 8B LLM to process a prompt of 1M tokens\n(i.e., the pre-filling stage) on a single A100 GPU. Existing methods for\nspeeding up prefilling often fail to maintain acceptable accuracy or efficiency\nwhen applied to long-context LLMs. To address this gap, we introduce MInference\n(Milliontokens Inference), a sparse calculation method designed to accelerate\npre-filling of long-sequence processing. Specifically, we identify three unique\npatterns in long-context attention matrices-the A-shape, Vertical-Slash, and\nBlock-Sparsethat can be leveraged for efficient sparse computation on GPUs. We\ndetermine the optimal pattern for each attention head offline and dynamically\nbuild sparse indices based on the assigned pattern during inference. With the\npattern and sparse indices, we perform efficient sparse attention calculations\nvia our optimized GPU kernels to significantly reduce the latency in the\npre-filling stage of long-context LLMs. Our proposed technique can be directly\napplied to existing LLMs without any modifications to the pre-training setup or\nadditional fine-tuning. By evaluating on a wide range of downstream tasks,\nincluding InfiniteBench, RULER, PG-19, and Needle In A Haystack, and models\nincluding LLaMA-3-1M, GLM4-1M, Yi-200K, Phi-3-128K, and Qwen2-128K, we\ndemonstrate that MInference effectively reduces inference latency by up to 10x\nfor pre-filling on an A100, while maintaining accuracy. Our code is available\nat https://aka.ms/MInference."
                },
                "authors": [
                    {
                        "name": "Huiqiang Jiang"
                    },
                    {
                        "name": "Yucheng Li"
                    },
                    {
                        "name": "Chengruidong Zhang"
                    },
                    {
                        "name": "Qianhui Wu"
                    },
                    {
                        "name": "Xufang Luo"
                    },
                    {
                        "name": "Surin Ahn"
                    },
                    {
                        "name": "Zhenhua Han"
                    },
                    {
                        "name": "Amir H. Abdi"
                    },
                    {
                        "name": "Dongsheng Li"
                    },
                    {
                        "name": "Chin-Yew Lin"
                    },
                    {
                        "name": "Yuqing Yang"
                    },
                    {
                        "name": "Lili Qiu"
                    }
                ],
                "author_detail": {
                    "name": "Lili Qiu"
                },
                "author": "Lili Qiu",
                "arxiv_comment": "Accepted at NeurIPS 2024 (Spotlight)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.02490v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.02490v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.07869v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.07869v2",
                "updated": "2024-10-30T14:49:49Z",
                "updated_parsed": [
                    2024,
                    10,
                    30,
                    14,
                    49,
                    49,
                    2,
                    304,
                    0
                ],
                "published": "2024-10-10T12:41:19Z",
                "published_parsed": [
                    2024,
                    10,
                    10,
                    12,
                    41,
                    19,
                    3,
                    284,
                    0
                ],
                "title": "Benchmarking Agentic Workflow Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Benchmarking Agentic Workflow Generation"
                },
                "summary": "Large Language Models (LLMs), with their exceptional ability to handle a wide\nrange of tasks, have driven significant advancements in tackling reasoning and\nplanning tasks, wherein decomposing complex problems into executable workflows\nis a crucial step in this process. Existing workflow evaluation frameworks\neither focus solely on holistic performance or suffer from limitations such as\nrestricted scenario coverage, simplistic workflow structures, and lax\nevaluation standards. To this end, we introduce WorFBench, a unified workflow\ngeneration benchmark with multi-faceted scenarios and intricate graph workflow\nstructures. Additionally, we present WorFEval, a systemic evaluation protocol\nutilizing subsequence and subgraph matching algorithms to accurately quantify\nthe LLM agent's workflow generation capabilities. Through comprehensive\nevaluations across different types of LLMs, we discover distinct gaps between\nthe sequence planning capabilities and graph planning capabilities of LLM\nagents, with even GPT-4 exhibiting a gap of around 15%. We also train two\nopen-source models and evaluate their generalization abilities on held-out\ntasks. Furthermore, we observe that the generated workflows can enhance\ndownstream tasks, enabling them to achieve superior performance with less time\nduring inference. Code and dataset are available at\nhttps://github.com/zjunlp/WorFBench.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs), with their exceptional ability to handle a wide\nrange of tasks, have driven significant advancements in tackling reasoning and\nplanning tasks, wherein decomposing complex problems into executable workflows\nis a crucial step in this process. Existing workflow evaluation frameworks\neither focus solely on holistic performance or suffer from limitations such as\nrestricted scenario coverage, simplistic workflow structures, and lax\nevaluation standards. To this end, we introduce WorFBench, a unified workflow\ngeneration benchmark with multi-faceted scenarios and intricate graph workflow\nstructures. Additionally, we present WorFEval, a systemic evaluation protocol\nutilizing subsequence and subgraph matching algorithms to accurately quantify\nthe LLM agent's workflow generation capabilities. Through comprehensive\nevaluations across different types of LLMs, we discover distinct gaps between\nthe sequence planning capabilities and graph planning capabilities of LLM\nagents, with even GPT-4 exhibiting a gap of around 15%. We also train two\nopen-source models and evaluate their generalization abilities on held-out\ntasks. Furthermore, we observe that the generated workflows can enhance\ndownstream tasks, enabling them to achieve superior performance with less time\nduring inference. Code and dataset are available at\nhttps://github.com/zjunlp/WorFBench."
                },
                "authors": [
                    {
                        "name": "Shuofei Qiao"
                    },
                    {
                        "name": "Runnan Fang"
                    },
                    {
                        "name": "Zhisong Qiu"
                    },
                    {
                        "name": "Xiaobin Wang"
                    },
                    {
                        "name": "Ningyu Zhang"
                    },
                    {
                        "name": "Yong Jiang"
                    },
                    {
                        "name": "Pengjun Xie"
                    },
                    {
                        "name": "Fei Huang"
                    },
                    {
                        "name": "Huajun Chen"
                    }
                ],
                "author_detail": {
                    "name": "Huajun Chen"
                },
                "author": "Huajun Chen",
                "arxiv_comment": "Work in progress (v2), update OpenAI o1 and Claude-3.5 results on\n  WorFBench",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.07869v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.07869v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.23074v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.23074v1",
                "updated": "2024-10-30T14:46:43Z",
                "updated_parsed": [
                    2024,
                    10,
                    30,
                    14,
                    46,
                    43,
                    2,
                    304,
                    0
                ],
                "published": "2024-10-30T14:46:43Z",
                "published_parsed": [
                    2024,
                    10,
                    30,
                    14,
                    46,
                    43,
                    2,
                    304,
                    0
                ],
                "title": "Multi-Programming Language Sandbox for LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-Programming Language Sandbox for LLMs"
                },
                "summary": "We introduce MPLSandbox, an out-of-the-box multi-programming language sandbox\ndesigned to provide unified and comprehensive feedback from compiler and\nanalysis tools for Large Language Models (LLMs). It can automatically identify\nthe programming language of the code, compiling and executing it within an\nisolated sub-sandbox to ensure safety and stability. In addition, MPLSandbox\nalso integrates both traditional and LLM-based code analysis tools, providing a\ncomprehensive analysis of generated code. MPLSandbox can be effortlessly\nintegrated into the training and deployment of LLMs to improve the quality and\ncorrectness of their generated code. It also helps researchers streamline their\nworkflows for various LLM-based code-related tasks, reducing the development\ncost. To validate the effectiveness of MPLSandbox, we integrate it into\ntraining and deployment approaches, and also employ it to optimize workflows\nfor a wide range of real-world code-related tasks. Our goal is to enhance\nresearcher productivity on LLM-based code-related tasks by simplifying and\nautomating workflows through delegation to MPLSandbox.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce MPLSandbox, an out-of-the-box multi-programming language sandbox\ndesigned to provide unified and comprehensive feedback from compiler and\nanalysis tools for Large Language Models (LLMs). It can automatically identify\nthe programming language of the code, compiling and executing it within an\nisolated sub-sandbox to ensure safety and stability. In addition, MPLSandbox\nalso integrates both traditional and LLM-based code analysis tools, providing a\ncomprehensive analysis of generated code. MPLSandbox can be effortlessly\nintegrated into the training and deployment of LLMs to improve the quality and\ncorrectness of their generated code. It also helps researchers streamline their\nworkflows for various LLM-based code-related tasks, reducing the development\ncost. To validate the effectiveness of MPLSandbox, we integrate it into\ntraining and deployment approaches, and also employ it to optimize workflows\nfor a wide range of real-world code-related tasks. Our goal is to enhance\nresearcher productivity on LLM-based code-related tasks by simplifying and\nautomating workflows through delegation to MPLSandbox."
                },
                "authors": [
                    {
                        "name": "Shihan Dou"
                    },
                    {
                        "name": "Jiazheng Zhang"
                    },
                    {
                        "name": "Jianxiang Zang"
                    },
                    {
                        "name": "Yunbo Tao"
                    },
                    {
                        "name": "Haoxiang Jia"
                    },
                    {
                        "name": "Shichun Liu"
                    },
                    {
                        "name": "Yuming Yang"
                    },
                    {
                        "name": "Shenxi Wu"
                    },
                    {
                        "name": "Shaoqing Zhang"
                    },
                    {
                        "name": "Muling Wu"
                    },
                    {
                        "name": "Changze Lv"
                    },
                    {
                        "name": "Limao Xiong"
                    },
                    {
                        "name": "Wenyu Zhan"
                    },
                    {
                        "name": "Lin Zhang"
                    },
                    {
                        "name": "Rongxiang Weng"
                    },
                    {
                        "name": "Jingang Wang"
                    },
                    {
                        "name": "Xunliang Cai"
                    },
                    {
                        "name": "Yueming Wu"
                    },
                    {
                        "name": "Ming Wen"
                    },
                    {
                        "name": "Rui Zheng"
                    },
                    {
                        "name": "Tao Ji"
                    },
                    {
                        "name": "Yixin Cao"
                    },
                    {
                        "name": "Tao Gui"
                    },
                    {
                        "name": "Xipeng Qiu"
                    },
                    {
                        "name": "Qi Zhang"
                    },
                    {
                        "name": "Xuanjing Huang"
                    }
                ],
                "author_detail": {
                    "name": "Xuanjing Huang"
                },
                "author": "Xuanjing Huang",
                "arxiv_comment": "25 pages, 14 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.23074v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.23074v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.17215v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.17215v2",
                "updated": "2024-10-30T14:45:26Z",
                "updated_parsed": [
                    2024,
                    10,
                    30,
                    14,
                    45,
                    26,
                    2,
                    304,
                    0
                ],
                "published": "2024-10-22T17:40:32Z",
                "published_parsed": [
                    2024,
                    10,
                    22,
                    17,
                    40,
                    32,
                    1,
                    296,
                    0
                ],
                "title": "MiniPLM: Knowledge Distillation for Pre-Training Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MiniPLM: Knowledge Distillation for Pre-Training Language Models"
                },
                "summary": "Knowledge distillation (KD) is widely used to train small, high-performing\nstudent language models (LMs) using large teacher LMs. While effective in\nfine-tuning, KD during pre-training faces challenges in efficiency,\nflexibility, and effectiveness. Existing methods either incur high\ncomputational costs due to online teacher inference, require tokenization\nmatching between teacher and student LMs, or risk losing the difficulty and\ndiversity of the teacher-generated training data. To address these issues, we\npropose MiniPLM, a KD framework for pre-training LMs by refining the training\ndata distribution with the teacher's knowledge. For efficiency, MiniPLM\nperforms offline teacher LM inference, allowing KD for multiple student LMs\nwithout adding training-time costs. For flexibility, MiniPLM operates solely on\nthe training corpus, enabling KD across model families. For effectiveness,\nMiniPLM leverages the differences between large and small LMs to enhance the\ndifficulty and diversity of the training data, helping student LMs acquire\nversatile and sophisticated knowledge. Extensive experiments demonstrate that\nMiniPLM boosts the student LMs' performance on 9 widely used downstream tasks,\nimproves the language modeling capabilities, and reduces pre-training\ncomputation. The benefit of MiniPLM extends to large pre-training scales,\nevidenced by the extrapolation of the scaling curves. Further analysis reveals\nthat MiniPLM supports KD across model families and enhances the utilization of\npre-training data. Our model, code, and data are available at\nhttps://github.com/thu-coai/MiniPLM.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Knowledge distillation (KD) is widely used to train small, high-performing\nstudent language models (LMs) using large teacher LMs. While effective in\nfine-tuning, KD during pre-training faces challenges in efficiency,\nflexibility, and effectiveness. Existing methods either incur high\ncomputational costs due to online teacher inference, require tokenization\nmatching between teacher and student LMs, or risk losing the difficulty and\ndiversity of the teacher-generated training data. To address these issues, we\npropose MiniPLM, a KD framework for pre-training LMs by refining the training\ndata distribution with the teacher's knowledge. For efficiency, MiniPLM\nperforms offline teacher LM inference, allowing KD for multiple student LMs\nwithout adding training-time costs. For flexibility, MiniPLM operates solely on\nthe training corpus, enabling KD across model families. For effectiveness,\nMiniPLM leverages the differences between large and small LMs to enhance the\ndifficulty and diversity of the training data, helping student LMs acquire\nversatile and sophisticated knowledge. Extensive experiments demonstrate that\nMiniPLM boosts the student LMs' performance on 9 widely used downstream tasks,\nimproves the language modeling capabilities, and reduces pre-training\ncomputation. The benefit of MiniPLM extends to large pre-training scales,\nevidenced by the extrapolation of the scaling curves. Further analysis reveals\nthat MiniPLM supports KD across model families and enhances the utilization of\npre-training data. Our model, code, and data are available at\nhttps://github.com/thu-coai/MiniPLM."
                },
                "authors": [
                    {
                        "name": "Yuxian Gu"
                    },
                    {
                        "name": "Hao Zhou"
                    },
                    {
                        "name": "Fandong Meng"
                    },
                    {
                        "name": "Jie Zhou"
                    },
                    {
                        "name": "Minlie Huang"
                    }
                ],
                "author_detail": {
                    "name": "Minlie Huang"
                },
                "author": "Minlie Huang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.17215v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.17215v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.23069v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.23069v1",
                "updated": "2024-10-30T14:43:33Z",
                "updated_parsed": [
                    2024,
                    10,
                    30,
                    14,
                    43,
                    33,
                    2,
                    304,
                    0
                ],
                "published": "2024-10-30T14:43:33Z",
                "published_parsed": [
                    2024,
                    10,
                    30,
                    14,
                    43,
                    33,
                    2,
                    304,
                    0
                ],
                "title": "LLMs Integration in Software Engineering Team Projects: Roles, Impact,\n  and a Pedagogical Design Space for AI Tools in Computing Education",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLMs Integration in Software Engineering Team Projects: Roles, Impact,\n  and a Pedagogical Design Space for AI Tools in Computing Education"
                },
                "summary": "This work takes a pedagogical lens to explore the implications of generative\nAI (GenAI) models and tools, such as ChatGPT and GitHub Copilot, in a\nsemester-long 2nd-year undergraduate Software Engineering Team Project.\nQualitative findings from survey (39 students) and interviews (eight students)\nprovide insights into the students' views on the impact of GenAI use on their\ncoding experience, learning, and self-efficacy. Our results address a\nparticular gap in understanding the role and implications of GenAI on teamwork,\nteam-efficacy, and team dynamics. The analysis of the learning aspects is\ndistinguished by the application of learning and pedagogy informed lenses to\ndiscuss the data. We propose a preliminary design space for GenAI-based\nprogramming learning tools highlighting the importance of considering the roles\nthat GenAI can play during the learning process, the varying support-ability\npatterns that can be applied to each role, and the importance of supporting\ntransparency in GenAI for team members and students in addition to educators.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This work takes a pedagogical lens to explore the implications of generative\nAI (GenAI) models and tools, such as ChatGPT and GitHub Copilot, in a\nsemester-long 2nd-year undergraduate Software Engineering Team Project.\nQualitative findings from survey (39 students) and interviews (eight students)\nprovide insights into the students' views on the impact of GenAI use on their\ncoding experience, learning, and self-efficacy. Our results address a\nparticular gap in understanding the role and implications of GenAI on teamwork,\nteam-efficacy, and team dynamics. The analysis of the learning aspects is\ndistinguished by the application of learning and pedagogy informed lenses to\ndiscuss the data. We propose a preliminary design space for GenAI-based\nprogramming learning tools highlighting the importance of considering the roles\nthat GenAI can play during the learning process, the varying support-ability\npatterns that can be applied to each role, and the importance of supporting\ntransparency in GenAI for team members and students in addition to educators."
                },
                "authors": [
                    {
                        "name": "Ahmed Kharrufa"
                    },
                    {
                        "name": "Sami Alghamdi"
                    },
                    {
                        "name": "Abeer Aziz"
                    },
                    {
                        "name": "Christopher Bull"
                    }
                ],
                "author_detail": {
                    "name": "Christopher Bull"
                },
                "author": "Christopher Bull",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.23069v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.23069v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.08384v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.08384v2",
                "updated": "2024-10-30T14:33:27Z",
                "updated_parsed": [
                    2024,
                    10,
                    30,
                    14,
                    33,
                    27,
                    2,
                    304,
                    0
                ],
                "published": "2024-06-12T16:34:26Z",
                "published_parsed": [
                    2024,
                    6,
                    12,
                    16,
                    34,
                    26,
                    2,
                    164,
                    0
                ],
                "title": "Diff-A-Riff: Musical Accompaniment Co-creation via Latent Diffusion\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diff-A-Riff: Musical Accompaniment Co-creation via Latent Diffusion\n  Models"
                },
                "summary": "Recent advancements in deep generative models present new opportunities for\nmusic production but also pose challenges, such as high computational demands\nand limited audio quality. Moreover, current systems frequently rely solely on\ntext input and typically focus on producing complete musical pieces, which is\nincompatible with existing workflows in music production. To address these\nissues, we introduce \"Diff-A-Riff,\" a Latent Diffusion Model designed to\ngenerate high-quality instrumental accompaniments adaptable to any musical\ncontext. This model offers control through either audio references, text\nprompts, or both, and produces 48kHz pseudo-stereo audio while significantly\nreducing inference time and memory usage. We demonstrate the model's\ncapabilities through objective metrics and subjective listening tests, with\nextensive examples available on the accompanying website:\nsonycslparis.github.io/diffariff-companion/",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advancements in deep generative models present new opportunities for\nmusic production but also pose challenges, such as high computational demands\nand limited audio quality. Moreover, current systems frequently rely solely on\ntext input and typically focus on producing complete musical pieces, which is\nincompatible with existing workflows in music production. To address these\nissues, we introduce \"Diff-A-Riff,\" a Latent Diffusion Model designed to\ngenerate high-quality instrumental accompaniments adaptable to any musical\ncontext. This model offers control through either audio references, text\nprompts, or both, and produces 48kHz pseudo-stereo audio while significantly\nreducing inference time and memory usage. We demonstrate the model's\ncapabilities through objective metrics and subjective listening tests, with\nextensive examples available on the accompanying website:\nsonycslparis.github.io/diffariff-companion/"
                },
                "authors": [
                    {
                        "name": "Javier Nistal"
                    },
                    {
                        "name": "Marco Pasini"
                    },
                    {
                        "name": "Cyran Aouameur"
                    },
                    {
                        "name": "Maarten Grachten"
                    },
                    {
                        "name": "Stefan Lattner"
                    }
                ],
                "author_detail": {
                    "name": "Stefan Lattner"
                },
                "author": "Stefan Lattner",
                "arxiv_comment": "8 pages, 2 figures, 3 tables",
                "arxiv_journal_ref": "Proc. of the 25th International Society for Music Information\n  Retrieval, 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.08384v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.08384v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SD",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.11387v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.11387v3",
                "updated": "2024-10-30T14:31:25Z",
                "updated_parsed": [
                    2024,
                    10,
                    30,
                    14,
                    31,
                    25,
                    2,
                    304,
                    0
                ],
                "published": "2024-10-15T08:24:05Z",
                "published_parsed": [
                    2024,
                    10,
                    15,
                    8,
                    24,
                    5,
                    1,
                    289,
                    0
                ],
                "title": "LLM2Swarm: Robot Swarms that Responsively Reason, Plan, and Collaborate\n  through LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM2Swarm: Robot Swarms that Responsively Reason, Plan, and Collaborate\n  through LLMs"
                },
                "summary": "Robot swarms are composed of many simple robots that communicate and\ncollaborate to fulfill complex tasks. Robot controllers usually need to be\nspecified by experts on a case-by-case basis via programming code. This process\nis time-consuming, prone to errors, and unable to take into account all\nsituations that may be encountered during deployment. On the other hand, recent\nLarge Language Models (LLMs) have demonstrated reasoning and planning\ncapabilities, introduced new ways to interact with and program machines, and\nincorporate both domain-specific and commonsense knowledge. Hence, we propose\nto address the aforementioned challenges by integrating LLMs with robot swarms\nand show the potential in proofs of concept (showcases). For this integration,\nwe explore two approaches. The first approach is 'indirect integration,' where\nLLMs are used to synthesize and validate the robot controllers. This approach\nmay reduce development time and human error before deployment. Moreover, during\ndeployment, it could be used for on-the-fly creation of new robot behaviors.\nThe second approach is 'direct integration,' where each robot locally executes\na separate LLM instance during deployment for robot-robot collaboration and\nhuman-swarm interaction. These local LLM instances enable each robot to reason,\nplan, and collaborate using natural language, as demonstrated in our showcases\nwhere the robots are able to detect a variety of anomalies, without prior\ninformation about the nature of these anomalies. To enable further research on\nour mainly conceptual contribution, we release the software and videos for our\nLLM2Swarm system: https://github.com/Pold87/LLM2Swarm.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Robot swarms are composed of many simple robots that communicate and\ncollaborate to fulfill complex tasks. Robot controllers usually need to be\nspecified by experts on a case-by-case basis via programming code. This process\nis time-consuming, prone to errors, and unable to take into account all\nsituations that may be encountered during deployment. On the other hand, recent\nLarge Language Models (LLMs) have demonstrated reasoning and planning\ncapabilities, introduced new ways to interact with and program machines, and\nincorporate both domain-specific and commonsense knowledge. Hence, we propose\nto address the aforementioned challenges by integrating LLMs with robot swarms\nand show the potential in proofs of concept (showcases). For this integration,\nwe explore two approaches. The first approach is 'indirect integration,' where\nLLMs are used to synthesize and validate the robot controllers. This approach\nmay reduce development time and human error before deployment. Moreover, during\ndeployment, it could be used for on-the-fly creation of new robot behaviors.\nThe second approach is 'direct integration,' where each robot locally executes\na separate LLM instance during deployment for robot-robot collaboration and\nhuman-swarm interaction. These local LLM instances enable each robot to reason,\nplan, and collaborate using natural language, as demonstrated in our showcases\nwhere the robots are able to detect a variety of anomalies, without prior\ninformation about the nature of these anomalies. To enable further research on\nour mainly conceptual contribution, we release the software and videos for our\nLLM2Swarm system: https://github.com/Pold87/LLM2Swarm."
                },
                "authors": [
                    {
                        "name": "Volker Strobel"
                    },
                    {
                        "name": "Marco Dorigo"
                    },
                    {
                        "name": "Mario Fritz"
                    }
                ],
                "author_detail": {
                    "name": "Mario Fritz"
                },
                "author": "Mario Fritz",
                "arxiv_comment": "Accepted at NeurIPS 2024 Workshop on Open-World Agents. Code:\n  https://github.com/Pold87/LLM2Swarm/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.11387v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.11387v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.20441v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.20441v4",
                "updated": "2024-10-30T14:29:37Z",
                "updated_parsed": [
                    2024,
                    10,
                    30,
                    14,
                    29,
                    37,
                    2,
                    304,
                    0
                ],
                "published": "2024-05-30T19:35:06Z",
                "published_parsed": [
                    2024,
                    5,
                    30,
                    19,
                    35,
                    6,
                    3,
                    151,
                    0
                ],
                "title": "SECURE: Benchmarking Large Language Models for Cybersecurity",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SECURE: Benchmarking Large Language Models for Cybersecurity"
                },
                "summary": "Large Language Models (LLMs) have demonstrated potential in cybersecurity\napplications but have also caused lower confidence due to problems like\nhallucinations and a lack of truthfulness. Existing benchmarks provide general\nevaluations but do not sufficiently address the practical and applied aspects\nof LLM performance in cybersecurity-specific tasks. To address this gap, we\nintroduce the SECURE (Security Extraction, Understanding \\& Reasoning\nEvaluation), a benchmark designed to assess LLMs performance in realistic\ncybersecurity scenarios. SECURE includes six datasets focussed on the\nIndustrial Control System sector to evaluate knowledge extraction,\nunderstanding, and reasoning based on industry-standard sources. Our study\nevaluates seven state-of-the-art models on these tasks, providing insights into\ntheir strengths and weaknesses in cybersecurity contexts, and offer\nrecommendations for improving LLMs reliability as cyber advisory tools.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have demonstrated potential in cybersecurity\napplications but have also caused lower confidence due to problems like\nhallucinations and a lack of truthfulness. Existing benchmarks provide general\nevaluations but do not sufficiently address the practical and applied aspects\nof LLM performance in cybersecurity-specific tasks. To address this gap, we\nintroduce the SECURE (Security Extraction, Understanding \\& Reasoning\nEvaluation), a benchmark designed to assess LLMs performance in realistic\ncybersecurity scenarios. SECURE includes six datasets focussed on the\nIndustrial Control System sector to evaluate knowledge extraction,\nunderstanding, and reasoning based on industry-standard sources. Our study\nevaluates seven state-of-the-art models on these tasks, providing insights into\ntheir strengths and weaknesses in cybersecurity contexts, and offer\nrecommendations for improving LLMs reliability as cyber advisory tools."
                },
                "authors": [
                    {
                        "name": "Dipkamal Bhusal"
                    },
                    {
                        "name": "Md Tanvirul Alam"
                    },
                    {
                        "name": "Le Nguyen"
                    },
                    {
                        "name": "Ashim Mahara"
                    },
                    {
                        "name": "Zachary Lightcap"
                    },
                    {
                        "name": "Rodney Frazier"
                    },
                    {
                        "name": "Romy Fieblinger"
                    },
                    {
                        "name": "Grace Long Torales"
                    },
                    {
                        "name": "Benjamin A. Blakely"
                    },
                    {
                        "name": "Nidhi Rastogi"
                    }
                ],
                "author_detail": {
                    "name": "Nidhi Rastogi"
                },
                "author": "Nidhi Rastogi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.20441v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.20441v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.23054v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.23054v1",
                "updated": "2024-10-30T14:21:33Z",
                "updated_parsed": [
                    2024,
                    10,
                    30,
                    14,
                    21,
                    33,
                    2,
                    304,
                    0
                ],
                "published": "2024-10-30T14:21:33Z",
                "published_parsed": [
                    2024,
                    10,
                    30,
                    14,
                    21,
                    33,
                    2,
                    304,
                    0
                ],
                "title": "Controlling Language and Diffusion Models by Transporting Activations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Controlling Language and Diffusion Models by Transporting Activations"
                },
                "summary": "The increasing capabilities of large generative models and their ever more\nwidespread deployment have raised concerns about their reliability, safety, and\npotential misuse. To address these issues, recent works have proposed to\ncontrol model generation by steering model activations in order to effectively\ninduce or prevent the emergence of concepts or behaviors in the generated\noutput. In this paper we introduce Activation Transport (AcT), a general\nframework to steer activations guided by optimal transport theory that\ngeneralizes many previous activation-steering works. AcT is modality-agnostic\nand provides fine-grained control over the model behavior with negligible\ncomputational overhead, while minimally impacting model abilities. We\nexperimentally show the effectiveness and versatility of our approach by\naddressing key challenges in large language models (LLMs) and text-to-image\ndiffusion models (T2Is). For LLMs, we show that AcT can effectively mitigate\ntoxicity, induce arbitrary concepts, and increase their truthfulness. In T2Is,\nwe show how AcT enables fine-grained style control and concept negation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The increasing capabilities of large generative models and their ever more\nwidespread deployment have raised concerns about their reliability, safety, and\npotential misuse. To address these issues, recent works have proposed to\ncontrol model generation by steering model activations in order to effectively\ninduce or prevent the emergence of concepts or behaviors in the generated\noutput. In this paper we introduce Activation Transport (AcT), a general\nframework to steer activations guided by optimal transport theory that\ngeneralizes many previous activation-steering works. AcT is modality-agnostic\nand provides fine-grained control over the model behavior with negligible\ncomputational overhead, while minimally impacting model abilities. We\nexperimentally show the effectiveness and versatility of our approach by\naddressing key challenges in large language models (LLMs) and text-to-image\ndiffusion models (T2Is). For LLMs, we show that AcT can effectively mitigate\ntoxicity, induce arbitrary concepts, and increase their truthfulness. In T2Is,\nwe show how AcT enables fine-grained style control and concept negation."
                },
                "authors": [
                    {
                        "name": "Pau Rodriguez"
                    },
                    {
                        "name": "Arno Blaas"
                    },
                    {
                        "name": "Michal Klein"
                    },
                    {
                        "name": "Luca Zappella"
                    },
                    {
                        "name": "Nicholas Apostoloff"
                    },
                    {
                        "name": "Marco Cuturi"
                    },
                    {
                        "name": "Xavier Suau"
                    }
                ],
                "author_detail": {
                    "name": "Xavier Suau"
                },
                "author": "Xavier Suau",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.23054v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.23054v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "68T07, 49Q22",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.6; I.2.7; I.4.8",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.15383v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.15383v2",
                "updated": "2024-10-30T14:19:57Z",
                "updated_parsed": [
                    2024,
                    10,
                    30,
                    14,
                    19,
                    57,
                    2,
                    304,
                    0
                ],
                "published": "2024-05-24T09:31:26Z",
                "published_parsed": [
                    2024,
                    5,
                    24,
                    9,
                    31,
                    26,
                    4,
                    145,
                    0
                ],
                "title": "Generating Code World Models with Large Language Models Guided by Monte\n  Carlo Tree Search",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Generating Code World Models with Large Language Models Guided by Monte\n  Carlo Tree Search"
                },
                "summary": "In this work we consider Code World Models, world models generated by a Large\nLanguage Model (LLM) in the form of Python code for model-based Reinforcement\nLearning (RL). Calling code instead of LLMs for planning has potential to be\nmore precise, reliable, interpretable, and extremely efficient. However,\nwriting appropriate Code World Models requires the ability to understand\ncomplex instructions, to generate exact code with non-trivial logic and to\nself-debug a long program with feedback from unit tests and environment\ntrajectories. To address these challenges, we propose Generate, Improve and Fix\nwith Monte Carlo Tree Search (GIF-MCTS), a new code generation strategy for\nLLMs. To test our approach in an offline RL setting, we introduce the Code\nWorld Models Benchmark (CWMB), a suite of program synthesis and planning tasks\ncomprised of 18 diverse RL environments paired with corresponding textual\ndescriptions and curated trajectories. GIF-MCTS surpasses all baselines on the\nCWMB and two other benchmarks, and we show that the Code World Models\nsynthesized with it can be successfully used for planning, resulting in\nmodel-based RL agents with greatly improved sample efficiency and inference\nspeed.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this work we consider Code World Models, world models generated by a Large\nLanguage Model (LLM) in the form of Python code for model-based Reinforcement\nLearning (RL). Calling code instead of LLMs for planning has potential to be\nmore precise, reliable, interpretable, and extremely efficient. However,\nwriting appropriate Code World Models requires the ability to understand\ncomplex instructions, to generate exact code with non-trivial logic and to\nself-debug a long program with feedback from unit tests and environment\ntrajectories. To address these challenges, we propose Generate, Improve and Fix\nwith Monte Carlo Tree Search (GIF-MCTS), a new code generation strategy for\nLLMs. To test our approach in an offline RL setting, we introduce the Code\nWorld Models Benchmark (CWMB), a suite of program synthesis and planning tasks\ncomprised of 18 diverse RL environments paired with corresponding textual\ndescriptions and curated trajectories. GIF-MCTS surpasses all baselines on the\nCWMB and two other benchmarks, and we show that the Code World Models\nsynthesized with it can be successfully used for planning, resulting in\nmodel-based RL agents with greatly improved sample efficiency and inference\nspeed."
                },
                "authors": [
                    {
                        "name": "Nicola Dainese"
                    },
                    {
                        "name": "Matteo Merler"
                    },
                    {
                        "name": "Minttu Alakuijala"
                    },
                    {
                        "name": "Pekka Marttinen"
                    }
                ],
                "author_detail": {
                    "name": "Pekka Marttinen"
                },
                "author": "Pekka Marttinen",
                "arxiv_comment": "Accepted at NeurIPS 2024, Main Track. 11 pages in main text, 40 pages\n  including references and supplementary materials. 2 figures and 3 tables in\n  the main text, 9 figures and 12 tables when including the supplementary\n  materials. Website at https://sites.google.com/view/code-world-models/home",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.15383v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.15383v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.13073v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.13073v3",
                "updated": "2024-10-30T14:15:49Z",
                "updated_parsed": [
                    2024,
                    10,
                    30,
                    14,
                    15,
                    49,
                    2,
                    304,
                    0
                ],
                "published": "2024-10-16T22:25:15Z",
                "published_parsed": [
                    2024,
                    10,
                    16,
                    22,
                    25,
                    15,
                    2,
                    290,
                    0
                ],
                "title": "PromptExp: Multi-granularity Prompt Explanation of Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PromptExp: Multi-granularity Prompt Explanation of Large Language Models"
                },
                "summary": "Large Language Models excel in tasks like natural language understanding and\ntext generation. Prompt engineering plays a critical role in leveraging LLM\neffectively. However, LLMs black-box nature hinders its interpretability and\neffective prompting engineering. A wide range of model explanation approaches\nhave been developed for deep learning models, However, these local explanations\nare designed for single-output tasks like classification and regression,and\ncannot be directly applied to LLMs, which generate sequences of tokens. Recent\nefforts in LLM explanation focus on natural language explanations, but they are\nprone to hallucinations and inaccuracies. To address this, we introduce\nPromptExp , a framework for multi-granularity prompt explanations by\naggregating token-level insights. PromptExp introduces two token-level\nexplanation approaches: 1. an aggregation-based approach combining local\nexplanation techniques, and 2. a perturbation-based approach with novel\ntechniques to evaluate token masking impact. PromptExp supports both white-box\nand black-box explanations and extends explanations to higher granularity\nlevels, enabling flexible analysis. We evaluate PromptExp in case studies such\nas sentiment analysis, showing the perturbation-based approach performs best\nusing semantic similarity to assess perturbation impact. Furthermore, we\nconducted a user study to confirm PromptExp's accuracy and practical value, and\ndemonstrate its potential to enhance LLM interpretability.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models excel in tasks like natural language understanding and\ntext generation. Prompt engineering plays a critical role in leveraging LLM\neffectively. However, LLMs black-box nature hinders its interpretability and\neffective prompting engineering. A wide range of model explanation approaches\nhave been developed for deep learning models, However, these local explanations\nare designed for single-output tasks like classification and regression,and\ncannot be directly applied to LLMs, which generate sequences of tokens. Recent\nefforts in LLM explanation focus on natural language explanations, but they are\nprone to hallucinations and inaccuracies. To address this, we introduce\nPromptExp , a framework for multi-granularity prompt explanations by\naggregating token-level insights. PromptExp introduces two token-level\nexplanation approaches: 1. an aggregation-based approach combining local\nexplanation techniques, and 2. a perturbation-based approach with novel\ntechniques to evaluate token masking impact. PromptExp supports both white-box\nand black-box explanations and extends explanations to higher granularity\nlevels, enabling flexible analysis. We evaluate PromptExp in case studies such\nas sentiment analysis, showing the perturbation-based approach performs best\nusing semantic similarity to assess perturbation impact. Furthermore, we\nconducted a user study to confirm PromptExp's accuracy and practical value, and\ndemonstrate its potential to enhance LLM interpretability."
                },
                "authors": [
                    {
                        "name": "Ximing Dong"
                    },
                    {
                        "name": "Shaowei Wang"
                    },
                    {
                        "name": "Dayi Lin"
                    },
                    {
                        "name": "Gopi Krishnan Rajbahadur"
                    },
                    {
                        "name": "Boquan Zhou"
                    },
                    {
                        "name": "Shichao Liu"
                    },
                    {
                        "name": "Ahmed E. Hassan"
                    }
                ],
                "author_detail": {
                    "name": "Ahmed E. Hassan"
                },
                "author": "Ahmed E. Hassan",
                "arxiv_comment": "11 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.13073v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.13073v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.23041v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.23041v1",
                "updated": "2024-10-30T14:08:50Z",
                "updated_parsed": [
                    2024,
                    10,
                    30,
                    14,
                    8,
                    50,
                    2,
                    304,
                    0
                ],
                "published": "2024-10-30T14:08:50Z",
                "published_parsed": [
                    2024,
                    10,
                    30,
                    14,
                    8,
                    50,
                    2,
                    304,
                    0
                ],
                "title": "Emotional RAG: Enhancing Role-Playing Agents through Emotional Retrieval",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Emotional RAG: Enhancing Role-Playing Agents through Emotional Retrieval"
                },
                "summary": "As LLMs exhibit a high degree of human-like capability, increasing attention\nhas been paid to role-playing research areas in which responses generated by\nLLMs are expected to mimic human replies. This has promoted the exploration of\nrole-playing agents in various applications, such as chatbots that can engage\nin natural conversations with users and virtual assistants that can provide\npersonalized support and guidance. The crucial factor in the role-playing task\nis the effective utilization of character memory, which stores characters'\nprofiles, experiences, and historical dialogues. Retrieval Augmented Generation\n(RAG) technology is used to access the related memory to enhance the response\ngeneration of role-playing agents. Most existing studies retrieve related\ninformation based on the semantic similarity of memory to maintain characters'\npersonalized traits, and few attempts have been made to incorporate the\nemotional factor in the retrieval argument generation (RAG) of LLMs. Inspired\nby the Mood-Dependent Memory theory, which indicates that people recall an\nevent better if they somehow reinstate during recall the original emotion they\nexperienced during learning, we propose a novel emotion-aware memory retrieval\nframework, termed Emotional RAG, which recalls the related memory with\nconsideration of emotional state in role-playing agents. Specifically, we\ndesign two kinds of retrieval strategies, i.e., combination strategy and\nsequential strategy, to incorporate both memory semantic and emotional states\nduring the retrieval process. Extensive experiments on three representative\nrole-playing datasets demonstrate that our Emotional RAG framework outperforms\nthe method without considering the emotional factor in maintaining the\npersonalities of role-playing agents. This provides evidence to further\nreinforce the Mood-Dependent Memory theory in psychology.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As LLMs exhibit a high degree of human-like capability, increasing attention\nhas been paid to role-playing research areas in which responses generated by\nLLMs are expected to mimic human replies. This has promoted the exploration of\nrole-playing agents in various applications, such as chatbots that can engage\nin natural conversations with users and virtual assistants that can provide\npersonalized support and guidance. The crucial factor in the role-playing task\nis the effective utilization of character memory, which stores characters'\nprofiles, experiences, and historical dialogues. Retrieval Augmented Generation\n(RAG) technology is used to access the related memory to enhance the response\ngeneration of role-playing agents. Most existing studies retrieve related\ninformation based on the semantic similarity of memory to maintain characters'\npersonalized traits, and few attempts have been made to incorporate the\nemotional factor in the retrieval argument generation (RAG) of LLMs. Inspired\nby the Mood-Dependent Memory theory, which indicates that people recall an\nevent better if they somehow reinstate during recall the original emotion they\nexperienced during learning, we propose a novel emotion-aware memory retrieval\nframework, termed Emotional RAG, which recalls the related memory with\nconsideration of emotional state in role-playing agents. Specifically, we\ndesign two kinds of retrieval strategies, i.e., combination strategy and\nsequential strategy, to incorporate both memory semantic and emotional states\nduring the retrieval process. Extensive experiments on three representative\nrole-playing datasets demonstrate that our Emotional RAG framework outperforms\nthe method without considering the emotional factor in maintaining the\npersonalities of role-playing agents. This provides evidence to further\nreinforce the Mood-Dependent Memory theory in psychology."
                },
                "authors": [
                    {
                        "name": "Le Huang"
                    },
                    {
                        "name": "Hengzhi Lan"
                    },
                    {
                        "name": "Zijun Sun"
                    },
                    {
                        "name": "Chuan Shi"
                    },
                    {
                        "name": "Ting Bai"
                    }
                ],
                "author_detail": {
                    "name": "Ting Bai"
                },
                "author": "Ting Bai",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.23041v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.23041v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.14516v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.14516v4",
                "updated": "2024-10-30T14:06:12Z",
                "updated_parsed": [
                    2024,
                    10,
                    30,
                    14,
                    6,
                    12,
                    2,
                    304,
                    0
                ],
                "published": "2024-10-18T14:55:14Z",
                "published_parsed": [
                    2024,
                    10,
                    18,
                    14,
                    55,
                    14,
                    4,
                    292,
                    0
                ],
                "title": "Do LLMs \"know\" internally when they follow instructions?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Do LLMs \"know\" internally when they follow instructions?"
                },
                "summary": "Instruction-following is crucial for building AI agents with large language\nmodels (LLMs), as these models must adhere strictly to user-provided\nconstraints and guidelines. However, LLMs often fail to follow even simple and\nclear instructions. To improve instruction-following behavior and prevent\nundesirable outputs, a deeper understanding of how LLMs' internal states relate\nto these outcomes is required. Our analysis of LLM internal states reveal a\ndimension in the input embedding space linked to successful\ninstruction-following. We demonstrate that modifying representations along this\ndimension improves instruction-following success rates compared to random\nchanges, without compromising response quality. Further investigation reveals\nthat this dimension is more closely related to the phrasing of prompts rather\nthan the inherent difficulty of the task or instructions. This discovery also\nsuggests explanations for why LLMs sometimes fail to follow clear instructions\nand why prompt engineering is often effective, even when the content remains\nlargely unchanged. This work provides insight into the internal workings of\nLLMs' instruction-following, paving the way for reliable LLM agents.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Instruction-following is crucial for building AI agents with large language\nmodels (LLMs), as these models must adhere strictly to user-provided\nconstraints and guidelines. However, LLMs often fail to follow even simple and\nclear instructions. To improve instruction-following behavior and prevent\nundesirable outputs, a deeper understanding of how LLMs' internal states relate\nto these outcomes is required. Our analysis of LLM internal states reveal a\ndimension in the input embedding space linked to successful\ninstruction-following. We demonstrate that modifying representations along this\ndimension improves instruction-following success rates compared to random\nchanges, without compromising response quality. Further investigation reveals\nthat this dimension is more closely related to the phrasing of prompts rather\nthan the inherent difficulty of the task or instructions. This discovery also\nsuggests explanations for why LLMs sometimes fail to follow clear instructions\nand why prompt engineering is often effective, even when the content remains\nlargely unchanged. This work provides insight into the internal workings of\nLLMs' instruction-following, paving the way for reliable LLM agents."
                },
                "authors": [
                    {
                        "name": "Juyeon Heo"
                    },
                    {
                        "name": "Christina Heinze-Deml"
                    },
                    {
                        "name": "Oussama Elachqar"
                    },
                    {
                        "name": "Shirley Ren"
                    },
                    {
                        "name": "Udhay Nallasamy"
                    },
                    {
                        "name": "Andy Miller"
                    },
                    {
                        "name": "Kwan Ho Ryan Chan"
                    },
                    {
                        "name": "Jaya Narain"
                    }
                ],
                "author_detail": {
                    "name": "Jaya Narain"
                },
                "author": "Jaya Narain",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.14516v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.14516v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.23036v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.23036v1",
                "updated": "2024-10-30T14:03:37Z",
                "updated_parsed": [
                    2024,
                    10,
                    30,
                    14,
                    3,
                    37,
                    2,
                    304,
                    0
                ],
                "published": "2024-10-30T14:03:37Z",
                "published_parsed": [
                    2024,
                    10,
                    30,
                    14,
                    3,
                    37,
                    2,
                    304,
                    0
                ],
                "title": "Evidence for a Sharp CO Snowline Transition in a Protoplanetary Disk and\n  Implications for Millimeter-wave Observations of CO Isotopologues",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Evidence for a Sharp CO Snowline Transition in a Protoplanetary Disk and\n  Implications for Millimeter-wave Observations of CO Isotopologues"
                },
                "summary": "Observations of CO isotopologue emission from protoplanetary disks at\nmillimeter wavelengths are a powerful tool for probing the CO snowline, an\nimportant marker for disk chemistry, and also for estimating total disk gas\nmass, a key quantity for planet formation. We use simple models to demonstrate\nthat the vertical thickness of an isothermal layer around the disk midplane has\nimportant effects on the CO column density radial profile, with a thick layer\nproducing a sharp CO snowline transition. We simulate ngVLA and ALMA images to\nshow that this sharp change in CO column density can be detected in the\nderivative of the radial profile of emission from optically thin CO\nisotopologue lines. We apply this method to archival ALMA observations of the\ndisk around the Herbig Ae star HD 163296 in the C$^{17}$O and C$^{18}$O J=1-0\nand J=2-1 lines to identify a sharp CO snowline transition near 80 au (0.8\narcsec at 101 pc), and show the CO column density decreases by more than a\nfactor of 20. This finding is consistent with previous inferences from the\nsteep rise of N$_2$H$^+$ emission, which marks the location where CO depletes.\nWe also demonstrate that the disk's thermal structure introduces a significant\nsystematic uncertainty to estimates of total disk gas mass derived from these\nlines. The substantial improvement in sensitivity envisioned for the ngVLA over\nALMA for observations of ground state lines of CO isotopologues has the\npotential to extend this approach to a much larger population of disks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Observations of CO isotopologue emission from protoplanetary disks at\nmillimeter wavelengths are a powerful tool for probing the CO snowline, an\nimportant marker for disk chemistry, and also for estimating total disk gas\nmass, a key quantity for planet formation. We use simple models to demonstrate\nthat the vertical thickness of an isothermal layer around the disk midplane has\nimportant effects on the CO column density radial profile, with a thick layer\nproducing a sharp CO snowline transition. We simulate ngVLA and ALMA images to\nshow that this sharp change in CO column density can be detected in the\nderivative of the radial profile of emission from optically thin CO\nisotopologue lines. We apply this method to archival ALMA observations of the\ndisk around the Herbig Ae star HD 163296 in the C$^{17}$O and C$^{18}$O J=1-0\nand J=2-1 lines to identify a sharp CO snowline transition near 80 au (0.8\narcsec at 101 pc), and show the CO column density decreases by more than a\nfactor of 20. This finding is consistent with previous inferences from the\nsteep rise of N$_2$H$^+$ emission, which marks the location where CO depletes.\nWe also demonstrate that the disk's thermal structure introduces a significant\nsystematic uncertainty to estimates of total disk gas mass derived from these\nlines. The substantial improvement in sensitivity envisioned for the ngVLA over\nALMA for observations of ground state lines of CO isotopologues has the\npotential to extend this approach to a much larger population of disks."
                },
                "authors": [
                    {
                        "name": "Chunhua Qi"
                    },
                    {
                        "name": "David J. Wilner"
                    }
                ],
                "author_detail": {
                    "name": "David J. Wilner"
                },
                "author": "David J. Wilner",
                "arxiv_comment": "Accepted for publication in ApJ; 15 pages, 8 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.23036v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.23036v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.EP",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.EP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "astro-ph.SR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.23022v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.23022v1",
                "updated": "2024-10-30T13:52:43Z",
                "updated_parsed": [
                    2024,
                    10,
                    30,
                    13,
                    52,
                    43,
                    2,
                    304,
                    0
                ],
                "published": "2024-10-30T13:52:43Z",
                "published_parsed": [
                    2024,
                    10,
                    30,
                    13,
                    52,
                    43,
                    2,
                    304,
                    0
                ],
                "title": "Online Intrinsic Rewards for Decision Making Agents from Large Language\n  Model Feedback",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Online Intrinsic Rewards for Decision Making Agents from Large Language\n  Model Feedback"
                },
                "summary": "Automatically synthesizing dense rewards from natural language descriptions\nis a promising paradigm in reinforcement learning (RL), with applications to\nsparse reward problems, open-ended exploration, and hierarchical skill design.\nRecent works have made promising steps by exploiting the prior knowledge of\nlarge language models (LLMs). However, these approaches suffer from important\nlimitations: they are either not scalable to problems requiring billions of\nenvironment samples; or are limited to reward functions expressible by compact\ncode, which may require source code and have difficulty capturing nuanced\nsemantics; or require a diverse offline dataset, which may not exist or be\nimpossible to collect. In this work, we address these limitations through a\ncombination of algorithmic and systems-level contributions. We propose ONI, a\ndistributed architecture that simultaneously learns an RL policy and an\nintrinsic reward function using LLM feedback. Our approach annotates the\nagent's collected experience via an asynchronous LLM server, which is then\ndistilled into an intrinsic reward model. We explore a range of algorithmic\nchoices for reward modeling with varying complexity, including hashing,\nclassification, and ranking models. By studying their relative tradeoffs, we\nshed light on questions regarding intrinsic reward design for sparse reward\nproblems. Our approach achieves state-of-the-art performance across a range of\nchallenging, sparse reward tasks from the NetHack Learning Environment in a\nsimple unified process, solely using the agent's gathered experience, without\nrequiring external datasets nor source code. We make our code available at\n\\url{URL} (coming soon).",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Automatically synthesizing dense rewards from natural language descriptions\nis a promising paradigm in reinforcement learning (RL), with applications to\nsparse reward problems, open-ended exploration, and hierarchical skill design.\nRecent works have made promising steps by exploiting the prior knowledge of\nlarge language models (LLMs). However, these approaches suffer from important\nlimitations: they are either not scalable to problems requiring billions of\nenvironment samples; or are limited to reward functions expressible by compact\ncode, which may require source code and have difficulty capturing nuanced\nsemantics; or require a diverse offline dataset, which may not exist or be\nimpossible to collect. In this work, we address these limitations through a\ncombination of algorithmic and systems-level contributions. We propose ONI, a\ndistributed architecture that simultaneously learns an RL policy and an\nintrinsic reward function using LLM feedback. Our approach annotates the\nagent's collected experience via an asynchronous LLM server, which is then\ndistilled into an intrinsic reward model. We explore a range of algorithmic\nchoices for reward modeling with varying complexity, including hashing,\nclassification, and ranking models. By studying their relative tradeoffs, we\nshed light on questions regarding intrinsic reward design for sparse reward\nproblems. Our approach achieves state-of-the-art performance across a range of\nchallenging, sparse reward tasks from the NetHack Learning Environment in a\nsimple unified process, solely using the agent's gathered experience, without\nrequiring external datasets nor source code. We make our code available at\n\\url{URL} (coming soon)."
                },
                "authors": [
                    {
                        "name": "Qinqing Zheng"
                    },
                    {
                        "name": "Mikael Henaff"
                    },
                    {
                        "name": "Amy Zhang"
                    },
                    {
                        "name": "Aditya Grover"
                    },
                    {
                        "name": "Brandon Amos"
                    }
                ],
                "author_detail": {
                    "name": "Brandon Amos"
                },
                "author": "Brandon Amos",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.23022v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.23022v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.19074v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.19074v2",
                "updated": "2024-10-30T13:52:39Z",
                "updated_parsed": [
                    2024,
                    10,
                    30,
                    13,
                    52,
                    39,
                    2,
                    304,
                    0
                ],
                "published": "2024-10-24T18:31:20Z",
                "published_parsed": [
                    2024,
                    10,
                    24,
                    18,
                    31,
                    20,
                    3,
                    298,
                    0
                ],
                "title": "A Generalized Framework for Multiscale State-Space Modeling with Nested\n  Nonlinear Dynamics: An Application to Bayesian Learning under Switching\n  Regimes",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Generalized Framework for Multiscale State-Space Modeling with Nested\n  Nonlinear Dynamics: An Application to Bayesian Learning under Switching\n  Regimes"
                },
                "summary": "In this work, we introduce a generalized framework for multiscale state-space\nmodeling that incorporates nested nonlinear dynamics, with a specific focus on\nBayesian learning under switching regimes. Our framework captures the complex\ninteractions between fast and slow processes within systems, allowing for the\nanalysis of how these dynamics influence each other across various temporal\nscales. We model these interactions through a hierarchical structure in which\nfiner time-scale dynamics are nested within coarser ones, while facilitating\nfeedback between the scales. To promote the practical application of our\nframework, we address the problem of identifying switching regimes and\ntransient dynamics. In particular, we develop a Bayesian learning approach to\nestimate latent states and indicators corresponding to switching dynamics,\nenabling the model to adapt effectively to regime changes. We employ Sequential\nMonte Carlo, or particle filtering, for inference. We illustrate the utility of\nour framework through simulations. The results demonstrate that our Bayesian\nlearning approach effectively tracks state transitions and achieves accurate\nidentification of switching dynamics in multiscale systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this work, we introduce a generalized framework for multiscale state-space\nmodeling that incorporates nested nonlinear dynamics, with a specific focus on\nBayesian learning under switching regimes. Our framework captures the complex\ninteractions between fast and slow processes within systems, allowing for the\nanalysis of how these dynamics influence each other across various temporal\nscales. We model these interactions through a hierarchical structure in which\nfiner time-scale dynamics are nested within coarser ones, while facilitating\nfeedback between the scales. To promote the practical application of our\nframework, we address the problem of identifying switching regimes and\ntransient dynamics. In particular, we develop a Bayesian learning approach to\nestimate latent states and indicators corresponding to switching dynamics,\nenabling the model to adapt effectively to regime changes. We employ Sequential\nMonte Carlo, or particle filtering, for inference. We illustrate the utility of\nour framework through simulations. The results demonstrate that our Bayesian\nlearning approach effectively tracks state transitions and achieves accurate\nidentification of switching dynamics in multiscale systems."
                },
                "authors": [
                    {
                        "name": "Nayely Vélez-Cruz"
                    },
                    {
                        "name": "Manfred D. Laubichler"
                    }
                ],
                "author_detail": {
                    "name": "Manfred D. Laubichler"
                },
                "author": "Manfred D. Laubichler",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.19074v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.19074v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ML",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2401.16727v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2401.16727v4",
                "updated": "2024-10-30T13:49:43Z",
                "updated_parsed": [
                    2024,
                    10,
                    30,
                    13,
                    49,
                    43,
                    2,
                    304,
                    0
                ],
                "published": "2024-01-30T03:51:44Z",
                "published_parsed": [
                    2024,
                    1,
                    30,
                    3,
                    51,
                    44,
                    1,
                    30,
                    0
                ],
                "title": "Recent Advances in Hate Speech Moderation: Multimodality and the Role of\n  Large Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent Advances in Hate Speech Moderation: Multimodality and the Role of\n  Large Models"
                },
                "summary": "In the evolving landscape of online communication, moderating hate speech\n(HS) presents an intricate challenge, compounded by the multimodal nature of\ndigital content. This comprehensive survey delves into the recent strides in HS\nmoderation, spotlighting the burgeoning role of large language models (LLMs)\nand large multimodal models (LMMs). Our exploration begins with a thorough\nanalysis of current literature, revealing the nuanced interplay between\ntextual, visual, and auditory elements in propagating HS. We uncover a notable\ntrend towards integrating these modalities, primarily due to the complexity and\nsubtlety with which HS is disseminated. A significant emphasis is placed on the\nadvances facilitated by LLMs and LMMs, which have begun to redefine the\nboundaries of detection and moderation capabilities. We identify existing gaps\nin research, particularly in the context of underrepresented languages and\ncultures, and the need for solutions to handle low-resource settings. The\nsurvey concludes with a forward-looking perspective, outlining potential\navenues for future research, including the exploration of novel AI\nmethodologies, the ethical governance of AI in moderation, and the development\nof more nuanced, context-aware systems. This comprehensive overview aims to\ncatalyze further research and foster a collaborative effort towards more\nsophisticated, responsible, and human-centric approaches to HS moderation in\nthe digital era. WARNING: This paper contains offensive examples.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In the evolving landscape of online communication, moderating hate speech\n(HS) presents an intricate challenge, compounded by the multimodal nature of\ndigital content. This comprehensive survey delves into the recent strides in HS\nmoderation, spotlighting the burgeoning role of large language models (LLMs)\nand large multimodal models (LMMs). Our exploration begins with a thorough\nanalysis of current literature, revealing the nuanced interplay between\ntextual, visual, and auditory elements in propagating HS. We uncover a notable\ntrend towards integrating these modalities, primarily due to the complexity and\nsubtlety with which HS is disseminated. A significant emphasis is placed on the\nadvances facilitated by LLMs and LMMs, which have begun to redefine the\nboundaries of detection and moderation capabilities. We identify existing gaps\nin research, particularly in the context of underrepresented languages and\ncultures, and the need for solutions to handle low-resource settings. The\nsurvey concludes with a forward-looking perspective, outlining potential\navenues for future research, including the exploration of novel AI\nmethodologies, the ethical governance of AI in moderation, and the development\nof more nuanced, context-aware systems. This comprehensive overview aims to\ncatalyze further research and foster a collaborative effort towards more\nsophisticated, responsible, and human-centric approaches to HS moderation in\nthe digital era. WARNING: This paper contains offensive examples."
                },
                "authors": [
                    {
                        "name": "Ming Shan Hee"
                    },
                    {
                        "name": "Shivam Sharma"
                    },
                    {
                        "name": "Rui Cao"
                    },
                    {
                        "name": "Palash Nandi"
                    },
                    {
                        "name": "Preslav Nakov"
                    },
                    {
                        "name": "Tanmoy Chakraborty"
                    },
                    {
                        "name": "Roy Ka-Wei Lee"
                    }
                ],
                "author_detail": {
                    "name": "Roy Ka-Wei Lee"
                },
                "author": "Roy Ka-Wei Lee",
                "arxiv_comment": "Accepted at EMNLP'24 (Findings)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2401.16727v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2401.16727v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.00552v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.00552v4",
                "updated": "2024-10-30T13:49:11Z",
                "updated_parsed": [
                    2024,
                    10,
                    30,
                    13,
                    49,
                    11,
                    2,
                    304,
                    0
                ],
                "published": "2024-05-01T14:50:58Z",
                "published_parsed": [
                    2024,
                    5,
                    1,
                    14,
                    50,
                    58,
                    2,
                    122,
                    0
                ],
                "title": "Long-Term Human Trajectory Prediction using 3D Dynamic Scene Graphs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Long-Term Human Trajectory Prediction using 3D Dynamic Scene Graphs"
                },
                "summary": "We present a novel approach for long-term human trajectory prediction in\nindoor human-centric environments, which is essential for long-horizon robot\nplanning in these environments. State-of-the-art human trajectory prediction\nmethods are limited by their focus on collision avoidance and short-term\nplanning, and their inability to model complex interactions of humans with the\nenvironment. In contrast, our approach overcomes these limitations by\npredicting sequences of human interactions with the environment and using this\ninformation to guide trajectory predictions over a horizon of up to 60s. We\nleverage Large Language Models (LLMs) to predict interactions with the\nenvironment by conditioning the LLM prediction on rich contextual information\nabout the scene. This information is given as a 3D Dynamic Scene Graph that\nencodes the geometry, semantics, and traversability of the environment into a\nhierarchical representation. We then ground these interaction sequences into\nmulti-modal spatio-temporal distributions over human positions using a\nprobabilistic approach based on continuous-time Markov Chains. To evaluate our\napproach, we introduce a new semi-synthetic dataset of long-term human\ntrajectories in complex indoor environments, which also includes annotations of\nhuman-object interactions. We show in thorough experimental evaluations that\nour approach achieves a 54% lower average negative log-likelihood and a 26.5%\nlower Best-of-20 displacement error compared to the best non-privileged (i.e.,\nevaluated in a zero-shot fashion on the dataset) baselines for a time horizon\nof 60s.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present a novel approach for long-term human trajectory prediction in\nindoor human-centric environments, which is essential for long-horizon robot\nplanning in these environments. State-of-the-art human trajectory prediction\nmethods are limited by their focus on collision avoidance and short-term\nplanning, and their inability to model complex interactions of humans with the\nenvironment. In contrast, our approach overcomes these limitations by\npredicting sequences of human interactions with the environment and using this\ninformation to guide trajectory predictions over a horizon of up to 60s. We\nleverage Large Language Models (LLMs) to predict interactions with the\nenvironment by conditioning the LLM prediction on rich contextual information\nabout the scene. This information is given as a 3D Dynamic Scene Graph that\nencodes the geometry, semantics, and traversability of the environment into a\nhierarchical representation. We then ground these interaction sequences into\nmulti-modal spatio-temporal distributions over human positions using a\nprobabilistic approach based on continuous-time Markov Chains. To evaluate our\napproach, we introduce a new semi-synthetic dataset of long-term human\ntrajectories in complex indoor environments, which also includes annotations of\nhuman-object interactions. We show in thorough experimental evaluations that\nour approach achieves a 54% lower average negative log-likelihood and a 26.5%\nlower Best-of-20 displacement error compared to the best non-privileged (i.e.,\nevaluated in a zero-shot fashion on the dataset) baselines for a time horizon\nof 60s."
                },
                "authors": [
                    {
                        "name": "Nicolas Gorlo"
                    },
                    {
                        "name": "Lukas Schmid"
                    },
                    {
                        "name": "Luca Carlone"
                    }
                ],
                "author_detail": {
                    "name": "Luca Carlone"
                },
                "author": "Luca Carlone",
                "arxiv_doi": "10.1109/LRA.2024.3482169",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1109/LRA.2024.3482169",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2405.00552v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.00552v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "8 pages, 6 figures. Accepted at IEEE Robotics and Automation Letters\n  (RA-L). Code released at: https://github.com/MIT-SPARK/LP2",
                "arxiv_journal_ref": "IEEE Robotics and Automation Letters, vol. 9, no. 12, pp.\n  10978-10985, Dec. 2024",
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.14515v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.14515v3",
                "updated": "2024-10-30T13:38:10Z",
                "updated_parsed": [
                    2024,
                    10,
                    30,
                    13,
                    38,
                    10,
                    2,
                    304,
                    0
                ],
                "published": "2024-06-20T17:26:01Z",
                "published_parsed": [
                    2024,
                    6,
                    20,
                    17,
                    26,
                    1,
                    3,
                    172,
                    0
                ],
                "title": "MMBench-Video: A Long-Form Multi-Shot Benchmark for Holistic Video\n  Understanding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MMBench-Video: A Long-Form Multi-Shot Benchmark for Holistic Video\n  Understanding"
                },
                "summary": "The advent of large vision-language models (LVLMs) has spurred research into\ntheir applications in multi-modal contexts, particularly in video\nunderstanding. Traditional VideoQA benchmarks, despite providing quantitative\nmetrics, often fail to encompass the full spectrum of video content and\ninadequately assess models' temporal comprehension. To address these\nlimitations, we introduce MMBench-Video, a quantitative benchmark designed to\nrigorously evaluate LVLMs' proficiency in video understanding. MMBench-Video\nincorporates lengthy videos from YouTube and employs free-form questions,\nmirroring practical use cases. The benchmark is meticulously crafted to probe\nthe models' temporal reasoning skills, with all questions human-annotated\naccording to a carefully constructed ability taxonomy. We employ GPT-4 for\nautomated assessment, demonstrating superior accuracy and robustness over\nearlier LLM-based evaluations. Utilizing MMBench-Video, we have conducted\ncomprehensive evaluations that include both proprietary and open-source LVLMs\nfor images and videos. MMBench-Video stands as a valuable resource for the\nresearch community, facilitating improved evaluation of LVLMs and catalyzing\nprogress in the field of video understanding. The evalutation code of\nMMBench-Video will be integrated into VLMEvalKit:\nhttps://github.com/open-compass/VLMEvalKit.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The advent of large vision-language models (LVLMs) has spurred research into\ntheir applications in multi-modal contexts, particularly in video\nunderstanding. Traditional VideoQA benchmarks, despite providing quantitative\nmetrics, often fail to encompass the full spectrum of video content and\ninadequately assess models' temporal comprehension. To address these\nlimitations, we introduce MMBench-Video, a quantitative benchmark designed to\nrigorously evaluate LVLMs' proficiency in video understanding. MMBench-Video\nincorporates lengthy videos from YouTube and employs free-form questions,\nmirroring practical use cases. The benchmark is meticulously crafted to probe\nthe models' temporal reasoning skills, with all questions human-annotated\naccording to a carefully constructed ability taxonomy. We employ GPT-4 for\nautomated assessment, demonstrating superior accuracy and robustness over\nearlier LLM-based evaluations. Utilizing MMBench-Video, we have conducted\ncomprehensive evaluations that include both proprietary and open-source LVLMs\nfor images and videos. MMBench-Video stands as a valuable resource for the\nresearch community, facilitating improved evaluation of LVLMs and catalyzing\nprogress in the field of video understanding. The evalutation code of\nMMBench-Video will be integrated into VLMEvalKit:\nhttps://github.com/open-compass/VLMEvalKit."
                },
                "authors": [
                    {
                        "name": "Xinyu Fang"
                    },
                    {
                        "name": "Kangrui Mao"
                    },
                    {
                        "name": "Haodong Duan"
                    },
                    {
                        "name": "Xiangyu Zhao"
                    },
                    {
                        "name": "Yining Li"
                    },
                    {
                        "name": "Dahua Lin"
                    },
                    {
                        "name": "Kai Chen"
                    }
                ],
                "author_detail": {
                    "name": "Kai Chen"
                },
                "author": "Kai Chen",
                "arxiv_comment": "Accepted in NeurIPS 2024 Datasets and Benchmarks Track",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.14515v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.14515v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.19044v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.19044v2",
                "updated": "2024-10-30T13:35:25Z",
                "updated_parsed": [
                    2024,
                    10,
                    30,
                    13,
                    35,
                    25,
                    2,
                    304,
                    0
                ],
                "published": "2024-06-27T09:53:14Z",
                "published_parsed": [
                    2024,
                    6,
                    27,
                    9,
                    53,
                    14,
                    3,
                    179,
                    0
                ],
                "title": "A Star Cluster Population of High Mass Black Hole Mergers in\n  Gravitational Wave Data",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Star Cluster Population of High Mass Black Hole Mergers in\n  Gravitational Wave Data"
                },
                "summary": "Stellar evolution theories predict a gap in the black hole birth mass\nspectrum as the result of pair instability processes in the cores of massive\nstars.This gap, however, is not seen in the binary black hole masses inferred\nfrom gravitational wave data. One explanation is that black holes form\ndynamically in dense star clusters where smaller black holes merge to form more\nmassive black holes, populating the mass gap. We show that this model predicts\na distribution of the effective and precessing spin parameters, $\\chi_{\\rm\neff}$ and $\\chi_{\\rm p}$, within the mass gap that is insensitive to\nassumptions about black hole natal spins and other astrophysical parameters. We\nanalyze the distribution of $\\chi_{\\rm eff}$ as a function of primary mass for\nthe black hole binaries in the third gravitational wave transient catalog. We\ninfer the presence of a high-mass and isotropically spinning population of\nblack holes that is consistent with hierarchical formation in dense star\nclusters and a pair-instability mass gap with a lower edge at $44^{+6}_{-4}\nM_\\odot$. We compute a Bayes factor $\\mathcal{B}>10^4$ relative to models that\ndo not allow for a high-mass population with a distinct $\\chi_{\\rm eff}$\ndistribution.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Stellar evolution theories predict a gap in the black hole birth mass\nspectrum as the result of pair instability processes in the cores of massive\nstars.This gap, however, is not seen in the binary black hole masses inferred\nfrom gravitational wave data. One explanation is that black holes form\ndynamically in dense star clusters where smaller black holes merge to form more\nmassive black holes, populating the mass gap. We show that this model predicts\na distribution of the effective and precessing spin parameters, $\\chi_{\\rm\neff}$ and $\\chi_{\\rm p}$, within the mass gap that is insensitive to\nassumptions about black hole natal spins and other astrophysical parameters. We\nanalyze the distribution of $\\chi_{\\rm eff}$ as a function of primary mass for\nthe black hole binaries in the third gravitational wave transient catalog. We\ninfer the presence of a high-mass and isotropically spinning population of\nblack holes that is consistent with hierarchical formation in dense star\nclusters and a pair-instability mass gap with a lower edge at $44^{+6}_{-4}\nM_\\odot$. We compute a Bayes factor $\\mathcal{B}>10^4$ relative to models that\ndo not allow for a high-mass population with a distinct $\\chi_{\\rm eff}$\ndistribution."
                },
                "authors": [
                    {
                        "name": "Fabio Antonini"
                    },
                    {
                        "name": "Isobel M. Romero-Shaw"
                    },
                    {
                        "name": "Thomas Callister"
                    }
                ],
                "author_detail": {
                    "name": "Thomas Callister"
                },
                "author": "Thomas Callister",
                "arxiv_comment": "Accepted by PRL",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.19044v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.19044v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.HE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.HE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "astro-ph.SR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.23005v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.23005v1",
                "updated": "2024-10-30T13:31:54Z",
                "updated_parsed": [
                    2024,
                    10,
                    30,
                    13,
                    31,
                    54,
                    2,
                    304,
                    0
                ],
                "published": "2024-10-30T13:31:54Z",
                "published_parsed": [
                    2024,
                    10,
                    30,
                    13,
                    31,
                    54,
                    2,
                    304,
                    0
                ],
                "title": "Improving Musical Accompaniment Co-creation via Diffusion Transformers",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Improving Musical Accompaniment Co-creation via Diffusion Transformers"
                },
                "summary": "Building upon Diff-A-Riff, a latent diffusion model for musical instrument\naccompaniment generation, we present a series of improvements targeting\nquality, diversity, inference speed, and text-driven control. First, we upgrade\nthe underlying autoencoder to a stereo-capable model with superior fidelity and\nreplace the latent U-Net with a Diffusion Transformer. Additionally, we refine\ntext prompting by training a cross-modality predictive network to translate\ntext-derived CLAP embeddings to audio-derived CLAP embeddings. Finally, we\nimprove inference speed by training the latent model using a consistency\nframework, achieving competitive quality with fewer denoising steps. Our model\nis evaluated against the original Diff-A-Riff variant using objective metrics\nin ablation experiments, demonstrating promising advancements in all targeted\nareas. Sound examples are available at:\nhttps://sonycslparis.github.io/improved_dar/.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Building upon Diff-A-Riff, a latent diffusion model for musical instrument\naccompaniment generation, we present a series of improvements targeting\nquality, diversity, inference speed, and text-driven control. First, we upgrade\nthe underlying autoencoder to a stereo-capable model with superior fidelity and\nreplace the latent U-Net with a Diffusion Transformer. Additionally, we refine\ntext prompting by training a cross-modality predictive network to translate\ntext-derived CLAP embeddings to audio-derived CLAP embeddings. Finally, we\nimprove inference speed by training the latent model using a consistency\nframework, achieving competitive quality with fewer denoising steps. Our model\nis evaluated against the original Diff-A-Riff variant using objective metrics\nin ablation experiments, demonstrating promising advancements in all targeted\nareas. Sound examples are available at:\nhttps://sonycslparis.github.io/improved_dar/."
                },
                "authors": [
                    {
                        "name": "Javier Nistal"
                    },
                    {
                        "name": "Marco Pasini"
                    },
                    {
                        "name": "Stefan Lattner"
                    }
                ],
                "author_detail": {
                    "name": "Stefan Lattner"
                },
                "author": "Stefan Lattner",
                "arxiv_comment": "5 pages; 1 table",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.23005v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.23005v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SD",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.23000v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.23000v2",
                "updated": "2024-10-31T03:04:28Z",
                "updated_parsed": [
                    2024,
                    10,
                    31,
                    3,
                    4,
                    28,
                    3,
                    305,
                    0
                ],
                "published": "2024-10-30T13:29:36Z",
                "published_parsed": [
                    2024,
                    10,
                    30,
                    13,
                    29,
                    36,
                    2,
                    304,
                    0
                ],
                "title": "Long$^2$RAG: Evaluating Long-Context & Long-Form Retrieval-Augmented\n  Generation with Key Point Recall",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Long$^2$RAG: Evaluating Long-Context & Long-Form Retrieval-Augmented\n  Generation with Key Point Recall"
                },
                "summary": "Retrieval-augmented generation (RAG) is a promising approach to address the\nlimitations of fixed knowledge in large language models (LLMs). However,\ncurrent benchmarks for evaluating RAG systems suffer from two key deficiencies:\n(1) they fail to adequately measure LLMs' capability in handling long-context\nretrieval due to a lack of datasets that reflect the characteristics of\nretrieved documents, and (2) they lack a comprehensive evaluation method for\nassessing LLMs' ability to generate long-form responses that effectively\nexploits retrieved information. To address these shortcomings, we introduce the\nLong$^2$RAG benchmark and the Key Point Recall (KPR) metric. Long$^2$RAG\ncomprises 280 questions spanning 10 domains and across 8 question categories,\neach associated with 5 retrieved documents with an average length of 2,444\nwords. KPR evaluates the extent to which LLMs incorporate key points extracted\nfrom the retrieved documents into their generated responses, providing a more\nnuanced assessment of their ability to exploit retrieved information.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Retrieval-augmented generation (RAG) is a promising approach to address the\nlimitations of fixed knowledge in large language models (LLMs). However,\ncurrent benchmarks for evaluating RAG systems suffer from two key deficiencies:\n(1) they fail to adequately measure LLMs' capability in handling long-context\nretrieval due to a lack of datasets that reflect the characteristics of\nretrieved documents, and (2) they lack a comprehensive evaluation method for\nassessing LLMs' ability to generate long-form responses that effectively\nexploits retrieved information. To address these shortcomings, we introduce the\nLong$^2$RAG benchmark and the Key Point Recall (KPR) metric. Long$^2$RAG\ncomprises 280 questions spanning 10 domains and across 8 question categories,\neach associated with 5 retrieved documents with an average length of 2,444\nwords. KPR evaluates the extent to which LLMs incorporate key points extracted\nfrom the retrieved documents into their generated responses, providing a more\nnuanced assessment of their ability to exploit retrieved information."
                },
                "authors": [
                    {
                        "name": "Zehan Qi"
                    },
                    {
                        "name": "Rongwu Xu"
                    },
                    {
                        "name": "Zhijiang Guo"
                    },
                    {
                        "name": "Cunxiang Wang"
                    },
                    {
                        "name": "Hao Zhang"
                    },
                    {
                        "name": "Wei Xu"
                    }
                ],
                "author_detail": {
                    "name": "Wei Xu"
                },
                "author": "Wei Xu",
                "arxiv_comment": "Accepted to EMNLP'24 (Findings). Camera-ready version",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.23000v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.23000v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.21554v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.21554v2",
                "updated": "2024-10-30T13:25:17Z",
                "updated_parsed": [
                    2024,
                    10,
                    30,
                    13,
                    25,
                    17,
                    2,
                    304,
                    0
                ],
                "published": "2024-10-28T21:31:50Z",
                "published_parsed": [
                    2024,
                    10,
                    28,
                    21,
                    31,
                    50,
                    0,
                    302,
                    0
                ],
                "title": "Information diffusion assumptions can distort our understanding of\n  social network dynamics",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Information diffusion assumptions can distort our understanding of\n  social network dynamics"
                },
                "summary": "To analyze the flow of information online, experts often rely on\nplatform-provided data from social media companies, which typically attribute\nall resharing actions to an original poster. This obscures the true dynamics of\nhow information spreads online, as users can be exposed to content in various\nways. While most researchers analyze data as it is provided by the platform and\noverlook this issue, some attempt to infer the structure of these information\ncascades. However, the absence of ground truth about actual diffusion cascades\nmakes verifying the efficacy of these efforts impossible. This study\ninvestigates the implications of the common practice of ignoring reconstruction\nall together. Two case studies involving data from Twitter and Bluesky reveal\nthat reconstructing cascades significantly alters the identification of\ninfluential users, therefore affecting downstream analyses in general. We also\npropose a novel reconstruction approach that allows us to evaluate the effects\nof different assumptions made during the cascade inference procedure. Analysis\nof the diffusion of over 40,000 true and false news stories on Twitter reveals\nthat the assumptions made during the reconstruction procedure drastically\ndistort both microscopic and macroscopic properties of cascade networks. This\nwork highlights the challenges of studying information spreading processes on\ncomplex networks and has significant implications for the broader study of\ndigital platforms.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "To analyze the flow of information online, experts often rely on\nplatform-provided data from social media companies, which typically attribute\nall resharing actions to an original poster. This obscures the true dynamics of\nhow information spreads online, as users can be exposed to content in various\nways. While most researchers analyze data as it is provided by the platform and\noverlook this issue, some attempt to infer the structure of these information\ncascades. However, the absence of ground truth about actual diffusion cascades\nmakes verifying the efficacy of these efforts impossible. This study\ninvestigates the implications of the common practice of ignoring reconstruction\nall together. Two case studies involving data from Twitter and Bluesky reveal\nthat reconstructing cascades significantly alters the identification of\ninfluential users, therefore affecting downstream analyses in general. We also\npropose a novel reconstruction approach that allows us to evaluate the effects\nof different assumptions made during the cascade inference procedure. Analysis\nof the diffusion of over 40,000 true and false news stories on Twitter reveals\nthat the assumptions made during the reconstruction procedure drastically\ndistort both microscopic and macroscopic properties of cascade networks. This\nwork highlights the challenges of studying information spreading processes on\ncomplex networks and has significant implications for the broader study of\ndigital platforms."
                },
                "authors": [
                    {
                        "name": "Matthew R. DeVerna"
                    },
                    {
                        "name": "Francesco Pierri"
                    },
                    {
                        "name": "Rachith Aiyappa"
                    },
                    {
                        "name": "Diogo Pacheco"
                    },
                    {
                        "name": "John Bryden"
                    },
                    {
                        "name": "Filippo Menczer"
                    }
                ],
                "author_detail": {
                    "name": "Filippo Menczer"
                },
                "author": "Filippo Menczer",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.21554v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.21554v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2403.13488v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2403.13488v2",
                "updated": "2024-10-30T13:24:30Z",
                "updated_parsed": [
                    2024,
                    10,
                    30,
                    13,
                    24,
                    30,
                    2,
                    304,
                    0
                ],
                "published": "2024-03-20T10:45:44Z",
                "published_parsed": [
                    2024,
                    3,
                    20,
                    10,
                    45,
                    44,
                    2,
                    80,
                    0
                ],
                "title": "The DeepJoint algorithm: An innovative approach for studying the\n  longitudinal evolution of quantitative mammographic density and its\n  association with screen-detected breast cancer risk",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The DeepJoint algorithm: An innovative approach for studying the\n  longitudinal evolution of quantitative mammographic density and its\n  association with screen-detected breast cancer risk"
                },
                "summary": "Mammographic density is a dynamic risk factor for breast cancer and affects\nthe sensitivity of mammography-based screening. While automated machine and\ndeep learning-based methods provide more consistent and precise measurements\ncompared to subjective BI-RADS assessments, they often fail to account for the\nlongitudinal evolution of density. Many of these methods assess mammographic\ndensity in a cross-sectional manner, overlooking correlations in repeated\nmeasures, irregular visit intervals, missing data, and informative dropouts.\nJoint models, however, are well-suited for capturing the longitudinal\nrelationship between biomarkers and survival outcomes. We present the DeepJoint\nalgorithm, an open-source solution that integrates deep learning for\nquantitative mammographic density estimation with joint modeling to assess the\nlongitudinal relationship between mammographic density and breast cancer risk.\nOur method efficiently analyzes processed mammograms from various\nmanufacturers, estimating both dense area and percent density--established risk\nfactors for breast cancer. We utilize a joint model to explore their\nassociation with breast cancer risk and provide individualized risk\npredictions. Bayesian inference and the Monte Carlo consensus algorithm make\nthe approach reliable for large screening datasets. Our method allows for\naccurate analysis of processed mammograms from multiple manufacturers, offering\na comprehensive view of breast cancer risk based on individual longitudinal\ndensity profiles. The complete pipeline is publicly available, promoting\nbroader application and comparison with other methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mammographic density is a dynamic risk factor for breast cancer and affects\nthe sensitivity of mammography-based screening. While automated machine and\ndeep learning-based methods provide more consistent and precise measurements\ncompared to subjective BI-RADS assessments, they often fail to account for the\nlongitudinal evolution of density. Many of these methods assess mammographic\ndensity in a cross-sectional manner, overlooking correlations in repeated\nmeasures, irregular visit intervals, missing data, and informative dropouts.\nJoint models, however, are well-suited for capturing the longitudinal\nrelationship between biomarkers and survival outcomes. We present the DeepJoint\nalgorithm, an open-source solution that integrates deep learning for\nquantitative mammographic density estimation with joint modeling to assess the\nlongitudinal relationship between mammographic density and breast cancer risk.\nOur method efficiently analyzes processed mammograms from various\nmanufacturers, estimating both dense area and percent density--established risk\nfactors for breast cancer. We utilize a joint model to explore their\nassociation with breast cancer risk and provide individualized risk\npredictions. Bayesian inference and the Monte Carlo consensus algorithm make\nthe approach reliable for large screening datasets. Our method allows for\naccurate analysis of processed mammograms from multiple manufacturers, offering\na comprehensive view of breast cancer risk based on individual longitudinal\ndensity profiles. The complete pipeline is publicly available, promoting\nbroader application and comparison with other methods."
                },
                "authors": [
                    {
                        "name": "Manel Rakez"
                    },
                    {
                        "name": "Julien Guillaumin"
                    },
                    {
                        "name": "Aurelien Chick"
                    },
                    {
                        "name": "Gaelle Coureau"
                    },
                    {
                        "name": "Foucauld Chamming's"
                    },
                    {
                        "name": "Pierre Fillard"
                    },
                    {
                        "name": "Brice Amadeo"
                    },
                    {
                        "name": "Virginie Rondeau"
                    }
                ],
                "author_detail": {
                    "name": "Virginie Rondeau"
                },
                "author": "Virginie Rondeau",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2403.13488v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2403.13488v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.AP",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.AP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.22997v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.22997v1",
                "updated": "2024-10-30T13:22:55Z",
                "updated_parsed": [
                    2024,
                    10,
                    30,
                    13,
                    22,
                    55,
                    2,
                    304,
                    0
                ],
                "published": "2024-10-30T13:22:55Z",
                "published_parsed": [
                    2024,
                    10,
                    30,
                    13,
                    22,
                    55,
                    2,
                    304,
                    0
                ],
                "title": "A Comparison of Prompt Engineering Techniques for Task Planning and\n  Execution in Service Robotics",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Comparison of Prompt Engineering Techniques for Task Planning and\n  Execution in Service Robotics"
                },
                "summary": "Recent advances in LLM have been instrumental in autonomous robot control and\nhuman-robot interaction by leveraging their vast general knowledge and\ncapabilities to understand and reason across a wide range of tasks and\nscenarios. Previous works have investigated various prompt engineering\ntechniques for improving the performance of \\glspl{LLM} to accomplish tasks,\nwhile others have proposed methods that utilize LLMs to plan and execute tasks\nbased on the available functionalities of a given robot platform. In this work,\nwe consider both lines of research by comparing prompt engineering techniques\nand combinations thereof within the application of high-level task planning and\nexecution in service robotics. We define a diverse set of tasks and a simple\nset of functionalities in simulation, and measure task completion accuracy and\nexecution time for several state-of-the-art models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advances in LLM have been instrumental in autonomous robot control and\nhuman-robot interaction by leveraging their vast general knowledge and\ncapabilities to understand and reason across a wide range of tasks and\nscenarios. Previous works have investigated various prompt engineering\ntechniques for improving the performance of \\glspl{LLM} to accomplish tasks,\nwhile others have proposed methods that utilize LLMs to plan and execute tasks\nbased on the available functionalities of a given robot platform. In this work,\nwe consider both lines of research by comparing prompt engineering techniques\nand combinations thereof within the application of high-level task planning and\nexecution in service robotics. We define a diverse set of tasks and a simple\nset of functionalities in simulation, and measure task completion accuracy and\nexecution time for several state-of-the-art models."
                },
                "authors": [
                    {
                        "name": "Jonas Bode"
                    },
                    {
                        "name": "Bastian Pätzold"
                    },
                    {
                        "name": "Raphael Memmesheimer"
                    },
                    {
                        "name": "Sven Behnke"
                    }
                ],
                "author_detail": {
                    "name": "Sven Behnke"
                },
                "author": "Sven Behnke",
                "arxiv_comment": "6 pages, 3 figures, 2 tables, to be published in the 2024 IEEE-RAS\n  International Conference on Humanoid Robots, We make our code, including all\n  prompts, available at https://github.com/AIS-Bonn/Prompt_Engineering",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.22997v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.22997v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.22995v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.22995v1",
                "updated": "2024-10-30T13:19:44Z",
                "updated_parsed": [
                    2024,
                    10,
                    30,
                    13,
                    19,
                    44,
                    2,
                    304,
                    0
                ],
                "published": "2024-10-30T13:19:44Z",
                "published_parsed": [
                    2024,
                    10,
                    30,
                    13,
                    19,
                    44,
                    2,
                    304,
                    0
                ],
                "title": "VisAidMath: Benchmarking Visual-Aided Mathematical Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "VisAidMath: Benchmarking Visual-Aided Mathematical Reasoning"
                },
                "summary": "Although previous research on large language models (LLMs) and large\nmulti-modal models (LMMs) has systematically explored mathematical\nproblem-solving (MPS) within visual contexts, the analysis of how these models\nprocess visual information during problem-solving remains insufficient. To\naddress this gap, we present VisAidMath, a benchmark for evaluating the MPS\nprocess related to visual information. We follow a rigorous data curation\npipeline involving both automated processes and manual annotations to ensure\ndata quality and reliability. Consequently, this benchmark includes 1,200\nchallenging problems from various mathematical branches, vision-aid\nformulations, and difficulty levels, collected from diverse sources such as\ntextbooks, examination papers, and Olympiad problems. Based on the proposed\nbenchmark, we conduct comprehensive evaluations on ten mainstream LLMs and\nLMMs, highlighting deficiencies in the visual-aided reasoning process. For\nexample, GPT-4V only achieves 45.33% accuracy in the visual-aided reasoning\ntask, even with a drop of 2 points when provided with golden visual aids.\nIn-depth analysis reveals that the main cause of deficiencies lies in\nhallucination regarding the implicit visual reasoning process, shedding light\non future research directions in the visual-aided MPS process.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Although previous research on large language models (LLMs) and large\nmulti-modal models (LMMs) has systematically explored mathematical\nproblem-solving (MPS) within visual contexts, the analysis of how these models\nprocess visual information during problem-solving remains insufficient. To\naddress this gap, we present VisAidMath, a benchmark for evaluating the MPS\nprocess related to visual information. We follow a rigorous data curation\npipeline involving both automated processes and manual annotations to ensure\ndata quality and reliability. Consequently, this benchmark includes 1,200\nchallenging problems from various mathematical branches, vision-aid\nformulations, and difficulty levels, collected from diverse sources such as\ntextbooks, examination papers, and Olympiad problems. Based on the proposed\nbenchmark, we conduct comprehensive evaluations on ten mainstream LLMs and\nLMMs, highlighting deficiencies in the visual-aided reasoning process. For\nexample, GPT-4V only achieves 45.33% accuracy in the visual-aided reasoning\ntask, even with a drop of 2 points when provided with golden visual aids.\nIn-depth analysis reveals that the main cause of deficiencies lies in\nhallucination regarding the implicit visual reasoning process, shedding light\non future research directions in the visual-aided MPS process."
                },
                "authors": [
                    {
                        "name": "Jingkun Ma"
                    },
                    {
                        "name": "Runzhe Zhan"
                    },
                    {
                        "name": "Derek F. Wong"
                    },
                    {
                        "name": "Yang Li"
                    },
                    {
                        "name": "Di Sun"
                    },
                    {
                        "name": "Hou Pong Chan"
                    },
                    {
                        "name": "Lidia S. Chao"
                    }
                ],
                "author_detail": {
                    "name": "Lidia S. Chao"
                },
                "author": "Lidia S. Chao",
                "arxiv_comment": "58 pages, 28 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.22995v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.22995v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.00649v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.00649v2",
                "updated": "2024-10-30T13:18:41Z",
                "updated_parsed": [
                    2024,
                    10,
                    30,
                    13,
                    18,
                    41,
                    2,
                    304,
                    0
                ],
                "published": "2024-06-30T10:21:41Z",
                "published_parsed": [
                    2024,
                    6,
                    30,
                    10,
                    21,
                    41,
                    6,
                    182,
                    0
                ],
                "title": "Particle Semi-Implicit Variational Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Particle Semi-Implicit Variational Inference"
                },
                "summary": "Semi-implicit variational inference (SIVI) enriches the expressiveness of\nvariational families by utilizing a kernel and a mixing distribution to\nhierarchically define the variational distribution. Existing SIVI methods\nparameterize the mixing distribution using implicit distributions, leading to\nintractable variational densities. As a result, directly maximizing the\nevidence lower bound (ELBO) is not possible, so they resort to one of the\nfollowing: optimizing bounds on the ELBO, employing costly inner-loop Markov\nchain Monte Carlo runs, or solving minimax objectives. In this paper, we\npropose a novel method for SIVI called Particle Variational Inference (PVI)\nwhich employs empirical measures to approximate the optimal mixing\ndistributions characterized as the minimizer of a free energy functional. PVI\narises naturally as a particle approximation of a Euclidean--Wasserstein\ngradient flow and, unlike prior works, it directly optimizes the ELBO whilst\nmaking no parametric assumption about the mixing distribution. Our empirical\nresults demonstrate that PVI performs favourably compared to other SIVI methods\nacross various tasks. Moreover, we provide a theoretical analysis of the\nbehaviour of the gradient flow of a related free energy functional:\nestablishing the existence and uniqueness of solutions as well as propagation\nof chaos results.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Semi-implicit variational inference (SIVI) enriches the expressiveness of\nvariational families by utilizing a kernel and a mixing distribution to\nhierarchically define the variational distribution. Existing SIVI methods\nparameterize the mixing distribution using implicit distributions, leading to\nintractable variational densities. As a result, directly maximizing the\nevidence lower bound (ELBO) is not possible, so they resort to one of the\nfollowing: optimizing bounds on the ELBO, employing costly inner-loop Markov\nchain Monte Carlo runs, or solving minimax objectives. In this paper, we\npropose a novel method for SIVI called Particle Variational Inference (PVI)\nwhich employs empirical measures to approximate the optimal mixing\ndistributions characterized as the minimizer of a free energy functional. PVI\narises naturally as a particle approximation of a Euclidean--Wasserstein\ngradient flow and, unlike prior works, it directly optimizes the ELBO whilst\nmaking no parametric assumption about the mixing distribution. Our empirical\nresults demonstrate that PVI performs favourably compared to other SIVI methods\nacross various tasks. Moreover, we provide a theoretical analysis of the\nbehaviour of the gradient flow of a related free energy functional:\nestablishing the existence and uniqueness of solutions as well as propagation\nof chaos results."
                },
                "authors": [
                    {
                        "name": "Jen Ning Lim"
                    },
                    {
                        "name": "Adam M. Johansen"
                    }
                ],
                "author_detail": {
                    "name": "Adam M. Johansen"
                },
                "author": "Adam M. Johansen",
                "arxiv_comment": "NeurIPS 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.00649v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.00649v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ML",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.21980v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.21980v2",
                "updated": "2024-10-30T13:13:52Z",
                "updated_parsed": [
                    2024,
                    10,
                    30,
                    13,
                    13,
                    52,
                    2,
                    304,
                    0
                ],
                "published": "2024-10-29T12:10:20Z",
                "published_parsed": [
                    2024,
                    10,
                    29,
                    12,
                    10,
                    20,
                    1,
                    303,
                    0
                ],
                "title": "Impact of cosmology dependence of baryonic feedback in weak lensing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Impact of cosmology dependence of baryonic feedback in weak lensing"
                },
                "summary": "Robust modeling of non-linear scales is critical for accurate cosmological\ninference in Stage IV surveys. For weak lensing analyses in particular, a key\nchallenge arises from the incomplete understanding of how non-gravitational\nprocesses, such as supernovae and active galactic nuclei - collectively known\nas baryonic feedback - affect the matter distribution. Several existing methods\nfor modeling baryonic feedback treat it independently from the underlying\ncosmology, an assumption which has been found to be inaccurate by\nhydrodynamical simulations. In this work, we examine the impact of this\ncoupling between baryonic feedback and cosmology on parameter inference at LSST\nY1 precision. We build mock 3$\\times$2pt data vectors using the Magneticum\nsuite of hydrodynamical simulations, which span a wide range of cosmologies\nwhile keeping subgrid parameters fixed. We perform simulated likelihood\nanalyses for two baryon mitigation techniques: (i) the Principal Component\nAnalysis (PCA) method which identifies eigenmodes for capturing the effect\nbaryonic feedback on the data vector and (ii) HMCode2020 (Mead et al. 2021)\nwhich analytically models the modification in the matter distribution using a\nhalo model approach. Our results show that the PCA method is robust to the\ncoupling between cosmology and baryonic feedback, whereas, when using\nHMCode2020 there can be up to $0.5\\sigma$ bias in $\\Omega_\\text{m}$-$S_8$. For\nHMCode2020, the bias also correlates with the input cosmology while for PCA we\nfind no such correlation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Robust modeling of non-linear scales is critical for accurate cosmological\ninference in Stage IV surveys. For weak lensing analyses in particular, a key\nchallenge arises from the incomplete understanding of how non-gravitational\nprocesses, such as supernovae and active galactic nuclei - collectively known\nas baryonic feedback - affect the matter distribution. Several existing methods\nfor modeling baryonic feedback treat it independently from the underlying\ncosmology, an assumption which has been found to be inaccurate by\nhydrodynamical simulations. In this work, we examine the impact of this\ncoupling between baryonic feedback and cosmology on parameter inference at LSST\nY1 precision. We build mock 3$\\times$2pt data vectors using the Magneticum\nsuite of hydrodynamical simulations, which span a wide range of cosmologies\nwhile keeping subgrid parameters fixed. We perform simulated likelihood\nanalyses for two baryon mitigation techniques: (i) the Principal Component\nAnalysis (PCA) method which identifies eigenmodes for capturing the effect\nbaryonic feedback on the data vector and (ii) HMCode2020 (Mead et al. 2021)\nwhich analytically models the modification in the matter distribution using a\nhalo model approach. Our results show that the PCA method is robust to the\ncoupling between cosmology and baryonic feedback, whereas, when using\nHMCode2020 there can be up to $0.5\\sigma$ bias in $\\Omega_\\text{m}$-$S_8$. For\nHMCode2020, the bias also correlates with the input cosmology while for PCA we\nfind no such correlation."
                },
                "authors": [
                    {
                        "name": "Pranjal R. S."
                    },
                    {
                        "name": "Elisabeth Krause"
                    },
                    {
                        "name": "Klaus Dolag"
                    },
                    {
                        "name": "Karim Benabed"
                    },
                    {
                        "name": "Tim Eifler"
                    },
                    {
                        "name": "Emma Ayçoberry"
                    },
                    {
                        "name": "Yohan Dubois"
                    }
                ],
                "author_detail": {
                    "name": "Yohan Dubois"
                },
                "author": "Yohan Dubois",
                "arxiv_comment": "correction to acknowledgements",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.21980v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.21980v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.CO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.CO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.22980v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.22980v1",
                "updated": "2024-10-30T12:45:12Z",
                "updated_parsed": [
                    2024,
                    10,
                    30,
                    12,
                    45,
                    12,
                    2,
                    304,
                    0
                ],
                "published": "2024-10-30T12:45:12Z",
                "published_parsed": [
                    2024,
                    10,
                    30,
                    12,
                    45,
                    12,
                    2,
                    304,
                    0
                ],
                "title": "Efficient End-to-End 6-Dof Grasp Detection Framework for Edge Devices\n  with Hierarchical Heatmaps and Feature Propagation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient End-to-End 6-Dof Grasp Detection Framework for Edge Devices\n  with Hierarchical Heatmaps and Feature Propagation"
                },
                "summary": "6-DoF grasp detection is critically important for the advancement of\nintelligent embodied systems, as it provides feasible robot poses for object\ngrasping. Various methods have been proposed to detect 6-DoF grasps through the\nextraction of 3D geometric features from RGBD or point cloud data. However,\nmost of these approaches encounter challenges during real robot deployment due\nto their significant computational demands, which can be particularly\nproblematic for mobile robot platforms, especially those reliant on edge\ncomputing devices. This paper presents an Efficient End-to-End Grasp Detection\nNetwork (E3GNet) for 6-DoF grasp detection utilizing hierarchical heatmap\nrepresentations. E3GNet effectively identifies high-quality and diverse grasps\nin cluttered real-world environments. Benefiting from our end-to-end\nmethodology and efficient network design, our approach surpasses previous\nmethods in model inference efficiency and achieves real-time 6-Dof grasp\ndetection on edge devices. Furthermore, real-world experiments validate the\neffectiveness of our method, achieving a satisfactory 94% object grasping\nsuccess rate.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "6-DoF grasp detection is critically important for the advancement of\nintelligent embodied systems, as it provides feasible robot poses for object\ngrasping. Various methods have been proposed to detect 6-DoF grasps through the\nextraction of 3D geometric features from RGBD or point cloud data. However,\nmost of these approaches encounter challenges during real robot deployment due\nto their significant computational demands, which can be particularly\nproblematic for mobile robot platforms, especially those reliant on edge\ncomputing devices. This paper presents an Efficient End-to-End Grasp Detection\nNetwork (E3GNet) for 6-DoF grasp detection utilizing hierarchical heatmap\nrepresentations. E3GNet effectively identifies high-quality and diverse grasps\nin cluttered real-world environments. Benefiting from our end-to-end\nmethodology and efficient network design, our approach surpasses previous\nmethods in model inference efficiency and achieves real-time 6-Dof grasp\ndetection on edge devices. Furthermore, real-world experiments validate the\neffectiveness of our method, achieving a satisfactory 94% object grasping\nsuccess rate."
                },
                "authors": [
                    {
                        "name": "Kaiqin Yang. Yixiang Dai"
                    },
                    {
                        "name": "Guijin Wang"
                    },
                    {
                        "name": "Siang Chen"
                    }
                ],
                "author_detail": {
                    "name": "Siang Chen"
                },
                "author": "Siang Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.22980v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.22980v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.22977v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.22977v1",
                "updated": "2024-10-30T12:42:38Z",
                "updated_parsed": [
                    2024,
                    10,
                    30,
                    12,
                    42,
                    38,
                    2,
                    304,
                    0
                ],
                "published": "2024-10-30T12:42:38Z",
                "published_parsed": [
                    2024,
                    10,
                    30,
                    12,
                    42,
                    38,
                    2,
                    304,
                    0
                ],
                "title": "Bonafide at LegalLens 2024 Shared Task: Using Lightweight DeBERTa Based\n  Encoder For Legal Violation Detection and Resolution",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Bonafide at LegalLens 2024 Shared Task: Using Lightweight DeBERTa Based\n  Encoder For Legal Violation Detection and Resolution"
                },
                "summary": "In this work, we present two systems -- Named Entity Resolution (NER) and\nNatural Language Inference (NLI) -- for detecting legal violations within\nunstructured textual data and for associating these violations with potentially\naffected individuals, respectively. Both these systems are lightweight DeBERTa\nbased encoders that outperform the LLM baselines. The proposed NER system\nachieved an F1 score of 60.01\\% on Subtask A of the LegalLens challenge, which\nfocuses on identifying violations. The proposed NLI system achieved an F1 score\nof 84.73\\% on Subtask B of the LegalLens challenge, which focuses on resolving\nthese violations by matching them with pre-existing legal complaints of class\naction cases. Our NER system ranked sixth and NLI system ranked fifth on the\nLegalLens leaderboard. We release the trained models and inference scripts.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this work, we present two systems -- Named Entity Resolution (NER) and\nNatural Language Inference (NLI) -- for detecting legal violations within\nunstructured textual data and for associating these violations with potentially\naffected individuals, respectively. Both these systems are lightweight DeBERTa\nbased encoders that outperform the LLM baselines. The proposed NER system\nachieved an F1 score of 60.01\\% on Subtask A of the LegalLens challenge, which\nfocuses on identifying violations. The proposed NLI system achieved an F1 score\nof 84.73\\% on Subtask B of the LegalLens challenge, which focuses on resolving\nthese violations by matching them with pre-existing legal complaints of class\naction cases. Our NER system ranked sixth and NLI system ranked fifth on the\nLegalLens leaderboard. We release the trained models and inference scripts."
                },
                "authors": [
                    {
                        "name": "Shikha Bordia"
                    }
                ],
                "author_detail": {
                    "name": "Shikha Bordia"
                },
                "author": "Shikha Bordia",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.22977v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.22977v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.22971v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.22971v1",
                "updated": "2024-10-30T12:38:49Z",
                "updated_parsed": [
                    2024,
                    10,
                    30,
                    12,
                    38,
                    49,
                    2,
                    304,
                    0
                ],
                "published": "2024-10-30T12:38:49Z",
                "published_parsed": [
                    2024,
                    10,
                    30,
                    12,
                    38,
                    49,
                    2,
                    304,
                    0
                ],
                "title": "Private Synthetic Text Generation with Diffusion Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Private Synthetic Text Generation with Diffusion Models"
                },
                "summary": "How capable are diffusion models of generating synthetics texts? Recent\nresearch shows their strengths, with performance reaching that of\nauto-regressive LLMs. But are they also good in generating synthetic data if\nthe training was under differential privacy? Here the evidence is missing, yet\nthe promises from private image generation look strong. In this paper we\naddress this open question by extensive experiments. At the same time, we\ncritically assess (and reimplement) previous works on synthetic private text\ngeneration with LLMs and reveal some unmet assumptions that might have led to\nviolating the differential privacy guarantees. Our results partly contradict\nprevious non-private findings and show that fully open-source LLMs outperform\ndiffusion models in the privacy regime. Our complete source codes, datasets,\nand experimental setup is publicly available to foster future research.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "How capable are diffusion models of generating synthetics texts? Recent\nresearch shows their strengths, with performance reaching that of\nauto-regressive LLMs. But are they also good in generating synthetic data if\nthe training was under differential privacy? Here the evidence is missing, yet\nthe promises from private image generation look strong. In this paper we\naddress this open question by extensive experiments. At the same time, we\ncritically assess (and reimplement) previous works on synthetic private text\ngeneration with LLMs and reveal some unmet assumptions that might have led to\nviolating the differential privacy guarantees. Our results partly contradict\nprevious non-private findings and show that fully open-source LLMs outperform\ndiffusion models in the privacy regime. Our complete source codes, datasets,\nand experimental setup is publicly available to foster future research."
                },
                "authors": [
                    {
                        "name": "Sebastian Ochs"
                    },
                    {
                        "name": "Ivan Habernal"
                    }
                ],
                "author_detail": {
                    "name": "Ivan Habernal"
                },
                "author": "Ivan Habernal",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.22971v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.22971v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.22959v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.22959v1",
                "updated": "2024-10-30T12:16:35Z",
                "updated_parsed": [
                    2024,
                    10,
                    30,
                    12,
                    16,
                    35,
                    2,
                    304,
                    0
                ],
                "published": "2024-10-30T12:16:35Z",
                "published_parsed": [
                    2024,
                    10,
                    30,
                    12,
                    16,
                    35,
                    2,
                    304,
                    0
                ],
                "title": "EnsIR: An Ensemble Algorithm for Image Restoration via Gaussian Mixture\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "EnsIR: An Ensemble Algorithm for Image Restoration via Gaussian Mixture\n  Models"
                },
                "summary": "Image restoration has experienced significant advancements due to the\ndevelopment of deep learning. Nevertheless, it encounters challenges related to\nill-posed problems, resulting in deviations between single model predictions\nand ground-truths. Ensemble learning, as a powerful machine learning technique,\naims to address these deviations by combining the predictions of multiple base\nmodels. Most existing works adopt ensemble learning during the design of\nrestoration models, while only limited research focuses on the inference-stage\nensemble of pre-trained restoration models. Regression-based methods fail to\nenable efficient inference, leading researchers in academia and industry to\nprefer averaging as their choice for post-training ensemble. To address this,\nwe reformulate the ensemble problem of image restoration into Gaussian mixture\nmodels (GMMs) and employ an expectation maximization (EM)-based algorithm to\nestimate ensemble weights for aggregating prediction candidates. We estimate\nthe range-wise ensemble weights on a reference set and store them in a lookup\ntable (LUT) for efficient ensemble inference on the test set. Our algorithm is\nmodel-agnostic and training-free, allowing seamless integration and enhancement\nof various pre-trained image restoration models. It consistently outperforms\nregression based methods and averaging ensemble approaches on 14 benchmarks\nacross 3 image restoration tasks, including super-resolution, deblurring and\nderaining. The codes and all estimated weights have been released in Github.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Image restoration has experienced significant advancements due to the\ndevelopment of deep learning. Nevertheless, it encounters challenges related to\nill-posed problems, resulting in deviations between single model predictions\nand ground-truths. Ensemble learning, as a powerful machine learning technique,\naims to address these deviations by combining the predictions of multiple base\nmodels. Most existing works adopt ensemble learning during the design of\nrestoration models, while only limited research focuses on the inference-stage\nensemble of pre-trained restoration models. Regression-based methods fail to\nenable efficient inference, leading researchers in academia and industry to\nprefer averaging as their choice for post-training ensemble. To address this,\nwe reformulate the ensemble problem of image restoration into Gaussian mixture\nmodels (GMMs) and employ an expectation maximization (EM)-based algorithm to\nestimate ensemble weights for aggregating prediction candidates. We estimate\nthe range-wise ensemble weights on a reference set and store them in a lookup\ntable (LUT) for efficient ensemble inference on the test set. Our algorithm is\nmodel-agnostic and training-free, allowing seamless integration and enhancement\nof various pre-trained image restoration models. It consistently outperforms\nregression based methods and averaging ensemble approaches on 14 benchmarks\nacross 3 image restoration tasks, including super-resolution, deblurring and\nderaining. The codes and all estimated weights have been released in Github."
                },
                "authors": [
                    {
                        "name": "Shangquan Sun"
                    },
                    {
                        "name": "Wenqi Ren"
                    },
                    {
                        "name": "Zikun Liu"
                    },
                    {
                        "name": "Hyunhee Park"
                    },
                    {
                        "name": "Rui Wang"
                    },
                    {
                        "name": "Xiaochun Cao"
                    }
                ],
                "author_detail": {
                    "name": "Xiaochun Cao"
                },
                "author": "Xiaochun Cao",
                "arxiv_comment": "10 pages for main manuscript, additional 17 pages for appendix, 18\n  figures, 17MB",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.22959v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.22959v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.12220v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.12220v2",
                "updated": "2024-10-30T12:14:35Z",
                "updated_parsed": [
                    2024,
                    10,
                    30,
                    12,
                    14,
                    35,
                    2,
                    304,
                    0
                ],
                "published": "2024-07-17T00:06:30Z",
                "published_parsed": [
                    2024,
                    7,
                    17,
                    0,
                    6,
                    30,
                    2,
                    199,
                    0
                ],
                "title": "Questionable practices in machine learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Questionable practices in machine learning"
                },
                "summary": "Evaluating modern ML models is hard. The strong incentive for researchers and\ncompanies to report a state-of-the-art result on some metric often leads to\nquestionable research practices (QRPs): bad practices which fall short of\noutright research fraud. We describe 44 such practices which can undermine\nreported results, giving examples where possible. Our list emphasises the\nevaluation of large language models (LLMs) on public benchmarks. We also\ndiscuss \"irreproducible research practices\", i.e. decisions that make it\ndifficult or impossible for other researchers to reproduce, build on or audit\nprevious research.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Evaluating modern ML models is hard. The strong incentive for researchers and\ncompanies to report a state-of-the-art result on some metric often leads to\nquestionable research practices (QRPs): bad practices which fall short of\noutright research fraud. We describe 44 such practices which can undermine\nreported results, giving examples where possible. Our list emphasises the\nevaluation of large language models (LLMs) on public benchmarks. We also\ndiscuss \"irreproducible research practices\", i.e. decisions that make it\ndifficult or impossible for other researchers to reproduce, build on or audit\nprevious research."
                },
                "authors": [
                    {
                        "name": "Gavin Leech"
                    },
                    {
                        "name": "Juan J. Vazquez"
                    },
                    {
                        "name": "Niclas Kupper"
                    },
                    {
                        "name": "Misha Yagudin"
                    },
                    {
                        "name": "Laurence Aitchison"
                    }
                ],
                "author_detail": {
                    "name": "Laurence Aitchison"
                },
                "author": "Laurence Aitchison",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.12220v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.12220v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.22954v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.22954v1",
                "updated": "2024-10-30T12:09:29Z",
                "updated_parsed": [
                    2024,
                    10,
                    30,
                    12,
                    9,
                    29,
                    2,
                    304,
                    0
                ],
                "published": "2024-10-30T12:09:29Z",
                "published_parsed": [
                    2024,
                    10,
                    30,
                    12,
                    9,
                    29,
                    2,
                    304,
                    0
                ],
                "title": "Retrieval-Augmented Generation with Estimation of Source Reliability",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Retrieval-Augmented Generation with Estimation of Source Reliability"
                },
                "summary": "Retrieval-augmented generation (RAG) addresses key limitations of large\nlanguage models (LLMs), such as hallucinations and outdated knowledge, by\nincorporating external databases. These databases typically consult multiple\nsources to encompass up-to-date and various information. However, standard RAG\nmethods often overlook the heterogeneous source reliability in the multi-source\ndatabase and retrieve documents solely based on relevance, making them prone to\npropagating misinformation. To address this, we propose Reliability-Aware RAG\n(RA-RAG) which estimates the reliability of multiple sources and incorporates\nthis information into both retrieval and aggregation processes. Specifically,\nit iteratively estimates source reliability and true answers for a set of\nqueries with no labelling. Then, it selectively retrieves relevant documents\nfrom a few of reliable sources and aggregates them using weighted majority\nvoting, where the selective retrieval ensures scalability while not\ncompromising the performance. We also introduce a benchmark designed to reflect\nreal-world scenarios with heterogeneous source reliability and demonstrate the\neffectiveness of RA-RAG compared to a set of baselines.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Retrieval-augmented generation (RAG) addresses key limitations of large\nlanguage models (LLMs), such as hallucinations and outdated knowledge, by\nincorporating external databases. These databases typically consult multiple\nsources to encompass up-to-date and various information. However, standard RAG\nmethods often overlook the heterogeneous source reliability in the multi-source\ndatabase and retrieve documents solely based on relevance, making them prone to\npropagating misinformation. To address this, we propose Reliability-Aware RAG\n(RA-RAG) which estimates the reliability of multiple sources and incorporates\nthis information into both retrieval and aggregation processes. Specifically,\nit iteratively estimates source reliability and true answers for a set of\nqueries with no labelling. Then, it selectively retrieves relevant documents\nfrom a few of reliable sources and aggregates them using weighted majority\nvoting, where the selective retrieval ensures scalability while not\ncompromising the performance. We also introduce a benchmark designed to reflect\nreal-world scenarios with heterogeneous source reliability and demonstrate the\neffectiveness of RA-RAG compared to a set of baselines."
                },
                "authors": [
                    {
                        "name": "Jeongyeon Hwang"
                    },
                    {
                        "name": "Junyoung Park"
                    },
                    {
                        "name": "Hyejin Park"
                    },
                    {
                        "name": "Sangdon Park"
                    },
                    {
                        "name": "Jungseul Ok"
                    }
                ],
                "author_detail": {
                    "name": "Jungseul Ok"
                },
                "author": "Jungseul Ok",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.22954v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.22954v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.01288v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.01288v2",
                "updated": "2024-10-30T12:08:42Z",
                "updated_parsed": [
                    2024,
                    10,
                    30,
                    12,
                    8,
                    42,
                    2,
                    304,
                    0
                ],
                "published": "2024-06-03T12:59:17Z",
                "published_parsed": [
                    2024,
                    6,
                    3,
                    12,
                    59,
                    17,
                    0,
                    155,
                    0
                ],
                "title": "Improved Few-Shot Jailbreaking Can Circumvent Aligned Language Models\n  and Their Defenses",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Improved Few-Shot Jailbreaking Can Circumvent Aligned Language Models\n  and Their Defenses"
                },
                "summary": "Recently, Anil et al. (2024) show that many-shot (up to hundreds of)\ndemonstrations can jailbreak state-of-the-art LLMs by exploiting their\nlong-context capability. Nevertheless, is it possible to use few-shot\ndemonstrations to efficiently jailbreak LLMs within limited context sizes?\nWhile the vanilla few-shot jailbreaking may be inefficient, we propose improved\ntechniques such as injecting special system tokens like [/INST] and employing\ndemo-level random search from a collected demo pool. These simple techniques\nresult in surprisingly effective jailbreaking against aligned LLMs (even with\nadvanced defenses). For examples, our method achieves >80% (mostly >95%) ASRs\non Llama-2-7B and Llama-3-8B without multiple restarts, even if the models are\nenhanced by strong defenses such as perplexity detection and/or SmoothLLM,\nwhich is challenging for suffix-based jailbreaking. In addition, we conduct\ncomprehensive and elaborate (e.g., making sure to use correct system prompts)\nevaluations against other aligned LLMs and advanced defenses, where our method\nconsistently achieves nearly 100% ASRs. Our code is available at\nhttps://github.com/sail-sg/I-FSJ.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recently, Anil et al. (2024) show that many-shot (up to hundreds of)\ndemonstrations can jailbreak state-of-the-art LLMs by exploiting their\nlong-context capability. Nevertheless, is it possible to use few-shot\ndemonstrations to efficiently jailbreak LLMs within limited context sizes?\nWhile the vanilla few-shot jailbreaking may be inefficient, we propose improved\ntechniques such as injecting special system tokens like [/INST] and employing\ndemo-level random search from a collected demo pool. These simple techniques\nresult in surprisingly effective jailbreaking against aligned LLMs (even with\nadvanced defenses). For examples, our method achieves >80% (mostly >95%) ASRs\non Llama-2-7B and Llama-3-8B without multiple restarts, even if the models are\nenhanced by strong defenses such as perplexity detection and/or SmoothLLM,\nwhich is challenging for suffix-based jailbreaking. In addition, we conduct\ncomprehensive and elaborate (e.g., making sure to use correct system prompts)\nevaluations against other aligned LLMs and advanced defenses, where our method\nconsistently achieves nearly 100% ASRs. Our code is available at\nhttps://github.com/sail-sg/I-FSJ."
                },
                "authors": [
                    {
                        "name": "Xiaosen Zheng"
                    },
                    {
                        "name": "Tianyu Pang"
                    },
                    {
                        "name": "Chao Du"
                    },
                    {
                        "name": "Qian Liu"
                    },
                    {
                        "name": "Jing Jiang"
                    },
                    {
                        "name": "Min Lin"
                    }
                ],
                "author_detail": {
                    "name": "Min Lin"
                },
                "author": "Min Lin",
                "arxiv_comment": "NeurIPS 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.01288v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.01288v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.22948v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.22948v1",
                "updated": "2024-10-30T12:05:12Z",
                "updated_parsed": [
                    2024,
                    10,
                    30,
                    12,
                    5,
                    12,
                    2,
                    304,
                    0
                ],
                "published": "2024-10-30T12:05:12Z",
                "published_parsed": [
                    2024,
                    10,
                    30,
                    12,
                    5,
                    12,
                    2,
                    304,
                    0
                ],
                "title": "ELBOing Stein: Variational Bayes with Stein Mixture Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ELBOing Stein: Variational Bayes with Stein Mixture Inference"
                },
                "summary": "Stein variational gradient descent (SVGD) [Liu and Wang, 2016] performs\napproximate Bayesian inference by representing the posterior with a set of\nparticles. However, SVGD suffers from variance collapse, i.e. poor predictions\ndue to underestimating uncertainty [Ba et al., 2021], even for\nmoderately-dimensional models such as small Bayesian neural networks (BNNs). To\naddress this issue, we generalize SVGD by letting each particle parameterize a\ncomponent distribution in a mixture model. Our method, Stein Mixture Inference\n(SMI), optimizes a lower bound to the evidence (ELBO) and introduces\nuser-specified guides parameterized by particles. SMI extends the Nonlinear\nSVGD framework [Wang and Liu, 2019] to the case of variational Bayes. SMI\neffectively avoids variance collapse, judging by a previously described test\ndeveloped for this purpose, and performs well on standard data sets. In\naddition, SMI requires considerably fewer particles than SVGD to accurately\nestimate uncertainty for small BNNs. The synergistic combination of NSVGD, ELBO\noptimization and user-specified guides establishes a promising approach towards\nvariational Bayesian inference in the case of tall and wide data.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Stein variational gradient descent (SVGD) [Liu and Wang, 2016] performs\napproximate Bayesian inference by representing the posterior with a set of\nparticles. However, SVGD suffers from variance collapse, i.e. poor predictions\ndue to underestimating uncertainty [Ba et al., 2021], even for\nmoderately-dimensional models such as small Bayesian neural networks (BNNs). To\naddress this issue, we generalize SVGD by letting each particle parameterize a\ncomponent distribution in a mixture model. Our method, Stein Mixture Inference\n(SMI), optimizes a lower bound to the evidence (ELBO) and introduces\nuser-specified guides parameterized by particles. SMI extends the Nonlinear\nSVGD framework [Wang and Liu, 2019] to the case of variational Bayes. SMI\neffectively avoids variance collapse, judging by a previously described test\ndeveloped for this purpose, and performs well on standard data sets. In\naddition, SMI requires considerably fewer particles than SVGD to accurately\nestimate uncertainty for small BNNs. The synergistic combination of NSVGD, ELBO\noptimization and user-specified guides establishes a promising approach towards\nvariational Bayesian inference in the case of tall and wide data."
                },
                "authors": [
                    {
                        "name": "Ola Rønning"
                    },
                    {
                        "name": "Eric Nalisnick"
                    },
                    {
                        "name": "Christophe Ley"
                    },
                    {
                        "name": "Padhraic Smyth"
                    },
                    {
                        "name": "Thomas Hamelryck"
                    }
                ],
                "author_detail": {
                    "name": "Thomas Hamelryck"
                },
                "author": "Thomas Hamelryck",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.22948v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.22948v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.22946v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.22946v1",
                "updated": "2024-10-30T12:04:22Z",
                "updated_parsed": [
                    2024,
                    10,
                    30,
                    12,
                    4,
                    22,
                    2,
                    304,
                    0
                ],
                "published": "2024-10-30T12:04:22Z",
                "published_parsed": [
                    2024,
                    10,
                    30,
                    12,
                    4,
                    22,
                    2,
                    304,
                    0
                ],
                "title": "KALAM: toolKit for Automating high-Level synthesis of Analog computing\n  systeMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "KALAM: toolKit for Automating high-Level synthesis of Analog computing\n  systeMs"
                },
                "summary": "Diverse computing paradigms have emerged to meet the growing needs for\nintelligent energy-efficient systems. The Margin Propagation (MP) framework,\nbeing one such initiative in the analog computing domain, stands out due to its\nscalability across biasing conditions, temperatures, and diminishing process\ntechnology nodes. However, the lack of digital-like automation tools for\ndesigning analog systems (including that of MP analog) hinders their adoption\nfor designing large systems. The inherent scalability and modularity of MP\nsystems present a unique opportunity in this regard. This paper introduces\nKALAM (toolKit for Automating high-Level synthesis of Analog computing\nsysteMs), which leverages factor graphs as the foundational paradigm for\nsynthesizing MP-based analog computing systems. Factor graphs are the basis of\nvarious signal processing tasks and, when coupled with MP, can be used to\ndesign scalable and energy-efficient analog signal processors. Using Python\nscripting language, the KALAM automation flow translates an input factor graph\nto its equivalent SPICE-compatible circuit netlist that can be used to validate\nthe intended functionality. KALAM also allows the integration of design\noptimization strategies such as precision tuning, variable elimination, and\nmathematical simplification. We demonstrate KALAM's versatility for tasks such\nas Bayesian inference, Low-Density Parity Check (LDPC) decoding, and Artificial\nNeural Networks (ANN). Simulation results of the netlists align closely with\nsoftware implementations, affirming the efficacy of our proposed automation\ntool.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diverse computing paradigms have emerged to meet the growing needs for\nintelligent energy-efficient systems. The Margin Propagation (MP) framework,\nbeing one such initiative in the analog computing domain, stands out due to its\nscalability across biasing conditions, temperatures, and diminishing process\ntechnology nodes. However, the lack of digital-like automation tools for\ndesigning analog systems (including that of MP analog) hinders their adoption\nfor designing large systems. The inherent scalability and modularity of MP\nsystems present a unique opportunity in this regard. This paper introduces\nKALAM (toolKit for Automating high-Level synthesis of Analog computing\nsysteMs), which leverages factor graphs as the foundational paradigm for\nsynthesizing MP-based analog computing systems. Factor graphs are the basis of\nvarious signal processing tasks and, when coupled with MP, can be used to\ndesign scalable and energy-efficient analog signal processors. Using Python\nscripting language, the KALAM automation flow translates an input factor graph\nto its equivalent SPICE-compatible circuit netlist that can be used to validate\nthe intended functionality. KALAM also allows the integration of design\noptimization strategies such as precision tuning, variable elimination, and\nmathematical simplification. We demonstrate KALAM's versatility for tasks such\nas Bayesian inference, Low-Density Parity Check (LDPC) decoding, and Artificial\nNeural Networks (ANN). Simulation results of the netlists align closely with\nsoftware implementations, affirming the efficacy of our proposed automation\ntool."
                },
                "authors": [
                    {
                        "name": "Ankita Nandi"
                    },
                    {
                        "name": "Krishil Gandhi"
                    },
                    {
                        "name": "Mahendra Pratap Singh"
                    },
                    {
                        "name": "Shantanu Chakrabartty"
                    },
                    {
                        "name": "Chetan Singh Thakur"
                    }
                ],
                "author_detail": {
                    "name": "Chetan Singh Thakur"
                },
                "author": "Chetan Singh Thakur",
                "arxiv_comment": "5 Pages, 4 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.22946v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.22946v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.ET",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.17515v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.17515v3",
                "updated": "2024-10-30T12:04:18Z",
                "updated_parsed": [
                    2024,
                    10,
                    30,
                    12,
                    4,
                    18,
                    2,
                    304,
                    0
                ],
                "published": "2024-09-26T03:50:22Z",
                "published_parsed": [
                    2024,
                    9,
                    26,
                    3,
                    50,
                    22,
                    3,
                    270,
                    0
                ],
                "title": "From News to Forecast: Integrating Event Analysis in LLM-Based Time\n  Series Forecasting with Reflection",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "From News to Forecast: Integrating Event Analysis in LLM-Based Time\n  Series Forecasting with Reflection"
                },
                "summary": "This paper introduces a novel approach that leverages Large Language Models\n(LLMs) and Generative Agents to enhance time series forecasting by reasoning\nacross both text and time series data. With language as a medium, our method\nadaptively integrates social events into forecasting models, aligning news\ncontent with time series fluctuations to provide richer insights. Specifically,\nwe utilize LLM-based agents to iteratively filter out irrelevant news and\nemploy human-like reasoning to evaluate predictions. This enables the model to\nanalyze complex events, such as unexpected incidents and shifts in social\nbehavior, and continuously refine the selection logic of news and the\nrobustness of the agent's output. By integrating selected news events with time\nseries data, we fine-tune a pre-trained LLM to predict sequences of digits in\ntime series. The results demonstrate significant improvements in forecasting\naccuracy, suggesting a potential paradigm shift in time series forecasting\nthrough the effective utilization of unstructured news data.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper introduces a novel approach that leverages Large Language Models\n(LLMs) and Generative Agents to enhance time series forecasting by reasoning\nacross both text and time series data. With language as a medium, our method\nadaptively integrates social events into forecasting models, aligning news\ncontent with time series fluctuations to provide richer insights. Specifically,\nwe utilize LLM-based agents to iteratively filter out irrelevant news and\nemploy human-like reasoning to evaluate predictions. This enables the model to\nanalyze complex events, such as unexpected incidents and shifts in social\nbehavior, and continuously refine the selection logic of news and the\nrobustness of the agent's output. By integrating selected news events with time\nseries data, we fine-tune a pre-trained LLM to predict sequences of digits in\ntime series. The results demonstrate significant improvements in forecasting\naccuracy, suggesting a potential paradigm shift in time series forecasting\nthrough the effective utilization of unstructured news data."
                },
                "authors": [
                    {
                        "name": "Xinlei Wang"
                    },
                    {
                        "name": "Maike Feng"
                    },
                    {
                        "name": "Jing Qiu"
                    },
                    {
                        "name": "Jinjin Gu"
                    },
                    {
                        "name": "Junhua Zhao"
                    }
                ],
                "author_detail": {
                    "name": "Junhua Zhao"
                },
                "author": "Junhua Zhao",
                "arxiv_comment": "This paper has been accepted for NeurIPS 2024. Code and data are\n  available at https://github.com/ameliawong1996/From_News_to_Forecast",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.17515v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.17515v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.22944v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.22944v1",
                "updated": "2024-10-30T12:01:48Z",
                "updated_parsed": [
                    2024,
                    10,
                    30,
                    12,
                    1,
                    48,
                    2,
                    304,
                    0
                ],
                "published": "2024-10-30T12:01:48Z",
                "published_parsed": [
                    2024,
                    10,
                    30,
                    12,
                    1,
                    48,
                    2,
                    304,
                    0
                ],
                "title": "Focus On This, Not That! Steering LLMs With Adaptive Feature\n  Specification",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Focus On This, Not That! Steering LLMs With Adaptive Feature\n  Specification"
                },
                "summary": "Despite the success of Instruction Tuning (IT) in training large language\nmodels (LLMs) to perform arbitrary user-specified tasks, these models often\nstill leverage spurious or biased features learned from their training data,\nleading to undesired behaviours when deploying them in new contexts. In this\nwork, we introduce Focus Instruction Tuning (FIT), which trains LLMs to\ncondition their responses by focusing on specific features whilst ignoring\nothers, leading to different behaviours based on what features are specified.\nAcross several experimental settings, we show that focus-tuned models can be\nadaptively steered by focusing on different features at inference-time: for\ninstance, robustness can be improved by focusing on task-causal features and\nignoring spurious features, and social bias can be mitigated by ignoring\ndemographic categories. Furthermore, FIT can steer behaviour in new contexts,\ngeneralising under distribution shift and to new unseen features at inference\ntime, and thereby facilitating more robust, fair, and controllable LLM\napplications in real-world environments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Despite the success of Instruction Tuning (IT) in training large language\nmodels (LLMs) to perform arbitrary user-specified tasks, these models often\nstill leverage spurious or biased features learned from their training data,\nleading to undesired behaviours when deploying them in new contexts. In this\nwork, we introduce Focus Instruction Tuning (FIT), which trains LLMs to\ncondition their responses by focusing on specific features whilst ignoring\nothers, leading to different behaviours based on what features are specified.\nAcross several experimental settings, we show that focus-tuned models can be\nadaptively steered by focusing on different features at inference-time: for\ninstance, robustness can be improved by focusing on task-causal features and\nignoring spurious features, and social bias can be mitigated by ignoring\ndemographic categories. Furthermore, FIT can steer behaviour in new contexts,\ngeneralising under distribution shift and to new unseen features at inference\ntime, and thereby facilitating more robust, fair, and controllable LLM\napplications in real-world environments."
                },
                "authors": [
                    {
                        "name": "Tom A. Lamb"
                    },
                    {
                        "name": "Adam Davies"
                    },
                    {
                        "name": "Alasdair Paren"
                    },
                    {
                        "name": "Philip H. S. Torr"
                    },
                    {
                        "name": "Francesco Pinto"
                    }
                ],
                "author_detail": {
                    "name": "Francesco Pinto"
                },
                "author": "Francesco Pinto",
                "arxiv_comment": "28pages, 14 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.22944v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.22944v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2403.00418v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2403.00418v3",
                "updated": "2024-10-30T12:00:42Z",
                "updated_parsed": [
                    2024,
                    10,
                    30,
                    12,
                    0,
                    42,
                    2,
                    304,
                    0
                ],
                "published": "2024-03-01T10:10:34Z",
                "published_parsed": [
                    2024,
                    3,
                    1,
                    10,
                    10,
                    34,
                    4,
                    61,
                    0
                ],
                "title": "LLMs for Targeted Sentiment in News Headlines: Exploring the\n  Descriptive-Prescriptive Dilemma",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLMs for Targeted Sentiment in News Headlines: Exploring the\n  Descriptive-Prescriptive Dilemma"
                },
                "summary": "News headlines often evoke sentiment by intentionally portraying entities in\nparticular ways, making targeted sentiment analysis (TSA) of headlines a\nworthwhile but difficult task. Due to its subjectivity, creating TSA datasets\ncan involve various annotation paradigms, from descriptive to prescriptive,\neither encouraging or limiting subjectivity. LLMs are a good fit for TSA due to\ntheir broad linguistic and world knowledge and in-context learning abilities,\nyet their performance depends on prompt design. In this paper, we compare the\naccuracy of state-of-the-art LLMs and fine-tuned encoder models for TSA of news\nheadlines using descriptive and prescriptive datasets across several languages.\nExploring the descriptive--prescriptive continuum, we analyze how performance\nis affected by prompt prescriptiveness, ranging from plain zero-shot to\nelaborate few-shot prompts. Finally, we evaluate the ability of LLMs to\nquantify uncertainty via calibration error and comparison to human label\nvariation. We find that LLMs outperform fine-tuned encoders on descriptive\ndatasets, while calibration and F1-score generally improve with increased\nprescriptiveness, yet the optimal level varies.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "News headlines often evoke sentiment by intentionally portraying entities in\nparticular ways, making targeted sentiment analysis (TSA) of headlines a\nworthwhile but difficult task. Due to its subjectivity, creating TSA datasets\ncan involve various annotation paradigms, from descriptive to prescriptive,\neither encouraging or limiting subjectivity. LLMs are a good fit for TSA due to\ntheir broad linguistic and world knowledge and in-context learning abilities,\nyet their performance depends on prompt design. In this paper, we compare the\naccuracy of state-of-the-art LLMs and fine-tuned encoder models for TSA of news\nheadlines using descriptive and prescriptive datasets across several languages.\nExploring the descriptive--prescriptive continuum, we analyze how performance\nis affected by prompt prescriptiveness, ranging from plain zero-shot to\nelaborate few-shot prompts. Finally, we evaluate the ability of LLMs to\nquantify uncertainty via calibration error and comparison to human label\nvariation. We find that LLMs outperform fine-tuned encoders on descriptive\ndatasets, while calibration and F1-score generally improve with increased\nprescriptiveness, yet the optimal level varies."
                },
                "authors": [
                    {
                        "name": "Jana Juroš"
                    },
                    {
                        "name": "Laura Majer"
                    },
                    {
                        "name": "Jan Šnajder"
                    }
                ],
                "author_detail": {
                    "name": "Jan Šnajder"
                },
                "author": "Jan Šnajder",
                "arxiv_comment": "Presented at 14th Workshop on Computational Approaches to\n  Subjectivity, Sentiment, & Social Media Analysis (WASSA) at ACL 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2403.00418v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2403.00418v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.21573v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.21573v2",
                "updated": "2024-10-30T11:56:17Z",
                "updated_parsed": [
                    2024,
                    10,
                    30,
                    11,
                    56,
                    17,
                    2,
                    304,
                    0
                ],
                "published": "2024-10-28T22:09:43Z",
                "published_parsed": [
                    2024,
                    10,
                    28,
                    22,
                    9,
                    43,
                    0,
                    302,
                    0
                ],
                "title": "Thank You, Stingray: Multilingual Large Language Models Can Not (Yet)\n  Disambiguate Cross-Lingual Word Sense",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Thank You, Stingray: Multilingual Large Language Models Can Not (Yet)\n  Disambiguate Cross-Lingual Word Sense"
                },
                "summary": "Multilingual large language models (LLMs) have gained prominence, but\nconcerns arise regarding their reliability beyond English. This study addresses\nthe gap in cross-lingual semantic evaluation by introducing a novel benchmark\nfor cross-lingual sense disambiguation, StingrayBench. In this paper, we\ndemonstrate using false friends -- words that are orthographically similar but\nhave completely different meanings in two languages -- as a possible approach\nto pinpoint the limitation of cross-lingual sense disambiguation in LLMs. We\ncollect false friends in four language pairs, namely Indonesian-Malay,\nIndonesian-Tagalog, Chinese-Japanese, and English-German; and challenge LLMs to\ndistinguish the use of them in context. In our analysis of various models, we\nobserve they tend to be biased toward higher-resource languages. We also\npropose new metrics for quantifying the cross-lingual sense bias and\ncomprehension based on our benchmark. Our work contributes to developing more\ndiverse and inclusive language modeling, promoting fairer access for the wider\nmultilingual community.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multilingual large language models (LLMs) have gained prominence, but\nconcerns arise regarding their reliability beyond English. This study addresses\nthe gap in cross-lingual semantic evaluation by introducing a novel benchmark\nfor cross-lingual sense disambiguation, StingrayBench. In this paper, we\ndemonstrate using false friends -- words that are orthographically similar but\nhave completely different meanings in two languages -- as a possible approach\nto pinpoint the limitation of cross-lingual sense disambiguation in LLMs. We\ncollect false friends in four language pairs, namely Indonesian-Malay,\nIndonesian-Tagalog, Chinese-Japanese, and English-German; and challenge LLMs to\ndistinguish the use of them in context. In our analysis of various models, we\nobserve they tend to be biased toward higher-resource languages. We also\npropose new metrics for quantifying the cross-lingual sense bias and\ncomprehension based on our benchmark. Our work contributes to developing more\ndiverse and inclusive language modeling, promoting fairer access for the wider\nmultilingual community."
                },
                "authors": [
                    {
                        "name": "Samuel Cahyawijaya"
                    },
                    {
                        "name": "Ruochen Zhang"
                    },
                    {
                        "name": "Holy Lovenia"
                    },
                    {
                        "name": "Jan Christian Blaise Cruz"
                    },
                    {
                        "name": "Elisa Gilbert"
                    },
                    {
                        "name": "Hiroki Nomoto"
                    },
                    {
                        "name": "Alham Fikri Aji"
                    }
                ],
                "author_detail": {
                    "name": "Alham Fikri Aji"
                },
                "author": "Alham Fikri Aji",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.21573v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.21573v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.22939v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.22939v1",
                "updated": "2024-10-30T11:49:06Z",
                "updated_parsed": [
                    2024,
                    10,
                    30,
                    11,
                    49,
                    6,
                    2,
                    304,
                    0
                ],
                "published": "2024-10-30T11:49:06Z",
                "published_parsed": [
                    2024,
                    10,
                    30,
                    11,
                    49,
                    6,
                    2,
                    304,
                    0
                ],
                "title": "AdaptiveISP: Learning an Adaptive Image Signal Processor for Object\n  Detection",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AdaptiveISP: Learning an Adaptive Image Signal Processor for Object\n  Detection"
                },
                "summary": "Image Signal Processors (ISPs) convert raw sensor signals into digital\nimages, which significantly influence the image quality and the performance of\ndownstream computer vision tasks. Designing ISP pipeline and tuning ISP\nparameters are two key steps for building an imaging and vision system. To find\noptimal ISP configurations, recent works use deep neural networks as a proxy to\nsearch for ISP parameters or ISP pipelines. However, these methods are\nprimarily designed to maximize the image quality, which are sub-optimal in the\nperformance of high-level computer vision tasks such as detection, recognition,\nand tracking. Moreover, after training, the learned ISP pipelines are mostly\nfixed at the inference time, whose performance degrades in dynamic scenes. To\njointly optimize ISP structures and parameters, we propose AdaptiveISP, a\ntask-driven and scene-adaptive ISP. One key observation is that for the\nmajority of input images, only a few processing modules are needed to improve\nthe performance of downstream recognition tasks, and only a few inputs require\nmore processing. Based on this, AdaptiveISP utilizes deep reinforcement\nlearning to automatically generate an optimal ISP pipeline and the associated\nISP parameters to maximize the detection performance. Experimental results show\nthat AdaptiveISP not only surpasses the prior state-of-the-art methods for\nobject detection but also dynamically manages the trade-off between detection\nperformance and computational cost, especially suitable for scenes with large\ndynamic range variations. Project website:\nhttps://openimaginglab.github.io/AdaptiveISP/.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Image Signal Processors (ISPs) convert raw sensor signals into digital\nimages, which significantly influence the image quality and the performance of\ndownstream computer vision tasks. Designing ISP pipeline and tuning ISP\nparameters are two key steps for building an imaging and vision system. To find\noptimal ISP configurations, recent works use deep neural networks as a proxy to\nsearch for ISP parameters or ISP pipelines. However, these methods are\nprimarily designed to maximize the image quality, which are sub-optimal in the\nperformance of high-level computer vision tasks such as detection, recognition,\nand tracking. Moreover, after training, the learned ISP pipelines are mostly\nfixed at the inference time, whose performance degrades in dynamic scenes. To\njointly optimize ISP structures and parameters, we propose AdaptiveISP, a\ntask-driven and scene-adaptive ISP. One key observation is that for the\nmajority of input images, only a few processing modules are needed to improve\nthe performance of downstream recognition tasks, and only a few inputs require\nmore processing. Based on this, AdaptiveISP utilizes deep reinforcement\nlearning to automatically generate an optimal ISP pipeline and the associated\nISP parameters to maximize the detection performance. Experimental results show\nthat AdaptiveISP not only surpasses the prior state-of-the-art methods for\nobject detection but also dynamically manages the trade-off between detection\nperformance and computational cost, especially suitable for scenes with large\ndynamic range variations. Project website:\nhttps://openimaginglab.github.io/AdaptiveISP/."
                },
                "authors": [
                    {
                        "name": "Yujin Wang"
                    },
                    {
                        "name": "Tianyi Xu"
                    },
                    {
                        "name": "Fan Zhang"
                    },
                    {
                        "name": "Tianfan Xue"
                    },
                    {
                        "name": "Jinwei Gu"
                    }
                ],
                "author_detail": {
                    "name": "Jinwei Gu"
                },
                "author": "Jinwei Gu",
                "arxiv_comment": "Accepted at NeurIPS2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.22939v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.22939v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.12311v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.12311v2",
                "updated": "2024-10-30T11:47:40Z",
                "updated_parsed": [
                    2024,
                    10,
                    30,
                    11,
                    47,
                    40,
                    2,
                    304,
                    0
                ],
                "published": "2024-06-18T06:32:23Z",
                "published_parsed": [
                    2024,
                    6,
                    18,
                    6,
                    32,
                    23,
                    1,
                    170,
                    0
                ],
                "title": "Mixture of Scales: Memory-Efficient Token-Adaptive Binarization for\n  Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mixture of Scales: Memory-Efficient Token-Adaptive Binarization for\n  Large Language Models"
                },
                "summary": "Binarization, which converts weight parameters to binary values, has emerged\nas an effective strategy to reduce the size of large language models (LLMs).\nHowever, typical binarization techniques significantly diminish linguistic\neffectiveness of LLMs. To address this issue, we introduce a novel binarization\ntechnique called Mixture of Scales (BinaryMoS). Unlike conventional methods,\nBinaryMoS employs multiple scaling experts for binary weights, dynamically\nmerging these experts for each token to adaptively generate scaling factors.\nThis token-adaptive approach boosts the representational power of binarized\nLLMs by enabling contextual adjustments to the values of binary weights.\nMoreover, because this adaptive process only involves the scaling factors\nrather than the entire weight matrix, BinaryMoS maintains compression\nefficiency similar to traditional static binarization methods. Our experimental\nresults reveal that BinaryMoS surpasses conventional binarization techniques in\nvarious natural language processing tasks and even outperforms 2-bit\nquantization methods, all while maintaining similar model size to static\nbinarization techniques.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Binarization, which converts weight parameters to binary values, has emerged\nas an effective strategy to reduce the size of large language models (LLMs).\nHowever, typical binarization techniques significantly diminish linguistic\neffectiveness of LLMs. To address this issue, we introduce a novel binarization\ntechnique called Mixture of Scales (BinaryMoS). Unlike conventional methods,\nBinaryMoS employs multiple scaling experts for binary weights, dynamically\nmerging these experts for each token to adaptively generate scaling factors.\nThis token-adaptive approach boosts the representational power of binarized\nLLMs by enabling contextual adjustments to the values of binary weights.\nMoreover, because this adaptive process only involves the scaling factors\nrather than the entire weight matrix, BinaryMoS maintains compression\nefficiency similar to traditional static binarization methods. Our experimental\nresults reveal that BinaryMoS surpasses conventional binarization techniques in\nvarious natural language processing tasks and even outperforms 2-bit\nquantization methods, all while maintaining similar model size to static\nbinarization techniques."
                },
                "authors": [
                    {
                        "name": "Dongwon Jo"
                    },
                    {
                        "name": "Taesu Kim"
                    },
                    {
                        "name": "Yulhwa Kim"
                    },
                    {
                        "name": "Jae-Joon Kim"
                    }
                ],
                "author_detail": {
                    "name": "Jae-Joon Kim"
                },
                "author": "Jae-Joon Kim",
                "arxiv_comment": "38th Conference on Neural Information Processing Systems (NeurIPS\n  2024)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.12311v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.12311v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.22932v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.22932v1",
                "updated": "2024-10-30T11:38:13Z",
                "updated_parsed": [
                    2024,
                    10,
                    30,
                    11,
                    38,
                    13,
                    2,
                    304,
                    0
                ],
                "published": "2024-10-30T11:38:13Z",
                "published_parsed": [
                    2024,
                    10,
                    30,
                    11,
                    38,
                    13,
                    2,
                    304,
                    0
                ],
                "title": "Multi-Agent Large Language Models for Conversational Task-Solving",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-Agent Large Language Models for Conversational Task-Solving"
                },
                "summary": "In an era where single large language models have dominated the landscape of\nartificial intelligence for years, multi-agent systems arise as new\nprotagonists in conversational task-solving. While previous studies have\nshowcased their potential in reasoning tasks and creative endeavors, an\nanalysis of their limitations concerning the conversational paradigms and the\nimpact of individual agents is missing. It remains unascertained how\nmulti-agent discussions perform across tasks of varying complexity and how the\nstructure of these conversations influences the process. To fill that gap, this\nwork systematically evaluates multi-agent systems across various discussion\nparadigms, assessing their strengths and weaknesses in both generative tasks\nand question-answering tasks. Alongside the experiments, I propose a taxonomy\nof 20 multi-agent research studies from 2022 to 2024, followed by the\nintroduction of a framework for deploying multi-agent LLMs in conversational\ntask-solving. I demonstrate that while multi-agent systems excel in complex\nreasoning tasks, outperforming a single model by leveraging expert personas,\nthey fail on basic tasks. Concretely, I identify three challenges that arise:\n1) While longer discussions enhance reasoning, agents fail to maintain\nconformity to strict task requirements, which leads to problem drift, making\nshorter conversations more effective for basic tasks. 2) Prolonged discussions\nrisk alignment collapse, raising new safety concerns for these systems. 3) I\nshowcase discussion monopolization through long generations, posing the problem\nof fairness in decision-making for tasks like summarization. This work uncovers\nboth the potential and challenges that arise with multi-agent interaction and\nvarying conversational paradigms, providing insights into how future research\ncould improve the efficiency, performance, and safety of multi-agent LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In an era where single large language models have dominated the landscape of\nartificial intelligence for years, multi-agent systems arise as new\nprotagonists in conversational task-solving. While previous studies have\nshowcased their potential in reasoning tasks and creative endeavors, an\nanalysis of their limitations concerning the conversational paradigms and the\nimpact of individual agents is missing. It remains unascertained how\nmulti-agent discussions perform across tasks of varying complexity and how the\nstructure of these conversations influences the process. To fill that gap, this\nwork systematically evaluates multi-agent systems across various discussion\nparadigms, assessing their strengths and weaknesses in both generative tasks\nand question-answering tasks. Alongside the experiments, I propose a taxonomy\nof 20 multi-agent research studies from 2022 to 2024, followed by the\nintroduction of a framework for deploying multi-agent LLMs in conversational\ntask-solving. I demonstrate that while multi-agent systems excel in complex\nreasoning tasks, outperforming a single model by leveraging expert personas,\nthey fail on basic tasks. Concretely, I identify three challenges that arise:\n1) While longer discussions enhance reasoning, agents fail to maintain\nconformity to strict task requirements, which leads to problem drift, making\nshorter conversations more effective for basic tasks. 2) Prolonged discussions\nrisk alignment collapse, raising new safety concerns for these systems. 3) I\nshowcase discussion monopolization through long generations, posing the problem\nof fairness in decision-making for tasks like summarization. This work uncovers\nboth the potential and challenges that arise with multi-agent interaction and\nvarying conversational paradigms, providing insights into how future research\ncould improve the efficiency, performance, and safety of multi-agent LLMs."
                },
                "authors": [
                    {
                        "name": "Jonas Becker"
                    }
                ],
                "author_detail": {
                    "name": "Jonas Becker"
                },
                "author": "Jonas Becker",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.22932v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.22932v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.21759v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.21759v2",
                "updated": "2024-10-30T11:20:46Z",
                "updated_parsed": [
                    2024,
                    10,
                    30,
                    11,
                    20,
                    46,
                    2,
                    304,
                    0
                ],
                "published": "2024-10-29T05:50:17Z",
                "published_parsed": [
                    2024,
                    10,
                    29,
                    5,
                    50,
                    17,
                    1,
                    303,
                    0
                ],
                "title": "IntLoRA: Integral Low-rank Adaptation of Quantized Diffusion Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "IntLoRA: Integral Low-rank Adaptation of Quantized Diffusion Models"
                },
                "summary": "Fine-tuning large-scale text-to-image diffusion models for various downstream\ntasks has yielded impressive results. However, the heavy computational burdens\nof tuning large models prevent personal customization. Recent advances have\nattempted to employ parameter-efficient fine-tuning (PEFT) techniques to adapt\nthe floating-point (FP) or quantized pre-trained weights. Nonetheless, the\nadaptation parameters in existing works are still restricted to FP arithmetic,\nhindering hardware-friendly acceleration. In this work, we propose IntLoRA, to\nfurther push the efficiency limits by using integer type (INT) low-rank\nparameters to adapt the quantized diffusion models. By working in the integer\narithmetic, our IntLoRA offers three key advantages: (i) for fine-tuning, the\npre-trained weights are quantized, reducing memory usage; (ii) for storage,\nboth pre-trained and low-rank weights are in INT which consumes less disk\nspace; (iii) for inference, IntLoRA weights can be naturally merged into\nquantized pre-trained weights through efficient integer multiplication or\nbit-shifting, eliminating additional post-training quantization. Extensive\nexperiments demonstrate that IntLoRA can achieve performance on par with or\neven superior to the vanilla LoRA, accompanied by significant efficiency\nimprovements. Code is available at \\url{https://github.com/csguoh/IntLoRA}.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Fine-tuning large-scale text-to-image diffusion models for various downstream\ntasks has yielded impressive results. However, the heavy computational burdens\nof tuning large models prevent personal customization. Recent advances have\nattempted to employ parameter-efficient fine-tuning (PEFT) techniques to adapt\nthe floating-point (FP) or quantized pre-trained weights. Nonetheless, the\nadaptation parameters in existing works are still restricted to FP arithmetic,\nhindering hardware-friendly acceleration. In this work, we propose IntLoRA, to\nfurther push the efficiency limits by using integer type (INT) low-rank\nparameters to adapt the quantized diffusion models. By working in the integer\narithmetic, our IntLoRA offers three key advantages: (i) for fine-tuning, the\npre-trained weights are quantized, reducing memory usage; (ii) for storage,\nboth pre-trained and low-rank weights are in INT which consumes less disk\nspace; (iii) for inference, IntLoRA weights can be naturally merged into\nquantized pre-trained weights through efficient integer multiplication or\nbit-shifting, eliminating additional post-training quantization. Extensive\nexperiments demonstrate that IntLoRA can achieve performance on par with or\neven superior to the vanilla LoRA, accompanied by significant efficiency\nimprovements. Code is available at \\url{https://github.com/csguoh/IntLoRA}."
                },
                "authors": [
                    {
                        "name": "Hang Guo"
                    },
                    {
                        "name": "Yawei Li"
                    },
                    {
                        "name": "Tao Dai"
                    },
                    {
                        "name": "Shu-Tao Xia"
                    },
                    {
                        "name": "Luca Benini"
                    }
                ],
                "author_detail": {
                    "name": "Luca Benini"
                },
                "author": "Luca Benini",
                "arxiv_comment": "Technical Report",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.21759v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.21759v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.22916v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.22916v1",
                "updated": "2024-10-30T11:14:33Z",
                "updated_parsed": [
                    2024,
                    10,
                    30,
                    11,
                    14,
                    33,
                    2,
                    304,
                    0
                ],
                "published": "2024-10-30T11:14:33Z",
                "published_parsed": [
                    2024,
                    10,
                    30,
                    11,
                    14,
                    33,
                    2,
                    304,
                    0
                ],
                "title": "Explainable Behavior Cloning: Teaching Large Language Model Agents\n  through Learning by Demonstration",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Explainable Behavior Cloning: Teaching Large Language Model Agents\n  through Learning by Demonstration"
                },
                "summary": "Autonomous mobile app interaction has become increasingly important with\ngrowing complexity of mobile applications. Developing intelligent agents that\ncan effectively navigate and interact with mobile apps remains a significant\nchallenge. In this paper, we propose an Explainable Behavior Cloning LLM Agent\n(EBC-LLMAgent), a novel approach that combines large language models (LLMs)\nwith behavior cloning by learning demonstrations to create intelligent and\nexplainable agents for autonomous mobile app interaction. EBC-LLMAgent consists\nof three core modules: Demonstration Encoding, Code Generation, and UI Mapping,\nwhich work synergistically to capture user demonstrations, generate executable\ncodes, and establish accurate correspondence between code and UI elements. We\nintroduce the Behavior Cloning Chain Fusion technique to enhance the\ngeneralization capabilities of the agent. Extensive experiments on five popular\nmobile applications from diverse domains demonstrate the superior performance\nof EBC-LLMAgent, achieving high success rates in task completion, efficient\ngeneralization to unseen scenarios, and the generation of meaningful\nexplanations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Autonomous mobile app interaction has become increasingly important with\ngrowing complexity of mobile applications. Developing intelligent agents that\ncan effectively navigate and interact with mobile apps remains a significant\nchallenge. In this paper, we propose an Explainable Behavior Cloning LLM Agent\n(EBC-LLMAgent), a novel approach that combines large language models (LLMs)\nwith behavior cloning by learning demonstrations to create intelligent and\nexplainable agents for autonomous mobile app interaction. EBC-LLMAgent consists\nof three core modules: Demonstration Encoding, Code Generation, and UI Mapping,\nwhich work synergistically to capture user demonstrations, generate executable\ncodes, and establish accurate correspondence between code and UI elements. We\nintroduce the Behavior Cloning Chain Fusion technique to enhance the\ngeneralization capabilities of the agent. Extensive experiments on five popular\nmobile applications from diverse domains demonstrate the superior performance\nof EBC-LLMAgent, achieving high success rates in task completion, efficient\ngeneralization to unseen scenarios, and the generation of meaningful\nexplanations."
                },
                "authors": [
                    {
                        "name": "Yanchu Guan"
                    },
                    {
                        "name": "Dong Wang"
                    },
                    {
                        "name": "Yan Wang"
                    },
                    {
                        "name": "Haiqing Wang"
                    },
                    {
                        "name": "Renen Sun"
                    },
                    {
                        "name": "Chenyi Zhuang"
                    },
                    {
                        "name": "Jinjie Gu"
                    },
                    {
                        "name": "Zhixuan Chu"
                    }
                ],
                "author_detail": {
                    "name": "Zhixuan Chu"
                },
                "author": "Zhixuan Chu",
                "arxiv_comment": "20 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.22916v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.22916v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.22898v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.22898v1",
                "updated": "2024-10-30T10:57:46Z",
                "updated_parsed": [
                    2024,
                    10,
                    30,
                    10,
                    57,
                    46,
                    2,
                    304,
                    0
                ],
                "published": "2024-10-30T10:57:46Z",
                "published_parsed": [
                    2024,
                    10,
                    30,
                    10,
                    57,
                    46,
                    2,
                    304,
                    0
                ],
                "title": "YOLOv11 for Vehicle Detection: Advancements, Performance, and\n  Applications in Intelligent Transportation Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "YOLOv11 for Vehicle Detection: Advancements, Performance, and\n  Applications in Intelligent Transportation Systems"
                },
                "summary": "Accurate vehicle detection is essential for the development of intelligent\ntransportation systems, autonomous driving, and traffic monitoring. This paper\npresents a detailed analysis of YOLO11, the latest advancement in the YOLO\nseries of deep learning models, focusing exclusively on vehicle detection\ntasks. Building upon the success of its predecessors, YOLO11 introduces\narchitectural improvements designed to enhance detection speed, accuracy, and\nrobustness in complex environments. Using a comprehensive dataset comprising\nmultiple vehicle types-cars, trucks, buses, motorcycles, and bicycles we\nevaluate YOLO11's performance using metrics such as precision, recall, F1\nscore, and mean average precision (mAP). Our findings demonstrate that YOLO11\nsurpasses previous versions (YOLOv8 and YOLOv10) in detecting smaller and more\noccluded vehicles while maintaining a competitive inference time, making it\nwell-suited for real-time applications. Comparative analysis shows significant\nimprovements in the detection of complex vehicle geometries, further\ncontributing to the development of efficient and scalable vehicle detection\nsystems. This research highlights YOLO11's potential to enhance autonomous\nvehicle performance and traffic monitoring systems, offering insights for\nfuture developments in the field.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Accurate vehicle detection is essential for the development of intelligent\ntransportation systems, autonomous driving, and traffic monitoring. This paper\npresents a detailed analysis of YOLO11, the latest advancement in the YOLO\nseries of deep learning models, focusing exclusively on vehicle detection\ntasks. Building upon the success of its predecessors, YOLO11 introduces\narchitectural improvements designed to enhance detection speed, accuracy, and\nrobustness in complex environments. Using a comprehensive dataset comprising\nmultiple vehicle types-cars, trucks, buses, motorcycles, and bicycles we\nevaluate YOLO11's performance using metrics such as precision, recall, F1\nscore, and mean average precision (mAP). Our findings demonstrate that YOLO11\nsurpasses previous versions (YOLOv8 and YOLOv10) in detecting smaller and more\noccluded vehicles while maintaining a competitive inference time, making it\nwell-suited for real-time applications. Comparative analysis shows significant\nimprovements in the detection of complex vehicle geometries, further\ncontributing to the development of efficient and scalable vehicle detection\nsystems. This research highlights YOLO11's potential to enhance autonomous\nvehicle performance and traffic monitoring systems, offering insights for\nfuture developments in the field."
                },
                "authors": [
                    {
                        "name": "Mujadded Al Rabbani Alif"
                    }
                ],
                "author_detail": {
                    "name": "Mujadded Al Rabbani Alif"
                },
                "author": "Mujadded Al Rabbani Alif",
                "arxiv_comment": "16 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.22898v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.22898v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.16298v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.16298v2",
                "updated": "2024-10-30T10:55:11Z",
                "updated_parsed": [
                    2024,
                    10,
                    30,
                    10,
                    55,
                    11,
                    2,
                    304,
                    0
                ],
                "published": "2024-10-07T05:04:13Z",
                "published_parsed": [
                    2024,
                    10,
                    7,
                    5,
                    4,
                    13,
                    0,
                    281,
                    0
                ],
                "title": "Hardware-Software Co-optimised Fast and Accurate Deep Reconfigurable\n  Spiking Inference Accelerator Architecture Design Methodology",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Hardware-Software Co-optimised Fast and Accurate Deep Reconfigurable\n  Spiking Inference Accelerator Architecture Design Methodology"
                },
                "summary": "Spiking Neural Networks (SNNs) have emerged as a promising approach to\nimprove the energy efficiency of machine learning models, as they naturally\nimplement event-driven computations while avoiding expensive multiplication\noperations. In this paper, we develop a hardware-software co-optimisation\nstrategy to port software-trained deep neural networks (DNN) to\nreduced-precision spiking models demonstrating fast and accurate inference in a\nnovel event-driven CMOS reconfigurable spiking inference accelerator.\nExperimental results show that a reduced-precision Resnet-18 and VGG-11 SNN\nmodels achieves classification accuracy within 1% of the baseline\nfull-precision DNN model within 8 spike timesteps. We also demonstrate an FPGA\nprototype implementation of the spiking inference accelerator with a throughput\nof 38.4 giga operations per second (GOPS) consuming 1.54 Watts on PYNQ-Z2 FPGA.\nThis corresponds to 0.6 GOPS per processing element and 2.25,GOPS/DSP slice,\nwhich is 2x and 4.5x higher utilisation efficiency respectively compared to the\nstate-of-the-art. Our co-optimisation strategy can be employed to develop deep\nreduced precision SNN models and port them to resource-efficient event-driven\nhardware accelerators for edge applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Spiking Neural Networks (SNNs) have emerged as a promising approach to\nimprove the energy efficiency of machine learning models, as they naturally\nimplement event-driven computations while avoiding expensive multiplication\noperations. In this paper, we develop a hardware-software co-optimisation\nstrategy to port software-trained deep neural networks (DNN) to\nreduced-precision spiking models demonstrating fast and accurate inference in a\nnovel event-driven CMOS reconfigurable spiking inference accelerator.\nExperimental results show that a reduced-precision Resnet-18 and VGG-11 SNN\nmodels achieves classification accuracy within 1% of the baseline\nfull-precision DNN model within 8 spike timesteps. We also demonstrate an FPGA\nprototype implementation of the spiking inference accelerator with a throughput\nof 38.4 giga operations per second (GOPS) consuming 1.54 Watts on PYNQ-Z2 FPGA.\nThis corresponds to 0.6 GOPS per processing element and 2.25,GOPS/DSP slice,\nwhich is 2x and 4.5x higher utilisation efficiency respectively compared to the\nstate-of-the-art. Our co-optimisation strategy can be employed to develop deep\nreduced precision SNN models and port them to resource-efficient event-driven\nhardware accelerators for edge applications."
                },
                "authors": [
                    {
                        "name": "Anagha Nimbekar"
                    },
                    {
                        "name": "Prabodh Katti"
                    },
                    {
                        "name": "Chen Li"
                    },
                    {
                        "name": "Bashir M. Al-Hashimi"
                    },
                    {
                        "name": "Amit Acharyya"
                    },
                    {
                        "name": "Bipin Rajendran"
                    }
                ],
                "author_detail": {
                    "name": "Bipin Rajendran"
                },
                "author": "Bipin Rajendran",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.16298v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.16298v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.08706v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.08706v2",
                "updated": "2024-10-30T10:30:57Z",
                "updated_parsed": [
                    2024,
                    10,
                    30,
                    10,
                    30,
                    57,
                    2,
                    304,
                    0
                ],
                "published": "2024-09-13T10:48:35Z",
                "published_parsed": [
                    2024,
                    9,
                    13,
                    10,
                    48,
                    35,
                    4,
                    257,
                    0
                ],
                "title": "L3Cube-IndicQuest: A Benchmark Question Answering Dataset for Evaluating\n  Knowledge of LLMs in Indic Context",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "L3Cube-IndicQuest: A Benchmark Question Answering Dataset for Evaluating\n  Knowledge of LLMs in Indic Context"
                },
                "summary": "Large Language Models (LLMs) have made significant progress in incorporating\nIndic languages within multilingual models. However, it is crucial to\nquantitatively assess whether these languages perform comparably to globally\ndominant ones, such as English. Currently, there is a lack of benchmark\ndatasets specifically designed to evaluate the regional knowledge of LLMs in\nvarious Indic languages. In this paper, we present the L3Cube-IndicQuest, a\ngold-standard factual question-answering benchmark dataset designed to evaluate\nhow well multilingual LLMs capture regional knowledge across various Indic\nlanguages. The dataset contains 200 question-answer pairs, each for English and\n19 Indic languages, covering five domains specific to the Indic region. We aim\nfor this dataset to serve as a benchmark, providing ground truth for evaluating\nthe performance of LLMs in understanding and representing knowledge relevant to\nthe Indian context. The IndicQuest can be used for both reference-based\nevaluation and LLM-as-a-judge evaluation. The dataset is shared publicly at\nhttps://github.com/l3cube-pune/indic-nlp .",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have made significant progress in incorporating\nIndic languages within multilingual models. However, it is crucial to\nquantitatively assess whether these languages perform comparably to globally\ndominant ones, such as English. Currently, there is a lack of benchmark\ndatasets specifically designed to evaluate the regional knowledge of LLMs in\nvarious Indic languages. In this paper, we present the L3Cube-IndicQuest, a\ngold-standard factual question-answering benchmark dataset designed to evaluate\nhow well multilingual LLMs capture regional knowledge across various Indic\nlanguages. The dataset contains 200 question-answer pairs, each for English and\n19 Indic languages, covering five domains specific to the Indic region. We aim\nfor this dataset to serve as a benchmark, providing ground truth for evaluating\nthe performance of LLMs in understanding and representing knowledge relevant to\nthe Indian context. The IndicQuest can be used for both reference-based\nevaluation and LLM-as-a-judge evaluation. The dataset is shared publicly at\nhttps://github.com/l3cube-pune/indic-nlp ."
                },
                "authors": [
                    {
                        "name": "Pritika Rohera"
                    },
                    {
                        "name": "Chaitrali Ginimav"
                    },
                    {
                        "name": "Akanksha Salunke"
                    },
                    {
                        "name": "Gayatri Sawant"
                    },
                    {
                        "name": "Raviraj Joshi"
                    }
                ],
                "author_detail": {
                    "name": "Raviraj Joshi"
                },
                "author": "Raviraj Joshi",
                "arxiv_comment": "Accepted at PACLIC 38 (2024)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.08706v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.08706v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.05798v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.05798v3",
                "updated": "2024-10-30T10:28:36Z",
                "updated_parsed": [
                    2024,
                    10,
                    30,
                    10,
                    28,
                    36,
                    2,
                    304,
                    0
                ],
                "published": "2024-09-09T17:02:47Z",
                "published_parsed": [
                    2024,
                    9,
                    9,
                    17,
                    2,
                    47,
                    0,
                    253,
                    0
                ],
                "title": "Enhancing Preference-based Linear Bandits via Human Response Time",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Enhancing Preference-based Linear Bandits via Human Response Time"
                },
                "summary": "Interactive preference learning systems present humans with queries as pairs\nof options; humans then select their preferred choice, allowing the system to\ninfer preferences from these binary choices. While binary choice feedback is\nsimple and widely used, it offers limited information about preference\nstrength. To address this, we leverage human response times, which inversely\ncorrelate with preference strength, as complementary information. We introduce\na computationally efficient method based on the EZ-diffusion model, combining\nchoices and response times to estimate the underlying human utility function.\nTheoretical and empirical comparisons with traditional choice-only estimators\nshow that for queries where humans have strong preferences (i.e., \"easy\"\nqueries), response times provide valuable complementary information and enhance\nutility estimates. We integrate this estimator into preference-based linear\nbandits for fixed-budget best-arm identification. Simulations on three\nreal-world datasets demonstrate that incorporating response times significantly\naccelerates preference learning.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Interactive preference learning systems present humans with queries as pairs\nof options; humans then select their preferred choice, allowing the system to\ninfer preferences from these binary choices. While binary choice feedback is\nsimple and widely used, it offers limited information about preference\nstrength. To address this, we leverage human response times, which inversely\ncorrelate with preference strength, as complementary information. We introduce\na computationally efficient method based on the EZ-diffusion model, combining\nchoices and response times to estimate the underlying human utility function.\nTheoretical and empirical comparisons with traditional choice-only estimators\nshow that for queries where humans have strong preferences (i.e., \"easy\"\nqueries), response times provide valuable complementary information and enhance\nutility estimates. We integrate this estimator into preference-based linear\nbandits for fixed-budget best-arm identification. Simulations on three\nreal-world datasets demonstrate that incorporating response times significantly\naccelerates preference learning."
                },
                "authors": [
                    {
                        "name": "Shen Li"
                    },
                    {
                        "name": "Yuyang Zhang"
                    },
                    {
                        "name": "Zhaolin Ren"
                    },
                    {
                        "name": "Claire Liang"
                    },
                    {
                        "name": "Na Li"
                    },
                    {
                        "name": "Julie A. Shah"
                    }
                ],
                "author_detail": {
                    "name": "Julie A. Shah"
                },
                "author": "Julie A. Shah",
                "arxiv_comment": "To appear in NeurIPS 2024 (Oral)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.05798v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.05798v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "econ.EM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.22884v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.22884v1",
                "updated": "2024-10-30T10:25:35Z",
                "updated_parsed": [
                    2024,
                    10,
                    30,
                    10,
                    25,
                    35,
                    2,
                    304,
                    0
                ],
                "published": "2024-10-30T10:25:35Z",
                "published_parsed": [
                    2024,
                    10,
                    30,
                    10,
                    25,
                    35,
                    2,
                    304,
                    0
                ],
                "title": "Stealing User Prompts from Mixture of Experts",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Stealing User Prompts from Mixture of Experts"
                },
                "summary": "Mixture-of-Experts (MoE) models improve the efficiency and scalability of\ndense language models by routing each token to a small number of experts in\neach layer. In this paper, we show how an adversary that can arrange for their\nqueries to appear in the same batch of examples as a victim's queries can\nexploit Expert-Choice-Routing to fully disclose a victim's prompt. We\nsuccessfully demonstrate the effectiveness of this attack on a two-layer\nMixtral model, exploiting the tie-handling behavior of the torch.topk CUDA\nimplementation. Our results show that we can extract the entire prompt using\n$O({VM}^2)$ queries (with vocabulary size $V$ and prompt length $M$) or 100\nqueries on average per token in the setting we consider. This is the first\nattack to exploit architectural flaws for the purpose of extracting user\nprompts, introducing a new class of LLM vulnerabilities.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mixture-of-Experts (MoE) models improve the efficiency and scalability of\ndense language models by routing each token to a small number of experts in\neach layer. In this paper, we show how an adversary that can arrange for their\nqueries to appear in the same batch of examples as a victim's queries can\nexploit Expert-Choice-Routing to fully disclose a victim's prompt. We\nsuccessfully demonstrate the effectiveness of this attack on a two-layer\nMixtral model, exploiting the tie-handling behavior of the torch.topk CUDA\nimplementation. Our results show that we can extract the entire prompt using\n$O({VM}^2)$ queries (with vocabulary size $V$ and prompt length $M$) or 100\nqueries on average per token in the setting we consider. This is the first\nattack to exploit architectural flaws for the purpose of extracting user\nprompts, introducing a new class of LLM vulnerabilities."
                },
                "authors": [
                    {
                        "name": "Itay Yona"
                    },
                    {
                        "name": "Ilia Shumailov"
                    },
                    {
                        "name": "Jamie Hayes"
                    },
                    {
                        "name": "Nicholas Carlini"
                    }
                ],
                "author_detail": {
                    "name": "Nicholas Carlini"
                },
                "author": "Nicholas Carlini",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.22884v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.22884v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.19572v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.19572v3",
                "updated": "2024-10-30T10:15:31Z",
                "updated_parsed": [
                    2024,
                    10,
                    30,
                    10,
                    15,
                    31,
                    2,
                    304,
                    0
                ],
                "published": "2024-10-25T14:07:53Z",
                "published_parsed": [
                    2024,
                    10,
                    25,
                    14,
                    7,
                    53,
                    4,
                    299,
                    0
                ],
                "title": "ChunkRAG: Novel LLM-Chunk Filtering Method for RAG Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ChunkRAG: Novel LLM-Chunk Filtering Method for RAG Systems"
                },
                "summary": "Retrieval-Augmented Generation (RAG) systems using large language models\n(LLMs) often generate inaccurate responses due to the retrieval of irrelevant\nor loosely related information. Existing methods, which operate at the document\nlevel, fail to effectively filter out such content. We propose LLM-driven chunk\nfiltering, ChunkRAG, a framework that enhances RAG systems by evaluating and\nfiltering retrieved information at the chunk level. Our approach employs\nsemantic chunking to divide documents into coherent sections and utilizes\nLLM-based relevance scoring to assess each chunk's alignment with the user's\nquery. By filtering out less pertinent chunks before the generation phase, we\nsignificantly reduce hallucinations and improve factual accuracy. Experiments\nshow that our method outperforms existing RAG models, achieving higher accuracy\non tasks requiring precise information retrieval. This advancement enhances the\nreliability of RAG systems, making them particularly beneficial for\napplications like fact-checking and multi-hop reasoning.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Retrieval-Augmented Generation (RAG) systems using large language models\n(LLMs) often generate inaccurate responses due to the retrieval of irrelevant\nor loosely related information. Existing methods, which operate at the document\nlevel, fail to effectively filter out such content. We propose LLM-driven chunk\nfiltering, ChunkRAG, a framework that enhances RAG systems by evaluating and\nfiltering retrieved information at the chunk level. Our approach employs\nsemantic chunking to divide documents into coherent sections and utilizes\nLLM-based relevance scoring to assess each chunk's alignment with the user's\nquery. By filtering out less pertinent chunks before the generation phase, we\nsignificantly reduce hallucinations and improve factual accuracy. Experiments\nshow that our method outperforms existing RAG models, achieving higher accuracy\non tasks requiring precise information retrieval. This advancement enhances the\nreliability of RAG systems, making them particularly beneficial for\napplications like fact-checking and multi-hop reasoning."
                },
                "authors": [
                    {
                        "name": "Ishneet Sukhvinder Singh"
                    },
                    {
                        "name": "Ritvik Aggarwal"
                    },
                    {
                        "name": "Ibrahim Allahverdiyev"
                    },
                    {
                        "name": "Muhammad Taha"
                    },
                    {
                        "name": "Aslihan Akalin"
                    },
                    {
                        "name": "Kevin Zhu"
                    },
                    {
                        "name": "Sean O'Brien"
                    }
                ],
                "author_detail": {
                    "name": "Sean O'Brien"
                },
                "author": "Sean O'Brien",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.19572v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.19572v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.22874v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.22874v1",
                "updated": "2024-10-30T10:11:53Z",
                "updated_parsed": [
                    2024,
                    10,
                    30,
                    10,
                    11,
                    53,
                    2,
                    304,
                    0
                ],
                "published": "2024-10-30T10:11:53Z",
                "published_parsed": [
                    2024,
                    10,
                    30,
                    10,
                    11,
                    53,
                    2,
                    304,
                    0
                ],
                "title": "Eliciting Critical Reasoning in Retrieval-Augmented Language Models via\n  Contrastive Explanations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Eliciting Critical Reasoning in Retrieval-Augmented Language Models via\n  Contrastive Explanations"
                },
                "summary": "Retrieval-augmented generation (RAG) has emerged as a critical mechanism in\ncontemporary NLP to support Large Language Models(LLMs) in systematically\naccessing richer factual context. However, the integration of RAG mechanisms\nbrings its inherent challenges, as LLMs need to deal with potentially noisy\ncontexts. Recent studies have shown that LLMs still struggle to critically\nanalyse RAG-based in-context information, a limitation that may lead to\nincorrect inferences and hallucinations. In this paper, we investigate how to\nelicit critical reasoning in RAG via contrastive explanations. In particular,\nwe propose Contrastive-RAG (C-RAG), a framework that (i) retrieves relevant\ndocuments given a query, (ii) selects and exemplifies relevant passages, and\n(iii) generates explanations that explicitly contrast the relevance of the\npassages to (iv) support the final answer. We show the impact of C-RAG building\ncontrastive reasoning demonstrations from LLMs to instruct smaller models for\nretrieval-augmented tasks. Extensive experiments demonstrate that C-RAG\nimproves state-of-the-art RAG models while (a) requiring significantly fewer\nprompts and demonstrations and (b) being robust to perturbations in the\nretrieved documents.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Retrieval-augmented generation (RAG) has emerged as a critical mechanism in\ncontemporary NLP to support Large Language Models(LLMs) in systematically\naccessing richer factual context. However, the integration of RAG mechanisms\nbrings its inherent challenges, as LLMs need to deal with potentially noisy\ncontexts. Recent studies have shown that LLMs still struggle to critically\nanalyse RAG-based in-context information, a limitation that may lead to\nincorrect inferences and hallucinations. In this paper, we investigate how to\nelicit critical reasoning in RAG via contrastive explanations. In particular,\nwe propose Contrastive-RAG (C-RAG), a framework that (i) retrieves relevant\ndocuments given a query, (ii) selects and exemplifies relevant passages, and\n(iii) generates explanations that explicitly contrast the relevance of the\npassages to (iv) support the final answer. We show the impact of C-RAG building\ncontrastive reasoning demonstrations from LLMs to instruct smaller models for\nretrieval-augmented tasks. Extensive experiments demonstrate that C-RAG\nimproves state-of-the-art RAG models while (a) requiring significantly fewer\nprompts and demonstrations and (b) being robust to perturbations in the\nretrieved documents."
                },
                "authors": [
                    {
                        "name": "Leonardo Ranaldi"
                    },
                    {
                        "name": "Marco Valentino"
                    },
                    {
                        "name": "Andrè Freitas"
                    }
                ],
                "author_detail": {
                    "name": "Andrè Freitas"
                },
                "author": "Andrè Freitas",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.22874v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.22874v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.05404v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.05404v2",
                "updated": "2024-10-30T10:01:06Z",
                "updated_parsed": [
                    2024,
                    10,
                    30,
                    10,
                    1,
                    6,
                    2,
                    304,
                    0
                ],
                "published": "2024-09-09T08:05:43Z",
                "published_parsed": [
                    2024,
                    9,
                    9,
                    8,
                    5,
                    43,
                    0,
                    253,
                    0
                ],
                "title": "DFabric: Scaling Out Data Parallel Applications with CXL-Ethernet Hybrid\n  Interconnects",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DFabric: Scaling Out Data Parallel Applications with CXL-Ethernet Hybrid\n  Interconnects"
                },
                "summary": "Emerging interconnects, such as CXL and NVLink, have been integrated into the\nintra-host topology to scale more accelerators and facilitate efficient\ncommunication between them, such as GPUs. To keep pace with the accelerator's\ngrowing computing throughput, the interconnect has seen substantial enhancement\nin link bandwidth, e.g., 256GBps for CXL 3.0 links, which surpasses Ethernet\nand InfiniBand network links by an order of magnitude or more. Consequently,\nwhen data-intensive jobs, such as LLM training, scale across multiple hosts\nbeyond the reach limit of the interconnect, the performance is significantly\nhindered by the limiting bandwidth of the network infrastructure. We address\nthe problem by proposing DFabric, a two-tier interconnect architecture. We\naddress the problem by proposing DFabric, a two-tier interconnect architecture.\nFirst, DFabric disaggregates rack's computing units with an interconnect\nfabric, i.e., CXL fabric, which scales at rack-level, so that they can enjoy\nintra-rack efficient interconnecting. Second, DFabric disaggregates NICs from\nhosts, and consolidates them to form a NIC pool with CXL fabric. By providing\nsufficient aggregated capacity comparable to interconnect bandwidth, the NIC\npool bridges efficient communication across racks or beyond the reach limit of\ninterconnect fabric. However, the local memory accessing becomes the bottleneck\nwhen enabling each host to utilize the NIC pool efficiently. To the end,\nDFabric builds a memory pool with sufficient bandwidth by disaggregating host\nlocal memory and adding more memory devices. We have implemented a prototype of\nDFabric that can run applications transparently. We validated its performance\ngain by running various microbenchmarks and compute-intensive applications such\nas DNN and graph.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Emerging interconnects, such as CXL and NVLink, have been integrated into the\nintra-host topology to scale more accelerators and facilitate efficient\ncommunication between them, such as GPUs. To keep pace with the accelerator's\ngrowing computing throughput, the interconnect has seen substantial enhancement\nin link bandwidth, e.g., 256GBps for CXL 3.0 links, which surpasses Ethernet\nand InfiniBand network links by an order of magnitude or more. Consequently,\nwhen data-intensive jobs, such as LLM training, scale across multiple hosts\nbeyond the reach limit of the interconnect, the performance is significantly\nhindered by the limiting bandwidth of the network infrastructure. We address\nthe problem by proposing DFabric, a two-tier interconnect architecture. We\naddress the problem by proposing DFabric, a two-tier interconnect architecture.\nFirst, DFabric disaggregates rack's computing units with an interconnect\nfabric, i.e., CXL fabric, which scales at rack-level, so that they can enjoy\nintra-rack efficient interconnecting. Second, DFabric disaggregates NICs from\nhosts, and consolidates them to form a NIC pool with CXL fabric. By providing\nsufficient aggregated capacity comparable to interconnect bandwidth, the NIC\npool bridges efficient communication across racks or beyond the reach limit of\ninterconnect fabric. However, the local memory accessing becomes the bottleneck\nwhen enabling each host to utilize the NIC pool efficiently. To the end,\nDFabric builds a memory pool with sufficient bandwidth by disaggregating host\nlocal memory and adding more memory devices. We have implemented a prototype of\nDFabric that can run applications transparently. We validated its performance\ngain by running various microbenchmarks and compute-intensive applications such\nas DNN and graph."
                },
                "authors": [
                    {
                        "name": "Xu Zhang"
                    },
                    {
                        "name": "Ke Liu"
                    },
                    {
                        "name": "Yisong Chang"
                    },
                    {
                        "name": "Ke Zhang"
                    },
                    {
                        "name": "Mingyu Chen"
                    }
                ],
                "author_detail": {
                    "name": "Mingyu Chen"
                },
                "author": "Mingyu Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.05404v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.05404v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.ET",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.10248v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.10248v4",
                "updated": "2024-10-30T09:48:52Z",
                "updated_parsed": [
                    2024,
                    10,
                    30,
                    9,
                    48,
                    52,
                    2,
                    304,
                    0
                ],
                "published": "2024-06-08T13:40:38Z",
                "published_parsed": [
                    2024,
                    6,
                    8,
                    13,
                    40,
                    38,
                    5,
                    160,
                    0
                ],
                "title": "On the Worst Prompt Performance of Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "On the Worst Prompt Performance of Large Language Models"
                },
                "summary": "The performance of large language models (LLMs) is acutely sensitive to the\nphrasing of prompts, which raises significant concerns about their reliability\nin real-world scenarios. Existing studies often divide prompts into task-level\ninstructions and case-level inputs and primarily focus on evaluating and\nimproving robustness against variations in tasks-level instructions. However,\nthis setup fails to fully address the diversity of real-world user queries and\nassumes the existence of task-specific datasets. To address these limitations,\nwe introduce RobustAlpacaEval, a new benchmark that consists of semantically\nequivalent case-level queries and emphasizes the importance of using the worst\nprompt performance to gauge the lower bound of model performance. Extensive\nexperiments on RobustAlpacaEval with ChatGPT and six open-source LLMs from the\nLlama, Mistral, and Gemma families uncover substantial variability in model\nperformance; for instance, a difference of 45.48% between the worst and best\nperformance for the Llama-2-70B-chat model, with its worst performance dipping\nas low as 9.38%. We further illustrate the difficulty in identifying the worst\nprompt from both model-agnostic and model-dependent perspectives, emphasizing\nthe absence of a shortcut to characterize the worst prompt. We also attempt to\nenhance the worst prompt performance using existing prompt engineering and\nprompt consistency methods, but find that their impact is limited. These\nfindings underscore the need to create more resilient LLMs that can maintain\nhigh performance across diverse prompts. Data and code are available at\nhttps://github.com/cbwbuaa/On-the-Worst-Prompt- Performance-of-LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The performance of large language models (LLMs) is acutely sensitive to the\nphrasing of prompts, which raises significant concerns about their reliability\nin real-world scenarios. Existing studies often divide prompts into task-level\ninstructions and case-level inputs and primarily focus on evaluating and\nimproving robustness against variations in tasks-level instructions. However,\nthis setup fails to fully address the diversity of real-world user queries and\nassumes the existence of task-specific datasets. To address these limitations,\nwe introduce RobustAlpacaEval, a new benchmark that consists of semantically\nequivalent case-level queries and emphasizes the importance of using the worst\nprompt performance to gauge the lower bound of model performance. Extensive\nexperiments on RobustAlpacaEval with ChatGPT and six open-source LLMs from the\nLlama, Mistral, and Gemma families uncover substantial variability in model\nperformance; for instance, a difference of 45.48% between the worst and best\nperformance for the Llama-2-70B-chat model, with its worst performance dipping\nas low as 9.38%. We further illustrate the difficulty in identifying the worst\nprompt from both model-agnostic and model-dependent perspectives, emphasizing\nthe absence of a shortcut to characterize the worst prompt. We also attempt to\nenhance the worst prompt performance using existing prompt engineering and\nprompt consistency methods, but find that their impact is limited. These\nfindings underscore the need to create more resilient LLMs that can maintain\nhigh performance across diverse prompts. Data and code are available at\nhttps://github.com/cbwbuaa/On-the-Worst-Prompt- Performance-of-LLMs."
                },
                "authors": [
                    {
                        "name": "Bowen Cao"
                    },
                    {
                        "name": "Deng Cai"
                    },
                    {
                        "name": "Zhisong Zhang"
                    },
                    {
                        "name": "Yuexian Zou"
                    },
                    {
                        "name": "Wai Lam"
                    }
                ],
                "author_detail": {
                    "name": "Wai Lam"
                },
                "author": "Wai Lam",
                "arxiv_comment": "Accepted at NeurIPS 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.10248v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.10248v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.22857v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.22857v1",
                "updated": "2024-10-30T09:42:47Z",
                "updated_parsed": [
                    2024,
                    10,
                    30,
                    9,
                    42,
                    47,
                    2,
                    304,
                    0
                ],
                "published": "2024-10-30T09:42:47Z",
                "published_parsed": [
                    2024,
                    10,
                    30,
                    9,
                    42,
                    47,
                    2,
                    304,
                    0
                ],
                "title": "DAVINCI: A Single-Stage Architecture for Constrained CAD Sketch\n  Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DAVINCI: A Single-Stage Architecture for Constrained CAD Sketch\n  Inference"
                },
                "summary": "This work presents DAVINCI, a unified architecture for single-stage\nComputer-Aided Design (CAD) sketch parameterization and constraint inference\ndirectly from raster sketch images. By jointly learning both outputs, DAVINCI\nminimizes error accumulation and enhances the performance of constrained CAD\nsketch inference. Notably, DAVINCI achieves state-of-the-art results on the\nlarge-scale SketchGraphs dataset, demonstrating effectiveness on both precise\nand hand-drawn raster CAD sketches. To reduce DAVINCI's reliance on large-scale\nannotated datasets, we explore the efficacy of CAD sketch augmentations. We\nintroduce Constraint-Preserving Transformations (CPTs), i.e. random\npermutations of the parametric primitives of a CAD sketch that preserve its\nconstraints. This data augmentation strategy allows DAVINCI to achieve\nreasonable performance when trained with only 0.1% of the SketchGraphs dataset.\nFurthermore, this work contributes a new version of SketchGraphs, augmented\nwith CPTs. The newly introduced CPTSketchGraphs dataset includes 80 million\nCPT-augmented sketches, thus providing a rich resource for future research in\nthe CAD sketch domain.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This work presents DAVINCI, a unified architecture for single-stage\nComputer-Aided Design (CAD) sketch parameterization and constraint inference\ndirectly from raster sketch images. By jointly learning both outputs, DAVINCI\nminimizes error accumulation and enhances the performance of constrained CAD\nsketch inference. Notably, DAVINCI achieves state-of-the-art results on the\nlarge-scale SketchGraphs dataset, demonstrating effectiveness on both precise\nand hand-drawn raster CAD sketches. To reduce DAVINCI's reliance on large-scale\nannotated datasets, we explore the efficacy of CAD sketch augmentations. We\nintroduce Constraint-Preserving Transformations (CPTs), i.e. random\npermutations of the parametric primitives of a CAD sketch that preserve its\nconstraints. This data augmentation strategy allows DAVINCI to achieve\nreasonable performance when trained with only 0.1% of the SketchGraphs dataset.\nFurthermore, this work contributes a new version of SketchGraphs, augmented\nwith CPTs. The newly introduced CPTSketchGraphs dataset includes 80 million\nCPT-augmented sketches, thus providing a rich resource for future research in\nthe CAD sketch domain."
                },
                "authors": [
                    {
                        "name": "Ahmet Serdar Karadeniz"
                    },
                    {
                        "name": "Dimitrios Mallis"
                    },
                    {
                        "name": "Nesryne Mejri"
                    },
                    {
                        "name": "Kseniya Cherenkova"
                    },
                    {
                        "name": "Anis Kacem"
                    },
                    {
                        "name": "Djamila Aouada"
                    }
                ],
                "author_detail": {
                    "name": "Djamila Aouada"
                },
                "author": "Djamila Aouada",
                "arxiv_comment": "Accepted at BMVC 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.22857v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.22857v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            }
        ]
    },
    {
        "keyword": "LLM Deployment",
        "arxiv_results": [
            {
                "id": "http://arxiv.org/abs/2406.13236v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.13236v2",
                "updated": "2024-10-30T17:59:08Z",
                "updated_parsed": [
                    2024,
                    10,
                    30,
                    17,
                    59,
                    8,
                    2,
                    304,
                    0
                ],
                "published": "2024-06-19T05:53:27Z",
                "published_parsed": [
                    2024,
                    6,
                    19,
                    5,
                    53,
                    27,
                    2,
                    171,
                    0
                ],
                "title": "Data Contamination Can Cross Language Barriers",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Data Contamination Can Cross Language Barriers"
                },
                "summary": "The opacity in developing large language models (LLMs) is raising growing\nconcerns about the potential contamination of public benchmarks in the\npre-training data. Existing contamination detection methods are typically based\non the text overlap between training and evaluation data, which can be too\nsuperficial to reflect deeper forms of contamination. In this paper, we first\npresent a cross-lingual form of contamination that inflates LLMs' performance\nwhile evading current detection methods, deliberately injected by overfitting\nLLMs on the translated versions of benchmark test sets. Then, we propose\ngeneralization-based approaches to unmask such deeply concealed contamination.\nSpecifically, we examine the LLM's performance change after modifying the\noriginal benchmark by replacing the false answer choices with correct ones from\nother questions. Contaminated models can hardly generalize to such easier\nsituations, where the false choices can be \\emph{not even wrong}, as all\nchoices are correct in their memorization. Experimental results demonstrate\nthat cross-lingual contamination can easily fool existing detection methods,\nbut not ours. In addition, we discuss the potential utilization of\ncross-lingual contamination in interpreting LLMs' working mechanisms and in\npost-training LLMs for enhanced multilingual capabilities. The code and dataset\nwe use can be obtained from \\url{https://github.com/ShangDataLab/Deep-Contam}.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The opacity in developing large language models (LLMs) is raising growing\nconcerns about the potential contamination of public benchmarks in the\npre-training data. Existing contamination detection methods are typically based\non the text overlap between training and evaluation data, which can be too\nsuperficial to reflect deeper forms of contamination. In this paper, we first\npresent a cross-lingual form of contamination that inflates LLMs' performance\nwhile evading current detection methods, deliberately injected by overfitting\nLLMs on the translated versions of benchmark test sets. Then, we propose\ngeneralization-based approaches to unmask such deeply concealed contamination.\nSpecifically, we examine the LLM's performance change after modifying the\noriginal benchmark by replacing the false answer choices with correct ones from\nother questions. Contaminated models can hardly generalize to such easier\nsituations, where the false choices can be \\emph{not even wrong}, as all\nchoices are correct in their memorization. Experimental results demonstrate\nthat cross-lingual contamination can easily fool existing detection methods,\nbut not ours. In addition, we discuss the potential utilization of\ncross-lingual contamination in interpreting LLMs' working mechanisms and in\npost-training LLMs for enhanced multilingual capabilities. The code and dataset\nwe use can be obtained from \\url{https://github.com/ShangDataLab/Deep-Contam}."
                },
                "authors": [
                    {
                        "name": "Feng Yao"
                    },
                    {
                        "name": "Yufan Zhuang"
                    },
                    {
                        "name": "Zihao Sun"
                    },
                    {
                        "name": "Sunan Xu"
                    },
                    {
                        "name": "Animesh Kumar"
                    },
                    {
                        "name": "Jingbo Shang"
                    }
                ],
                "author_detail": {
                    "name": "Jingbo Shang"
                },
                "author": "Jingbo Shang",
                "arxiv_comment": "EMNLP 2024 Main camera-ready version",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.13236v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.13236v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.20533v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.20533v2",
                "updated": "2024-10-30T17:56:22Z",
                "updated_parsed": [
                    2024,
                    10,
                    30,
                    17,
                    56,
                    22,
                    2,
                    304,
                    0
                ],
                "published": "2024-10-27T17:55:27Z",
                "published_parsed": [
                    2024,
                    10,
                    27,
                    17,
                    55,
                    27,
                    6,
                    301,
                    0
                ],
                "title": "Guiding Through Complexity: What Makes Good Supervision for Hard\n  Reasoning Tasks?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Guiding Through Complexity: What Makes Good Supervision for Hard\n  Reasoning Tasks?"
                },
                "summary": "How can \"weak teacher models\" such as average human annotators or existing AI\nsystems, effectively supervise LLMs to improve performance on hard reasoning\ntasks, especially those that challenge and requires expertise or daily practice\nfrom the teacher models? In this paper, we seek for empirical answers to this\nquestion by investigating various data-driven strategies that offer supervision\ndata at different quality levels upon tasks of varying complexity. Two\nintuitive strategies emerge for teacher models to provide supervision during\nalignment training: 1) using lower-quality supervision from complete tasks that\nmatch the difficulty of the target reasoning tasks, and 2) leveraging\nhigher-quality supervision from easier subtasks that are less challenging.\nInterestingly, we find that even when the outcome error rate for hard task\nsupervision is high (e.g., 90\\%), training on such data can outperform\nperfectly correct supervision on easier subtasks on multiple hard math\nbenchmarks. We further identify a more critical factor influencing training\nperformance: step-wise error rates, which indicate the severity of errors in\nsolutions. Specifically, training on hard task supervision with the same\noutcome error rates but disparate step-wise error rates can lead to a 30\\%\naccuracy gap on MATH benchmark. Our results also reveal that supplementing hard\ntask supervision with the corresponding subtask supervision can yield notable\nperformance improvements than simply combining rephrased hard full task\nsupervision, suggesting new avenues for data augmentation. Data and code are\nreleased at \\url{https://github.com/hexuan21/Weak-to-Strong}.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "How can \"weak teacher models\" such as average human annotators or existing AI\nsystems, effectively supervise LLMs to improve performance on hard reasoning\ntasks, especially those that challenge and requires expertise or daily practice\nfrom the teacher models? In this paper, we seek for empirical answers to this\nquestion by investigating various data-driven strategies that offer supervision\ndata at different quality levels upon tasks of varying complexity. Two\nintuitive strategies emerge for teacher models to provide supervision during\nalignment training: 1) using lower-quality supervision from complete tasks that\nmatch the difficulty of the target reasoning tasks, and 2) leveraging\nhigher-quality supervision from easier subtasks that are less challenging.\nInterestingly, we find that even when the outcome error rate for hard task\nsupervision is high (e.g., 90\\%), training on such data can outperform\nperfectly correct supervision on easier subtasks on multiple hard math\nbenchmarks. We further identify a more critical factor influencing training\nperformance: step-wise error rates, which indicate the severity of errors in\nsolutions. Specifically, training on hard task supervision with the same\noutcome error rates but disparate step-wise error rates can lead to a 30\\%\naccuracy gap on MATH benchmark. Our results also reveal that supplementing hard\ntask supervision with the corresponding subtask supervision can yield notable\nperformance improvements than simply combining rephrased hard full task\nsupervision, suggesting new avenues for data augmentation. Data and code are\nreleased at \\url{https://github.com/hexuan21/Weak-to-Strong}."
                },
                "authors": [
                    {
                        "name": "Xuan He"
                    },
                    {
                        "name": "Da Yin"
                    },
                    {
                        "name": "Nanyun Peng"
                    }
                ],
                "author_detail": {
                    "name": "Nanyun Peng"
                },
                "author": "Nanyun Peng",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.20533v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.20533v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.22217v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.22217v2",
                "updated": "2024-10-30T17:51:26Z",
                "updated_parsed": [
                    2024,
                    10,
                    30,
                    17,
                    51,
                    26,
                    2,
                    304,
                    0
                ],
                "published": "2024-10-29T16:48:22Z",
                "published_parsed": [
                    2024,
                    10,
                    29,
                    16,
                    48,
                    22,
                    1,
                    303,
                    0
                ],
                "title": "Towards Unifying Understanding and Generation in the Era of Vision\n  Foundation Models: A Survey from the Autoregression Perspective",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Towards Unifying Understanding and Generation in the Era of Vision\n  Foundation Models: A Survey from the Autoregression Perspective"
                },
                "summary": "Autoregression in large language models (LLMs) has shown impressive\nscalability by unifying all language tasks into the next token prediction\nparadigm. Recently, there is a growing interest in extending this success to\nvision foundation models. In this survey, we review the recent advances and\ndiscuss future directions for autoregressive vision foundation models. First,\nwe present the trend for next generation of vision foundation models, i.e.,\nunifying both understanding and generation in vision tasks. We then analyze the\nlimitations of existing vision foundation models, and present a formal\ndefinition of autoregression with its advantages. Later, we categorize\nautoregressive vision foundation models from their vision tokenizers and\nautoregression backbones. Finally, we discuss several promising research\nchallenges and directions. To the best of our knowledge, this is the first\nsurvey to comprehensively summarize autoregressive vision foundation models\nunder the trend of unifying understanding and generation. A collection of\nrelated resources is available at https://github.com/EmmaSRH/ARVFM.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Autoregression in large language models (LLMs) has shown impressive\nscalability by unifying all language tasks into the next token prediction\nparadigm. Recently, there is a growing interest in extending this success to\nvision foundation models. In this survey, we review the recent advances and\ndiscuss future directions for autoregressive vision foundation models. First,\nwe present the trend for next generation of vision foundation models, i.e.,\nunifying both understanding and generation in vision tasks. We then analyze the\nlimitations of existing vision foundation models, and present a formal\ndefinition of autoregression with its advantages. Later, we categorize\nautoregressive vision foundation models from their vision tokenizers and\nautoregression backbones. Finally, we discuss several promising research\nchallenges and directions. To the best of our knowledge, this is the first\nsurvey to comprehensively summarize autoregressive vision foundation models\nunder the trend of unifying understanding and generation. A collection of\nrelated resources is available at https://github.com/EmmaSRH/ARVFM."
                },
                "authors": [
                    {
                        "name": "Shenghao Xie"
                    },
                    {
                        "name": "Wenqiang Zu"
                    },
                    {
                        "name": "Mingyang Zhao"
                    },
                    {
                        "name": "Duo Su"
                    },
                    {
                        "name": "Shilong Liu"
                    },
                    {
                        "name": "Ruohua Shi"
                    },
                    {
                        "name": "Guoqi Li"
                    },
                    {
                        "name": "Shanghang Zhang"
                    },
                    {
                        "name": "Lei Ma"
                    }
                ],
                "author_detail": {
                    "name": "Lei Ma"
                },
                "author": "Lei Ma",
                "arxiv_comment": "17 pages, 1 table, 2 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.22217v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.22217v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.23252v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.23252v1",
                "updated": "2024-10-30T17:35:44Z",
                "updated_parsed": [
                    2024,
                    10,
                    30,
                    17,
                    35,
                    44,
                    2,
                    304,
                    0
                ],
                "published": "2024-10-30T17:35:44Z",
                "published_parsed": [
                    2024,
                    10,
                    30,
                    17,
                    35,
                    44,
                    2,
                    304,
                    0
                ],
                "title": "Evaluating Cultural and Social Awareness of LLM Web Agents",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Evaluating Cultural and Social Awareness of LLM Web Agents"
                },
                "summary": "As large language models (LLMs) expand into performing as agents for\nreal-world applications beyond traditional NLP tasks, evaluating their\nrobustness becomes increasingly important. However, existing benchmarks often\noverlook critical dimensions like cultural and social awareness. To address\nthese, we introduce CASA, a benchmark designed to assess LLM agents'\nsensitivity to cultural and social norms across two web-based tasks: online\nshopping and social discussion forums. Our approach evaluates LLM agents'\nability to detect and appropriately respond to norm-violating user queries and\nobservations. Furthermore, we propose a comprehensive evaluation framework that\nmeasures awareness coverage, helpfulness in managing user queries, and the\nviolation rate when facing misleading web content. Experiments show that\ncurrent LLMs perform significantly better in non-agent than in web-based agent\nenvironments, with agents achieving less than 10% awareness coverage and over\n40% violation rates. To improve performance, we explore two methods: prompting\nand fine-tuning, and find that combining both methods can offer complementary\nadvantages -- fine-tuning on culture-specific datasets significantly enhances\nthe agents' ability to generalize across different regions, while prompting\nboosts the agents' ability to navigate complex tasks. These findings highlight\nthe importance of constantly benchmarking LLM agents' cultural and social\nawareness during the development cycle.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As large language models (LLMs) expand into performing as agents for\nreal-world applications beyond traditional NLP tasks, evaluating their\nrobustness becomes increasingly important. However, existing benchmarks often\noverlook critical dimensions like cultural and social awareness. To address\nthese, we introduce CASA, a benchmark designed to assess LLM agents'\nsensitivity to cultural and social norms across two web-based tasks: online\nshopping and social discussion forums. Our approach evaluates LLM agents'\nability to detect and appropriately respond to norm-violating user queries and\nobservations. Furthermore, we propose a comprehensive evaluation framework that\nmeasures awareness coverage, helpfulness in managing user queries, and the\nviolation rate when facing misleading web content. Experiments show that\ncurrent LLMs perform significantly better in non-agent than in web-based agent\nenvironments, with agents achieving less than 10% awareness coverage and over\n40% violation rates. To improve performance, we explore two methods: prompting\nand fine-tuning, and find that combining both methods can offer complementary\nadvantages -- fine-tuning on culture-specific datasets significantly enhances\nthe agents' ability to generalize across different regions, while prompting\nboosts the agents' ability to navigate complex tasks. These findings highlight\nthe importance of constantly benchmarking LLM agents' cultural and social\nawareness during the development cycle."
                },
                "authors": [
                    {
                        "name": "Haoyi Qiu"
                    },
                    {
                        "name": "Alexander R. Fabbri"
                    },
                    {
                        "name": "Divyansh Agarwal"
                    },
                    {
                        "name": "Kung-Hsiang Huang"
                    },
                    {
                        "name": "Sarah Tan"
                    },
                    {
                        "name": "Nanyun Peng"
                    },
                    {
                        "name": "Chien-Sheng Wu"
                    }
                ],
                "author_detail": {
                    "name": "Chien-Sheng Wu"
                },
                "author": "Chien-Sheng Wu",
                "arxiv_comment": "Work in progress",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.23252v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.23252v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.23242v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.23242v1",
                "updated": "2024-10-30T17:28:28Z",
                "updated_parsed": [
                    2024,
                    10,
                    30,
                    17,
                    28,
                    28,
                    2,
                    304,
                    0
                ],
                "published": "2024-10-30T17:28:28Z",
                "published_parsed": [
                    2024,
                    10,
                    30,
                    17,
                    28,
                    28,
                    2,
                    304,
                    0
                ],
                "title": "A little less conversation, a little more action, please: Investigating\n  the physical common-sense of LLMs in a 3D embodied environment",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A little less conversation, a little more action, please: Investigating\n  the physical common-sense of LLMs in a 3D embodied environment"
                },
                "summary": "As general-purpose tools, Large Language Models (LLMs) must often reason\nabout everyday physical environments. In a question-and-answer capacity,\nunderstanding the interactions of physical objects may be necessary to give\nappropriate responses. Moreover, LLMs are increasingly used as reasoning\nengines in agentic systems, designing and controlling their action sequences.\nThe vast majority of research has tackled this issue using static benchmarks,\ncomprised of text or image-based questions about the physical world. However,\nthese benchmarks do not capture the complexity and nuance of real-life physical\nprocesses. Here we advocate for a second, relatively unexplored, approach:\n'embodying' the LLMs by granting them control of an agent within a 3D\nenvironment. We present the first embodied and cognitively meaningful\nevaluation of physical common-sense reasoning in LLMs. Our framework allows\ndirect comparison of LLMs with other embodied agents, such as those based on\nDeep Reinforcement Learning, and human and non-human animals. We employ the\nAnimal-AI (AAI) environment, a simulated 3D virtual laboratory, to study\nphysical common-sense reasoning in LLMs. For this, we use the AAI Testbed, a\nsuite of experiments that replicate laboratory studies with non-human animals,\nto study physical reasoning capabilities including distance estimation,\ntracking out-of-sight objects, and tool use. We demonstrate that\nstate-of-the-art multi-modal models with no finetuning can complete this style\nof task, allowing meaningful comparison to the entrants of the 2019 Animal-AI\nOlympics competition and to human children. Our results show that LLMs are\ncurrently outperformed by human children on these tasks. We argue that this\napproach allows the study of physical reasoning using ecologically valid\nexperiments drawn directly from cognitive science, improving the predictability\nand reliability of LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As general-purpose tools, Large Language Models (LLMs) must often reason\nabout everyday physical environments. In a question-and-answer capacity,\nunderstanding the interactions of physical objects may be necessary to give\nappropriate responses. Moreover, LLMs are increasingly used as reasoning\nengines in agentic systems, designing and controlling their action sequences.\nThe vast majority of research has tackled this issue using static benchmarks,\ncomprised of text or image-based questions about the physical world. However,\nthese benchmarks do not capture the complexity and nuance of real-life physical\nprocesses. Here we advocate for a second, relatively unexplored, approach:\n'embodying' the LLMs by granting them control of an agent within a 3D\nenvironment. We present the first embodied and cognitively meaningful\nevaluation of physical common-sense reasoning in LLMs. Our framework allows\ndirect comparison of LLMs with other embodied agents, such as those based on\nDeep Reinforcement Learning, and human and non-human animals. We employ the\nAnimal-AI (AAI) environment, a simulated 3D virtual laboratory, to study\nphysical common-sense reasoning in LLMs. For this, we use the AAI Testbed, a\nsuite of experiments that replicate laboratory studies with non-human animals,\nto study physical reasoning capabilities including distance estimation,\ntracking out-of-sight objects, and tool use. We demonstrate that\nstate-of-the-art multi-modal models with no finetuning can complete this style\nof task, allowing meaningful comparison to the entrants of the 2019 Animal-AI\nOlympics competition and to human children. Our results show that LLMs are\ncurrently outperformed by human children on these tasks. We argue that this\napproach allows the study of physical reasoning using ecologically valid\nexperiments drawn directly from cognitive science, improving the predictability\nand reliability of LLMs."
                },
                "authors": [
                    {
                        "name": "Matteo G. Mecattaf"
                    },
                    {
                        "name": "Ben Slater"
                    },
                    {
                        "name": "Marko Tešić"
                    },
                    {
                        "name": "Jonathan Prunty"
                    },
                    {
                        "name": "Konstantinos Voudouris"
                    },
                    {
                        "name": "Lucy G. Cheke"
                    }
                ],
                "author_detail": {
                    "name": "Lucy G. Cheke"
                },
                "author": "Lucy G. Cheke",
                "arxiv_comment": "25 pages, 4 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.23242v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.23242v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.23234v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.23234v1",
                "updated": "2024-10-30T17:22:45Z",
                "updated_parsed": [
                    2024,
                    10,
                    30,
                    17,
                    22,
                    45,
                    2,
                    304,
                    0
                ],
                "published": "2024-10-30T17:22:45Z",
                "published_parsed": [
                    2024,
                    10,
                    30,
                    17,
                    22,
                    45,
                    2,
                    304,
                    0
                ],
                "title": "EMOTION: Expressive Motion Sequence Generation for Humanoid Robots with\n  In-Context Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "EMOTION: Expressive Motion Sequence Generation for Humanoid Robots with\n  In-Context Learning"
                },
                "summary": "This paper introduces a framework, called EMOTION, for generating expressive\nmotion sequences in humanoid robots, enhancing their ability to engage in\nhumanlike non-verbal communication. Non-verbal cues such as facial expressions,\ngestures, and body movements play a crucial role in effective interpersonal\ninteractions. Despite the advancements in robotic behaviors, existing methods\noften fall short in mimicking the diversity and subtlety of human non-verbal\ncommunication. To address this gap, our approach leverages the in-context\nlearning capability of large language models (LLMs) to dynamically generate\nsocially appropriate gesture motion sequences for human-robot interaction. We\nuse this framework to generate 10 different expressive gestures and conduct\nonline user studies comparing the naturalness and understandability of the\nmotions generated by EMOTION and its human-feedback version, EMOTION++, against\nthose by human operators. The results demonstrate that our approach either\nmatches or surpasses human performance in generating understandable and natural\nrobot motions under certain scenarios. We also provide design implications for\nfuture research to consider a set of variables when generating expressive\nrobotic gestures.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper introduces a framework, called EMOTION, for generating expressive\nmotion sequences in humanoid robots, enhancing their ability to engage in\nhumanlike non-verbal communication. Non-verbal cues such as facial expressions,\ngestures, and body movements play a crucial role in effective interpersonal\ninteractions. Despite the advancements in robotic behaviors, existing methods\noften fall short in mimicking the diversity and subtlety of human non-verbal\ncommunication. To address this gap, our approach leverages the in-context\nlearning capability of large language models (LLMs) to dynamically generate\nsocially appropriate gesture motion sequences for human-robot interaction. We\nuse this framework to generate 10 different expressive gestures and conduct\nonline user studies comparing the naturalness and understandability of the\nmotions generated by EMOTION and its human-feedback version, EMOTION++, against\nthose by human operators. The results demonstrate that our approach either\nmatches or surpasses human performance in generating understandable and natural\nrobot motions under certain scenarios. We also provide design implications for\nfuture research to consider a set of variables when generating expressive\nrobotic gestures."
                },
                "authors": [
                    {
                        "name": "Peide Huang"
                    },
                    {
                        "name": "Yuhan Hu"
                    },
                    {
                        "name": "Nataliya Nechyporenko"
                    },
                    {
                        "name": "Daehwa Kim"
                    },
                    {
                        "name": "Walter Talbott"
                    },
                    {
                        "name": "Jian Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Jian Zhang"
                },
                "author": "Jian Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.23234v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.23234v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2306.06755v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2306.06755v4",
                "updated": "2024-10-30T17:22:41Z",
                "updated_parsed": [
                    2024,
                    10,
                    30,
                    17,
                    22,
                    41,
                    2,
                    304,
                    0
                ],
                "published": "2023-06-11T19:47:52Z",
                "published_parsed": [
                    2023,
                    6,
                    11,
                    19,
                    47,
                    52,
                    6,
                    162,
                    0
                ],
                "title": "CoTran: An LLM-based Code Translator using Reinforcement Learning with\n  Feedback from Compiler and Symbolic Execution",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CoTran: An LLM-based Code Translator using Reinforcement Learning with\n  Feedback from Compiler and Symbolic Execution"
                },
                "summary": "In this paper, we present an LLM-based code translation method and an\nassociated tool called CoTran, that translates whole-programs from one\nhigh-level programming language to another. Existing LLM-based code translation\nmethods lack training to ensure that the translated code reliably compiles or\nbears substantial functional equivalence to the input code. In our work, we\nfine-tune an LLM using reinforcement learning, incorporating compiler feedback,\nand symbolic execution (symexec)-based testing feedback to assess functional\nequivalence between the input and output programs. The idea is to guide an LLM\nduring fine-tuning, via compiler and symexec-based testing feedback, by letting\nit know how far it is from producing perfect translations. We conduct extensive\nexperiments comparing CoTran with 14 other code translation tools, including\nhuman-written transpilers, LLM-based translation tools, and ChatGPT. Using a\nbenchmark of over \\num{57000} code pairs in Java and Python, we demonstrate\nthat CoTran outperforms the other tools on relevant metrics such as compilation\naccuracy (CompAcc) and functional equivalence accuracy (FEqAcc). For example,\nin Python-to-Java translation, CoTran achieves 48.68% FEqAcc and 76.98%\nCompAcc, whereas the nearest competing tool (PLBART-base) gets 38.26% and\n75.77% respectively. Additionally, CoTran, built on top of CodeT5, improves\nFEqAcc by +14.89% and CompAcc by +8.14% for Python-to-Java (resp., +12.94% and\n+4.30% for Java-to-Python).",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this paper, we present an LLM-based code translation method and an\nassociated tool called CoTran, that translates whole-programs from one\nhigh-level programming language to another. Existing LLM-based code translation\nmethods lack training to ensure that the translated code reliably compiles or\nbears substantial functional equivalence to the input code. In our work, we\nfine-tune an LLM using reinforcement learning, incorporating compiler feedback,\nand symbolic execution (symexec)-based testing feedback to assess functional\nequivalence between the input and output programs. The idea is to guide an LLM\nduring fine-tuning, via compiler and symexec-based testing feedback, by letting\nit know how far it is from producing perfect translations. We conduct extensive\nexperiments comparing CoTran with 14 other code translation tools, including\nhuman-written transpilers, LLM-based translation tools, and ChatGPT. Using a\nbenchmark of over \\num{57000} code pairs in Java and Python, we demonstrate\nthat CoTran outperforms the other tools on relevant metrics such as compilation\naccuracy (CompAcc) and functional equivalence accuracy (FEqAcc). For example,\nin Python-to-Java translation, CoTran achieves 48.68% FEqAcc and 76.98%\nCompAcc, whereas the nearest competing tool (PLBART-base) gets 38.26% and\n75.77% respectively. Additionally, CoTran, built on top of CodeT5, improves\nFEqAcc by +14.89% and CompAcc by +8.14% for Python-to-Java (resp., +12.94% and\n+4.30% for Java-to-Python)."
                },
                "authors": [
                    {
                        "name": "Prithwish Jana"
                    },
                    {
                        "name": "Piyush Jha"
                    },
                    {
                        "name": "Haoyang Ju"
                    },
                    {
                        "name": "Gautham Kishore"
                    },
                    {
                        "name": "Aryan Mahajan"
                    },
                    {
                        "name": "Vijay Ganesh"
                    }
                ],
                "author_detail": {
                    "name": "Vijay Ganesh"
                },
                "author": "Vijay Ganesh",
                "arxiv_doi": "10.3233/FAIA240968",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.3233/FAIA240968",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2306.06755v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2306.06755v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "The paper has been published at the 27th European Conference on\n  Artificial Intelligence (ECAI-2024) and is available at\n  https://ebooks.iospress.nl/doi/10.3233/FAIA240968. This arXiv version is the\n  full version that includes the supplementary material (Appendix)",
                "arxiv_primary_category": {
                    "term": "cs.PL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.PL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.7; I.2.5; D.2",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.23230v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.23230v2",
                "updated": "2024-10-31T04:20:22Z",
                "updated_parsed": [
                    2024,
                    10,
                    31,
                    4,
                    20,
                    22,
                    3,
                    305,
                    0
                ],
                "published": "2024-10-30T17:18:53Z",
                "published_parsed": [
                    2024,
                    10,
                    30,
                    17,
                    18,
                    53,
                    2,
                    304,
                    0
                ],
                "title": "Aligning Audio-Visual Joint Representations with an Agentic Workflow",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Aligning Audio-Visual Joint Representations with an Agentic Workflow"
                },
                "summary": "Visual content and accompanied audio signals naturally formulate a joint\nrepresentation to improve audio-visual (AV) related applications. While studies\ndevelop various AV representation learning frameworks, the importance of AV\ndata alignment is usually undermined for achieving high-quality representation.\nWe observe that an audio signal may contain background noise interference.\nAlso, non-synchronization may appear between audio and video streams. These\nnon-strict data alignment limits representation quality and downgrade\napplication performance. In this paper, we propose to improve AV joint\nrepresentations from a data-centric perspective by aligning audio signals to\nvisual data. Our alignment is conducted in an agentic workflow controlled by an\nLLM-based assistant named AVAgent. For each input AV data pair, our AVAgent\nuses a multi-modal LLM to convert audio and visual data into language\ndescriptions separately (i.e., tool use). Then, AVAgent reasons whether this\npaired data is aligned well and plans to edit the audio signal if needed (i.e.,\nplanning). The audio editing is executed by predefined actions that filter\nnoise or augment data. Moreover, we use a VLM to evaluate how modified audio\nsignals match the visual content and provide feedback to AVAgent (i.e.,\nreflection). The tool use, planning, and reflection steps operate cyclically to\nbecome an agentic workflow where audio signals are gradually aligned to visual\ncontent. To this end, existing methods can directly leverage the aligned AV\ndata via our agentic workflow to improve AV joint representations. The\nexperimental results comprehensively demonstrate the state-of-the-art\nperformance of the proposed approach against previous baselines in diverse\ndownstream tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Visual content and accompanied audio signals naturally formulate a joint\nrepresentation to improve audio-visual (AV) related applications. While studies\ndevelop various AV representation learning frameworks, the importance of AV\ndata alignment is usually undermined for achieving high-quality representation.\nWe observe that an audio signal may contain background noise interference.\nAlso, non-synchronization may appear between audio and video streams. These\nnon-strict data alignment limits representation quality and downgrade\napplication performance. In this paper, we propose to improve AV joint\nrepresentations from a data-centric perspective by aligning audio signals to\nvisual data. Our alignment is conducted in an agentic workflow controlled by an\nLLM-based assistant named AVAgent. For each input AV data pair, our AVAgent\nuses a multi-modal LLM to convert audio and visual data into language\ndescriptions separately (i.e., tool use). Then, AVAgent reasons whether this\npaired data is aligned well and plans to edit the audio signal if needed (i.e.,\nplanning). The audio editing is executed by predefined actions that filter\nnoise or augment data. Moreover, we use a VLM to evaluate how modified audio\nsignals match the visual content and provide feedback to AVAgent (i.e.,\nreflection). The tool use, planning, and reflection steps operate cyclically to\nbecome an agentic workflow where audio signals are gradually aligned to visual\ncontent. To this end, existing methods can directly leverage the aligned AV\ndata via our agentic workflow to improve AV joint representations. The\nexperimental results comprehensively demonstrate the state-of-the-art\nperformance of the proposed approach against previous baselines in diverse\ndownstream tasks."
                },
                "authors": [
                    {
                        "name": "Shentong Mo"
                    },
                    {
                        "name": "Yibing Song"
                    }
                ],
                "author_detail": {
                    "name": "Yibing Song"
                },
                "author": "Yibing Song",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.23230v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.23230v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.02551v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.02551v2",
                "updated": "2024-10-30T17:16:44Z",
                "updated_parsed": [
                    2024,
                    10,
                    30,
                    17,
                    16,
                    44,
                    2,
                    304,
                    0
                ],
                "published": "2024-07-02T16:19:25Z",
                "published_parsed": [
                    2024,
                    7,
                    2,
                    16,
                    19,
                    25,
                    1,
                    184,
                    0
                ],
                "title": "Breach By A Thousand Leaks: Unsafe Information Leakage in `Safe' AI\n  Responses",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Breach By A Thousand Leaks: Unsafe Information Leakage in `Safe' AI\n  Responses"
                },
                "summary": "Vulnerability of Frontier language models to misuse and jailbreaks has\nprompted the development of safety measures like filters and alignment training\nin an effort to ensure safety through robustness to adversarially crafted\nprompts. We assert that robustness is fundamentally insufficient for ensuring\nsafety goals, and current defenses and evaluation methods fail to account for\nrisks of dual-intent queries and their composition for malicious goals. To\nquantify these risks, we introduce a new safety evaluation framework based on\nimpermissible information leakage of model outputs and demonstrate how our\nproposed question-decomposition attack can extract dangerous knowledge from a\ncensored LLM more effectively than traditional jailbreaking. Underlying our\nproposed evaluation method is a novel information-theoretic threat model of\ninferential adversaries, distinguished from security adversaries, such as\njailbreaks, in that success is measured by inferring impermissible knowledge\nfrom victim outputs as opposed to forcing explicitly impermissible outputs from\nthe victim. Through our information-theoretic framework, we show that to ensure\nsafety against inferential adversaries, defense mechanisms must ensure\ninformation censorship, bounding the leakage of impermissible information.\nHowever, we prove that such defenses inevitably incur a safety-utility\ntrade-off.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Vulnerability of Frontier language models to misuse and jailbreaks has\nprompted the development of safety measures like filters and alignment training\nin an effort to ensure safety through robustness to adversarially crafted\nprompts. We assert that robustness is fundamentally insufficient for ensuring\nsafety goals, and current defenses and evaluation methods fail to account for\nrisks of dual-intent queries and their composition for malicious goals. To\nquantify these risks, we introduce a new safety evaluation framework based on\nimpermissible information leakage of model outputs and demonstrate how our\nproposed question-decomposition attack can extract dangerous knowledge from a\ncensored LLM more effectively than traditional jailbreaking. Underlying our\nproposed evaluation method is a novel information-theoretic threat model of\ninferential adversaries, distinguished from security adversaries, such as\njailbreaks, in that success is measured by inferring impermissible knowledge\nfrom victim outputs as opposed to forcing explicitly impermissible outputs from\nthe victim. Through our information-theoretic framework, we show that to ensure\nsafety against inferential adversaries, defense mechanisms must ensure\ninformation censorship, bounding the leakage of impermissible information.\nHowever, we prove that such defenses inevitably incur a safety-utility\ntrade-off."
                },
                "authors": [
                    {
                        "name": "David Glukhov"
                    },
                    {
                        "name": "Ziwen Han"
                    },
                    {
                        "name": "Ilia Shumailov"
                    },
                    {
                        "name": "Vardan Papyan"
                    },
                    {
                        "name": "Nicolas Papernot"
                    }
                ],
                "author_detail": {
                    "name": "Nicolas Papernot"
                },
                "author": "Nicolas Papernot",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.02551v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.02551v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.23223v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.23223v1",
                "updated": "2024-10-30T17:13:02Z",
                "updated_parsed": [
                    2024,
                    10,
                    30,
                    17,
                    13,
                    2,
                    2,
                    304,
                    0
                ],
                "published": "2024-10-30T17:13:02Z",
                "published_parsed": [
                    2024,
                    10,
                    30,
                    17,
                    13,
                    2,
                    2,
                    304,
                    0
                ],
                "title": "COMAL: A Convergent Meta-Algorithm for Aligning LLMs with General\n  Preferences",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "COMAL: A Convergent Meta-Algorithm for Aligning LLMs with General\n  Preferences"
                },
                "summary": "Many alignment methods, including reinforcement learning from human feedback\n(RLHF), rely on the Bradley-Terry reward assumption, which is insufficient to\ncapture the full range of general human preferences. To achieve robust\nalignment with general preferences, we model the alignment problem as a\ntwo-player zero-sum game, where the Nash equilibrium policy guarantees a 50%\nwin rate against any competing policy. However, previous algorithms for finding\nthe Nash policy either diverge or converge to a Nash policy in a modified game,\neven in a simple synthetic setting, thereby failing to maintain the 50% win\nrate guarantee against all other policies. We propose a meta-algorithm,\nConvergent Meta Alignment Algorithm (COMAL), for language model alignment with\ngeneral preferences, inspired by convergent algorithms in game theory.\nTheoretically, we prove that our meta-algorithm converges to an exact Nash\npolicy in the last iterate. Additionally, our meta-algorithm is simple and can\nbe integrated with many existing methods designed for RLHF and preference\noptimization with minimal changes. Experimental results demonstrate the\neffectiveness of the proposed framework when combined with existing preference\npolicy optimization methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Many alignment methods, including reinforcement learning from human feedback\n(RLHF), rely on the Bradley-Terry reward assumption, which is insufficient to\ncapture the full range of general human preferences. To achieve robust\nalignment with general preferences, we model the alignment problem as a\ntwo-player zero-sum game, where the Nash equilibrium policy guarantees a 50%\nwin rate against any competing policy. However, previous algorithms for finding\nthe Nash policy either diverge or converge to a Nash policy in a modified game,\neven in a simple synthetic setting, thereby failing to maintain the 50% win\nrate guarantee against all other policies. We propose a meta-algorithm,\nConvergent Meta Alignment Algorithm (COMAL), for language model alignment with\ngeneral preferences, inspired by convergent algorithms in game theory.\nTheoretically, we prove that our meta-algorithm converges to an exact Nash\npolicy in the last iterate. Additionally, our meta-algorithm is simple and can\nbe integrated with many existing methods designed for RLHF and preference\noptimization with minimal changes. Experimental results demonstrate the\neffectiveness of the proposed framework when combined with existing preference\npolicy optimization methods."
                },
                "authors": [
                    {
                        "name": "Yixin Liu"
                    },
                    {
                        "name": "Argyris Oikonomou"
                    },
                    {
                        "name": "Weiqiang Zheng"
                    },
                    {
                        "name": "Yang Cai"
                    },
                    {
                        "name": "Arman Cohan"
                    }
                ],
                "author_detail": {
                    "name": "Arman Cohan"
                },
                "author": "Arman Cohan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.23223v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.23223v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.GT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.06007v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.06007v2",
                "updated": "2024-10-30T17:08:16Z",
                "updated_parsed": [
                    2024,
                    10,
                    30,
                    17,
                    8,
                    16,
                    2,
                    304,
                    0
                ],
                "published": "2024-06-10T04:07:09Z",
                "published_parsed": [
                    2024,
                    6,
                    10,
                    4,
                    7,
                    9,
                    0,
                    162,
                    0
                ],
                "title": "CARES: A Comprehensive Benchmark of Trustworthiness in Medical Vision\n  Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CARES: A Comprehensive Benchmark of Trustworthiness in Medical Vision\n  Language Models"
                },
                "summary": "Artificial intelligence has significantly impacted medical applications,\nparticularly with the advent of Medical Large Vision Language Models\n(Med-LVLMs), sparking optimism for the future of automated and personalized\nhealthcare. However, the trustworthiness of Med-LVLMs remains unverified,\nposing significant risks for future model deployment. In this paper, we\nintroduce CARES and aim to comprehensively evaluate the Trustworthiness of\nMed-LVLMs across the medical domain. We assess the trustworthiness of Med-LVLMs\nacross five dimensions, including trustfulness, fairness, safety, privacy, and\nrobustness. CARES comprises about 41K question-answer pairs in both closed and\nopen-ended formats, covering 16 medical image modalities and 27 anatomical\nregions. Our analysis reveals that the models consistently exhibit concerns\nregarding trustworthiness, often displaying factual inaccuracies and failing to\nmaintain fairness across different demographic groups. Furthermore, they are\nvulnerable to attacks and demonstrate a lack of privacy awareness. We publicly\nrelease our benchmark and code in https://cares-ai.github.io/.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Artificial intelligence has significantly impacted medical applications,\nparticularly with the advent of Medical Large Vision Language Models\n(Med-LVLMs), sparking optimism for the future of automated and personalized\nhealthcare. However, the trustworthiness of Med-LVLMs remains unverified,\nposing significant risks for future model deployment. In this paper, we\nintroduce CARES and aim to comprehensively evaluate the Trustworthiness of\nMed-LVLMs across the medical domain. We assess the trustworthiness of Med-LVLMs\nacross five dimensions, including trustfulness, fairness, safety, privacy, and\nrobustness. CARES comprises about 41K question-answer pairs in both closed and\nopen-ended formats, covering 16 medical image modalities and 27 anatomical\nregions. Our analysis reveals that the models consistently exhibit concerns\nregarding trustworthiness, often displaying factual inaccuracies and failing to\nmaintain fairness across different demographic groups. Furthermore, they are\nvulnerable to attacks and demonstrate a lack of privacy awareness. We publicly\nrelease our benchmark and code in https://cares-ai.github.io/."
                },
                "authors": [
                    {
                        "name": "Peng Xia"
                    },
                    {
                        "name": "Ze Chen"
                    },
                    {
                        "name": "Juanxi Tian"
                    },
                    {
                        "name": "Yangrui Gong"
                    },
                    {
                        "name": "Ruibo Hou"
                    },
                    {
                        "name": "Yue Xu"
                    },
                    {
                        "name": "Zhenbang Wu"
                    },
                    {
                        "name": "Zhiyuan Fan"
                    },
                    {
                        "name": "Yiyang Zhou"
                    },
                    {
                        "name": "Kangyu Zhu"
                    },
                    {
                        "name": "Wenhao Zheng"
                    },
                    {
                        "name": "Zhaoyang Wang"
                    },
                    {
                        "name": "Xiao Wang"
                    },
                    {
                        "name": "Xuchao Zhang"
                    },
                    {
                        "name": "Chetan Bansal"
                    },
                    {
                        "name": "Marc Niethammer"
                    },
                    {
                        "name": "Junzhou Huang"
                    },
                    {
                        "name": "Hongtu Zhu"
                    },
                    {
                        "name": "Yun Li"
                    },
                    {
                        "name": "Jimeng Sun"
                    },
                    {
                        "name": "Zongyuan Ge"
                    },
                    {
                        "name": "Gang Li"
                    },
                    {
                        "name": "James Zou"
                    },
                    {
                        "name": "Huaxiu Yao"
                    }
                ],
                "author_detail": {
                    "name": "Huaxiu Yao"
                },
                "author": "Huaxiu Yao",
                "arxiv_comment": "NeurIPS 2024 Datasets and Benchmarks Track",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.06007v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.06007v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.23214v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.23214v2",
                "updated": "2024-10-31T01:34:16Z",
                "updated_parsed": [
                    2024,
                    10,
                    31,
                    1,
                    34,
                    16,
                    3,
                    305,
                    0
                ],
                "published": "2024-10-30T17:02:54Z",
                "published_parsed": [
                    2024,
                    10,
                    30,
                    17,
                    2,
                    54,
                    2,
                    304,
                    0
                ],
                "title": "Grounding by Trying: LLMs with Reinforcement Learning-Enhanced Retrieval",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Grounding by Trying: LLMs with Reinforcement Learning-Enhanced Retrieval"
                },
                "summary": "The hallucinations of large language models (LLMs) are increasingly mitigated\nby allowing LLMs to search for information and to ground their answers in real\nsources. Unfortunately, LLMs often struggle with posing the right search\nqueries, especially when dealing with complex or otherwise indirect topics.\nObserving that LLMs can learn to search for relevant facts by $\\textit{trying}$\ndifferent queries and learning to up-weight queries that successfully produce\nrelevant results, we introduce $\\underline{Le}$arning to $\\underline{Re}$trieve\nby $\\underline{T}$rying (LeReT), a reinforcement learning framework that\nexplores search queries and uses preference-based optimization to improve their\nquality. LeReT can improve the absolute retrieval accuracy by up to 29% and the\ndownstream generator evaluations by 17%. The simplicity and flexibility of\nLeReT allows it to be applied to arbitrary off-the-shelf retrievers and makes\nit a promising technique for improving general LLM pipelines. Project website:\nhttp://sherylhsu.com/LeReT/.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The hallucinations of large language models (LLMs) are increasingly mitigated\nby allowing LLMs to search for information and to ground their answers in real\nsources. Unfortunately, LLMs often struggle with posing the right search\nqueries, especially when dealing with complex or otherwise indirect topics.\nObserving that LLMs can learn to search for relevant facts by $\\textit{trying}$\ndifferent queries and learning to up-weight queries that successfully produce\nrelevant results, we introduce $\\underline{Le}$arning to $\\underline{Re}$trieve\nby $\\underline{T}$rying (LeReT), a reinforcement learning framework that\nexplores search queries and uses preference-based optimization to improve their\nquality. LeReT can improve the absolute retrieval accuracy by up to 29% and the\ndownstream generator evaluations by 17%. The simplicity and flexibility of\nLeReT allows it to be applied to arbitrary off-the-shelf retrievers and makes\nit a promising technique for improving general LLM pipelines. Project website:\nhttp://sherylhsu.com/LeReT/."
                },
                "authors": [
                    {
                        "name": "Sheryl Hsu"
                    },
                    {
                        "name": "Omar Khattab"
                    },
                    {
                        "name": "Chelsea Finn"
                    },
                    {
                        "name": "Archit Sharma"
                    }
                ],
                "author_detail": {
                    "name": "Archit Sharma"
                },
                "author": "Archit Sharma",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.23214v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.23214v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.23213v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.23213v1",
                "updated": "2024-10-30T17:01:28Z",
                "updated_parsed": [
                    2024,
                    10,
                    30,
                    17,
                    1,
                    28,
                    2,
                    304,
                    0
                ],
                "published": "2024-10-30T17:01:28Z",
                "published_parsed": [
                    2024,
                    10,
                    30,
                    17,
                    1,
                    28,
                    2,
                    304,
                    0
                ],
                "title": "ELMGS: Enhancing memory and computation scaLability through coMpression\n  for 3D Gaussian Splatting",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ELMGS: Enhancing memory and computation scaLability through coMpression\n  for 3D Gaussian Splatting"
                },
                "summary": "3D models have recently been popularized by the potentiality of end-to-end\ntraining offered first by Neural Radiance Fields and most recently by 3D\nGaussian Splatting models. The latter has the big advantage of naturally\nproviding fast training convergence and high editability. However, as the\nresearch around these is still in its infancy, there is still a gap in the\nliterature regarding the model's scalability. In this work, we propose an\napproach enabling both memory and computation scalability of such models. More\nspecifically, we propose an iterative pruning strategy that removes redundant\ninformation encoded in the model. We also enhance compressibility for the model\nby including in the optimization strategy a differentiable quantization and\nentropy coding estimator. Our results on popular benchmarks showcase the\neffectiveness of the proposed approach and open the road to the broad\ndeployability of such a solution even on resource-constrained devices.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "3D models have recently been popularized by the potentiality of end-to-end\ntraining offered first by Neural Radiance Fields and most recently by 3D\nGaussian Splatting models. The latter has the big advantage of naturally\nproviding fast training convergence and high editability. However, as the\nresearch around these is still in its infancy, there is still a gap in the\nliterature regarding the model's scalability. In this work, we propose an\napproach enabling both memory and computation scalability of such models. More\nspecifically, we propose an iterative pruning strategy that removes redundant\ninformation encoded in the model. We also enhance compressibility for the model\nby including in the optimization strategy a differentiable quantization and\nentropy coding estimator. Our results on popular benchmarks showcase the\neffectiveness of the proposed approach and open the road to the broad\ndeployability of such a solution even on resource-constrained devices."
                },
                "authors": [
                    {
                        "name": "Muhammad Salman Ali"
                    },
                    {
                        "name": "Sung-Ho Bae"
                    },
                    {
                        "name": "Enzo Tartaglione"
                    }
                ],
                "author_detail": {
                    "name": "Enzo Tartaglione"
                },
                "author": "Enzo Tartaglione",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.23213v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.23213v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.05148v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.05148v3",
                "updated": "2024-10-30T16:52:42Z",
                "updated_parsed": [
                    2024,
                    10,
                    30,
                    16,
                    52,
                    42,
                    2,
                    304,
                    0
                ],
                "published": "2024-08-09T16:07:37Z",
                "published_parsed": [
                    2024,
                    8,
                    9,
                    16,
                    7,
                    37,
                    4,
                    222,
                    0
                ],
                "title": "Impacts of floating-point non-associativity on reproducibility for HPC\n  and deep learning applications",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Impacts of floating-point non-associativity on reproducibility for HPC\n  and deep learning applications"
                },
                "summary": "Run to run variability in parallel programs caused by floating-point\nnon-associativity has been known to significantly affect reproducibility in\niterative algorithms, due to accumulating errors. Non-reproducibility can\ncritically affect the efficiency and effectiveness of correctness testing for\nstochastic programs. Recently, the sensitivity of deep learning training and\ninference pipelines to floating-point non-associativity has been found to\nsometimes be extreme. It can prevent certification for commercial applications,\naccurate assessment of robustness and sensitivity, and bug detection. New\napproaches in scientific computing applications have coupled deep learning\nmodels with high-performance computing, leading to an aggravation of debugging\nand testing challenges. Here we perform an investigation of the statistical\nproperties of floating-point non-associativity within modern parallel\nprogramming models, and analyze performance and productivity impacts of\nreplacing atomic operations with deterministic alternatives on GPUs. We examine\nthe recently-added deterministic options in PyTorch within the context of GPU\ndeployment for deep learning, uncovering and quantifying the impacts of input\nparameters triggering run to run variability and reporting on the reliability\nand completeness of the documentation. Finally, we evaluate the strategy of\nexploiting automatic determinism that could be provided by deterministic\nhardware, using the Groq accelerator for inference portions of the deep\nlearning pipeline. We demonstrate the benefits that a hardware-based strategy\ncan provide within reproducibility and correctness efforts.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Run to run variability in parallel programs caused by floating-point\nnon-associativity has been known to significantly affect reproducibility in\niterative algorithms, due to accumulating errors. Non-reproducibility can\ncritically affect the efficiency and effectiveness of correctness testing for\nstochastic programs. Recently, the sensitivity of deep learning training and\ninference pipelines to floating-point non-associativity has been found to\nsometimes be extreme. It can prevent certification for commercial applications,\naccurate assessment of robustness and sensitivity, and bug detection. New\napproaches in scientific computing applications have coupled deep learning\nmodels with high-performance computing, leading to an aggravation of debugging\nand testing challenges. Here we perform an investigation of the statistical\nproperties of floating-point non-associativity within modern parallel\nprogramming models, and analyze performance and productivity impacts of\nreplacing atomic operations with deterministic alternatives on GPUs. We examine\nthe recently-added deterministic options in PyTorch within the context of GPU\ndeployment for deep learning, uncovering and quantifying the impacts of input\nparameters triggering run to run variability and reporting on the reliability\nand completeness of the documentation. Finally, we evaluate the strategy of\nexploiting automatic determinism that could be provided by deterministic\nhardware, using the Groq accelerator for inference portions of the deep\nlearning pipeline. We demonstrate the benefits that a hardware-based strategy\ncan provide within reproducibility and correctness efforts."
                },
                "authors": [
                    {
                        "name": "Sanjif Shanmugavelu"
                    },
                    {
                        "name": "Mathieu Taillefumier"
                    },
                    {
                        "name": "Christopher Culver"
                    },
                    {
                        "name": "Oscar Hernandez"
                    },
                    {
                        "name": "Mark Coletti"
                    },
                    {
                        "name": "Ada Sedova"
                    }
                ],
                "author_detail": {
                    "name": "Ada Sedova"
                },
                "author": "Ada Sedova",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.05148v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.05148v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.03294v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.03294v3",
                "updated": "2024-10-30T16:51:39Z",
                "updated_parsed": [
                    2024,
                    10,
                    30,
                    16,
                    51,
                    39,
                    2,
                    304,
                    0
                ],
                "published": "2024-10-04T10:12:24Z",
                "published_parsed": [
                    2024,
                    10,
                    4,
                    10,
                    12,
                    24,
                    4,
                    278,
                    0
                ],
                "title": "Resource-aware Mixed-precision Quantization for Enhancing Deployability\n  of Transformers for Time-series Forecasting on Embedded FPGAs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Resource-aware Mixed-precision Quantization for Enhancing Deployability\n  of Transformers for Time-series Forecasting on Embedded FPGAs"
                },
                "summary": "This study addresses the deployment challenges of integer-only quantized\nTransformers on resource-constrained embedded FPGAs (Xilinx Spartan-7 XC7S15).\nWe enhanced the flexibility of our VHDL template by introducing a selectable\nresource type for storing intermediate results across model layers, thereby\nbreaking the deployment bottleneck by utilizing BRAM efficiently. Moreover, we\ndeveloped a resource-aware mixed-precision quantization approach that enables\nresearchers to explore hardware-level quantization strategies without requiring\nextensive expertise in Neural Architecture Search. This method provides\naccurate resource utilization estimates with a precision discrepancy as low as\n3%, compared to actual deployment metrics. Compared to previous work, our\napproach has successfully facilitated the deployment of model configurations\nutilizing mixed-precision quantization, thus overcoming the limitations\ninherent in five previously non-deployable configurations with uniform\nquantization bitwidths. Consequently, this research enhances the applicability\nof Transformers in embedded systems, facilitating a broader range of\nTransformer-powered applications on edge devices.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This study addresses the deployment challenges of integer-only quantized\nTransformers on resource-constrained embedded FPGAs (Xilinx Spartan-7 XC7S15).\nWe enhanced the flexibility of our VHDL template by introducing a selectable\nresource type for storing intermediate results across model layers, thereby\nbreaking the deployment bottleneck by utilizing BRAM efficiently. Moreover, we\ndeveloped a resource-aware mixed-precision quantization approach that enables\nresearchers to explore hardware-level quantization strategies without requiring\nextensive expertise in Neural Architecture Search. This method provides\naccurate resource utilization estimates with a precision discrepancy as low as\n3%, compared to actual deployment metrics. Compared to previous work, our\napproach has successfully facilitated the deployment of model configurations\nutilizing mixed-precision quantization, thus overcoming the limitations\ninherent in five previously non-deployable configurations with uniform\nquantization bitwidths. Consequently, this research enhances the applicability\nof Transformers in embedded systems, facilitating a broader range of\nTransformer-powered applications on edge devices."
                },
                "authors": [
                    {
                        "name": "Tianheng Ling"
                    },
                    {
                        "name": "Chao Qian"
                    },
                    {
                        "name": "Gregor Schiele"
                    }
                ],
                "author_detail": {
                    "name": "Gregor Schiele"
                },
                "author": "Gregor Schiele",
                "arxiv_comment": "Accepted by the 21st EAI International Conference on Mobile and\n  Ubiquitous Systems: Computing, Networking and Services (MobiQuitous2024). 20\n  pages, 8 figures, 6 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.03294v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.03294v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.23197v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.23197v1",
                "updated": "2024-10-30T16:49:40Z",
                "updated_parsed": [
                    2024,
                    10,
                    30,
                    16,
                    49,
                    40,
                    2,
                    304,
                    0
                ],
                "published": "2024-10-30T16:49:40Z",
                "published_parsed": [
                    2024,
                    10,
                    30,
                    16,
                    49,
                    40,
                    2,
                    304,
                    0
                ],
                "title": "Magnetic diagnostics of prominence eruptions through the Hanle effect of\n  the He I 1083 nm line",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Magnetic diagnostics of prominence eruptions through the Hanle effect of\n  the He I 1083 nm line"
                },
                "summary": "The magnetic field vector of the solar corona is not regularly and\ncomprehensively being measured, because of the complexity and degeneracy\ninherently present in the types of observations currently available. To address\nsome of the current limitations of coronal polarimetry, we present computations\nthat demonstrate the possibility of magnetometry using the unsaturated Hanle\neffect of the He I 1083 nm line. The main purpose of this investigation is to\nshow how the geometric properties of the linear polarization of this line can\nbe used to routinely diagnose the orientation of the field in erupting\nprominences, thus providing an important constraint on the B$_z$ determination\nat 1 AU. For this work, we adopted a simplified magnetic model of a flux rope,\nconsisting of a toroidal helical structure embedded in a hydrostatically\nstratified corona. Our results demonstrate the possibility to discern different\norientations of the magnetic field vector in such structures under rather\ngeneral and practicable viewing conditions. In particular, observations from\nthe Sun-Earth Lagrange points are found to provide excellent locations for the\ndeployment of synoptic instruments aiming at the estimation of the magnetic\nfield of Earth-directed Coronal Mass Ejections. We complete our demonstration\nby showing how a small (~5 cm) space-borne coronagraph can achieve sufficient\nsignal-to-noise ratios to make the coronal magnetometry goal outlined above\nfeasible.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The magnetic field vector of the solar corona is not regularly and\ncomprehensively being measured, because of the complexity and degeneracy\ninherently present in the types of observations currently available. To address\nsome of the current limitations of coronal polarimetry, we present computations\nthat demonstrate the possibility of magnetometry using the unsaturated Hanle\neffect of the He I 1083 nm line. The main purpose of this investigation is to\nshow how the geometric properties of the linear polarization of this line can\nbe used to routinely diagnose the orientation of the field in erupting\nprominences, thus providing an important constraint on the B$_z$ determination\nat 1 AU. For this work, we adopted a simplified magnetic model of a flux rope,\nconsisting of a toroidal helical structure embedded in a hydrostatically\nstratified corona. Our results demonstrate the possibility to discern different\norientations of the magnetic field vector in such structures under rather\ngeneral and practicable viewing conditions. In particular, observations from\nthe Sun-Earth Lagrange points are found to provide excellent locations for the\ndeployment of synoptic instruments aiming at the estimation of the magnetic\nfield of Earth-directed Coronal Mass Ejections. We complete our demonstration\nby showing how a small (~5 cm) space-borne coronagraph can achieve sufficient\nsignal-to-noise ratios to make the coronal magnetometry goal outlined above\nfeasible."
                },
                "authors": [
                    {
                        "name": "Momchil Molnar"
                    },
                    {
                        "name": "Roberto Casini"
                    }
                ],
                "author_detail": {
                    "name": "Roberto Casini"
                },
                "author": "Roberto Casini",
                "arxiv_comment": "Accepted for publication in ApJ",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.23197v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.23197v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.SR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.SR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.10372v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.10372v3",
                "updated": "2024-10-30T16:45:15Z",
                "updated_parsed": [
                    2024,
                    10,
                    30,
                    16,
                    45,
                    15,
                    2,
                    304,
                    0
                ],
                "published": "2024-09-16T15:15:51Z",
                "published_parsed": [
                    2024,
                    9,
                    16,
                    15,
                    15,
                    51,
                    0,
                    260,
                    0
                ],
                "title": "Instigating Cooperation among LLM Agents Using Adaptive Information\n  Modulation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Instigating Cooperation among LLM Agents Using Adaptive Information\n  Modulation"
                },
                "summary": "This paper introduces a novel framework combining LLM agents as proxies for\nhuman strategic behavior with reinforcement learning (RL) to engage these\nagents in evolving strategic interactions within team environments. Our\napproach extends traditional agent-based simulations by using strategic LLM\nagents (SLA) and introducing dynamic and adaptive governance through a\npro-social promoting RL agent (PPA) that modulates information access across\nagents in a network, optimizing social welfare and promoting pro-social\nbehavior. Through validation in iterative games, including the prisoner\ndilemma, we demonstrate that SLA agents exhibit nuanced strategic adaptations.\nThe PPA agent effectively learns to adjust information transparency, resulting\nin enhanced cooperation rates. This framework offers significant insights into\nAI-mediated social dynamics, contributing to the deployment of AI in real-world\nteam settings.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper introduces a novel framework combining LLM agents as proxies for\nhuman strategic behavior with reinforcement learning (RL) to engage these\nagents in evolving strategic interactions within team environments. Our\napproach extends traditional agent-based simulations by using strategic LLM\nagents (SLA) and introducing dynamic and adaptive governance through a\npro-social promoting RL agent (PPA) that modulates information access across\nagents in a network, optimizing social welfare and promoting pro-social\nbehavior. Through validation in iterative games, including the prisoner\ndilemma, we demonstrate that SLA agents exhibit nuanced strategic adaptations.\nThe PPA agent effectively learns to adjust information transparency, resulting\nin enhanced cooperation rates. This framework offers significant insights into\nAI-mediated social dynamics, contributing to the deployment of AI in real-world\nteam settings."
                },
                "authors": [
                    {
                        "name": "Qiliang Chen"
                    },
                    {
                        "name": "Sepehr Ilami"
                    },
                    {
                        "name": "Nunzio Lore"
                    },
                    {
                        "name": "Babak Heydari"
                    }
                ],
                "author_detail": {
                    "name": "Babak Heydari"
                },
                "author": "Babak Heydari",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.10372v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.10372v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.GT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.23182v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.23182v1",
                "updated": "2024-10-30T16:38:09Z",
                "updated_parsed": [
                    2024,
                    10,
                    30,
                    16,
                    38,
                    9,
                    2,
                    304,
                    0
                ],
                "published": "2024-10-30T16:38:09Z",
                "published_parsed": [
                    2024,
                    10,
                    30,
                    16,
                    38,
                    9,
                    2,
                    304,
                    0
                ],
                "title": "ProTransformer: Robustify Transformers via Plug-and-Play Paradigm",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ProTransformer: Robustify Transformers via Plug-and-Play Paradigm"
                },
                "summary": "Transformer-based architectures have dominated various areas of machine\nlearning in recent years. In this paper, we introduce a novel robust attention\nmechanism designed to enhance the resilience of transformer-based\narchitectures. Crucially, this technique can be integrated into existing\ntransformers as a plug-and-play layer, improving their robustness without the\nneed for additional training or fine-tuning. Through comprehensive experiments\nand ablation studies, we demonstrate that our ProTransformer significantly\nenhances the robustness of transformer models across a variety of prediction\ntasks, attack mechanisms, backbone architectures, and data domains. Notably,\nwithout further fine-tuning, the ProTransformer consistently improves the\nperformance of vanilla transformers by 19.5%, 28.3%, 16.1%, and 11.4% for BERT,\nALBERT, DistilBERT, and RoBERTa, respectively, under the classical TextFooler\nattack. Furthermore, ProTransformer shows promising resilience in large\nlanguage models (LLMs) against prompting-based attacks, improving the\nperformance of T5 and LLaMA by 24.8% and 17.8%, respectively, and enhancing\nVicuna by an average of 10.4% against the Jailbreaking attack. Beyond the\nlanguage domain, ProTransformer also demonstrates outstanding robustness in\nboth vision and graph domains.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Transformer-based architectures have dominated various areas of machine\nlearning in recent years. In this paper, we introduce a novel robust attention\nmechanism designed to enhance the resilience of transformer-based\narchitectures. Crucially, this technique can be integrated into existing\ntransformers as a plug-and-play layer, improving their robustness without the\nneed for additional training or fine-tuning. Through comprehensive experiments\nand ablation studies, we demonstrate that our ProTransformer significantly\nenhances the robustness of transformer models across a variety of prediction\ntasks, attack mechanisms, backbone architectures, and data domains. Notably,\nwithout further fine-tuning, the ProTransformer consistently improves the\nperformance of vanilla transformers by 19.5%, 28.3%, 16.1%, and 11.4% for BERT,\nALBERT, DistilBERT, and RoBERTa, respectively, under the classical TextFooler\nattack. Furthermore, ProTransformer shows promising resilience in large\nlanguage models (LLMs) against prompting-based attacks, improving the\nperformance of T5 and LLaMA by 24.8% and 17.8%, respectively, and enhancing\nVicuna by an average of 10.4% against the Jailbreaking attack. Beyond the\nlanguage domain, ProTransformer also demonstrates outstanding robustness in\nboth vision and graph domains."
                },
                "authors": [
                    {
                        "name": "Zhichao Hou"
                    },
                    {
                        "name": "Weizhi Gao"
                    },
                    {
                        "name": "Yuchen Shen"
                    },
                    {
                        "name": "Feiyi Wang"
                    },
                    {
                        "name": "Xiaorui Liu"
                    }
                ],
                "author_detail": {
                    "name": "Xiaorui Liu"
                },
                "author": "Xiaorui Liu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.23182v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.23182v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.23180v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.23180v1",
                "updated": "2024-10-30T16:37:04Z",
                "updated_parsed": [
                    2024,
                    10,
                    30,
                    16,
                    37,
                    4,
                    2,
                    304,
                    0
                ],
                "published": "2024-10-30T16:37:04Z",
                "published_parsed": [
                    2024,
                    10,
                    30,
                    16,
                    37,
                    4,
                    2,
                    304,
                    0
                ],
                "title": "ReasoningRec: Bridging Personalized Recommendations and\n  Human-Interpretable Explanations through LLM Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ReasoningRec: Bridging Personalized Recommendations and\n  Human-Interpretable Explanations through LLM Reasoning"
                },
                "summary": "This paper presents ReasoningRec, a reasoning-based recommendation framework\nthat leverages Large Language Models (LLMs) to bridge the gap between\nrecommendations and human-interpretable explanations. In contrast to\nconventional recommendation systems that rely on implicit user-item\ninteractions, ReasoningRec employs LLMs to model users and items, focusing on\npreferences, aversions, and explanatory reasoning. The framework utilizes a\nlarger LLM to generate synthetic explanations for user preferences,\nsubsequently used to fine-tune a smaller LLM for enhanced recommendation\naccuracy and human-interpretable explanation. Our experimental study\ninvestigates the impact of reasoning and contextual information on personalized\nrecommendations, revealing that the quality of contextual and personalized data\nsignificantly influences the LLM's capacity to generate plausible explanations.\nEmpirical evaluations demonstrate that ReasoningRec surpasses state-of-the-art\nmethods by up to 12.5\\% in recommendation prediction while concurrently\nproviding human-intelligible explanations. The code is available here:\nhttps://github.com/millenniumbismay/reasoningrec.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper presents ReasoningRec, a reasoning-based recommendation framework\nthat leverages Large Language Models (LLMs) to bridge the gap between\nrecommendations and human-interpretable explanations. In contrast to\nconventional recommendation systems that rely on implicit user-item\ninteractions, ReasoningRec employs LLMs to model users and items, focusing on\npreferences, aversions, and explanatory reasoning. The framework utilizes a\nlarger LLM to generate synthetic explanations for user preferences,\nsubsequently used to fine-tune a smaller LLM for enhanced recommendation\naccuracy and human-interpretable explanation. Our experimental study\ninvestigates the impact of reasoning and contextual information on personalized\nrecommendations, revealing that the quality of contextual and personalized data\nsignificantly influences the LLM's capacity to generate plausible explanations.\nEmpirical evaluations demonstrate that ReasoningRec surpasses state-of-the-art\nmethods by up to 12.5\\% in recommendation prediction while concurrently\nproviding human-intelligible explanations. The code is available here:\nhttps://github.com/millenniumbismay/reasoningrec."
                },
                "authors": [
                    {
                        "name": "Millennium Bismay"
                    },
                    {
                        "name": "Xiangjue Dong"
                    },
                    {
                        "name": "James Caverlee"
                    }
                ],
                "author_detail": {
                    "name": "James Caverlee"
                },
                "author": "James Caverlee",
                "arxiv_comment": "Large Language Model, Recommendation, Human-Interpretable Reasoning,\n  Personalization",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.23180v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.23180v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.14670v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.14670v2",
                "updated": "2024-10-30T16:33:48Z",
                "updated_parsed": [
                    2024,
                    10,
                    30,
                    16,
                    33,
                    48,
                    2,
                    304,
                    0
                ],
                "published": "2024-06-20T18:47:43Z",
                "published_parsed": [
                    2024,
                    6,
                    20,
                    18,
                    47,
                    43,
                    3,
                    172,
                    0
                ],
                "title": "Exploring Design Choices for Building Language-Specific LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Exploring Design Choices for Building Language-Specific LLMs"
                },
                "summary": "Despite rapid progress in large language models (LLMs), their performance on\na vast majority of languages remains unsatisfactory. In this paper, we study\nbuilding language-specific LLMs by adapting monolingual and multilingual LLMs.\nWe conduct systematic experiments on how design choices (base model selection,\nvocabulary extension, and continued pretraining) impact the adapted LLM, both\nin terms of efficiency (how many tokens are needed to encode the same amount of\ninformation) and end task performance. We find that (1) the initial performance\nof LLM does not always correlate with the final performance after the\nadaptation. Adapting an English-centric models can yield better results than\nadapting multilingual models despite their worse initial performance on\nlow-resource languages. (2) Efficiency can easily improved with simple\nvocabulary extension and continued pretraining in most LLMs we study, and (3)\nThe optimal adaptation method (choice of the base model, new vocabulary size,\ntraining data, initialization strategy) is highly language-dependent, and the\nsimplest embedding initialization works well across various experimental\nsettings. Together, our work lays foundations on efficiently building\nlanguage-specific LLMs by adapting existing LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Despite rapid progress in large language models (LLMs), their performance on\na vast majority of languages remains unsatisfactory. In this paper, we study\nbuilding language-specific LLMs by adapting monolingual and multilingual LLMs.\nWe conduct systematic experiments on how design choices (base model selection,\nvocabulary extension, and continued pretraining) impact the adapted LLM, both\nin terms of efficiency (how many tokens are needed to encode the same amount of\ninformation) and end task performance. We find that (1) the initial performance\nof LLM does not always correlate with the final performance after the\nadaptation. Adapting an English-centric models can yield better results than\nadapting multilingual models despite their worse initial performance on\nlow-resource languages. (2) Efficiency can easily improved with simple\nvocabulary extension and continued pretraining in most LLMs we study, and (3)\nThe optimal adaptation method (choice of the base model, new vocabulary size,\ntraining data, initialization strategy) is highly language-dependent, and the\nsimplest embedding initialization works well across various experimental\nsettings. Together, our work lays foundations on efficiently building\nlanguage-specific LLMs by adapting existing LLMs."
                },
                "authors": [
                    {
                        "name": "Atula Tejaswi"
                    },
                    {
                        "name": "Nilesh Gupta"
                    },
                    {
                        "name": "Eunsol Choi"
                    }
                ],
                "author_detail": {
                    "name": "Eunsol Choi"
                },
                "author": "Eunsol Choi",
                "arxiv_comment": "Accepted to EMNLP 2024 Findings",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.14670v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.14670v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.22309v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.22309v2",
                "updated": "2024-10-30T16:30:30Z",
                "updated_parsed": [
                    2024,
                    10,
                    30,
                    16,
                    30,
                    30,
                    2,
                    304,
                    0
                ],
                "published": "2024-10-29T17:53:10Z",
                "published_parsed": [
                    2024,
                    10,
                    29,
                    17,
                    53,
                    10,
                    1,
                    303,
                    0
                ],
                "title": "GPT-4o reads the mind in the eyes",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GPT-4o reads the mind in the eyes"
                },
                "summary": "Large Language Models (LLMs) are capable of reproducing human-like\ninferences, including inferences about emotions and mental states, from text.\nWhether this capability extends beyond text to other modalities remains\nunclear. Humans possess a sophisticated ability to read the mind in the eyes of\nother people. Here we tested whether this ability is also present in GPT-4o, a\nmultimodal LLM. Using two versions of a widely used theory of mind test, the\nReading the Mind in Eyes Test and the Multiracial Reading the Mind in the Eyes\nTest, we found that GPT-4o outperformed humans in interpreting mental states\nfrom upright faces but underperformed humans when faces were inverted. While\nhumans in our sample showed no difference between White and Non-white faces,\nGPT-4o's accuracy was higher for White than for Non-white faces. GPT-4o's\nerrors were not random but revealed a highly consistent, yet incorrect,\nprocessing of mental-state information across trials, with an\norientation-dependent error structure that qualitatively differed from that of\nhumans for inverted faces but not for upright faces. These findings highlight\nhow advanced mental state inference abilities and human-like face processing\nsignatures, such as inversion effects, coexist in GPT-4o alongside substantial\ndifferences in information processing compared to humans.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) are capable of reproducing human-like\ninferences, including inferences about emotions and mental states, from text.\nWhether this capability extends beyond text to other modalities remains\nunclear. Humans possess a sophisticated ability to read the mind in the eyes of\nother people. Here we tested whether this ability is also present in GPT-4o, a\nmultimodal LLM. Using two versions of a widely used theory of mind test, the\nReading the Mind in Eyes Test and the Multiracial Reading the Mind in the Eyes\nTest, we found that GPT-4o outperformed humans in interpreting mental states\nfrom upright faces but underperformed humans when faces were inverted. While\nhumans in our sample showed no difference between White and Non-white faces,\nGPT-4o's accuracy was higher for White than for Non-white faces. GPT-4o's\nerrors were not random but revealed a highly consistent, yet incorrect,\nprocessing of mental-state information across trials, with an\norientation-dependent error structure that qualitatively differed from that of\nhumans for inverted faces but not for upright faces. These findings highlight\nhow advanced mental state inference abilities and human-like face processing\nsignatures, such as inversion effects, coexist in GPT-4o alongside substantial\ndifferences in information processing compared to humans."
                },
                "authors": [
                    {
                        "name": "James W. A. Strachan"
                    },
                    {
                        "name": "Oriana Pansardi"
                    },
                    {
                        "name": "Eugenio Scaliti"
                    },
                    {
                        "name": "Marco Celotto"
                    },
                    {
                        "name": "Krati Saxena"
                    },
                    {
                        "name": "Chunzhi Yi"
                    },
                    {
                        "name": "Fabio Manzi"
                    },
                    {
                        "name": "Alessandro Rufo"
                    },
                    {
                        "name": "Guido Manzi"
                    },
                    {
                        "name": "Michael S. A. Graziano"
                    },
                    {
                        "name": "Stefano Panzeri"
                    },
                    {
                        "name": "Cristina Becchio"
                    }
                ],
                "author_detail": {
                    "name": "Cristina Becchio"
                },
                "author": "Cristina Becchio",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.22309v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.22309v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.23166v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.23166v1",
                "updated": "2024-10-30T16:18:22Z",
                "updated_parsed": [
                    2024,
                    10,
                    30,
                    16,
                    18,
                    22,
                    2,
                    304,
                    0
                ],
                "published": "2024-10-30T16:18:22Z",
                "published_parsed": [
                    2024,
                    10,
                    30,
                    16,
                    18,
                    22,
                    2,
                    304,
                    0
                ],
                "title": "SciPIP: An LLM-based Scientific Paper Idea Proposer",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SciPIP: An LLM-based Scientific Paper Idea Proposer"
                },
                "summary": "The exponential growth of knowledge and the increasing complexity of\ninterdisciplinary research pose significant challenges for researchers,\nincluding information overload and difficulties in exploring novel ideas. The\nadvancements in large language models (LLMs), such as GPT-4, have shown great\npotential in enhancing idea proposals, but how to effectively utilize large\nmodels for reasonable idea proposal has not been thoroughly explored. This\npaper proposes a scientific paper idea proposer (SciPIP). Based on a\nuser-provided research background, SciPIP retrieves helpful papers from a\nliterature database while leveraging the capabilities of LLMs to generate more\nnovel and feasible ideas. To this end, 1) we construct a literature retrieval\ndatabase, extracting lots of papers' multi-dimension information for fast\naccess. Then, a literature retrieval method based on semantics, entity, and\ncitation co-occurrences is proposed to search relevant literature from multiple\naspects based on the user-provided background. 2) After literature retrieval,\nwe introduce dual-path idea proposal strategies, where one path infers\nsolutions from the retrieved literature and the other path generates original\nideas through model brainstorming. We then combine the two to achieve a good\nbalance between feasibility and originality. Through extensive experiments on\nthe natural language processing (NLP) field, we demonstrate that SciPIP can\nretrieve citations similar to those of existing top conference papers and\ngenerate many ideas consistent with them. Additionally, we evaluate the\noriginality of other ideas generated by SciPIP using large language models,\nfurther validating the effectiveness of our proposed method. The code and the\ndatabase are released at https://github.com/cheerss/SciPIP.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The exponential growth of knowledge and the increasing complexity of\ninterdisciplinary research pose significant challenges for researchers,\nincluding information overload and difficulties in exploring novel ideas. The\nadvancements in large language models (LLMs), such as GPT-4, have shown great\npotential in enhancing idea proposals, but how to effectively utilize large\nmodels for reasonable idea proposal has not been thoroughly explored. This\npaper proposes a scientific paper idea proposer (SciPIP). Based on a\nuser-provided research background, SciPIP retrieves helpful papers from a\nliterature database while leveraging the capabilities of LLMs to generate more\nnovel and feasible ideas. To this end, 1) we construct a literature retrieval\ndatabase, extracting lots of papers' multi-dimension information for fast\naccess. Then, a literature retrieval method based on semantics, entity, and\ncitation co-occurrences is proposed to search relevant literature from multiple\naspects based on the user-provided background. 2) After literature retrieval,\nwe introduce dual-path idea proposal strategies, where one path infers\nsolutions from the retrieved literature and the other path generates original\nideas through model brainstorming. We then combine the two to achieve a good\nbalance between feasibility and originality. Through extensive experiments on\nthe natural language processing (NLP) field, we demonstrate that SciPIP can\nretrieve citations similar to those of existing top conference papers and\ngenerate many ideas consistent with them. Additionally, we evaluate the\noriginality of other ideas generated by SciPIP using large language models,\nfurther validating the effectiveness of our proposed method. The code and the\ndatabase are released at https://github.com/cheerss/SciPIP."
                },
                "authors": [
                    {
                        "name": "Wenxiao Wang"
                    },
                    {
                        "name": "Lihui Gu"
                    },
                    {
                        "name": "Liye Zhang"
                    },
                    {
                        "name": "Yunxiang Luo"
                    },
                    {
                        "name": "Yi Dai"
                    },
                    {
                        "name": "Chen Shen"
                    },
                    {
                        "name": "Liang Xie"
                    },
                    {
                        "name": "Binbin Lin"
                    },
                    {
                        "name": "Xiaofei He"
                    },
                    {
                        "name": "Jieping Ye"
                    }
                ],
                "author_detail": {
                    "name": "Jieping Ye"
                },
                "author": "Jieping Ye",
                "arxiv_comment": "25 pages, 5 figures, 19 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.23166v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.23166v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.19581v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.19581v2",
                "updated": "2024-10-30T16:12:36Z",
                "updated_parsed": [
                    2024,
                    10,
                    30,
                    16,
                    12,
                    36,
                    2,
                    304,
                    0
                ],
                "published": "2024-05-30T00:17:44Z",
                "published_parsed": [
                    2024,
                    5,
                    30,
                    0,
                    17,
                    44,
                    3,
                    151,
                    0
                ],
                "title": "Source Code Foundation Models are Transferable Binary Analysis Knowledge\n  Bases",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Source Code Foundation Models are Transferable Binary Analysis Knowledge\n  Bases"
                },
                "summary": "Human-Oriented Binary Reverse Engineering (HOBRE) lies at the intersection of\nbinary and source code, aiming to lift binary code to human-readable content\nrelevant to source code, thereby bridging the binary-source semantic gap.\nRecent advancements in uni-modal code model pre-training, particularly in\ngenerative Source Code Foundation Models (SCFMs) and binary understanding\nmodels, have laid the groundwork for transfer learning applicable to HOBRE.\nHowever, existing approaches for HOBRE rely heavily on uni-modal models like\nSCFMs for supervised fine-tuning or general LLMs for prompting, resulting in\nsub-optimal performance. Inspired by recent progress in large multi-modal\nmodels, we propose that it is possible to harness the strengths of uni-modal\ncode models from both sides to bridge the semantic gap effectively. In this\npaper, we introduce a novel probe-and-recover framework that incorporates a\nbinary-source encoder-decoder model and black-box LLMs for binary analysis. Our\napproach leverages the pre-trained knowledge within SCFMs to synthesize\nrelevant, symbol-rich code fragments as context. This additional context\nenables black-box LLMs to enhance recovery accuracy. We demonstrate significant\nimprovements in zero-shot binary summarization and binary function name\nrecovery, with a 10.3% relative gain in CHRF and a 16.7% relative gain in a\nGPT4-based metric for summarization, as well as a 6.7% and 7.4% absolute\nincrease in token-level precision and recall for name recovery, respectively.\nThese results highlight the effectiveness of our approach in automating and\nimproving binary code analysis.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Human-Oriented Binary Reverse Engineering (HOBRE) lies at the intersection of\nbinary and source code, aiming to lift binary code to human-readable content\nrelevant to source code, thereby bridging the binary-source semantic gap.\nRecent advancements in uni-modal code model pre-training, particularly in\ngenerative Source Code Foundation Models (SCFMs) and binary understanding\nmodels, have laid the groundwork for transfer learning applicable to HOBRE.\nHowever, existing approaches for HOBRE rely heavily on uni-modal models like\nSCFMs for supervised fine-tuning or general LLMs for prompting, resulting in\nsub-optimal performance. Inspired by recent progress in large multi-modal\nmodels, we propose that it is possible to harness the strengths of uni-modal\ncode models from both sides to bridge the semantic gap effectively. In this\npaper, we introduce a novel probe-and-recover framework that incorporates a\nbinary-source encoder-decoder model and black-box LLMs for binary analysis. Our\napproach leverages the pre-trained knowledge within SCFMs to synthesize\nrelevant, symbol-rich code fragments as context. This additional context\nenables black-box LLMs to enhance recovery accuracy. We demonstrate significant\nimprovements in zero-shot binary summarization and binary function name\nrecovery, with a 10.3% relative gain in CHRF and a 16.7% relative gain in a\nGPT4-based metric for summarization, as well as a 6.7% and 7.4% absolute\nincrease in token-level precision and recall for name recovery, respectively.\nThese results highlight the effectiveness of our approach in automating and\nimproving binary code analysis."
                },
                "authors": [
                    {
                        "name": "Zian Su"
                    },
                    {
                        "name": "Xiangzhe Xu"
                    },
                    {
                        "name": "Ziyang Huang"
                    },
                    {
                        "name": "Kaiyuan Zhang"
                    },
                    {
                        "name": "Xiangyu Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Xiangyu Zhang"
                },
                "author": "Xiangyu Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.19581v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.19581v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.18975v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.18975v2",
                "updated": "2024-10-30T16:10:33Z",
                "updated_parsed": [
                    2024,
                    10,
                    30,
                    16,
                    10,
                    33,
                    2,
                    304,
                    0
                ],
                "published": "2024-10-24T17:59:31Z",
                "published_parsed": [
                    2024,
                    10,
                    24,
                    17,
                    59,
                    31,
                    3,
                    298,
                    0
                ],
                "title": "Unbounded: A Generative Infinite Game of Character Life Simulation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Unbounded: A Generative Infinite Game of Character Life Simulation"
                },
                "summary": "We introduce the concept of a generative infinite game, a video game that\ntranscends the traditional boundaries of finite, hard-coded systems by using\ngenerative models. Inspired by James P. Carse's distinction between finite and\ninfinite games, we leverage recent advances in generative AI to create\nUnbounded: a game of character life simulation that is fully encapsulated in\ngenerative models. Specifically, Unbounded draws inspiration from sandbox life\nsimulations and allows you to interact with your autonomous virtual character\nin a virtual world by feeding, playing with and guiding it - with open-ended\nmechanics generated by an LLM, some of which can be emergent. In order to\ndevelop Unbounded, we propose technical innovations in both the LLM and visual\ngeneration domains. Specifically, we present: (1) a specialized, distilled\nlarge language model (LLM) that dynamically generates game mechanics,\nnarratives, and character interactions in real-time, and (2) a new dynamic\nregional image prompt Adapter (IP-Adapter) for vision models that ensures\nconsistent yet flexible visual generation of a character across multiple\nenvironments. We evaluate our system through both qualitative and quantitative\nanalysis, showing significant improvements in character life simulation, user\ninstruction following, narrative coherence, and visual consistency for both\ncharacters and the environments compared to traditional related approaches.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce the concept of a generative infinite game, a video game that\ntranscends the traditional boundaries of finite, hard-coded systems by using\ngenerative models. Inspired by James P. Carse's distinction between finite and\ninfinite games, we leverage recent advances in generative AI to create\nUnbounded: a game of character life simulation that is fully encapsulated in\ngenerative models. Specifically, Unbounded draws inspiration from sandbox life\nsimulations and allows you to interact with your autonomous virtual character\nin a virtual world by feeding, playing with and guiding it - with open-ended\nmechanics generated by an LLM, some of which can be emergent. In order to\ndevelop Unbounded, we propose technical innovations in both the LLM and visual\ngeneration domains. Specifically, we present: (1) a specialized, distilled\nlarge language model (LLM) that dynamically generates game mechanics,\nnarratives, and character interactions in real-time, and (2) a new dynamic\nregional image prompt Adapter (IP-Adapter) for vision models that ensures\nconsistent yet flexible visual generation of a character across multiple\nenvironments. We evaluate our system through both qualitative and quantitative\nanalysis, showing significant improvements in character life simulation, user\ninstruction following, narrative coherence, and visual consistency for both\ncharacters and the environments compared to traditional related approaches."
                },
                "authors": [
                    {
                        "name": "Jialu Li"
                    },
                    {
                        "name": "Yuanzhen Li"
                    },
                    {
                        "name": "Neal Wadhwa"
                    },
                    {
                        "name": "Yael Pritch"
                    },
                    {
                        "name": "David E. Jacobs"
                    },
                    {
                        "name": "Michael Rubinstein"
                    },
                    {
                        "name": "Mohit Bansal"
                    },
                    {
                        "name": "Nataniel Ruiz"
                    }
                ],
                "author_detail": {
                    "name": "Nataniel Ruiz"
                },
                "author": "Nataniel Ruiz",
                "arxiv_comment": "Project page: https://generative-infinite-game.github.io/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.18975v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.18975v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.GR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.03656v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.03656v3",
                "updated": "2024-10-30T15:51:06Z",
                "updated_parsed": [
                    2024,
                    10,
                    30,
                    15,
                    51,
                    6,
                    2,
                    304,
                    0
                ],
                "published": "2024-07-04T05:54:19Z",
                "published_parsed": [
                    2024,
                    7,
                    4,
                    5,
                    54,
                    19,
                    3,
                    186,
                    0
                ],
                "title": "WildDESED: An LLM-Powered Dataset for Wild Domestic Environment Sound\n  Event Detection System",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "WildDESED: An LLM-Powered Dataset for Wild Domestic Environment Sound\n  Event Detection System"
                },
                "summary": "This work aims to advance sound event detection (SED) research by presenting\na new large language model (LLM)-powered dataset namely wild domestic\nenvironment sound event detection (WildDESED). It is crafted as an extension to\nthe original DESED dataset to reflect diverse acoustic variability and complex\nnoises in home settings. We leveraged LLMs to generate eight different domestic\nscenarios based on target sound categories of the DESED dataset. Then we\nenriched the scenarios with a carefully tailored mixture of noises selected\nfrom AudioSet and ensured no overlap with target sound. We consider widely\npopular convolutional neural recurrent network to study WildDESED dataset,\nwhich depicts its challenging nature. We then apply curriculum learning by\ngradually increasing noise complexity to enhance the model's generalization\ncapabilities across various noise levels. Our results with this approach show\nimprovements within the noisy environment, validating the effectiveness on the\nWildDESED dataset promoting noise-robust SED advancements.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This work aims to advance sound event detection (SED) research by presenting\na new large language model (LLM)-powered dataset namely wild domestic\nenvironment sound event detection (WildDESED). It is crafted as an extension to\nthe original DESED dataset to reflect diverse acoustic variability and complex\nnoises in home settings. We leveraged LLMs to generate eight different domestic\nscenarios based on target sound categories of the DESED dataset. Then we\nenriched the scenarios with a carefully tailored mixture of noises selected\nfrom AudioSet and ensured no overlap with target sound. We consider widely\npopular convolutional neural recurrent network to study WildDESED dataset,\nwhich depicts its challenging nature. We then apply curriculum learning by\ngradually increasing noise complexity to enhance the model's generalization\ncapabilities across various noise levels. Our results with this approach show\nimprovements within the noisy environment, validating the effectiveness on the\nWildDESED dataset promoting noise-robust SED advancements."
                },
                "authors": [
                    {
                        "name": "Yang Xiao"
                    },
                    {
                        "name": "Rohan Kumar Das"
                    }
                ],
                "author_detail": {
                    "name": "Rohan Kumar Das"
                },
                "author": "Rohan Kumar Das",
                "arxiv_comment": "DCASE WS 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.03656v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.03656v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.AS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.23136v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.23136v1",
                "updated": "2024-10-30T15:48:36Z",
                "updated_parsed": [
                    2024,
                    10,
                    30,
                    15,
                    48,
                    36,
                    2,
                    304,
                    0
                ],
                "published": "2024-10-30T15:48:36Z",
                "published_parsed": [
                    2024,
                    10,
                    30,
                    15,
                    48,
                    36,
                    2,
                    304,
                    0
                ],
                "title": "Real-Time Personalization for LLM-based Recommendation with Customized\n  In-Context Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Real-Time Personalization for LLM-based Recommendation with Customized\n  In-Context Learning"
                },
                "summary": "Frequently updating Large Language Model (LLM)-based recommender systems to\nadapt to new user interests -- as done for traditional ones -- is impractical\ndue to high training costs, even with acceleration methods. This work explores\nadapting to dynamic user interests without any model updates by leveraging\nIn-Context Learning (ICL), which allows LLMs to learn new tasks from few-shot\nexamples provided in the input. Using new-interest examples as the ICL few-shot\nexamples, LLMs may learn real-time interest directly, avoiding the need for\nmodel updates. However, existing LLM-based recommenders often lose the\nin-context learning ability during recommendation tuning, while the original\nLLM's in-context learning lacks recommendation-specific focus. To address this,\nwe propose RecICL, which customizes recommendation-specific in-context learning\nfor real-time recommendations. RecICL organizes training examples in an\nin-context learning format, ensuring that in-context learning ability is\npreserved and aligned with the recommendation task during tuning.\n  Extensive experiments demonstrate RecICL's effectiveness in delivering\nreal-time recommendations without requiring model updates. Our code is\navailable at https://github.com/ym689/rec_icl.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Frequently updating Large Language Model (LLM)-based recommender systems to\nadapt to new user interests -- as done for traditional ones -- is impractical\ndue to high training costs, even with acceleration methods. This work explores\nadapting to dynamic user interests without any model updates by leveraging\nIn-Context Learning (ICL), which allows LLMs to learn new tasks from few-shot\nexamples provided in the input. Using new-interest examples as the ICL few-shot\nexamples, LLMs may learn real-time interest directly, avoiding the need for\nmodel updates. However, existing LLM-based recommenders often lose the\nin-context learning ability during recommendation tuning, while the original\nLLM's in-context learning lacks recommendation-specific focus. To address this,\nwe propose RecICL, which customizes recommendation-specific in-context learning\nfor real-time recommendations. RecICL organizes training examples in an\nin-context learning format, ensuring that in-context learning ability is\npreserved and aligned with the recommendation task during tuning.\n  Extensive experiments demonstrate RecICL's effectiveness in delivering\nreal-time recommendations without requiring model updates. Our code is\navailable at https://github.com/ym689/rec_icl."
                },
                "authors": [
                    {
                        "name": "Keqin Bao"
                    },
                    {
                        "name": "Ming Yan"
                    },
                    {
                        "name": "Yang Zhang"
                    },
                    {
                        "name": "Jizhi Zhang"
                    },
                    {
                        "name": "Wenjie Wang"
                    },
                    {
                        "name": "Fuli Feng"
                    },
                    {
                        "name": "Xiangnan He"
                    }
                ],
                "author_detail": {
                    "name": "Xiangnan He"
                },
                "author": "Xiangnan He",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.23136v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.23136v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.23123v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.23123v1",
                "updated": "2024-10-30T15:31:54Z",
                "updated_parsed": [
                    2024,
                    10,
                    30,
                    15,
                    31,
                    54,
                    2,
                    304,
                    0
                ],
                "published": "2024-10-30T15:31:54Z",
                "published_parsed": [
                    2024,
                    10,
                    30,
                    15,
                    31,
                    54,
                    2,
                    304,
                    0
                ],
                "title": "On Memorization of Large Language Models in Logical Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "On Memorization of Large Language Models in Logical Reasoning"
                },
                "summary": "Large language models (LLMs) achieve good performance on challenging\nreasoning benchmarks, yet could also make basic reasoning mistakes. This\ncontrasting behavior is puzzling when it comes to understanding the mechanisms\nbehind LLMs' reasoning capabilities. One hypothesis is that the increasingly\nhigh and nearly saturated performance on common reasoning benchmarks could be\ndue to the memorization of similar problems. In this paper, we systematically\ninvestigate this hypothesis with a quantitative measurement of memorization in\nreasoning tasks, using a dynamically generated logical reasoning benchmark\nbased on Knights and Knaves (K&K) puzzles. We found that LLMs could interpolate\nthe training puzzles (achieving near-perfect accuracy) after fine-tuning, yet\nfail when those puzzles are slightly perturbed, suggesting that the models\nheavily rely on memorization to solve those training puzzles. On the other\nhand, we show that while fine-tuning leads to heavy memorization, it also\nconsistently improves generalization performance. In-depth analyses with\nperturbation tests, cross difficulty-level transferability, probing model\ninternals, and fine-tuning with wrong answers suggest that the LLMs learn to\nreason on K&K puzzles despite training data memorization. This phenomenon\nindicates that LLMs exhibit a complex interplay between memorization and\ngenuine reasoning abilities. Finally, our analysis with per-sample memorization\nscore sheds light on how LLMs switch between reasoning and memorization in\nsolving logical puzzles. Our code and data are available at\nhttps://memkklogic.github.io.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) achieve good performance on challenging\nreasoning benchmarks, yet could also make basic reasoning mistakes. This\ncontrasting behavior is puzzling when it comes to understanding the mechanisms\nbehind LLMs' reasoning capabilities. One hypothesis is that the increasingly\nhigh and nearly saturated performance on common reasoning benchmarks could be\ndue to the memorization of similar problems. In this paper, we systematically\ninvestigate this hypothesis with a quantitative measurement of memorization in\nreasoning tasks, using a dynamically generated logical reasoning benchmark\nbased on Knights and Knaves (K&K) puzzles. We found that LLMs could interpolate\nthe training puzzles (achieving near-perfect accuracy) after fine-tuning, yet\nfail when those puzzles are slightly perturbed, suggesting that the models\nheavily rely on memorization to solve those training puzzles. On the other\nhand, we show that while fine-tuning leads to heavy memorization, it also\nconsistently improves generalization performance. In-depth analyses with\nperturbation tests, cross difficulty-level transferability, probing model\ninternals, and fine-tuning with wrong answers suggest that the LLMs learn to\nreason on K&K puzzles despite training data memorization. This phenomenon\nindicates that LLMs exhibit a complex interplay between memorization and\ngenuine reasoning abilities. Finally, our analysis with per-sample memorization\nscore sheds light on how LLMs switch between reasoning and memorization in\nsolving logical puzzles. Our code and data are available at\nhttps://memkklogic.github.io."
                },
                "authors": [
                    {
                        "name": "Chulin Xie"
                    },
                    {
                        "name": "Yangsibo Huang"
                    },
                    {
                        "name": "Chiyuan Zhang"
                    },
                    {
                        "name": "Da Yu"
                    },
                    {
                        "name": "Xinyun Chen"
                    },
                    {
                        "name": "Bill Yuchen Lin"
                    },
                    {
                        "name": "Bo Li"
                    },
                    {
                        "name": "Badih Ghazi"
                    },
                    {
                        "name": "Ravi Kumar"
                    }
                ],
                "author_detail": {
                    "name": "Ravi Kumar"
                },
                "author": "Ravi Kumar",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.23123v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.23123v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.18952v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.18952v2",
                "updated": "2024-10-30T15:28:02Z",
                "updated_parsed": [
                    2024,
                    10,
                    30,
                    15,
                    28,
                    2,
                    2,
                    304,
                    0
                ],
                "published": "2024-10-24T17:52:31Z",
                "published_parsed": [
                    2024,
                    10,
                    24,
                    17,
                    52,
                    31,
                    3,
                    298,
                    0
                ],
                "title": "Dynamic Vocabulary Pruning in Early-Exit LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Dynamic Vocabulary Pruning in Early-Exit LLMs"
                },
                "summary": "Increasing the size of large language models (LLMs) has been shown to lead to\nbetter performance. However, this comes at the cost of slower and more\nexpensive inference. Early-exiting is a promising approach for improving the\nefficiency of LLM inference by enabling next token prediction at intermediate\nlayers. Yet, the large vocabulary size in modern LLMs makes the confidence\nestimation required for exit decisions computationally expensive, diminishing\nthe efficiency gains. To address this, we propose dynamically pruning the\nvocabulary at test time for each token. Specifically, the vocabulary is pruned\nat one of the initial layers, and the smaller vocabulary is then used\nthroughout the rest of the forward pass. Our experiments demonstrate that such\npost-hoc dynamic vocabulary pruning improves the efficiency of confidence\nestimation in early-exit LLMs while maintaining competitive performance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Increasing the size of large language models (LLMs) has been shown to lead to\nbetter performance. However, this comes at the cost of slower and more\nexpensive inference. Early-exiting is a promising approach for improving the\nefficiency of LLM inference by enabling next token prediction at intermediate\nlayers. Yet, the large vocabulary size in modern LLMs makes the confidence\nestimation required for exit decisions computationally expensive, diminishing\nthe efficiency gains. To address this, we propose dynamically pruning the\nvocabulary at test time for each token. Specifically, the vocabulary is pruned\nat one of the initial layers, and the smaller vocabulary is then used\nthroughout the rest of the forward pass. Our experiments demonstrate that such\npost-hoc dynamic vocabulary pruning improves the efficiency of confidence\nestimation in early-exit LLMs while maintaining competitive performance."
                },
                "authors": [
                    {
                        "name": "Jort Vincenti"
                    },
                    {
                        "name": "Karim Abdel Sadek"
                    },
                    {
                        "name": "Joan Velja"
                    },
                    {
                        "name": "Matteo Nulli"
                    },
                    {
                        "name": "Metod Jazbec"
                    }
                ],
                "author_detail": {
                    "name": "Metod Jazbec"
                },
                "author": "Metod Jazbec",
                "arxiv_journal_ref": "NeurIPS 2024 ENLSP Workshop",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.18952v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.18952v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.23111v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.23111v1",
                "updated": "2024-10-30T15:23:44Z",
                "updated_parsed": [
                    2024,
                    10,
                    30,
                    15,
                    23,
                    44,
                    2,
                    304,
                    0
                ],
                "published": "2024-10-30T15:23:44Z",
                "published_parsed": [
                    2024,
                    10,
                    30,
                    15,
                    23,
                    44,
                    2,
                    304,
                    0
                ],
                "title": "Why Gradient Subspace? Identifying and Mitigating LoRA's Bottlenecks in\n  Federated Fine-Tuning of Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Why Gradient Subspace? Identifying and Mitigating LoRA's Bottlenecks in\n  Federated Fine-Tuning of Large Language Models"
                },
                "summary": "Large Language Models (LLMs) have demonstrated remarkable capabilities across\nvarious domains, particularly in task generalization for both text and vision\ndata. While fine-tuning these models can significantly enhance their\nperformance on specific downstream tasks, it often requires high-quality data\nthat cannot be shared due to privacy concerns. Federated Learning (FL) offers a\npromising solution for collaborative training without direct data sharing.\nHowever, many parameter-efficient fine-tuning strategies for LLMs in FL,\nparticularly those based on Low-Rank Adaptation (LoRA), face limitations. In\nthis paper, we critically analyze the convergence and performance guarantees of\npopular FL frameworks utilizing LoRA, highlighting its suboptimal nature due to\nconstrained subspace learning of low-rank matrices. This limitation hinders\neffective fine-tuning of LLMs in federated settings. Through rigorous\nanalytical and empirical evaluations, we demonstrate that direct weight\naveraging outperforms LoRA-based strategies, leading to superior performance\nfor fine-tuned models. Our comprehensive comparison exposes inefficiencies in\nLoRA approaches and underscores the advantages of full-rank weight aggregation.\nWe extend our analysis to low-rank gradient-based optimizers, such as GaLore,\nused during local training steps. Our findings show that GaLore is a more\neffective alternative, outperforming federated LoRA methods like FlexLoRA and\nFFA-LoRA across both text and image modalities. While privacy remains paramount\nin FL discourse, our focus is on assessing performance outcomes of federated\nfine-tuned models and evaluating various FL frameworks from both theoretical\nand empirical perspectives. Our findings advocate reassessing the reliance on\nLoRA within FL contexts, paving the way for more efficient training\nmethodologies.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have demonstrated remarkable capabilities across\nvarious domains, particularly in task generalization for both text and vision\ndata. While fine-tuning these models can significantly enhance their\nperformance on specific downstream tasks, it often requires high-quality data\nthat cannot be shared due to privacy concerns. Federated Learning (FL) offers a\npromising solution for collaborative training without direct data sharing.\nHowever, many parameter-efficient fine-tuning strategies for LLMs in FL,\nparticularly those based on Low-Rank Adaptation (LoRA), face limitations. In\nthis paper, we critically analyze the convergence and performance guarantees of\npopular FL frameworks utilizing LoRA, highlighting its suboptimal nature due to\nconstrained subspace learning of low-rank matrices. This limitation hinders\neffective fine-tuning of LLMs in federated settings. Through rigorous\nanalytical and empirical evaluations, we demonstrate that direct weight\naveraging outperforms LoRA-based strategies, leading to superior performance\nfor fine-tuned models. Our comprehensive comparison exposes inefficiencies in\nLoRA approaches and underscores the advantages of full-rank weight aggregation.\nWe extend our analysis to low-rank gradient-based optimizers, such as GaLore,\nused during local training steps. Our findings show that GaLore is a more\neffective alternative, outperforming federated LoRA methods like FlexLoRA and\nFFA-LoRA across both text and image modalities. While privacy remains paramount\nin FL discourse, our focus is on assessing performance outcomes of federated\nfine-tuned models and evaluating various FL frameworks from both theoretical\nand empirical perspectives. Our findings advocate reassessing the reliance on\nLoRA within FL contexts, paving the way for more efficient training\nmethodologies."
                },
                "authors": [
                    {
                        "name": "Navyansh Mahla"
                    },
                    {
                        "name": "Ganesh Ramakrishnan"
                    }
                ],
                "author_detail": {
                    "name": "Ganesh Ramakrishnan"
                },
                "author": "Ganesh Ramakrishnan",
                "arxiv_comment": "24 pages, 10 figures, pre-print",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.23111v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.23111v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2310.08975v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2310.08975v3",
                "updated": "2024-10-30T15:22:12Z",
                "updated_parsed": [
                    2024,
                    10,
                    30,
                    15,
                    22,
                    12,
                    2,
                    304,
                    0
                ],
                "published": "2023-10-13T09:45:14Z",
                "published_parsed": [
                    2023,
                    10,
                    13,
                    9,
                    45,
                    14,
                    4,
                    286,
                    0
                ],
                "title": "ChatKBQA: A Generate-then-Retrieve Framework for Knowledge Base Question\n  Answering with Fine-tuned Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ChatKBQA: A Generate-then-Retrieve Framework for Knowledge Base Question\n  Answering with Fine-tuned Large Language Models"
                },
                "summary": "Knowledge Base Question Answering (KBQA) aims to answer natural language\nquestions over large-scale knowledge bases (KBs), which can be summarized into\ntwo crucial steps: knowledge retrieval and semantic parsing. However, three\ncore challenges remain: inefficient knowledge retrieval, mistakes of retrieval\nadversely impacting semantic parsing, and the complexity of previous KBQA\nmethods. To tackle these challenges, we introduce ChatKBQA, a novel and simple\ngenerate-then-retrieve KBQA framework, which proposes first generating the\nlogical form with fine-tuned LLMs, then retrieving and replacing entities and\nrelations with an unsupervised retrieval method, to improve both generation and\nretrieval more directly. Experimental results show that ChatKBQA achieves new\nstate-of-the-art performance on standard KBQA datasets, WebQSP, and CWQ. This\nwork can also be regarded as a new paradigm for combining LLMs with knowledge\ngraphs (KGs) for interpretable and knowledge-required question answering. Our\ncode is publicly available.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Knowledge Base Question Answering (KBQA) aims to answer natural language\nquestions over large-scale knowledge bases (KBs), which can be summarized into\ntwo crucial steps: knowledge retrieval and semantic parsing. However, three\ncore challenges remain: inefficient knowledge retrieval, mistakes of retrieval\nadversely impacting semantic parsing, and the complexity of previous KBQA\nmethods. To tackle these challenges, we introduce ChatKBQA, a novel and simple\ngenerate-then-retrieve KBQA framework, which proposes first generating the\nlogical form with fine-tuned LLMs, then retrieving and replacing entities and\nrelations with an unsupervised retrieval method, to improve both generation and\nretrieval more directly. Experimental results show that ChatKBQA achieves new\nstate-of-the-art performance on standard KBQA datasets, WebQSP, and CWQ. This\nwork can also be regarded as a new paradigm for combining LLMs with knowledge\ngraphs (KGs) for interpretable and knowledge-required question answering. Our\ncode is publicly available."
                },
                "authors": [
                    {
                        "name": "Haoran Luo"
                    },
                    {
                        "name": "Haihong E"
                    },
                    {
                        "name": "Zichen Tang"
                    },
                    {
                        "name": "Shiyao Peng"
                    },
                    {
                        "name": "Yikai Guo"
                    },
                    {
                        "name": "Wentai Zhang"
                    },
                    {
                        "name": "Chenghao Ma"
                    },
                    {
                        "name": "Guanting Dong"
                    },
                    {
                        "name": "Meina Song"
                    },
                    {
                        "name": "Wei Lin"
                    },
                    {
                        "name": "Yifan Zhu"
                    },
                    {
                        "name": "Luu Anh Tuan"
                    }
                ],
                "author_detail": {
                    "name": "Luu Anh Tuan"
                },
                "author": "Luu Anh Tuan",
                "arxiv_doi": "10.18653/v1/2024.findings-acl.122",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.18653/v1/2024.findings-acl.122",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2310.08975v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2310.08975v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "Accepted by Findings of ACL 2024",
                "arxiv_journal_ref": "ACL 2024",
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.23099v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.23099v1",
                "updated": "2024-10-30T15:11:58Z",
                "updated_parsed": [
                    2024,
                    10,
                    30,
                    15,
                    11,
                    58,
                    2,
                    304,
                    0
                ],
                "published": "2024-10-30T15:11:58Z",
                "published_parsed": [
                    2024,
                    10,
                    30,
                    15,
                    11,
                    58,
                    2,
                    304,
                    0
                ],
                "title": "Comparative Analysis of Demonstration Selection Algorithms for LLM\n  In-Context Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Comparative Analysis of Demonstration Selection Algorithms for LLM\n  In-Context Learning"
                },
                "summary": "In-context learning can help Large Language Models (LLMs) to adapt new tasks\nwithout additional training. However, this performance heavily depends on the\nquality of the demonstrations, driving research into effective demonstration\nselection algorithms to optimize this process. These algorithms assist users in\nselecting the best $k$ input-label pairs (demonstration examples) based on a\ngiven test input, enabling LLMs to in-context learn the relationship between\nthe provided examples and the test inputs. Despite all the proposed\ndemonstration selection algorithms, their efficiency and effectiveness remain\nunclear. This lack of clarity make it difficult to apply these algorithms in\nreal-world scenarios and poses challenges for future research aimed at\ndeveloping improved methods. This paper revisits six proposed algorithms,\nevaluating them on five datasets from both efficiency and effectiveness\nperspectives. Our experiments reveal significant variations in algorithm\nperformance across different tasks, with some methods struggling to outperform\nrandom selection in certain scenarios. We also find that increasing the number\nof demonstrations does not always lead to better performance, and that there\nare often trade-offs between accuracy and computational efficiency. Our code is\navailable at https://github.com/Tizzzzy/Demonstration_Selection_Overview.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In-context learning can help Large Language Models (LLMs) to adapt new tasks\nwithout additional training. However, this performance heavily depends on the\nquality of the demonstrations, driving research into effective demonstration\nselection algorithms to optimize this process. These algorithms assist users in\nselecting the best $k$ input-label pairs (demonstration examples) based on a\ngiven test input, enabling LLMs to in-context learn the relationship between\nthe provided examples and the test inputs. Despite all the proposed\ndemonstration selection algorithms, their efficiency and effectiveness remain\nunclear. This lack of clarity make it difficult to apply these algorithms in\nreal-world scenarios and poses challenges for future research aimed at\ndeveloping improved methods. This paper revisits six proposed algorithms,\nevaluating them on five datasets from both efficiency and effectiveness\nperspectives. Our experiments reveal significant variations in algorithm\nperformance across different tasks, with some methods struggling to outperform\nrandom selection in certain scenarios. We also find that increasing the number\nof demonstrations does not always lead to better performance, and that there\nare often trade-offs between accuracy and computational efficiency. Our code is\navailable at https://github.com/Tizzzzy/Demonstration_Selection_Overview."
                },
                "authors": [
                    {
                        "name": "Dong Shu"
                    },
                    {
                        "name": "Mengnan Du"
                    }
                ],
                "author_detail": {
                    "name": "Mengnan Du"
                },
                "author": "Mengnan Du",
                "arxiv_comment": "6 pages, 4 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.23099v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.23099v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.23090v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.23090v1",
                "updated": "2024-10-30T15:06:32Z",
                "updated_parsed": [
                    2024,
                    10,
                    30,
                    15,
                    6,
                    32,
                    2,
                    304,
                    0
                ],
                "published": "2024-10-30T15:06:32Z",
                "published_parsed": [
                    2024,
                    10,
                    30,
                    15,
                    6,
                    32,
                    2,
                    304,
                    0
                ],
                "title": "CORAL: Benchmarking Multi-turn Conversational Retrieval-Augmentation\n  Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CORAL: Benchmarking Multi-turn Conversational Retrieval-Augmentation\n  Generation"
                },
                "summary": "Retrieval-Augmented Generation (RAG) has become a powerful paradigm for\nenhancing large language models (LLMs) through external knowledge retrieval.\nDespite its widespread attention, existing academic research predominantly\nfocuses on single-turn RAG, leaving a significant gap in addressing the\ncomplexities of multi-turn conversations found in real-world applications. To\nbridge this gap, we introduce CORAL, a large-scale benchmark designed to assess\nRAG systems in realistic multi-turn conversational settings. CORAL includes\ndiverse information-seeking conversations automatically derived from Wikipedia\nand tackles key challenges such as open-domain coverage, knowledge intensity,\nfree-form responses, and topic shifts. It supports three core tasks of\nconversational RAG: passage retrieval, response generation, and citation\nlabeling. We propose a unified framework to standardize various conversational\nRAG methods and conduct a comprehensive evaluation of these methods on CORAL,\ndemonstrating substantial opportunities for improving existing approaches.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Retrieval-Augmented Generation (RAG) has become a powerful paradigm for\nenhancing large language models (LLMs) through external knowledge retrieval.\nDespite its widespread attention, existing academic research predominantly\nfocuses on single-turn RAG, leaving a significant gap in addressing the\ncomplexities of multi-turn conversations found in real-world applications. To\nbridge this gap, we introduce CORAL, a large-scale benchmark designed to assess\nRAG systems in realistic multi-turn conversational settings. CORAL includes\ndiverse information-seeking conversations automatically derived from Wikipedia\nand tackles key challenges such as open-domain coverage, knowledge intensity,\nfree-form responses, and topic shifts. It supports three core tasks of\nconversational RAG: passage retrieval, response generation, and citation\nlabeling. We propose a unified framework to standardize various conversational\nRAG methods and conduct a comprehensive evaluation of these methods on CORAL,\ndemonstrating substantial opportunities for improving existing approaches."
                },
                "authors": [
                    {
                        "name": "Yiruo Cheng"
                    },
                    {
                        "name": "Kelong Mao"
                    },
                    {
                        "name": "Ziliang Zhao"
                    },
                    {
                        "name": "Guanting Dong"
                    },
                    {
                        "name": "Hongjin Qian"
                    },
                    {
                        "name": "Yongkang Wu"
                    },
                    {
                        "name": "Tetsuya Sakai"
                    },
                    {
                        "name": "Ji-Rong Wen"
                    },
                    {
                        "name": "Zhicheng Dou"
                    }
                ],
                "author_detail": {
                    "name": "Zhicheng Dou"
                },
                "author": "Zhicheng Dou",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.23090v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.23090v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.23089v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.23089v1",
                "updated": "2024-10-30T15:05:17Z",
                "updated_parsed": [
                    2024,
                    10,
                    30,
                    15,
                    5,
                    17,
                    2,
                    304,
                    0
                ],
                "published": "2024-10-30T15:05:17Z",
                "published_parsed": [
                    2024,
                    10,
                    30,
                    15,
                    5,
                    17,
                    2,
                    304,
                    0
                ],
                "title": "PIP-MM: Pre-Integrating Prompt Information into Visual Encoding via\n  Existing MLLM Structures",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PIP-MM: Pre-Integrating Prompt Information into Visual Encoding via\n  Existing MLLM Structures"
                },
                "summary": "The Multimodal Large Language Models (MLLMs) have activated the\ncapabilitiesof Large Language Models (LLMs) in solving visual-language tasks by\nintegratingvisual information. The prevailing approach in existing MLLMs\ninvolvesemploying an image encoder to extract visual features, converting\nthesefeatures into visual tokens via an adapter, and then integrating them with\ntheprompt into the LLM. However, because the process of image encoding\nisprompt-agnostic, the extracted visual features only provide a\ncoarsedescription of the image, impossible to focus on the requirements of\ntheprompt. On one hand, it is easy for image features to lack information\naboutthe prompt-specified objects, resulting in unsatisfactory responses. On\ntheother hand, the visual features contain a large amount of\nirrelevantinformation, which not only increases the burden on memory but also\nworsens thegeneration effectiveness. To address the aforementioned issues, we\npropose\\textbf{PIP-MM}, a framework that\n\\textbf{P}re-\\textbf{I}ntegrates\\textbf{P}rompt information into the visual\nencoding process using existingmodules of MLLMs. Specifically, We utilize the\nfrozen LLM in the MLLM tovectorize the input prompt, which summarizes the\nrequirements of the prompt.Then, we input the prompt vector into our trained\nMulti-Layer Perceptron (MLP)to align with the visual input requirements, and\nsubsequently replace the classembedding in the image encoder. Since our model\nonly requires adding atrainable MLP, it can be applied to any MLLM. To validate\nthe effectiveness ofPIP-MM, we conducted experiments on multiple benchmarks.\nAutomated evaluationmetrics and manual assessments demonstrate the strong\nperformance of PIP-MM.Particularly noteworthy is that our model maintains\nexcellent generationresults even when half of the visual tokens are reduced.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Multimodal Large Language Models (MLLMs) have activated the\ncapabilitiesof Large Language Models (LLMs) in solving visual-language tasks by\nintegratingvisual information. The prevailing approach in existing MLLMs\ninvolvesemploying an image encoder to extract visual features, converting\nthesefeatures into visual tokens via an adapter, and then integrating them with\ntheprompt into the LLM. However, because the process of image encoding\nisprompt-agnostic, the extracted visual features only provide a\ncoarsedescription of the image, impossible to focus on the requirements of\ntheprompt. On one hand, it is easy for image features to lack information\naboutthe prompt-specified objects, resulting in unsatisfactory responses. On\ntheother hand, the visual features contain a large amount of\nirrelevantinformation, which not only increases the burden on memory but also\nworsens thegeneration effectiveness. To address the aforementioned issues, we\npropose\\textbf{PIP-MM}, a framework that\n\\textbf{P}re-\\textbf{I}ntegrates\\textbf{P}rompt information into the visual\nencoding process using existingmodules of MLLMs. Specifically, We utilize the\nfrozen LLM in the MLLM tovectorize the input prompt, which summarizes the\nrequirements of the prompt.Then, we input the prompt vector into our trained\nMulti-Layer Perceptron (MLP)to align with the visual input requirements, and\nsubsequently replace the classembedding in the image encoder. Since our model\nonly requires adding atrainable MLP, it can be applied to any MLLM. To validate\nthe effectiveness ofPIP-MM, we conducted experiments on multiple benchmarks.\nAutomated evaluationmetrics and manual assessments demonstrate the strong\nperformance of PIP-MM.Particularly noteworthy is that our model maintains\nexcellent generationresults even when half of the visual tokens are reduced."
                },
                "authors": [
                    {
                        "name": "Tianxiang Wu"
                    },
                    {
                        "name": "Minxin Nie"
                    },
                    {
                        "name": "Ziqiang Cao"
                    }
                ],
                "author_detail": {
                    "name": "Ziqiang Cao"
                },
                "author": "Ziqiang Cao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.23089v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.23089v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.23086v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.23086v1",
                "updated": "2024-10-30T15:02:54Z",
                "updated_parsed": [
                    2024,
                    10,
                    30,
                    15,
                    2,
                    54,
                    2,
                    304,
                    0
                ],
                "published": "2024-10-30T15:02:54Z",
                "published_parsed": [
                    2024,
                    10,
                    30,
                    15,
                    2,
                    54,
                    2,
                    304,
                    0
                ],
                "title": "From Hype to Reality: The Road Ahead of Deploying DRL in 6G Networks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "From Hype to Reality: The Road Ahead of Deploying DRL in 6G Networks"
                },
                "summary": "The industrial landscape is rapidly evolving with the advent of 6G\napplications, which demand massive connectivity, high computational capacity,\nand ultra-low latency. These requirements present new challenges, which can no\nlonger be efficiently addressed by conventional strategies. In response, this\narticle underscores the transformative potential of Deep Reinforcement Learning\n(DRL) for 6G, highlighting its advantages over classic machine learning\nsolutions in meeting the demands of 6G. The necessity of DRL is further\nvalidated through three DRL applications in an end-to-end communication\nprocedure, including wireless access control, baseband function placement, and\nnetwork slicing coordination. However, DRL-based network management initiatives\nare far from mature. We extend the discussion to identify the challenges of\napplying DRL in practical networks and explore potential solutions along with\ntheir respective limitations. In the end, these insights are validated through\na practical DRL deployment in managing network slices on the testbed.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The industrial landscape is rapidly evolving with the advent of 6G\napplications, which demand massive connectivity, high computational capacity,\nand ultra-low latency. These requirements present new challenges, which can no\nlonger be efficiently addressed by conventional strategies. In response, this\narticle underscores the transformative potential of Deep Reinforcement Learning\n(DRL) for 6G, highlighting its advantages over classic machine learning\nsolutions in meeting the demands of 6G. The necessity of DRL is further\nvalidated through three DRL applications in an end-to-end communication\nprocedure, including wireless access control, baseband function placement, and\nnetwork slicing coordination. However, DRL-based network management initiatives\nare far from mature. We extend the discussion to identify the challenges of\napplying DRL in practical networks and explore potential solutions along with\ntheir respective limitations. In the end, these insights are validated through\na practical DRL deployment in managing network slices on the testbed."
                },
                "authors": [
                    {
                        "name": "Haiyuan Li"
                    },
                    {
                        "name": "Hari Madhukumar"
                    },
                    {
                        "name": "Peizheng Li"
                    },
                    {
                        "name": "Yiran Teng"
                    },
                    {
                        "name": "Shuangyi Yan"
                    },
                    {
                        "name": "Dimitra Simeonidou"
                    }
                ],
                "author_detail": {
                    "name": "Dimitra Simeonidou"
                },
                "author": "Dimitra Simeonidou",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.23086v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.23086v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.23082v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.23082v1",
                "updated": "2024-10-30T14:55:13Z",
                "updated_parsed": [
                    2024,
                    10,
                    30,
                    14,
                    55,
                    13,
                    2,
                    304,
                    0
                ],
                "published": "2024-10-30T14:55:13Z",
                "published_parsed": [
                    2024,
                    10,
                    30,
                    14,
                    55,
                    13,
                    2,
                    304,
                    0
                ],
                "title": "An Event-Based Digital Compute-In-Memory Accelerator with Flexible\n  Operand Resolution and Layer-Wise Weight/Output Stationarity",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "An Event-Based Digital Compute-In-Memory Accelerator with Flexible\n  Operand Resolution and Layer-Wise Weight/Output Stationarity"
                },
                "summary": "Compute-in-memory (CIM) accelerators for spiking neural networks (SNNs) are\npromising solutions to enable $\\mu$s-level inference latency and ultra-low\nenergy in edge vision applications. Yet, their current lack of flexibility at\nboth the circuit and system levels prevents their deployment in a wide range of\nreal-life scenarios. In this work, we propose a novel digital CIM macro that\nsupports arbitrary operand resolution and shape, with a unified CIM storage for\nweights and membrane potentials. These circuit-level techniques enable a hybrid\nweight- and output-stationary dataflow at the system level to maximize operand\nreuse, thereby minimizing costly on- and off-chip data movements during the SNN\nexecution. Measurement results of a fabricated FlexSpIM prototype in 40-nm CMOS\ndemonstrate a 2$\\times$ increase in bit-normalized energy efficiency compared\nto prior fixed-precision digital CIM-SNNs, while providing resolution\nreconfiguration with bitwise granularity. Our approach can save up to 90%\nenergy in large-scale systems, while reaching a state-of-the-art classification\naccuracy of 95.8% on the IBM DVS gesture dataset.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Compute-in-memory (CIM) accelerators for spiking neural networks (SNNs) are\npromising solutions to enable $\\mu$s-level inference latency and ultra-low\nenergy in edge vision applications. Yet, their current lack of flexibility at\nboth the circuit and system levels prevents their deployment in a wide range of\nreal-life scenarios. In this work, we propose a novel digital CIM macro that\nsupports arbitrary operand resolution and shape, with a unified CIM storage for\nweights and membrane potentials. These circuit-level techniques enable a hybrid\nweight- and output-stationary dataflow at the system level to maximize operand\nreuse, thereby minimizing costly on- and off-chip data movements during the SNN\nexecution. Measurement results of a fabricated FlexSpIM prototype in 40-nm CMOS\ndemonstrate a 2$\\times$ increase in bit-normalized energy efficiency compared\nto prior fixed-precision digital CIM-SNNs, while providing resolution\nreconfiguration with bitwise granularity. Our approach can save up to 90%\nenergy in large-scale systems, while reaching a state-of-the-art classification\naccuracy of 95.8% on the IBM DVS gesture dataset."
                },
                "authors": [
                    {
                        "name": "Nicolas Chauvaux"
                    },
                    {
                        "name": "Adrian Kneip"
                    },
                    {
                        "name": "Christoph Posch"
                    },
                    {
                        "name": "Kofi Makinwa"
                    },
                    {
                        "name": "Charlotte Frenkel"
                    }
                ],
                "author_detail": {
                    "name": "Charlotte Frenkel"
                },
                "author": "Charlotte Frenkel",
                "arxiv_comment": "5 pages, 7 figures, submitted to IEEE ISCAS 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.23082v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.23082v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "B.2.0; B.3.0; B.6.0; B.7.0; C.3",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.13147v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.13147v4",
                "updated": "2024-10-30T14:54:25Z",
                "updated_parsed": [
                    2024,
                    10,
                    30,
                    14,
                    54,
                    25,
                    2,
                    304,
                    0
                ],
                "published": "2024-10-17T02:04:57Z",
                "published_parsed": [
                    2024,
                    10,
                    17,
                    2,
                    4,
                    57,
                    3,
                    291,
                    0
                ],
                "title": "Utilizing Large Language Models in an iterative paradigm with Domain\n  feedback for Zero-shot Molecule optimization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Utilizing Large Language Models in an iterative paradigm with Domain\n  feedback for Zero-shot Molecule optimization"
                },
                "summary": "Molecule optimization is a critical task in drug discovery to optimize\ndesired properties of a given molecule through chemical modification. Despite\nLarge Language Models (LLMs) holding the potential to efficiently simulate this\ntask by using natural language to direct the optimization, straightforwardly\nutilizing shows limited performance. In this work, we facilitate utilizing LLMs\nin an iterative paradigm by proposing a simple yet highly effective domain\nfeedback provider, namely $\\text{Re}^3$DF. In detail, $\\text{Re}^3$DF harnesses\nan external toolkit, RDKit, to handle the molecule hallucination, if the\nmodified molecule is chemically invalid. Otherwise, its desired properties are\ncomputed and compared to the original one, establishing reliable domain\nfeedback with correct direction and distance towards the objective, followed by\na retrieved example, to explicitly guide the LLM to refine the modified\nmolecule. We conduct experiments across both single- and multi-property\nobjectives with 2 thresholds, where $\\text{Re}^3$DF shows significant\nimprovements. Particularly, for 20 single-property objectives, $\\text{Re}^3$DF\nenhances Hit ratio by 16.95% and 20.76% under loose and strict thresholds,\nrespectively. For 32 multi-property objectives, $\\text{Re}^3$DF enhances Hit\nratio by 6.04% and 5.25%.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Molecule optimization is a critical task in drug discovery to optimize\ndesired properties of a given molecule through chemical modification. Despite\nLarge Language Models (LLMs) holding the potential to efficiently simulate this\ntask by using natural language to direct the optimization, straightforwardly\nutilizing shows limited performance. In this work, we facilitate utilizing LLMs\nin an iterative paradigm by proposing a simple yet highly effective domain\nfeedback provider, namely $\\text{Re}^3$DF. In detail, $\\text{Re}^3$DF harnesses\nan external toolkit, RDKit, to handle the molecule hallucination, if the\nmodified molecule is chemically invalid. Otherwise, its desired properties are\ncomputed and compared to the original one, establishing reliable domain\nfeedback with correct direction and distance towards the objective, followed by\na retrieved example, to explicitly guide the LLM to refine the modified\nmolecule. We conduct experiments across both single- and multi-property\nobjectives with 2 thresholds, where $\\text{Re}^3$DF shows significant\nimprovements. Particularly, for 20 single-property objectives, $\\text{Re}^3$DF\nenhances Hit ratio by 16.95% and 20.76% under loose and strict thresholds,\nrespectively. For 32 multi-property objectives, $\\text{Re}^3$DF enhances Hit\nratio by 6.04% and 5.25%."
                },
                "authors": [
                    {
                        "name": "Khiem Le"
                    },
                    {
                        "name": "Nitesh V. Chawla"
                    }
                ],
                "author_detail": {
                    "name": "Nitesh V. Chawla"
                },
                "author": "Nitesh V. Chawla",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.13147v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.13147v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.23079v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.23079v1",
                "updated": "2024-10-30T14:53:37Z",
                "updated_parsed": [
                    2024,
                    10,
                    30,
                    14,
                    53,
                    37,
                    2,
                    304,
                    0
                ],
                "published": "2024-10-30T14:53:37Z",
                "published_parsed": [
                    2024,
                    10,
                    30,
                    14,
                    53,
                    37,
                    2,
                    304,
                    0
                ],
                "title": "BUZZ: Beehive-structured Sparse KV Cache with Segmented Heavy Hitters\n  for Efficient LLM Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "BUZZ: Beehive-structured Sparse KV Cache with Segmented Heavy Hitters\n  for Efficient LLM Inference"
                },
                "summary": "Large language models (LLMs) are essential in natural language processing but\noften struggle with inference speed and computational efficiency, limiting\nreal-time deployment. The key-value (KV) cache mechanism reduces computational\noverhead in transformer models, but challenges in maintaining contextual\nunderstanding remain. In this paper, we propose BUZZ, a novel KV caching\nalgorithm that leverages structured contextual information to minimize cache\nmemory usage while enhancing inference speed. BUZZ employs a beehive-structured\nsparse cache, incorporating a sliding window to capture recent information and\ndynamically segmenting historical tokens into chunks to prioritize important\ntokens in local neighborhoods. We evaluate BUZZ on four real-world datasets:\nCNN/Daily Mail, XSUM, Wikitext, and 10-QA. Our results demonstrate that BUZZ\n(1) reduces cache memory usage by $\\textbf{2.5}\\times$ in LLM inference while\nmaintaining over 99% accuracy in long-text summarization, and (2) surpasses\nstate-of-the-art performance in multi-document question answering by\n$\\textbf{7.69%}$ under the same memory limit, where full cache methods\nencounter out-of-memory issues. Additionally, BUZZ achieves significant\ninference speedup with a $\\log{n}$ time complexity. The code is available at\nhttps://github.com/JunqiZhao888/buzz-llm.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) are essential in natural language processing but\noften struggle with inference speed and computational efficiency, limiting\nreal-time deployment. The key-value (KV) cache mechanism reduces computational\noverhead in transformer models, but challenges in maintaining contextual\nunderstanding remain. In this paper, we propose BUZZ, a novel KV caching\nalgorithm that leverages structured contextual information to minimize cache\nmemory usage while enhancing inference speed. BUZZ employs a beehive-structured\nsparse cache, incorporating a sliding window to capture recent information and\ndynamically segmenting historical tokens into chunks to prioritize important\ntokens in local neighborhoods. We evaluate BUZZ on four real-world datasets:\nCNN/Daily Mail, XSUM, Wikitext, and 10-QA. Our results demonstrate that BUZZ\n(1) reduces cache memory usage by $\\textbf{2.5}\\times$ in LLM inference while\nmaintaining over 99% accuracy in long-text summarization, and (2) surpasses\nstate-of-the-art performance in multi-document question answering by\n$\\textbf{7.69%}$ under the same memory limit, where full cache methods\nencounter out-of-memory issues. Additionally, BUZZ achieves significant\ninference speedup with a $\\log{n}$ time complexity. The code is available at\nhttps://github.com/JunqiZhao888/buzz-llm."
                },
                "authors": [
                    {
                        "name": "Junqi Zhao"
                    },
                    {
                        "name": "Zhijin Fang"
                    },
                    {
                        "name": "Shu Li"
                    },
                    {
                        "name": "Shaohui Yang"
                    },
                    {
                        "name": "Shichao He"
                    }
                ],
                "author_detail": {
                    "name": "Shichao He"
                },
                "author": "Shichao He",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.23079v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.23079v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.02490v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.02490v2",
                "updated": "2024-10-30T14:53:22Z",
                "updated_parsed": [
                    2024,
                    10,
                    30,
                    14,
                    53,
                    22,
                    2,
                    304,
                    0
                ],
                "published": "2024-07-02T17:59:56Z",
                "published_parsed": [
                    2024,
                    7,
                    2,
                    17,
                    59,
                    56,
                    1,
                    184,
                    0
                ],
                "title": "MInference 1.0: Accelerating Pre-filling for Long-Context LLMs via\n  Dynamic Sparse Attention",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MInference 1.0: Accelerating Pre-filling for Long-Context LLMs via\n  Dynamic Sparse Attention"
                },
                "summary": "The computational challenges of Large Language Model (LLM) inference remain a\nsignificant barrier to their widespread deployment, especially as prompt\nlengths continue to increase. Due to the quadratic complexity of the attention\ncomputation, it takes 30 minutes for an 8B LLM to process a prompt of 1M tokens\n(i.e., the pre-filling stage) on a single A100 GPU. Existing methods for\nspeeding up prefilling often fail to maintain acceptable accuracy or efficiency\nwhen applied to long-context LLMs. To address this gap, we introduce MInference\n(Milliontokens Inference), a sparse calculation method designed to accelerate\npre-filling of long-sequence processing. Specifically, we identify three unique\npatterns in long-context attention matrices-the A-shape, Vertical-Slash, and\nBlock-Sparsethat can be leveraged for efficient sparse computation on GPUs. We\ndetermine the optimal pattern for each attention head offline and dynamically\nbuild sparse indices based on the assigned pattern during inference. With the\npattern and sparse indices, we perform efficient sparse attention calculations\nvia our optimized GPU kernels to significantly reduce the latency in the\npre-filling stage of long-context LLMs. Our proposed technique can be directly\napplied to existing LLMs without any modifications to the pre-training setup or\nadditional fine-tuning. By evaluating on a wide range of downstream tasks,\nincluding InfiniteBench, RULER, PG-19, and Needle In A Haystack, and models\nincluding LLaMA-3-1M, GLM4-1M, Yi-200K, Phi-3-128K, and Qwen2-128K, we\ndemonstrate that MInference effectively reduces inference latency by up to 10x\nfor pre-filling on an A100, while maintaining accuracy. Our code is available\nat https://aka.ms/MInference.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The computational challenges of Large Language Model (LLM) inference remain a\nsignificant barrier to their widespread deployment, especially as prompt\nlengths continue to increase. Due to the quadratic complexity of the attention\ncomputation, it takes 30 minutes for an 8B LLM to process a prompt of 1M tokens\n(i.e., the pre-filling stage) on a single A100 GPU. Existing methods for\nspeeding up prefilling often fail to maintain acceptable accuracy or efficiency\nwhen applied to long-context LLMs. To address this gap, we introduce MInference\n(Milliontokens Inference), a sparse calculation method designed to accelerate\npre-filling of long-sequence processing. Specifically, we identify three unique\npatterns in long-context attention matrices-the A-shape, Vertical-Slash, and\nBlock-Sparsethat can be leveraged for efficient sparse computation on GPUs. We\ndetermine the optimal pattern for each attention head offline and dynamically\nbuild sparse indices based on the assigned pattern during inference. With the\npattern and sparse indices, we perform efficient sparse attention calculations\nvia our optimized GPU kernels to significantly reduce the latency in the\npre-filling stage of long-context LLMs. Our proposed technique can be directly\napplied to existing LLMs without any modifications to the pre-training setup or\nadditional fine-tuning. By evaluating on a wide range of downstream tasks,\nincluding InfiniteBench, RULER, PG-19, and Needle In A Haystack, and models\nincluding LLaMA-3-1M, GLM4-1M, Yi-200K, Phi-3-128K, and Qwen2-128K, we\ndemonstrate that MInference effectively reduces inference latency by up to 10x\nfor pre-filling on an A100, while maintaining accuracy. Our code is available\nat https://aka.ms/MInference."
                },
                "authors": [
                    {
                        "name": "Huiqiang Jiang"
                    },
                    {
                        "name": "Yucheng Li"
                    },
                    {
                        "name": "Chengruidong Zhang"
                    },
                    {
                        "name": "Qianhui Wu"
                    },
                    {
                        "name": "Xufang Luo"
                    },
                    {
                        "name": "Surin Ahn"
                    },
                    {
                        "name": "Zhenhua Han"
                    },
                    {
                        "name": "Amir H. Abdi"
                    },
                    {
                        "name": "Dongsheng Li"
                    },
                    {
                        "name": "Chin-Yew Lin"
                    },
                    {
                        "name": "Yuqing Yang"
                    },
                    {
                        "name": "Lili Qiu"
                    }
                ],
                "author_detail": {
                    "name": "Lili Qiu"
                },
                "author": "Lili Qiu",
                "arxiv_comment": "Accepted at NeurIPS 2024 (Spotlight)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.02490v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.02490v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.07869v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.07869v2",
                "updated": "2024-10-30T14:49:49Z",
                "updated_parsed": [
                    2024,
                    10,
                    30,
                    14,
                    49,
                    49,
                    2,
                    304,
                    0
                ],
                "published": "2024-10-10T12:41:19Z",
                "published_parsed": [
                    2024,
                    10,
                    10,
                    12,
                    41,
                    19,
                    3,
                    284,
                    0
                ],
                "title": "Benchmarking Agentic Workflow Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Benchmarking Agentic Workflow Generation"
                },
                "summary": "Large Language Models (LLMs), with their exceptional ability to handle a wide\nrange of tasks, have driven significant advancements in tackling reasoning and\nplanning tasks, wherein decomposing complex problems into executable workflows\nis a crucial step in this process. Existing workflow evaluation frameworks\neither focus solely on holistic performance or suffer from limitations such as\nrestricted scenario coverage, simplistic workflow structures, and lax\nevaluation standards. To this end, we introduce WorFBench, a unified workflow\ngeneration benchmark with multi-faceted scenarios and intricate graph workflow\nstructures. Additionally, we present WorFEval, a systemic evaluation protocol\nutilizing subsequence and subgraph matching algorithms to accurately quantify\nthe LLM agent's workflow generation capabilities. Through comprehensive\nevaluations across different types of LLMs, we discover distinct gaps between\nthe sequence planning capabilities and graph planning capabilities of LLM\nagents, with even GPT-4 exhibiting a gap of around 15%. We also train two\nopen-source models and evaluate their generalization abilities on held-out\ntasks. Furthermore, we observe that the generated workflows can enhance\ndownstream tasks, enabling them to achieve superior performance with less time\nduring inference. Code and dataset are available at\nhttps://github.com/zjunlp/WorFBench.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs), with their exceptional ability to handle a wide\nrange of tasks, have driven significant advancements in tackling reasoning and\nplanning tasks, wherein decomposing complex problems into executable workflows\nis a crucial step in this process. Existing workflow evaluation frameworks\neither focus solely on holistic performance or suffer from limitations such as\nrestricted scenario coverage, simplistic workflow structures, and lax\nevaluation standards. To this end, we introduce WorFBench, a unified workflow\ngeneration benchmark with multi-faceted scenarios and intricate graph workflow\nstructures. Additionally, we present WorFEval, a systemic evaluation protocol\nutilizing subsequence and subgraph matching algorithms to accurately quantify\nthe LLM agent's workflow generation capabilities. Through comprehensive\nevaluations across different types of LLMs, we discover distinct gaps between\nthe sequence planning capabilities and graph planning capabilities of LLM\nagents, with even GPT-4 exhibiting a gap of around 15%. We also train two\nopen-source models and evaluate their generalization abilities on held-out\ntasks. Furthermore, we observe that the generated workflows can enhance\ndownstream tasks, enabling them to achieve superior performance with less time\nduring inference. Code and dataset are available at\nhttps://github.com/zjunlp/WorFBench."
                },
                "authors": [
                    {
                        "name": "Shuofei Qiao"
                    },
                    {
                        "name": "Runnan Fang"
                    },
                    {
                        "name": "Zhisong Qiu"
                    },
                    {
                        "name": "Xiaobin Wang"
                    },
                    {
                        "name": "Ningyu Zhang"
                    },
                    {
                        "name": "Yong Jiang"
                    },
                    {
                        "name": "Pengjun Xie"
                    },
                    {
                        "name": "Fei Huang"
                    },
                    {
                        "name": "Huajun Chen"
                    }
                ],
                "author_detail": {
                    "name": "Huajun Chen"
                },
                "author": "Huajun Chen",
                "arxiv_comment": "Work in progress (v2), update OpenAI o1 and Claude-3.5 results on\n  WorFBench",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.07869v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.07869v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.23074v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.23074v1",
                "updated": "2024-10-30T14:46:43Z",
                "updated_parsed": [
                    2024,
                    10,
                    30,
                    14,
                    46,
                    43,
                    2,
                    304,
                    0
                ],
                "published": "2024-10-30T14:46:43Z",
                "published_parsed": [
                    2024,
                    10,
                    30,
                    14,
                    46,
                    43,
                    2,
                    304,
                    0
                ],
                "title": "Multi-Programming Language Sandbox for LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-Programming Language Sandbox for LLMs"
                },
                "summary": "We introduce MPLSandbox, an out-of-the-box multi-programming language sandbox\ndesigned to provide unified and comprehensive feedback from compiler and\nanalysis tools for Large Language Models (LLMs). It can automatically identify\nthe programming language of the code, compiling and executing it within an\nisolated sub-sandbox to ensure safety and stability. In addition, MPLSandbox\nalso integrates both traditional and LLM-based code analysis tools, providing a\ncomprehensive analysis of generated code. MPLSandbox can be effortlessly\nintegrated into the training and deployment of LLMs to improve the quality and\ncorrectness of their generated code. It also helps researchers streamline their\nworkflows for various LLM-based code-related tasks, reducing the development\ncost. To validate the effectiveness of MPLSandbox, we integrate it into\ntraining and deployment approaches, and also employ it to optimize workflows\nfor a wide range of real-world code-related tasks. Our goal is to enhance\nresearcher productivity on LLM-based code-related tasks by simplifying and\nautomating workflows through delegation to MPLSandbox.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce MPLSandbox, an out-of-the-box multi-programming language sandbox\ndesigned to provide unified and comprehensive feedback from compiler and\nanalysis tools for Large Language Models (LLMs). It can automatically identify\nthe programming language of the code, compiling and executing it within an\nisolated sub-sandbox to ensure safety and stability. In addition, MPLSandbox\nalso integrates both traditional and LLM-based code analysis tools, providing a\ncomprehensive analysis of generated code. MPLSandbox can be effortlessly\nintegrated into the training and deployment of LLMs to improve the quality and\ncorrectness of their generated code. It also helps researchers streamline their\nworkflows for various LLM-based code-related tasks, reducing the development\ncost. To validate the effectiveness of MPLSandbox, we integrate it into\ntraining and deployment approaches, and also employ it to optimize workflows\nfor a wide range of real-world code-related tasks. Our goal is to enhance\nresearcher productivity on LLM-based code-related tasks by simplifying and\nautomating workflows through delegation to MPLSandbox."
                },
                "authors": [
                    {
                        "name": "Shihan Dou"
                    },
                    {
                        "name": "Jiazheng Zhang"
                    },
                    {
                        "name": "Jianxiang Zang"
                    },
                    {
                        "name": "Yunbo Tao"
                    },
                    {
                        "name": "Haoxiang Jia"
                    },
                    {
                        "name": "Shichun Liu"
                    },
                    {
                        "name": "Yuming Yang"
                    },
                    {
                        "name": "Shenxi Wu"
                    },
                    {
                        "name": "Shaoqing Zhang"
                    },
                    {
                        "name": "Muling Wu"
                    },
                    {
                        "name": "Changze Lv"
                    },
                    {
                        "name": "Limao Xiong"
                    },
                    {
                        "name": "Wenyu Zhan"
                    },
                    {
                        "name": "Lin Zhang"
                    },
                    {
                        "name": "Rongxiang Weng"
                    },
                    {
                        "name": "Jingang Wang"
                    },
                    {
                        "name": "Xunliang Cai"
                    },
                    {
                        "name": "Yueming Wu"
                    },
                    {
                        "name": "Ming Wen"
                    },
                    {
                        "name": "Rui Zheng"
                    },
                    {
                        "name": "Tao Ji"
                    },
                    {
                        "name": "Yixin Cao"
                    },
                    {
                        "name": "Tao Gui"
                    },
                    {
                        "name": "Xipeng Qiu"
                    },
                    {
                        "name": "Qi Zhang"
                    },
                    {
                        "name": "Xuanjing Huang"
                    }
                ],
                "author_detail": {
                    "name": "Xuanjing Huang"
                },
                "author": "Xuanjing Huang",
                "arxiv_comment": "25 pages, 14 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.23074v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.23074v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.23069v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.23069v1",
                "updated": "2024-10-30T14:43:33Z",
                "updated_parsed": [
                    2024,
                    10,
                    30,
                    14,
                    43,
                    33,
                    2,
                    304,
                    0
                ],
                "published": "2024-10-30T14:43:33Z",
                "published_parsed": [
                    2024,
                    10,
                    30,
                    14,
                    43,
                    33,
                    2,
                    304,
                    0
                ],
                "title": "LLMs Integration in Software Engineering Team Projects: Roles, Impact,\n  and a Pedagogical Design Space for AI Tools in Computing Education",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLMs Integration in Software Engineering Team Projects: Roles, Impact,\n  and a Pedagogical Design Space for AI Tools in Computing Education"
                },
                "summary": "This work takes a pedagogical lens to explore the implications of generative\nAI (GenAI) models and tools, such as ChatGPT and GitHub Copilot, in a\nsemester-long 2nd-year undergraduate Software Engineering Team Project.\nQualitative findings from survey (39 students) and interviews (eight students)\nprovide insights into the students' views on the impact of GenAI use on their\ncoding experience, learning, and self-efficacy. Our results address a\nparticular gap in understanding the role and implications of GenAI on teamwork,\nteam-efficacy, and team dynamics. The analysis of the learning aspects is\ndistinguished by the application of learning and pedagogy informed lenses to\ndiscuss the data. We propose a preliminary design space for GenAI-based\nprogramming learning tools highlighting the importance of considering the roles\nthat GenAI can play during the learning process, the varying support-ability\npatterns that can be applied to each role, and the importance of supporting\ntransparency in GenAI for team members and students in addition to educators.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This work takes a pedagogical lens to explore the implications of generative\nAI (GenAI) models and tools, such as ChatGPT and GitHub Copilot, in a\nsemester-long 2nd-year undergraduate Software Engineering Team Project.\nQualitative findings from survey (39 students) and interviews (eight students)\nprovide insights into the students' views on the impact of GenAI use on their\ncoding experience, learning, and self-efficacy. Our results address a\nparticular gap in understanding the role and implications of GenAI on teamwork,\nteam-efficacy, and team dynamics. The analysis of the learning aspects is\ndistinguished by the application of learning and pedagogy informed lenses to\ndiscuss the data. We propose a preliminary design space for GenAI-based\nprogramming learning tools highlighting the importance of considering the roles\nthat GenAI can play during the learning process, the varying support-ability\npatterns that can be applied to each role, and the importance of supporting\ntransparency in GenAI for team members and students in addition to educators."
                },
                "authors": [
                    {
                        "name": "Ahmed Kharrufa"
                    },
                    {
                        "name": "Sami Alghamdi"
                    },
                    {
                        "name": "Abeer Aziz"
                    },
                    {
                        "name": "Christopher Bull"
                    }
                ],
                "author_detail": {
                    "name": "Christopher Bull"
                },
                "author": "Christopher Bull",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.23069v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.23069v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.11387v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.11387v3",
                "updated": "2024-10-30T14:31:25Z",
                "updated_parsed": [
                    2024,
                    10,
                    30,
                    14,
                    31,
                    25,
                    2,
                    304,
                    0
                ],
                "published": "2024-10-15T08:24:05Z",
                "published_parsed": [
                    2024,
                    10,
                    15,
                    8,
                    24,
                    5,
                    1,
                    289,
                    0
                ],
                "title": "LLM2Swarm: Robot Swarms that Responsively Reason, Plan, and Collaborate\n  through LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM2Swarm: Robot Swarms that Responsively Reason, Plan, and Collaborate\n  through LLMs"
                },
                "summary": "Robot swarms are composed of many simple robots that communicate and\ncollaborate to fulfill complex tasks. Robot controllers usually need to be\nspecified by experts on a case-by-case basis via programming code. This process\nis time-consuming, prone to errors, and unable to take into account all\nsituations that may be encountered during deployment. On the other hand, recent\nLarge Language Models (LLMs) have demonstrated reasoning and planning\ncapabilities, introduced new ways to interact with and program machines, and\nincorporate both domain-specific and commonsense knowledge. Hence, we propose\nto address the aforementioned challenges by integrating LLMs with robot swarms\nand show the potential in proofs of concept (showcases). For this integration,\nwe explore two approaches. The first approach is 'indirect integration,' where\nLLMs are used to synthesize and validate the robot controllers. This approach\nmay reduce development time and human error before deployment. Moreover, during\ndeployment, it could be used for on-the-fly creation of new robot behaviors.\nThe second approach is 'direct integration,' where each robot locally executes\na separate LLM instance during deployment for robot-robot collaboration and\nhuman-swarm interaction. These local LLM instances enable each robot to reason,\nplan, and collaborate using natural language, as demonstrated in our showcases\nwhere the robots are able to detect a variety of anomalies, without prior\ninformation about the nature of these anomalies. To enable further research on\nour mainly conceptual contribution, we release the software and videos for our\nLLM2Swarm system: https://github.com/Pold87/LLM2Swarm.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Robot swarms are composed of many simple robots that communicate and\ncollaborate to fulfill complex tasks. Robot controllers usually need to be\nspecified by experts on a case-by-case basis via programming code. This process\nis time-consuming, prone to errors, and unable to take into account all\nsituations that may be encountered during deployment. On the other hand, recent\nLarge Language Models (LLMs) have demonstrated reasoning and planning\ncapabilities, introduced new ways to interact with and program machines, and\nincorporate both domain-specific and commonsense knowledge. Hence, we propose\nto address the aforementioned challenges by integrating LLMs with robot swarms\nand show the potential in proofs of concept (showcases). For this integration,\nwe explore two approaches. The first approach is 'indirect integration,' where\nLLMs are used to synthesize and validate the robot controllers. This approach\nmay reduce development time and human error before deployment. Moreover, during\ndeployment, it could be used for on-the-fly creation of new robot behaviors.\nThe second approach is 'direct integration,' where each robot locally executes\na separate LLM instance during deployment for robot-robot collaboration and\nhuman-swarm interaction. These local LLM instances enable each robot to reason,\nplan, and collaborate using natural language, as demonstrated in our showcases\nwhere the robots are able to detect a variety of anomalies, without prior\ninformation about the nature of these anomalies. To enable further research on\nour mainly conceptual contribution, we release the software and videos for our\nLLM2Swarm system: https://github.com/Pold87/LLM2Swarm."
                },
                "authors": [
                    {
                        "name": "Volker Strobel"
                    },
                    {
                        "name": "Marco Dorigo"
                    },
                    {
                        "name": "Mario Fritz"
                    }
                ],
                "author_detail": {
                    "name": "Mario Fritz"
                },
                "author": "Mario Fritz",
                "arxiv_comment": "Accepted at NeurIPS 2024 Workshop on Open-World Agents. Code:\n  https://github.com/Pold87/LLM2Swarm/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.11387v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.11387v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.20441v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.20441v4",
                "updated": "2024-10-30T14:29:37Z",
                "updated_parsed": [
                    2024,
                    10,
                    30,
                    14,
                    29,
                    37,
                    2,
                    304,
                    0
                ],
                "published": "2024-05-30T19:35:06Z",
                "published_parsed": [
                    2024,
                    5,
                    30,
                    19,
                    35,
                    6,
                    3,
                    151,
                    0
                ],
                "title": "SECURE: Benchmarking Large Language Models for Cybersecurity",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SECURE: Benchmarking Large Language Models for Cybersecurity"
                },
                "summary": "Large Language Models (LLMs) have demonstrated potential in cybersecurity\napplications but have also caused lower confidence due to problems like\nhallucinations and a lack of truthfulness. Existing benchmarks provide general\nevaluations but do not sufficiently address the practical and applied aspects\nof LLM performance in cybersecurity-specific tasks. To address this gap, we\nintroduce the SECURE (Security Extraction, Understanding \\& Reasoning\nEvaluation), a benchmark designed to assess LLMs performance in realistic\ncybersecurity scenarios. SECURE includes six datasets focussed on the\nIndustrial Control System sector to evaluate knowledge extraction,\nunderstanding, and reasoning based on industry-standard sources. Our study\nevaluates seven state-of-the-art models on these tasks, providing insights into\ntheir strengths and weaknesses in cybersecurity contexts, and offer\nrecommendations for improving LLMs reliability as cyber advisory tools.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have demonstrated potential in cybersecurity\napplications but have also caused lower confidence due to problems like\nhallucinations and a lack of truthfulness. Existing benchmarks provide general\nevaluations but do not sufficiently address the practical and applied aspects\nof LLM performance in cybersecurity-specific tasks. To address this gap, we\nintroduce the SECURE (Security Extraction, Understanding \\& Reasoning\nEvaluation), a benchmark designed to assess LLMs performance in realistic\ncybersecurity scenarios. SECURE includes six datasets focussed on the\nIndustrial Control System sector to evaluate knowledge extraction,\nunderstanding, and reasoning based on industry-standard sources. Our study\nevaluates seven state-of-the-art models on these tasks, providing insights into\ntheir strengths and weaknesses in cybersecurity contexts, and offer\nrecommendations for improving LLMs reliability as cyber advisory tools."
                },
                "authors": [
                    {
                        "name": "Dipkamal Bhusal"
                    },
                    {
                        "name": "Md Tanvirul Alam"
                    },
                    {
                        "name": "Le Nguyen"
                    },
                    {
                        "name": "Ashim Mahara"
                    },
                    {
                        "name": "Zachary Lightcap"
                    },
                    {
                        "name": "Rodney Frazier"
                    },
                    {
                        "name": "Romy Fieblinger"
                    },
                    {
                        "name": "Grace Long Torales"
                    },
                    {
                        "name": "Benjamin A. Blakely"
                    },
                    {
                        "name": "Nidhi Rastogi"
                    }
                ],
                "author_detail": {
                    "name": "Nidhi Rastogi"
                },
                "author": "Nidhi Rastogi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.20441v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.20441v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.23054v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.23054v1",
                "updated": "2024-10-30T14:21:33Z",
                "updated_parsed": [
                    2024,
                    10,
                    30,
                    14,
                    21,
                    33,
                    2,
                    304,
                    0
                ],
                "published": "2024-10-30T14:21:33Z",
                "published_parsed": [
                    2024,
                    10,
                    30,
                    14,
                    21,
                    33,
                    2,
                    304,
                    0
                ],
                "title": "Controlling Language and Diffusion Models by Transporting Activations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Controlling Language and Diffusion Models by Transporting Activations"
                },
                "summary": "The increasing capabilities of large generative models and their ever more\nwidespread deployment have raised concerns about their reliability, safety, and\npotential misuse. To address these issues, recent works have proposed to\ncontrol model generation by steering model activations in order to effectively\ninduce or prevent the emergence of concepts or behaviors in the generated\noutput. In this paper we introduce Activation Transport (AcT), a general\nframework to steer activations guided by optimal transport theory that\ngeneralizes many previous activation-steering works. AcT is modality-agnostic\nand provides fine-grained control over the model behavior with negligible\ncomputational overhead, while minimally impacting model abilities. We\nexperimentally show the effectiveness and versatility of our approach by\naddressing key challenges in large language models (LLMs) and text-to-image\ndiffusion models (T2Is). For LLMs, we show that AcT can effectively mitigate\ntoxicity, induce arbitrary concepts, and increase their truthfulness. In T2Is,\nwe show how AcT enables fine-grained style control and concept negation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The increasing capabilities of large generative models and their ever more\nwidespread deployment have raised concerns about their reliability, safety, and\npotential misuse. To address these issues, recent works have proposed to\ncontrol model generation by steering model activations in order to effectively\ninduce or prevent the emergence of concepts or behaviors in the generated\noutput. In this paper we introduce Activation Transport (AcT), a general\nframework to steer activations guided by optimal transport theory that\ngeneralizes many previous activation-steering works. AcT is modality-agnostic\nand provides fine-grained control over the model behavior with negligible\ncomputational overhead, while minimally impacting model abilities. We\nexperimentally show the effectiveness and versatility of our approach by\naddressing key challenges in large language models (LLMs) and text-to-image\ndiffusion models (T2Is). For LLMs, we show that AcT can effectively mitigate\ntoxicity, induce arbitrary concepts, and increase their truthfulness. In T2Is,\nwe show how AcT enables fine-grained style control and concept negation."
                },
                "authors": [
                    {
                        "name": "Pau Rodriguez"
                    },
                    {
                        "name": "Arno Blaas"
                    },
                    {
                        "name": "Michal Klein"
                    },
                    {
                        "name": "Luca Zappella"
                    },
                    {
                        "name": "Nicholas Apostoloff"
                    },
                    {
                        "name": "Marco Cuturi"
                    },
                    {
                        "name": "Xavier Suau"
                    }
                ],
                "author_detail": {
                    "name": "Xavier Suau"
                },
                "author": "Xavier Suau",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.23054v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.23054v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "68T07, 49Q22",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.6; I.2.7; I.4.8",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.15383v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.15383v2",
                "updated": "2024-10-30T14:19:57Z",
                "updated_parsed": [
                    2024,
                    10,
                    30,
                    14,
                    19,
                    57,
                    2,
                    304,
                    0
                ],
                "published": "2024-05-24T09:31:26Z",
                "published_parsed": [
                    2024,
                    5,
                    24,
                    9,
                    31,
                    26,
                    4,
                    145,
                    0
                ],
                "title": "Generating Code World Models with Large Language Models Guided by Monte\n  Carlo Tree Search",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Generating Code World Models with Large Language Models Guided by Monte\n  Carlo Tree Search"
                },
                "summary": "In this work we consider Code World Models, world models generated by a Large\nLanguage Model (LLM) in the form of Python code for model-based Reinforcement\nLearning (RL). Calling code instead of LLMs for planning has potential to be\nmore precise, reliable, interpretable, and extremely efficient. However,\nwriting appropriate Code World Models requires the ability to understand\ncomplex instructions, to generate exact code with non-trivial logic and to\nself-debug a long program with feedback from unit tests and environment\ntrajectories. To address these challenges, we propose Generate, Improve and Fix\nwith Monte Carlo Tree Search (GIF-MCTS), a new code generation strategy for\nLLMs. To test our approach in an offline RL setting, we introduce the Code\nWorld Models Benchmark (CWMB), a suite of program synthesis and planning tasks\ncomprised of 18 diverse RL environments paired with corresponding textual\ndescriptions and curated trajectories. GIF-MCTS surpasses all baselines on the\nCWMB and two other benchmarks, and we show that the Code World Models\nsynthesized with it can be successfully used for planning, resulting in\nmodel-based RL agents with greatly improved sample efficiency and inference\nspeed.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this work we consider Code World Models, world models generated by a Large\nLanguage Model (LLM) in the form of Python code for model-based Reinforcement\nLearning (RL). Calling code instead of LLMs for planning has potential to be\nmore precise, reliable, interpretable, and extremely efficient. However,\nwriting appropriate Code World Models requires the ability to understand\ncomplex instructions, to generate exact code with non-trivial logic and to\nself-debug a long program with feedback from unit tests and environment\ntrajectories. To address these challenges, we propose Generate, Improve and Fix\nwith Monte Carlo Tree Search (GIF-MCTS), a new code generation strategy for\nLLMs. To test our approach in an offline RL setting, we introduce the Code\nWorld Models Benchmark (CWMB), a suite of program synthesis and planning tasks\ncomprised of 18 diverse RL environments paired with corresponding textual\ndescriptions and curated trajectories. GIF-MCTS surpasses all baselines on the\nCWMB and two other benchmarks, and we show that the Code World Models\nsynthesized with it can be successfully used for planning, resulting in\nmodel-based RL agents with greatly improved sample efficiency and inference\nspeed."
                },
                "authors": [
                    {
                        "name": "Nicola Dainese"
                    },
                    {
                        "name": "Matteo Merler"
                    },
                    {
                        "name": "Minttu Alakuijala"
                    },
                    {
                        "name": "Pekka Marttinen"
                    }
                ],
                "author_detail": {
                    "name": "Pekka Marttinen"
                },
                "author": "Pekka Marttinen",
                "arxiv_comment": "Accepted at NeurIPS 2024, Main Track. 11 pages in main text, 40 pages\n  including references and supplementary materials. 2 figures and 3 tables in\n  the main text, 9 figures and 12 tables when including the supplementary\n  materials. Website at https://sites.google.com/view/code-world-models/home",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.15383v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.15383v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.23049v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.23049v1",
                "updated": "2024-10-30T14:16:59Z",
                "updated_parsed": [
                    2024,
                    10,
                    30,
                    14,
                    16,
                    59,
                    2,
                    304,
                    0
                ],
                "published": "2024-10-30T14:16:59Z",
                "published_parsed": [
                    2024,
                    10,
                    30,
                    14,
                    16,
                    59,
                    2,
                    304,
                    0
                ],
                "title": "TumblerBots: Tumbling Robotic sensors for Minimally-invasive Benthic\n  Monitoring",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TumblerBots: Tumbling Robotic sensors for Minimally-invasive Benthic\n  Monitoring"
                },
                "summary": "Robotic systems show significant promise for water environmental sensing\napplications such as water quality monitoring, pollution mapping and\nbiodiversity data collection.\n  Conventional deployment methods often disrupt fragile ecosystems, preventing\ndepiction of the undisturbed environmental condition. In response to this\nchallenge, we propose a novel framework utilizing a lightweight tumbler system\nequipped with a sensing unit, deployed via a drone. This design minimizes\ndisruption to the water habitat by maintaining a slow descent. The sensing unit\nis detached once on the water surface, enabling precise and non-invasive data\ncollection from the benthic zone.\n  The tumbler is designed to be lightweight and compact, enabling deployment\nvia a drone. The sensing pod, which detaches from the tumbler and descends to\nthe bottom of the water body, is equipped with temperature and pressure\nsensors, as well as a buoyancy system. The later, activated upon task\ncompletion, utilizes a silicon membrane inflated via a chemical reaction. The\nreaction generates a pressure of 70 kPa, causing the silicon membrane to expand\nby 30\\%, which exceeds the 5.7\\% volume increase required for positive\nbuoyancy. The tumblers, made from ecofriendly materials to minimize\nenvironmental impact when lost during the mission, were tested for their\ngliding ratio and descent rate. They exhibit a low descent rate, in the range\nof 0.8 to 2.5 meters per seconds, which minimizes disturbance to the ecosystem\nupon water landing. Additionally, the system demonstrated robustness in\nmoderate to strong wind conditions during outdoor tests, validating the overall\nframework.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Robotic systems show significant promise for water environmental sensing\napplications such as water quality monitoring, pollution mapping and\nbiodiversity data collection.\n  Conventional deployment methods often disrupt fragile ecosystems, preventing\ndepiction of the undisturbed environmental condition. In response to this\nchallenge, we propose a novel framework utilizing a lightweight tumbler system\nequipped with a sensing unit, deployed via a drone. This design minimizes\ndisruption to the water habitat by maintaining a slow descent. The sensing unit\nis detached once on the water surface, enabling precise and non-invasive data\ncollection from the benthic zone.\n  The tumbler is designed to be lightweight and compact, enabling deployment\nvia a drone. The sensing pod, which detaches from the tumbler and descends to\nthe bottom of the water body, is equipped with temperature and pressure\nsensors, as well as a buoyancy system. The later, activated upon task\ncompletion, utilizes a silicon membrane inflated via a chemical reaction. The\nreaction generates a pressure of 70 kPa, causing the silicon membrane to expand\nby 30\\%, which exceeds the 5.7\\% volume increase required for positive\nbuoyancy. The tumblers, made from ecofriendly materials to minimize\nenvironmental impact when lost during the mission, were tested for their\ngliding ratio and descent rate. They exhibit a low descent rate, in the range\nof 0.8 to 2.5 meters per seconds, which minimizes disturbance to the ecosystem\nupon water landing. Additionally, the system demonstrated robustness in\nmoderate to strong wind conditions during outdoor tests, validating the overall\nframework."
                },
                "authors": [
                    {
                        "name": "L. Romanello"
                    },
                    {
                        "name": "A. Teboul"
                    },
                    {
                        "name": "F. Wiesemuller"
                    },
                    {
                        "name": "P. H. Nguyen"
                    },
                    {
                        "name": "M. Kovac"
                    },
                    {
                        "name": "S. F. Armanini"
                    }
                ],
                "author_detail": {
                    "name": "S. F. Armanini"
                },
                "author": "S. F. Armanini",
                "arxiv_comment": "Submitted to IEEE Robosoft 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.23049v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.23049v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.13073v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.13073v3",
                "updated": "2024-10-30T14:15:49Z",
                "updated_parsed": [
                    2024,
                    10,
                    30,
                    14,
                    15,
                    49,
                    2,
                    304,
                    0
                ],
                "published": "2024-10-16T22:25:15Z",
                "published_parsed": [
                    2024,
                    10,
                    16,
                    22,
                    25,
                    15,
                    2,
                    290,
                    0
                ],
                "title": "PromptExp: Multi-granularity Prompt Explanation of Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PromptExp: Multi-granularity Prompt Explanation of Large Language Models"
                },
                "summary": "Large Language Models excel in tasks like natural language understanding and\ntext generation. Prompt engineering plays a critical role in leveraging LLM\neffectively. However, LLMs black-box nature hinders its interpretability and\neffective prompting engineering. A wide range of model explanation approaches\nhave been developed for deep learning models, However, these local explanations\nare designed for single-output tasks like classification and regression,and\ncannot be directly applied to LLMs, which generate sequences of tokens. Recent\nefforts in LLM explanation focus on natural language explanations, but they are\nprone to hallucinations and inaccuracies. To address this, we introduce\nPromptExp , a framework for multi-granularity prompt explanations by\naggregating token-level insights. PromptExp introduces two token-level\nexplanation approaches: 1. an aggregation-based approach combining local\nexplanation techniques, and 2. a perturbation-based approach with novel\ntechniques to evaluate token masking impact. PromptExp supports both white-box\nand black-box explanations and extends explanations to higher granularity\nlevels, enabling flexible analysis. We evaluate PromptExp in case studies such\nas sentiment analysis, showing the perturbation-based approach performs best\nusing semantic similarity to assess perturbation impact. Furthermore, we\nconducted a user study to confirm PromptExp's accuracy and practical value, and\ndemonstrate its potential to enhance LLM interpretability.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models excel in tasks like natural language understanding and\ntext generation. Prompt engineering plays a critical role in leveraging LLM\neffectively. However, LLMs black-box nature hinders its interpretability and\neffective prompting engineering. A wide range of model explanation approaches\nhave been developed for deep learning models, However, these local explanations\nare designed for single-output tasks like classification and regression,and\ncannot be directly applied to LLMs, which generate sequences of tokens. Recent\nefforts in LLM explanation focus on natural language explanations, but they are\nprone to hallucinations and inaccuracies. To address this, we introduce\nPromptExp , a framework for multi-granularity prompt explanations by\naggregating token-level insights. PromptExp introduces two token-level\nexplanation approaches: 1. an aggregation-based approach combining local\nexplanation techniques, and 2. a perturbation-based approach with novel\ntechniques to evaluate token masking impact. PromptExp supports both white-box\nand black-box explanations and extends explanations to higher granularity\nlevels, enabling flexible analysis. We evaluate PromptExp in case studies such\nas sentiment analysis, showing the perturbation-based approach performs best\nusing semantic similarity to assess perturbation impact. Furthermore, we\nconducted a user study to confirm PromptExp's accuracy and practical value, and\ndemonstrate its potential to enhance LLM interpretability."
                },
                "authors": [
                    {
                        "name": "Ximing Dong"
                    },
                    {
                        "name": "Shaowei Wang"
                    },
                    {
                        "name": "Dayi Lin"
                    },
                    {
                        "name": "Gopi Krishnan Rajbahadur"
                    },
                    {
                        "name": "Boquan Zhou"
                    },
                    {
                        "name": "Shichao Liu"
                    },
                    {
                        "name": "Ahmed E. Hassan"
                    }
                ],
                "author_detail": {
                    "name": "Ahmed E. Hassan"
                },
                "author": "Ahmed E. Hassan",
                "arxiv_comment": "11 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.13073v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.13073v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.23041v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.23041v1",
                "updated": "2024-10-30T14:08:50Z",
                "updated_parsed": [
                    2024,
                    10,
                    30,
                    14,
                    8,
                    50,
                    2,
                    304,
                    0
                ],
                "published": "2024-10-30T14:08:50Z",
                "published_parsed": [
                    2024,
                    10,
                    30,
                    14,
                    8,
                    50,
                    2,
                    304,
                    0
                ],
                "title": "Emotional RAG: Enhancing Role-Playing Agents through Emotional Retrieval",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Emotional RAG: Enhancing Role-Playing Agents through Emotional Retrieval"
                },
                "summary": "As LLMs exhibit a high degree of human-like capability, increasing attention\nhas been paid to role-playing research areas in which responses generated by\nLLMs are expected to mimic human replies. This has promoted the exploration of\nrole-playing agents in various applications, such as chatbots that can engage\nin natural conversations with users and virtual assistants that can provide\npersonalized support and guidance. The crucial factor in the role-playing task\nis the effective utilization of character memory, which stores characters'\nprofiles, experiences, and historical dialogues. Retrieval Augmented Generation\n(RAG) technology is used to access the related memory to enhance the response\ngeneration of role-playing agents. Most existing studies retrieve related\ninformation based on the semantic similarity of memory to maintain characters'\npersonalized traits, and few attempts have been made to incorporate the\nemotional factor in the retrieval argument generation (RAG) of LLMs. Inspired\nby the Mood-Dependent Memory theory, which indicates that people recall an\nevent better if they somehow reinstate during recall the original emotion they\nexperienced during learning, we propose a novel emotion-aware memory retrieval\nframework, termed Emotional RAG, which recalls the related memory with\nconsideration of emotional state in role-playing agents. Specifically, we\ndesign two kinds of retrieval strategies, i.e., combination strategy and\nsequential strategy, to incorporate both memory semantic and emotional states\nduring the retrieval process. Extensive experiments on three representative\nrole-playing datasets demonstrate that our Emotional RAG framework outperforms\nthe method without considering the emotional factor in maintaining the\npersonalities of role-playing agents. This provides evidence to further\nreinforce the Mood-Dependent Memory theory in psychology.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As LLMs exhibit a high degree of human-like capability, increasing attention\nhas been paid to role-playing research areas in which responses generated by\nLLMs are expected to mimic human replies. This has promoted the exploration of\nrole-playing agents in various applications, such as chatbots that can engage\nin natural conversations with users and virtual assistants that can provide\npersonalized support and guidance. The crucial factor in the role-playing task\nis the effective utilization of character memory, which stores characters'\nprofiles, experiences, and historical dialogues. Retrieval Augmented Generation\n(RAG) technology is used to access the related memory to enhance the response\ngeneration of role-playing agents. Most existing studies retrieve related\ninformation based on the semantic similarity of memory to maintain characters'\npersonalized traits, and few attempts have been made to incorporate the\nemotional factor in the retrieval argument generation (RAG) of LLMs. Inspired\nby the Mood-Dependent Memory theory, which indicates that people recall an\nevent better if they somehow reinstate during recall the original emotion they\nexperienced during learning, we propose a novel emotion-aware memory retrieval\nframework, termed Emotional RAG, which recalls the related memory with\nconsideration of emotional state in role-playing agents. Specifically, we\ndesign two kinds of retrieval strategies, i.e., combination strategy and\nsequential strategy, to incorporate both memory semantic and emotional states\nduring the retrieval process. Extensive experiments on three representative\nrole-playing datasets demonstrate that our Emotional RAG framework outperforms\nthe method without considering the emotional factor in maintaining the\npersonalities of role-playing agents. This provides evidence to further\nreinforce the Mood-Dependent Memory theory in psychology."
                },
                "authors": [
                    {
                        "name": "Le Huang"
                    },
                    {
                        "name": "Hengzhi Lan"
                    },
                    {
                        "name": "Zijun Sun"
                    },
                    {
                        "name": "Chuan Shi"
                    },
                    {
                        "name": "Ting Bai"
                    }
                ],
                "author_detail": {
                    "name": "Ting Bai"
                },
                "author": "Ting Bai",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.23041v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.23041v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2304.00910v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2304.00910v4",
                "updated": "2024-10-30T14:08:46Z",
                "updated_parsed": [
                    2024,
                    10,
                    30,
                    14,
                    8,
                    46,
                    2,
                    304,
                    0
                ],
                "published": "2023-04-03T11:57:10Z",
                "published_parsed": [
                    2023,
                    4,
                    3,
                    11,
                    57,
                    10,
                    0,
                    93,
                    0
                ],
                "title": "Integrating One-Shot View Planning with a Single Next-Best View via\n  Long-Tail Multiview Sampling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Integrating One-Shot View Planning with a Single Next-Best View via\n  Long-Tail Multiview Sampling"
                },
                "summary": "Existing view planning systems either adopt an iterative paradigm using\nnext-best views (NBV) or a one-shot pipeline relying on the set-covering\nview-planning (SCVP) network. However, neither of these methods can\nconcurrently guarantee both high-quality and high-efficiency reconstruction of\n3D unknown objects. To tackle this challenge, we introduce a crucial\nhypothesis: with the availability of more information about the unknown object,\nthe prediction quality of the SCVP network improves. There are two ways to\nprovide extra information: (1) leveraging perception data obtained from NBVs,\nand (2) training on an expanded dataset of multiview inputs. In this work, we\nintroduce a novel combined pipeline that incorporates a single NBV before\nactivating the proposed multiview-activated (MA-)SCVP network. The MA-SCVP is\ntrained on a multiview dataset generated by our long-tail sampling method,\nwhich addresses the issue of unbalanced multiview inputs and enhances the\nnetwork performance. Extensive simulated experiments substantiate that our\nsystem demonstrates a significant surface coverage increase and a substantial\n45% reduction in movement cost compared to state-of-the-art systems. Real-world\nexperiments justify the capability of our system for generalization and\ndeployment.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Existing view planning systems either adopt an iterative paradigm using\nnext-best views (NBV) or a one-shot pipeline relying on the set-covering\nview-planning (SCVP) network. However, neither of these methods can\nconcurrently guarantee both high-quality and high-efficiency reconstruction of\n3D unknown objects. To tackle this challenge, we introduce a crucial\nhypothesis: with the availability of more information about the unknown object,\nthe prediction quality of the SCVP network improves. There are two ways to\nprovide extra information: (1) leveraging perception data obtained from NBVs,\nand (2) training on an expanded dataset of multiview inputs. In this work, we\nintroduce a novel combined pipeline that incorporates a single NBV before\nactivating the proposed multiview-activated (MA-)SCVP network. The MA-SCVP is\ntrained on a multiview dataset generated by our long-tail sampling method,\nwhich addresses the issue of unbalanced multiview inputs and enhances the\nnetwork performance. Extensive simulated experiments substantiate that our\nsystem demonstrates a significant surface coverage increase and a substantial\n45% reduction in movement cost compared to state-of-the-art systems. Real-world\nexperiments justify the capability of our system for generalization and\ndeployment."
                },
                "authors": [
                    {
                        "name": "Sicong Pan"
                    },
                    {
                        "name": "Hao Hu"
                    },
                    {
                        "name": "Hui Wei"
                    },
                    {
                        "name": "Nils Dengler"
                    },
                    {
                        "name": "Tobias Zaenker"
                    },
                    {
                        "name": "Murad Dawood"
                    },
                    {
                        "name": "Maren Bennewitz"
                    }
                ],
                "author_detail": {
                    "name": "Maren Bennewitz"
                },
                "author": "Maren Bennewitz",
                "arxiv_comment": "Accepted to IEEE Transactions on Robotics. Full appendices version",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2304.00910v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2304.00910v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.14516v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.14516v4",
                "updated": "2024-10-30T14:06:12Z",
                "updated_parsed": [
                    2024,
                    10,
                    30,
                    14,
                    6,
                    12,
                    2,
                    304,
                    0
                ],
                "published": "2024-10-18T14:55:14Z",
                "published_parsed": [
                    2024,
                    10,
                    18,
                    14,
                    55,
                    14,
                    4,
                    292,
                    0
                ],
                "title": "Do LLMs \"know\" internally when they follow instructions?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Do LLMs \"know\" internally when they follow instructions?"
                },
                "summary": "Instruction-following is crucial for building AI agents with large language\nmodels (LLMs), as these models must adhere strictly to user-provided\nconstraints and guidelines. However, LLMs often fail to follow even simple and\nclear instructions. To improve instruction-following behavior and prevent\nundesirable outputs, a deeper understanding of how LLMs' internal states relate\nto these outcomes is required. Our analysis of LLM internal states reveal a\ndimension in the input embedding space linked to successful\ninstruction-following. We demonstrate that modifying representations along this\ndimension improves instruction-following success rates compared to random\nchanges, without compromising response quality. Further investigation reveals\nthat this dimension is more closely related to the phrasing of prompts rather\nthan the inherent difficulty of the task or instructions. This discovery also\nsuggests explanations for why LLMs sometimes fail to follow clear instructions\nand why prompt engineering is often effective, even when the content remains\nlargely unchanged. This work provides insight into the internal workings of\nLLMs' instruction-following, paving the way for reliable LLM agents.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Instruction-following is crucial for building AI agents with large language\nmodels (LLMs), as these models must adhere strictly to user-provided\nconstraints and guidelines. However, LLMs often fail to follow even simple and\nclear instructions. To improve instruction-following behavior and prevent\nundesirable outputs, a deeper understanding of how LLMs' internal states relate\nto these outcomes is required. Our analysis of LLM internal states reveal a\ndimension in the input embedding space linked to successful\ninstruction-following. We demonstrate that modifying representations along this\ndimension improves instruction-following success rates compared to random\nchanges, without compromising response quality. Further investigation reveals\nthat this dimension is more closely related to the phrasing of prompts rather\nthan the inherent difficulty of the task or instructions. This discovery also\nsuggests explanations for why LLMs sometimes fail to follow clear instructions\nand why prompt engineering is often effective, even when the content remains\nlargely unchanged. This work provides insight into the internal workings of\nLLMs' instruction-following, paving the way for reliable LLM agents."
                },
                "authors": [
                    {
                        "name": "Juyeon Heo"
                    },
                    {
                        "name": "Christina Heinze-Deml"
                    },
                    {
                        "name": "Oussama Elachqar"
                    },
                    {
                        "name": "Shirley Ren"
                    },
                    {
                        "name": "Udhay Nallasamy"
                    },
                    {
                        "name": "Andy Miller"
                    },
                    {
                        "name": "Kwan Ho Ryan Chan"
                    },
                    {
                        "name": "Jaya Narain"
                    }
                ],
                "author_detail": {
                    "name": "Jaya Narain"
                },
                "author": "Jaya Narain",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.14516v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.14516v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.23031v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.23031v1",
                "updated": "2024-10-30T14:01:31Z",
                "updated_parsed": [
                    2024,
                    10,
                    30,
                    14,
                    1,
                    31,
                    2,
                    304,
                    0
                ],
                "published": "2024-10-30T14:01:31Z",
                "published_parsed": [
                    2024,
                    10,
                    30,
                    14,
                    1,
                    31,
                    2,
                    304,
                    0
                ],
                "title": "Offline Reinforcement Learning and Sequence Modeling for Downlink Link\n  Adaptation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Offline Reinforcement Learning and Sequence Modeling for Downlink Link\n  Adaptation"
                },
                "summary": "Contemporary radio access networks employ link adaption (LA) algorithms to\noptimize the modulation and coding schemes to adapt to the prevailing\npropagation conditions and are near-optimal in terms of the achieved spectral\nefficiency. LA is a challenging task in the presence of mobility, fast fading,\nand imperfect channel quality information and limited knowledge of the receiver\ncharacteristics at the transmitter, which render model-based LA algorithms\ncomplex and suboptimal. Model-based LA is especially difficult as connected\nuser equipment devices become increasingly heterogeneous in terms of receiver\ncapabilities, antenna configurations and hardware characteristics. Recognizing\nthese difficulties, previous works have proposed reinforcement learning (RL)\nfor LA, which faces deployment difficulties due to their potential negative\nimpacts on live performance. To address this challenge, this paper considers\noffline RL to learn LA policies from data acquired in live networks with\nminimal or no intrusive effects on the network operation. We propose three LA\ndesigns based on batch-constrained deep Q-learning, conservative Q-learning,\nand decision transformers, showing that offline RL algorithms can achieve\nperformance of state-of-the-art online RL methods when data is collected with a\nproper behavioral policy.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Contemporary radio access networks employ link adaption (LA) algorithms to\noptimize the modulation and coding schemes to adapt to the prevailing\npropagation conditions and are near-optimal in terms of the achieved spectral\nefficiency. LA is a challenging task in the presence of mobility, fast fading,\nand imperfect channel quality information and limited knowledge of the receiver\ncharacteristics at the transmitter, which render model-based LA algorithms\ncomplex and suboptimal. Model-based LA is especially difficult as connected\nuser equipment devices become increasingly heterogeneous in terms of receiver\ncapabilities, antenna configurations and hardware characteristics. Recognizing\nthese difficulties, previous works have proposed reinforcement learning (RL)\nfor LA, which faces deployment difficulties due to their potential negative\nimpacts on live performance. To address this challenge, this paper considers\noffline RL to learn LA policies from data acquired in live networks with\nminimal or no intrusive effects on the network operation. We propose three LA\ndesigns based on batch-constrained deep Q-learning, conservative Q-learning,\nand decision transformers, showing that offline RL algorithms can achieve\nperformance of state-of-the-art online RL methods when data is collected with a\nproper behavioral policy."
                },
                "authors": [
                    {
                        "name": "Samuele Peri"
                    },
                    {
                        "name": "Alessio Russo"
                    },
                    {
                        "name": "Gabor Fodor"
                    },
                    {
                        "name": "Pablo Soldati"
                    }
                ],
                "author_detail": {
                    "name": "Pablo Soldati"
                },
                "author": "Pablo Soldati",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.23031v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.23031v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.13765v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.13765v2",
                "updated": "2024-10-30T13:55:03Z",
                "updated_parsed": [
                    2024,
                    10,
                    30,
                    13,
                    55,
                    3,
                    2,
                    304,
                    0
                ],
                "published": "2024-05-22T15:50:40Z",
                "published_parsed": [
                    2024,
                    5,
                    22,
                    15,
                    50,
                    40,
                    2,
                    143,
                    0
                ],
                "title": "On the stability of gradient descent with second order dynamics for\n  time-varying cost functions",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "On the stability of gradient descent with second order dynamics for\n  time-varying cost functions"
                },
                "summary": "Gradient based optimization algorithms deployed in Machine Learning (ML)\napplications are often analyzed and compared by their convergence rates or\nregret bounds. While these rates and bounds convey valuable information they\ndon't always directly translate to stability guarantees. Stability and similar\nconcepts, like robustness, will become ever more important as we move towards\ndeploying models in real-time and safety critical systems. In this work we\nbuild upon the results in Gaudio et al. 2021 and Moreu & Annaswamy 2022 for\ngradient descent with second order dynamics when applied to explicitly time\nvarying cost functions and provide more general stability guarantees. These\nmore general results can aid in the design and certification of these\noptimization schemes so as to help ensure safe and reliable deployment for\nreal-time learning applications. We also hope that the techniques provided here\nwill stimulate and cross-fertilize the analysis that occurs on the same\nalgorithms from the online learning and stochastic optimization communities.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Gradient based optimization algorithms deployed in Machine Learning (ML)\napplications are often analyzed and compared by their convergence rates or\nregret bounds. While these rates and bounds convey valuable information they\ndon't always directly translate to stability guarantees. Stability and similar\nconcepts, like robustness, will become ever more important as we move towards\ndeploying models in real-time and safety critical systems. In this work we\nbuild upon the results in Gaudio et al. 2021 and Moreu & Annaswamy 2022 for\ngradient descent with second order dynamics when applied to explicitly time\nvarying cost functions and provide more general stability guarantees. These\nmore general results can aid in the design and certification of these\noptimization schemes so as to help ensure safe and reliable deployment for\nreal-time learning applications. We also hope that the techniques provided here\nwill stimulate and cross-fertilize the analysis that occurs on the same\nalgorithms from the online learning and stochastic optimization communities."
                },
                "authors": [
                    {
                        "name": "Travis E. Gibson"
                    },
                    {
                        "name": "Sawal Acharya"
                    },
                    {
                        "name": "Anjali Parashar"
                    },
                    {
                        "name": "Joseph E. Gaudio"
                    },
                    {
                        "name": "Anurdha M. Annaswamy"
                    }
                ],
                "author_detail": {
                    "name": "Anurdha M. Annaswamy"
                },
                "author": "Anurdha M. Annaswamy",
                "arxiv_comment": "20 pages, 3 figures v2 changes: -experiments added -new section was\n  added \"What does acceleration look like with time varying cost functions?\n  -title changed",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.13765v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.13765v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.OC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.23022v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.23022v1",
                "updated": "2024-10-30T13:52:43Z",
                "updated_parsed": [
                    2024,
                    10,
                    30,
                    13,
                    52,
                    43,
                    2,
                    304,
                    0
                ],
                "published": "2024-10-30T13:52:43Z",
                "published_parsed": [
                    2024,
                    10,
                    30,
                    13,
                    52,
                    43,
                    2,
                    304,
                    0
                ],
                "title": "Online Intrinsic Rewards for Decision Making Agents from Large Language\n  Model Feedback",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Online Intrinsic Rewards for Decision Making Agents from Large Language\n  Model Feedback"
                },
                "summary": "Automatically synthesizing dense rewards from natural language descriptions\nis a promising paradigm in reinforcement learning (RL), with applications to\nsparse reward problems, open-ended exploration, and hierarchical skill design.\nRecent works have made promising steps by exploiting the prior knowledge of\nlarge language models (LLMs). However, these approaches suffer from important\nlimitations: they are either not scalable to problems requiring billions of\nenvironment samples; or are limited to reward functions expressible by compact\ncode, which may require source code and have difficulty capturing nuanced\nsemantics; or require a diverse offline dataset, which may not exist or be\nimpossible to collect. In this work, we address these limitations through a\ncombination of algorithmic and systems-level contributions. We propose ONI, a\ndistributed architecture that simultaneously learns an RL policy and an\nintrinsic reward function using LLM feedback. Our approach annotates the\nagent's collected experience via an asynchronous LLM server, which is then\ndistilled into an intrinsic reward model. We explore a range of algorithmic\nchoices for reward modeling with varying complexity, including hashing,\nclassification, and ranking models. By studying their relative tradeoffs, we\nshed light on questions regarding intrinsic reward design for sparse reward\nproblems. Our approach achieves state-of-the-art performance across a range of\nchallenging, sparse reward tasks from the NetHack Learning Environment in a\nsimple unified process, solely using the agent's gathered experience, without\nrequiring external datasets nor source code. We make our code available at\n\\url{URL} (coming soon).",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Automatically synthesizing dense rewards from natural language descriptions\nis a promising paradigm in reinforcement learning (RL), with applications to\nsparse reward problems, open-ended exploration, and hierarchical skill design.\nRecent works have made promising steps by exploiting the prior knowledge of\nlarge language models (LLMs). However, these approaches suffer from important\nlimitations: they are either not scalable to problems requiring billions of\nenvironment samples; or are limited to reward functions expressible by compact\ncode, which may require source code and have difficulty capturing nuanced\nsemantics; or require a diverse offline dataset, which may not exist or be\nimpossible to collect. In this work, we address these limitations through a\ncombination of algorithmic and systems-level contributions. We propose ONI, a\ndistributed architecture that simultaneously learns an RL policy and an\nintrinsic reward function using LLM feedback. Our approach annotates the\nagent's collected experience via an asynchronous LLM server, which is then\ndistilled into an intrinsic reward model. We explore a range of algorithmic\nchoices for reward modeling with varying complexity, including hashing,\nclassification, and ranking models. By studying their relative tradeoffs, we\nshed light on questions regarding intrinsic reward design for sparse reward\nproblems. Our approach achieves state-of-the-art performance across a range of\nchallenging, sparse reward tasks from the NetHack Learning Environment in a\nsimple unified process, solely using the agent's gathered experience, without\nrequiring external datasets nor source code. We make our code available at\n\\url{URL} (coming soon)."
                },
                "authors": [
                    {
                        "name": "Qinqing Zheng"
                    },
                    {
                        "name": "Mikael Henaff"
                    },
                    {
                        "name": "Amy Zhang"
                    },
                    {
                        "name": "Aditya Grover"
                    },
                    {
                        "name": "Brandon Amos"
                    }
                ],
                "author_detail": {
                    "name": "Brandon Amos"
                },
                "author": "Brandon Amos",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.23022v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.23022v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2401.16727v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2401.16727v4",
                "updated": "2024-10-30T13:49:43Z",
                "updated_parsed": [
                    2024,
                    10,
                    30,
                    13,
                    49,
                    43,
                    2,
                    304,
                    0
                ],
                "published": "2024-01-30T03:51:44Z",
                "published_parsed": [
                    2024,
                    1,
                    30,
                    3,
                    51,
                    44,
                    1,
                    30,
                    0
                ],
                "title": "Recent Advances in Hate Speech Moderation: Multimodality and the Role of\n  Large Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent Advances in Hate Speech Moderation: Multimodality and the Role of\n  Large Models"
                },
                "summary": "In the evolving landscape of online communication, moderating hate speech\n(HS) presents an intricate challenge, compounded by the multimodal nature of\ndigital content. This comprehensive survey delves into the recent strides in HS\nmoderation, spotlighting the burgeoning role of large language models (LLMs)\nand large multimodal models (LMMs). Our exploration begins with a thorough\nanalysis of current literature, revealing the nuanced interplay between\ntextual, visual, and auditory elements in propagating HS. We uncover a notable\ntrend towards integrating these modalities, primarily due to the complexity and\nsubtlety with which HS is disseminated. A significant emphasis is placed on the\nadvances facilitated by LLMs and LMMs, which have begun to redefine the\nboundaries of detection and moderation capabilities. We identify existing gaps\nin research, particularly in the context of underrepresented languages and\ncultures, and the need for solutions to handle low-resource settings. The\nsurvey concludes with a forward-looking perspective, outlining potential\navenues for future research, including the exploration of novel AI\nmethodologies, the ethical governance of AI in moderation, and the development\nof more nuanced, context-aware systems. This comprehensive overview aims to\ncatalyze further research and foster a collaborative effort towards more\nsophisticated, responsible, and human-centric approaches to HS moderation in\nthe digital era. WARNING: This paper contains offensive examples.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In the evolving landscape of online communication, moderating hate speech\n(HS) presents an intricate challenge, compounded by the multimodal nature of\ndigital content. This comprehensive survey delves into the recent strides in HS\nmoderation, spotlighting the burgeoning role of large language models (LLMs)\nand large multimodal models (LMMs). Our exploration begins with a thorough\nanalysis of current literature, revealing the nuanced interplay between\ntextual, visual, and auditory elements in propagating HS. We uncover a notable\ntrend towards integrating these modalities, primarily due to the complexity and\nsubtlety with which HS is disseminated. A significant emphasis is placed on the\nadvances facilitated by LLMs and LMMs, which have begun to redefine the\nboundaries of detection and moderation capabilities. We identify existing gaps\nin research, particularly in the context of underrepresented languages and\ncultures, and the need for solutions to handle low-resource settings. The\nsurvey concludes with a forward-looking perspective, outlining potential\navenues for future research, including the exploration of novel AI\nmethodologies, the ethical governance of AI in moderation, and the development\nof more nuanced, context-aware systems. This comprehensive overview aims to\ncatalyze further research and foster a collaborative effort towards more\nsophisticated, responsible, and human-centric approaches to HS moderation in\nthe digital era. WARNING: This paper contains offensive examples."
                },
                "authors": [
                    {
                        "name": "Ming Shan Hee"
                    },
                    {
                        "name": "Shivam Sharma"
                    },
                    {
                        "name": "Rui Cao"
                    },
                    {
                        "name": "Palash Nandi"
                    },
                    {
                        "name": "Preslav Nakov"
                    },
                    {
                        "name": "Tanmoy Chakraborty"
                    },
                    {
                        "name": "Roy Ka-Wei Lee"
                    }
                ],
                "author_detail": {
                    "name": "Roy Ka-Wei Lee"
                },
                "author": "Roy Ka-Wei Lee",
                "arxiv_comment": "Accepted at EMNLP'24 (Findings)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2401.16727v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2401.16727v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.00552v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.00552v4",
                "updated": "2024-10-30T13:49:11Z",
                "updated_parsed": [
                    2024,
                    10,
                    30,
                    13,
                    49,
                    11,
                    2,
                    304,
                    0
                ],
                "published": "2024-05-01T14:50:58Z",
                "published_parsed": [
                    2024,
                    5,
                    1,
                    14,
                    50,
                    58,
                    2,
                    122,
                    0
                ],
                "title": "Long-Term Human Trajectory Prediction using 3D Dynamic Scene Graphs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Long-Term Human Trajectory Prediction using 3D Dynamic Scene Graphs"
                },
                "summary": "We present a novel approach for long-term human trajectory prediction in\nindoor human-centric environments, which is essential for long-horizon robot\nplanning in these environments. State-of-the-art human trajectory prediction\nmethods are limited by their focus on collision avoidance and short-term\nplanning, and their inability to model complex interactions of humans with the\nenvironment. In contrast, our approach overcomes these limitations by\npredicting sequences of human interactions with the environment and using this\ninformation to guide trajectory predictions over a horizon of up to 60s. We\nleverage Large Language Models (LLMs) to predict interactions with the\nenvironment by conditioning the LLM prediction on rich contextual information\nabout the scene. This information is given as a 3D Dynamic Scene Graph that\nencodes the geometry, semantics, and traversability of the environment into a\nhierarchical representation. We then ground these interaction sequences into\nmulti-modal spatio-temporal distributions over human positions using a\nprobabilistic approach based on continuous-time Markov Chains. To evaluate our\napproach, we introduce a new semi-synthetic dataset of long-term human\ntrajectories in complex indoor environments, which also includes annotations of\nhuman-object interactions. We show in thorough experimental evaluations that\nour approach achieves a 54% lower average negative log-likelihood and a 26.5%\nlower Best-of-20 displacement error compared to the best non-privileged (i.e.,\nevaluated in a zero-shot fashion on the dataset) baselines for a time horizon\nof 60s.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present a novel approach for long-term human trajectory prediction in\nindoor human-centric environments, which is essential for long-horizon robot\nplanning in these environments. State-of-the-art human trajectory prediction\nmethods are limited by their focus on collision avoidance and short-term\nplanning, and their inability to model complex interactions of humans with the\nenvironment. In contrast, our approach overcomes these limitations by\npredicting sequences of human interactions with the environment and using this\ninformation to guide trajectory predictions over a horizon of up to 60s. We\nleverage Large Language Models (LLMs) to predict interactions with the\nenvironment by conditioning the LLM prediction on rich contextual information\nabout the scene. This information is given as a 3D Dynamic Scene Graph that\nencodes the geometry, semantics, and traversability of the environment into a\nhierarchical representation. We then ground these interaction sequences into\nmulti-modal spatio-temporal distributions over human positions using a\nprobabilistic approach based on continuous-time Markov Chains. To evaluate our\napproach, we introduce a new semi-synthetic dataset of long-term human\ntrajectories in complex indoor environments, which also includes annotations of\nhuman-object interactions. We show in thorough experimental evaluations that\nour approach achieves a 54% lower average negative log-likelihood and a 26.5%\nlower Best-of-20 displacement error compared to the best non-privileged (i.e.,\nevaluated in a zero-shot fashion on the dataset) baselines for a time horizon\nof 60s."
                },
                "authors": [
                    {
                        "name": "Nicolas Gorlo"
                    },
                    {
                        "name": "Lukas Schmid"
                    },
                    {
                        "name": "Luca Carlone"
                    }
                ],
                "author_detail": {
                    "name": "Luca Carlone"
                },
                "author": "Luca Carlone",
                "arxiv_doi": "10.1109/LRA.2024.3482169",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1109/LRA.2024.3482169",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2405.00552v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.00552v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "8 pages, 6 figures. Accepted at IEEE Robotics and Automation Letters\n  (RA-L). Code released at: https://github.com/MIT-SPARK/LP2",
                "arxiv_journal_ref": "IEEE Robotics and Automation Letters, vol. 9, no. 12, pp.\n  10978-10985, Dec. 2024",
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.14515v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.14515v3",
                "updated": "2024-10-30T13:38:10Z",
                "updated_parsed": [
                    2024,
                    10,
                    30,
                    13,
                    38,
                    10,
                    2,
                    304,
                    0
                ],
                "published": "2024-06-20T17:26:01Z",
                "published_parsed": [
                    2024,
                    6,
                    20,
                    17,
                    26,
                    1,
                    3,
                    172,
                    0
                ],
                "title": "MMBench-Video: A Long-Form Multi-Shot Benchmark for Holistic Video\n  Understanding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MMBench-Video: A Long-Form Multi-Shot Benchmark for Holistic Video\n  Understanding"
                },
                "summary": "The advent of large vision-language models (LVLMs) has spurred research into\ntheir applications in multi-modal contexts, particularly in video\nunderstanding. Traditional VideoQA benchmarks, despite providing quantitative\nmetrics, often fail to encompass the full spectrum of video content and\ninadequately assess models' temporal comprehension. To address these\nlimitations, we introduce MMBench-Video, a quantitative benchmark designed to\nrigorously evaluate LVLMs' proficiency in video understanding. MMBench-Video\nincorporates lengthy videos from YouTube and employs free-form questions,\nmirroring practical use cases. The benchmark is meticulously crafted to probe\nthe models' temporal reasoning skills, with all questions human-annotated\naccording to a carefully constructed ability taxonomy. We employ GPT-4 for\nautomated assessment, demonstrating superior accuracy and robustness over\nearlier LLM-based evaluations. Utilizing MMBench-Video, we have conducted\ncomprehensive evaluations that include both proprietary and open-source LVLMs\nfor images and videos. MMBench-Video stands as a valuable resource for the\nresearch community, facilitating improved evaluation of LVLMs and catalyzing\nprogress in the field of video understanding. The evalutation code of\nMMBench-Video will be integrated into VLMEvalKit:\nhttps://github.com/open-compass/VLMEvalKit.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The advent of large vision-language models (LVLMs) has spurred research into\ntheir applications in multi-modal contexts, particularly in video\nunderstanding. Traditional VideoQA benchmarks, despite providing quantitative\nmetrics, often fail to encompass the full spectrum of video content and\ninadequately assess models' temporal comprehension. To address these\nlimitations, we introduce MMBench-Video, a quantitative benchmark designed to\nrigorously evaluate LVLMs' proficiency in video understanding. MMBench-Video\nincorporates lengthy videos from YouTube and employs free-form questions,\nmirroring practical use cases. The benchmark is meticulously crafted to probe\nthe models' temporal reasoning skills, with all questions human-annotated\naccording to a carefully constructed ability taxonomy. We employ GPT-4 for\nautomated assessment, demonstrating superior accuracy and robustness over\nearlier LLM-based evaluations. Utilizing MMBench-Video, we have conducted\ncomprehensive evaluations that include both proprietary and open-source LVLMs\nfor images and videos. MMBench-Video stands as a valuable resource for the\nresearch community, facilitating improved evaluation of LVLMs and catalyzing\nprogress in the field of video understanding. The evalutation code of\nMMBench-Video will be integrated into VLMEvalKit:\nhttps://github.com/open-compass/VLMEvalKit."
                },
                "authors": [
                    {
                        "name": "Xinyu Fang"
                    },
                    {
                        "name": "Kangrui Mao"
                    },
                    {
                        "name": "Haodong Duan"
                    },
                    {
                        "name": "Xiangyu Zhao"
                    },
                    {
                        "name": "Yining Li"
                    },
                    {
                        "name": "Dahua Lin"
                    },
                    {
                        "name": "Kai Chen"
                    }
                ],
                "author_detail": {
                    "name": "Kai Chen"
                },
                "author": "Kai Chen",
                "arxiv_comment": "Accepted in NeurIPS 2024 Datasets and Benchmarks Track",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.14515v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.14515v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.23000v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.23000v2",
                "updated": "2024-10-31T03:04:28Z",
                "updated_parsed": [
                    2024,
                    10,
                    31,
                    3,
                    4,
                    28,
                    3,
                    305,
                    0
                ],
                "published": "2024-10-30T13:29:36Z",
                "published_parsed": [
                    2024,
                    10,
                    30,
                    13,
                    29,
                    36,
                    2,
                    304,
                    0
                ],
                "title": "Long$^2$RAG: Evaluating Long-Context & Long-Form Retrieval-Augmented\n  Generation with Key Point Recall",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Long$^2$RAG: Evaluating Long-Context & Long-Form Retrieval-Augmented\n  Generation with Key Point Recall"
                },
                "summary": "Retrieval-augmented generation (RAG) is a promising approach to address the\nlimitations of fixed knowledge in large language models (LLMs). However,\ncurrent benchmarks for evaluating RAG systems suffer from two key deficiencies:\n(1) they fail to adequately measure LLMs' capability in handling long-context\nretrieval due to a lack of datasets that reflect the characteristics of\nretrieved documents, and (2) they lack a comprehensive evaluation method for\nassessing LLMs' ability to generate long-form responses that effectively\nexploits retrieved information. To address these shortcomings, we introduce the\nLong$^2$RAG benchmark and the Key Point Recall (KPR) metric. Long$^2$RAG\ncomprises 280 questions spanning 10 domains and across 8 question categories,\neach associated with 5 retrieved documents with an average length of 2,444\nwords. KPR evaluates the extent to which LLMs incorporate key points extracted\nfrom the retrieved documents into their generated responses, providing a more\nnuanced assessment of their ability to exploit retrieved information.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Retrieval-augmented generation (RAG) is a promising approach to address the\nlimitations of fixed knowledge in large language models (LLMs). However,\ncurrent benchmarks for evaluating RAG systems suffer from two key deficiencies:\n(1) they fail to adequately measure LLMs' capability in handling long-context\nretrieval due to a lack of datasets that reflect the characteristics of\nretrieved documents, and (2) they lack a comprehensive evaluation method for\nassessing LLMs' ability to generate long-form responses that effectively\nexploits retrieved information. To address these shortcomings, we introduce the\nLong$^2$RAG benchmark and the Key Point Recall (KPR) metric. Long$^2$RAG\ncomprises 280 questions spanning 10 domains and across 8 question categories,\neach associated with 5 retrieved documents with an average length of 2,444\nwords. KPR evaluates the extent to which LLMs incorporate key points extracted\nfrom the retrieved documents into their generated responses, providing a more\nnuanced assessment of their ability to exploit retrieved information."
                },
                "authors": [
                    {
                        "name": "Zehan Qi"
                    },
                    {
                        "name": "Rongwu Xu"
                    },
                    {
                        "name": "Zhijiang Guo"
                    },
                    {
                        "name": "Cunxiang Wang"
                    },
                    {
                        "name": "Hao Zhang"
                    },
                    {
                        "name": "Wei Xu"
                    }
                ],
                "author_detail": {
                    "name": "Wei Xu"
                },
                "author": "Wei Xu",
                "arxiv_comment": "Accepted to EMNLP'24 (Findings). Camera-ready version",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.23000v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.23000v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.22997v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.22997v1",
                "updated": "2024-10-30T13:22:55Z",
                "updated_parsed": [
                    2024,
                    10,
                    30,
                    13,
                    22,
                    55,
                    2,
                    304,
                    0
                ],
                "published": "2024-10-30T13:22:55Z",
                "published_parsed": [
                    2024,
                    10,
                    30,
                    13,
                    22,
                    55,
                    2,
                    304,
                    0
                ],
                "title": "A Comparison of Prompt Engineering Techniques for Task Planning and\n  Execution in Service Robotics",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Comparison of Prompt Engineering Techniques for Task Planning and\n  Execution in Service Robotics"
                },
                "summary": "Recent advances in LLM have been instrumental in autonomous robot control and\nhuman-robot interaction by leveraging their vast general knowledge and\ncapabilities to understand and reason across a wide range of tasks and\nscenarios. Previous works have investigated various prompt engineering\ntechniques for improving the performance of \\glspl{LLM} to accomplish tasks,\nwhile others have proposed methods that utilize LLMs to plan and execute tasks\nbased on the available functionalities of a given robot platform. In this work,\nwe consider both lines of research by comparing prompt engineering techniques\nand combinations thereof within the application of high-level task planning and\nexecution in service robotics. We define a diverse set of tasks and a simple\nset of functionalities in simulation, and measure task completion accuracy and\nexecution time for several state-of-the-art models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advances in LLM have been instrumental in autonomous robot control and\nhuman-robot interaction by leveraging their vast general knowledge and\ncapabilities to understand and reason across a wide range of tasks and\nscenarios. Previous works have investigated various prompt engineering\ntechniques for improving the performance of \\glspl{LLM} to accomplish tasks,\nwhile others have proposed methods that utilize LLMs to plan and execute tasks\nbased on the available functionalities of a given robot platform. In this work,\nwe consider both lines of research by comparing prompt engineering techniques\nand combinations thereof within the application of high-level task planning and\nexecution in service robotics. We define a diverse set of tasks and a simple\nset of functionalities in simulation, and measure task completion accuracy and\nexecution time for several state-of-the-art models."
                },
                "authors": [
                    {
                        "name": "Jonas Bode"
                    },
                    {
                        "name": "Bastian Pätzold"
                    },
                    {
                        "name": "Raphael Memmesheimer"
                    },
                    {
                        "name": "Sven Behnke"
                    }
                ],
                "author_detail": {
                    "name": "Sven Behnke"
                },
                "author": "Sven Behnke",
                "arxiv_comment": "6 pages, 3 figures, 2 tables, to be published in the 2024 IEEE-RAS\n  International Conference on Humanoid Robots, We make our code, including all\n  prompts, available at https://github.com/AIS-Bonn/Prompt_Engineering",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.22997v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.22997v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.22995v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.22995v1",
                "updated": "2024-10-30T13:19:44Z",
                "updated_parsed": [
                    2024,
                    10,
                    30,
                    13,
                    19,
                    44,
                    2,
                    304,
                    0
                ],
                "published": "2024-10-30T13:19:44Z",
                "published_parsed": [
                    2024,
                    10,
                    30,
                    13,
                    19,
                    44,
                    2,
                    304,
                    0
                ],
                "title": "VisAidMath: Benchmarking Visual-Aided Mathematical Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "VisAidMath: Benchmarking Visual-Aided Mathematical Reasoning"
                },
                "summary": "Although previous research on large language models (LLMs) and large\nmulti-modal models (LMMs) has systematically explored mathematical\nproblem-solving (MPS) within visual contexts, the analysis of how these models\nprocess visual information during problem-solving remains insufficient. To\naddress this gap, we present VisAidMath, a benchmark for evaluating the MPS\nprocess related to visual information. We follow a rigorous data curation\npipeline involving both automated processes and manual annotations to ensure\ndata quality and reliability. Consequently, this benchmark includes 1,200\nchallenging problems from various mathematical branches, vision-aid\nformulations, and difficulty levels, collected from diverse sources such as\ntextbooks, examination papers, and Olympiad problems. Based on the proposed\nbenchmark, we conduct comprehensive evaluations on ten mainstream LLMs and\nLMMs, highlighting deficiencies in the visual-aided reasoning process. For\nexample, GPT-4V only achieves 45.33% accuracy in the visual-aided reasoning\ntask, even with a drop of 2 points when provided with golden visual aids.\nIn-depth analysis reveals that the main cause of deficiencies lies in\nhallucination regarding the implicit visual reasoning process, shedding light\non future research directions in the visual-aided MPS process.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Although previous research on large language models (LLMs) and large\nmulti-modal models (LMMs) has systematically explored mathematical\nproblem-solving (MPS) within visual contexts, the analysis of how these models\nprocess visual information during problem-solving remains insufficient. To\naddress this gap, we present VisAidMath, a benchmark for evaluating the MPS\nprocess related to visual information. We follow a rigorous data curation\npipeline involving both automated processes and manual annotations to ensure\ndata quality and reliability. Consequently, this benchmark includes 1,200\nchallenging problems from various mathematical branches, vision-aid\nformulations, and difficulty levels, collected from diverse sources such as\ntextbooks, examination papers, and Olympiad problems. Based on the proposed\nbenchmark, we conduct comprehensive evaluations on ten mainstream LLMs and\nLMMs, highlighting deficiencies in the visual-aided reasoning process. For\nexample, GPT-4V only achieves 45.33% accuracy in the visual-aided reasoning\ntask, even with a drop of 2 points when provided with golden visual aids.\nIn-depth analysis reveals that the main cause of deficiencies lies in\nhallucination regarding the implicit visual reasoning process, shedding light\non future research directions in the visual-aided MPS process."
                },
                "authors": [
                    {
                        "name": "Jingkun Ma"
                    },
                    {
                        "name": "Runzhe Zhan"
                    },
                    {
                        "name": "Derek F. Wong"
                    },
                    {
                        "name": "Yang Li"
                    },
                    {
                        "name": "Di Sun"
                    },
                    {
                        "name": "Hou Pong Chan"
                    },
                    {
                        "name": "Lidia S. Chao"
                    }
                ],
                "author_detail": {
                    "name": "Lidia S. Chao"
                },
                "author": "Lidia S. Chao",
                "arxiv_comment": "58 pages, 28 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.22995v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.22995v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.14398v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.14398v3",
                "updated": "2024-10-30T12:56:44Z",
                "updated_parsed": [
                    2024,
                    10,
                    30,
                    12,
                    56,
                    44,
                    2,
                    304,
                    0
                ],
                "published": "2024-05-23T10:15:29Z",
                "published_parsed": [
                    2024,
                    5,
                    23,
                    10,
                    15,
                    29,
                    3,
                    144,
                    0
                ],
                "title": "SpGesture: Source-Free Domain-adaptive sEMG-based Gesture Recognition\n  with Jaccard Attentive Spiking Neural Network",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SpGesture: Source-Free Domain-adaptive sEMG-based Gesture Recognition\n  with Jaccard Attentive Spiking Neural Network"
                },
                "summary": "Surface electromyography (sEMG) based gesture recognition offers a natural\nand intuitive interaction modality for wearable devices. Despite significant\nadvancements in sEMG-based gesture-recognition models, existing methods often\nsuffer from high computational latency and increased energy consumption.\nAdditionally, the inherent instability of sEMG signals, combined with their\nsensitivity to distribution shifts in real-world settings, compromises model\nrobustness. To tackle these challenges, we propose a novel SpGesture framework\nbased on Spiking Neural Networks, which possesses several unique merits\ncompared with existing methods: (1) Robustness: By utilizing membrane potential\nas a memory list, we pioneer the introduction of Source-Free Domain Adaptation\ninto SNN for the first time. This enables SpGesture to mitigate the accuracy\ndegradation caused by distribution shifts. (2) High Accuracy: With a novel\nSpiking Jaccard Attention, SpGesture enhances the SNNs' ability to represent\nsEMG features, leading to a notable rise in system accuracy. To validate\nSpGesture's performance, we collected a new sEMG gesture dataset which has\ndifferent forearm postures, where SpGesture achieved the highest accuracy among\nthe baselines ($89.26\\%$). Moreover, the actual deployment on the CPU\ndemonstrated a system latency below 100ms, well within real-time requirements.\nThis impressive performance showcases SpGesture's potential to enhance the\napplicability of sEMG in real-world scenarios. The code is available at\nhttps://github.com/guoweiyu/SpGesture/.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Surface electromyography (sEMG) based gesture recognition offers a natural\nand intuitive interaction modality for wearable devices. Despite significant\nadvancements in sEMG-based gesture-recognition models, existing methods often\nsuffer from high computational latency and increased energy consumption.\nAdditionally, the inherent instability of sEMG signals, combined with their\nsensitivity to distribution shifts in real-world settings, compromises model\nrobustness. To tackle these challenges, we propose a novel SpGesture framework\nbased on Spiking Neural Networks, which possesses several unique merits\ncompared with existing methods: (1) Robustness: By utilizing membrane potential\nas a memory list, we pioneer the introduction of Source-Free Domain Adaptation\ninto SNN for the first time. This enables SpGesture to mitigate the accuracy\ndegradation caused by distribution shifts. (2) High Accuracy: With a novel\nSpiking Jaccard Attention, SpGesture enhances the SNNs' ability to represent\nsEMG features, leading to a notable rise in system accuracy. To validate\nSpGesture's performance, we collected a new sEMG gesture dataset which has\ndifferent forearm postures, where SpGesture achieved the highest accuracy among\nthe baselines ($89.26\\%$). Moreover, the actual deployment on the CPU\ndemonstrated a system latency below 100ms, well within real-time requirements.\nThis impressive performance showcases SpGesture's potential to enhance the\napplicability of sEMG in real-world scenarios. The code is available at\nhttps://github.com/guoweiyu/SpGesture/."
                },
                "authors": [
                    {
                        "name": "Weiyu Guo"
                    },
                    {
                        "name": "Ying Sun"
                    },
                    {
                        "name": "Yijie Xu"
                    },
                    {
                        "name": "Ziyue Qiao"
                    },
                    {
                        "name": "Yongkui Yang"
                    },
                    {
                        "name": "Hui Xiong"
                    }
                ],
                "author_detail": {
                    "name": "Hui Xiong"
                },
                "author": "Hui Xiong",
                "arxiv_comment": "Accepted by NeurIPS 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.14398v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.14398v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.22982v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.22982v1",
                "updated": "2024-10-30T12:46:15Z",
                "updated_parsed": [
                    2024,
                    10,
                    30,
                    12,
                    46,
                    15,
                    2,
                    304,
                    0
                ],
                "published": "2024-10-30T12:46:15Z",
                "published_parsed": [
                    2024,
                    10,
                    30,
                    12,
                    46,
                    15,
                    2,
                    304,
                    0
                ],
                "title": "PDSR: Efficient UAV Deployment for Swift and Accurate Post-Disaster\n  Search and Rescue",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PDSR: Efficient UAV Deployment for Swift and Accurate Post-Disaster\n  Search and Rescue"
                },
                "summary": "This paper introduces a comprehensive framework for Post-Disaster Search and\nRescue (PDSR), aiming to optimize search and rescue operations leveraging\nUnmanned Aerial Vehicles (UAVs). The primary goal is to improve the precision\nand availability of sensing capabilities, particularly in various catastrophic\nscenarios. Central to this concept is the rapid deployment of UAV swarms\nequipped with diverse sensing, communication, and intelligence capabilities,\nfunctioning as an integrated system that incorporates multiple technologies and\napproaches for efficient detection of individuals buried beneath rubble or\ndebris following a disaster. Within this framework, we propose architectural\nsolution and address associated challenges to ensure optimal performance in\nreal-world disaster scenarios. The proposed framework aims to achieve complete\ncoverage of damaged areas significantly faster than traditional methods using a\nmulti-tier swarm architecture. Furthermore, integrating multi-modal sensing\ndata with machine learning for data fusion could enhance detection accuracy,\nensuring precise identification of survivors.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper introduces a comprehensive framework for Post-Disaster Search and\nRescue (PDSR), aiming to optimize search and rescue operations leveraging\nUnmanned Aerial Vehicles (UAVs). The primary goal is to improve the precision\nand availability of sensing capabilities, particularly in various catastrophic\nscenarios. Central to this concept is the rapid deployment of UAV swarms\nequipped with diverse sensing, communication, and intelligence capabilities,\nfunctioning as an integrated system that incorporates multiple technologies and\napproaches for efficient detection of individuals buried beneath rubble or\ndebris following a disaster. Within this framework, we propose architectural\nsolution and address associated challenges to ensure optimal performance in\nreal-world disaster scenarios. The proposed framework aims to achieve complete\ncoverage of damaged areas significantly faster than traditional methods using a\nmulti-tier swarm architecture. Furthermore, integrating multi-modal sensing\ndata with machine learning for data fusion could enhance detection accuracy,\nensuring precise identification of survivors."
                },
                "authors": [
                    {
                        "name": "Alaa Awad Abdellatif"
                    },
                    {
                        "name": "Ali Elmancy"
                    },
                    {
                        "name": "Amr Mohamed"
                    },
                    {
                        "name": "Ahmed Massoud"
                    },
                    {
                        "name": "Wadha Lebda"
                    },
                    {
                        "name": "Khalid K. Naji"
                    }
                ],
                "author_detail": {
                    "name": "Khalid K. Naji"
                },
                "author": "Khalid K. Naji",
                "arxiv_comment": "This paper is currently under review at IEEE IoT Magazine",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.22982v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.22982v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.22980v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.22980v1",
                "updated": "2024-10-30T12:45:12Z",
                "updated_parsed": [
                    2024,
                    10,
                    30,
                    12,
                    45,
                    12,
                    2,
                    304,
                    0
                ],
                "published": "2024-10-30T12:45:12Z",
                "published_parsed": [
                    2024,
                    10,
                    30,
                    12,
                    45,
                    12,
                    2,
                    304,
                    0
                ],
                "title": "Efficient End-to-End 6-Dof Grasp Detection Framework for Edge Devices\n  with Hierarchical Heatmaps and Feature Propagation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient End-to-End 6-Dof Grasp Detection Framework for Edge Devices\n  with Hierarchical Heatmaps and Feature Propagation"
                },
                "summary": "6-DoF grasp detection is critically important for the advancement of\nintelligent embodied systems, as it provides feasible robot poses for object\ngrasping. Various methods have been proposed to detect 6-DoF grasps through the\nextraction of 3D geometric features from RGBD or point cloud data. However,\nmost of these approaches encounter challenges during real robot deployment due\nto their significant computational demands, which can be particularly\nproblematic for mobile robot platforms, especially those reliant on edge\ncomputing devices. This paper presents an Efficient End-to-End Grasp Detection\nNetwork (E3GNet) for 6-DoF grasp detection utilizing hierarchical heatmap\nrepresentations. E3GNet effectively identifies high-quality and diverse grasps\nin cluttered real-world environments. Benefiting from our end-to-end\nmethodology and efficient network design, our approach surpasses previous\nmethods in model inference efficiency and achieves real-time 6-Dof grasp\ndetection on edge devices. Furthermore, real-world experiments validate the\neffectiveness of our method, achieving a satisfactory 94% object grasping\nsuccess rate.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "6-DoF grasp detection is critically important for the advancement of\nintelligent embodied systems, as it provides feasible robot poses for object\ngrasping. Various methods have been proposed to detect 6-DoF grasps through the\nextraction of 3D geometric features from RGBD or point cloud data. However,\nmost of these approaches encounter challenges during real robot deployment due\nto their significant computational demands, which can be particularly\nproblematic for mobile robot platforms, especially those reliant on edge\ncomputing devices. This paper presents an Efficient End-to-End Grasp Detection\nNetwork (E3GNet) for 6-DoF grasp detection utilizing hierarchical heatmap\nrepresentations. E3GNet effectively identifies high-quality and diverse grasps\nin cluttered real-world environments. Benefiting from our end-to-end\nmethodology and efficient network design, our approach surpasses previous\nmethods in model inference efficiency and achieves real-time 6-Dof grasp\ndetection on edge devices. Furthermore, real-world experiments validate the\neffectiveness of our method, achieving a satisfactory 94% object grasping\nsuccess rate."
                },
                "authors": [
                    {
                        "name": "Kaiqin Yang. Yixiang Dai"
                    },
                    {
                        "name": "Guijin Wang"
                    },
                    {
                        "name": "Siang Chen"
                    }
                ],
                "author_detail": {
                    "name": "Siang Chen"
                },
                "author": "Siang Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.22980v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.22980v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.22977v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.22977v1",
                "updated": "2024-10-30T12:42:38Z",
                "updated_parsed": [
                    2024,
                    10,
                    30,
                    12,
                    42,
                    38,
                    2,
                    304,
                    0
                ],
                "published": "2024-10-30T12:42:38Z",
                "published_parsed": [
                    2024,
                    10,
                    30,
                    12,
                    42,
                    38,
                    2,
                    304,
                    0
                ],
                "title": "Bonafide at LegalLens 2024 Shared Task: Using Lightweight DeBERTa Based\n  Encoder For Legal Violation Detection and Resolution",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Bonafide at LegalLens 2024 Shared Task: Using Lightweight DeBERTa Based\n  Encoder For Legal Violation Detection and Resolution"
                },
                "summary": "In this work, we present two systems -- Named Entity Resolution (NER) and\nNatural Language Inference (NLI) -- for detecting legal violations within\nunstructured textual data and for associating these violations with potentially\naffected individuals, respectively. Both these systems are lightweight DeBERTa\nbased encoders that outperform the LLM baselines. The proposed NER system\nachieved an F1 score of 60.01\\% on Subtask A of the LegalLens challenge, which\nfocuses on identifying violations. The proposed NLI system achieved an F1 score\nof 84.73\\% on Subtask B of the LegalLens challenge, which focuses on resolving\nthese violations by matching them with pre-existing legal complaints of class\naction cases. Our NER system ranked sixth and NLI system ranked fifth on the\nLegalLens leaderboard. We release the trained models and inference scripts.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this work, we present two systems -- Named Entity Resolution (NER) and\nNatural Language Inference (NLI) -- for detecting legal violations within\nunstructured textual data and for associating these violations with potentially\naffected individuals, respectively. Both these systems are lightweight DeBERTa\nbased encoders that outperform the LLM baselines. The proposed NER system\nachieved an F1 score of 60.01\\% on Subtask A of the LegalLens challenge, which\nfocuses on identifying violations. The proposed NLI system achieved an F1 score\nof 84.73\\% on Subtask B of the LegalLens challenge, which focuses on resolving\nthese violations by matching them with pre-existing legal complaints of class\naction cases. Our NER system ranked sixth and NLI system ranked fifth on the\nLegalLens leaderboard. We release the trained models and inference scripts."
                },
                "authors": [
                    {
                        "name": "Shikha Bordia"
                    }
                ],
                "author_detail": {
                    "name": "Shikha Bordia"
                },
                "author": "Shikha Bordia",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.22977v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.22977v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.22971v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.22971v1",
                "updated": "2024-10-30T12:38:49Z",
                "updated_parsed": [
                    2024,
                    10,
                    30,
                    12,
                    38,
                    49,
                    2,
                    304,
                    0
                ],
                "published": "2024-10-30T12:38:49Z",
                "published_parsed": [
                    2024,
                    10,
                    30,
                    12,
                    38,
                    49,
                    2,
                    304,
                    0
                ],
                "title": "Private Synthetic Text Generation with Diffusion Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Private Synthetic Text Generation with Diffusion Models"
                },
                "summary": "How capable are diffusion models of generating synthetics texts? Recent\nresearch shows their strengths, with performance reaching that of\nauto-regressive LLMs. But are they also good in generating synthetic data if\nthe training was under differential privacy? Here the evidence is missing, yet\nthe promises from private image generation look strong. In this paper we\naddress this open question by extensive experiments. At the same time, we\ncritically assess (and reimplement) previous works on synthetic private text\ngeneration with LLMs and reveal some unmet assumptions that might have led to\nviolating the differential privacy guarantees. Our results partly contradict\nprevious non-private findings and show that fully open-source LLMs outperform\ndiffusion models in the privacy regime. Our complete source codes, datasets,\nand experimental setup is publicly available to foster future research.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "How capable are diffusion models of generating synthetics texts? Recent\nresearch shows their strengths, with performance reaching that of\nauto-regressive LLMs. But are they also good in generating synthetic data if\nthe training was under differential privacy? Here the evidence is missing, yet\nthe promises from private image generation look strong. In this paper we\naddress this open question by extensive experiments. At the same time, we\ncritically assess (and reimplement) previous works on synthetic private text\ngeneration with LLMs and reveal some unmet assumptions that might have led to\nviolating the differential privacy guarantees. Our results partly contradict\nprevious non-private findings and show that fully open-source LLMs outperform\ndiffusion models in the privacy regime. Our complete source codes, datasets,\nand experimental setup is publicly available to foster future research."
                },
                "authors": [
                    {
                        "name": "Sebastian Ochs"
                    },
                    {
                        "name": "Ivan Habernal"
                    }
                ],
                "author_detail": {
                    "name": "Ivan Habernal"
                },
                "author": "Ivan Habernal",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.22971v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.22971v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.12220v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.12220v2",
                "updated": "2024-10-30T12:14:35Z",
                "updated_parsed": [
                    2024,
                    10,
                    30,
                    12,
                    14,
                    35,
                    2,
                    304,
                    0
                ],
                "published": "2024-07-17T00:06:30Z",
                "published_parsed": [
                    2024,
                    7,
                    17,
                    0,
                    6,
                    30,
                    2,
                    199,
                    0
                ],
                "title": "Questionable practices in machine learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Questionable practices in machine learning"
                },
                "summary": "Evaluating modern ML models is hard. The strong incentive for researchers and\ncompanies to report a state-of-the-art result on some metric often leads to\nquestionable research practices (QRPs): bad practices which fall short of\noutright research fraud. We describe 44 such practices which can undermine\nreported results, giving examples where possible. Our list emphasises the\nevaluation of large language models (LLMs) on public benchmarks. We also\ndiscuss \"irreproducible research practices\", i.e. decisions that make it\ndifficult or impossible for other researchers to reproduce, build on or audit\nprevious research.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Evaluating modern ML models is hard. The strong incentive for researchers and\ncompanies to report a state-of-the-art result on some metric often leads to\nquestionable research practices (QRPs): bad practices which fall short of\noutright research fraud. We describe 44 such practices which can undermine\nreported results, giving examples where possible. Our list emphasises the\nevaluation of large language models (LLMs) on public benchmarks. We also\ndiscuss \"irreproducible research practices\", i.e. decisions that make it\ndifficult or impossible for other researchers to reproduce, build on or audit\nprevious research."
                },
                "authors": [
                    {
                        "name": "Gavin Leech"
                    },
                    {
                        "name": "Juan J. Vazquez"
                    },
                    {
                        "name": "Niclas Kupper"
                    },
                    {
                        "name": "Misha Yagudin"
                    },
                    {
                        "name": "Laurence Aitchison"
                    }
                ],
                "author_detail": {
                    "name": "Laurence Aitchison"
                },
                "author": "Laurence Aitchison",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.12220v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.12220v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.22954v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.22954v1",
                "updated": "2024-10-30T12:09:29Z",
                "updated_parsed": [
                    2024,
                    10,
                    30,
                    12,
                    9,
                    29,
                    2,
                    304,
                    0
                ],
                "published": "2024-10-30T12:09:29Z",
                "published_parsed": [
                    2024,
                    10,
                    30,
                    12,
                    9,
                    29,
                    2,
                    304,
                    0
                ],
                "title": "Retrieval-Augmented Generation with Estimation of Source Reliability",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Retrieval-Augmented Generation with Estimation of Source Reliability"
                },
                "summary": "Retrieval-augmented generation (RAG) addresses key limitations of large\nlanguage models (LLMs), such as hallucinations and outdated knowledge, by\nincorporating external databases. These databases typically consult multiple\nsources to encompass up-to-date and various information. However, standard RAG\nmethods often overlook the heterogeneous source reliability in the multi-source\ndatabase and retrieve documents solely based on relevance, making them prone to\npropagating misinformation. To address this, we propose Reliability-Aware RAG\n(RA-RAG) which estimates the reliability of multiple sources and incorporates\nthis information into both retrieval and aggregation processes. Specifically,\nit iteratively estimates source reliability and true answers for a set of\nqueries with no labelling. Then, it selectively retrieves relevant documents\nfrom a few of reliable sources and aggregates them using weighted majority\nvoting, where the selective retrieval ensures scalability while not\ncompromising the performance. We also introduce a benchmark designed to reflect\nreal-world scenarios with heterogeneous source reliability and demonstrate the\neffectiveness of RA-RAG compared to a set of baselines.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Retrieval-augmented generation (RAG) addresses key limitations of large\nlanguage models (LLMs), such as hallucinations and outdated knowledge, by\nincorporating external databases. These databases typically consult multiple\nsources to encompass up-to-date and various information. However, standard RAG\nmethods often overlook the heterogeneous source reliability in the multi-source\ndatabase and retrieve documents solely based on relevance, making them prone to\npropagating misinformation. To address this, we propose Reliability-Aware RAG\n(RA-RAG) which estimates the reliability of multiple sources and incorporates\nthis information into both retrieval and aggregation processes. Specifically,\nit iteratively estimates source reliability and true answers for a set of\nqueries with no labelling. Then, it selectively retrieves relevant documents\nfrom a few of reliable sources and aggregates them using weighted majority\nvoting, where the selective retrieval ensures scalability while not\ncompromising the performance. We also introduce a benchmark designed to reflect\nreal-world scenarios with heterogeneous source reliability and demonstrate the\neffectiveness of RA-RAG compared to a set of baselines."
                },
                "authors": [
                    {
                        "name": "Jeongyeon Hwang"
                    },
                    {
                        "name": "Junyoung Park"
                    },
                    {
                        "name": "Hyejin Park"
                    },
                    {
                        "name": "Sangdon Park"
                    },
                    {
                        "name": "Jungseul Ok"
                    }
                ],
                "author_detail": {
                    "name": "Jungseul Ok"
                },
                "author": "Jungseul Ok",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.22954v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.22954v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.01288v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.01288v2",
                "updated": "2024-10-30T12:08:42Z",
                "updated_parsed": [
                    2024,
                    10,
                    30,
                    12,
                    8,
                    42,
                    2,
                    304,
                    0
                ],
                "published": "2024-06-03T12:59:17Z",
                "published_parsed": [
                    2024,
                    6,
                    3,
                    12,
                    59,
                    17,
                    0,
                    155,
                    0
                ],
                "title": "Improved Few-Shot Jailbreaking Can Circumvent Aligned Language Models\n  and Their Defenses",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Improved Few-Shot Jailbreaking Can Circumvent Aligned Language Models\n  and Their Defenses"
                },
                "summary": "Recently, Anil et al. (2024) show that many-shot (up to hundreds of)\ndemonstrations can jailbreak state-of-the-art LLMs by exploiting their\nlong-context capability. Nevertheless, is it possible to use few-shot\ndemonstrations to efficiently jailbreak LLMs within limited context sizes?\nWhile the vanilla few-shot jailbreaking may be inefficient, we propose improved\ntechniques such as injecting special system tokens like [/INST] and employing\ndemo-level random search from a collected demo pool. These simple techniques\nresult in surprisingly effective jailbreaking against aligned LLMs (even with\nadvanced defenses). For examples, our method achieves >80% (mostly >95%) ASRs\non Llama-2-7B and Llama-3-8B without multiple restarts, even if the models are\nenhanced by strong defenses such as perplexity detection and/or SmoothLLM,\nwhich is challenging for suffix-based jailbreaking. In addition, we conduct\ncomprehensive and elaborate (e.g., making sure to use correct system prompts)\nevaluations against other aligned LLMs and advanced defenses, where our method\nconsistently achieves nearly 100% ASRs. Our code is available at\nhttps://github.com/sail-sg/I-FSJ.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recently, Anil et al. (2024) show that many-shot (up to hundreds of)\ndemonstrations can jailbreak state-of-the-art LLMs by exploiting their\nlong-context capability. Nevertheless, is it possible to use few-shot\ndemonstrations to efficiently jailbreak LLMs within limited context sizes?\nWhile the vanilla few-shot jailbreaking may be inefficient, we propose improved\ntechniques such as injecting special system tokens like [/INST] and employing\ndemo-level random search from a collected demo pool. These simple techniques\nresult in surprisingly effective jailbreaking against aligned LLMs (even with\nadvanced defenses). For examples, our method achieves >80% (mostly >95%) ASRs\non Llama-2-7B and Llama-3-8B without multiple restarts, even if the models are\nenhanced by strong defenses such as perplexity detection and/or SmoothLLM,\nwhich is challenging for suffix-based jailbreaking. In addition, we conduct\ncomprehensive and elaborate (e.g., making sure to use correct system prompts)\nevaluations against other aligned LLMs and advanced defenses, where our method\nconsistently achieves nearly 100% ASRs. Our code is available at\nhttps://github.com/sail-sg/I-FSJ."
                },
                "authors": [
                    {
                        "name": "Xiaosen Zheng"
                    },
                    {
                        "name": "Tianyu Pang"
                    },
                    {
                        "name": "Chao Du"
                    },
                    {
                        "name": "Qian Liu"
                    },
                    {
                        "name": "Jing Jiang"
                    },
                    {
                        "name": "Min Lin"
                    }
                ],
                "author_detail": {
                    "name": "Min Lin"
                },
                "author": "Min Lin",
                "arxiv_comment": "NeurIPS 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.01288v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.01288v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.17515v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.17515v3",
                "updated": "2024-10-30T12:04:18Z",
                "updated_parsed": [
                    2024,
                    10,
                    30,
                    12,
                    4,
                    18,
                    2,
                    304,
                    0
                ],
                "published": "2024-09-26T03:50:22Z",
                "published_parsed": [
                    2024,
                    9,
                    26,
                    3,
                    50,
                    22,
                    3,
                    270,
                    0
                ],
                "title": "From News to Forecast: Integrating Event Analysis in LLM-Based Time\n  Series Forecasting with Reflection",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "From News to Forecast: Integrating Event Analysis in LLM-Based Time\n  Series Forecasting with Reflection"
                },
                "summary": "This paper introduces a novel approach that leverages Large Language Models\n(LLMs) and Generative Agents to enhance time series forecasting by reasoning\nacross both text and time series data. With language as a medium, our method\nadaptively integrates social events into forecasting models, aligning news\ncontent with time series fluctuations to provide richer insights. Specifically,\nwe utilize LLM-based agents to iteratively filter out irrelevant news and\nemploy human-like reasoning to evaluate predictions. This enables the model to\nanalyze complex events, such as unexpected incidents and shifts in social\nbehavior, and continuously refine the selection logic of news and the\nrobustness of the agent's output. By integrating selected news events with time\nseries data, we fine-tune a pre-trained LLM to predict sequences of digits in\ntime series. The results demonstrate significant improvements in forecasting\naccuracy, suggesting a potential paradigm shift in time series forecasting\nthrough the effective utilization of unstructured news data.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper introduces a novel approach that leverages Large Language Models\n(LLMs) and Generative Agents to enhance time series forecasting by reasoning\nacross both text and time series data. With language as a medium, our method\nadaptively integrates social events into forecasting models, aligning news\ncontent with time series fluctuations to provide richer insights. Specifically,\nwe utilize LLM-based agents to iteratively filter out irrelevant news and\nemploy human-like reasoning to evaluate predictions. This enables the model to\nanalyze complex events, such as unexpected incidents and shifts in social\nbehavior, and continuously refine the selection logic of news and the\nrobustness of the agent's output. By integrating selected news events with time\nseries data, we fine-tune a pre-trained LLM to predict sequences of digits in\ntime series. The results demonstrate significant improvements in forecasting\naccuracy, suggesting a potential paradigm shift in time series forecasting\nthrough the effective utilization of unstructured news data."
                },
                "authors": [
                    {
                        "name": "Xinlei Wang"
                    },
                    {
                        "name": "Maike Feng"
                    },
                    {
                        "name": "Jing Qiu"
                    },
                    {
                        "name": "Jinjin Gu"
                    },
                    {
                        "name": "Junhua Zhao"
                    }
                ],
                "author_detail": {
                    "name": "Junhua Zhao"
                },
                "author": "Junhua Zhao",
                "arxiv_comment": "This paper has been accepted for NeurIPS 2024. Code and data are\n  available at https://github.com/ameliawong1996/From_News_to_Forecast",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.17515v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.17515v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.22945v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.22945v1",
                "updated": "2024-10-30T12:02:34Z",
                "updated_parsed": [
                    2024,
                    10,
                    30,
                    12,
                    2,
                    34,
                    2,
                    304,
                    0
                ],
                "published": "2024-10-30T12:02:34Z",
                "published_parsed": [
                    2024,
                    10,
                    30,
                    12,
                    2,
                    34,
                    2,
                    304,
                    0
                ],
                "title": "Broad-band, high-gain, low-frequency Antennas for Radio Detection of\n  Earth-skimming Tau Neutrinos",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Broad-band, high-gain, low-frequency Antennas for Radio Detection of\n  Earth-skimming Tau Neutrinos"
                },
                "summary": "A promising approach to detect high-energy tau neutrinos is through the\nmeasurement of impulsive radio emission from horizontal air showers initiated\nin the Earth's atmosphere. Observations at frequencies between 30 and 80 MHz\nseem particularly promising -- if high-gain antennas focused at the horizon and\nblocking out as much as possible of the noisy sky are employed. Due to the\nlarge wavelengths, however, designing an antenna with the required properties\nis highly non-trivial at such low frequencies. In this article, we explore\nsuitable antenna designs that provide the desired high gain, possess a smooth\nbeam, are insensitive to ground conditions, are easily impedance-matched over\nthe wide band, and are mechanically simple for deployment in large numbers in\ninaccessible terrain. In particular, we consider the \"rhombus\" antenna design\nfor both horizontally and vertically polarized radiation a very attractive\noption for tau neutrino detection efforts in the atmosphere with the radio\ntechnique.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A promising approach to detect high-energy tau neutrinos is through the\nmeasurement of impulsive radio emission from horizontal air showers initiated\nin the Earth's atmosphere. Observations at frequencies between 30 and 80 MHz\nseem particularly promising -- if high-gain antennas focused at the horizon and\nblocking out as much as possible of the noisy sky are employed. Due to the\nlarge wavelengths, however, designing an antenna with the required properties\nis highly non-trivial at such low frequencies. In this article, we explore\nsuitable antenna designs that provide the desired high gain, possess a smooth\nbeam, are insensitive to ground conditions, are easily impedance-matched over\nthe wide band, and are mechanically simple for deployment in large numbers in\ninaccessible terrain. In particular, we consider the \"rhombus\" antenna design\nfor both horizontally and vertically polarized radiation a very attractive\noption for tau neutrino detection efforts in the atmosphere with the radio\ntechnique."
                },
                "authors": [
                    {
                        "name": "Tim Huege"
                    },
                    {
                        "name": "Oliver Krömer"
                    }
                ],
                "author_detail": {
                    "name": "Oliver Krömer"
                },
                "author": "Oliver Krömer",
                "arxiv_comment": "15 pages, to be submitted to JINST",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.22945v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.22945v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.IM",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.IM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.22944v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.22944v1",
                "updated": "2024-10-30T12:01:48Z",
                "updated_parsed": [
                    2024,
                    10,
                    30,
                    12,
                    1,
                    48,
                    2,
                    304,
                    0
                ],
                "published": "2024-10-30T12:01:48Z",
                "published_parsed": [
                    2024,
                    10,
                    30,
                    12,
                    1,
                    48,
                    2,
                    304,
                    0
                ],
                "title": "Focus On This, Not That! Steering LLMs With Adaptive Feature\n  Specification",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Focus On This, Not That! Steering LLMs With Adaptive Feature\n  Specification"
                },
                "summary": "Despite the success of Instruction Tuning (IT) in training large language\nmodels (LLMs) to perform arbitrary user-specified tasks, these models often\nstill leverage spurious or biased features learned from their training data,\nleading to undesired behaviours when deploying them in new contexts. In this\nwork, we introduce Focus Instruction Tuning (FIT), which trains LLMs to\ncondition their responses by focusing on specific features whilst ignoring\nothers, leading to different behaviours based on what features are specified.\nAcross several experimental settings, we show that focus-tuned models can be\nadaptively steered by focusing on different features at inference-time: for\ninstance, robustness can be improved by focusing on task-causal features and\nignoring spurious features, and social bias can be mitigated by ignoring\ndemographic categories. Furthermore, FIT can steer behaviour in new contexts,\ngeneralising under distribution shift and to new unseen features at inference\ntime, and thereby facilitating more robust, fair, and controllable LLM\napplications in real-world environments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Despite the success of Instruction Tuning (IT) in training large language\nmodels (LLMs) to perform arbitrary user-specified tasks, these models often\nstill leverage spurious or biased features learned from their training data,\nleading to undesired behaviours when deploying them in new contexts. In this\nwork, we introduce Focus Instruction Tuning (FIT), which trains LLMs to\ncondition their responses by focusing on specific features whilst ignoring\nothers, leading to different behaviours based on what features are specified.\nAcross several experimental settings, we show that focus-tuned models can be\nadaptively steered by focusing on different features at inference-time: for\ninstance, robustness can be improved by focusing on task-causal features and\nignoring spurious features, and social bias can be mitigated by ignoring\ndemographic categories. Furthermore, FIT can steer behaviour in new contexts,\ngeneralising under distribution shift and to new unseen features at inference\ntime, and thereby facilitating more robust, fair, and controllable LLM\napplications in real-world environments."
                },
                "authors": [
                    {
                        "name": "Tom A. Lamb"
                    },
                    {
                        "name": "Adam Davies"
                    },
                    {
                        "name": "Alasdair Paren"
                    },
                    {
                        "name": "Philip H. S. Torr"
                    },
                    {
                        "name": "Francesco Pinto"
                    }
                ],
                "author_detail": {
                    "name": "Francesco Pinto"
                },
                "author": "Francesco Pinto",
                "arxiv_comment": "28pages, 14 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.22944v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.22944v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2403.00418v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2403.00418v3",
                "updated": "2024-10-30T12:00:42Z",
                "updated_parsed": [
                    2024,
                    10,
                    30,
                    12,
                    0,
                    42,
                    2,
                    304,
                    0
                ],
                "published": "2024-03-01T10:10:34Z",
                "published_parsed": [
                    2024,
                    3,
                    1,
                    10,
                    10,
                    34,
                    4,
                    61,
                    0
                ],
                "title": "LLMs for Targeted Sentiment in News Headlines: Exploring the\n  Descriptive-Prescriptive Dilemma",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLMs for Targeted Sentiment in News Headlines: Exploring the\n  Descriptive-Prescriptive Dilemma"
                },
                "summary": "News headlines often evoke sentiment by intentionally portraying entities in\nparticular ways, making targeted sentiment analysis (TSA) of headlines a\nworthwhile but difficult task. Due to its subjectivity, creating TSA datasets\ncan involve various annotation paradigms, from descriptive to prescriptive,\neither encouraging or limiting subjectivity. LLMs are a good fit for TSA due to\ntheir broad linguistic and world knowledge and in-context learning abilities,\nyet their performance depends on prompt design. In this paper, we compare the\naccuracy of state-of-the-art LLMs and fine-tuned encoder models for TSA of news\nheadlines using descriptive and prescriptive datasets across several languages.\nExploring the descriptive--prescriptive continuum, we analyze how performance\nis affected by prompt prescriptiveness, ranging from plain zero-shot to\nelaborate few-shot prompts. Finally, we evaluate the ability of LLMs to\nquantify uncertainty via calibration error and comparison to human label\nvariation. We find that LLMs outperform fine-tuned encoders on descriptive\ndatasets, while calibration and F1-score generally improve with increased\nprescriptiveness, yet the optimal level varies.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "News headlines often evoke sentiment by intentionally portraying entities in\nparticular ways, making targeted sentiment analysis (TSA) of headlines a\nworthwhile but difficult task. Due to its subjectivity, creating TSA datasets\ncan involve various annotation paradigms, from descriptive to prescriptive,\neither encouraging or limiting subjectivity. LLMs are a good fit for TSA due to\ntheir broad linguistic and world knowledge and in-context learning abilities,\nyet their performance depends on prompt design. In this paper, we compare the\naccuracy of state-of-the-art LLMs and fine-tuned encoder models for TSA of news\nheadlines using descriptive and prescriptive datasets across several languages.\nExploring the descriptive--prescriptive continuum, we analyze how performance\nis affected by prompt prescriptiveness, ranging from plain zero-shot to\nelaborate few-shot prompts. Finally, we evaluate the ability of LLMs to\nquantify uncertainty via calibration error and comparison to human label\nvariation. We find that LLMs outperform fine-tuned encoders on descriptive\ndatasets, while calibration and F1-score generally improve with increased\nprescriptiveness, yet the optimal level varies."
                },
                "authors": [
                    {
                        "name": "Jana Juroš"
                    },
                    {
                        "name": "Laura Majer"
                    },
                    {
                        "name": "Jan Šnajder"
                    }
                ],
                "author_detail": {
                    "name": "Jan Šnajder"
                },
                "author": "Jan Šnajder",
                "arxiv_comment": "Presented at 14th Workshop on Computational Approaches to\n  Subjectivity, Sentiment, & Social Media Analysis (WASSA) at ACL 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2403.00418v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2403.00418v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.21573v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.21573v2",
                "updated": "2024-10-30T11:56:17Z",
                "updated_parsed": [
                    2024,
                    10,
                    30,
                    11,
                    56,
                    17,
                    2,
                    304,
                    0
                ],
                "published": "2024-10-28T22:09:43Z",
                "published_parsed": [
                    2024,
                    10,
                    28,
                    22,
                    9,
                    43,
                    0,
                    302,
                    0
                ],
                "title": "Thank You, Stingray: Multilingual Large Language Models Can Not (Yet)\n  Disambiguate Cross-Lingual Word Sense",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Thank You, Stingray: Multilingual Large Language Models Can Not (Yet)\n  Disambiguate Cross-Lingual Word Sense"
                },
                "summary": "Multilingual large language models (LLMs) have gained prominence, but\nconcerns arise regarding their reliability beyond English. This study addresses\nthe gap in cross-lingual semantic evaluation by introducing a novel benchmark\nfor cross-lingual sense disambiguation, StingrayBench. In this paper, we\ndemonstrate using false friends -- words that are orthographically similar but\nhave completely different meanings in two languages -- as a possible approach\nto pinpoint the limitation of cross-lingual sense disambiguation in LLMs. We\ncollect false friends in four language pairs, namely Indonesian-Malay,\nIndonesian-Tagalog, Chinese-Japanese, and English-German; and challenge LLMs to\ndistinguish the use of them in context. In our analysis of various models, we\nobserve they tend to be biased toward higher-resource languages. We also\npropose new metrics for quantifying the cross-lingual sense bias and\ncomprehension based on our benchmark. Our work contributes to developing more\ndiverse and inclusive language modeling, promoting fairer access for the wider\nmultilingual community.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multilingual large language models (LLMs) have gained prominence, but\nconcerns arise regarding their reliability beyond English. This study addresses\nthe gap in cross-lingual semantic evaluation by introducing a novel benchmark\nfor cross-lingual sense disambiguation, StingrayBench. In this paper, we\ndemonstrate using false friends -- words that are orthographically similar but\nhave completely different meanings in two languages -- as a possible approach\nto pinpoint the limitation of cross-lingual sense disambiguation in LLMs. We\ncollect false friends in four language pairs, namely Indonesian-Malay,\nIndonesian-Tagalog, Chinese-Japanese, and English-German; and challenge LLMs to\ndistinguish the use of them in context. In our analysis of various models, we\nobserve they tend to be biased toward higher-resource languages. We also\npropose new metrics for quantifying the cross-lingual sense bias and\ncomprehension based on our benchmark. Our work contributes to developing more\ndiverse and inclusive language modeling, promoting fairer access for the wider\nmultilingual community."
                },
                "authors": [
                    {
                        "name": "Samuel Cahyawijaya"
                    },
                    {
                        "name": "Ruochen Zhang"
                    },
                    {
                        "name": "Holy Lovenia"
                    },
                    {
                        "name": "Jan Christian Blaise Cruz"
                    },
                    {
                        "name": "Elisa Gilbert"
                    },
                    {
                        "name": "Hiroki Nomoto"
                    },
                    {
                        "name": "Alham Fikri Aji"
                    }
                ],
                "author_detail": {
                    "name": "Alham Fikri Aji"
                },
                "author": "Alham Fikri Aji",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.21573v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.21573v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.12311v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.12311v2",
                "updated": "2024-10-30T11:47:40Z",
                "updated_parsed": [
                    2024,
                    10,
                    30,
                    11,
                    47,
                    40,
                    2,
                    304,
                    0
                ],
                "published": "2024-06-18T06:32:23Z",
                "published_parsed": [
                    2024,
                    6,
                    18,
                    6,
                    32,
                    23,
                    1,
                    170,
                    0
                ],
                "title": "Mixture of Scales: Memory-Efficient Token-Adaptive Binarization for\n  Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mixture of Scales: Memory-Efficient Token-Adaptive Binarization for\n  Large Language Models"
                },
                "summary": "Binarization, which converts weight parameters to binary values, has emerged\nas an effective strategy to reduce the size of large language models (LLMs).\nHowever, typical binarization techniques significantly diminish linguistic\neffectiveness of LLMs. To address this issue, we introduce a novel binarization\ntechnique called Mixture of Scales (BinaryMoS). Unlike conventional methods,\nBinaryMoS employs multiple scaling experts for binary weights, dynamically\nmerging these experts for each token to adaptively generate scaling factors.\nThis token-adaptive approach boosts the representational power of binarized\nLLMs by enabling contextual adjustments to the values of binary weights.\nMoreover, because this adaptive process only involves the scaling factors\nrather than the entire weight matrix, BinaryMoS maintains compression\nefficiency similar to traditional static binarization methods. Our experimental\nresults reveal that BinaryMoS surpasses conventional binarization techniques in\nvarious natural language processing tasks and even outperforms 2-bit\nquantization methods, all while maintaining similar model size to static\nbinarization techniques.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Binarization, which converts weight parameters to binary values, has emerged\nas an effective strategy to reduce the size of large language models (LLMs).\nHowever, typical binarization techniques significantly diminish linguistic\neffectiveness of LLMs. To address this issue, we introduce a novel binarization\ntechnique called Mixture of Scales (BinaryMoS). Unlike conventional methods,\nBinaryMoS employs multiple scaling experts for binary weights, dynamically\nmerging these experts for each token to adaptively generate scaling factors.\nThis token-adaptive approach boosts the representational power of binarized\nLLMs by enabling contextual adjustments to the values of binary weights.\nMoreover, because this adaptive process only involves the scaling factors\nrather than the entire weight matrix, BinaryMoS maintains compression\nefficiency similar to traditional static binarization methods. Our experimental\nresults reveal that BinaryMoS surpasses conventional binarization techniques in\nvarious natural language processing tasks and even outperforms 2-bit\nquantization methods, all while maintaining similar model size to static\nbinarization techniques."
                },
                "authors": [
                    {
                        "name": "Dongwon Jo"
                    },
                    {
                        "name": "Taesu Kim"
                    },
                    {
                        "name": "Yulhwa Kim"
                    },
                    {
                        "name": "Jae-Joon Kim"
                    }
                ],
                "author_detail": {
                    "name": "Jae-Joon Kim"
                },
                "author": "Jae-Joon Kim",
                "arxiv_comment": "38th Conference on Neural Information Processing Systems (NeurIPS\n  2024)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.12311v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.12311v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.22932v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.22932v1",
                "updated": "2024-10-30T11:38:13Z",
                "updated_parsed": [
                    2024,
                    10,
                    30,
                    11,
                    38,
                    13,
                    2,
                    304,
                    0
                ],
                "published": "2024-10-30T11:38:13Z",
                "published_parsed": [
                    2024,
                    10,
                    30,
                    11,
                    38,
                    13,
                    2,
                    304,
                    0
                ],
                "title": "Multi-Agent Large Language Models for Conversational Task-Solving",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-Agent Large Language Models for Conversational Task-Solving"
                },
                "summary": "In an era where single large language models have dominated the landscape of\nartificial intelligence for years, multi-agent systems arise as new\nprotagonists in conversational task-solving. While previous studies have\nshowcased their potential in reasoning tasks and creative endeavors, an\nanalysis of their limitations concerning the conversational paradigms and the\nimpact of individual agents is missing. It remains unascertained how\nmulti-agent discussions perform across tasks of varying complexity and how the\nstructure of these conversations influences the process. To fill that gap, this\nwork systematically evaluates multi-agent systems across various discussion\nparadigms, assessing their strengths and weaknesses in both generative tasks\nand question-answering tasks. Alongside the experiments, I propose a taxonomy\nof 20 multi-agent research studies from 2022 to 2024, followed by the\nintroduction of a framework for deploying multi-agent LLMs in conversational\ntask-solving. I demonstrate that while multi-agent systems excel in complex\nreasoning tasks, outperforming a single model by leveraging expert personas,\nthey fail on basic tasks. Concretely, I identify three challenges that arise:\n1) While longer discussions enhance reasoning, agents fail to maintain\nconformity to strict task requirements, which leads to problem drift, making\nshorter conversations more effective for basic tasks. 2) Prolonged discussions\nrisk alignment collapse, raising new safety concerns for these systems. 3) I\nshowcase discussion monopolization through long generations, posing the problem\nof fairness in decision-making for tasks like summarization. This work uncovers\nboth the potential and challenges that arise with multi-agent interaction and\nvarying conversational paradigms, providing insights into how future research\ncould improve the efficiency, performance, and safety of multi-agent LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In an era where single large language models have dominated the landscape of\nartificial intelligence for years, multi-agent systems arise as new\nprotagonists in conversational task-solving. While previous studies have\nshowcased their potential in reasoning tasks and creative endeavors, an\nanalysis of their limitations concerning the conversational paradigms and the\nimpact of individual agents is missing. It remains unascertained how\nmulti-agent discussions perform across tasks of varying complexity and how the\nstructure of these conversations influences the process. To fill that gap, this\nwork systematically evaluates multi-agent systems across various discussion\nparadigms, assessing their strengths and weaknesses in both generative tasks\nand question-answering tasks. Alongside the experiments, I propose a taxonomy\nof 20 multi-agent research studies from 2022 to 2024, followed by the\nintroduction of a framework for deploying multi-agent LLMs in conversational\ntask-solving. I demonstrate that while multi-agent systems excel in complex\nreasoning tasks, outperforming a single model by leveraging expert personas,\nthey fail on basic tasks. Concretely, I identify three challenges that arise:\n1) While longer discussions enhance reasoning, agents fail to maintain\nconformity to strict task requirements, which leads to problem drift, making\nshorter conversations more effective for basic tasks. 2) Prolonged discussions\nrisk alignment collapse, raising new safety concerns for these systems. 3) I\nshowcase discussion monopolization through long generations, posing the problem\nof fairness in decision-making for tasks like summarization. This work uncovers\nboth the potential and challenges that arise with multi-agent interaction and\nvarying conversational paradigms, providing insights into how future research\ncould improve the efficiency, performance, and safety of multi-agent LLMs."
                },
                "authors": [
                    {
                        "name": "Jonas Becker"
                    }
                ],
                "author_detail": {
                    "name": "Jonas Becker"
                },
                "author": "Jonas Becker",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.22932v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.22932v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.22916v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.22916v1",
                "updated": "2024-10-30T11:14:33Z",
                "updated_parsed": [
                    2024,
                    10,
                    30,
                    11,
                    14,
                    33,
                    2,
                    304,
                    0
                ],
                "published": "2024-10-30T11:14:33Z",
                "published_parsed": [
                    2024,
                    10,
                    30,
                    11,
                    14,
                    33,
                    2,
                    304,
                    0
                ],
                "title": "Explainable Behavior Cloning: Teaching Large Language Model Agents\n  through Learning by Demonstration",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Explainable Behavior Cloning: Teaching Large Language Model Agents\n  through Learning by Demonstration"
                },
                "summary": "Autonomous mobile app interaction has become increasingly important with\ngrowing complexity of mobile applications. Developing intelligent agents that\ncan effectively navigate and interact with mobile apps remains a significant\nchallenge. In this paper, we propose an Explainable Behavior Cloning LLM Agent\n(EBC-LLMAgent), a novel approach that combines large language models (LLMs)\nwith behavior cloning by learning demonstrations to create intelligent and\nexplainable agents for autonomous mobile app interaction. EBC-LLMAgent consists\nof three core modules: Demonstration Encoding, Code Generation, and UI Mapping,\nwhich work synergistically to capture user demonstrations, generate executable\ncodes, and establish accurate correspondence between code and UI elements. We\nintroduce the Behavior Cloning Chain Fusion technique to enhance the\ngeneralization capabilities of the agent. Extensive experiments on five popular\nmobile applications from diverse domains demonstrate the superior performance\nof EBC-LLMAgent, achieving high success rates in task completion, efficient\ngeneralization to unseen scenarios, and the generation of meaningful\nexplanations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Autonomous mobile app interaction has become increasingly important with\ngrowing complexity of mobile applications. Developing intelligent agents that\ncan effectively navigate and interact with mobile apps remains a significant\nchallenge. In this paper, we propose an Explainable Behavior Cloning LLM Agent\n(EBC-LLMAgent), a novel approach that combines large language models (LLMs)\nwith behavior cloning by learning demonstrations to create intelligent and\nexplainable agents for autonomous mobile app interaction. EBC-LLMAgent consists\nof three core modules: Demonstration Encoding, Code Generation, and UI Mapping,\nwhich work synergistically to capture user demonstrations, generate executable\ncodes, and establish accurate correspondence between code and UI elements. We\nintroduce the Behavior Cloning Chain Fusion technique to enhance the\ngeneralization capabilities of the agent. Extensive experiments on five popular\nmobile applications from diverse domains demonstrate the superior performance\nof EBC-LLMAgent, achieving high success rates in task completion, efficient\ngeneralization to unseen scenarios, and the generation of meaningful\nexplanations."
                },
                "authors": [
                    {
                        "name": "Yanchu Guan"
                    },
                    {
                        "name": "Dong Wang"
                    },
                    {
                        "name": "Yan Wang"
                    },
                    {
                        "name": "Haiqing Wang"
                    },
                    {
                        "name": "Renen Sun"
                    },
                    {
                        "name": "Chenyi Zhuang"
                    },
                    {
                        "name": "Jinjie Gu"
                    },
                    {
                        "name": "Zhixuan Chu"
                    }
                ],
                "author_detail": {
                    "name": "Zhixuan Chu"
                },
                "author": "Zhixuan Chu",
                "arxiv_comment": "20 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.22916v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.22916v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.08706v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.08706v2",
                "updated": "2024-10-30T10:30:57Z",
                "updated_parsed": [
                    2024,
                    10,
                    30,
                    10,
                    30,
                    57,
                    2,
                    304,
                    0
                ],
                "published": "2024-09-13T10:48:35Z",
                "published_parsed": [
                    2024,
                    9,
                    13,
                    10,
                    48,
                    35,
                    4,
                    257,
                    0
                ],
                "title": "L3Cube-IndicQuest: A Benchmark Question Answering Dataset for Evaluating\n  Knowledge of LLMs in Indic Context",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "L3Cube-IndicQuest: A Benchmark Question Answering Dataset for Evaluating\n  Knowledge of LLMs in Indic Context"
                },
                "summary": "Large Language Models (LLMs) have made significant progress in incorporating\nIndic languages within multilingual models. However, it is crucial to\nquantitatively assess whether these languages perform comparably to globally\ndominant ones, such as English. Currently, there is a lack of benchmark\ndatasets specifically designed to evaluate the regional knowledge of LLMs in\nvarious Indic languages. In this paper, we present the L3Cube-IndicQuest, a\ngold-standard factual question-answering benchmark dataset designed to evaluate\nhow well multilingual LLMs capture regional knowledge across various Indic\nlanguages. The dataset contains 200 question-answer pairs, each for English and\n19 Indic languages, covering five domains specific to the Indic region. We aim\nfor this dataset to serve as a benchmark, providing ground truth for evaluating\nthe performance of LLMs in understanding and representing knowledge relevant to\nthe Indian context. The IndicQuest can be used for both reference-based\nevaluation and LLM-as-a-judge evaluation. The dataset is shared publicly at\nhttps://github.com/l3cube-pune/indic-nlp .",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have made significant progress in incorporating\nIndic languages within multilingual models. However, it is crucial to\nquantitatively assess whether these languages perform comparably to globally\ndominant ones, such as English. Currently, there is a lack of benchmark\ndatasets specifically designed to evaluate the regional knowledge of LLMs in\nvarious Indic languages. In this paper, we present the L3Cube-IndicQuest, a\ngold-standard factual question-answering benchmark dataset designed to evaluate\nhow well multilingual LLMs capture regional knowledge across various Indic\nlanguages. The dataset contains 200 question-answer pairs, each for English and\n19 Indic languages, covering five domains specific to the Indic region. We aim\nfor this dataset to serve as a benchmark, providing ground truth for evaluating\nthe performance of LLMs in understanding and representing knowledge relevant to\nthe Indian context. The IndicQuest can be used for both reference-based\nevaluation and LLM-as-a-judge evaluation. The dataset is shared publicly at\nhttps://github.com/l3cube-pune/indic-nlp ."
                },
                "authors": [
                    {
                        "name": "Pritika Rohera"
                    },
                    {
                        "name": "Chaitrali Ginimav"
                    },
                    {
                        "name": "Akanksha Salunke"
                    },
                    {
                        "name": "Gayatri Sawant"
                    },
                    {
                        "name": "Raviraj Joshi"
                    }
                ],
                "author_detail": {
                    "name": "Raviraj Joshi"
                },
                "author": "Raviraj Joshi",
                "arxiv_comment": "Accepted at PACLIC 38 (2024)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.08706v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.08706v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.22884v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.22884v1",
                "updated": "2024-10-30T10:25:35Z",
                "updated_parsed": [
                    2024,
                    10,
                    30,
                    10,
                    25,
                    35,
                    2,
                    304,
                    0
                ],
                "published": "2024-10-30T10:25:35Z",
                "published_parsed": [
                    2024,
                    10,
                    30,
                    10,
                    25,
                    35,
                    2,
                    304,
                    0
                ],
                "title": "Stealing User Prompts from Mixture of Experts",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Stealing User Prompts from Mixture of Experts"
                },
                "summary": "Mixture-of-Experts (MoE) models improve the efficiency and scalability of\ndense language models by routing each token to a small number of experts in\neach layer. In this paper, we show how an adversary that can arrange for their\nqueries to appear in the same batch of examples as a victim's queries can\nexploit Expert-Choice-Routing to fully disclose a victim's prompt. We\nsuccessfully demonstrate the effectiveness of this attack on a two-layer\nMixtral model, exploiting the tie-handling behavior of the torch.topk CUDA\nimplementation. Our results show that we can extract the entire prompt using\n$O({VM}^2)$ queries (with vocabulary size $V$ and prompt length $M$) or 100\nqueries on average per token in the setting we consider. This is the first\nattack to exploit architectural flaws for the purpose of extracting user\nprompts, introducing a new class of LLM vulnerabilities.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mixture-of-Experts (MoE) models improve the efficiency and scalability of\ndense language models by routing each token to a small number of experts in\neach layer. In this paper, we show how an adversary that can arrange for their\nqueries to appear in the same batch of examples as a victim's queries can\nexploit Expert-Choice-Routing to fully disclose a victim's prompt. We\nsuccessfully demonstrate the effectiveness of this attack on a two-layer\nMixtral model, exploiting the tie-handling behavior of the torch.topk CUDA\nimplementation. Our results show that we can extract the entire prompt using\n$O({VM}^2)$ queries (with vocabulary size $V$ and prompt length $M$) or 100\nqueries on average per token in the setting we consider. This is the first\nattack to exploit architectural flaws for the purpose of extracting user\nprompts, introducing a new class of LLM vulnerabilities."
                },
                "authors": [
                    {
                        "name": "Itay Yona"
                    },
                    {
                        "name": "Ilia Shumailov"
                    },
                    {
                        "name": "Jamie Hayes"
                    },
                    {
                        "name": "Nicholas Carlini"
                    }
                ],
                "author_detail": {
                    "name": "Nicholas Carlini"
                },
                "author": "Nicholas Carlini",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.22884v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.22884v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.19572v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.19572v3",
                "updated": "2024-10-30T10:15:31Z",
                "updated_parsed": [
                    2024,
                    10,
                    30,
                    10,
                    15,
                    31,
                    2,
                    304,
                    0
                ],
                "published": "2024-10-25T14:07:53Z",
                "published_parsed": [
                    2024,
                    10,
                    25,
                    14,
                    7,
                    53,
                    4,
                    299,
                    0
                ],
                "title": "ChunkRAG: Novel LLM-Chunk Filtering Method for RAG Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ChunkRAG: Novel LLM-Chunk Filtering Method for RAG Systems"
                },
                "summary": "Retrieval-Augmented Generation (RAG) systems using large language models\n(LLMs) often generate inaccurate responses due to the retrieval of irrelevant\nor loosely related information. Existing methods, which operate at the document\nlevel, fail to effectively filter out such content. We propose LLM-driven chunk\nfiltering, ChunkRAG, a framework that enhances RAG systems by evaluating and\nfiltering retrieved information at the chunk level. Our approach employs\nsemantic chunking to divide documents into coherent sections and utilizes\nLLM-based relevance scoring to assess each chunk's alignment with the user's\nquery. By filtering out less pertinent chunks before the generation phase, we\nsignificantly reduce hallucinations and improve factual accuracy. Experiments\nshow that our method outperforms existing RAG models, achieving higher accuracy\non tasks requiring precise information retrieval. This advancement enhances the\nreliability of RAG systems, making them particularly beneficial for\napplications like fact-checking and multi-hop reasoning.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Retrieval-Augmented Generation (RAG) systems using large language models\n(LLMs) often generate inaccurate responses due to the retrieval of irrelevant\nor loosely related information. Existing methods, which operate at the document\nlevel, fail to effectively filter out such content. We propose LLM-driven chunk\nfiltering, ChunkRAG, a framework that enhances RAG systems by evaluating and\nfiltering retrieved information at the chunk level. Our approach employs\nsemantic chunking to divide documents into coherent sections and utilizes\nLLM-based relevance scoring to assess each chunk's alignment with the user's\nquery. By filtering out less pertinent chunks before the generation phase, we\nsignificantly reduce hallucinations and improve factual accuracy. Experiments\nshow that our method outperforms existing RAG models, achieving higher accuracy\non tasks requiring precise information retrieval. This advancement enhances the\nreliability of RAG systems, making them particularly beneficial for\napplications like fact-checking and multi-hop reasoning."
                },
                "authors": [
                    {
                        "name": "Ishneet Sukhvinder Singh"
                    },
                    {
                        "name": "Ritvik Aggarwal"
                    },
                    {
                        "name": "Ibrahim Allahverdiyev"
                    },
                    {
                        "name": "Muhammad Taha"
                    },
                    {
                        "name": "Aslihan Akalin"
                    },
                    {
                        "name": "Kevin Zhu"
                    },
                    {
                        "name": "Sean O'Brien"
                    }
                ],
                "author_detail": {
                    "name": "Sean O'Brien"
                },
                "author": "Sean O'Brien",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.19572v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.19572v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.22874v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.22874v1",
                "updated": "2024-10-30T10:11:53Z",
                "updated_parsed": [
                    2024,
                    10,
                    30,
                    10,
                    11,
                    53,
                    2,
                    304,
                    0
                ],
                "published": "2024-10-30T10:11:53Z",
                "published_parsed": [
                    2024,
                    10,
                    30,
                    10,
                    11,
                    53,
                    2,
                    304,
                    0
                ],
                "title": "Eliciting Critical Reasoning in Retrieval-Augmented Language Models via\n  Contrastive Explanations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Eliciting Critical Reasoning in Retrieval-Augmented Language Models via\n  Contrastive Explanations"
                },
                "summary": "Retrieval-augmented generation (RAG) has emerged as a critical mechanism in\ncontemporary NLP to support Large Language Models(LLMs) in systematically\naccessing richer factual context. However, the integration of RAG mechanisms\nbrings its inherent challenges, as LLMs need to deal with potentially noisy\ncontexts. Recent studies have shown that LLMs still struggle to critically\nanalyse RAG-based in-context information, a limitation that may lead to\nincorrect inferences and hallucinations. In this paper, we investigate how to\nelicit critical reasoning in RAG via contrastive explanations. In particular,\nwe propose Contrastive-RAG (C-RAG), a framework that (i) retrieves relevant\ndocuments given a query, (ii) selects and exemplifies relevant passages, and\n(iii) generates explanations that explicitly contrast the relevance of the\npassages to (iv) support the final answer. We show the impact of C-RAG building\ncontrastive reasoning demonstrations from LLMs to instruct smaller models for\nretrieval-augmented tasks. Extensive experiments demonstrate that C-RAG\nimproves state-of-the-art RAG models while (a) requiring significantly fewer\nprompts and demonstrations and (b) being robust to perturbations in the\nretrieved documents.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Retrieval-augmented generation (RAG) has emerged as a critical mechanism in\ncontemporary NLP to support Large Language Models(LLMs) in systematically\naccessing richer factual context. However, the integration of RAG mechanisms\nbrings its inherent challenges, as LLMs need to deal with potentially noisy\ncontexts. Recent studies have shown that LLMs still struggle to critically\nanalyse RAG-based in-context information, a limitation that may lead to\nincorrect inferences and hallucinations. In this paper, we investigate how to\nelicit critical reasoning in RAG via contrastive explanations. In particular,\nwe propose Contrastive-RAG (C-RAG), a framework that (i) retrieves relevant\ndocuments given a query, (ii) selects and exemplifies relevant passages, and\n(iii) generates explanations that explicitly contrast the relevance of the\npassages to (iv) support the final answer. We show the impact of C-RAG building\ncontrastive reasoning demonstrations from LLMs to instruct smaller models for\nretrieval-augmented tasks. Extensive experiments demonstrate that C-RAG\nimproves state-of-the-art RAG models while (a) requiring significantly fewer\nprompts and demonstrations and (b) being robust to perturbations in the\nretrieved documents."
                },
                "authors": [
                    {
                        "name": "Leonardo Ranaldi"
                    },
                    {
                        "name": "Marco Valentino"
                    },
                    {
                        "name": "Andrè Freitas"
                    }
                ],
                "author_detail": {
                    "name": "Andrè Freitas"
                },
                "author": "Andrè Freitas",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.22874v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.22874v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.05404v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.05404v2",
                "updated": "2024-10-30T10:01:06Z",
                "updated_parsed": [
                    2024,
                    10,
                    30,
                    10,
                    1,
                    6,
                    2,
                    304,
                    0
                ],
                "published": "2024-09-09T08:05:43Z",
                "published_parsed": [
                    2024,
                    9,
                    9,
                    8,
                    5,
                    43,
                    0,
                    253,
                    0
                ],
                "title": "DFabric: Scaling Out Data Parallel Applications with CXL-Ethernet Hybrid\n  Interconnects",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DFabric: Scaling Out Data Parallel Applications with CXL-Ethernet Hybrid\n  Interconnects"
                },
                "summary": "Emerging interconnects, such as CXL and NVLink, have been integrated into the\nintra-host topology to scale more accelerators and facilitate efficient\ncommunication between them, such as GPUs. To keep pace with the accelerator's\ngrowing computing throughput, the interconnect has seen substantial enhancement\nin link bandwidth, e.g., 256GBps for CXL 3.0 links, which surpasses Ethernet\nand InfiniBand network links by an order of magnitude or more. Consequently,\nwhen data-intensive jobs, such as LLM training, scale across multiple hosts\nbeyond the reach limit of the interconnect, the performance is significantly\nhindered by the limiting bandwidth of the network infrastructure. We address\nthe problem by proposing DFabric, a two-tier interconnect architecture. We\naddress the problem by proposing DFabric, a two-tier interconnect architecture.\nFirst, DFabric disaggregates rack's computing units with an interconnect\nfabric, i.e., CXL fabric, which scales at rack-level, so that they can enjoy\nintra-rack efficient interconnecting. Second, DFabric disaggregates NICs from\nhosts, and consolidates them to form a NIC pool with CXL fabric. By providing\nsufficient aggregated capacity comparable to interconnect bandwidth, the NIC\npool bridges efficient communication across racks or beyond the reach limit of\ninterconnect fabric. However, the local memory accessing becomes the bottleneck\nwhen enabling each host to utilize the NIC pool efficiently. To the end,\nDFabric builds a memory pool with sufficient bandwidth by disaggregating host\nlocal memory and adding more memory devices. We have implemented a prototype of\nDFabric that can run applications transparently. We validated its performance\ngain by running various microbenchmarks and compute-intensive applications such\nas DNN and graph.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Emerging interconnects, such as CXL and NVLink, have been integrated into the\nintra-host topology to scale more accelerators and facilitate efficient\ncommunication between them, such as GPUs. To keep pace with the accelerator's\ngrowing computing throughput, the interconnect has seen substantial enhancement\nin link bandwidth, e.g., 256GBps for CXL 3.0 links, which surpasses Ethernet\nand InfiniBand network links by an order of magnitude or more. Consequently,\nwhen data-intensive jobs, such as LLM training, scale across multiple hosts\nbeyond the reach limit of the interconnect, the performance is significantly\nhindered by the limiting bandwidth of the network infrastructure. We address\nthe problem by proposing DFabric, a two-tier interconnect architecture. We\naddress the problem by proposing DFabric, a two-tier interconnect architecture.\nFirst, DFabric disaggregates rack's computing units with an interconnect\nfabric, i.e., CXL fabric, which scales at rack-level, so that they can enjoy\nintra-rack efficient interconnecting. Second, DFabric disaggregates NICs from\nhosts, and consolidates them to form a NIC pool with CXL fabric. By providing\nsufficient aggregated capacity comparable to interconnect bandwidth, the NIC\npool bridges efficient communication across racks or beyond the reach limit of\ninterconnect fabric. However, the local memory accessing becomes the bottleneck\nwhen enabling each host to utilize the NIC pool efficiently. To the end,\nDFabric builds a memory pool with sufficient bandwidth by disaggregating host\nlocal memory and adding more memory devices. We have implemented a prototype of\nDFabric that can run applications transparently. We validated its performance\ngain by running various microbenchmarks and compute-intensive applications such\nas DNN and graph."
                },
                "authors": [
                    {
                        "name": "Xu Zhang"
                    },
                    {
                        "name": "Ke Liu"
                    },
                    {
                        "name": "Yisong Chang"
                    },
                    {
                        "name": "Ke Zhang"
                    },
                    {
                        "name": "Mingyu Chen"
                    }
                ],
                "author_detail": {
                    "name": "Mingyu Chen"
                },
                "author": "Mingyu Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.05404v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.05404v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.ET",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.10248v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.10248v4",
                "updated": "2024-10-30T09:48:52Z",
                "updated_parsed": [
                    2024,
                    10,
                    30,
                    9,
                    48,
                    52,
                    2,
                    304,
                    0
                ],
                "published": "2024-06-08T13:40:38Z",
                "published_parsed": [
                    2024,
                    6,
                    8,
                    13,
                    40,
                    38,
                    5,
                    160,
                    0
                ],
                "title": "On the Worst Prompt Performance of Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "On the Worst Prompt Performance of Large Language Models"
                },
                "summary": "The performance of large language models (LLMs) is acutely sensitive to the\nphrasing of prompts, which raises significant concerns about their reliability\nin real-world scenarios. Existing studies often divide prompts into task-level\ninstructions and case-level inputs and primarily focus on evaluating and\nimproving robustness against variations in tasks-level instructions. However,\nthis setup fails to fully address the diversity of real-world user queries and\nassumes the existence of task-specific datasets. To address these limitations,\nwe introduce RobustAlpacaEval, a new benchmark that consists of semantically\nequivalent case-level queries and emphasizes the importance of using the worst\nprompt performance to gauge the lower bound of model performance. Extensive\nexperiments on RobustAlpacaEval with ChatGPT and six open-source LLMs from the\nLlama, Mistral, and Gemma families uncover substantial variability in model\nperformance; for instance, a difference of 45.48% between the worst and best\nperformance for the Llama-2-70B-chat model, with its worst performance dipping\nas low as 9.38%. We further illustrate the difficulty in identifying the worst\nprompt from both model-agnostic and model-dependent perspectives, emphasizing\nthe absence of a shortcut to characterize the worst prompt. We also attempt to\nenhance the worst prompt performance using existing prompt engineering and\nprompt consistency methods, but find that their impact is limited. These\nfindings underscore the need to create more resilient LLMs that can maintain\nhigh performance across diverse prompts. Data and code are available at\nhttps://github.com/cbwbuaa/On-the-Worst-Prompt- Performance-of-LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The performance of large language models (LLMs) is acutely sensitive to the\nphrasing of prompts, which raises significant concerns about their reliability\nin real-world scenarios. Existing studies often divide prompts into task-level\ninstructions and case-level inputs and primarily focus on evaluating and\nimproving robustness against variations in tasks-level instructions. However,\nthis setup fails to fully address the diversity of real-world user queries and\nassumes the existence of task-specific datasets. To address these limitations,\nwe introduce RobustAlpacaEval, a new benchmark that consists of semantically\nequivalent case-level queries and emphasizes the importance of using the worst\nprompt performance to gauge the lower bound of model performance. Extensive\nexperiments on RobustAlpacaEval with ChatGPT and six open-source LLMs from the\nLlama, Mistral, and Gemma families uncover substantial variability in model\nperformance; for instance, a difference of 45.48% between the worst and best\nperformance for the Llama-2-70B-chat model, with its worst performance dipping\nas low as 9.38%. We further illustrate the difficulty in identifying the worst\nprompt from both model-agnostic and model-dependent perspectives, emphasizing\nthe absence of a shortcut to characterize the worst prompt. We also attempt to\nenhance the worst prompt performance using existing prompt engineering and\nprompt consistency methods, but find that their impact is limited. These\nfindings underscore the need to create more resilient LLMs that can maintain\nhigh performance across diverse prompts. Data and code are available at\nhttps://github.com/cbwbuaa/On-the-Worst-Prompt- Performance-of-LLMs."
                },
                "authors": [
                    {
                        "name": "Bowen Cao"
                    },
                    {
                        "name": "Deng Cai"
                    },
                    {
                        "name": "Zhisong Zhang"
                    },
                    {
                        "name": "Yuexian Zou"
                    },
                    {
                        "name": "Wai Lam"
                    }
                ],
                "author_detail": {
                    "name": "Wai Lam"
                },
                "author": "Wai Lam",
                "arxiv_comment": "Accepted at NeurIPS 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.10248v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.10248v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2311.16466v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2311.16466v3",
                "updated": "2024-10-30T09:29:11Z",
                "updated_parsed": [
                    2024,
                    10,
                    30,
                    9,
                    29,
                    11,
                    2,
                    304,
                    0
                ],
                "published": "2023-11-28T04:07:34Z",
                "published_parsed": [
                    2023,
                    11,
                    28,
                    4,
                    7,
                    34,
                    1,
                    332,
                    0
                ],
                "title": "The Adoption and Efficacy of Large Language Models: Evidence From\n  Consumer Complaints in the Financial Industry",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Adoption and Efficacy of Large Language Models: Evidence From\n  Consumer Complaints in the Financial Industry"
                },
                "summary": "Large Language Models (LLMs) are reshaping consumer decision-making,\nparticularly in communication with firms, yet our understanding of their impact\nremains limited. This research explores the effect of LLMs on consumer\ncomplaints submitted to the Consumer Financial Protection Bureau from 2015 to\n2024, documenting the adoption of LLMs for drafting complaints and evaluating\nthe likelihood of obtaining relief from financial firms. Utilizing a leading AI\ndetection tool, we analyzed over 1 million complaints and identified a\nsignificant increase in LLM usage following the release of ChatGPT. We\nestablish a causal relationship between LLM usage and an increased likelihood\nof obtaining relief by employing instrumental variables to address endogeneity\nin LLM adoption. Experimental data further support this link, demonstrating\nthat LLMs enhance the clarity and persuasiveness of consumer narratives. Our\nfindings suggest that facilitating access to LLMs can help firms better\nunderstand consumer concerns and level the playing field among consumers. This\nunderscores the importance of policies promoting technological accessibility,\nenabling all consumers to effectively voice their concerns.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) are reshaping consumer decision-making,\nparticularly in communication with firms, yet our understanding of their impact\nremains limited. This research explores the effect of LLMs on consumer\ncomplaints submitted to the Consumer Financial Protection Bureau from 2015 to\n2024, documenting the adoption of LLMs for drafting complaints and evaluating\nthe likelihood of obtaining relief from financial firms. Utilizing a leading AI\ndetection tool, we analyzed over 1 million complaints and identified a\nsignificant increase in LLM usage following the release of ChatGPT. We\nestablish a causal relationship between LLM usage and an increased likelihood\nof obtaining relief by employing instrumental variables to address endogeneity\nin LLM adoption. Experimental data further support this link, demonstrating\nthat LLMs enhance the clarity and persuasiveness of consumer narratives. Our\nfindings suggest that facilitating access to LLMs can help firms better\nunderstand consumer concerns and level the playing field among consumers. This\nunderscores the importance of policies promoting technological accessibility,\nenabling all consumers to effectively voice their concerns."
                },
                "authors": [
                    {
                        "name": "Minkyu Shin"
                    },
                    {
                        "name": "Jin Kim"
                    },
                    {
                        "name": "Jiwoong Shin"
                    }
                ],
                "author_detail": {
                    "name": "Jiwoong Shin"
                },
                "author": "Jiwoong Shin",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2311.16466v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2311.16466v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.13185v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.13185v5",
                "updated": "2024-10-30T09:17:59Z",
                "updated_parsed": [
                    2024,
                    10,
                    30,
                    9,
                    17,
                    59,
                    2,
                    304,
                    0
                ],
                "published": "2024-10-17T03:26:37Z",
                "published_parsed": [
                    2024,
                    10,
                    17,
                    3,
                    26,
                    37,
                    3,
                    291,
                    0
                ],
                "title": "Chain of Ideas: Revolutionizing Research Via Novel Idea Development with\n  LLM Agents",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Chain of Ideas: Revolutionizing Research Via Novel Idea Development with\n  LLM Agents"
                },
                "summary": "Effective research ideation is a critical step for scientific research.\nHowever, the exponential increase in scientific literature makes it challenging\nfor researchers to stay current with recent advances and identify meaningful\nresearch directions. Recent developments in large language models~(LLMs)\nsuggest a promising avenue for automating the generation of novel research\nideas. However, existing methods for idea generation either trivially prompt\nLLMs or directly expose LLMs to extensive literature without indicating useful\ninformation. Inspired by the research process of human researchers, we propose\na Chain-of-Ideas~(CoI) agent, an LLM-based agent that organizes relevant\nliterature in a chain structure to effectively mirror the progressive\ndevelopment in a research domain. This organization facilitates LLMs to capture\nthe current advancements in research, thereby enhancing their ideation\ncapabilities. Furthermore, we propose Idea Arena, an evaluation protocol that\ncan comprehensively evaluate idea generation methods from different\nperspectives, aligning closely with the preferences of human researchers.\nExperimental results indicate that the CoI agent consistently outperforms other\nmethods and shows comparable quality as humans in research idea generation.\nMoreover, our CoI agent is budget-friendly, with a minimum cost of \\$0.50 to\ngenerate a candidate idea and its corresponding experimental design.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Effective research ideation is a critical step for scientific research.\nHowever, the exponential increase in scientific literature makes it challenging\nfor researchers to stay current with recent advances and identify meaningful\nresearch directions. Recent developments in large language models~(LLMs)\nsuggest a promising avenue for automating the generation of novel research\nideas. However, existing methods for idea generation either trivially prompt\nLLMs or directly expose LLMs to extensive literature without indicating useful\ninformation. Inspired by the research process of human researchers, we propose\na Chain-of-Ideas~(CoI) agent, an LLM-based agent that organizes relevant\nliterature in a chain structure to effectively mirror the progressive\ndevelopment in a research domain. This organization facilitates LLMs to capture\nthe current advancements in research, thereby enhancing their ideation\ncapabilities. Furthermore, we propose Idea Arena, an evaluation protocol that\ncan comprehensively evaluate idea generation methods from different\nperspectives, aligning closely with the preferences of human researchers.\nExperimental results indicate that the CoI agent consistently outperforms other\nmethods and shows comparable quality as humans in research idea generation.\nMoreover, our CoI agent is budget-friendly, with a minimum cost of \\$0.50 to\ngenerate a candidate idea and its corresponding experimental design."
                },
                "authors": [
                    {
                        "name": "Long Li"
                    },
                    {
                        "name": "Weiwen Xu"
                    },
                    {
                        "name": "Jiayan Guo"
                    },
                    {
                        "name": "Ruochen Zhao"
                    },
                    {
                        "name": "Xingxuan Li"
                    },
                    {
                        "name": "Yuqian Yuan"
                    },
                    {
                        "name": "Boqiang Zhang"
                    },
                    {
                        "name": "Yuming Jiang"
                    },
                    {
                        "name": "Yifei Xin"
                    },
                    {
                        "name": "Ronghao Dang"
                    },
                    {
                        "name": "Deli Zhao"
                    },
                    {
                        "name": "Yu Rong"
                    },
                    {
                        "name": "Tian Feng"
                    },
                    {
                        "name": "Lidong Bing"
                    }
                ],
                "author_detail": {
                    "name": "Lidong Bing"
                },
                "author": "Lidong Bing",
                "arxiv_comment": "10 pages,5 figures, conference",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.13185v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.13185v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.22832v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.22832v1",
                "updated": "2024-10-30T09:15:51Z",
                "updated_parsed": [
                    2024,
                    10,
                    30,
                    9,
                    15,
                    51,
                    2,
                    304,
                    0
                ],
                "published": "2024-10-30T09:15:51Z",
                "published_parsed": [
                    2024,
                    10,
                    30,
                    9,
                    15,
                    51,
                    2,
                    304,
                    0
                ],
                "title": "HijackRAG: Hijacking Attacks against Retrieval-Augmented Large Language\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "HijackRAG: Hijacking Attacks against Retrieval-Augmented Large Language\n  Models"
                },
                "summary": "Retrieval-Augmented Generation (RAG) systems enhance large language models\n(LLMs) by integrating external knowledge, making them adaptable and\ncost-effective for various applications. However, the growing reliance on these\nsystems also introduces potential security risks. In this work, we reveal a\nnovel vulnerability, the retrieval prompt hijack attack (HijackRAG), which\nenables attackers to manipulate the retrieval mechanisms of RAG systems by\ninjecting malicious texts into the knowledge database. When the RAG system\nencounters target questions, it generates the attacker's pre-determined answers\ninstead of the correct ones, undermining the integrity and trustworthiness of\nthe system. We formalize HijackRAG as an optimization problem and propose both\nblack-box and white-box attack strategies tailored to different levels of the\nattacker's knowledge. Extensive experiments on multiple benchmark datasets show\nthat HijackRAG consistently achieves high attack success rates, outperforming\nexisting baseline attacks. Furthermore, we demonstrate that the attack is\ntransferable across different retriever models, underscoring the widespread\nrisk it poses to RAG systems. Lastly, our exploration of various defense\nmechanisms reveals that they are insufficient to counter HijackRAG, emphasizing\nthe urgent need for more robust security measures to protect RAG systems in\nreal-world deployments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Retrieval-Augmented Generation (RAG) systems enhance large language models\n(LLMs) by integrating external knowledge, making them adaptable and\ncost-effective for various applications. However, the growing reliance on these\nsystems also introduces potential security risks. In this work, we reveal a\nnovel vulnerability, the retrieval prompt hijack attack (HijackRAG), which\nenables attackers to manipulate the retrieval mechanisms of RAG systems by\ninjecting malicious texts into the knowledge database. When the RAG system\nencounters target questions, it generates the attacker's pre-determined answers\ninstead of the correct ones, undermining the integrity and trustworthiness of\nthe system. We formalize HijackRAG as an optimization problem and propose both\nblack-box and white-box attack strategies tailored to different levels of the\nattacker's knowledge. Extensive experiments on multiple benchmark datasets show\nthat HijackRAG consistently achieves high attack success rates, outperforming\nexisting baseline attacks. Furthermore, we demonstrate that the attack is\ntransferable across different retriever models, underscoring the widespread\nrisk it poses to RAG systems. Lastly, our exploration of various defense\nmechanisms reveals that they are insufficient to counter HijackRAG, emphasizing\nthe urgent need for more robust security measures to protect RAG systems in\nreal-world deployments."
                },
                "authors": [
                    {
                        "name": "Yucheng Zhang"
                    },
                    {
                        "name": "Qinfeng Li"
                    },
                    {
                        "name": "Tianyu Du"
                    },
                    {
                        "name": "Xuhong Zhang"
                    },
                    {
                        "name": "Xinkui Zhao"
                    },
                    {
                        "name": "Zhengwen Feng"
                    },
                    {
                        "name": "Jianwei Yin"
                    }
                ],
                "author_detail": {
                    "name": "Jianwei Yin"
                },
                "author": "Jianwei Yin",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.22832v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.22832v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.13187v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.13187v2",
                "updated": "2024-10-30T09:10:35Z",
                "updated_parsed": [
                    2024,
                    10,
                    30,
                    9,
                    10,
                    35,
                    2,
                    304,
                    0
                ],
                "published": "2024-10-17T03:32:02Z",
                "published_parsed": [
                    2024,
                    10,
                    17,
                    3,
                    32,
                    2,
                    3,
                    291,
                    0
                ],
                "title": "aiXcoder-7B: A Lightweight and Effective Large Language Model for Code\n  Completion",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "aiXcoder-7B: A Lightweight and Effective Large Language Model for Code\n  Completion"
                },
                "summary": "Large Language Models (LLMs) have been widely used in code completion, and\nresearchers are focusing on scaling up LLMs to improve their accuracy. However,\nlarger LLMs will increase the response time of code completion and decrease the\ndevelopers' productivity. In this paper, we propose a lightweight and effective\nLLM for code completion named aiXcoder-7B. Compared to existing LLMs,\naiXcoder-7B achieves higher code completion accuracy while having smaller\nscales (i.e., 7 billion parameters). We attribute the superiority of\naiXcoder-7B to three key factors: (1) Multi-objective training. We employ three\ntraining objectives, one of which is our proposed Structured Fill-In-the-Middle\n(SFIM). SFIM considers the syntax structures in code and effectively improves\nthe performance of LLMs for code. (2) Diverse data sampling strategies. They\nconsider inter-file relationships and enhance the capability of LLMs in\nunderstanding cross-file contexts. (3) Extensive high-quality data. We\nestablish a rigorous data collection pipeline and consume a total of 1.2\ntrillion unique tokens for training aiXcoder-7B. This vast volume of data\nenables aiXcoder-7B to learn a broad distribution of code. We evaluate\naiXcoder-7B in five popular code completion benchmarks and a new benchmark\ncollected by this paper. The results show that aiXcoder-7B outperforms the\nlatest six LLMs with similar sizes and even surpasses four larger LLMs (e.g.,\nStarCoder2-15B and CodeLlama-34B), positioning aiXcoder-7B as a lightweight and\neffective LLM for academia and industry. Finally, we summarize three valuable\ninsights for helping practitioners train the next generations of LLMs for code.\naiXcoder-7B has been open-souced and gained significant attention. As of the\nsubmission date, aiXcoder-7B has received 2,193 GitHub Stars.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have been widely used in code completion, and\nresearchers are focusing on scaling up LLMs to improve their accuracy. However,\nlarger LLMs will increase the response time of code completion and decrease the\ndevelopers' productivity. In this paper, we propose a lightweight and effective\nLLM for code completion named aiXcoder-7B. Compared to existing LLMs,\naiXcoder-7B achieves higher code completion accuracy while having smaller\nscales (i.e., 7 billion parameters). We attribute the superiority of\naiXcoder-7B to three key factors: (1) Multi-objective training. We employ three\ntraining objectives, one of which is our proposed Structured Fill-In-the-Middle\n(SFIM). SFIM considers the syntax structures in code and effectively improves\nthe performance of LLMs for code. (2) Diverse data sampling strategies. They\nconsider inter-file relationships and enhance the capability of LLMs in\nunderstanding cross-file contexts. (3) Extensive high-quality data. We\nestablish a rigorous data collection pipeline and consume a total of 1.2\ntrillion unique tokens for training aiXcoder-7B. This vast volume of data\nenables aiXcoder-7B to learn a broad distribution of code. We evaluate\naiXcoder-7B in five popular code completion benchmarks and a new benchmark\ncollected by this paper. The results show that aiXcoder-7B outperforms the\nlatest six LLMs with similar sizes and even surpasses four larger LLMs (e.g.,\nStarCoder2-15B and CodeLlama-34B), positioning aiXcoder-7B as a lightweight and\neffective LLM for academia and industry. Finally, we summarize three valuable\ninsights for helping practitioners train the next generations of LLMs for code.\naiXcoder-7B has been open-souced and gained significant attention. As of the\nsubmission date, aiXcoder-7B has received 2,193 GitHub Stars."
                },
                "authors": [
                    {
                        "name": "Siyuan Jiang"
                    },
                    {
                        "name": "Jia Li"
                    },
                    {
                        "name": "He Zong"
                    },
                    {
                        "name": "Huanyu Liu"
                    },
                    {
                        "name": "Hao Zhu"
                    },
                    {
                        "name": "Shukai Hu"
                    },
                    {
                        "name": "Erlu Li"
                    },
                    {
                        "name": "Jiazheng Ding"
                    },
                    {
                        "name": "Yu Han"
                    },
                    {
                        "name": "Wei Ning"
                    },
                    {
                        "name": "Gen Wang"
                    },
                    {
                        "name": "Yihong Dong"
                    },
                    {
                        "name": "Kechi Zhang"
                    },
                    {
                        "name": "Ge Li"
                    }
                ],
                "author_detail": {
                    "name": "Ge Li"
                },
                "author": "Ge Li",
                "arxiv_comment": "aiXcoder-7B is available at\n  https://github.com/aixcoder-plugin/aiXcoder-7B",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.13187v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.13187v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.22821v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.22821v1",
                "updated": "2024-10-30T08:57:59Z",
                "updated_parsed": [
                    2024,
                    10,
                    30,
                    8,
                    57,
                    59,
                    2,
                    304,
                    0
                ],
                "published": "2024-10-30T08:57:59Z",
                "published_parsed": [
                    2024,
                    10,
                    30,
                    8,
                    57,
                    59,
                    2,
                    304,
                    0
                ],
                "title": "EvoCodeBench: An Evolving Code Generation Benchmark with Domain-Specific\n  Evaluations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "EvoCodeBench: An Evolving Code Generation Benchmark with Domain-Specific\n  Evaluations"
                },
                "summary": "How to evaluate Large Language Models (LLMs) in code generation remains an\nopen question. Existing benchmarks have two limitations - data leakage and lack\nof domain-specific evaluation. The former hurts the fairness of benchmarks, and\nthe latter hinders practitioners from selecting superior LLMs for specific\nprogramming domains. To address these two limitations, we propose a new\nbenchmark - EvoCodeBench, which has the following advances: (1) Evolving data.\nEvoCodeBench will be dynamically updated every period (e.g., 6 months) to avoid\ndata leakage. This paper releases the first version - EvoCodeBench-2403,\ncontaining 275 samples from 25 repositories. (2) A domain taxonomy and domain\nlabels. Based on the statistics of open-source communities, we design a\nprogramming domain taxonomy consisting of 10 popular domains. Based on the\ntaxonomy, we annotate each sample in EvoCodeBench with a domain label. (3)\nDomain-specific evaluations. Besides the Pass@k, we compute the Domain-Specific\nImprovement (DSI) and define LLMs' comfort and strange domains. These\nevaluations help practitioners select superior LLMs in specific domains and\ndiscover the shortcomings of existing LLMs. We evaluate 8 popular LLMs (e.g.,\ngpt-4, DeepSeek Coder) on EvoCodeBench and summarize some insights.\nEvoCodeBench reveals the actual abilities of these LLMs in real-world\nrepositories. For example, the highest Pass@1 of gpt-4 on EvoCodeBench-2403 is\nonly 20.74%. Besides, we evaluate LLMs in different domains and discover their\ncomfort and strange domains. For example, gpt-4 performs best in most domains\nbut falls behind others in the Internet domain. StarCoder 2-15B unexpectedly\nperforms well in the Database domain and even outperforms 33B LLMs.\nEvoCodeBench has been released.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "How to evaluate Large Language Models (LLMs) in code generation remains an\nopen question. Existing benchmarks have two limitations - data leakage and lack\nof domain-specific evaluation. The former hurts the fairness of benchmarks, and\nthe latter hinders practitioners from selecting superior LLMs for specific\nprogramming domains. To address these two limitations, we propose a new\nbenchmark - EvoCodeBench, which has the following advances: (1) Evolving data.\nEvoCodeBench will be dynamically updated every period (e.g., 6 months) to avoid\ndata leakage. This paper releases the first version - EvoCodeBench-2403,\ncontaining 275 samples from 25 repositories. (2) A domain taxonomy and domain\nlabels. Based on the statistics of open-source communities, we design a\nprogramming domain taxonomy consisting of 10 popular domains. Based on the\ntaxonomy, we annotate each sample in EvoCodeBench with a domain label. (3)\nDomain-specific evaluations. Besides the Pass@k, we compute the Domain-Specific\nImprovement (DSI) and define LLMs' comfort and strange domains. These\nevaluations help practitioners select superior LLMs in specific domains and\ndiscover the shortcomings of existing LLMs. We evaluate 8 popular LLMs (e.g.,\ngpt-4, DeepSeek Coder) on EvoCodeBench and summarize some insights.\nEvoCodeBench reveals the actual abilities of these LLMs in real-world\nrepositories. For example, the highest Pass@1 of gpt-4 on EvoCodeBench-2403 is\nonly 20.74%. Besides, we evaluate LLMs in different domains and discover their\ncomfort and strange domains. For example, gpt-4 performs best in most domains\nbut falls behind others in the Internet domain. StarCoder 2-15B unexpectedly\nperforms well in the Database domain and even outperforms 33B LLMs.\nEvoCodeBench has been released."
                },
                "authors": [
                    {
                        "name": "Jia Li"
                    },
                    {
                        "name": "Ge Li"
                    },
                    {
                        "name": "Xuanming Zhang"
                    },
                    {
                        "name": "Yunfei Zhao"
                    },
                    {
                        "name": "Yihong Dong"
                    },
                    {
                        "name": "Zhi Jin"
                    },
                    {
                        "name": "Binhua Li"
                    },
                    {
                        "name": "Fei Huang"
                    },
                    {
                        "name": "Yongbin Li"
                    }
                ],
                "author_detail": {
                    "name": "Yongbin Li"
                },
                "author": "Yongbin Li",
                "arxiv_comment": "Accepted by the 38th Conference on Neural Information Processing\n  Systems (NeurIPS 2024)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.22821v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.22821v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.20288v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.20288v3",
                "updated": "2024-10-30T08:56:08Z",
                "updated_parsed": [
                    2024,
                    10,
                    30,
                    8,
                    56,
                    8,
                    2,
                    304,
                    0
                ],
                "published": "2024-09-30T13:44:00Z",
                "published_parsed": [
                    2024,
                    9,
                    30,
                    13,
                    44,
                    0,
                    0,
                    274,
                    0
                ],
                "title": "LexEval: A Comprehensive Chinese Legal Benchmark for Evaluating Large\n  Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LexEval: A Comprehensive Chinese Legal Benchmark for Evaluating Large\n  Language Models"
                },
                "summary": "Large language models (LLMs) have made significant progress in natural\nlanguage processing tasks and demonstrate considerable potential in the legal\ndomain. However, legal applications demand high standards of accuracy,\nreliability, and fairness. Applying existing LLMs to legal systems without\ncareful evaluation of their potential and limitations could pose significant\nrisks in legal practice. To this end, we introduce a standardized comprehensive\nChinese legal benchmark LexEval. This benchmark is notable in the following\nthree aspects: (1) Ability Modeling: We propose a new taxonomy of legal\ncognitive abilities to organize different tasks. (2) Scale: To our knowledge,\nLexEval is currently the largest Chinese legal evaluation dataset, comprising\n23 tasks and 14,150 questions. (3) Data: we utilize formatted existing\ndatasets, exam datasets and newly annotated datasets by legal experts to\ncomprehensively evaluate the various capabilities of LLMs. LexEval not only\nfocuses on the ability of LLMs to apply fundamental legal knowledge but also\ndedicates efforts to examining the ethical issues involved in their\napplication. We evaluated 38 open-source and commercial LLMs and obtained some\ninteresting findings. The experiments and findings offer valuable insights into\nthe challenges and potential solutions for developing Chinese legal systems and\nLLM evaluation pipelines. The LexEval dataset and leaderboard are publicly\navailable at \\url{https://github.com/CSHaitao/LexEval} and will be continuously\nupdated.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have made significant progress in natural\nlanguage processing tasks and demonstrate considerable potential in the legal\ndomain. However, legal applications demand high standards of accuracy,\nreliability, and fairness. Applying existing LLMs to legal systems without\ncareful evaluation of their potential and limitations could pose significant\nrisks in legal practice. To this end, we introduce a standardized comprehensive\nChinese legal benchmark LexEval. This benchmark is notable in the following\nthree aspects: (1) Ability Modeling: We propose a new taxonomy of legal\ncognitive abilities to organize different tasks. (2) Scale: To our knowledge,\nLexEval is currently the largest Chinese legal evaluation dataset, comprising\n23 tasks and 14,150 questions. (3) Data: we utilize formatted existing\ndatasets, exam datasets and newly annotated datasets by legal experts to\ncomprehensively evaluate the various capabilities of LLMs. LexEval not only\nfocuses on the ability of LLMs to apply fundamental legal knowledge but also\ndedicates efforts to examining the ethical issues involved in their\napplication. We evaluated 38 open-source and commercial LLMs and obtained some\ninteresting findings. The experiments and findings offer valuable insights into\nthe challenges and potential solutions for developing Chinese legal systems and\nLLM evaluation pipelines. The LexEval dataset and leaderboard are publicly\navailable at \\url{https://github.com/CSHaitao/LexEval} and will be continuously\nupdated."
                },
                "authors": [
                    {
                        "name": "Haitao Li"
                    },
                    {
                        "name": "You Chen"
                    },
                    {
                        "name": "Qingyao Ai"
                    },
                    {
                        "name": "Yueyue Wu"
                    },
                    {
                        "name": "Ruizhe Zhang"
                    },
                    {
                        "name": "Yiqun Liu"
                    }
                ],
                "author_detail": {
                    "name": "Yiqun Liu"
                },
                "author": "Yiqun Liu",
                "arxiv_comment": "NeurIPs 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.20288v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.20288v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2403.02745v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2403.02745v2",
                "updated": "2024-10-30T08:54:38Z",
                "updated_parsed": [
                    2024,
                    10,
                    30,
                    8,
                    54,
                    38,
                    2,
                    304,
                    0
                ],
                "published": "2024-03-05T07:58:12Z",
                "published_parsed": [
                    2024,
                    3,
                    5,
                    7,
                    58,
                    12,
                    1,
                    65,
                    0
                ],
                "title": "CURATRON: Complete and Robust Preference Data for Rigorous Alignment of\n  Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CURATRON: Complete and Robust Preference Data for Rigorous Alignment of\n  Large Language Models"
                },
                "summary": "This paper addresses the challenges of aligning large language models (LLMs)\nwith human values via preference learning (PL), focusing on incomplete and\ncorrupted data in preference datasets. We propose a novel method for robustly\nand completely recalibrating values within these datasets to enhance LLMs'\nresilience against the issues. In particular, we devise a guaranteed polynomial\ntime ranking algorithm that robustifies several existing models, such as the\nclassic Bradley-Terry-Luce (BTL) (Bradley and Terry, 1952) model and certain\ngeneralizations of it. To the best of our knowledge, our present work is the\nfirst to propose an algorithm that provably recovers an $\\epsilon$-optimal\nranking with high probability while allowing as large as $O(n)$ perturbed\npairwise comparison results per model response. Furthermore, we show robust\nrecovery results in the partially observed setting. Our experiments confirm\nthat our algorithms handle adversarial noise and unobserved comparisons well in\nboth general and LLM preference dataset settings. This work contributes to the\ndevelopment and scaling of more reliable and ethically aligned AI models by\nequipping the dataset curation pipeline with the ability to handle missing and\nmaliciously manipulated inputs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper addresses the challenges of aligning large language models (LLMs)\nwith human values via preference learning (PL), focusing on incomplete and\ncorrupted data in preference datasets. We propose a novel method for robustly\nand completely recalibrating values within these datasets to enhance LLMs'\nresilience against the issues. In particular, we devise a guaranteed polynomial\ntime ranking algorithm that robustifies several existing models, such as the\nclassic Bradley-Terry-Luce (BTL) (Bradley and Terry, 1952) model and certain\ngeneralizations of it. To the best of our knowledge, our present work is the\nfirst to propose an algorithm that provably recovers an $\\epsilon$-optimal\nranking with high probability while allowing as large as $O(n)$ perturbed\npairwise comparison results per model response. Furthermore, we show robust\nrecovery results in the partially observed setting. Our experiments confirm\nthat our algorithms handle adversarial noise and unobserved comparisons well in\nboth general and LLM preference dataset settings. This work contributes to the\ndevelopment and scaling of more reliable and ethically aligned AI models by\nequipping the dataset curation pipeline with the ability to handle missing and\nmaliciously manipulated inputs."
                },
                "authors": [
                    {
                        "name": "Son The Nguyen"
                    },
                    {
                        "name": "Niranjan Uma Naresh"
                    },
                    {
                        "name": "Theja Tulabandhula"
                    }
                ],
                "author_detail": {
                    "name": "Theja Tulabandhula"
                },
                "author": "Theja Tulabandhula",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2403.02745v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2403.02745v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.22818v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.22818v1",
                "updated": "2024-10-30T08:53:33Z",
                "updated_parsed": [
                    2024,
                    10,
                    30,
                    8,
                    53,
                    33,
                    2,
                    304,
                    0
                ],
                "published": "2024-10-30T08:53:33Z",
                "published_parsed": [
                    2024,
                    10,
                    30,
                    8,
                    53,
                    33,
                    2,
                    304,
                    0
                ],
                "title": "A test-free semantic mistakes localization framework in Neural Code\n  Translation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A test-free semantic mistakes localization framework in Neural Code\n  Translation"
                },
                "summary": "In the task of code translation, neural network-based models have been shown\nto frequently produce semantically erroneous code that deviates from the\noriginal logic of the source code. This issue persists even with advanced large\nmodels. Although a recent approach proposed using test cases to identify these\nsemantic errors, it relies heavily on the quality of the test cases and is not\napplicable to code snippets without test cases in real-world scenarios.\nTherefore, We present EISP, a static analysis framework based on the Large\nLanguage Model (LLM).First, the framework generates a semantic mapping between\nsource code and translated code. Next, each sub-code fragment is identified by\nrecursively traversing the abstract syntax tree of the source code, and its\ncorresponding translated code fragment is found through the semantic mapping.\nFinally, EISP connects each pair of sub-code fragments with fine-grained\nknowledge hints through an AI chain to assist LLMs in discovering semantic\nmistakes in the translated code. In our benchmark evaluation, the EISP\nframework, based on GPT-4o mini, achieved an accuracy of 82.3\\%, representing a\n20.3\\% improvement over baseline methods using the same base model, and a 7.4\\%\nimprovement compared to dynamic analysis methods that require test cases and\nmanual intervention. To our knowledge, EISP is the first tool to locate\nsemantic errors in translated code without test cases or compilable code. This\ninnovative tool provides the software engineering community with a new way to\ndeal with code fragments without test cases.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In the task of code translation, neural network-based models have been shown\nto frequently produce semantically erroneous code that deviates from the\noriginal logic of the source code. This issue persists even with advanced large\nmodels. Although a recent approach proposed using test cases to identify these\nsemantic errors, it relies heavily on the quality of the test cases and is not\napplicable to code snippets without test cases in real-world scenarios.\nTherefore, We present EISP, a static analysis framework based on the Large\nLanguage Model (LLM).First, the framework generates a semantic mapping between\nsource code and translated code. Next, each sub-code fragment is identified by\nrecursively traversing the abstract syntax tree of the source code, and its\ncorresponding translated code fragment is found through the semantic mapping.\nFinally, EISP connects each pair of sub-code fragments with fine-grained\nknowledge hints through an AI chain to assist LLMs in discovering semantic\nmistakes in the translated code. In our benchmark evaluation, the EISP\nframework, based on GPT-4o mini, achieved an accuracy of 82.3\\%, representing a\n20.3\\% improvement over baseline methods using the same base model, and a 7.4\\%\nimprovement compared to dynamic analysis methods that require test cases and\nmanual intervention. To our knowledge, EISP is the first tool to locate\nsemantic errors in translated code without test cases or compilable code. This\ninnovative tool provides the software engineering community with a new way to\ndeal with code fragments without test cases."
                },
                "authors": [
                    {
                        "name": "Lei Chen"
                    },
                    {
                        "name": "Sai Zhang"
                    },
                    {
                        "name": "Fangzhou Xu"
                    },
                    {
                        "name": "Zhenchang Xing"
                    },
                    {
                        "name": "Liang Wan"
                    },
                    {
                        "name": "Xiaowang Zhang"
                    },
                    {
                        "name": "Zhiyong Feng"
                    }
                ],
                "author_detail": {
                    "name": "Zhiyong Feng"
                },
                "author": "Zhiyong Feng",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.22818v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.22818v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2305.14434v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2305.14434v2",
                "updated": "2024-10-30T08:51:59Z",
                "updated_parsed": [
                    2024,
                    10,
                    30,
                    8,
                    51,
                    59,
                    2,
                    304,
                    0
                ],
                "published": "2023-05-23T18:01:49Z",
                "published_parsed": [
                    2023,
                    5,
                    23,
                    18,
                    1,
                    49,
                    1,
                    143,
                    0
                ],
                "title": "Domain-Expanded ASTE: Rethinking Generalization in Aspect Sentiment\n  Triplet Extraction",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Domain-Expanded ASTE: Rethinking Generalization in Aspect Sentiment\n  Triplet Extraction"
                },
                "summary": "Aspect Sentiment Triplet Extraction (ASTE) is a challenging task in sentiment\nanalysis, aiming to provide fine-grained insights into human sentiments.\nHowever, existing benchmarks are limited to two domains and do not evaluate\nmodel performance on unseen domains, raising concerns about the generalization\nof proposed methods. Furthermore, it remains unclear if large language models\n(LLMs) can effectively handle complex sentiment tasks like ASTE. In this work,\nwe address the issue of generalization in ASTE from both a benchmarking and\nmodeling perspective. We introduce a domain-expanded benchmark by annotating\nsamples from diverse domains, enabling evaluation of models in both in-domain\nand out-of-domain settings. Additionally, we propose CASE, a simple and\neffective decoding strategy that enhances trustworthiness and performance of\nLLMs in ASTE. Through comprehensive experiments involving multiple tasks,\nsettings, and models, we demonstrate that CASE can serve as a general decoding\nstrategy for complex sentiment tasks. By expanding the scope of evaluation and\nproviding a more reliable decoding strategy, we aim to inspire the research\ncommunity to reevaluate the generalizability of benchmarks and models for ASTE.\nOur code, data, and models are available at\nhttps://github.com/DAMO-NLP-SG/domain-expanded-aste.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Aspect Sentiment Triplet Extraction (ASTE) is a challenging task in sentiment\nanalysis, aiming to provide fine-grained insights into human sentiments.\nHowever, existing benchmarks are limited to two domains and do not evaluate\nmodel performance on unseen domains, raising concerns about the generalization\nof proposed methods. Furthermore, it remains unclear if large language models\n(LLMs) can effectively handle complex sentiment tasks like ASTE. In this work,\nwe address the issue of generalization in ASTE from both a benchmarking and\nmodeling perspective. We introduce a domain-expanded benchmark by annotating\nsamples from diverse domains, enabling evaluation of models in both in-domain\nand out-of-domain settings. Additionally, we propose CASE, a simple and\neffective decoding strategy that enhances trustworthiness and performance of\nLLMs in ASTE. Through comprehensive experiments involving multiple tasks,\nsettings, and models, we demonstrate that CASE can serve as a general decoding\nstrategy for complex sentiment tasks. By expanding the scope of evaluation and\nproviding a more reliable decoding strategy, we aim to inspire the research\ncommunity to reevaluate the generalizability of benchmarks and models for ASTE.\nOur code, data, and models are available at\nhttps://github.com/DAMO-NLP-SG/domain-expanded-aste."
                },
                "authors": [
                    {
                        "name": "Yew Ken Chia"
                    },
                    {
                        "name": "Hui Chen"
                    },
                    {
                        "name": "Wei Han"
                    },
                    {
                        "name": "Guizhen Chen"
                    },
                    {
                        "name": "Sharifah Mahani Aljunied"
                    },
                    {
                        "name": "Soujanya Poria"
                    },
                    {
                        "name": "Lidong Bing"
                    }
                ],
                "author_detail": {
                    "name": "Lidong Bing"
                },
                "author": "Lidong Bing",
                "arxiv_comment": "EMNLP 2024 SiCon",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2305.14434v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2305.14434v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.22815v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.22815v1",
                "updated": "2024-10-30T08:48:21Z",
                "updated_parsed": [
                    2024,
                    10,
                    30,
                    8,
                    48,
                    21,
                    2,
                    304,
                    0
                ],
                "published": "2024-10-30T08:48:21Z",
                "published_parsed": [
                    2024,
                    10,
                    30,
                    8,
                    48,
                    21,
                    2,
                    304,
                    0
                ],
                "title": "Towards Robust and Efficient Federated Low-Rank Adaptation with\n  Heterogeneous Clients",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Towards Robust and Efficient Federated Low-Rank Adaptation with\n  Heterogeneous Clients"
                },
                "summary": "Federated fine-tuning for Large Language Models (LLMs) has recently gained\nattention due to the heavy communication overhead of transmitting large model\nupdates. Low Rank Adaptation (LoRA) has been proposed as a solution, yet its\napplication in federated learning is complicated by discordance in aggregation.\nExisting methods addressing this discordance often suffer from performance\ndegradation at low ranks in heterogeneous data settings. In response, we\nintroduce LoRA-A2 (Low Rank Adaptation with Alternating freeze and Adaptive\nrank selection), which demonstrates robustness in challenging settings with low\nranks and high data heterogeneity. Our experimental findings reveal that\nLoRA-A2 maintains performance even under extreme heterogeneity and low rank\nconditions, achieving up to a 99.8% reduction in uploaded parameters compared\nto full fine-tuning without compromising performance. This adaptive mechanism\nboosts robustness and communication efficiency in federated fine-tuning,\nenabling the practical deployment of LLMs in resource-constrained environments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Federated fine-tuning for Large Language Models (LLMs) has recently gained\nattention due to the heavy communication overhead of transmitting large model\nupdates. Low Rank Adaptation (LoRA) has been proposed as a solution, yet its\napplication in federated learning is complicated by discordance in aggregation.\nExisting methods addressing this discordance often suffer from performance\ndegradation at low ranks in heterogeneous data settings. In response, we\nintroduce LoRA-A2 (Low Rank Adaptation with Alternating freeze and Adaptive\nrank selection), which demonstrates robustness in challenging settings with low\nranks and high data heterogeneity. Our experimental findings reveal that\nLoRA-A2 maintains performance even under extreme heterogeneity and low rank\nconditions, achieving up to a 99.8% reduction in uploaded parameters compared\nto full fine-tuning without compromising performance. This adaptive mechanism\nboosts robustness and communication efficiency in federated fine-tuning,\nenabling the practical deployment of LLMs in resource-constrained environments."
                },
                "authors": [
                    {
                        "name": "Jabin Koo"
                    },
                    {
                        "name": "Minwoo Jang"
                    },
                    {
                        "name": "Jungseul Ok"
                    }
                ],
                "author_detail": {
                    "name": "Jungseul Ok"
                },
                "author": "Jungseul Ok",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.22815v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.22815v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.22809v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.22809v1",
                "updated": "2024-10-30T08:41:13Z",
                "updated_parsed": [
                    2024,
                    10,
                    30,
                    8,
                    41,
                    13,
                    2,
                    304,
                    0
                ],
                "published": "2024-10-30T08:41:13Z",
                "published_parsed": [
                    2024,
                    10,
                    30,
                    8,
                    41,
                    13,
                    2,
                    304,
                    0
                ],
                "title": "Causality-Enhanced Behavior Sequence Modeling in LLMs for Personalized\n  Recommendation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Causality-Enhanced Behavior Sequence Modeling in LLMs for Personalized\n  Recommendation"
                },
                "summary": "Recent advancements in recommender systems have focused on leveraging Large\nLanguage Models (LLMs) to improve user preference modeling, yielding promising\noutcomes. However, current LLM-based approaches struggle to fully leverage user\nbehavior sequences, resulting in suboptimal preference modeling for\npersonalized recommendations. In this study, we propose a novel Counterfactual\nFine-Tuning (CFT) method to address this issue by explicitly emphasizing the\nrole of behavior sequences when generating recommendations. Specifically, we\nemploy counterfactual reasoning to identify the causal effects of behavior\nsequences on model output and introduce a task that directly fits the\nground-truth labels based on these effects, achieving the goal of explicit\nemphasis. Additionally, we develop a token-level weighting mechanism to adjust\nthe emphasis strength for different item tokens, reflecting the diminishing\ninfluence of behavior sequences from earlier to later tokens during predicting\nan item. Extensive experiments on real-world datasets demonstrate that CFT\neffectively improves behavior sequence modeling. Our codes are available at\nhttps://github.com/itsmeyjt/CFT.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advancements in recommender systems have focused on leveraging Large\nLanguage Models (LLMs) to improve user preference modeling, yielding promising\noutcomes. However, current LLM-based approaches struggle to fully leverage user\nbehavior sequences, resulting in suboptimal preference modeling for\npersonalized recommendations. In this study, we propose a novel Counterfactual\nFine-Tuning (CFT) method to address this issue by explicitly emphasizing the\nrole of behavior sequences when generating recommendations. Specifically, we\nemploy counterfactual reasoning to identify the causal effects of behavior\nsequences on model output and introduce a task that directly fits the\nground-truth labels based on these effects, achieving the goal of explicit\nemphasis. Additionally, we develop a token-level weighting mechanism to adjust\nthe emphasis strength for different item tokens, reflecting the diminishing\ninfluence of behavior sequences from earlier to later tokens during predicting\nan item. Extensive experiments on real-world datasets demonstrate that CFT\neffectively improves behavior sequence modeling. Our codes are available at\nhttps://github.com/itsmeyjt/CFT."
                },
                "authors": [
                    {
                        "name": "Yang Zhang"
                    },
                    {
                        "name": "Juntao You"
                    },
                    {
                        "name": "Yimeng Bai"
                    },
                    {
                        "name": "Jizhi Zhang"
                    },
                    {
                        "name": "Keqin Bao"
                    },
                    {
                        "name": "Wenjie Wang"
                    },
                    {
                        "name": "Tat-Seng Chua"
                    }
                ],
                "author_detail": {
                    "name": "Tat-Seng Chua"
                },
                "author": "Tat-Seng Chua",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.22809v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.22809v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.22793v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.22793v2",
                "updated": "2024-10-31T07:20:35Z",
                "updated_parsed": [
                    2024,
                    10,
                    31,
                    7,
                    20,
                    35,
                    3,
                    305,
                    0
                ],
                "published": "2024-10-30T08:17:10Z",
                "published_parsed": [
                    2024,
                    10,
                    30,
                    8,
                    17,
                    10,
                    2,
                    304,
                    0
                ],
                "title": "Less is More: DocString Compression in Code Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Less is More: DocString Compression in Code Generation"
                },
                "summary": "The widespread use of Large Language Models (LLMs) in software engineering\nhas intensified the need for improved model and resource efficiency. In\nparticular, for neural code generation, LLMs are used to translate\nfunction/method signature and DocString to executable code. DocStrings which\ncapture user re quirements for the code and used as the prompt for LLMs, often\ncontains redundant information. Recent advancements in prompt compression have\nshown promising results in Natural Language Processing (NLP), but their\napplicability to code generation remains uncertain. Our empirical study show\nthat the state-of-the-art prompt compression methods achieve only about 10%\nreduction, as further reductions would cause significant performance\ndegradation. In our study, we propose a novel compression method, ShortenDoc,\ndedicated to DocString compression for code generation. Our extensive\nexperiments on six code generation datasets, five open-source LLMs (1B to 10B\nparameters), and one closed-source LLM GPT-4o confirm that ShortenDoc achieves\n25-40% compression while preserving the quality of generated code,\noutperforming other baseline methods at similar compression levels. The benefit\nof this research is to improve efficiency and reduce the cost while maintaining\nthe quality of the generated code, especially when calling third-party APIs,\nand is able to reduce the token processing cost by 25-40%.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The widespread use of Large Language Models (LLMs) in software engineering\nhas intensified the need for improved model and resource efficiency. In\nparticular, for neural code generation, LLMs are used to translate\nfunction/method signature and DocString to executable code. DocStrings which\ncapture user re quirements for the code and used as the prompt for LLMs, often\ncontains redundant information. Recent advancements in prompt compression have\nshown promising results in Natural Language Processing (NLP), but their\napplicability to code generation remains uncertain. Our empirical study show\nthat the state-of-the-art prompt compression methods achieve only about 10%\nreduction, as further reductions would cause significant performance\ndegradation. In our study, we propose a novel compression method, ShortenDoc,\ndedicated to DocString compression for code generation. Our extensive\nexperiments on six code generation datasets, five open-source LLMs (1B to 10B\nparameters), and one closed-source LLM GPT-4o confirm that ShortenDoc achieves\n25-40% compression while preserving the quality of generated code,\noutperforming other baseline methods at similar compression levels. The benefit\nof this research is to improve efficiency and reduce the cost while maintaining\nthe quality of the generated code, especially when calling third-party APIs,\nand is able to reduce the token processing cost by 25-40%."
                },
                "authors": [
                    {
                        "name": "Guang Yang"
                    },
                    {
                        "name": "Yu Zhou"
                    },
                    {
                        "name": "Wei Cheng"
                    },
                    {
                        "name": "Xiangyu Zhang"
                    },
                    {
                        "name": "Xiang Chen"
                    },
                    {
                        "name": "Terry Yue Zhuo"
                    },
                    {
                        "name": "Ke Liu"
                    },
                    {
                        "name": "Xin Zhou"
                    },
                    {
                        "name": "David Lo"
                    },
                    {
                        "name": "Taolue Chen"
                    }
                ],
                "author_detail": {
                    "name": "Taolue Chen"
                },
                "author": "Taolue Chen",
                "arxiv_comment": "UNDER REVIEW",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.22793v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.22793v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.02351v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.02351v2",
                "updated": "2024-10-30T07:57:46Z",
                "updated_parsed": [
                    2024,
                    10,
                    30,
                    7,
                    57,
                    46,
                    2,
                    304,
                    0
                ],
                "published": "2024-07-02T15:16:46Z",
                "published_parsed": [
                    2024,
                    7,
                    2,
                    15,
                    16,
                    46,
                    1,
                    184,
                    0
                ],
                "title": "Generative Large Language Models in Automated Fact-Checking: A Survey",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Generative Large Language Models in Automated Fact-Checking: A Survey"
                },
                "summary": "The dissemination of false information on online platforms presents a serious\nsocietal challenge. While manual fact-checking remains crucial, Large Language\nModels (LLMs) offer promising opportunities to support fact-checkers with their\nvast knowledge and advanced reasoning capabilities. This survey explores the\napplication of generative LLMs in fact-checking, highlighting various\napproaches and techniques for prompting or fine-tuning these models. By\nproviding an overview of existing methods and their limitations, the survey\naims to enhance the understanding of how LLMs can be used in fact-checking and\nto facilitate further progress in their integration into the fact-checking\nprocess.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The dissemination of false information on online platforms presents a serious\nsocietal challenge. While manual fact-checking remains crucial, Large Language\nModels (LLMs) offer promising opportunities to support fact-checkers with their\nvast knowledge and advanced reasoning capabilities. This survey explores the\napplication of generative LLMs in fact-checking, highlighting various\napproaches and techniques for prompting or fine-tuning these models. By\nproviding an overview of existing methods and their limitations, the survey\naims to enhance the understanding of how LLMs can be used in fact-checking and\nto facilitate further progress in their integration into the fact-checking\nprocess."
                },
                "authors": [
                    {
                        "name": "Ivan Vykopal"
                    },
                    {
                        "name": "Matúš Pikuliak"
                    },
                    {
                        "name": "Simon Ostermann"
                    },
                    {
                        "name": "Marián Šimko"
                    }
                ],
                "author_detail": {
                    "name": "Marián Šimko"
                },
                "author": "Marián Šimko",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.02351v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.02351v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.12382v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.12382v2",
                "updated": "2024-10-30T07:57:14Z",
                "updated_parsed": [
                    2024,
                    10,
                    30,
                    7,
                    57,
                    14,
                    2,
                    304,
                    0
                ],
                "published": "2024-06-18T08:14:28Z",
                "published_parsed": [
                    2024,
                    6,
                    18,
                    8,
                    14,
                    28,
                    1,
                    170,
                    0
                ],
                "title": "From Instance Training to Instruction Learning: Task Adapters Generation\n  from Instructions",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "From Instance Training to Instruction Learning: Task Adapters Generation\n  from Instructions"
                },
                "summary": "Large language models (LLMs) have acquired the ability to solve general tasks\nby utilizing instruction finetuning (IFT). However, IFT still relies heavily on\ninstance training of extensive task data, which greatly limits the adaptability\nof LLMs to real-world scenarios where labeled task instances are scarce and\nbroader task generalization becomes paramount. Contrary to LLMs, humans acquire\nskills and complete tasks not merely through repeated practice but also by\nunderstanding and following instructional guidelines. This paper is dedicated\nto simulating human learning to address the shortcomings of instance training,\nfocusing on instruction learning to enhance cross-task generalization. Within\nthis context, we introduce Task Adapters Generation from Instructions (TAGI),\nwhich automatically constructs the task-specific model in a parameter\ngeneration manner based on the given task instructions without retraining for\nunseen tasks. Specifically, we utilize knowledge distillation to enhance the\nconsistency between TAGI developed through Learning with Instruction and\ntask-specific models developed through Training with Instance, by aligning the\nlabels, output logits, and adapter parameters between them. TAGI is endowed\nwith cross-task generalization capabilities through a two-stage training\nprocess that includes hypernetwork pretraining and finetuning. We evaluate TAGI\non the Super-Natural Instructions and P3 datasets. The experimental results\ndemonstrate that TAGI can match or even outperform traditional meta-trained\nmodels and other hypernetwork models, while significantly reducing\ncomputational requirements.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have acquired the ability to solve general tasks\nby utilizing instruction finetuning (IFT). However, IFT still relies heavily on\ninstance training of extensive task data, which greatly limits the adaptability\nof LLMs to real-world scenarios where labeled task instances are scarce and\nbroader task generalization becomes paramount. Contrary to LLMs, humans acquire\nskills and complete tasks not merely through repeated practice but also by\nunderstanding and following instructional guidelines. This paper is dedicated\nto simulating human learning to address the shortcomings of instance training,\nfocusing on instruction learning to enhance cross-task generalization. Within\nthis context, we introduce Task Adapters Generation from Instructions (TAGI),\nwhich automatically constructs the task-specific model in a parameter\ngeneration manner based on the given task instructions without retraining for\nunseen tasks. Specifically, we utilize knowledge distillation to enhance the\nconsistency between TAGI developed through Learning with Instruction and\ntask-specific models developed through Training with Instance, by aligning the\nlabels, output logits, and adapter parameters between them. TAGI is endowed\nwith cross-task generalization capabilities through a two-stage training\nprocess that includes hypernetwork pretraining and finetuning. We evaluate TAGI\non the Super-Natural Instructions and P3 datasets. The experimental results\ndemonstrate that TAGI can match or even outperform traditional meta-trained\nmodels and other hypernetwork models, while significantly reducing\ncomputational requirements."
                },
                "authors": [
                    {
                        "name": "Huanxuan Liao"
                    },
                    {
                        "name": "Yao Xu"
                    },
                    {
                        "name": "Shizhu He"
                    },
                    {
                        "name": "Yuanzhe Zhang"
                    },
                    {
                        "name": "Yanchao Hao"
                    },
                    {
                        "name": "Shengping Liu"
                    },
                    {
                        "name": "Kang Liu"
                    },
                    {
                        "name": "Jun Zhao"
                    }
                ],
                "author_detail": {
                    "name": "Jun Zhao"
                },
                "author": "Jun Zhao",
                "arxiv_comment": "accepted NeurIPS 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.12382v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.12382v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.22782v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.22782v1",
                "updated": "2024-10-30T07:53:52Z",
                "updated_parsed": [
                    2024,
                    10,
                    30,
                    7,
                    53,
                    52,
                    2,
                    304,
                    0
                ],
                "published": "2024-10-30T07:53:52Z",
                "published_parsed": [
                    2024,
                    10,
                    30,
                    7,
                    53,
                    52,
                    2,
                    304,
                    0
                ],
                "title": "MALoRA: Mixture of Asymmetric Low-Rank Adaptation for Enhanced\n  Multi-Task Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MALoRA: Mixture of Asymmetric Low-Rank Adaptation for Enhanced\n  Multi-Task Learning"
                },
                "summary": "Parameter-Efficient Fine-Tuning (PEFT) methods like LoRA have significantly\nimproved the adaptation of LLMs to downstream tasks in a resource-efficient\nmanner. However, in multi-task scenarios, challenges such as training imbalance\nand the seesaw effect frequently emerge. Mixture-of-LoRA (MoLoRA), which\ncombines LoRA with sparse Mixture-of-Experts, mitigates some of these issues by\npromoting task-specific learning across experts. Despite this, MoLoRA remains\ninefficient in terms of training speed, parameter utilization, and overall\nmulti-task performance. In this paper, we propose Mixture of Asymmetric\nLow-Rank Adaptaion (MALoRA), a flexible fine-tuning framework that leverages\nasymmetric optimization across LoRA experts. MALoRA reduces the number of\ntrainable parameters by 30% to 48%, increases training speed by 1.2x, and\nmatches the computational efficiency of single-task LoRA models. Additionally,\nMALoRA addresses overfitting issues commonly seen in high-rank configurations,\nenhancing performance stability. Extensive experiments across diverse\nmulti-task learning scenarios demonstrate that MALoRA consistently outperforms\nall baseline methods in both inter-domain and intra-domain tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Parameter-Efficient Fine-Tuning (PEFT) methods like LoRA have significantly\nimproved the adaptation of LLMs to downstream tasks in a resource-efficient\nmanner. However, in multi-task scenarios, challenges such as training imbalance\nand the seesaw effect frequently emerge. Mixture-of-LoRA (MoLoRA), which\ncombines LoRA with sparse Mixture-of-Experts, mitigates some of these issues by\npromoting task-specific learning across experts. Despite this, MoLoRA remains\ninefficient in terms of training speed, parameter utilization, and overall\nmulti-task performance. In this paper, we propose Mixture of Asymmetric\nLow-Rank Adaptaion (MALoRA), a flexible fine-tuning framework that leverages\nasymmetric optimization across LoRA experts. MALoRA reduces the number of\ntrainable parameters by 30% to 48%, increases training speed by 1.2x, and\nmatches the computational efficiency of single-task LoRA models. Additionally,\nMALoRA addresses overfitting issues commonly seen in high-rank configurations,\nenhancing performance stability. Extensive experiments across diverse\nmulti-task learning scenarios demonstrate that MALoRA consistently outperforms\nall baseline methods in both inter-domain and intra-domain tasks."
                },
                "authors": [
                    {
                        "name": "Xujia Wang"
                    },
                    {
                        "name": "Haiyan Zhao"
                    },
                    {
                        "name": "Shuo Wang"
                    },
                    {
                        "name": "Hanqing Wang"
                    },
                    {
                        "name": "Zhiyuan Liu"
                    }
                ],
                "author_detail": {
                    "name": "Zhiyuan Liu"
                },
                "author": "Zhiyuan Liu",
                "arxiv_comment": "14 pages, 5 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.22782v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.22782v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.7",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.19608v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.19608v2",
                "updated": "2024-10-30T07:52:32Z",
                "updated_parsed": [
                    2024,
                    10,
                    30,
                    7,
                    52,
                    32,
                    2,
                    304,
                    0
                ],
                "published": "2024-09-29T08:18:50Z",
                "published_parsed": [
                    2024,
                    9,
                    29,
                    8,
                    18,
                    50,
                    6,
                    273,
                    0
                ],
                "title": "Causal Deciphering and Inpainting in Spatio-Temporal Dynamics via\n  Diffusion Model",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Causal Deciphering and Inpainting in Spatio-Temporal Dynamics via\n  Diffusion Model"
                },
                "summary": "Spatio-temporal (ST) prediction has garnered a De facto attention in earth\nsciences, such as meteorological prediction, human mobility perception.\nHowever, the scarcity of data coupled with the high expenses involved in sensor\ndeployment results in notable data imbalances. Furthermore, models that are\nexcessively customized and devoid of causal connections further undermine the\ngeneralizability and interpretability. To this end, we establish a causal\nframework for ST predictions, termed CaPaint, which targets to identify causal\nregions in data and endow model with causal reasoning ability in a two-stage\nprocess. Going beyond this process, we utilize the back-door adjustment to\nspecifically address the sub-regions identified as non-causal in the upstream\nphase. Specifically, we employ a novel image inpainting technique. By using a\nfine-tuned unconditional Diffusion Probabilistic Model (DDPM) as the generative\nprior, we in-fill the masks defined as environmental parts, offering the\npossibility of reliable extrapolation for potential data distributions. CaPaint\novercomes the high complexity dilemma of optimal ST causal discovery models by\nreducing the data generation complexity from exponential to quasi-linear\nlevels. Extensive experiments conducted on five real-world ST benchmarks\ndemonstrate that integrating the CaPaint concept allows models to achieve\nimprovements ranging from 4.3% to 77.3%. Moreover, compared to traditional\nmainstream ST augmenters, CaPaint underscores the potential of diffusion models\nin ST enhancement, offering a novel paradigm for this field. Our project is\navailable at https://anonymous.4open.science/r/12345-DFCC.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Spatio-temporal (ST) prediction has garnered a De facto attention in earth\nsciences, such as meteorological prediction, human mobility perception.\nHowever, the scarcity of data coupled with the high expenses involved in sensor\ndeployment results in notable data imbalances. Furthermore, models that are\nexcessively customized and devoid of causal connections further undermine the\ngeneralizability and interpretability. To this end, we establish a causal\nframework for ST predictions, termed CaPaint, which targets to identify causal\nregions in data and endow model with causal reasoning ability in a two-stage\nprocess. Going beyond this process, we utilize the back-door adjustment to\nspecifically address the sub-regions identified as non-causal in the upstream\nphase. Specifically, we employ a novel image inpainting technique. By using a\nfine-tuned unconditional Diffusion Probabilistic Model (DDPM) as the generative\nprior, we in-fill the masks defined as environmental parts, offering the\npossibility of reliable extrapolation for potential data distributions. CaPaint\novercomes the high complexity dilemma of optimal ST causal discovery models by\nreducing the data generation complexity from exponential to quasi-linear\nlevels. Extensive experiments conducted on five real-world ST benchmarks\ndemonstrate that integrating the CaPaint concept allows models to achieve\nimprovements ranging from 4.3% to 77.3%. Moreover, compared to traditional\nmainstream ST augmenters, CaPaint underscores the potential of diffusion models\nin ST enhancement, offering a novel paradigm for this field. Our project is\navailable at https://anonymous.4open.science/r/12345-DFCC."
                },
                "authors": [
                    {
                        "name": "Yifan Duan"
                    },
                    {
                        "name": "Jian Zhao"
                    },
                    {
                        "name": "pengcheng"
                    },
                    {
                        "name": "Junyuan Mao"
                    },
                    {
                        "name": "Hao Wu"
                    },
                    {
                        "name": "Jingyu Xu"
                    },
                    {
                        "name": "Shilong Wang"
                    },
                    {
                        "name": "Caoyuan Ma"
                    },
                    {
                        "name": "Kai Wang"
                    },
                    {
                        "name": "Kun Wang"
                    },
                    {
                        "name": "Xuelong Li"
                    }
                ],
                "author_detail": {
                    "name": "Xuelong Li"
                },
                "author": "Xuelong Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.19608v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.19608v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.06629v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.06629v2",
                "updated": "2024-10-30T07:46:35Z",
                "updated_parsed": [
                    2024,
                    10,
                    30,
                    7,
                    46,
                    35,
                    2,
                    304,
                    0
                ],
                "published": "2024-10-09T07:23:13Z",
                "published_parsed": [
                    2024,
                    10,
                    9,
                    7,
                    23,
                    13,
                    2,
                    283,
                    0
                ],
                "title": "Application of Large Language Models to Quantum State Simulation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Application of Large Language Models to Quantum State Simulation"
                },
                "summary": "Quantum computers leverage the unique advantages of quantum mechanics to\nachieve acceleration over classical computers for certain problems. Currently,\nvarious quantum simulators provide powerful tools for researchers, but\nsimulating quantum evolution with these simulators often incurs high time\ncosts. Additionally, resource consumption grows exponentially as the number of\nquantum bits increases. To address this issue, our research aims to utilize\nLarge Language Models (LLMs) to simulate quantum circuits. This paper details\nthe process of constructing 1-qubit and 2-qubit quantum simulator models,\nextending to multiple qubits, and ultimately implementing a 3-qubit example.\nOur study demonstrates that LLMs can effectively learn and predict the\nevolution patterns among quantum bits, with minimal error compared to the\ntheoretical output states. Even when dealing with quantum circuits comprising\nan exponential number of quantum gates, LLMs remain computationally efficient.\nOverall, our results highlight the potential of LLMs to predict the outputs of\ncomplex quantum dynamics, achieving speeds far surpassing those required to run\nthe same process on a quantum computer. This finding provides new insights and\ntools for applying machine learning methods in the field of quantum computing.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Quantum computers leverage the unique advantages of quantum mechanics to\nachieve acceleration over classical computers for certain problems. Currently,\nvarious quantum simulators provide powerful tools for researchers, but\nsimulating quantum evolution with these simulators often incurs high time\ncosts. Additionally, resource consumption grows exponentially as the number of\nquantum bits increases. To address this issue, our research aims to utilize\nLarge Language Models (LLMs) to simulate quantum circuits. This paper details\nthe process of constructing 1-qubit and 2-qubit quantum simulator models,\nextending to multiple qubits, and ultimately implementing a 3-qubit example.\nOur study demonstrates that LLMs can effectively learn and predict the\nevolution patterns among quantum bits, with minimal error compared to the\ntheoretical output states. Even when dealing with quantum circuits comprising\nan exponential number of quantum gates, LLMs remain computationally efficient.\nOverall, our results highlight the potential of LLMs to predict the outputs of\ncomplex quantum dynamics, achieving speeds far surpassing those required to run\nthe same process on a quantum computer. This finding provides new insights and\ntools for applying machine learning methods in the field of quantum computing."
                },
                "authors": [
                    {
                        "name": "Shuangxiang Zhou"
                    },
                    {
                        "name": "Ronghang Chen"
                    },
                    {
                        "name": "Zheng An"
                    },
                    {
                        "name": "Shi-Yao Hou"
                    }
                ],
                "author_detail": {
                    "name": "Shi-Yao Hou"
                },
                "author": "Shi-Yao Hou",
                "arxiv_comment": "13 pages, 18 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.06629v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.06629v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "quant-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "quant-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.22770v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.22770v1",
                "updated": "2024-10-30T07:39:42Z",
                "updated_parsed": [
                    2024,
                    10,
                    30,
                    7,
                    39,
                    42,
                    2,
                    304,
                    0
                ],
                "published": "2024-10-30T07:39:42Z",
                "published_parsed": [
                    2024,
                    10,
                    30,
                    7,
                    39,
                    42,
                    2,
                    304,
                    0
                ],
                "title": "InjecGuard: Benchmarking and Mitigating Over-defense in Prompt Injection\n  Guardrail Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "InjecGuard: Benchmarking and Mitigating Over-defense in Prompt Injection\n  Guardrail Models"
                },
                "summary": "Prompt injection attacks pose a critical threat to large language models\n(LLMs), enabling goal hijacking and data leakage. Prompt guard models, though\neffective in defense, suffer from over-defense -- falsely flagging benign\ninputs as malicious due to trigger word bias. To address this issue, we\nintroduce NotInject, an evaluation dataset that systematically measures\nover-defense across various prompt guard models. NotInject contains 339 benign\nsamples enriched with trigger words common in prompt injection attacks,\nenabling fine-grained evaluation. Our results show that state-of-the-art models\nsuffer from over-defense issues, with accuracy dropping close to random\nguessing levels (60%). To mitigate this, we propose InjecGuard, a novel prompt\nguard model that incorporates a new training strategy, Mitigating Over-defense\nfor Free (MOF), which significantly reduces the bias on trigger words.\nInjecGuard demonstrates state-of-the-art performance on diverse benchmarks\nincluding NotInject, surpassing the existing best model by 30.8%, offering a\nrobust and open-source solution for detecting prompt injection attacks. The\ncode and datasets are released at https://github.com/SaFoLab-WISC/InjecGuard.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Prompt injection attacks pose a critical threat to large language models\n(LLMs), enabling goal hijacking and data leakage. Prompt guard models, though\neffective in defense, suffer from over-defense -- falsely flagging benign\ninputs as malicious due to trigger word bias. To address this issue, we\nintroduce NotInject, an evaluation dataset that systematically measures\nover-defense across various prompt guard models. NotInject contains 339 benign\nsamples enriched with trigger words common in prompt injection attacks,\nenabling fine-grained evaluation. Our results show that state-of-the-art models\nsuffer from over-defense issues, with accuracy dropping close to random\nguessing levels (60%). To mitigate this, we propose InjecGuard, a novel prompt\nguard model that incorporates a new training strategy, Mitigating Over-defense\nfor Free (MOF), which significantly reduces the bias on trigger words.\nInjecGuard demonstrates state-of-the-art performance on diverse benchmarks\nincluding NotInject, surpassing the existing best model by 30.8%, offering a\nrobust and open-source solution for detecting prompt injection attacks. The\ncode and datasets are released at https://github.com/SaFoLab-WISC/InjecGuard."
                },
                "authors": [
                    {
                        "name": "Hao Li"
                    },
                    {
                        "name": "Xiaogeng Liu"
                    },
                    {
                        "name": "Chaowei Xiao"
                    }
                ],
                "author_detail": {
                    "name": "Chaowei Xiao"
                },
                "author": "Chaowei Xiao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.22770v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.22770v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.22767v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.22767v1",
                "updated": "2024-10-30T07:36:23Z",
                "updated_parsed": [
                    2024,
                    10,
                    30,
                    7,
                    36,
                    23,
                    2,
                    304,
                    0
                ],
                "published": "2024-10-30T07:36:23Z",
                "published_parsed": [
                    2024,
                    10,
                    30,
                    7,
                    36,
                    23,
                    2,
                    304,
                    0
                ],
                "title": "Beyond Ontology in Dialogue State Tracking for Goal-Oriented Chatbot",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Beyond Ontology in Dialogue State Tracking for Goal-Oriented Chatbot"
                },
                "summary": "Goal-oriented chatbots are essential for automating user tasks, such as\nbooking flights or making restaurant reservations. A key component of these\nsystems is Dialogue State Tracking (DST), which interprets user intent and\nmaintains the dialogue state. However, existing DST methods often rely on fixed\nontologies and manually compiled slot values, limiting their adaptability to\nopen-domain dialogues. We propose a novel approach that leverages instruction\ntuning and advanced prompt strategies to enhance DST performance, without\nrelying on any predefined ontologies. Our method enables Large Language Model\n(LLM) to infer dialogue states through carefully designed prompts and includes\nan anti-hallucination mechanism to ensure accurate tracking in diverse\nconversation contexts. Additionally, we employ a Variational Graph Auto-Encoder\n(VGAE) to model and predict subsequent user intent. Our approach achieved\nstate-of-the-art with a JGA of 42.57% outperforming existing ontology-less DST\nmodels, and performed well in open-domain real-world conversations. This work\npresents a significant advancement in creating more adaptive and accurate\ngoal-oriented chatbots.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Goal-oriented chatbots are essential for automating user tasks, such as\nbooking flights or making restaurant reservations. A key component of these\nsystems is Dialogue State Tracking (DST), which interprets user intent and\nmaintains the dialogue state. However, existing DST methods often rely on fixed\nontologies and manually compiled slot values, limiting their adaptability to\nopen-domain dialogues. We propose a novel approach that leverages instruction\ntuning and advanced prompt strategies to enhance DST performance, without\nrelying on any predefined ontologies. Our method enables Large Language Model\n(LLM) to infer dialogue states through carefully designed prompts and includes\nan anti-hallucination mechanism to ensure accurate tracking in diverse\nconversation contexts. Additionally, we employ a Variational Graph Auto-Encoder\n(VGAE) to model and predict subsequent user intent. Our approach achieved\nstate-of-the-art with a JGA of 42.57% outperforming existing ontology-less DST\nmodels, and performed well in open-domain real-world conversations. This work\npresents a significant advancement in creating more adaptive and accurate\ngoal-oriented chatbots."
                },
                "authors": [
                    {
                        "name": "Sejin Lee"
                    },
                    {
                        "name": "Dongha Kim"
                    },
                    {
                        "name": "Min Song"
                    }
                ],
                "author_detail": {
                    "name": "Min Song"
                },
                "author": "Min Song",
                "arxiv_comment": "There are 10 chapters, including references, and 2 figures used. To\n  be presented at the 15th IEEE International Conference on Knowledge Graphs\n  (ICKG2024)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.22767v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.22767v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            }
        ]
    }
]